dynamic conditional independence models markov chain monte carlo methods carlo nicola best walter gilks dipartimento di informatica sistemistica universita di pavia pavia italy 
tel fax mail carlo laplace mrc biostatistics unit institute public health robinson way cambridge cb sr tel fax mail gilks mrc cam ac uk dynamic statistical modeling situations observations arise sequentially causing model expand progressive incorporation new data items new unknown parameters 
example clinical monitoring new patient specific parameters introduced new patient 
markov chain monte carlo mcmc posterior inference need redone expansion stage 
methods slow real time implementation 
combining mcmc importance resampling show real time posterior updating effected 
proposed dynamic sampling algorithms utilize posterior samples previous expansion stages exploit conditional independence groups parameters allow samples parameters longer interest discarded patient dies discharged 
apply methods monitoring heart transplant recipients infection 
key words bayesian inference conditional independence model dynamic model graphical model importance sampling markov chain monte carlo metropolis hastings algorithm sequential updating 
develop sampling methods models observations arise sequentially 
interest applications analysis incoming data required real time clinical monitoring 
suppose model expands progressively incorporating new data new parameters 
example clinical monitoring new patient specific parameters introduced new patient 
loss generality imagine observations arrive integer times 
time new data accompanied possibly empty set new model parameters missing data phi model comprises unknowns phi phi dynamic model dm data parameters incorporated expansion stage may stages uninteresting patient dies discharged 
word parameter mean model unknown including missing data 
sampling methods bayesian inference prediction include importance sampling markov chain monte carlo mcmc 
suppose time sample values phi phi posterior distribution phi phi jf 
arrival new data item shifts interest new posterior phi phi jf prompting generate new sample values phi phi phi phi jf 
computing new sample sensible try information contained available sample conventional mcmc sampling possible new data item available sample parameter values discarded new sample created restarting mcmc scratch entire model 
waste information causes response new data slow 
particular hamper application method real time contexts 
difficulty avoided adopting sampling methods mcmc 
kong liu wong henceforth klw propose method sequential updating posterior distributions importance sampling 
klw retain original parameter sample replacing unweighted sample averages weighted averages 
method incorporates new data item adapting weights 
method directly applicable dms expanding parameter space 
smith gelfand propose sampling importance resampling sir sequential updating scheme 
discuss sequential analysis data dynamic hierarchical model special case dms 
obtain closed forms posterior predictive distributions interest 
doing assume knowledge variance matrices scalar factor linearity structural equations error normality 
west considers sequential analysis non linear dynamic models sampling method uses kernel density reconstruction techniques coupled importance resampling 
propose methods respects developments klw 
adapts importance sampling approach expanding parameter spaces second combines importance sampling mcmc sampling 
methods exploit conditional independence groups model parameters allowing sampled values parameters longer interest discarded 
section assume general conditional independence structure dm describe graph whittaker 
graph expands progressive incorporation new parameter data nodes 
sections propose alternative methods sampling dm 
illustrate methods types problem motivate consider section application patient monitoring 
dynamic conditional independence models structure dynamics dm section characterize class dms terms conditional independence structure 
assume time parameters phi phi divided sets delta ffl data divided sets way conditional independence structure holds 
represents phi represents phi phi 
undirected graph states variables phi conditionally independent delta ffl 
call delta disengaged nodes ffl engaged nodes 
algorithms proposed assume longer interested disengaged nodes allows updating posterior distributions engaged parameters needing access disengaged nodes 
context patient monitoring disengaged nodes include parameters data patients died time model time may number ways partitioning current parameters data sets depicted 
example delta empty 
case conditional independence assumption 
algorithm updating posterior distributions modeller disengaged engaged structure general dm 
circles represent unknown parameters rectangles represent data 
leftmost nodes represent data parameters arrived time remaining nodes represent data parameters arriving interest allocate parameters data possible disengaged portion graph obviously care conditional independence structure model consistent 
time new data item arrives accompanied new parameters phi structure dm just variables appended depicted 
information contained processed assimilated dm restructures graph ready incorporate observation restructuring parameters set phi engaged disengaged flows ffl delta similarly elements engaged disengaged flows usually necessarily parameter data item dwells engaged portion dm number expansion stages flowing disengaged portion 
disengaged remain forever 
possible conditional independence assumptions represented 
mechanism summarized phi ffl delta delta delta dynamic posterior sampling denotes joint distribution xjy denotes conditional distribution denotes marginal distribution shall denote collection samples distribution interest 
disengaged engaged new disengaged engaged just appending just assimilating structure general dm just appending observation observation assimilated model ready accept observation suppose time posterior sample size fffl ffl jd assimilation information contained data consists calculating sample size ffl phi ffl phi jd distribution parameters interest posterior observing prepare observations reconfigure graph 
ffl ffl phi extract available sample sample fffl sample form appropriate time 
describe methods generating sample making sample computational efficiency 
process chosen stage computing method originator sample fffl sampling importance resampling sir consider generic time suppose sample fffl 
assimilate information contained new observation propose method sampling importance resampling sir rubin 
step 
sampling sample single value phi phi jffl distribution having support full conditional distribution phi jffl 
obtain joint sample fffl phi step 
importance resampling generate sample ffl phi sampling replacement fffl phi specifically select th member fffl phi set ffl phi ffl phi probability proportional phi jffl phi jffl jffl theta phi jffl phi jffl evaluated ffl phi 
observation available reconfigure graph described section 
note depend disengaged portion graph delta shown proportional usual form importance weight delta ffl jd delta ffl jd phi jffl conditional independence assumptions 
new data parameters phi assimilated disengaged nodes applications comprise greater part graph 
primary motivation conditional independence assumptions heart methodology 
method klw viewed special case sir chosen conditional posterior phi jffl resampling step avoided posterior sample assimilation simply sample fffl phi generated step 
klw method resampling keeps element sample importance weight obtained multiplicative accumulation weights updating stages current th stage 
liu chen shall refer henceforth lc propose interesting modification klw method resampling similar step occasionally carried importance weights reset 
lc suggest performing step time variance weights high justify 
resampling updating stage may lead progressive sample 
circumstances modification sir algorithm accumulate weights updating stages suggested lc may beneficial 
lc suggest resampling steps may better resampling provide theoretical support suggestion 
intuitive explanation resampling kills elements sample placed far main body posterior distribution sampling stage model expansion effective portion samples created preceding stages 
posterior inference sir algorithm described produces stage samples fffl approximately drawn posterior distribution ffl jd time common monte carlo methods bayesian inference samples straightforwardly summarize posterior attribute interest 
example posterior mean parameter approximated mean sample fffl similarly posterior variances covariances quantiles 
similarly kernel density estimates ffl jd posterior sample 
principle approximations accurate desired increasing sample sizes fn specifically function ffl sample mean ffl simulation consistent estimator mean ffl jd see appendix 
variance estimator large sample sizes shown appendix theta gamma ts denotes expectation posterior distribution ffl jd denotes expectation importance sampling distribution phi jffl ts delta ffl jd delta ffl jd phi jffl gamma gamma resampling stage contributes additional variance component 
unfortunately simulation consistent estimator available output samples 
jensen inequality upper bound gamma ts gamma ts noting ts gamma defined simulation consistent estimator ffl gamma index sample stage survives sample stage calculation give retrospective guidance adequacy sample sizes fn sample sizes judged inadequate efficient batch sampling stages see section repeat sequentially 
general difficult 
informal useful indicator sample size adequacy provided number originator samples fffl survive stage importance distributions method involving importance sampling computational efficiency essential distribution easy sample contribution small 
implies ts small extreme values ts extreme assured small extreme difficult guarantee practice different functions may simultaneously interest 
reasonable strategy choose small extreme phi tails phi jffl dominate phi jffl 
natural candidates prior distribution prior phi jffl prior jffl phi posterior full conditional distribution post phi jffl post jffl importance weights prior post derived 
klw post data carry little information phi prior may choice tails dominate phi jffl 
carries substantial information phi prior poor choice samples generated prior fall outside region high posterior support 
contrast post deliver samples region high posterior support 
post may easy sample importance weights post may available closed form 
post standard distribution tailored methods available sampling univariate log concave adaptive rejection sampling gilks wild 
usual problems sampling posteriors apply 
usual solution mcmc sampling 
situation entail running mcmc convergence obtain phi step 
describe proposed alternative algorithm 
metropolis hastings importance resampling section generic time suppose sample 
assimilate information contained new observation propose algorithm steps step set 
sample integer uniformly set put ffl ffl step sample single value phi single iteration metropolis hastings algorithm metropolis hastings having phi ffl stationary distribution 
step increment sample uniformly set probability min phi gamma jffl phi gamma ffl gamma set ffl ffl set ffl ffl gamma algorithm iterates round steps 
mcmc sampling number initial iterations usually discarded burn phase markov chain 
iterations performed generate sequence ffl phi desired sample 
observation available reconfigure graph described section 
algorithm combines principles metropolis hastings algorithm importance resampling call metropolis hastings importance resampling 
posterior inference stationary distribution chain stage ffl phi ffi ffl ffl jffl phi jffl jffl easily seen noting steps define metropolis hastings transition kernels stationary distributions full conditional distributions phi ffl note support restricted set fffl stage sir section post generates independent samples 
follows importance weight post depend phi simulation consistent estimator posterior mean stage 
variance formulae apply multiplied efficiency factor reflecting autocorrelations ffl phi positively autocorrelated samples efficiency factor 
efficiency factors estimated methods described ripley geyer 
comparison methods possible sample directly post evaluate closed form sir efficient 
sir different sampling distribution prior need 
situation efficient sir despite dependent samples 
stress general sir preferable rapidly mixing mcmc delta ffl phi jd sufficient time sir produce samples restricted support discussed section 
methods designed situations rapid response required 
section compare sir application patient monitoring 
processing data batches dynamic sampling free caveats 
sir previously values drawn posterior distribution parameter moved engaged portion graph changes posterior distribution induced incoming data accounted solely importance weights 
unfortunately posterior changes pronounced stage model expansion samples longer adequately represent current posterior distribution parameters 
typical symptom situation high dispersion importance weights leading large values variances 
problem may alleviated processing data batches assimilating simultaneously data acquired interval time avoiding intermediate resampling stages consequent accumulation terms variance formula 
klw lc recommend similar strategy 
example stage may process data acquired interval single batch call full batch processing 
methodology applies immediately simple notational device rescaling time phi representing data parameters acquired time interval 
illustrative application compare previously described algorithms illustrate types problem may useful application patient monitoring 
application uses hierarchical random effects model represent conditionally independent observations series patients monitoring 
time point patients longer observation 
died belong disengaged section model patients monitored kept engaged section 
new observation generated monitoring causes model expand append new data nodes 
addition time new subject enters monitoring new patient specific parameters appended 
context dynamic sampling may receiving new patient observation rapidly update posterior distribution subject specific parameters individual engaged patients parameters disengaged patients 
may prove useful applications involving line patient forecasting 
specific application described concerns monitoring patients heart 
clinical background data drugs prevent rejection organ transplant recipients may patients severe infections cmv 
cmv infection manifest similar manner ordinary flu fever 
patients progress point called deterioration patient develops life threatening symptoms infection lungs liver tract eyes central nervous system 
patients develop deterioration called symptomatic 
cmv infected patients monitored serial measurements marker called cmv provides direct measure amount cmv antigens blood 
commercially available drugs shown inhibit cmv replication may cause severe side effects decrease platelet count renal impairment 
drugs usually administered deterioration occurred patient 
argue earlier intervention reduce risk serious complications 
time avoid imposing drug associated side effects patients defeat virus developing deterioration 
propose predictive approach treatment cmv infection patient carefully monitored serial measurements cmv treatment started measured amounts predictive imminent deterioration 
prediction model described 
data heart transplant recipients followed time developed cmv infection 
data kindly provided dr grossi san pavia italy 
shows individual profiles logarithmic scale time zero represents time onset infection 
profiles patients developed deterioration shown solid curves time deterioration represented dot post deterioration data considered analysis 
profiles patients remaining deterioration free asymptomatic patients shown broken lines 
basis expert medical opinion log profiles shown adopted log linear growth curve model represent progression log ff log ff true values log patient times days onset infection log cmv asymptomatic symptomatic deterioration log cmv profiles patients studied 
time represents time onset infection 
profiles patients observed deteriorate drawn solid curves time deterioration time 
remaining profiles appear dashed curves 
respectively patient specific rate change log 
log measurement errors assumed follow student degrees freedom zero mean unknown precision parameter error variance oe gamma vague gamma prior distribution assumed patient specific intercept rate parameters allow unobserved heterogeneity individuals 
introduce tendency similarity patients distribution population taken bivariate normal unknown mean covariance sigma 
mean assumed priori follow bivariate normal distribution zero mean covariance whilst inverse covariance sigma gamma assumed priori follow wishart distribution degrees freedom scale matrix reflects vague prior information distribution relative provided data 
suggests patients deteriorate rises threshold value logarithmic scale 
consequently risk deterioration modelled piecewise linear hazard function ffi fi log ff gamma fl denotes max 
hazard rate patient time assumed small constant value ffi log remains unknown threshold fl hazard increases linearly rate fi logarithm threshold exceeded 
value ffi arbitrarily fixed reasons identifiability corresponds negligible hazard rate log threshold fl time measured days 
vague prior information represented assuming normal distribution zero mean large variance fl uniform distribution fi constrains fi positive 
dynamic updating patient profiles data available retrospectively example 
randomly selected heart transplant recipients act new patients currently clinical observation remaining previous patients longer monitored 
originator sample replicates model parameters generated full gibbs sampling available data previous patients 
data individual specific parameters associated patients allocated disengaged portion dm whilst population level parameters remained engaged section graph 
new measurements available new patients 
notation previous sections gives dm delta ffl fi fl sigmag phi sir update projected log profile new patient predict risk deterioration occurring days 
note table posterior means credible regions engaged parameters ffl processing data new patients 
parameter gs sir oe sigma sigma sigma gamma gamma gamma gamma gamma gamma gamma gamma gamma fl fi sir phi parameters generated bivariate normal distribution sigma prior section 
provide gold standard compare predictions sir restarted full gibbs sampler gs scratch include additional patient data 
results iteration update took minutes seconds seconds gs sir methods respectively mhz computer gb memory 
note samples discarded burn phase markov chain gs methods 
tables give posterior mean credible interval estimates engaged ffl new patient specific slope parameters phi respectively update methods gs sir 
shows comparison observed data predicted log profile risk deterioration new patients estimated update method 
patient randomly chosen new patients experienced deterioration whilst patients randomly chosen patients remained asymptomatic 
tables show estimates precise similar obtained full gibbs sampling 
computation time method order magnitude faster 
contrast sir method generates table posterior means credible regions slope parameters phi new patient processing new data 
patient gs sir gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma patient patient patient gs sir days infection onset predicted log profile risk clinical deterioration patients monitored example 
black dots show new data included update open circles indicate data available 
shaded band shows estimated credible region predicted log profile percentage value gives mean predicted risk deterioration occurring days time marked dashed line patient arrow marks time deterioration observed 
extremely poor estimates parameters predicted log credible regions reducing single value 
sir method begins generating phi parameters distribution prior inclusion new data 
sample posterior distribution obtained re weighting values likelihood new data see equation 
data accumulate posterior distribution tends diverge prior 
importance weights tend small occasional large values leading impoverished posterior sample 
contrast method effectively generates phi parameters directly approximate posterior distribution see section 
sir adapt situations new data conflicts prior 
example see predictions patient observed log profile declines rapidly day conflicts mainly large positive values mean log slope parameter prior population distribution 
sir predictions may improved generating larger originator sample resampling values update 
increase computational time 
alternatively justifiably argue problem stems mis specification model example linear growth curve clearly inappropriate patients 
patients measurements appear conform model patient method performs better sir method compared gold standard gs approach 
medical monitoring problem described section illustrative class mcmc applications addressed methods 
observations generated monitoring medical patient may arrive frequently response observation may required real time 
example clinician patient want immediate assessment prognostic import new observation patient 
kind circumstances may insufficient time new observation adequately converged mcmc sampling entire joint posterior distribution 
solution update posterior summaries parameters immediate interest 
patients critical conditions involving sampling parameters longer interest 
patients lost follow 
application areas proposed methods may potential interest include problems prediction long time series rapidly growing parameter space methods monitoring model adequacy 
example category consider prequential method model criticism proposed dawid 
implementation approach requires computing probability observation predictive distribution conditional previous observations 
forward predictive distributions associated individual data items may easily computed product proposed algorithms 
improvements proposed schemes possible 
currently methods parameter engaged new values sampled subsequent updating stages 
consequently data continue arrive ability samples cover posterior region interest may deteriorate cause approximation break 
problem may partly remedied proposed batch processing method 
solution updating stage available samples engaged parameters construct kernel density estimate current joint posterior distribution parameters renewed sample drawn west 
completely satisfactory solution problem necessary methods apply time time full mcmc scratch samples totally renewed 
applications renewal carried 
limit may envisage mcmc process set model parameters constantly running background proposed algorithms invoked times need immediate response new data 
note specific application refinements methodology may possible 
example klw suggests wide class problems may parameters interest analytically integrated imputed mcmc sampling gain statistical computational efficiency 
research partially funded eec aim project murst percent italian research council progetto sistemi 
data kindly provided dr grossi san pavia italy 
thomas spiegelhalter valuable suggestions 
billingsley 
probability measure nd edn 
john wiley new york 
dawid 
statistical theory prequential approach discussion 
roy 
statist 
soc 
ser 

dawid 
inference likelihood prequential frames discussion roy 
statist 
soc 
ser 


dynamic hierarchical models 
statist 
soc 

geyer 
practical markov chain monte carlo 
statist 
sci 
gilks wild 
adaptive rejection sampling gibbs sampling 
applied statistics 
hastings 
monte carlo sampling methods markov chains applications 
biometrika 
kong liu wong 
sequential imputations bayesian missing data problems 
technical report dept statistics university chicago harvard university 
liu chen 
blind deconvolution sequential imputations 
amer 
statist 
assoc appear 
metropolis rosenbluth rosenbluth teller teller 
equations state calculations fast computing machine 
chem 
phys 
ripley 
stochastic simulation john wiley sons new york 
rubin 
sampling importance resampling alternative data augmentation algorithm creating imputations fractions missing information modest sir algorithm 
comment tanner wong calculation posterior distributions data augmentation discussion 
amer 
statist 
assoc 
smith gelfand 
bayesian statistics tears 
american statistician 
west 
modelling mixtures discussion 
bayesian statistics eds bernardo berger dawid smith oxford university press 
whittaker 
graphical models applied multivariate statistics john wiley new york 
appendix proof variance formula notation convenient express expanding multistage sir algorithm section different notation 
denote vector disengaged engaged parameters expansion stage denote posterior distribution stage jx importance sampling density stage 
kt jx gamma expanding multistage sir algorithm follows stage sample independently stage gamma sample tj jx gamma 
sample tj gamma gamma ti gamma gamma ti gamma gamma gamma ti independently completing stage calculate ki 
ti defining empty set 
denote expectation distribution 
similarly denote expectation distribution jx 
define gjs gamma function gamma define 
define jx gamma du kt gw kt jx kt jx kt jx kt jx gamma kt note kt kt functions gamma central limit theorem lemma stating asymptotic distribution ratio sums independent identically distributed iid random variables 
prove general result variance formula follows special case 
lemma suppose fx iid sample size distribution 
suppose functions support 
gamma gamma gh oi proof proved delta method see example billingsley 
theorem assume gw 
positive integers kt gamma kt kt means 
proof proof induction 
show holds kk kkk gamma 
note mean conditionally iid random variables conditioning gamma finiteness support gamma assumptions 
equation follows immediately central limit theorem see example billingsley theorem 
assume holds particular derive limiting distribution gamma decomposition gamma kt gamma gamma kt gamma kt gamma kt gamma kt kt gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma note kt function gamma gamma function gamma determine limiting distribution kt jx gamma jx gamma ti gamma gamma ti gamma gamma gamma ti gamma 
gamma jx gamma gamma gamma gamma strong law large numbers gamma kt gamma jx gamma gamma kt gamma jx gamma gamma integrates result similar calculations lead kt gamma 
gamma gamma gamma exists finiteness support gamma assumptions 
kt gamma 
kt gamma 
limiting distribution kt applying continuity theorem characteristic functions billingsley theorem decomposition obtain gamma expf gamma gamma gamma kt gamma gamma real imaginary number 
determine limiting distribution kt kt gamma gw kt jx gamma ti gamma gamma ti gamma kt jx gamma ti gamma gamma ti form ratio statistic lemma 
applying lemma algebra obtain kt gamma 
expectations dominated convergence theorem billingsley theorem gamma gamma expf gamma recalling gamma function gamma constant respect gamma holds gamma completing inductive step 
holds positive integers corollary assume gw 
gamma gamma proof follows theorem setting 

