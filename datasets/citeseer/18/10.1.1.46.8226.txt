fl kluwer academic publishers boston 
manufactured netherlands 
survey methods scaling inductive algorithms foster provost provost acm org bell atlantic science technology avenue white plains new york kolluri venkat sis pitt edu department information science university pittsburgh pittsburgh pa lycos centre avenue pittsburgh pa editor usama fayyad 
defining challenges kdd research community enable inductive learning algorithms mine large databases 
summarizes categorizes compares existing scaling inductive algorithms 
concentrate algorithms build decision trees rule sets order provide focus specific details issues techniques generalize types data mining 
discussion important issues related scaling 
highlight similarities scaling techniques categorizing main approaches 
approach describe compare contrast different constituent techniques drawing specific examples published papers 
preceding analysis suggest proceed dealing large problem focus research 
keywords scaling inductive learning decision trees rule learning 
knowledge discovery data mining kdd community challenged develop inductive learning algorithms scale large data sets fayyad haussler stolorz fayyad piatetsky shapiro smyth brachman simoudis 
summarizes categorizes compares various existing methods 
restrict survey scope scalable algorithms consider issues efficient file system design storage design network interface design problem formulation relate design inductive algorithms 
believe categorization lessons apply generally analysis focuses primarily algorithms build feature vector classifiers include structural relational terms form decision trees rule sets 
address meaning scaling highlight important issues 
show similarities existing methods grouping high level categories 
category discuss techniques detail showing similarities differences techniques type 
conclude suggestions research practice emerge survey analysis 
provost kolluri 
scale 
organizations large repositories customer operations scientific sorts data 
fayyad 
cite representative examples databases containing gigabytes terabytes data 
kdd practitioners able apply inductive learning algorithms large data sets order discover useful knowledge 
question scalability asks algorithm process large data sets efficiently building best possible models 
existence large data sets sufficient motivate non trivial scaling efforts 
just select small subset data data mining 
commonly cited reason scaling increasing size training set increases accuracy learned classification models catlett 
cases degradation accuracy learning smaller samples stems overfitting due need allow program learn small disjuncts holte acker porter elements class description cover data items 
domains small disjuncts large portion class description provost 
domains high accuracy depends ability learn small disjuncts account special cases 
existence noise data complicates problem small sample impossible tell difference special case spurious data point 
overfitting small data sets may due existence large number features describing data 
large feature sets increase size space models 
searching evaluating candidate models increases likelihood chance program find model fits data jensen cohen increases need larger example sets haussler 
things get particularly difficult features need learn small disjuncts 
specifically large feature sets lead large sparsely populated model spaces program biased search models covering special cases small disjuncts choose 
data mining applications concerned predictive modeling discovery interesting knowledge large databases 
cases increasing accuracy may primary concern 
scaling may issue 
example ability learn small disjuncts interest scientists business analysts small disjuncts capture special cases unknown previously analysts know common cases 
classifier learning order swamped spurious small disjuncts essential data set large contain instances special case generalize confidence provost aronis 
clear scaling large data sets implies part fast learning algorithms developed 
course motivations fast learners 
example interactive induction buntine inductive learner human analyst interact real time requires fast scaling inductive algorithms learning algorithms order practicable 
wrapper approaches particular problem algorithm iteratively search feature subsets parameter settings kohavi sommerfield kohavi provost provost buchanan require fast learners systems run learning algorithms multiple times evaluating different conditions 
furthermore wrapper approach evaluation may involve multiple runs produce performance statistics cross validation 
experimenting learning biases requires large data set avoid overfitting due bias selection desjardins gordon 
final example popular practice learning multiple models combining predictions dietterich multiplies run time 

large large 
kdd community includes researchers practitioners diverse backgrounds including machine learning statistics databases 
researchers machine learning accustomed dealing flat files algorithms run minutes seconds desktop platform 
instances couple dozen features range large data sets 
database community deals gigabyte databases 
large database practitioner usually means databases warehouses gigabytes larger agrawal srikant 
course data data warehouse mined simultaneously 
practice data preprocessing techniques reduce orders magnitude size data set algorithms 
need data reduction preprocess may restriction view learning algorithms fundamental restriction data mining 
survey concentrates algorithms mining data take algorithmic perspective issue large 
published algorithms examples considered large data set mbyte gbyte range 
agrees huber assessment statistical perspective kdd invited talk huber data sizes megabytes qualitatively new serious scaling problems arise human algorithmic side 

scaling 
theoretical considerations issue scaling inherently pragmatic 
scaling learning algorithms issue speeding slow algorithm turning impracticable algorithm practicable 
crucial issue seldom fast run certain problem large problem feasibly deal 
point view complexity analyses scaling problems limiting factor data set number examples 
large number examples introduces potential problems time space complexity 
time complexity provost kolluri appropriate algorithmic question growth rate algorithm run time number examples increases 
important number attributes describing example number values attribute 
may expected time complexity analyses tell story 
number instances grows certain space constraints critical importantly absolute size main memory computing platform equipped 
described existing implementations learning algorithms operate training set entirely main memory 
furthermore algorithms achieve reduced run time complexity bookkeeping increases space 
matter run time computational complexity algorithm exceeding main memory limitation leads virtual memory thrashing algorithm scale provost hennessy 
goal learning considered 
evaluating effectiveness scaling technique complicated degradation quality learning permitted 
vast majority learning algorithms uses classification accuracy metric different algorithms compared 
cases interested methods scale substantial decrease accuracy 
problems require mining regularities data purposes classification metrics devised effectiveness measured compared system scales srikant agrawal 

high level characterization methods scaling diverse techniques proposed implemented scaling inductive algorithms 
similarities techniques apparent categorized main approaches 
cases techniques separate categories independent applied simultaneously 
main approaches ffl design fast algorithm ffl partition data ffl relational representation fast algorithm approach includes wide variety algorithm design techniques reducing asymptotic complexity optimizing search representation finding approximate solutions exact solutions advantage task inherent parallelism 
data partitioning approach involves breaking data set subsets learning subsets possibly combining results 
data partitioning useful avoid thrashing memory management systems occurs algorithms try process huge data sets main memory 
learning algorithm time complexity worse linear number examples processing small fixed size data subsets sequentially linear scaling inductive algorithms constant term dependent size subsets domingos 
case may possible system distributed processors mine subsets concurrently 
approach orthogonal selection example subsets select subsets relevant features focus attention 
relational representation approach addresses data feasibly treated flat file including large relational database large relational structures knowledge representation artificial intelligence 
literature techniques framed learning order logic learning relational databases flattening flat file learning augmented relational background knowledge 
summarizes general methods broad approaches scaling inductive algorithms 
discuss constituent methods detail section 
scaling methods main approach general method fast algorithm restricted model space powerful search heuristics algorithm programming optimizations parallelization data partitioning select instance subset select feature subset process subsets sequentially process subsets concurrently relational representations represent data relationally integrate data mining database management 
methods scaling inductive algorithms 
comparison methods grouping methods broad categories illustrates certain techniques previously considered related fact similar 
summarize methods category order highlight similarities differences strengths weaknesses 
survey exhaustive 
representative current state art places context number historically important examples lasting impact 
design fast learning algorithms 

fast algorithms straightforward approach scaling inductive learning produce efficient algorithms increase efficiency existing algorithms 
provost kolluri fast algorithms general method example technique restricted model space decision stump level tree powerful search heuristics greedy divide conquer avoid decision tree post processing search space pruning algorithm programming efficient data structures optimizations dynamic search space restructuring bookkeeping strategies optimized computing infrastructure parallelization search space parallelization parallel matching 
methods designing fast inductive algorithms course large problems fast linear time algorithm may sufficient practicable data mining 
usually case large problems sampling feature selection relational representations fast algorithm necessary 
just fast inductive algorithms depends course problem 
fast algorithms strives near linear time complexity number examples 
line huber observation maximum tolerable time complexity huber 
discussion learning algorithm design necessary choose analytical framework facilitates discussing different algorithms 
adopt commonly induction search framework data mining framed search space models model performs respect criteria simon lea mitchell 
framework naturally partitions fast algorithm design categories methods see 
restrict space models searched straightforward untrue principle small model space faster search large 
second large model space develop powerful search heuristics powerful means heuristics efficient find competitive models 
discuss particular examples methods proven effective efficient 
discuss algorithm program optimizations detailed literature 
discuss approaches parallelism speed inductive algorithms 

restricted model space approach designing fast learning algorithm restrict search easy model space 
clearest examples effective restricted model space learners long lived viable methods learning classifiers duda hart 
complex machine learning methods typically justified noting capture com scaling inductive algorithms plex non linear relationships data 
research symbolic neural learning shown simple models perform problems 
example shavlik 
show certain qualifications accuracy perceptron hardly distinguishable complicated learning algorithms 
level decision trees known decision stumps simple mappings values attribute class labels 
decision stumps shown achieve moderately high accuracy common benchmark databases iba langley holte 
restricted model spaces simple learning algorithms trained quickly 
haussler relates easy model spaces machine learning notion inductive bias mitchell valiant theoretical framework valiant 
gist model space easy search simply small decision stumps special structure weakens power expression linear discriminants 
tone research inductive algorithms changed markedly acceptance re acceptance restricted hypothesis space algorithms legitimate competitors 
reason change simple fast algorithms facilitate straightforward competitive benchmarking 
importantly scaling perspective competitive run time performance simple classifiers difficult justify complex algorithms 
interest developed simple classifiers perform 
example subsequent auer 
introduce theoretically founded algorithm called learning level decision trees 
show fifteen data sets produces level trees rival surpass de facto standard see quinlan 
interestingly practice considerably faster lim loh shih 

powerful search heuristics certainly domains leverage gained searching complex models 
size structure space models size sample necessary learn computational complexity algorithms search space intimately related 
typically searching complex models harder 
haussler points larger hypothesis space strictly necessary may computationally easier find consistent hypothesis 
hand larger hypothesis space 
examples required haussler 
consider large space formulae disjunctive normal form dnf 
inductive algorithms designed efficiency including learn decision trees decision lists rule sets search space variant dnf formulae pagallo haussler 
model space vast little structure facilitate search powerful heuristics necessary navigating efficiently haussler 
vast model space unusual learning algorithms search space directly generating alternative models choosing 
cases single model built evaluating components 
example evaluating individual conjunctions dnf class description built 
provost kolluri space conjunctions large especially learning large data set data set evaluation individual conjunctions 
practice scale large data sets run time complexity learning algorithm close linear number examples 
algorithm designers success greedy divide conquer approaches building class descriptions 
chose decision tree learners popular id quinlan cart breiman friedman olshen stone survey relatively fast typically produce competitive classifiers 
fact decision tree generator quinlan successor id de facto standard comparison machine learning research produces classifiers quickly 
non numeric data sets growth run time id linear number examples 
specifically asymptotic time complexity ea utgoff number examples training set number attributes 
numeric data typically require repetitive sorting inclusion adds log factor node 
practical run time complexity determined empirically worse data sets catlett 
possible explanation observation oates jensen size trees increases linearly number examples accuracy stabilizes 
factors run time complexity corresponds tree depth larger number attributes 
tree depth related logarithmically tree size number examples 
practical analyses tree size growing linearly adds log factor run time complexity 
empirical determinations large data sets established practical time complexity substantially better quadratic 
decision trees built greedy heuristic criticized lack comprehensibility situations rule sets desired modularity increased comprehensibility catlett 
common technique producing high accuracy rule sets known reduced error pruning grow rules algorithm prune rules order increase accuracy quinlan 
unfortunately reduced error pruning systems generally scale 
example rule learning variant rules reported require time cohen domingos 
algorithms effective finding high accuracy rule sets time complexity noisy domains cohen 
describes speeding rules parallel processing discuss 
furnkranz widmer show incremental reduced error pruning irep algorithm significant speedups obtained pruning rule learned applying separate conquer strategy pruned rule 
formal analysis predicts computational complexity log verified empirically cohen 
unfortunately accuracy class descriptions learned irep lower accuracy learned slower rules 
cohen details mod scaling inductive algorithms improve irep accuracy including different rule evaluation criteria different stopping criteria post processing optimization producing algorithm ripper 
shows ripper competitive rules terms error rate maintains log time complexity irep 
cohen estimates time complexity empirically 
different style rule learning traced back search data mining program buchanan smith white feigenbaum buchanan feigenbaum 
examples style rule learning include brute programs riddle segal etzioni segal etzioni pvm weiss galen tadepalli smyth goodman rl programs clearwater provost provost buchanan fawcett provost se trees schlimmer determination learning algorithm schlimmer 
programs view rule learning explicit search rule space rooted rule conditions antecedent rules specific adding conditions get root described detail webb 
allow massive searches large rule spaces search space reduced depth bounding various forms pruning 

algorithm programming optimizations algorithm optimization efficient data structures bit vectors hash tables binary search trees clever programming techniques engineering practice complements methods scaling practice give large speedups 
optimizations differ powerful search heuristics concentrate eliminating redundant unnecessary computations models induced affected 
optimizations remarkable appeared published 
rule space pruning techniques style rule learners guaranteed discard rules clearwater provost segal etzioni webb 
webb takes idea introducing techniques dynamic search space restructuring maximize amount search space removed pruning 
shows possible search exhaustively rule optimizes laplace accuracy estimate time categorical attribute value benchmark data set uci repository merz murphy 
search algorithm optimized carefully segal etzioni 
segal etzioni report training examples process rules second running sparc processor 
note significant additional speedups expected speed order magnitude machine clock rate 
domingos proposes improve rule learning efficiency growing rule full length place 
points commonly separate conquer methods induce rules evaluating rule regard effect rules 
avoid superfluous growth rule grown domingos cws algorithm evaluates context currently provost kolluri held rule set 
straightforward recomputation accuracy rule set rule modification expensive 
domingos details optimized procedure carefully eliminates redundant computation yielding procedure run time complexity average number values number classes total number antecedents resultant rule set 
principle domingos verifies empirically practice independent notable optimization pre sorting procedure learner sliq mehta agrawal rissanen 
described repetitive sorting reduces efficiency decision tree learners dealing numeric attributes 
sliq sorts training data just numeric attribute tree growth 
mentioned inductive algorithms load data main memory data set large algorithm run virtual memory thrashing render useless 
alternative approach load data memory accessing secondary storage needed 
secondary storage devices typically provide random access data algorithms designed process data sequential scans possible 
sliq takes advantage need single pass data level decision tree tree grown breadth 
data structures sliq uses pre sorting step size proportional number input records size memory resident structure limiting factor approach 
limitations addressed sprint system shafer agrawal mehta monolithic memory resident data structures 
sprint regarded widely standard scalable decision tree building dietterich 
honor comes increased scrutiny leads advances 
sprint maintains augmented vertical partitions data copied data structures attribute lists 
criticized reasons 
example maintaining data structures costly including potential size database gehrke ramakrishnan ganti associated significant increase scan cost graefe fayyad chaudhuri 
brings remarkable data mining optimization simple bookkeeping technique pointed independent research groups 
main insight matching hypotheses data necessary processing statistics results matching inferred sufficient cf 
learning statistical queries kearns 
separating generation sufficient statistics evaluation hypotheses allows treated separately data populate statistics data structure operating data structure affords optimized memory improved run time complexity 
specifically critical data mining operations choosing nodes constructing decision trees tally examples scaling inductive algorithms particular point search class labels associated different values attribute 
straightforward data structure store statistics contingency table example counts attribute indexed attribute value class 
av attribute values examples long av combined size sufficient statistics data structures smaller size data set 
john lent point data structures returned sql group queries 
techniques inductive algorithms pass example set node expansion algorithm pass example set level separate conquer decision tree learning attribute value pair 
tremendous run time efficiencies achieved attributes large value sets aronis provost 
fast contemporary rule space search algorithms segal etzioni domingos generate conjunct attribute value pair match example set compute statistics 
run time complexity depends average number values attribute 
domingos reports time complexity cws algorithm discussed 
bookkeeping techniques reduce rule learning complexity ea aronis provost 
aronis provost go show similar techniques speed learning hierarchically structured data almuallim return discuss relational representations 
various decision tree programs shown comparably fast lim loh shih remarkably considering programs similarity 
analysis code shows evaluating node splits builds sufficient statistics contingency table uses decide best split 
notes preliminary experiments additional optimizations show substantial additional speedups single processor successor observed substantially faster predecessor harris jones haines 
gehrke ramakrishnan ganti provide thorough treatment sufficient statistics build decision trees size data set exceeds main memory 
discuss options available sufficient statistics data structure big main memory rainforest family algorithms 
moore lee moore lee specialized data structures adtrees designed take best advantage sufficient statistics speeding inductive algorithms 
moore lee doubtful cost building adtrees worthwhile individual runs fast learning algorithms 
obvious candidates computationally intensive algorithms systems run algorithms times data interactive systems wrapper systems multiple models systems discussed 
inductive program optimized computing infrastructure 
data mining programs read large amounts data fact reading data take longer mining provost aronis 
parallel provost kolluri systems optimized data layout considerable difference grossman bailey 

parallelization process inductive learning decomposable levels illustrated main methods parallel learning parallelization parallel matching 
discuss inductive learning viewed search large space 
search space parallelization search space decomposed different processors search different portions space parallel cook holder similar parallelization forms heuristic search kumar rao rao kumar 
load balancing interprocess communication add additional complexity overhead 
general type parallelization address problem large data sets processor deal data subsample discuss 
zaki 
success search space parallelization decision tree algorithm advantage shared memory multiprocessor able avoid replicating communicating entire data set processors 
shared memory allows development effective load balancing techniques 
cook holder success search space parallelism scale data mining algorithms cook holder 
requests matching algorithm sequential machine parallel machine instances matching routines search 
parallel matching parallel learning successful lower level decomposition 
parallel matching approach observation search scaling inductive algorithms inductive learning different search problems 
inductive learning cost evaluating node high highly decomposable 
nodes search space partial rules decision tree branches hypothesized matched examples gather statistics 
parallel matching approach depicted compute intensive matching process migrating example set matching routines parallel machine main learning algorithm master may run sequential front 
parallel matching lathrop 
provost aronis parallelization sprint algorithm shafer agrawal mehta 
efforts straightforward parallelization matching routines 
sprint processor builds sublist attribute list decision tree node sends master portion statistics needed determine best split 
impressive speedups reported parallel matching minute learn examples cm connection machine bit slice processors provost aronis seconds learn examples ibm sp processors shafer agrawal mehta 
uses parallelization speed transformation decision trees rules rules parallel matching phases rule set postprocessing dividing rule set third 
reports impressive speedups efficiencies averaging learning tasks processors 
drawback parallel matching approach easy obtain access massively parallel hardware 
zaki points shared memory multiprocessor smp systems common presents parallel matching approach design smp version sprint 
distributing instances process parallel vertical partitions corresponding sprint attribute lists 
attribute lists divided equally processors return matching statistics master 
access parallel hardware may concern data resident data warehouse parallel infrastructure 
freitas take approach making existing parallel database server technology 
approach similar shown parallel data representation replaced existing parallel database system 
communication front back matching requests replaced sql queries 
commercial data mining system vendors cite approach confronted issue scaling technical details specific vendor supplied data mining systems elusive 
discuss database systems sql queries scaling detail section 
third approach parallelization partition data subsets run learners concurrently subsets 
approach described section 
parallel data mining treated detail book freitas 
provost kolluri 
data partitioning previous section addressed design algorithms fast run large example sets 
orthogonal approach partition data avoiding need run algorithms large data sets 
data partitioning data partitioning general method example technique select instance subset random sampling duplicate compaction stratified sampling select feature subset relevance knowledge statistical indications subset studies processing subsets sequentially independent multi subset learning sequential multi subset learning processing subsets concurrently learn multiple models pick best combine class descriptions combine predictions cooperative learning 
data partitioning methods techniques categorized separate subsets examples subsets features 
illustrates techniques selecting single subset learn 
furthermore multiple subsets chosen processed sequence concurrently 
depicts general model showing similarities partitioned data approaches 
systems approaches select subsets sn data selection procedure 
learning algorithms ln run corresponding subsets producing concept descriptions cn concept descriptions processed combining procedure selects cn combines produce final concept description 
systems differ particular procedures selection combining 
differ amount style interaction learning algorithms learned concept descriptions 

select subset instances common approach coping infeasibility learning large data sets select single sample large data set 
referring sampling degenerate form partitioned data system single subset chosen 
differences sampling techniques involve particular selection procedure 
scaling inductive algorithms 
final set large example subset selection procedure combining procedure 
learning data partitioning catlett studied variety procedures sampling instances large data set compared empirically results different techniques 
particular studied 
ffl random sampling selects subset examples randomly 
ffl duplicate compaction removes duplicated instances database 
computational effort proportional degree completeness desired 
ffl stratified sampling applicable class values uniformly distributed training sets 
examples minority class es selected greater frequency order distribution 
readers may difficulty accepting sampling method scaling large data sets sampling reduces size data set processed 
important examine function algorithm performing 
consider classifier induction algorithms 
take data sets input produce classification models output 
discussed question scalability asks algorithm process large data sets efficiently building provost kolluri best possible models 
example sampling produces models lower accuracy usefulness scaling question 
hand sampling produces equivalent better models sampling effective scaling mechanism 
sampling accepted statistics community observe powerful computationally intense procedure operating subsample data may fact provide superior accuracy sophisticated entire data base 
friedman 
wanting mine large data set important question process thing 
sampling effective 
answer depends data set 
just massive data set imply necessarily mine 
practice amount data grows rate increase accuracy slows forming familiar learning curve 
sampling effective depends dramatically rate increase slows 
difficult determine general small data set may depends factors known priori 
example discuss depends minimum size special cases learner discover order model phenomenon effectively 
willing bias learner explicitly implicitly learning small special cases determining sufficient sample sizes similar data mining problems provides relevant results 
example toivonen zaki 
discuss determination sufficient sample sizes finding association rules smaller predefined size tolerances probability error size error 
different view sufficient sample size sample complexity provided valiant theoretical framework valiant haussler hypothesis space allows calculation number examples sufficient learning high probability approximation true concept exists hypothesis space 
published provides differing views real world classifier learning curves level massive data sets needed 
catlett shows learning subsets data decreases accuracy 
despite advantages certain sampling strategies viz speed ups improving accuracy classifier random sampling noise free domains catlett concludes solution general problem scaling large data sets catlett 
noted time catlett study massive data sets smaller today processing times longer 
fact catlett data sets fewer instances 
data set study fit main memory modern desktop pc 
study kdd benefit replication catlett analyses consideration current state computing see stand decade technological improvements 
study harris jones haines analyze relationship data set size accuracy large business data sets instances estimating learning curves empirically 
algorithms level quite early cases algorithms decision tree learner successor particular continue show accuracy increases scaling inductive algorithms entire range data set sizes 
improvements accuracy upper size limit quite small difficult conclude continue order magnitude increase data set size 
authors note important question benefit improvements worth associated cost haines 
results catlett provide ample justification mining data outside main memory 
data set sizes massive modern standards 
processed main memory pc 
field benefit prominent examples need scale reasonable main memory limits 
oates jensen studied decision tree induction nineteen data sets looked specifically number examples necessary learning curves reached plateau 
regard plateau reached accuracy estimate certain tolerance maximum specifically percent experiments 
surprisingly nineteen data sets jensen plateau reached training examples 
course exists massive volume data sampling may necessary decreases accuracy 
example famous application inductive learning fayyad 
sampling techniques reduce terabytes raw data 
important consider possible sample efficiently 
consider necessary scan entire data set order produce random sample advantage sampling lost 
return point address database support scaling data mining 
heretofore discussed may called passive sampling size content training set determined induction begins 
inductive algorithms sample actively intermediate results induction progresses 
notion induction simultaneous search spaces space possible concepts space possible instances introduced simon lea elaborated provost buchanan provost provost buchanan 
scaling catlett studied active tactical sampling reduce complexity learning algorithms process large data sets 
particular search split values numeric attributes dominates decision tree inducers computation values sorted 
catlett looking subsets examples called searching split values numeric attributes run time decision tree learners reduced substantially sacrificing accuracy 
subsequent 
introduced information theoretic measures assess risk evaluation attributes decision tree induction 
particular show determine choice attribute confidently error tolerance determine large peephole required 
section discuss similar technique determining minimum number training examples sufficient satisfactory learning provost kolluri progressively sampling larger subsets model performance longer improves john langley frey fisher provost jensen oates 

select subset features far discussion data partitioning focused selecting subset examples 
turn problem selecting subset features 
important consider symmetry selecting instance subsets method selects rows data table selects columns 
space tradeoff symmetric amount space needed store table product number rows number columns 
point view scaling observations apply 
operating subset reduces induction time space requirements 
multiple subsets operated independently 
results induction subset may help determine learned models built components learned different subsets 
symmetry discussed detail provost buchanan provost provost buchanan 
full treatment feature selection scope 
data engineering visible literature algorithmic issues induction feature selection data engineering issue received just superficial treatment devijver kittler miller wettschereck aha mohri 
majority existing feature selection focused directly scaling 
focused phenomenon reducing size feature set done increase accuracy resultant class description 
purposes survey important clarify closely related reasons selecting feature subsets 
discussed section size feature set grows chances induction program overfit training set especially small training set 
select subset features increase accuracy 
ironically number examples increased feature selection necessary data fitting perspective feature selection necessary runtime perspective 
described run time inductive algorithms grows number attributes rate worse linear 
selecting subset features may important practical algorithm application independent selection increases accuracy 
selecting subset features common method reducing problem size neglected discussions scaling 
setting learning problem small set possibly relevant variables chosen representation 
restriction data collection apparatus knowledge relevance 
interaction domain experts indicate certain variables useful included 
variable describing problem auxiliary databases provide related information 
example zip code field link massive database demographic information 
practice additional fields added reason believe relevant 
scaling inductive algorithms prior relevance knowledge method selecting subset possible features 
approach describe problem features possible inexpensive empirical studies select subset 
little published statistical indications reduce number features purpose scaling techniques may viewed straightforward include publications 
example practitioners compute correlations individual features target concept select practically manageable subset features high correlations kaufman michalski high information gain wettschereck dietterich 
course simple methods may features useful combination 
chen yu address problem combination instance subsetting feature subsetting similar catlett described 
chen yu propose phase method attribute extraction improve efficiency deriving classification rules large training data set 
phase known feature extraction phase subset training data set analyzed identify relevant subset features 
second phase feature combination phase extracted features evaluated combination multi attribute predicates strong inference power identified chen han yu 
relief algorithm kononenko uses experiments randomly drawn examples nearest neighbor representation identify highly interdependent relevant features kononenko 
section discuss feature selection methods process subsets sequentially 

processing subsets sequentially efforts addressed learning multiple subsets combining results 
consider approaches subsets processed sequentially 
cases differences methods involve concept description learned previous iteration combining procedure operates 
noted approaches described section selection procedure partitions data set randomly subsets 
shows general model partitioned data learning 
precisely shows model independent multi subset learning interaction learning runs formed independently combined 
fayyad 
sequential independent multi subset approach decision tree learners rule sets extracted decision trees combination procedure greedy covering algorithm 
multiple subsets processed sequentially possible take advantage knowledge learned iteration guide learning iteration 
show approaches sequential multi subset learning 
model guided instance selection shown iterative active sampling technique class description helps determining incremental batch learning shown class description taken input learner building provost kolluri 
large example set 
sample final initial selection instance selection instance selection combining procedure 
sequential multi subset learning model guided instance selection sequential multi subset techniques researchers address learning large data sets 
quinlan model guided instance selection approach called windowing 
selection procedure begins choosing candidate examples randomly stratification 
called window augmented examples classifies incorrectly 
combining procedure simply chooses cn final concept description 
catlett studied windowing learning problems 
effect windowing learning time varied problem problem factor speedup factor slowdown 
severe slowdowns occur data noisy 
concluded windowing scaling solution noise free data sets 
continuous attributes windowing improve accuracy 
incremental batch learners clearwater cheng hirsh buchanan hybrids sampling incremental learning 
class description prior knowledge learning algorithm subset learning algorithm uses evaluate uses basis building windowing combining procedure chooses cn final concept description incremental batch learning cn constructed learning runs 
incremental batch learning approaches scale example sets large pure batch processing limits scaling inductive algorithms 

set large example random selection final combining procedure 
sequential multi subset learning incremental batch learning main memory leading increased accuracy simple sampling provost buchanan 
incremental batch learning offers speedups discussed learners theoretically scale linearly number examples entire example set fit main memory operating system page thrashing render learner useless 
incremental batch learning approach domingos transform algorithm run time complexity quadratic size example set linear algorithm 
incremental batch learning called multi layer incremental induction wu lo 
historically windowing decision tree learners incremental batch learning rule learners 
coincidence accidental 
modularity rules evaluated individually rule sets constructed easily multiple learning runs difficult decision trees 
furthermore separate conquer rule learning internally model guided instance selection induction progresses existing rule set reduce set examples subsequent learning 
partial rule set provided input separate conquer learner internally restrict subsequent search rules covered 
furnkranz furnkranz presents technique integrating model guided instance selection incremental batch learning calls integrative windowing 
provost kolluri furnkranz presents insightful analysis sequential multi subset rule learning pointing variety related providing crisp explanations important observations 
example windowing fails noisy domains classifier misclassify noisy examples subsequent windows increasing levels noise decreasing subsequent learning performance 
explains shows empirically sequential multi subset learning improves efficiency rule learning decision tree learning incremental batch learning rules need learned iteration 
unexpectedly single subsets sequential multi subset techniques may degrade classification accuracy compared learning entire data set 
hand especially model guided instance selection techniques may increase accuracy 
approaches incrementally process instance subsets 
similarly feature subsets processed iteratively 
sequential feature selection new common statistical treatments classifier formation devijver kittler 
common methods sequential forward selection sequential backward elimination 
noted typically addresses increasing accuracy scaling 
sequential backward elimination provides simple illustration difference iteration runs inductive algorithm features 
techniques sequential forward selection useful increasing accuracy scaling 
wrapper approaches kohavi kohavi john provost provost buchanan notable unify iterative example selection iterative feature selection iterative approaches 
mentioned wrapper approaches run underlying inductive algorithm different contexts attempt maximize criteria 
wrapper approaches feature selection fit framework data partitioning depicted selection procedures select columns rows 
kohavi john wrapper implement forward selection backward elimination order maximize accuracy 
order scale past limits computational platform provost buchanan implement various ad hoc feature selection strategies programmable wrapper implement incremental batch learning 
sequential feature selection techniques fall categories sequential instance selection techniques 
specifically approaches sequential forward selection influence selection case selecting columns rows 
alternatively approaches construction example combining class descriptions learned different feature subsets effective provost buchanan 

process subsets concurrently increase efficiency approaches parallelized distributing subsets multiple processors learning concept descriptions parallel combining 
differentiate approach parallel matching described degree scaling inductive algorithms autonomy afforded individual learners 
simply parallelizing subprocedure existing algorithm returning results master techniques loosely coupled collections independent algorithms 
type algorithm called distributed data mining subject kdd workshop kargupta chan 
concurrency precludes partitioned data approaches prior concept description needed input subsequent learning stage incremental batch learning 
independent multi subset approaches shown learned concurrently different learning algorithms combining take place sequential post process parallelized 
hall 
discuss approach learning decision trees 
similar distributed version approach fayyad 
system learns trees independently partitioned data trees converted rules 
rule sets merged method described williams resolves conflicts similar rules 
chan stolfo take concurrent approach different learning algorithms separate instantiations algorithm 
take independent multi subset approach key difference methods method forms hybrid specifically combining stage constructing selected pieces approach saves combines predictions multiple model approach ali pazzani 
domingos experiments type combining incremental batch learning finding superior simple union rule sets learned batches 
potential problem creating multiple model hybrid resulting loss comprehensibility 
prodromidis stolfo study methods evaluating composing pruning hybrid classifiers reduce size preserving improving predictive performance 
quite different approach creating comprehensible classifiers ensembles taken craven domingos guo 
authors machine learning algorithms induce understandable models complex learned classification systems craven 
specifically predictions ensemble training labels learn decision tree models hybrid performance comparable accuracy 
resultant single tree understandable multiple model hybrid 
shasha research group implemented pc li shasha parallel version quinlan uses different instantiation framework 
specifically selection procedure random partitioning data 
decision tree learned different subset examples 
combining procedure evaluates subset examples disjoint chooses best accuracy final concept description 
partitioned data techniques accuracy may degraded compared running single inductive algorithm data 
may avoided provost kolluri 

set large example final selection procedure combining procedure 
cooperation concurrent learners group learners cooperates obtain global view problem depicted 
key learners cooperate sharing modules knowledge individual rules look locally 
learners evaluate shared knowledge local data returning broadcasting statistics 
provost hennessy take approach style rule learning show guaranteed rule considered acceptable globally considered acceptable monolithic learner entire data set 
specifically matter data partitioned acceptable rule acceptable statistics subset 
subsets acceptable rules generated 
cooperation takes form requests learning algorithms server entire database verification statistics regarding best discovered rules narrows set rules acceptable globally 
combining procedure simply take union cooperation requests limited rules appear learning program interprocess communication minimal 
approach successful scaling large data sets 
scaling inductive algorithms sequential version cooperative approach basis partition savasere omiecinski navathe called efficient association rule algorithms terms database operations toivonen 
similarly scaling scientific discovery system cook holder concurrent cooperative approach best various techniques studied 
partition problem share best discoveries evaluated processors obtain global perspective 
know addressing distributed processing feature subsets building decision trees rule sets preliminary results suggest promise line inquiry 
consider concurrent version independent multi subset approach shown different feature subsets selected example subsets concept descriptions subsequently combined 
results sequential processing feature subsets suggest class description language modular useful modules rules selected different class descriptions possible create accurate class description running learner suitable subset features provost buchanan 
kargupta 
consider distributed processing feature subsets basis function concept representation 
turn third general approach scaling relational representation 

relational representations existing inductive learning programs designed handle large data sets 
particular majority designed assumption data set represented single memory resident table 
unfortunately producing flat files real world relational databases fraught problems 
flattening process quite time consuming substantial storage space needed keeping flat files leads problems relational databases designed avoid update delete anomalies 
flattening may create manageable databases data sets longer fit main memory 
example consider database tables customer table containing customers fields including address product preference state table containing states fields information state product table containing products fields information product 
furthermore assume average size field bytes 
vastly oversimplified example flattening mbyte database results gbyte flat file 
flattening demands choosing subset attributes describe data places inflexible restriction unexpected discoveries kdd system may 
summary mining smaller data sets typically faster especially fit main memory ability relational representations compress data critical 
furthermore flattening extremely large data sets simply provost kolluri relational representation general methods example technique represent data relationally hierarchical structures databases knowledge structures inductive logic programming ilp integrate data mining access sql queries database management push computation dbms utilize parallel database engine mine distributed databases 
methods relational representations feasible 
case need able mine relationally represented data efficiently 
describe relationally represented data mined directly scales representation efficient data stored fast database machine 
divided collection methods depicted 
discuss general issue mining relationally represented data regardless stored 
discuss mining compact relationally represented data set fit main memory 
case integrating data mining database management systems dbmss key 
treat data mining dbms integration separate approach unique set issues applies 
close section discussion mining distributed databases noting combine orthogonal scaling techniques may necessary issues privacy 

mining relational data argued storage efficiencies afforded relational representations mining data represented relationally 
simple form relational data data hierarchical tree structured attributes almuallim received relatively attention literature inductive learning 
data compression afforded tree structured attributes substantial especially tall trees 
example consider geographic hierarchies ranging fine grained descriptors zipcode coarse grained country 
data miner working exclusively flat files include attributes possible granularities choosing subset limit possible resultant discoveries 
tree structured attributes data miner represent hierarchy values separately maintaining economical representation data set 
example instance contains specific location location index hierarchy 
usually instances contain finest granularity hierarchy draw general comparisons 
efficient mining tree structured scaling inductive algorithms attributes treated depth 
improvements described aronis provost 
tree structured attributes allow representation simple relation isa relation attribute value pairs 
relation seen separate table relational database example state county table county zipcode table 
expanding data mining general relational databases obvious step advocated inductive learning research circles aronis kolluri provost buchanan kohavi 
ability handle databases allows practitioners compress unwieldy flat tables creates possibilities augmenting learning systems related knowledge 
field learning problem practitioners consider exist additional tables knowledge describing field 
noted selecting just right auxiliary databases knowledge bases begs question data mining requires identifying databases contain relevant useful knowledge 
envision augmenting fields related tables new data augmenting additional fields related tables 
scaling perspective necessary able learn context massive amounts background knowledge creating need higher degrees scaling data mining systems 
view unifies learning relational databases learning large amounts background knowledge 
parallel marker passing techniques aid augmenting inductive learners large networks relational background knowledge aronis provost 
aronis provost relational knowledge construct new terms added propositional concept description language 
field inductive logic programming ilp muggleton concentrates mining data knowledge expressed relational format 
ilp addresses harder problem type mining considering 
specifically data represented relationally results mining may represented relationally 
learning relational descriptions harder learning propositional ones relational algorithms considerably slower propositional ones scaling problem correspondingly harder 
blockeel de raedt jacobs observe full power standard ilp practical applications 
general approach speeding learning relational data avoid expensive little constructs 
aronis 
investigate induction data items linked relational background knowledge 
sake efficiency purposely avoid ary recursive relational terms 
blockeel 
study efficient subset ilp known learning interpretations 
particular study scaling order logical decision trees expressive propositional decision trees avoid expensive ilp constructs 
particular note allow mining data expressed relational databases 
part study blockeel provost kolluri consider application techniques sliq mehta agrawal rissanen scale learning massive data sets 
results encouraging learned run time complexity linear number examples large data sets mined efficiently efficiently relative ilp techniques non trivial tasks approximately examples mbytes processed day cpu time sun workstation 
scalable ilp project morik describe integration ilp algorithm dbms facilitating efficient learning directly dbms resident data subject section 

data mining dbms integration applications data stored efficient relational representation relational database easily accessible commercial database management system dbms 
data mining systems access data stored commercial dbms mine relational data directly 
extract data dbms memory resident flat file realizing benefits efficient storage discussed previous subsection 
approaches take advantage dbms efficient data retrieval 
relational data stored commercial dbms mined directly implementing core data manipulation operations dbms 
discussed section speed inductive programs determined primarily speed matching gathering sufficient statistics 
operations cast sql requests statistics agrawal shim agrawal shim data mining program avoid massive data uploads problems due main memory restrictions take advantage fast database machines optimized query processing 
proposal sql interface protocol john lent discuss basic operations various types data mining programs cast sql queries 
graefe fayyad chaudhuri show straightforward implementation deriving sufficient statistics sql databases select union operators results unacceptably poor performance 
poor performance stems manner database system implement query specifically database systems implement union query performing separate scan clause union 
deriving sufficient statistics unions similar 
authors propose take advantage similarity extending sql include new operator minimizes number scans required produce sufficient statistics 
shows schematic view system integrating data mining dbms 
dbminer data mining system han fu wang chiang gong koperski li lu rajan xia zaiane prototypical example integrated data mining dbms system 
data surveyor kersten siebes fayyad weir scaling inductive algorithms user interface inductive algorithms sql server data 
data mining dbms integration data mining dbms integration approach achieve competitive performance large data sets 
sarawagi 
discuss alternatives data mining dbms integration 
focus mining association rules illustrate principles apply generally 
particular point efforts extend sql support mining operations discuss expressing mining algorithms sql 
paragraphs parallel high level discussion gives details specific approaches 
noted common dbms data mining simple source records 
dbms data mining program coupled loosely cost switching contexts programs may prohibitive 
true especially records read individually 
block transfers sense 
tightly coupled approach pushes parts application program perform intensive computations retrieved set records database system bringing records database application program 
method encapsulate mining algorithm stored procedure 
approach allows programs share address space corresponding efficiencies maintaining programming flexibility 
hybrid approaches read data database temporary local cache transforming efficient format 
drawback caching approach need additional storage space 
somewhat different approach represent individual data mining operations user defined functions stored procedures placed sql data scan queries run dbms address space agrawal shim agrawal shim 
approach promises faster passing records user defined function faster passing stored procedure 
disadvantage cost rewriting entire mining algorithms user defined functions 
sarawagi consider general provost kolluri case preprocessor translates data mining operations appropriate form particular environment 
integrating data mining dbms takes advantage storage efficiencies relational representations existence indices fact dbmss typically reside powerful platforms optimized database operations 
described section scaling extended making parallel database server technology speed data intensive sql operations 
implementation freitas achieved order magnitude speedup workstation dbms technology making back processor sql server 
data surveyor kersten siebes uses parallel database engine 
fact data reside dbms reason consider integrating data mining 
may advantages dbms complex representations knowledge 
faced complex relational representations scaling problems realistic applications address existing knowledge representation systems provide high speed access large complex knowledge bases karp paley 
karp paley greenberg show employ dbms effectively storage subsystem large frame knowledge representations karp paley greenberg karp paley 
andersen hendler describe knowledge representation tools scale massive knowledge bases andersen hendler 
initially efficiency gains realized due massive parallelism increasing dbmss achieve increased efficiency allowing effective parallelization 
specifically dbms techniques support matching inference data management 
advances efficient handling large scale knowledge bases facilitate efforts mine augment data mining 

distributed databases enabling inductive programs learn relational databases available data mining vast amount data background knowledge distributed local network scattered internet 
example companies interested mining federations similar data stolfo fan lee prodromidis chan digital library research working facilitate access networked data information fox furuta 
desire take advantage collections comes need scale massive amounts distributed data background information 
scaling problem manifests issues discussed far survey plus additional constraints opportunities 
distributed data provide opportunity concurrent mining different subsets similar straightforward uses parallelism data partitioning distributed data may require distributed mining 
combining distributed databases may question variety reasons 
may simply big combine local system 
bandwidth communications channel may combining databases infeasible take scaling inductive algorithms long download data 
privacy issues may prevent unrestricted access data 
reasons database interest may accessible network transferring may feasible 
mining distributed databases requires system operate separate data partitions 
information transferred limited bandwidth privacy restrictions 
data sets rows columns distributed methods discussed section part appropriate 
stolfo approach implemented system discuss privacy concerns restrict federation banking data stolfo fan lee prodromidis chan stolfo prodromidis fan lee chan 
alternative distributed data mining scenario different database tables spread network 
consider simplistic example tables customer information geographic information product information 
real world situations different tables reside machine 
currently practical data mining comprises locating relevant tables different databases transferring carefully selected subsets data mining platform 
network access provided data sql servers tailored data mining servers inductive algorithm query remote databases necessary data mining 
ribeiro kaufman describe method performing knowledge discovery multiple databases foreign key values augment tables 
specifically propose tracing multiple databases foreign keys learning individual knowledge segments database 
world system aronis kolluri provost buchanan learns multiple distributed databases spread network spreading activation techniques 
require limited communication pass sets markers implemented sql queries 

discussion looking systematically body scaling inductive methods clear areas received relatively deep treatment 
suggests faced mining huge data set 
discuss research provide help perspectives statistics databases machine learning 

huge data set 
large data set fit main memory restricted model space learners tried effective building competitive classifiers quickly 
resulting simple classifiers satisfactory fast effective algorithms data sets fit main memory 
extending concept direction suggested build algorithms simplicity strategy search classifiers holte provost 
provost kolluri clear leverage obtained simple classifiers guide subsequent search address specific deficiencies performance holte 
data set interest fit main memory 
data set fit machine huge main memory mining just efficient 
research induction data set large fit main memory nearly comprehensive techniques clear choices apply 
revisit briefly need lack thereof mine entire data set 
data mining problems benefit increasing size data set decreases data set size grows yielding familiar concave learning curve 
eventually increase quality results negligible simply zero 
problem determine learning curve plateau 
algorithms calculating precisely priori tight bound size required data set difficult 
computational learning theory provide upper bounds concept classes particular kind learning ignored 
may particular learning algorithm bounds weak fewer instances needed 
course theoretical calculations method 
considering run time complexity inductive algorithms best linear number examples worse relatively inexpensive experiments conducted small samples order estimate number examples needed john langley frey fisher provost jensen oates 
cases number examples needed smaller number available procedures provide substantial practical speedups 
subsets examples sampled stratified sampling class dominates strongly 
subsets features selected doing empirical studies determine relevance 
practitioner chosen subset examples subset features algorithm efficient data representation may significant increase accuracy learning data fit main memory especially modern computer memory slots filled capacity 
straightforward methods exhausted best approach depends resources available 
massively parallel matching obvious choice increased scaling access massively parallel machine specialized programming talent available 
carefully examine tradeoffs matching gathering sufficient statistics 
advantage powerful tuned database systems data mining dbms integration idea cycles database engine readily available approach may require significant investment appropriate long term plans mine set data 
loss flexibility choosing modifying inductive methods ignored problem engineering large portion kdd process 
specialized programming talent usually required 
independent multi subset learning shows promise scaling retaining flexibility desktop data mining 
ability process subsets concur scaling inductive algorithms rently offers take advantage large number idle workstations networked institutions 
unfortunately currently dearth available technology facilitate learning specialized programming talent required 
notable exception jam java agents meta learning software downloadable stolfo web site stolfo 

research help 
issues data mining problem environmental characteristics dictate general approach little guidance choice various constituent methods 
approach methods studied isolation exist studies comparing relative merits 
example partitioned data approaches research just reached border proof concept stage comparative evaluation stage 
theoretical empirical research needed claim thorough understanding 
needed better treatment sampling 
kdd community including researchers different perspectives achieve common understanding sampling build artifacts enabling 
example effects stratified sampling predictive performance original distribution 
see chan stolfo 
time concept near way choose data subsets intelligently tentative classifier built 
consider contributed commonly cited kdd component fields statistics databases machine learning 
statistics long rich history theoretical sampling 
traditionally focused small samples hypothesis verification may applicable computer driven discovery directly tailored current context 
kdd embrace efforts statisticians new past research provide common theoretical understanding issue 
discussions sampling assume producing random samples efficiently large data sets difficult 
large databases simply true 
produce random sample single table database may require scanning entire table 
naive implementation may worse 
obvious implications claims algorithm efficiency sampling example asymptotic run time complexity linear total number instances 
better understanding sampling come database operations organizations allow efficient sampling fayyad 
remarkable advances learning effectively large numbers examples separately learning effectively large numbers features 
data mining problems need scaling massive numbers 
main memory learning algorithms fast effective deal experimentation possible type experimentation automated large degree moore lee provost provost kolluri buchanan kohavi john 
existing algorithms may provide strong baselines new approaches compared 
example known algorithm winnow littlestone effective detecting irrelevant features incremental straightforward augment algorithm reduce number features automatically number examples grows keeping total size data set small 
area need research effort mining large relational databases databases linked relational background knowledge 
research broad implications affecting development data mining dbms integrated systems algorithms learning main memory partitioned data approaches 
storage economies possible relational representations promises larger data sets processed main memory flat file representations 
furthermore ability mine distributed structured data knowledge efficiently dovetail nicely current efforts available vast amounts metadata indexed information fox furuta bringing view new research horizons 

reached reading organizing papers scaling inductive algorithms 
done establishment kdd coherent field study seemingly accelerated production relevant results 
interdisciplinary nature field research undertaken benefit insights substantially similar taken place subfield taken place necessary peripheral task research effort different focus 
striking similarities efforts far determine completely independent 
unexpected fact encouraging time comes fundamentally identical technical advances simultaneously independent groups 
hope survey helps establish common ground efforts reach higher 
design fast algorithms stands clear example effective incremental research clear chains advances fast rule learning fast decision tree learning 
research partitioned data approaches mining relationally represented data mature consisting mainly independent striking commonalities existing approaches 
believe areas ready emphasis comparative research followed hopefully significant incremental advances 
glaring gaps literature lack common understanding power sampling data mining dearth convincing examples need mine massive data sets 
scaling inductive algorithms 
indebted including john aronis lars asker bruce buchanan jason catlett pedro domingos phil chan doug fisher dan hennessy david jensen ronny kohavi rich segal sal stolfo anonymous referees previous papers influenced views scaling discussions 
pointed relevant 
peter huber gave invited talk kdd large huge statistician reactions kdd dm summary appears proceedings summary survey 
observations agreed included specific statements 
tom dietterich reviewed state scaling article trends machine learning dietterich 
development reviews occurred independently contemporaneously benefit insights producing final version 
pedro domingos tom fawcett usama fayyad rick doug metzler ron anonymous referees comments drafts usama fayyad encouraging turn informal summary formal survey 
partly supported national science foundation iri 
notes 
version available technical report web provost kolluri overview appears proceedings kdd provost kolluri 

study large data sets shows polynomial estimation run time complexity fall provost jensen oates 

run times illustration 
comparison inferred 

fact experiments harris jones haines conducted dual processor mhz compaq computer ram running windows nt 
run time examples minutes fifteen minutes examples 

flattening customer table requires customers fields customer bytes field bytes state table requires states fields state bytes field bytes product table requires products fields product bytes field bytes pre flattening total size bytes 
flatten appropriate state product information spliced customer record yielding new customer record comprising fields 
customers new space requirement customers fields customer bytes field bytes 

noted fast database machine mining database resident data directly considerably slower flat file mining 

justify calling linked tables knowledge recall direct correspondence relational databases knowledge representation structures hayes 

gaines analyzed extent prior knowledge reduces amount data needed effective learning 

remember size example set product number examples number features 
halving number features eliminating irrelevant ones may increase accuracy allow double number examples occupying fixed space 
provost kolluri agrawal shim 
developing tightly coupled applications ibm db cs relational database system methodology experience 
research report rj ibm 
agrawal shim 
developing tightly coupled data mining applications relational database system 
proceedings second international conference knowledge discovery data mining menlo park ca pp 

aaai press 
agrawal srikant 
fast algorithms mining association rules 
research report rj ibm 
ali pazzani 
error reduction learning multiple descriptions 
machine learning 
almuallim 
handling tree structure attributes decision tree learning 
proceedings twelfth international conference machine learning 
morgan kaufmann 
andersen hendler 
massively parallel matching knowledge structures 
kitano hendler eds massively parallel artificial intelligence 
aaai mit press 
aronis kolluri provost buchanan 
world knowledge discovery multiple distributed databases 
proceedings florida artificial intelligence research symposium flairs 
aronis provost 
efficiently constructing relational features background knowledge inductive machine learning 
working notes aaai workshop knowledge discovery databases seattle wa 
aronis provost 
increasing efficiency data mining algorithms breadthfirst marker propagation 
proceedings third international conference knowledge discovery data mining newport beach ca 
aronis provost buchanan 
exploiting background knowledge automated discovery 
proceedings second international conference knowledge discovery data mining menlo park ca pp 

aaai press 
auer holte 
theory applications agnostic pac learning small decision trees 
proceedings international conference machine learning pp 

blockeel de raedt jacobs 
scaling inductive logic programming learning interpretations 
data mining knowledge discovery appear 
breiman friedman olshen stone 
classification regression trees 
wadsworth international group 
morik 
direct access ilp algorithm database management system 
proceedings sponsored workshop data mining inductive logic programming 
buchanan feigenbaum 
dendral meta dendral applications dimensions 
artificial intelligence 
buchanan smith white feigenbaum 
applications artificial intelligence chemical inference 
automatic rule formation mass spectrometry means meta dendral program 
journal american chemical society 
buntine 

theory learning classification rules 
ph 
thesis school computer science university technology sydney australia 
catlett 

test flight 
proceedings eighth international workshop machine learning pp 

morgan kaufmann 
catlett 

machine learning large databases 
ph 
thesis school computer science university technology sydney australia 
chan stolfo 
parallel distributed learning meta learning 
working notes aaai workshop knowledge discovery databases pp 

chan stolfo 
accuracy meta learning scalable data mining 
journal intelligent information systems volume pp 

scaling inductive algorithms chan stolfo 
scalable learning non uniform class cost distributions case study credit card fraud detection 
proceedings fourth international conference knowledge discovery data mining pp 

chen han yu 
data mining overview database perspective 
ieee transactions knowledge data engineering 
chen 
yu 
multi attribute predicates mining classification rules 
technical report ibm research report 
clearwater cheng hirsh buchanan 
incremental batch learning 
proceedings sixth international workshop machine learning san mateo ca pp 

morgan kaufmann 
clearwater provost 
rl tool knowledge induction 
proceedings second international ieee conference tools artificial intelligence pp 

ieee press 
cohen 

efficient pruning methods separate conquer rule learning systems 
thirteenth international joint conference artificial intelligence pp 

morgan kaufmann 
cohen 

fast effective rule induction 
proceedings twelfth international conference machine learning pp 

cook holder 
accelerated learning connection machine 
proceedings second international ieee conference tools artificial intelligence san mateo ca pp 

morgan kaufmann 
craven 

extracting comprehensible models trained neural networks 
ph 
thesis university madison 
technical report 
provost 
small disjuncts action learning diagnose errors telephone network local loop 
utgoff ed machine learning proceedings tenth international conference pp 

morgan kaufmann publishers desjardins gordon 
special issue bias evaluation selection 
machine learning 
devijver kittler 
pattern recognition statistical approach 
prentice hall 
dietterich 

machine learning research current directions 
ai magazine 
domingos 

efficient specific general rule induction 
proceedings second international conference knowledge discovery data mining menlo park ca pp 

aaai press 
domingos 

linear time rule induction 
proceedings second international conference knowledge discovery data mining menlo park ca pp 

aaai press 
domingos 

knowledge acquisition examples multiple models 
fisher ed proceedings fourteenth international conference machine learning icml pp 

san francisco ca morgan kaufmann 
duda hart 
pattern classification scene analysis 
new york john wiley 


parka system massively parallel knowledge representation 
ph 
thesis department computer science university maryland college park maryland 
fawcett provost 
adaptive fraud detection 
data mining knowledge discovery 
fayyad 

editorial 
data mining knowledge discovery 
fayyad haussler stolorz 
kdd science data analysis issues examples 
proceedings second international conference data mining knowledge discovery menlo park ca pp 

aaai press 
fayyad piatetsky shapiro smyth 
knowledge discovery data mining unifying framework 
proceedings second international conference data mining knowledge discovery menlo park ca pp 

aaai press 
fayyad weir 
machine learning system automated cataloging large scale sky surveys 
proceedings tenth international conference machine learning 
morgan kaufmann 
provost kolluri fayyad piatetsky shapiro smyth 
data mining knowledge discovery overview 
fayyad piatetsky shapiro smyth eds advances knowledge discovery data mining 
menlo park ca aaai press 
fox furuta 
communications acm volume 
morgan kaufmann 
freitas 
sql primitives parallel db servers speed knowledge discovery large relational databases 
cybernetics systems proceedings thirteenth european meeting cybernetics systems research pp 

freitas 
mining large databases parallel processing 
norwell ma kluwer academic publishers 
frey fisher 
modeling decision tree performance power law 
heckerman whittaker eds proceedings seventh international workshop artificial intelligence statistics 
san francisco ca morgan kaufmann 
friedman 

data mining statistics connection 
proceedings th symposium interface computer science statistics 
furnkranz 

integrative windowing 
journal artificial intelligence research 
furnkranz widmer 
incremental reduced error pruning 
proceedings eleventh international machine learning conference new brunswick 
morgan kaufmann 
gaines 

knowledge worth ton data quantitative studies trade expertise data statistically founded empirical induction 
proceedings sixth international workshop machine learning san mateo ca pp 

morgan kaufmann 
cook holder 
exploiting parallelism scientific discovery system improve scalability 
journal american society information science 
appear 
gehrke ramakrishnan ganti 
rainforest fast decision tree construction large datasets 
proceedings fourth international conference large data bases new york ny 
graefe fayyad chaudhuri 
efficient gathering sufficient statistics classification large sql databases 
proceedings fourth international conference knowledge discovery data mining new york aaai press 
grossman bailey 
tutorial high performance data mining 
tutorial fourth international conference knowledge discovery data mining kdd 
guo 
knowledge probing distributed data mining 
working notes kdd workshop distributed data mining pp 

haines 

private communication 
hall chawla bowyer 
combining decision trees learned parallel 
working notes kdd workshop distributed data mining pp 

han fu wang chiang gong koperski li lu rajan xia zaiane 
dbminer system mining knowledge large relational databases 
proceedings second international conference data mining knowledge discovery menlo park ca pp 

aaai press 
harris jones haines 
sample size misclassification better 
working wp ams center advanced technologies 
haussler 

inductive bias ai learning algorithms valiant learning framework 
artificial intelligence 
hayes 

logic frames 
ed frame conceptions text understanding pp 

de gruyter 
kersten siebes 
data surveyor searching nuggets parallel 
fayyad piatetsky shapiro smyth eds advances knowledge discovery data mining pp 

menlo park aaai press 
holte 

simple classification rules perform commonly datasets 
machine learning 
holte acker porter 
concept learning problem small disjuncts 
proceedings eleventh international joint conference artificial intelligence san mateo ca pp 

morgan kaufmann 
scaling inductive algorithms huber 

large huge statistician reaction kdd dm 
proceedings third international conference knowledge discovery data mining menlo park ca pp 

aaai press 
iba langley 
induction level decision trees 
proceedings ninth international conference machine learning pp 

morgan kaufmann 
jensen 

private communication 
jensen cohen 
multiple comparisons induction algorithms 
machine learning appear 
john langley 
static versus dynamic sampling data mining 
proceedings second international conference knowledge discovery data mining pp 

aaai press 
john lent 
data 
proceedings third international conference knowledge discovery data mining menlo park ca pp 

aaai press 
kargupta chan eds 

kdd workshop distributed data mining 
kargupta johnson 
park hershberger 
scalable data mining distributed vertically partitioned feature space collective mining gene expression genetic algorithms 
working notes kdd workshop distributed data mining pp 

www eecs edu pubs bodhi ps karp paley 
knowledge representation large 
proceedings fourteenth international joint conference artificial intelligence menlo park ca pp 

aaai press 
karp paley greenberg 
storage system scalable knowledge representation 
proceedings third international conference information knowledge management 
kaufman michalski 
method reasoning structured continuous attributes knowledge discovery system 
proceedings second international conference knowledge discovery data mining menlo park ca pp 

aaai press 
kearns 

efficient noise tolerant learning statistical queries 
proceedings fifth acm symposium theory computing new york ny pp 

acm press 
kohavi 

wrappers performance enhancement oblivious decision graphs 
ph 
thesis dept computer science stanford university palo alto ca 
kohavi 

crossing chasm academic machine learning commercial data mining 
invited talk fifteenth international conference machine learning 
kohavi john 
wrappers feature subset selection 
artificial intelligence 
kohavi sommerfield 
feature subset selection wrapper model overfitting dynamic search space topology 
proceedings international conference knowledge discovery data mining menlo park ca 
aaai press 
kononenko 

estimating attributes analysis extensions relief 
bergadano raedt eds proceedings european conference machine learning 
kononenko 
overcoming myopia inductive learning algorithms relieff 
applied intelligence 


generating production rules parallel 
proceedings fourteenth national conference artificial intelligence aaai pp 

menlo park ca aaai press 
kumar rao 
parallel depth search part analysis 
international journal parallel programming 
lathrop webster smith winston 
ariel massively parallel symbolic learning assistant protein structure function 
winston eds ai mit expanding frontiers cambridge ma 
mit press 
li 

free parallel data mining 
ph thesis department computer science new york university 
provost kolluri lim 
loh 
shih 
comparison prediction accuracy complexity training time old new classification algorithms 
machine learning 
appear 
littlestone 

learning quickly irrelevant attributes abound new algorithm 
machine learning 
mehta agrawal rissanen 
sliq fast scalable classifier data mining 
proceedings fifth international conference extending database technology edbt avignon france 
merz murphy 
uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
miller 

subset selection regression 
chapman hall 
mitchell 

generalization search 
artificial intelligence 
mitchell 

need biases learning generalizations 
technical report report cbm tr rutgers university new brunswick nj 
moore lee 
efficient algorithms minimizing cross validation error 
proceedings eleventh international conference machine learning 
morgan kaufmann 
moore lee 
cached sufficient statistics efficient machine learning large datasets 
journal artificial intelligence research 
muggleton 

inductive logic programming 
london academic press 

supporting large scale computational science 
technical report id center applied scientific computing lawrence livermore national lab 
catlett russell 
decision theoretic subsampling induction large databases 
proceedings tenth international conference machine learning san mateo ca pp 

morgan kaufmann 
oates jensen 
effects training set size decision tree complexity 
fisher ed machine learning proceedings fourteenth international conference pp 

morgan kaufmann 
oates jensen 
large data sets lead overly complex models explanation solution 
agrawal stolorz eds proceedings fourth international conference knowledge discovery data mining kdd pp 

menlo park ca aaai press 
pagallo haussler 
boolean feature discovery empirical learning 
machine learning 
piatetsky shapiro brachman simoudis 
overview issues developing industrial data mining knowledge discovery applications 
proceedings second international conference knowledge discovery data mining menlo park ca pp 

aaai press 
prodromidis stolfo 
pruning meta classifiers distributed data mining system 
working notes kdd workshop distributed data mining pp 

provost 

policies selection bias inductive machine learning 
ph 
thesis department computer science university pittsburgh pittsburgh pa provost aronis 
scaling inductive learning massive parallelism 
machine learning 
provost buchanan 
inductive policy pragmatics bias selection 
machine learning 
provost hennessy 
scaling distributed machine learning cooperation 
proceedings thirteenth national conference artificial intelligence menlo park ca 
aaai press 
provost jensen oates 
efficient progressive sampling 
technical report department computer science university massachusetts amherst 
provost kolluri 
scaling inductive algorithms overview 
proceedings third international conference knowledge discovery data mining menlo park ca pp 

aaai press 
provost kolluri 
survey methods scaling inductive learning 
technical report isl intelligent systems laboratory university pittsburgh pittsburgh pa www pitt edu survey ps 
scaling inductive algorithms provost 

iterative weakening optimal near optimal policies selection search bias 
proceedings eleventh national conference artificial intelligence pp 
menlo park ca 
aaai press 
provost hennessy 
distributed machine learning scaling coarsegrained parallelism 
proceedings second international conference intelligent systems molecular biology 
quinlan 

learning efficient classification procedures application chess endgames 
michalski mitchell eds machine learning ai approach 
los altos ca morgan kaufmann 
quinlan 

simplifying decision trees 
international journal man machine studies 
quinlan 

induction decision trees 
machine learning 
quinlan 

programs machine learning 
san mateo ca morgan kaufmann 
rao kumar 
parallel depth search part implementation 
international journal parallel programming 
ribeiro kaufmann kerschberg 
knowledge discovery multiple databases 
proceedings international conference knowledge discovery data mining menlo park ca pp 

aaai press 
riddle segal etzioni 
representation design brute force induction boeing manufacturing domain 
applied artificial intelligence 


se tree characterization induction problem 
proceedings tenth international conference machine learning 
morgan kaufmann 
sarawagi thomas agrawal 
integrating association rule mining relational database systems alternatives implications 
proceedings acm sigmod international conference management data 
savasere omiecinski navathe 
efficient algorithm mining association rules large databases 
proceedings international conference large data bases pp 

morgan kaufmann 
schlimmer 

efficiently inducing determinations complete systematic search algorithm uses optimal pruning 
utgoff ed proceedings tenth international conference machine learning pp 

san mateo ca morgan kaufmann 
segal etzioni 
learning decision lists homogeneous rules 
proceedings twelfth national conference artificial intelligence menlo park ca pp 

aaai press 
segal etzioni 
learning decision lists homogenous rules 
proceedings twelfth national conference artificial intelligence seattle wa pp 

aaai press 
shafer agrawal mehta 
sprint scalable parallel classifier data mining 
proceedings second international conference large data bases mumbai india 
shasha 

pc 
cs nyu edu pc 
shavlik mooney towell 
experimental comparison symbolic connectionist learning algorithms 
machine learning 
simon lea 
problem solving rule induction unified view 
gregg ed knowledge cognition pp 

new jersey lawrence erlbaum associates 
smyth goodman 
information theoretic approach rule induction databases 
ieee transactions knowledge data engineering 
srikant agrawal 
mining quantitative association rules large relational tables 
proceedings acm sigmod conference management data montreal 
stolfo 

www cs columbia edu sal jam project 
stolfo fan lee prodromidis chan 
credit card fraud detection meta learning issues initial results 
proceedings aaai workshop ai approaches fraud detection risk management aaai technical report ws menlo park ca pp 

aaai press 
stolfo prodromidis fan lee chan 
jam java agents meta learning distributed databases 
proceedings aaai workshop provost kolluri ai approaches fraud detection risk management aaai technical report ws menlo park ca pp 

aaai press 
toivonen 

sampling large databases association rules 
proceedings fourth international conference large data bases 
utgoff 

incremental induction decision trees 
machine learning 
valiant 

theory learnable 
communications acm 
webb 

opus efficient admissible algorithm unordered search 
journal artificial intelligence research 
weiss galen tadepalli 
maximizing predictive value production rules 
artificial intelligence 
wettschereck aha mohri 
review comparative evaluation feature weighting methods lazy learning algorithms 
artificial intelligence review 
technical report aic naval research laboratory 
wettschereck dietterich 
experimental comparison nearestneighbor nearest hyperrectangle algorithms 
machine learning 
williams 

inducing combining multiple decision trees 
ph 
thesis australian national university canberra australia 
wu lo 
multi layer incremental induction 
proceedings fifth pacific rim international conference artificial intelligence pp 

springer verlag 
zaki 

scalable data mining rules 
ph 
thesis department computer science university rochester rochester ny 
zaki ho agrawal 
scalable parallel classification data mining shared memory multiprocessors 
proceedings ieee international conference data engineering 
zaki parthasarathy li ogihara 
evaluation sampling data mining association rules 
proceedings seventh international workshop research issues data engineering 
