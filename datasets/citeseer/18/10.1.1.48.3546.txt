canonical distortion measure vector quantization function approximation jonathan baxter department systems engineering australian national university canberra australia measure quality set vector quantization points means measuring distance random point quantization required 
common metrics hamming euclidean metrics mathematically simple inappropriate comparing natural signals speech images 
shown environment functions input space induces canonical distortion measure cdm depiction canonical justified shown optimizing reconstruction error respect cdm gives rise optimal piecewise constant approximations functions environment 
cdm calculated closed form different function classes 
algorithm training neural networks implement cdm encouraging experimental results 
consider problems appropriate distortion measures images handwritten characters images faces representations speech signals 
simple measures squared euclidean distance widely vector quantization applications correlate subjective notion distance problems 
example image slightly larger image look subjectively similar human observer squared euclidean separation measured pixel pixel basis large 
said images face viewed slightly different angles 
finding distortion measures accurately correlate subjective experience great practical utility vector quantization machine learning 
quantization poor distortion measure cause encoder inefficient available codebook vectors 
learning nearest neighbour techniques generating piecewise constant approximation target function effective appropriate measure distortion input vectors available 
purely philosophical perspective priori natural distortion measure particular space images signals 
generate distortion measure extra structure added problem 
argued required extra structure form environment functions example consider possible face classifier function space images classifier jon jon behaves follows jon image face jon image face 
note jon constant images face gives constant value images face constant value images face 
similarly exists classifiers environment correspond mary mary joe joe 
classifiers possess property constant images similar looking faces 
information appropriate distortion measure faces stored environment face classifiers 
similar considerations suggest class character classifiers generate natural distortion measure characters class spoken words generate natural distortion measure speech signals class smooth functions generate natural distortion measure regression problems 
formal justification assertion section explicit formula distortion measure generated function class 
distortion measure termed canonical distortion measure cdm function class 
section optimality property cdm proved generates optimal quantization points voronoi regions generating piecewise constant approximations functions environment 
section cdm explicitly calculated simple environments 
particular shown squared euclidean distance function cdm linear environment 
leads interesting observation squared euclidean distance optimal approximating linear functions fact optimal approximating linear functions 
section shown cdm may learnt sampling input space environment albeit toy experiment reported cdm learnt robot arm environment 
resulting cdm facilitate learning piecewise constant approximations functions environment 
functions learnt help cdm results compared 
learning cdm gives far better generalisation small training sets learning cdm 
related authors investigated possibility specially tailored distance functions machine learning vector quantization contexts 
authors measure distance takes account invariance respect affine transformations thickness transformations handwritten characters 
achieved notable improvement performance measure nearest neighbour classifier 
concept similarity chorus prototypes introduced edelman see closely related canonical distortion measure introduced :10.1.1.38.5181
fact certain assumptions environment show chorus prototypes similarity measure identical cdm 
authors proposed similarity measure images close relationship cdm defined certain restrictions functions environment 
close spirit authors considered vector quantization bayes classifier environment 
show optimal voronoi regions generated bayes risk reconstruction error optimal voronoi regions generated cdm classifier environment 
see section discussion relationship approaches cdm 
distortion measure vector quantization vector quantization real digital communication channel finite capacity transmitting continuous data speech signals images requires data transformed discrete representation 
typically probability space sigma sigma oe algebra subsets probability distribution chooses quantization codebook fx ae transmitting index nearest quantization point argmin transmitted distortion measure necessarily metric quantization points fx chosen expected distortion quantization minimal chosen minimize reconstruction error dp common approach minimizing reconstruction error lloyd algorithm iteratively improves set quantization points update see 
examples distortion measures hamming metric gamma ffi ffi kronecker delta function squared euclidean distortion measure vector valued kx gamma distortion measures convenient mathematical properties applicability particular problem domain 
example suppose space images images characters transmitted channel 
image character translated image character considered close context squared euclidean distance images large quite larger distance image image location 
squared euclidean distortion measure capture idea images close 
vector quantizer constructed inappropriate distortion measure require far larger set quantization points achieve satisfactory performance environments primarily interested subjective quality reconstruction 
applications vector quantization subjective quality reconstruction important eventually quality reconstructed speech signal image measured close appears original observer 
section see problem choosing appropriate distortion measure may solved principle idea environment quantization process 
formal definition canonical distortion measure cdm translated image close original image quite distant speech signals nearly identical miles apart euclidean perspective discussed environment classifiers character classifiers vary little images character similarly environment speech classifiers vary little utterances sentences words 
note set possible character classifiers word classifiers larger set particular classifiers english alphabet english language 
particular form letters arbitrary evidenced existence vastly different alphabets hebrew thing required character different examples object 
number different character classifiers potentially astronomical 
formalize idea environment functions determines distortion measure input space define environment probability space sigma pair set functions mapping space oe oe theta oe may metric probability measure environment defined induces natural distortion measure ae oe dq words ae average distance function chosen random environment 
note oe metric ae pseudo metric recall pseudo metric metric ae necessarily imply defined needs sigma algebra leave algebra unspecified follows simply assume appropriate exists 

ae referred canonical distortion measure cdm environment 
relation character transmission problem consist character classifiers set take oe gamma limited capacity learn characters take environmental probability measure support rough distinct elements contain just english letters 
meant imply possible characters distinguish 
precise chosen support vary environment environment 
images character support ae required 
images different characters classifiers corresponding 
classifiers characters subjectively similar behave similar way assign high value positive examples 
classifiers 
substituting shows ae larger images different characters images characters 
note ae depends environment probability measure problems different environments example character classification face recognition different environments space images generate different canonical distortion measures 
section show ae optimal distortion measure piecewise constant approximation functions environment aim 
fact different environments generate different ae shows optimal similarity measure data points highly dependent planning data 
distribution valued space distributions noisy functions modelled framework 
distance measure oe metric hellinger metric nonmetric divergence 
cdm inputs equal average distance distributions 
examples computable closed form linear environment suppose consists linear maps vector space dual isomorphic mind take measure lebesgue measure restrict support cube gammaff ff ff gammaff ff 
oe gamma ae reduced follows ae oe dq ff gammaff ff delta gamma delta ff ff gammaff delta delta delta ff gammaff gamma da dan ff kx gamma linear environment induces squared euclidean distance reverse true assumes ae gammax linear function class 
optimality result section squared euclidean distortion optimal wishes approximate linear functions input space optimal environments 
rare interested applying linear functions images speech signals face classifiers linear maps image space word classifiers linear maps speech signals squared euclidean distortion environments best thing 
quadratic environment gamma oe jy gamma ff 
ax uniformly distributed range gamma 
environment ae gamma jax gamma ay da jx gamma yj note ae gammay gammax zero distance apart ae 
reflects fact gammax notice ae ordinary euclidean distance scaled jx yj 
points fixed euclidean distance apart ae moved away origin 
reflects fact quadratic functions larger variation range large values small values seen calculating ball point ae set points ae 
order gammax gamma gammax gamma note euclidean diameter ball decreases inversely linearly euclidean distance origin 
optimality cdm section shown cdm optimal distortion measure goal find piecewise constant approximations functions environment 
piecewise constant approximations generated specifying quantization fx partition fx oe faithful fx sense piecewise constant approximation function defined information function transmitted person quantization partition function constructed person receiving 
natural way measure deviation context pseudo metric dp dp oe dp dp expected difference sample drawn random reconstruction error respect pair fx fx defined expected deviation approximation measured distribution ef dp dq quantization partition chosen minimize ef 
quantization fx distortion measure ae define partition ae fx ae ae ae fx ae ae ig break ties choosing partition smallest index 
call partition induced ae voronoi partition 
define ae ef ae reconstruction error input space ae defined replacing ae equation 
proofs lemma theorem straightforward omitted due space constraints 
lemma ae ae theorem reconstruction error ef minimal respect quantization fx minimizing reconstruction error ae partition ae induced cdm ae theorem states far generating piecewise constant approximations functions environment concerned better partition input space induced cdm optimal quantization set 
relationship cdm distance measures transformation invariant distance authors introduced technique comparing handwritten characters called transformation distance 
observed images characters invariant transformations rotation dilation shift line thickening 
denoting set transformations assuming parameterised real parameters noted character set gg forms dimensional manifold input space defined distance images inf ky gamma smallest euclidean distance transformed image called transformation distance 
order simplify computation approximated linearised version 
concentrate exact expression 
relating transformation distance cdm note invariance characters action equivalent assuming character classifiers environment invariant action words gx clearly ae cdm gives answer transformation distance case 
ae somewhat arbitrarily measures euclidean distance manifolds ae performs average functions environment compute distance 
transformation distance cdm share important property distance pair points respectively 
trivial construction see cdm 
ae ae combining triangle inequality gives ae ae ae ae ae ae ae running argument interchanged shows ae ae 
cdm edelman chorus prototypes edelman introduced concept representation called chorus prototypes 
idea train set real valued classifiers fn domain prototype objects interpreted probability example object 
objects just prototypes represented vector activations induce output prototype classifiers 
vector activations low dimensional representation ldr input space 
argued euclidean distance representation vectors corresponds distal similarity objects 
cdm chorus prototypes represent similarity making environment classifiers natural look connection 
connection assumes functions environment implemented linear maps composed fixed low dimensional representation 
ae fixed ldr form depend suppose minimal 
note take values outside range inter probabilities 
interpret output degree classification large positive values high confidence large negative values low confidence 
case environmental distribution distribution weights uniform symmetric region weight space section cdm inputs proportional euclidean distance transformed representations 
choose respective weight vectors linearly independent set functions assumed minimal 
chorus prototypes representation 
set 
note nonsingular 
input representation similarity prototypes equal wh 
delta classifier environment 
set ww gamma wh 
classifier environment representable linear combination prototype classifiers required chorus prototypes idea 
similarity measure thrun mitchell authors defined invariance function oe theta finite environment property exists oe undefined assume environmental distribution uniform quick calculation shows oe defined oe gamma jf ae thrun mitchell showed oe facilitate learning novel tasks lifelong learning framework 
learning cdm neural networks environments encountered practise speech recognition image recognition ae unknown 
section shown ae may estimated learnt feedforward neural networks 
experiment cdm ae learnt toy robot arm environment 
learnt cdm generate optimal voronoi regions input space compared voronoi regions true cdm calculated exactly environment 
piecewise constant approximations functions environment learnt respect voronoi partition results compared direct learning feedforward nets 
conclude learning piecewise constant approximations cdm gives far better generalisation performance direct learning approach toy problem 
sampling environment generate training sets learning cdm distribution environment distribution input space sampled 
ff fm samples fx xn samples pair estimate ae ae oe generates training triples ae ae xn ae xn data train neural network 
neural network sets inputs set set real valued output ae representing network estimate ae 
mean squared error network training set ae gamma ae estimate true distance network ae true cdm ae defined ae ae ae gamma ae dp dp note process sampling generate fm form multi task learning see robot arm environment generated link robot arms link lengths 
pair angles square distance robot arm origin 
sampling necessary condition empirical estimate cdm ae converge true cdm ae 
robot arm experiment artificial environment created test effectiveness training neural networks learn cdm 
environment chosen consist set link robot arm problems 
function environment corresponded robot arm links length see 
note term robot fairly loosely example doesn robotics 
function corresponding map gamma computes square distance arm origin angles links cos gamma 
link lengths chosen uniformly range ff goal train neural network correctly implement cdm ae 
note case ae calculated closed form ae cos gamma gamma cos gamma network architecture single hidden layer neural network tanh activation function hidden layer nodes linear output node 
experimentation hidden nodes sufficient 
network inputs knowledge ae symmetric built network output network inputs ae ae just raw network output ae 
note automatically symmetric choice estimate ae error measure ae ae gamma ae backpropagation compute gradient 
training sets generated sampling times environment generate case meant generating pairs uniformly random square input space gamma sampled uniformly random times generate xn empirical estimate ae constructed pairs separate cross validation set generated values training set 
network trained conjugate gradient descent gradient computed error cross set failed decrease iterations row 
network trained initial quantization set size chosen uniformly random fx xn empirical lloyd algorithm optimize positions quantization points 
trained neural network suitably distortion measure 
different experiments performed different values voronoi regions plotted optimal quantization set corresponding voronoi regions true cdm 
note regions generated neural net approximation similar generated true cdm 
striped nature voronoi regions provides strong hint input space fact dimensional 
true verified inspecting equation true cdm ae depends difference 
reconstruction error input picture left voronoi diagram generated quantization points neural network approximation cdm ae trained sample size functions input points 
righthand side voronoi diagram generated true cdm 
pixels colour belong voronoi region 
white dots quantization points lloyd algorithm 
note approximate cdm regions appear quantization point 
fact extra points regions consisting point 
inefficient usage quantization points results lloyd algorithm finds local minimum reconstruction error case reconstruction error nearly zero anyway 
space ae 
similar results values performance improving values increased 
recall lemma ae equals reconstruction error functions environment 

function picked random link lengths picked random 
values optimal quantization set generated stored 
value novel input chosen estimated ae ae expected generalisation error ex gamma ae small value ae indicates function environment learnable high accuracy procedure demonstrate new functions generated random environment learnt procedure cdm ae learnt sample size quantization points previous section 
function generalisation error estimated fine grid input space averaged note procedure just nearest neighbour ae distance metric quantization points intelligently placed 
functions 
average generalisation error close value ae 
comparison purposes functions learnt hidden node neural network assistance cdm 
average generalisation error functions displayed quantity piecewise constant approximations 
piecewise constant approach estimated cdm clearly far superior traditional approach case 
shown existence environment functions quantization process generates canonical distortion measure cdm ae input space 
cdm shown unifying concept seemingly disparate threads research 
proved generating optimal quantization set input space ae distortion measure automatically produces voronoi regions optimal forming approximations nn estimates functions environment 
cdm calculated closed form simple environments 
surprising result squared euclidean distortion measure cdm linear environment optimal interested approximating linear functions 
training examples standard average generalisation error function training samples robot arm environment standard approach vs training estimated cdm 
standard means learning function scratch hidden layer hidden node neural net 
theta theta refer number functions input examples train approximation cdm 
technique estimating cdm training neural network implement tested success toy environment 
needs done show cdm learnable complex environments 
supported part epsrc partly conducted author department computer science royal holloway college university london department mathematics london school economics 
jonathan baxter 
learning internal representations 
proceedings eighth international conference computational learning theory santa cruz california 
acm press 
richard caruana 
learning related tasks time backpropagation 
advances neural information processing 
cover thomas 
elements information theory 
john wiley sons new york 
shimon edelman 
representation similarity chorus prototypes 
minds machines 
gersho gray 
vector quantization signal processing 
kluwer academic publishers 
lloyd 
squares quantization pcm 
technical report bell laboratories 
karen robert gray 
combining image compression classification vector 
ieee transactions pattern analysis machine intelligence may 
lori pratt 
discriminability transfer neural networks 
stephen hanson jack cowan lee giles editors advances neural information processing systems pages san mateo 
morgan kaufmann 
patrice simard yann lecun john denker 
efficient pattern recognition new transformation distance 
hanson cowan lee giles editors advances neural information processing systems pages san mateo 
morgan kauffman 
sebastian thrun tom mitchell 
learning thing 
technical report cmu cs cmu 
