ridge regression learning algorithm dual variables saunders gammerman vovk royal holloway university london egham surrey tw ex uk alex dcs rhbnc ac uk study dual version ridge regression procedure 
allows perform non linear regression constructing linear regression function high dimensional feature space 
feature space representation result large increase number parameters algorithm 
order combat curse dimensionality algorithm allows kernel functions support vector methods 
discuss powerful family kernel functions constructed anova decomposition method kernel corresponding splines infinite number nodes 
introduces regression estimation algorithm combination elements dual version ridge regression applied anova enhancement splines 
experimental results boston housing data set indicate performance algorithm relative algorithms 
formulate regression estimation problem 
suppose set vectors xt supervisor gives real value vectors 
problem construct learning machine new subscripts indicate particular vector tth vector superscripts indicate particular vector element ith element vector 
set examples minimises measure discrepancy prediction value measure loss average square loss defined gamma supervisor answers predicted values number vectors test set 
squares ridge regression classical statistical algorithms known long time 
widely papers drucker regression conjunction high dimensional feature space 
original input vectors mapped feature space algorithms construct linear regression function feature space represents non linear regression original input space 
problem encountered algorithms feature space 
deal large number parameters leads serious computational difficulties impossible overcome 
order combat curse dimensionality problem describe dual version squares ridge regression algorithms allows kernel functions 
approach closely related vapnik kernel method support vector machine 
kernel functions represent dot products feature space allows algorithms feature space having carry computations space 
kernel functions take forms particular attention paid family kernel functions constructed anova decomposition vapnik see wahba 
major objectives 
show kernel functions overcome curse dimensionality mentioned algorithms 

demonstrate anova decomposition kernels constructed evaluate performance compared polynomial spline kernels real world data set 
results experiments performed known boston housing data set show squares ridge regression algorithms perform comparison algorithms 
results show anova kernels consider subset input parameters improve results obtained kernel function anova technique applied 
section dual form squares ridge regression 
ridge regression dual variables presenting algorithms dual variables original formulation squares ridge regression stated clarity 
suppose training set yt number examples vectors ir number attributes ir comparison class consists linear functions delta ir squares method recommends computing minimizes lt gamma delta labeling examples new example attributes predicted label delta ridge regression procedure slight modification squares method replaces objective function lt gamma delta fixed positive constant 
derive dual version ridge regression rr allow includes squares ls special case 
derivation partially follow vapnik 
start re expressing problem minimize expression constraints gamma delta introducing lagrange multipliers ff replace constrained optimization problem problem finding saddle point function ff gamma delta gamma accordance kuhn tucker theorem exist values lagrange multipliers ff ff kt minimum equals minimum constraints 
find optimal minimize maximize ff 
notice fixed values ff minimum equal value optimization problem equality attained ff ff kt doing find solution original constrained minimization problem 
differentiating obtain condition aw gamma ff ff lagrange multipliers usually interpreted reflecting importance corresponding constraints equation shows proportional linear combination taken weight proportional importance 
substituting obtain ff ff delta ff delta gamma ff ff gamma ff gamma ff ff delta ff gamma ff differentiating obtain ff importance tth constraint proportional corresponding residual substitution gives gamma ff ff delta gamma ff ff denoting theta matrix dot products delta differentiating ff obtain condition gamma kff gamma ff equivalent ff ai gamma recalling obtain prediction ridge regression procedure new unlabeled example delta ff delta ff delta ai gamma vector dot products delta lemma rr prediction label new unlabeled example ai gamma matrix dot products vectors xt training set vector dot products vectors training set delta simply function returns dot product vectors linear regression feature space simply function returns dot product vectors formula corresponds performing linear regression input space ir defined examples 
want construct linear regression feature space choose mapping original space higher dimensional feature space oe 
order lemma construct regression feature space function correspond dot product oe delta oe 
necessary know oe long know oe delta oe 
question functions correspond dot product feature space answered mercer theorem addressed vapnik discussion support vector methods 
illustration idea example simple kernel function 
see girosi 
suppose mapping function oe maps dimensional vector dimensions oe 
dot products take form oe delta oe delta possible kernel function delta generalised kernel function form delta dimensions 
kernel functions allows construct linear regression function high dimensional feature space corresponds non linear regression input space avoiding curse having carry computations high dimensional space 
particular kernel functions way combat curse dimensionality problems faced drucker regression function constructed feature space computations carried high dimensional space leading huge number parameters non trivial problems 
information kernel technique see vapnik wahba 
multiplicative kernels indicating anova decomposition form kernels brief description needed family kernels anova decomposition applied family multiplicative kernels 
refers set kernels multi dimensional case calculated product dimensional case 
onedimensional case dimensional case kn kernel anova decomposition applied spline kernel infinite number nodes see vapnik kimeldorf wahba 
spline approximation infinite number nodes defined interval expansion gamma dt unknown values unknown function defines expansion 
considered inner product kernel generates splines dimension infinite number nodes expressed gamma gamma dt note min function integral sign value zero 
sufficient consider interval min formula equivalent gamma min gammar jx gamma yj particular case linear splines xy jy gamma xj min min anova decomposition kernels anova decomposition kernels inspired statistics analyses different subsets variables 
actual decomposition adapted form kernels vapnik involve different subsets attributes examples certain size 
main reasons choosing anova decomposition 
firstly different subsets considered may group variables lead greater predictive power 
considering subsets input parameters anova decomposition reduces vc dimension set functions considering avoid overfitting training data 
dimensional kernel anova kernels defined follows kn kn kn kn vapnik recurrent procedure calculating value kn 
delta delta delta pn kp kp gamma gammas purposes kernels produced anova decomposition order considered alternative method anova decomposition consider order lower orders stitson experimental results experiments conducted boston housing data set known data set testing non linear regression methods see breiman saunders 
data set consists cases continuous variables binary variable determine median house price certain area boston thousands dollars 
continuous variables represent various values pertaining different locational economic structural features house 
prices lie units 
method drucker data set partitioned training set cases validation set cases test set cases 
partitioning carried randomly times order carry trials data 
trial ridge regression algorithm applied ffl kernel corresponds spline approximation infinite number nodes ffl kernel anova decomposition technique applied ffl polynomial kernels 
kernel set parameters order spline degree polynomial value coefficient selected gave smallest error validation set error test set measured 
experiment repeated support vector machine svm kernels exactly training files see stitson full details 
illustration number parameters considered ridge regression algorithm svm consider polynomial kernel outlined earlier degree 
maps input vectors high dimensional feature space equivalent evaluating different parameters 
results obtained experiments shown table 
measure error tests average squared error 
test files algorithm run square difference predicted actual value taken 
averaged test cases 
produces average error test available anonymous ftp ftp ftp ics uci com pub machine learning databases housing 
files average taken produces final error quoted rd column table 
variance measure table average squared difference squared error measured sample average squared error 
additional results noted 
breiman bagging average squared error drucker support vector regression polynomial kernels average squared error 
result obtained drucker slightly better obtained similar machine may due random selection training validation testing sets 
comparisons section give comparison results known results 
sv machines subsection describe detail connection approach support vector machine 
optimization problem minimizing constraints essentially special case general optimization problem minimize expression kwk constraints gamma delta ffl delta gamma ffl ffl constants 
optimization problem similar problem corresponding huber loss function considered vapnik chapter vapnik considers general regression functions form delta delta difference minor add extra attribute examples 
problem corresponds problem ffl vapnik gives dual statement fortiori problem reach closed form expression table experimental results boston housing data method kernel squared error variance ridge regression polynomial ridge regression splines ridge regression anova splines svm polynomial svm splines svm anova splines mainly interested positive values ffl 
mentioned derivation formula follows 
dual ridge regression known traditional statistics statisticians usually clever matrix manipulations lagrange method 
derivation modelled vapnik gives extra insight see equations 
excellent survey connections support vector machine done statistics refer reader wahba girosi 
formula known theory subsection explain connection readers familiar 
consider bayesian setting ffl vector weights distributed normal distribution mean covariance matrix ffl delta ffl ffl random variables distributed normally mean variance optimization problem constraints problem finding posterior mode normality assumption coincides posterior mean formula gives mean value random variable delta clean version label delta ffl example 
notice random variables delta jointly normal covariances cov cov delta ffl delta ffl delta cov delta cov delta ffl delta delta accordance formula best prediction delta gamma ai gamma coincides 
formula ridge regression included squares special case dual variables derived method lagrange multipliers 
perform linear regression feature space 
showed problem learning high dimensional space solved kernel functions 
allowed algorithm overcome curse dimensionality run efficiently large number parameters considered 
experimental results show ridge regression performs 
results indicate applying anova decomposition kernel achieve better results kernel technique applied 
ridge regression support vector method gave smaller error anova splines compared spline kernel 
weak part experimental section boston housing data useful benchmark applied algorithm wider range practical problems 
plan 
order confirm anova kernels outperform kernels form anova decomposition technique applied multiplicative kernels 
technique applying kernel functions overcome problems high dimensionality investigated see applied algorithms prove computationally difficult impossible faced large number parameters 
feel interesting direction developing results combine dual version ridge regression ideas gammerman obtain measure confidence predictions output algorithms 
expect case simple closed form formulas obtained 
acknowledgments epsrc providing financial support gr support vector bayesian learning algorithms 
referees thoughtful comments gratefully appreciated 
breiman 
bagging predictors 
technical report department statistics university california berkley 
ftp ftp stat edu pub tech reports ps drucker burges kaufman smola vapnik 
support vector regression machines 
advances neural information processing systems volume page 
mit press 
gammerman vapnik vovk 
learning transduction 
uncertainty artificial intelligence 
appear 
girosi 
equivalence approximations support vector machines 
technical report memo massachusetts institute technology artificial intelligence laboratory center biological computational learning department brain cognitive sciences may 
kimeldorf wahba 
results spline functions 
math 
anal 
appl 
saunders gammerman vovk 
ridge regression dual variables 
technical report royal holloway university london 
stitson gammerman vapnik vovk watkins weston 
support vector regression anova decomposition kernels 
technical report royal holloway university london 
vapnik 
nature statistical learning theory 
springer 
vapnik 
statistical learning theory 
gammerman editor computational learning probabilistic reasoning 
wiley 
vapnik 
statistical learning theory 
wiley forthcoming 
wahba 
spline models observational data volume cbms nsf regional conference series applied mathematics 
siam 
wahba 
support vector machines reproducing kernel hilbert spaces randomized 
technical report department statistics university wisconsin usa 
