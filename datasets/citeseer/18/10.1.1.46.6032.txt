classification pairwise coupling trevor hastie stanford university robert tibshirani university toronto discuss strategy classification involves estimating class probabilities pair classes coupling estimates 
coupling model similar bradley terry method paired comparisons 
study nature class probability estimates arise examine performance procedure real simulated datasets 
classifiers include linear discriminants nearest neighbors adaptive nonlinear methods support vector machine 
department statistics sequoia hall stanford university stanford california trevor stanford edu department preventive medicine biostatistics department statistics toronto edu consider discrimination problem classes training observations 
training observations consist predictor measurements predictors known class memberships 
goal predict class membership observation predictor vector typically class classification rules tend easier learn decision boundary requires attention 
friedman suggested approach class problem solve class problems test observation combine pairwise decisions form class decision 
friedman combination rule quite intuitive assign class wins pairwise comparisons 
friedman points rule equivalent bayes rule class posterior probabilities test point known argmax argmax note friedman rule requires estimate pairwise decision 
pairwise classifiers provide rule estimated class probabilities 
argue improve friedman procedure combining pairwise class probability estimates joint probability estimate classes 
leads consider problem 
set mutually exclusive events ak experts give pairwise probabilities ij prob ja 
set probabilities prob compatible ij general solution satisfying constraints may exist 
prob ja requiring gamma free parameters satisfy gamma constraints solution general 
example ij entries matrix delta delta delta compatible 
clear 
model prob ja forms basis bradley terry model paired comparisons bradley terry 
fit model maximizing negative kullback leibler distance criterion find best approximation ij set ij 
carry predictor value estimated probabilities predict class membership example solution 
solution qualitative sense event beats larger margin winner pairwise matches 
shows example procedures action 
data points classes class generated mixture gaussians 
linear discriminant model fit pair classes giving pairwise probability estimates ij panel shows friedman procedure applied pairwise rules 
shaded regions areas indecision class wins vote 
coupling procedure described section applied giving class probability estimates 
pairwise lda max pairwise lda coupling class lda class problem data class generated mixture gaussians 
panel shows maximum win procedure 
second panel shows decision boundary coupling pairwise linear discriminant rules 
third panel shows class lda boundaries 
test error rates shown parentheses 
decision boundaries resulting probabilities shown second panel 
procedure done reasonable job resolving confusion case producing decision boundaries similar class lda boundaries shown panel 
numbers parentheses plots test error rates large test sample population 
notice despite indeterminacy max wins procedure performs worse coupling procedure perform better lda 
show example coupling procedure substantially better max wins 
pairwise approach yields flexible class models class method 
example linear discriminant analysis lda assumes classes covariance 
pairwise application lda assumption pair classes 
organized follows 
coupling model algorithm section 
section discusses properties coupling solution relation max wins rule 
pairwise threshold optimization key advantage pairwise approach discussed section 
section examines performance various methods real simulated problems 
section apply coupling adaptive nonlinear classifier additive modelling 
application support vector machine described section sections look coupling applied nearest neighbour rules 
final section contains discussion 
coupling probabilities probabilities feature vector pk 
section drop argument calculations done separately 
assume ij observations training set estimated conditional probabilities ij prob iji 
model ij equivalently log ij log gamma log log nonlinear model wish find ij close ij 
gamma independent parameters gamma equations possible general find ij ij settle ij close observed ij 
closeness criterion average negative weighted kullback leibler distance ij ij ij ij log ij ij gamma ij log gamma ij gamma ij find maximize function 
model criterion formally equivalent bradley terry model preference data 
observes proportion ij ij preferences item sampling model binomial ij ij bin ij ij ij independent equivalent log likelihood model 
ij independent share common training set obtained common set classifiers 
furthermore binomial models apply case ij evaluations functions point randomness arises way functions constructed training data 
include ij weights crude way accounting different precisions pairwise probability estimates 
score gradient equations ij ij ij ij subject 
iterative procedure compute algorithm 
start guess corresponding ij 
repeat convergence delta ij ij ij ij renormalize recompute ij 
algorithm appears bradley terry 
updates step attempt modify sufficient statistics match expectation go part way 
prove appendix increases step 
bounded zero procedure converges 
convergence score equations satisfied ij consistent 
algorithm similar flavour iterative proportional scaling ips procedure log linear models 
ips long history dating back deming stephan 
bishop fienberg holland give modern treatment 
resulting classification rule argmax properties solution weights ij improve efficiency estimates little effect class sizes different 
simplicity facilitate comparison techniques section assume equal weighting ij 
examples experimented sophisticated weights ij ij gamma ij little difference practice 
simple non iterative estimate obtained row averages ij gamma estimates derived approximation identity gamma replacing ratio second ratios corresponding ij estimates starting values maximum likelihood procedure 
fact order sufficient classification rule required theorem proof satisfy ik ik ik jk ik jk increasing function 
similarly show looking closely find approximate solution pk tends underestimate differences 
specifically result shows closer equi probability vector kullback leibler distance theorem log log proof appendix 
equivalence coupling model results may known literature paired comparisons 
take closer look friedman rule assigning class wins pairwise comparisons classes 
ij ij 
define ij gamma argmax theorem tells start ij ij rules assign class 
second scenario agree case model ij holds exactly procedures classify largest correct probabilities 
general surprising things occur 
situation largest fr ij delta delta delta delta solution ij delta delta delta delta example classes ordering sense ij fr ij delta delta delta delta solution respect ordering 
shows example similar compare performance rules hatched area top left panel indeterminate region 
pairwise lda max pairwise lda coupling lda qda class problem similar data class generated mixture gaussians 
panel shows maximum wins procedure 
second panel shows decision boundary coupling pairwise linear discriminant rules 
third panel shows class lda boundaries fourth qda boundaries 
numbers captions error rates large test set population 
class achieving max 
top right panel coupling procedure resolved indeterminacy favor class weighing various probabilities 
interesting phenomenon occurring coupling reversed decision max win rule 
notice top left panel region left upper shaded wedge class region top right panel class region 
picking point region see matrix ij fr ij delta delta delta class narrowly wins class class beats far class 
example coupling improved misclassification rate numbers parentheses plots dramatically max win lda procedures 
qda performs little better example 
rule max wins may suffer excess variability compared coupling rule investigate performed simple experiment 
defined class probabilities gamma gamma set ij delta ij ji gamma ij ij standard normal variate ij truncated zero 
tried values scenarios class higher probability correct class 
shows average number times class number classes 
class number classes probability correct 
class probability predicting true class rules solid broken 
see text details problems 
selected rules solid broken 
averages simulations standard error 
number classes varies horizontal axis 
see rule outperforms 
pairwise threshold optimization pointed friedman approaching classification problem pairwise fashion allows optimize classifier way computationally burdensome class classifier 
number computations full optimization proportional total required gamma optimizations proportional discuss optimization classification threshold 
class problem logit ij ij 
normally classify class ij 
suppose find ij ij better 
define ij ij gamma ij ij logit gamma ij 
pairs apply coupling algorithm ij obtain probabilities 
way optimize gamma parameters separately optimize jointly parameters 
example benefit threshold optimization section 
examples class problem 
define classes plane follows generated uniformly square gamma theta gamma 
define centers gamma gamma gamma 
distance point jth center point assigned class satisfying argmin gamma log log log 
class observations 
example constructed usual linear discriminant threshold log optimal 
data shown decision boundary pairwise coupling lda solid 
threshold optimization class lda 
broken line shows boundary standard class lda 
threshold optimization accurately captured boundary 
simulated class problem showing pairwise coupled linear rule threshold optimization solid standard class linear discriminant rule broken 
see text details simulation 
various datasets 
table shows error rates class problem number datasets 
classifiers ffl lda linear discriminant analysis ffl qda quadratic discriminant analysis ffl max rule ffl max thresh rule threshold optimization 
threshold minimize training error classes grid possible values 
ffl coupled rule ffl coupled thresh rule threshold optimization 
summary datasets table 
real datasets available machine learning archive university california irvine ftp ics uci edu exception digits dataset 
consists constructed features classification handwritten digits available authors 
pairwise procedures outperform linear discriminant analysis problems 
threshold optimization improve performance friedman max rule coupling rule 
note quadratic discriminant analysis nearly pairwise coupling problems 
table summary datasets dataset training test classes features vowel waveform vehicle crabs digits class example adaptive nonlinear classification proposals adaptive estimation nonlinear regression surfaces 
proposals additive models hastie tibshirani mars multivariate additive regression splines friedman applied straightforward way classification problems 
section propose new way generalizing adaptive regression pairwise coupling idea 
start simple global procedure lda multiple logistic regression 
lda 
find pair classes highest confusion rate proportion times class observation classified class find maximize 
classes coded apply adaptive regression procedure 
pairs modelled linear regression 
pairwise coupling applied update joint class table training errors top row test errors bottom row different examples 
values mean standard errors simulations 
dataset lda qda max max thresh coupled coupled thresh vowel waveform vehicle crabs digits class probabilities procedure repeated fixed number iterations iterations 
stage find pair linearly modelled classes wit highest error rate allow nonlinear model pair 
advantage approach complex nonlinear functions classes needed 
contrast methods described set basis functions classes 
illustration applied methods data vowel recognition 
nonlinear regression procedure adaptive backfitting additive models cubic splines 
see hastie hastie tibshirani chapter details 
vowel example popular benchmark neural network algorithms consists training test data predictors classes 
obtained data benchmark collection maintained scott fahlman carnegie mellon university 
data contributed anthony robinson see robinson provided edited description 
ascii approximation international phonetic association symbol word eleven vowel sounds recorded table 
word uttered fifteen speakers 
male female speakers train networks male female speakers testing performance 
features derived utterance linear filtering speech signal 
iterations adaptive nonlinear classification technique procedure built nonlinear rules classes 
error rate decreased table words recording vowels vowel word vowel word heed hod hid hoard head hood hard heard hud levelled 
test set confusion matrices lda nonlinear pairwise procedure shown 
columns represent true class rows represent predicted class 
nonlinear construction successfully reduced error rates especially classes 
training test error rates shown table pairwise approach produces substantial improvement lda 
problem reasonably large training cases classes features 
computations example took minute silicon graphics challenge series computer 
computations examples simple classifier applied gamma problems took minute 
apply nonlinear classifier adaptive additive models pairwise problem computation increase considerably 
advantage adaptive approach section apply nonlinear classifier pairs need 
hastie tibshirani buja discuss approaches adapting regression procedures classification problems 
construct multi response version regression procedure simultaneously modelling outcomes terms set optimally chosen basis functions 
applying procedure indicator matrix coding responses obtains vector fitted values observation 
classify observation class largest fitted value 
idea known softmax machine learning literature 
experiments hastie 
procedure particularly demonstrate kind masking occur classes linearly aligned feature space 
approach apply linear discriminant analysis space fitted values multi response regression 
called flexible discriminant analysis fda hastie 
theory technique exploits connection linear discriminant analysis optimal scoring 
error rates softmax optimal scoring vowel data respectively hastie 

table vowel recognition results technique error rates training test lda adaptive backfitting pairwise coupling support vector machine boser guyon vapnik proposed class classifier finds hyperplane maximizing minimum signed distance plane training points 
specifically norm vector hyperplane delta minimizing functional jjwjj fl subject delta gamma outcome coded gamma 
classifier predicts class class 
nature criterion solution vector linear combination subset feature vectors called support vectors 
see vapnik complete discussion 
intercept minimizing training error 
normally optimize choice regularization parameter fl problem simplicity fairness procedures considered fixed value fl 
support vector machine shown promising results real world problems vapnik personal communication 
simple multiclass version attractive candidate pairwise coupling procedure 
proceed need obtain class probability estimates support vector machine follows define means class 
standard deviation gamma define gamma oe oe oe oe denotes gaussian density mean standard deviation oe 
construction satisfies consistent classification rule 
results multiclass support vector machine shown table 
sv max rule combine pairwise classifications sv coupled coupling rule performs coupled linear discriminant method 
experiments nearest neighbors nearest neighbor classifier chooses majority class closest training points target point 
typically euclidean distance jjx gamma jj gamma gamma measure distance test input training inputs table results support vector machine 
figures training test error rates single realization class problem mean standard error simulations 
data lda sv max sv coupled vowel waveform vehicle crabs digits class ave test error improvement vs lda view nearest neighbors follows 
class construct class probability estimates jjx gamma jj jth largest jjx gamma jj values 
classify class highest estimated probability 
way potentially improve performance nearest neighbors multiply probability estimate bias factor form estimates positive optimize biases friedman rosen burke goodman joint optimization parameters computationally difficult friedman suggested carrying pairwise fashion combining rules max wins procedure simple example due friedman illustrates bias factors help 
illustrated 
data points classes 
points class uniformly distributed rectangle theta second class uniformly distributed rectangle theta 
large circular neighbourhood class region near decision boundary tend points class misclassify 
order avoid misclassification standard nearest neighbour rule shrink neighborhood 
turn causes increase variance 
include bias factor class densities fairly compared neighborhood don shrink neighborhood 
fact friedman uses additive bias find multiplicative bias natural 
example case nearest neighbor biasing needed 
data points uniformly distributed rectangular regions separated broken line 
large circular neighbourhood centered class region near decision boundary tend points class misclassify 
biasing class densities small probability estimates discrete 
propose view nearest neighbor classification terms density estimation 
distance jth nearest neighbor computed separately class 
natural estimate class density dimension space number training points class assuming sample priors corresponding class probability estimates note identical usual definition nearest neighbor classification 
estimates suffer discreteness problem modified bias factor just 
biased pairwise probabilities combined coupling procedure 
example gaussian classes unequal covariance simulated example taken friedman gaussian classes dimensions 
mean vectors class chosen independent uniform random variables 
covariance matrices constructed eigenvectors square roots uniformly distributed dimensional unit sphere subject mutually orthogonal eigenvalues uniform 
observations class training set class test set 
optimal decision boundaries problem quadratic linear nearest neighbor methods suited 
friedman states bayes error rate 
shows test error rates linear discriminant analysis nearest neighbor paired versions threshold optimization 
see coupled classifiers nearly halve error rates case 
addition coupled rule works little better friedman max rule task 
friedman reports median test error rate thresholded version pairwise nearest neighbor 
pairwise thresholding example 
looked closely pairwise nearest neighbour rules rules constructed problem 
thresholding biased pairwise distances average 
average number nearest neighbours class standard nearest neighbour approach neighbours classes 
classes translates neighbours 
relative standard nn rule pairwise rule threshold optimization reduce bias able times near neighbours 
discussion geoffrey hinton suggested pairwise approaches classification suffer problem 
suppose example classifying handwritten digits digit say tends closer average feature space randomly chosen digit image digits 
prediction time test image say poorly written nn nn max nn coup lda lda max lda coup test errors simulations class gaussian example 
pairwise classifier 
classifiers trained give unreliable pairwise conditional probabilities 
classifier doesn give high conditional probabilities digit win tends receive higher probability random digits 
point may bad predict pairwise classifiers trained images type image prediction requires extrapolation feature space 
investigate validity point modified experiment 
true class class probabilities class gamma gamma rest 
true class probabilities true class class probabilities gamma gamma gamma remaining classes 
class finishes second true class 
ij defined 
generated realizations model choosing true class random time 
left panel shows probability correct classification coupling rule solid max rule broken 
comparing left panel see existence popular class increased error rate increase larger max rule consistently worse coupling rule 
simulation suggests hinton suggested problem may real 
clear non pairwise approaches fare better 
addition estimates probabilities lost 
right panel shows boxplots maximum class probability coupled classifier stratified classification correct 
surprisingly classifier errs tends sure prediction 
willing decide classify magnitude maximum class probability results improve 
example maximum class probability point error rate decreases 
number classes probability correct 
correct incorrect left panel probability predicting true class rules solid broken 
right panel maximum class probability coupled classifier stratified classification correct 
see text details problem 
suppose pairwise classifiers provide conditional probability estimates ij estimates variance ij say ij variances reciprocal weights coupling algorithm 
specifically replace ij ij ij algorithm 
theory help extrapolation problem mentioned point class far away training data classes high estimated variance classifier value 
experiments approach pairwise linear classifiers improve results unweighted coupling procedure 
refined approach incorporate covariances ij model pursued 
pairwise procedures friedman max win coupling offer improvements additional optimization efficiency gains possible simpler class scenarios 
situations perform exactly multiple class classifiers 
examples ffl pairwise rules qda class modelled gaussian distribution separate covariances ij derived bayes rule 
ffl generalization density class modelled fashion density estimates near neighbor methods density estimates bayes rule 
pairwise lda followed coupling offer nice compromise lda qda decision boundaries longer linear 
special case derive different coupling procedure globally logit scale guarantee linear decision boundaries 
nature currently progress jerry friedman 
acknowledgments jerry friedman sharing preprint pairwise classification acknowledge helpful discussions jerry geoff hinton radford neal david 
comments editor referees led valuable improvements manuscript 
trevor hastie partially supported dms national science foundation roi ca national institutes health 
rob tibshirani supported natural sciences engineering research council canada iris centre excellence 
appendix convergence algorithm effect update step algorithm single ff ij ij ij ij ij ff ij ff ij ji ji ji ff ij ji ffp resulting change gamma ij ij log ij ij ij ij gamma ij log ij ij ij ij ij gamma ij brevity ij ij ij ij gamma log gamma ij log gamma ij log gamma ij gamma ij log gamma gamma second line inequality log gamma 
line inequality log gamma verified noting stationary points value hessian positive definite 
note equality holds ij ij ij ij log likelihood increases step 
bounded algorithm converges 
note algorithm differs standard iterative proportional scaling doesn minimize iteration 
due non linearity model require line search step increase likelihood iteration converges quite quickly practice 
theorem log log proof ij ij gamma gamma log gamma log log gamma second line fact takes minimum equal 
minimum gamma theorem proved 
bishop fienberg holland 
discrete multivariate analysis mit press cambridge 
boser guyon vapnik 
training algorithm optimal margin classifiers proceedings colt ii philadelphia pa bradley terry 
rank analysis incomplete block designs 
method paired comparisons biometrics pp 

deming stephan 
squares adjustment sampled frequency table expected marginal totals known ann 
math 
statist 
pp 

friedman 
multivariate adaptive regression splines discussion annals statistics 
friedman 
approach classification technical report stanford university 
friedman 
bias variance loss curse dimensionality technical report stanford university 
hastie 
discussion flexible parsimonious smoothing additive modelling discussion friedman silverman technometrics 
hastie tibshirani 
generalized additive models chapman hall 
hastie tibshirani buja 
flexible discriminant analysis optimal scoring amer 
statist 
assoc 
pp 

rosen burke goodman 
local learning methods high dimensions beating bias variance recalibration nips workshop machines learn neural networks computing 
vapnik 
nature statistical learning theory springer verlag new york 

