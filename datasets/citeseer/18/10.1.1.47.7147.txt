techniques enhance performance memory consistency models kourosh gharachorloo anoop gupta john hennessy computer systems laboratory stanford university ca memory consistency model supported multiprocessor directly affects performance 
attempts relax consistency models allow buffering pipelining memory accesses 
unfortunately potential increase performance afforded relaxing consistency model accompanied complex programming model 
introduces general implementation techniques provide higher performance models 
technique involves prefetching values accesses delayed due consistency model constraints 
second technique employs speculative execution allow processor proceed consistency model requires memory accesses delayed 
combined techniques alleviate limitations imposed consistency model buffering pipelining memory accesses significantly reducing impact memory consistency model performance 
buffering pipelining attractive techniques hiding latency memory accesses large scale shared memory multiprocessors 
unconstrained techniques result intractable programming model machine 
consistency models provide tractable programming models introducing various restrictions amount buffering pipelining allowed 
memory consistency models proposed literature 
strictest model sequential consistency sc requires execution parallel program appear interleaving execution parallel processes sequential machine 
sequential consistency imposes severe restrictions buffering pipelining memory accesses 
strict models release consistency rc allows significant overlap memory accesses synchronization accesses identified classified acquires releases 
relaxed models discussed literature processor consistency pc weak consistency wc data race free drf 
models fall sequential release consistency models terms strictness 
constraints imposed consistency model satisfied placing restrictions order completion memory accesses process 
guarantee sequential consistency example sufficient delay access previous access completes 
relaxed models require fewer delay constraints 
presence delay constraints limits performance 
presents novel implementation techniques alleviate limitations imposed performance delay constraints 
technique hardware controlled non binding prefetch 
provides pipelining large latency accesses delayed due consistency constraints 
second technique speculative execution load accesses 
allows processor proceed load accesses delayed due earlier pending accesses 
combination techniques provides significant opportunity buffer pipeline accesses regardless consistency model supported 
consequently performance consistency models relaxed models enhanced 
importantly performance different consistency models equalized reducing impact consistency model performance 
result noteworthy light fact relaxed models accompanied complex programming model 
section provides background information various consistency models 
section discusses prefetch scheme detail 
speculative execution technique described section 
general discussion proposed techniques related sections 
conclude section 
background consistency models consistency model imposes restrictions order shared memory accesses initiated process 
strictest model originally proposed lamport sequential consistency sc 
sequential consistency requires execution parallel program appear interleaving execution parallel processes sequential machine 
processor consistency pc proposed goodman relax restrictions imposed sequential consistency 
processor consistency requires writes issued processor may observed order issued 
order writes processors occur observed third processor need identical 
sufficient constraints satisfy processor consistency specified formally 
relaxed consistency model derived relating memory request ordering synchronization points program 
weak consistency model wc proposed dubois idea guarantees consistent view memory synchronization points 
example consider process updating data structure critical section 
sc access critical section delayed previous access completes 
delays unnecessary programmer sure process rely data structure consistent critical section exited 
weak consistency exploits allowing accesses critical section pipelined 
correctness achieved guaranteeing previous accesses performed entering exiting critical section 
release consistency rc extension weak consistency exploits information synchronization classifying acquire release accesses 
acquire synchronization access lock operation process spinning flag set performed gain access set shared locations 
release synchronization access unlock operation process setting flag permission 
acquire accomplished reading shared location appropriate value read 
acquire associated read synchronization access see discussion read modify write accesses 
similarly release associated write synchronization access 
contrast wc rc require accesses release delayed release complete purpose release signal previous accesses complete say ordering accesses 
similarly rc require acquire delayed previous accesses 
data race free drf model proposed adve hill similar release consistency distinguish acquire release accesses 
discuss model similarity rc 
ordering restrictions imposed consistency model terms access allowed perform 
read considered performed return value bound modified write operations 
similarly write considered performed value written write operation visible processors 
simplicity assume write visible processors time 
techniques described easily extended allow case write visible processors different times 
notion performed having completed interchangeably rest 
shows restrictions imposed consistency models memory accesses process 
shown sequential consistency guaranteed requiring shared accesses perform program order 
processor consistency allows flexibility sc allowing read operations bypass previous write operations 
weak consistency release consistency differ sc pc exploit information synchronization accesses 
wc rc allow accesses synchronization operations pipelined shown 
numbers blocks denote order accesses occur program order 
shows rc provides flexibility exploiting information type synchronization 
delay arcs shown need observed correctness 
conventional way achieve delaying access required possibly empty set weak consistency release consistency models shown models respectively terminology 
load store load store load store load store load store load store weak consistency wc acquire acquire release release load store load store load store load store load store load store release consistency rc acquire release acquire release sequential consistency sc load load store load load store store store processor consistency pc load load store load load store store store perform performed load store load store loads stores perform order long local data control dependences observed ordering restrictions memory accesses 
accesses performed 
interesting alternative allow access partially fully proceed delay arcs demand access delayed simply detect remedy cases early access result incorrect behavior 
key observation majority accesses execution correct consistency model delay arcs enforced 
accesses allowed proceed delay 
accesses require delay correctness properly handled detecting problem correcting access memory system 
common case handled maximum speed preserving correctness 
prefetch speculative execution techniques described sections observations 
brevity techniques context sc rc represent extremes spectrum consistency models 
extension techniques models straightforward 
prefetching previous section described delay constraints imposed consistency model limit amount buffering pipelining memory accesses 
prefetching provides method increasing performance partially proceeding access delayed due consistency model constraints 
subsections describe prefetch scheme proposed provide insight strengths weaknesses technique 
description prefetching classified binding non binding controlled hardware ware 
binding prefetch value register load bound time prefetch completes 
places restrictions binding prefetch issued value stale processor modifies location interval prefetch 
hardware cache coherent architectures stanford dash multiprocessor provide prefetching non binding 
non binding prefetch data brought close processor cache kept coherent processor reads value 
non binding prefetching affect correctness consistency models simply performance boosting technique 
technique described section assumes hardware controlled non binding prefetch 
contrast technique previously proposed prefetch techniques section 
prefetching enhance performance partially servicing large latency accesses delayed due consistency model constraints 
read operation read prefetch bring data cache read shared state operation delayed due consistency constraints 
prefetch non binding guaranteed read operation return correct value allowed perform regardless prefetch completed 
majority cases expect result returned prefetch correct result 
time result may different location written time prefetch returns value time read allowed perform 
case prefetched location get invalidated updated depending coherence scheme 
invalidated read operation cache access new value memory system prefetch occurred 
case update protocol location kept date providing new value read operation 
write operation read exclusive prefetch acquire exclusive ownership line enabling write location complete quickly allowed perform 
read exclusive prefetch possible coherence scheme invalidation 
similar read prefetch case line invalidated processor writes location time read exclusive prefetch completes actual write operation allowed proceed 
addition exclusive ownership processor reads location time 
implementation subsection discusses requirements prefetch technique imposes multiprocessor architecture 
consider proposed prefetch technique incorporated processor environment 
assume general case processor load store buffer 
usual way enforce consistency model delay issue accesses buffer certain previous accesses complete 
prefetching incorporated framework having hardware automatically issue prefetch read prefetch reads read exclusive prefetch writes atomic read accesses load store buffer delayed due consistency constraints 
prefetch buffer may buffer multiple prefetch requests 
prefetches retired buffer fast cache memory system allow 
prefetch request checks cache see line 
prefetch discarded 
prefetch issued memory system 
prefetch response returns processor placed cache 
processor location prefetched result returned request combined prefetch request duplicate request sent completes soon prefetch result returns 
prefetch technique discussed imposes requirements memory system 
importantly architecture requires hardware coherent caches 
addition location prefetched needs 
effective writes prefetching requires invalidation coherence scheme 
update schemes difficult partially service write operation making new value available processors results write performed 
prefetching beneficial architecture needs high bandwidth pipelined memory system including caches sustain outstanding requests time 
cache busy memory prefetched access cache twice prefetch time actual 
previously mentioned accessing cache prefetch request desirable avoiding extraneous traffic 
believe double access major issue prefetch requests generated normal accesses delayed due consistency constraints definition requests cache time 
lookahead instruction stream beneficial hardware controlled prefetch schemes 
processors dynamic instruction scheduling decoding instruction decoupled execution instruction help providing lookahead 
branch prediction techniques allow execution instructions past unresolved conditional branches enhance 
aggressive lookahead provides hardware memory requests delayed load store buffers due consistency constraints especially stricter memory consistency models gives prefetching opportunity pipeline accesses 
strengths weaknesses hardware controlled non binding prefetching discussed subsection 
strengths weaknesses subsection presents example code segments provide intuition circumstances prefetching boosts performance prefetching fails 
shows code segments 
accesses read write shared locations 
assume processor non blocking reads branch prediction machinery 
cache coherence scheme assumed invalidation cache hit latency cycle cache latency cycles 
assume memory system accept access cycle caches lockup free 
assume processes writing locations examples lock synchronizations succeed lock free 
consider code segment left side 
code segment resembles producer process updating values memory locations 
system sequential consistency access delayed previous access performed 
accesses cache unlock access hits due fact exclusive ownership gained previous lock access 
accesses take total cycles perform 
system release consistency write accesses delayed lock write write unlock hit example read read read unlock hit hit lock example example code segments 
lock access performed unlock access delayed write accesses perform 
write accesses pipelined 
accesses take cycles 
prefetch technique described section boosts performance sequential release consistent systems 
concerning loop implement lock synchronization assume branch predictor takes path assumes lock synchronization succeeds 
lookahead instruction stream allows locations prefetched read exclusive mode 
regardless consistency model lock access serviced parallel prefetch write accesses 
result lock access returns write accesses satisfied quickly locations prefetched cache 
prefetching accesses complete cycles sc rc 
example prefetching boosts performance sc rc equalizes performance models 
consider second code segment right side 
code segment resembles consumer process reading memory locations 
read accesses critical section 
shown read location assumed hit cache read array depends value access appropriate element 
simplicity ignore delay due address calculation accessing array element 
sc accesses take cycles perform 
rc take cycles 
prefetch technique accesses take cycles sc cycles rc 
performance sc rc enhanced prefetching maximum performance achieved model 
reason simply address read access array depends value read access cache hit access allowed perform value processor read completes sc lock access completes rc 
prefetching boost performance pipelining accesses delayed due consistency constraints fails remedy cases order consumption return values important allow processor proceed efficiently 
summary prefetching effective technique pipelining large latency consistency model disallows 
prefetching fails boost performance order consumption prefetched values important 
cases occur applications accesses hit cache dispersed accesses order values returned cache hits critical achieving highest performance 
section describes speculative technique remedies shortcoming allowing processor consume return values order regardless consistency constraints 
combination prefetching stores speculative execution technique loads shown effective opportunity maximum pipelining buffering 
speculative execution section describes speculative execution technique load accesses 
example implementation part section 
seen technique particularly applicable superscalar designs proposed generation microprocessors 
subsection shows execution simple code segment speculative loads 
description idea speculative execution simple 
assume program order large latency access load access 
addition assume consistency model requires completion delayed completes 
speculative execution load accesses works follows 
processor obtains assumes return value access completes proceeds 
time completes return value processor current value speculation successful 
clearly computation correct delayed value access returns 
current value different speculated processor computation incorrect 
case need throw computation depended value repeat computation 
implementation scheme requires speculation mechanism obtain speculated value access detection mechanism determine speculation succeeded correction mechanism repeat computation speculation unsuccessful 
consider speculation mechanism 
reasonable thing perform access returned value 
case access cache hit value obtained quickly 
case cache return value obtained quickly access effectively pipelined previous accesses way similar prefetching 
general guessing value access beneficial value known constrained small set lock accesses 
regarding detection mechanism naive way detect incorrect speculated value repeat access consistency model allowed proceed circumstances check return value speculated value 
speculation mechanism performs speculative access keeps location cache possible determine speculated value correct simply monitoring coherence transactions location 
speculative execution technique implemented cache accessed access versus times required prefetch technique 
refer back accesses consistency model requires completion load access delayed completes 
speculative technique allows access issued processor allowed proceed return value 
detection mechanism follows 
invalidation update message location completed indicates value access may incorrect 
addition lack invalidation update messages indicates cases speculated value remains correct 
invalidation update occurs due false sharing location cache line 
second new value written speculated value 
conservatively assume speculated value incorrect case 
speculated value correct 
cache replacements need handled properly 
location completes invalidation update messages may longer reach cache 
speculated value assumed stale case willing repeat access completes check current value speculated value 
subsection provides implementation details mechanism 
speculated value determined wrong correction mechanism involves discarding computation depended speculated value repeating access computation 
mechanism correction mechanism processors branch prediction machinery ability execute instructions past unresolved branches 
branch prediction prediction determined incorrect instructions computation branch discarded new target instructions fetched 
similar way speculated value determined incorrect load access computation discarded instructions fetched executed achieve correctness 
speculative technique overcomes shortcoming prefetch technique allowing order consumption speculated values 
referring back second example consider speculative technique performs 
assume processes writing locations 
speculative execution achieves level pipelining achieved prefetching 
addition read access longer hinders performance return value allowed consumed previous accesses outstanding 
sc rc complete accesses cycles 
speculative execution load accesses issued soon address access known regardless consistency model supported 
similar prefetch technique speculative execution technique imposes requirements memory system 
hardware coherent caches required providing efficient detection mechanism 
addition high bandwidth pipelined memory system caches necessary sustain multiple outstanding requests 
example implementation subsection provides example implementation speculative technique 
processor dynamic scheduling branch prediction capability 
prefetching speculative technique benefits lookahead instruction stream provided processors 
addition correction mechanism branch prediction machinery easily extended handle correction speculative load accesses 
processors complex incorporating speculative execution load accesses design simple significantly add complexity 
subsection begins description dynamically scheduled processor chose base example implementation 
details implementing speculative execution load accesses discussed 
obtained organization base processor directly study johnson organization described smith match architecture 
shows structure processor 
brief description processor interested reader referred johnson thesis detail 
processor instruction memory cache btb decoder branch register file alu shifter data memory cache reorder buffer addr data unit load store structure johnson dynamically scheduled processor 
consists independent function units 
functional unit reservation station 
reservation stations instruction buffers decouple instruction decoding instruction execution allow dynamic scheduling instructions 
processor execute instructions order instructions fetched decoded program order 
addition processor allows execution instructions past unresolved conditional branches 
branch target buffer btb incorporated instruction cache provide conditional branch prediction 
reorder buffer architecture responsible functions 
function eliminate storage conflicts register renaming 
buffer provides extra storage necessary implement register renaming 
instruction decoded dynamically allocated location reorder buffer tag associated result register 
tag updated actual result value instruction completes 
instruction attempts read register correct execution achieved providing value tag reorder buffer value register file 
unresolved operand tags reservation stations updated appropriate value instruction corresponding result register completes 
second function reorder buffer allow processor execute instructions past unresolved conditional branches providing storage uncommitted results 
reorder buffer functions fifo queue instructions committed 
instruction head queue completes location belonging deallocated result value written register file 
processor decodes allocates instructions program order updates register file take place program order 
instructions kept fifo order instructions reorder buffer ahead branch depend branch instructions branch control dependent 
results instructions depend branch committed register file branch completes 
addition memory stores control dependent conditional branch held back branch completes 
branch mispredicted instructions branch invalidated reorder buffer reservation stations buffers appropriately cleared decoding execution started correct branch target 
load store reservation station address unit store buffer store data load store cache addr cache write data store tag load addr done acq speculative load buffer organization load store functional unit 
mechanism provided reorder buffer handling branches provide precise interrupts 
precise interrupts provided allow processor restart quickly need save restore lot state 
discuss effect requiring precise interrupts implementation consistency models section 
reorder buffer plays important role eliminating storage conflicts register renaming allowing conditional branches bypassed providing precise interrupts 
implement speculative load technique load store memory unit base processor needs modified rest components remain virtually unchanged 
shows components memory unit 
describe components shown left side 
components regardless speculative loads supported 
new component required supporting speculative loads speculative load buffer described 
load store reservation station holds decoded load store instructions program order 
instructions retired address unit fifo manner 
effective address memory instruction may depend unresolved operand possible address instruction head reservation station computed 
retiring instructions stalled effective address instruction head computed 
address unit responsible computing effective address doing virtual physical translation 
physical address obtained address data store operations placed store buffer 
retiring stores store buffer done fifo manner controlled reorder buffer assure precise interrupts mechanism explained paragraph 
load operations allowed bypass store buffer dependence checking done store buffer assure correct return value load 
implementation sufficient uniprocessor need add mechanisms enforce consistency constraints multiprocessor 
consider access order guaranteed sequential consistency 
conventional method achieving delay completion access previous access complete 
consider store operations delayed appropriately 
general store operation may need delayed certain previous load store operations completed 
mechanism delaying store aided fact stores withheld provide precise interrupts 
mechanism follows 
uncommitted instructions allocated location reorder buffer retired program order 
store instruction instruction head reorder buffer retired completes 
store instructions store retired reorder buffer soon address translation done 
reorder buffer controls store buffer signaling safe issue store memory system 
signal store reaches head reorder buffer 
consequently store issued previous loads computation complete 
mechanism satisfies requirements placed sc model store respect previous loads 
implementation simpler sc change policy retiring stores store head reorder buffer retired completes rc store head retired soon address translation done 
sc store delayed previous stores complete store buffer ends issuing stores time 
turn restrictions load accesses satisfied 
discuss requirements assuming speculative load mechanism 
sc sufficient delay load previous loads stores completed 
done stalling load store reservation station loads previous load performed store buffer empties 
speculative execution load accesses mechanism satisfying restrictions load accesses changed 
major component supporting mechanism buffer 
reservation station longer responsible delaying certain load accesses satisfy consistency constraints 
load issued soon effective address computed 
speculation mechanism comprises issuing load soon possible speculated result returns 
speculative load buffer provides detection mechanism signaling speculated result incorrect 
buffer works follows 
loads retired reservation station put buffer addition issued memory system 
fields entry shown load address acq done store tag 
load address field holds physical address load 
acq field set load considered acquire access 
sc loads treated acquires 
done field set load performed 
consistency constraints require load delayed previous store store tag uniquely identifies store 
null store tag specifies load depends previous stores 
store completes corresponding tag speculative load buffer nullified 
entries retired fifo manner 
conditions need satisfied entry head buffer retired 
store tag field equal null 
second done field set acq field set 
sc entry remains buffer previous load store accesses complete load access refers completes 
appendix describes atomic read modify write incorporated implementation 
describe detection mechanism 
coherence transactions monitored buffer invalidations ownership requests updates replacements 
load addresses buffer associatively replacement required processor accesses address maps cache line valid data different address 
avoid deadlock replacement request line outstanding access needs delayed access completes 
checked match address transactions 
multiple matches possible 
assume match closest head buffer reported 
match buffer address invalidated updated signals possibility incorrect speculation 
match address replaced signifies coherence transactions address sent processor 
case speculated value load assumed incorrect 
guaranteeing constraints release consistency done similar way sc 
conventional way provide rc delay release access previous accesses complete delay accesses acquire acquire completes 
consider delays stores 
mechanism provides precise interrupts holding back store accesses store buffer sufficient guaranteeing stores delayed previous acquire 
mechanism described stricter rc requires conservative implementation required providing precise interrupts 
mechanism guarantees release simply special store access delayed previous load accesses 
guarantee release delayed previous store accesses store buffer delays issue release operation previously issued stores complete 
contrast sc ordinary stores issued pipelined manner 
consider restriction load accesses rc 
conventional method involves delaying load access previous acquire access complete 
done stalling load store reservation station acquire access acquire completes 
reservation station need stalled speculative load technique 
similar implementation sc loads issued soon address known speculative load buffer responsible detecting incorrect values 
speculative load buffer description sc applies rc 
difference acq field set accesses considered acquire accesses rc 
rc entry remains speculative load buffer previous acquires completed 
furthermore acquire entry remains buffer completes 
detection mechanism described sc remains unchanged 
speculative load buffer signals incorrect speculated value computation depends value needs discarded repeated 
cases consider 
case coherence transaction invalidation update replacement arrives speculative load completed done field set 
case speculated value may instructions load 
conservatively assume instructions past load instruction depend value load mechanism handling branch misprediction treat load instruction mispredicted 
reorder buffer discards load instructions instructions fetched executed 
second case occurs coherence transaction arrives speculative load completed done field set 
case speculative load needs reissued instructions incorrect value 
done simply load access require instructions load discarded 
subsection possible ignore entry head buffer store tag null 
null store tag head entry signifies previous accesses required complete completed consistency model constraints allowed access perform time 
handle case properly need tag return values distinguish initial return value reached processor illustrates speculative loads stepping execution simple code segment 
illustrative example subsection step execution code segment shown top 
sequential consistency model assumed 
speculative technique loads prefetch technique stores employed 
shows contents buffers execution 
show detection correction mechanism action assuming speculated value location originally cache invalidated 
instructions assumed decoded placed reorder buffer 
addition assumed load store reservation station issued operations 
event shows loads exclusive prefetches stores issued 
store buffer buffering store operations speculative load buffer entries loads 
note speculated value load consumed processor 
second event occurs ownership arrives location completion store delayed reorder buffer uncommitted instruction ahead store observes precise interrupts 
event signifies arrival value location entry load removed reorder buffer speculative load buffer access completed 
load completes store buffer signaled reorder buffer allow store proceed 
location cached exclusive mode store completes quickly event 
store reaches head reorder buffer 
store buffer signaled turn allow store issue access merged previous exclusive prefetch request location 
point assume invalidation arrives location match location speculation buffer speculated value load instruction load instruction discarded event 
event shows instructions fetched speculative load issued location load speculative previous store store completed 
event shows arrival new value location value known load access issued 
note access completed entry remains reorder buffer store pending 
ownership location arrives event store completes retired form reorder buffer store buffer 
addition load longer considered speculative load retired reorder buffers 
execution completes value arrives event 
discussion implementation techniques provide greater opportunity buffering pipelining accesses previously proposed techniques 
section discusses implications associated techniques 
needs discarded return value repeated access 
addition case multiple matches address speculative load buffer guarantee initial return values corresponding loads 
reorder buffer store buffer speculative load buffer cache contents event ld ld st st ld ld pending ex prf pending ex prf pending valid ld pending ld pending valid exclusive ex prf pending valid ld pending acq done st tag ld addr ld ld ld st ld pending valid valid exclusive ex prf pending acq done st tag ld addr ld ld st ld pending valid valid exclusive ex prf pending acq done st tag ld addr ld ld st invalid valid valid exclusive ex prf pending ld pending valid valid exclusive ex prf pending acq done st tag ld addr ld st ld pending valid valid exclusive ex prf pending acq done st tag ld addr ld ld st ld ld st st ld acq done st tag ld addr ld ld ld st st st st st st st st st st st ld valid valid exclusive ld pending acq done st tag ld addr ld valid valid exclusive reads issued writes prefetched ownership arrives value arrives write completes invalidation arrives read reissued value arrives read reissued ownership arrives value arrives read write write read read hit example code segment ld ld st st ld ld st st ld ld st ld ld st illustration buffers execution code segment 
main idea prefetch speculative load techniques service accesses soon possible regardless constraints imposed consistency model 
course correctness needs maintained early service access helpful 
techniques provide performance benefits probability prefetched speculated value invalidated small 
reasons expect invalidations infrequent 
prefetched speculated locations invalidated loosely depends critical delay accesses obtain correct execution 
supported consistency model relaxed model rc delays imposed synchronization points 
applications time process releases synchronization long time process tries acquire synchronization 
implies process simultaneously accessing locations protected synchronization 
correctness achieved case need delay accesses acquire acquire completes delay release previous accesses complete 
cases supported consistency model strict sc strict delays imposed accesses rarely necessary correctness 
observed programs data races provide sequentially consistent results executed release consistent architecture see formal definition programs property 
delays imposed accesses typical sc implementation superfluous achieving correctness 
important substantiate observations extensive simulation experiments 
major implication techniques proposed performance different consistency models equalized techniques employed 
willing implement techniques choice consistency model supported hardware important 
course choice model software affect optimizations register allocation loop transformation compiler perform program 
choice software orthogonal choice consistency model support hardware 
related section discuss previous regarding prefetching speculative execution 
consider proposed techniques providing efficient implementations consistency models 
main advantage prefetch scheme described study non binding 
hardware controlled binding prefetching studied lee 
evaluated software controlled binding prefetching 
binding prefetching quite limited ability enhance performance consistency models 
example sc implementation described binding prefetch issued earlier actual access allowed issued 
non binding prefetching possible hardware provided 
software controlled non binding prefetching studied porterfield mowry gupta gharachorloo 
porterfield studied technique context uniprocessors mowry gharachorloo studies effect prefetching multiprocessors 
provide simulation results processors blocking reads show software controlled prefetching boosts performance consistency models diminishes performance difference models read read exclusive prefetches employed 
unfortunately software controlled prefetching requires programmer compiler anticipate accesses may benefit prefetching add appropriate instructions application issue prefetch location 
advantage prefetching require software help consume instructions processor cycles prefetch 
disadvantage hardware controlled prefetching prefetching window limited size instruction lookahead buffer theoretically non binding prefetching arbitrarily large window 
general possible combine software controlled non binding prefetching complement 
speculative execution possibly incorrect data values previously described tom knight context providing dynamic parallelism sequential lisp programs 
compiler assumed transform program sequences instructions called transaction blocks 
blocks executed parallel instructions side effect main memory delayed block committed 
sequential order blocks determines order blocks commit side effects 
mechanism described knight checks see block value changed side effecting instructions current block case conflict aborts execution block 
safe side effecting instructions blocks delayed making blocks fully restartable 
scheme loosely similar speculative load scheme discussed scheme unique way speculation enhance performance multiprocessors set consistency constraints 
adve hill proposed implementation sequential consistency efficient conventional implementations 
scheme requires invalidation cache coherence protocol 
points conventional implementation stalls full latency pending writes implementation stalls ownership gained 
implementation satisfy sequential consistency new value written visible processors previous writes processor completed 
gains expected limited latency obtaining ownership slightly smaller latency write complete 
addition proposed scheme provision hiding latency read accesses 
visibility control mechanism reduces stall time writes slightly affect stall time reads expect perform better conventional implementations 
contrast prefetch speculative load techniques provide greater opportunity buffering pipelining read write accesses 
proposed mechanism guaranteeing access order memory processor 
request contains processor identification sequence number 
consecutive requests processor get consecutive sequence numbers 
memory module access common data structure called sequence number table nst 
nst contains entries entry processor 
entry contains sequence number request performed corresponding processor 
allows mechanism guarantee accesses processor kept program order 
theoretically scheme enhance performance sequential consistency 
major disadvantage caches allowed 
severely hinder performance compared implementations allow shared locations cached 
furthermore technique scalable large number processors increment network update different nst grows quadratically connections number processors increases 
detection mechanism described section interesting extended detect violations sequential consistency architectures implement relaxed models release consistency 
release consistent architectures guaranteed provide sequential consistency programs free data races 
determining program free data races undecidable left programmer 
extension detection mechanism execution program determines execution sequentially consistent program data races may result sequentially inconsistent executions 
desirable conservatively detect violations sc programs free data races due absence correction mechanism detection technique conservative described 
addition extended technique needs check violations sc arising performing read write access order 
concluding remarks achieve higher performance number relaxed memory consistency models proposed shared memory multiprocessors 
unfortunately relaxed models complex programming model 
proposed techniques prefetching speculative execution boost performance consistency models 
techniques noteworthy allow strict models sequential consistency achieve performance close relaxed models release consistency 
cost course extra hardware complexity associated implementation techniques 
prefetch technique simple incorporate cache coherent multiprocessors speculative execution technique requires sophisticated hardware support 
mechanisms required implement speculative execution similar employed generation superscalar processors 
particular showed speculative technique incorporated processor design minimal additional hardware 
acknowledgments greatly appreciate insight provided ruby lee led prefetch technique described 
mike smith mike johnson helping details order issue processors 
sarita adve phillip gibbons todd mowry provided useful comments earlier version 
research supported darpa contract 
kourosh gharachorloo partly supported texas instruments 
anoop gupta partly supported nsf presidential young investigator award matching funds tandem trw 
sarita adve mark hill 
implementing sequential consistency cache systems 
proceedings international conference parallel processing pages august 
sarita adve mark hill 
weak ordering new definition 
proceedings th annual international symposium computer architecture pages may 
bernstein 
analysis programs parallel processing 
ieee transactions electronic computers ec october 
michel dubois christoph 
memory access dependencies shared memory multiprocessors 
ieee transactions software engineering june 
michel dubois christoph briggs 
memory access buffering multiprocessors 
proceedings th annual international symposium computer architecture pages june 
kourosh gharachorloo phillip gibbons 
detecting violations sequential consistency 
symposium parallel algorithms architectures july 
kourosh gharachorloo anoop gupta john hennessy 
performance evaluation memory consistency models shared memory multiprocessors 
fourth international conference architectural support programming languages operating systems pages april 
kourosh gharachorloo dan lenoski james laudon phillip gibbons anoop gupta john hennessy 
memory consistency event ordering scalable sharedmemory multiprocessors 
proceedings th annual international symposium computer architecture pages may 
james goodman 
cache consistency sequential consistency 
technical report sci committee march 

data prefetching multiprocessors memory hierarchies 
international conference supercomputing pages september 
william johnson 
super scalar processor design 
phd thesis stanford university june 
keller 
look ahead processors 
computing surveys 
tom knight 
architecture functional languages 
acm conference lisp functional programming 

lockup free instruction fetch prefetch cache organization 
proceedings th annual international symposium computer architecture pages 
leslie lamport 
multiprocessor computer correctly executes multiprocess programs 
ieee transactions computers september 
lee smith 
branch prediction strategies branch target buffer design 
ieee computer 
roland lun lee 
effectiveness caches data prefetch buffers large scale shared memory multiprocessors 
phd thesis university illinois urbanachampaign may 
dan lenoski james laudon kourosh gharachorloo anoop gupta john hennessy 
directory cache coherence protocol dash multiprocessor 
proceedings th annual international symposium computer architecture pages may 
todd mowry anoop gupta 
tolerating latency software controlled prefetching shared memory multiprocessors 
journal parallel distributed computing june 
allan porterfield 
software methods improvement cache performance supercomputer applications 
phd thesis department computer science rice university may 
christoph michel dubois 
concurrent resolution multiprocessor caches 
proceedings international conference parallel processing pages august 
smith 
implementation precise interrupts pipelined processors 
proceedings th annual international symposium computer architecture pages june 
michael smith monica lam mark horowitz 
boosting static scheduling superscalar processor 
proceedings th annual international symposium computer architecture pages may 

latency hiding access ordering scheme multiprocessors buffered multistage networks 
technical report department computer engineering lund university sweden november 

efficient hardware algorithm exploiting multiple arithmetic units 
ibm journal 
appendix read modify write accesses atomic read modify write accesses treated differently accesses far speculative execution concerned 
read modify write locations may cached 
simplest way handle locations delay access previous accesses required complete consistency model completed 
speculative load non cached read modify write accesses 
describe treatment read modify write accesses sc model 
extensions rc model straightforward 
reorder buffer treats read modify writes read write 
case normal reads read modify write retires head reorder buffer access completes 
addition similar normal write reorder buffer signals store buffer read modify write reaches head 
load store reservation station services read modify write splitting operations speculative load results read exclusive request actual atomic read modify write 
speculative load issued memory system placed speculative load buffer 
read modify write simply placed store buffer 
store tag speculative load entry set tag read modify write store buffer 
done field set exclusive ownership attained location return value sent processor 
actual read modify write occurs read modify write issued store buffer 
entry corresponding speculative load guaranteed head speculative load buffer actual read modify write issued 
case match speculative load entry read modify write issued read modify write computations discarded buffers repeated 
return result speculative read exclusive request ignored reaches processor read modify write issued store buffer processor simply waits return result read modify write 
match speculative load entry occurs read modify write issued computation read modify write discarded repeated 
case value read modify write return value issued atomic access 
speculative load entry retired speculative buffer read modify write completes 
