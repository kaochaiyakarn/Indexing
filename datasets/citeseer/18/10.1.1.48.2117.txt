appears machine learning proceedings eleventh international conference cohen hirsh eds morgan kaufmann san francisco ca sampling queries extract rules trained neural networks mark craven jude shavlik computer sciences department university wisconsin west dayton st madison wi craven cs wisc edu shavlik cs wisc edu concepts learned neural networks difficult understand represented large real valued parameters 
approach understanding trained neural networks extract symbolic rules describe classification behavior 
existing rule extraction approaches operate searching rules 
novel method casts rule extraction search problem learning problem 
addition learning training examples method exploits property networks efficiently queried 
describe algorithms extracting conjunctive rules experiments show method efficient conventional search approaches 
problem arises neural networks supervised learning tasks training usually difficult understand concept representations formed networks 
address limitation number approaches developed extracting symbolic representations trained networks craven shavlik fu gallant saito nakano thrun towell shavlik 
approaches cast rule extraction task search problem search involves finding rules explain activations output hidden units network 
novel approach extracting symbolic rules neural networks frames problem search task supervised learning task 
target concept learning task function computed network 
addition learning training examples method exploits property networks queried 
answer questions specific instances covered target concept 
experiments illustrate method significant efficiency advantage search approaches 
concept representation learned neural network usually difficult humans understand representation encoded large number real valued parameters 
important able inspect learned concept definition 
domains medical diagnosis users learning system understand system decisions order confident predictions wolberg press 
learning systems play important role process scientific discovery 
system may discover salient features input data importance previously recognized 
representations formed learner comprehensible discoveries accessible human review hunter klein 
existing search approaches extracting propositional rules trained networks fu gallant saito nakano thrun 
approaches find conjunctive rules searching combinations input values satisfied guarantee unit active regardless state inputs unit 
limitation methods computational complexity search exponential number input features 
real world problems involve large number features lapedes sejnowski rosenberg problem significant 
experiments demonstrate cases approach requires computation search methods get rule sets model networks degree accuracy 
significantly method efficient search approaches problems involve large number features 
contrast methods extract conjunctive rules towell shavlik developed approach extracting rules knowledge neural networks 
generalized approach ordinary neural networks employing special training procedure craven shavlik 
advantages describing networks rules search efficient conjunctive rules extracted rule sets usually concise 
describe learning approach extract rules addition conjunctive rules 
learning approach offers distinct advantages method extracting rules ordinary networks require special training regime network efficiently extract rules describe network arbitrary degree accuracy require hidden units approximated threshold units require extracted rules intermediate term represent hidden unit 
section describes rule extraction viewed search problem previous 
section introduces learning approach rule extraction empirically compares search method 
section describes approach generalized extract rules trained networks 
section discusses section provides concluding remarks 
rule extraction search section briefly define task rule extraction discuss previous approaches task 
dimensions characterize rule extraction methods strategy explore space possible rules method test hypothesized rules 
described investigates dimension 
section describe rule space explored top search section describe bottom learning approach 
section provides brief description approaches testing hypothesized rules learning approach rule extraction 
purposes define task follows trained neural network examples train produce concise accurate symbolic description network 
consider domains network knowledge network towell shavlik press topology initial weights network specified domain theory consisting symbolic inference rules 
threshold extracted rules gamma gamma network extracted rules 
network boolean inputs boolean output 
rules describe conditions instance predicted member class output unit active 
discrete output classes input features boolean nominal valued 
illustrates task rule extraction simple network 
layer network perceptron boolean input features boolean output feature 
network discrete output classes discrete valued input features exactly described set symbolic rules 
rules specify input feature values satisfied guarantee output state 
example rules describe conditions output unit activation unity 
output unit activation calculated follows ae ij activation input unit ij weight unit output unit threshold output unit 
research groups investigated methods operate conducting breadth search space possible conjunctive rules fu gallant saito nakano thrun 
shows search space network 
node space corresponds possible rule arcs indicate specialization relationships downward direction nodes 
node top graph represents general rule instances members class nodes bottom level represent specific instances 
ordinary breadth search continues goal node rule extraction search continues goal nodes maximally general rules 
visiting node search space involves testing rule corresponds node see accurately describes network 
rule tested considering abc ab bc abc ab bc ab rule search space 
node space corresponds possible conjunctive rule network 
top node represents general rule nodes bottom represent specific instances 
clarity nodes labelled 
constraints places network input output units 
example rule gamma specifies activation output unit activation regardless activations rule tested considering case undetermined features take values support activating output unit 
example minimally supportive activation false minimally supportive activation true 
rule determined true take values activation 
testing hypothesized rules multilayer networks somewhat problematic relationship input output necessarily monotonic may depend values input features 
words changing activation input may increase output unit activation cases decrease cases 
basic approaches testing rules multilayer networks discuss 
approach testing rules multilayer network treat network collection perceptrons extract rules hidden output unit separately craven shavlik fu gallant saito nakano towell shavlik 
refer decompositional approach 
approach rules unit expressed terms units feed 
advantage decompositional approach produces intermediate terms may result simpler descriptions 
disadvantage method requires hidden units network approximated threshold units extracted rules may provide accurate representation network 
approach testing rules multilayer networks technique developed thrun called validity interval analysis 
validity interval analysis enables extraction rules directly map inputs outputs multilayer network 
employs linear programming determine set constraints network activation values consistent 
test rule rule negated network input output unit activations constrained negated rule 
constraints validity interval analysis calculates activation range unit network 
inconsistency rule determined valid inconsistency indicates way satisfy negation rule 
advantage validity interval analysis decompositional approach require hidden units approximated threshold units 
suffers drawback find maximally general rules incorrect assumption activations hidden units independent 
number structure search spaces decompositional approaches differ 
decompositional approach requires searching separate rule space hidden output unit network 
approach hand involves searching space output class 
decompositional approach deals single layer network time search spaces may smaller problems 
see point consider network shown 
relationship input output monotonic necessary consider rules non negated 
search approaches rule extraction adequate problem domains require exploring large search spaces 
number nodes rule space exponential number input features consequently search combinatorics overwhelming real world problems 
approach reducing complexity rule searches cluster network weights equivalence classes extract rules craven shavlik towell shavlik press 
mentioned section approach requires special training technique may enable sufficiently accurate description network extracted requires hidden units approximated threshold units 
approach reducing search complexity limit depth search space explored fu gallant saito nakano 
domains rules quite deep search space method effective 
rule extraction learning section introduce approach extracting conjunctive rules trained networks employ top search uses learning process driven sampling queries 
hypothesis method typically require computation search rule extraction methods acquire rule sets model networks comparable degree accuracy 
algorithm approach involves viewing rule extraction learning task target concept function computed network input features simply network input features 
approach uses different oracles able answer queries concept learned 
examples oracle produces demand training examples rule learning algorithm 
subset oracle answers restricted subset queries angluin 
takes arguments class label conjunctive rule 
subset returns true instances covered rule members class false 
algorithm extracting conjunctive rules trained neural networks outlined table 
adaptation classical algorithm pac learning monotone dnf expressions valiant 
algorithm maintains dnf expression class 
algorithm repeatedly queries examples determines class returned example 
example covered current dnf expression class serves basis new rule new term dnf expression 
new rule initialized conjunction feature values returned example 
rule generalized repeatedly dropping antecedent making features undetermined calling subset ascertain rule agrees network 
subset returns true dropped antecedent left rule feature left undetermined put back rule 
cases may necessary extract rules output class 
example case problem involves classes say positive negative extract rules describe positive class closed world assumption identify members negative class 
cases examples oracle instructed throw away examples belong class covered 
note term dnf expression trivially converted conjunctive rule 
table conjunctive rule extraction algorithm 
initialize rules class class rc repeat examples classify covered rc learn new rule conjunctive rule formed antecedent dropped subset true rc rc stopping criterion met explain oracles practice 
recall subset oracle accepts class label rule returns true instances covered classified members class network 
operation equivalent testing rule search rule extraction method 
consequently implemented way decompositional approach rules extracted hidden output unit individually thrun validity interval analysis technique 
implementation subset oracle different case learning algorithm remains fundamentally 
recall function examples oracle provide unlimited source examples learning algorithm shown table 
initially examples oracle return members network training set 
point examples exhaust training set 
happened examples oracle finds examples randomly sampling instance space 
cases may desirable examples oracle return examples belong particular class 
done decompositional rule extraction approach hill climbing algorithm shown table 
algorithm randomly assigns value feature tests newly constructed example see member say positive class 
positive example random order imposed possible values features values considered order 
feature assigned value changed considered value doing increases total input output unit 
approach multilayer networks validity interval analysis case guarantee hill climbing find positive example 
section discuss proposed approach table examples oracle 
oracle operates single layer network returns positive examples 
create random example feature possible values vin randomly select vin calculate total input output unit return impose random order feature values consider values order value ij changing feature value ij increases ij return directed examples oracle general case 
note algorithm shown table employs stopping criterion determine set extracted rules provides sufficiently model network 
reasonable criteria 
example tuning set held aside estimate accuracy extracted rules 
alternatively patience criterion fahlman lebiere causes procedure quit certain number iterations resulted new rules 
experiments define stopping criterion show different methods accuracy extracted rule sets changes function amount computation 
important note fundamental difference search approach rule extraction learning approach way space rules explored 
approaches decompositional method validity interval analysis test hypothesized rules 
evaluating algorithm order evaluate approach empirically compare search method similar described section 
hypothesis learning approach requires computation search method get rule sets model networks comparable degree fidelity 
measure fidelity comparing classification performance rule set network extracted fidelity rule set fraction examples agrees network 
comparison examples examples oracle allowed access test sets see extracted rule sets learned concept descriptions generalize 
method applicable multilayer networks perceptrons experiments 
interest measuring relative efficiency exploring rule space approach versus approach single layer networks provide adequate milieu experiment 
evaluate approaches promoter domain towell 
promoters short sequences dna occur genes play critical role gene transcription 
feature domain represents position dna sequence feature takes values fa tg 
experiments available features subset contiguous features 
reduced subset small reasonable amount time run search method rules search space 
promoter dataset comprises examples half positive examples promoters 
train perceptrons different training set consisting total examples 
examples left training set measure fidelity extracted rule sets 
perceptrons features theta values input units output unit 
apply learning extraction method conventional search method trained perceptrons 
extract rules describe conditions output unit active predict sequence promoter 
search method conducts breadthfirst search space possible conjunctive rules 
graph shown organize rule space tree node search space visited 
order ensure extracted rules maximally general valid rule compared rules previous levels retained specific 
additionally employ optimization enables branches tree pruned rule tested rejected search determine sequence antecedents added rule valid 
sequence antecedents need explore descendents node 
computation involved determination essentially involved testing rule difference assuming undetermined features take values contribute minimally activating output unit assume take values contribute maximally 
computation required search algorithm quantified counting operations features nucleotides called region features thought important defining concept promoter 
ffl number nodes tested search ffl subsumption check comparisons previously extracted rules 
similarly computation required algorithm quantified counting operations ffl calls examples oracle ffl calls subset oracle ffl subsumption check comparisons individual terms dnf expression class calls examples oracle computational complexity operations number input features 
computational complexity calling examples oracle oracle produce examples class 
examples oracle produce positive examples time complexity log total number values input features 
experiments measure amount computation performed counting operations terms units 
visiting node search space making subsumption check counts unit 
calling examples oracle counts log units domain log 
operations indicate number times non constant time parts algorithm executed total count provides reasonable basis comparing learning algorithm search method 
shows test set fidelity extracted rule sets approaches averaged perceptrons 
analogous generalization curve conventional learning system 
axis indicates number operations examples calls subset calls comparisons learning method nodes visited comparisons method approaches note scale axis logarithmic 
axis denotes averaged fidelity rule sets measured test sets 
methods initially fidelity measure empty rule sets predict promoter class 
search approach requires fair amount time finds rules learning method immediately begins extracting rules fidelity quickly improves 
fact values axis method requires orders magnitude operations achieve fidelity 
learning method explores fewer nodes method extracted rule sets grow effort expended comparing training examples computational complexity subset calls may validity interval analysis multilayer networks 
operations learning method search method conjunctive rule set fidelity 
shows averaged fidelity extracted conjunctive rule sets number operations 
axis logarithmic scale shows number operations described text 
axis reports fidelity measured test sets extracted rules 
extracted rules see examples covered 
fidelity approaches increases monotonically acquired rules sound respect network 
observed level fidelity sizes rule sets extracted approaches comparable 
understand learning approach outperforms search extraction method problem domain helpful consider space rules explored 
search method explores space top learning approach explores space bottom 
effectiveness search approach determined depth needs explore space order find rules 
computational complexity search grows exponentially depth expensive search method find rules antecedents 
learning method hand able efficiently find rules antecedents 
positive training examples serve identify terminal nodes search space need covered rules 
terminal nodes initial rules learning method uses polynomialtime algorithm traverse upward search space maximally general rules 
disadvantage approach stochastically samples instance space may take long time find maximally general rules especially rules cover instances 
search approach hand methodically explores rule space may effective networks small number features result rules antecedents 
extracting rules section generalize algorithm section extract rules trained networks 
rules better suited describing neural networks conjunctive rules towell shavlik closely match inductive bias units neural network 
fewer rules conjunctive rules usually required describe network 
rules algorithm extracts consist conjunctions terms 
term satisfied antecedents satisfied 
example term fa cg logically equivalent 
rule satisfied terms satisfied 
table outlines algorithm extract rules trained networks 
manner algorithm table step learn conjunctive rule instance supplied examples oracle 
algorithm conjunction trivial rule set step generalize rule application operators ffl add value generalizes term adding feature value set antecedents 
ffl new term takes existing term splits terms form gamma gamma 
example fa cg fa bg fcg 
add value operator may add possible value feature antecedent set create internal disjunction michalski add feature represented 
new term operation able generalize term operator conjunction add value operator 
additionally new term may undesired effect specializing rule applied terms equal number features represented set antecedents disjunctions represented term internal disjunctions 
allow new term operator applied meets condition 
restriction place operator arbitrarily partition term gamma gamma possibilities term antecedents 
extracting rules perceptrons experiments able order antecedents magnitude weights require terms maintain order antecedents 
constraint reduces number possible partitions gamma 
table rule extraction algorithm 
class rc repeat examples classify covered rc learn conjunctive rule table trivially convert rule result applying add value new term subset additional operator applications stopping criterion met add value new term operators successively generalize rule generalizations result rule consistent network 
iteration loop possible operator applications selected applied rule 
subset oracle determine generalized rule consistent network 
loop continues operator applications tried rule generalized 
algorithm quite naive selects operator applications area investigate heuristics guiding selection 
conjunctive queries subset oracle implemented ways depending decompositional approach analysis 
decompositional approach addition calculating minimum contribution undetermined features subset oracle determine antecedents minimally satisfy term 
oracle wants determine unit active terms rule satisfied consider case term satisfied antecedents contribute activating unit 
subset oracle rules implemented validity interval analysis approach thrun described constraints incorporated linear programs test hypothesized rules 
evaluate version algorithm extract rules perceptrons experiment section 
count number operations adjust accounting calls subset reflect fact queries rules expensive handle queries conjunctive rules 
complexity answering query conjunctive rule rule set fidelity antecedents conjunctive antecedents rules conjunctive rules rule set sizes 
shows average number rules antecedents rule sets extracted conjunctive learning algorithms 
axis reports fidelity extracted rule sets axis indicates rule counts total number antecedents summed rules rule set 
number features complexity answering query term log find antecedents knuth 
calculate cost subset query experiment summing log term rule recall basic unit accounting operation 
primary advantage extracting rules rules typically concise conjunctive rules 
reports average number rules antecedents extracted conjunctive rule sets 
indicates smaller set rules able describe network level fidelity 
shows fidelity extracted rule sets averaged perceptrons 
curves conjunctive rules shown 
seen algorithm extracting rules initially efficient algorithm conjunctive rules asymptotically performance 
algorithm expends effort generalizing training examples requires fewer operations algorithm test example covered typically fewer rules test 
areas plan develop approach implementing directed examples oracle finding better intermediate terms extracted rules handling real valued features 
operations learning conjunctive rules learning rules search conjunctive rules rule set fidelity 
shows averaged fidelity rule sets number operations 
axis logarithmic scale shows number operations described text 
axis reports fidelity extracted rule sets 
shown fidelity curves conjunctive rules search method learning approach 
examples oracle selects training examples uniformly sampling instance space 
method inefficient examples oracle direct attention regions instance space covered rules 
plan develop evaluate examples oracle uses current set extracted rules influence sampling 
proposed approach previously extracted rules formulate propositional satisfiability sat problem 
solution sat problem example covered extracted rules 
general sat problem np hard hypothesize greedy local search algorithm gsat selman able efficiently find solutions cases uncovered examples 
uncovered examples difficult find examples oracle randomly generates training examples ineffective situations 
second area planned research investigate algorithms search useful intermediate terms rule extraction process 
intermediate terms aid rule set comprehensibility improving extracted descriptions better illustrating structure learned concepts 
decompositional approach rule extraction introduces intermediate terms corresponding hidden units network 
approach involves assumptions hidden unit network behaves boolean variable individual hidden units network correspond concepts meaningful context domain 
neural networks learn distributed rep hinton concept encoded activations hidden units hidden unit plays part representing different concepts 
representations right intermediate terms represent patterns activity groups hidden units individual hidden units 
order approach applicable problem domains plan extend handle real valued features 
researchers investigated issue extent gallant thrun plan incorporate techniques algorithm 
described novel approach casts problem extracting symbolic rules trained neural networks learning task 
approach exploits training examples queries learn concept descriptions accurately describe trained neural networks 
experiments demonstrate approach efficient conventional techniques 
described approach generalized extract rules 
believe approach provides promising advance goal readily interpreting trained neural networks 
wish rich maclin providing helpful comments earlier version nick street sebastian thrun interesting discussions regarding 
research partially supported doe de fg er onr nsf iri 
angluin 

queries concept learning 
machine learning 
craven shavlik 

learning symbolic rules artificial neural networks 
proceedings tenth international conference machine learning pp 
amherst ma 
morgan kaufmann 
fahlman lebiere 

learning architecture 
touretzky editor advances neural information processing systems volume 
morgan kaufmann san mateo ca 
fu 

rule learning searching adapted nets 
proceedings ninth national conference artificial intelligence pp 
anaheim ca 
aaai mit press 
gallant 

neural network learning expert systems 
mit press cambridge ma 
hinton 

learning distributed representations concepts 
proceedings eighth annual conference cognitive science society pp 
amherst ma 
erlbaum 
hunter klein 

finding relevant biomolecular features 
proceedings international conference intelligent systems molecular biology pp 
bethesda md aaai press 
knuth 

art computer programming volume 
addison wesley second edition 
lapedes barnes burks farber 

application neural networks machine learning algorithms dna sequence analysis 
bell marr editors computers dna volume vii 
addison wesley 
michalski 

theory methodology inductive learning 
artificial intelligence 
saito nakano 

medical diagnostic expert system pdp model 
proceedings ieee international conference neural networks pp 
san diego ca 
ieee press 
sejnowski rosenberg 

parallel networks learn pronounce english text 
complex systems 
selman levesque mitchell 

new method solving hard satisfiability problems 
proceedings tenth national conference artificial intelligence pp 
san jose ca 
aaai mit press 
thrun 

extracting provably correct rules artificial neural networks 
technical report iai tr institut ur informatik iii universit bonn 
towell shavlik 

extracting refined rules knowledge neural networks 
machine learning 
towell shavlik 
press 
knowledge artificial neural networks 
artificial intelligence 
towell shavlik noordewier 

refinement approximate domain theories knowledge neural networks 
proceedings eighth national conference artificial intelligence pp 
boston ma 
aaai mit press 
valiant 

theory learnable 
communications acm 
wolberg street mangasarian 
press 
machine learning techniques diagnose breast cancer image processed nuclear features fine needle 
cancer letters 
