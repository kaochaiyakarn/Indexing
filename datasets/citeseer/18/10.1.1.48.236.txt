classification pairwise proximity data graepel ralf herbrich peter klaus obermayer technical university berlin statistics research group 
fr neural information processing group 
fr 
berlin germany investigate problem learning classification task data represented terms pairwise proximities 
representation refer explicit feature representation data items general standard approach euclidean feature vectors pairwise proximities calculated 
approach combined linear embedding classification procedure resulting extension optimal hyperplane algorithm pseudo euclidean data 
alternative approach linear threshold model proximity values optimized structural risk minimization 
show prior knowledge problem incorporated choice distance measures examine different metrics generalization 
algorithms successfully applied protein structure data data cat cerebral cortex 
show better performance nearest neighbor classification 
areas pattern recognition machine learning neural computation common practice represent data feature vectors euclidean vector space 
kind representation convenient euclidean vector space offers powerful analytical tools data analysis available representations 
representation incorporates assumptions data may hold practitioner may aware 
severe restriction domain independent procedures construction features known 
general approach characterization set data items de fine proximity distance measure data items necessarily feature vectors provide learning algorithm proximity matrix set training data 
pairwise proximity measures defined structured objects graphs procedure provides bridge classical structural approaches pattern recognition 
additionally pairwise data occur frequently empirical sciences psychology psychophysics economics biochemistry algorithms developed kind data predominantly clustering multidimensional scaling fall realm unsupervised learning 
contrast nearest neighbor classification schemes suggest algorithms operate proximity data linear models 
brief discussion different kinds proximity data terms possible embeddings suggest optimal hyperplane algorithm classification applied distance data euclidean pseudo euclidean spaces :10.1.1.103.1189
subsequently general model introduced formulated linear threshold model proximities optimized principle structural risk minimization 
demonstrate choice proximity measure influences generalization behavior algorithm apply algorithms real world data biochemistry 
nature proximity data faced proximity data form matrix fp ij pairwise proximity values data items idea embed data suitable space visualization analysis 
referred multidimensional scaling torgerson suggested procedure linear embedding proximity data 
interpreting proximities euclidean distances unknown euclidean space calculate inner product matrix center mass data proximities ij gamma jp ij gamma jp mj gamma jp jp mn perform spectral decomposition choose columns sorted decreasing order magnitude eigenvalues embedding dimensional space achieved calculating rows order embed new data item characterized vector consisting pairwise proximities previously known data items calculates corresponding inner product vector ij ij replaced pm respectively obtains embedding gamma matrix negative eigenvalues distance data euclidean 
data isometrically embedded pseudo euclidean minkowski space 
gamma equipped bilinear form phi positive definite 
case distance measure takes form phi gamma gamma gamma theta symmetric matrix assumed full rank necessarily positive definite 
find basis matrix assumes form diag gammai gamma gamma pair gamma called signature space 
case serves reconstruct symmetric bilinear form embedding proceeds replaced diagonal contains modules eigenvalues eigenvalue spectrum effective dimensionality proximity preserving embedding obtained 
small number large positive eigenvalues data items reasonably embedded euclidean space 
ii small number positive negative eigenvalues large absolute value embedding pseudo euclidean space possible 
iii spectrum continuous relatively flat linear embedding possible gamma dimensions 
classification euclidean pseudo euclidean space training set theta matrix pairwise distances unknown data vectors euclidean space target class gamma data item 
assuming data linearly separable follow algorithm set linear model classification data space sign find weight vector threshold optimal hyperplane maximal margin minimizing kwk constraints 
equivalent maximizing wolfe dual ff ff ff ff gamma ff yx diag vector 
constraints ff 
optimal weight vector expressed linear combination training examples optimal threshold obtained evaluating gamma training example ff decision function fully evaluated inner products data vectors 
formulation allows learn distance data directly 
euclidean case apply distance matrix training data obtain inner product matrix introduce directly explicit embedding data wolfe dual 
true test phase inner products test vector training examples needed 
case pseudo euclidean distance data inner product matrix obtained distance matrix negative eigenvalues 
means corresponding data vectors embedded pseudo euclidean space gamma explained previous section 
serve hessian quadratic programming qp problem 
turns bilinear form pseudo euclidean spaces linear classification 
decision plane characterized equation mw illustrated fig 

fig 
shows plane just described space euclidean mw simply mirror image axes negative signature 
algorithm means reconstruct euclidean inner product matrix distance data proceed algorithm usual 
calculated flipping axes negative signature diag calculate du gamma mx phi phi phi phi phi hj delta delta delta delta delta delta delta delta delta delta delta delta mw plot decision line thick pseudo euclidean space signature diag gamma 
decision line described mw 
interpreted euclidean right angles mirror image axis gamma negative signature 
physics plot referred minkowski space time diagram corresponds space axis gamma time axis 
dashed diagonal lines indicate points mx zero length light cone 
serves hessian matrix normal classification 
note positive semi definite ensures unique solution qp problem 
learning linear decision function proximity space order cope general proximity data case iii section training set theta proximity matrix elements ij pairwise proximity values data items target class gamma data item 
assume proximity values satisfy reflexivity ii symmetry ij ji linear model classification new data item represented vector proximities proximities items training set sign comparing note equivalent vector proximities feature vector characterizing data item consequently algorithm previous section learn proximity model replaced replaced wolfe dual columns serve training data 
note formal correspondence imply columns proximity matrix euclidean feature vectors sv setting 
merely consider linear threshold model proximities data item training data items 
hessian qp problem square proximity matrix positive semi definite guarantees unique solution qp problem 
optimal coefficients ff test data item classified determining proximities elements training set conditions classification 
metric proximities consider examples order see learning pairwise metric data amounts 
example minimalistic metric objects defined follows ae decision functions simple class classification problem different minkowski metrics 
algorithm described sect 
applied city block metric euclidean metric maximum metric 
metrics result considerably different generalization behavior different support vectors circled 
corresponding theta proximity matrix full rank seen non vanishing determinant det gamma gamma gamma 
definition metric clear data item contained training set represented proximity vector assigned class 
metric qp problem solved analytically matrix inversion gamma gamma gamma gamma obtain classification sign gamma gamma gamma gamma delta sign gamma result means new data item assigned majority class training sample available information bayes optimal decision 
example demonstrates prior information case metric minimal information identity encoded chosen distance measure 
easy visualize example metric distance measures vectors consider minkowski metrics defined jx gamma minkowski metric equivalent euclidean distance 
case corresponds called city block metric distance sum absolute differences feature 
extreme maximum norm takes largest absolute difference feature values distance objects 
note increasing weight larger differences feature values literature multidimensional scaling minkowski metrics examine dominance features human perception 
minkowski metrics classification toy example observed different values lead different generalization behavior set data points seen fig 

apriori reason prefer metric particular metric equivalent incorporating prior knowledge solution problem 
cat cortex leave proteins fold ss fl ff fi gh size class cut flip axis proximity nn nn nn nn nn table classification results cat cortex protein data 
bold numbers indicate best results 
real world proximity data numerical experiments focused real world data sets terms proximity matrix class labels data item 
data set called cat cortex consists matrix connection strengths cortical areas cat 
data collected text figures available anatomical literature connections assigned proximity values follows self connection strong dense connection intermediate connection weak connection absent connection 
functional considerations areas assigned different regions auditory visual somatosensory ss fl 
classification task discriminate regions time 
second data set consists proximity matrix structural comparison protein sequences concept evolutionary distance 
majority proteins assigned classes globins hemoglobin ff ff hemoglobin fi fi heterogenous globins gh 
classification task assign proteins classes rest 
compared different procedures described class classification problems performing leave cross validation cat cortex dataset fold cross validation protein data set estimate generalization error 
table shows results 
cut refers simple method making inner product matrix positive semi definite neglecting projections eigenvectors negative eigenvalues 
flip axis flips axes negative signature described preserves information contained directions classification 
proximity refers model linear proximities introduced section 
seen proximity shows better generalization flip axis turn performs slightly better cut 
especially case cat cortex data set inner product matrix negative eigenvalues 
comparison lower part table shows corresponding cross validation results nearest neighbor natural choice needs pairwise proximities determine training data participate voting 
algorithms flip axis proximity perform consistently better nearest neighbor value optimally chosen 
contribution investigated nature proximity data suggested ways performing classification 
due generality proximity approach expect problems fruitfully cast framework 
focused classification problems regression considered proximity data analogous way 
noting support vector kernels covariance functions gaussian processes similarity measures vector spaces see approach gained lot popularity 
problem pairwise proximities number scales quadratically number objects consideration 
large scale practical applications problems missing data active data selection proximity data increasing importance 
acknowledgments prof fruitful discussions 
gunn providing support vector implementation 
indebted hofmann providing protein data set 
project funded technical university berlin fip 
borg 
multidimensional similarity structure analysis volume springer series statistics 
springer verlag berlin heidelberg 
boser guyon vapnik 
training algorithm optimal margin classifiers 
proceedings fifth annual workshop computational learning theory pages 
goldfarb 
progress pattern recognition volume chapter new approach pattern recognition pages 
elsevier science publishers 
graepel obermayer 
stochastic self organizing map proximity data 
neural computation accepted publication 
hofmann buhmann 
pairwise data clustering deterministic annealing 
ieee transactions pattern analysis machine intelligence 
buhmann 
multidimensional scaling deterministic annealing 
pelillo hancock editors energy minimization methods computer vision pattern recognition volume pages berlin heidelberg 
springer verlag 
young 
analysis connectivity cat cerebral cortex 
journal neuroscience 
torgerson 
theory methods scaling 
wiley new york 
vapnik 
nature statistical learning 
springer verlag berlin heidelberg germany 
weinshall jacobs gdalyahu 
classification non metric space 
advances neural information processing systems volume 
press 
