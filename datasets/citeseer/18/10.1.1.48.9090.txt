data cache parameter measurements li clark thomborson computer science department university auckland private bag auckland new zealand cs auckland ac nz extend prior research saavedra smith designing microbenchmarks measure cache performance 
saavedra smith characterise read accesses separately write accesses determine cache allocates write detect write back write policies assume address mapping function bit selection 
experimental results cpu cache structures mhz pentium mmx mhz pentium pro 

computer performance increasingly limited memory speed processor speed graphically illustrated page 
richard sites coming decade memory subsystem design important design issue microprocessors 
support opinion sites described results database study tpc benchmark mhz pentium pro mhz alpha systems measured cpu cycles instruction retired 
cpu cycles systems spent waiting memory 
cpus ran peak instruction issue rate 
computer architects people concerned memory systems performance 
memory bottlenecks increase frequency severity performance programmers algorithmic designers pay attention memory design 
problems involving small datasets performance primary data cache crucial consideration 
analyses experimental results cache performance guide appropriate selection design computer codes 
hand tuning necessary achieve optimal performance highly parallel architectures 
hand tuning expensive resulting executables portable 
reasons executables prepared little hand tuning optimising compiler fixed cost function machine performance model 
microbenchmarks choose appropriate parameters cache portion machine performance model guiding compiler optimisations memory operations inner loop computations consume vast majority time applications 
performance primary cache strong current interest computer architects cache forms important layer memory hierarchy modern system design 
particular cache provide extremely rapid access temporally local data nearby spatially local data 
microbenchmarks analyses give insight performance operation caches 
example show cache associativity affects random access time variously sized datasets 
show write series microbenchmarks conjunction analyses estimate capacity associativity blocksize time required cache hits read write accesses 
microbenchmarks similar inspired proposed saavedra smith 
benchmarks differentiate read write access times reporting average access time mix reads writes done saavedra smith 
experiments find read time quite dissimilar write time understand ratio reads writes typical codes 
note smith von proposed microbenchmarks parallel computers separately estimate read write access times 
microbenchmarks estimate cache performance parameters 
provide additional microbenchmarks determine presence absence cache policies commonly known allocate write 
analyses indicate microbenchmarks deliver correct results wide variety cache architectures 
particular saavedra smith assume particular bit selection function compute set index cache 
assume set index mapping function cache associativity replacement logic provide conflict free access aligned array size exceed total capacity cache 
example microbenchmarks applicable caching systems random ee xor set indexing functions 
smith discover advantage random functions study particular set indexing function shown great advantage codes significant disadvantage 
prudent design microbenchmark gives correct results bit selection function employed 
date tested accuracy microbenchmarks workstations different cache structures 
workstation cpu chip mhz pentium pro workstation mhz pentium mmx 
experimental results agreement manufacturer representations cpu chip primary cache capacity associativity block size write policy 

assumptions notation order estimate cache parameters assumptions computer system cache part 
particular assumptions way computer execute timed loop shown 
suitable timing routine clock available 
routine report wall clock time accurately documented precision system running computational tasks foreground 

background tasks running behalf operating system introduce systematic bias timing measurements example synchronising background task activity calls timing routine 

memory byte 

memory strides measured bytes 

length double bytes 

length int bytes 
adjust assumption port code platforms 

compiler replace loops straight line code 
compiler place frequently scalar operands data registers instructions cached separately data memory accesses inner loops array 

sufficient cpu resources available average loop time calculated kernel reflect average time required stride read memory example average time instruction fetch indexing calculation 

starting address array aligned memory 
dye bits address zero base logarithm size bytes 
double calloc sizeof double double stride double int int double start time clock stride read double start time clock start time clock timing baseline double start time clock double clocks sec return ensure computed 
timed kernel measurement cache size stride accesses 
number accesses 
read time stride accesses pc 
additional assumptions structure cache 

block line size cache power 
measure bytes 

block offset function bit selection function sensitive significant bits address gamma bitwise function 
typical mechanism cache optimises spatially local accesses 

cache associativity course 
positive integer 

sets cache set consists blocks bytes 
assume power 

total capacity cache measured bytes lab 
note implies 
note divides necessarily power 

set index mapping function necessarily bit selection function 
elements aligned array size cache resident simultaneously implying function maps aligned address ranges lb bytes uniformly exactly address block bytes mapped sets cache 
may expressed functional composition permutation lb usual bit selection function lb gamma number accesses 
read time stride accesses pc 
int sizeof double start time clock stride read ts double start time clock ts double 
kernel measurement cache block size variable stride reads 

note permutation lb may identity map case usual bit selection function employed 
examples non trivial lb proposed 

cache read access cause referenced block data brought cache displacing old block data lru fifo replacement algorithm 

cache write access cause referenced block data brought cache primary cache allocate write policy 

cache hit write access primary cache causes write access deeper layer memory hierarchy secondary cache primary memory secondary memory primary cache write policy 
access stride bytes 
read time strided accesses pc 

time required cache read loop shown larger time required cache hit read 

time required cache access write loop shown larger time required cache hit write cache employ write policy 
cache employ policy time required write access primary cache significantly affected data primary cache 

secondary cache disabled desired increase accuracy estimation cache parameters penalty 
note disabled secondary cache experiments pentium processors described 
addition parameters variables defined microbenchmarks variables defined item 
estimate parameters defined items 
total distinct elements referenced array holding total elements element byte double 
cases power 

average cache read resp 
write hit access time particular loop denoted resp 
measured nanoseconds 
note microbenchmark estimates cache computations loop access stride bytes 
read time strided accesses pc 

write somewhat larger latency influenced estimates arising loop 
average cache read resp 
write penalty denoted pr resp 
pw measured nanoseconds bandwidth loops 
write pr pw somewhat larger latency influenced estimates penalty arising loop 
note experiments pentium disabled secondary cache measured penalties reflection access bandwidths latencies primary memory dram bandwidths latencies secondary cache 

rate reads resp 
writes resp 
mw mw 
average time read resp 
write pr resp 
mw pw 

measurement cache read parameters section describe series microbenchmarks characterise cache performance parameters read accesses 
microbenchmarks similar ones described saavedra smith inner loops perform memory read memory read write 

capacity array larger cache cache resident reading assumptions 
loop iterations loop nest rate regime analysis saavedra smith 
arrays large cache resident cache inner loop iterations 
rate lb regime 
lb rate gamma narrow interval named saavedra smith 
call transitional regime 
suitable microbenchmark capacity easily described 
increase measured average stride increases sharply regime baseline reach regime plateau pr set value sufficiently large pr ticks clock measurement 
assumption pr choose appropriately large making initial estimate measuring 
cache size largest 
see figures experimental results workstations 
data figures see pc cache capacity bytes lies range capacity pc lies range ranges narrowed desired testing capacities powers 
note addition estimating capacity stride benchmark estimates quotient pr 
block size vary stride array accesses estimate block size bytes 
suitable inner loop shown 
lb rate regime 
lb rate regime 
set microbenchmark test noting lb cache 
reported average stride access time code ts minf bgt pr estimated blocksize numerically equal smallest value ts ts 
note assumed power 
estimate pr equal ts gamma recall estimated capacity microbenchmark described earlier 
experimental results running ts workstations shown figures 
note cases ts ts ts 
conclude primary caches byte blocks 
conclude pc uses sub block allocation sub block length bytes stride reads relatively efficient comparison stride reads 
contrast pc takes marginally time stride read stride read difference reflecting greater number cache tag updates cache data memory read case 
knuth vol 
int pra define double pra int pra pra 
constructing random permutation time 
start time clock pra tr double start time clock tr double 
kernel measurement cache associativity random accesses repetition 

associativity saavedra smith variable stride benchmark similar estimate associativity number random read accesses read ratio test results 
read ratio pc 
cache assumption bit selection function employed set indexing 
equivalent assuming function lb identity map notation set index function 
microbenchmark estimating associativity measurement insensitive lb code construct pseudo random permutation array indices gamma byte elements load block offset zero primary cache 
construction requires source pseudorandom variates uniformly independently distributed range gamma 
suitable language code readily available 
elements permutation define length sequence array elements accessed 
note access element iterations timing loop 
call mode access random repetition distinguish usual benchmark elements chosen access repetition index space array uniform independent fashion 
believe benchmark random access repetition yield accurate estimate associativity rate analogue theorem zero small case 
aware published analyses caches accessed randomly repetition 
combinatorics difficult indicated theorems proof 
theorem urn contains marbles exactly black choose marbles blindly probability choosing exactly black marbles gamma gamma ffi corollary byte elements chosen randomly repetition reading aligned array byte elements power greater number sets primary cache probability exactly elements competing single line primary cache theorem distinct data elements read accessed sequence cache associativity sequence read accesses repeated total times elements competing block offset zero single cache line ratio read sequences ae number random read accesses read ratio test results 
read ratio pc 
corollary rw lb expected read ratio timed portion program rw kp proof 
consider sequential accesses array pra addition random accesses array 
total size accessed region pra rw bytes 
rw lb lb accesses pra touch sets cache times 
cache lru fifo replacement policy cause read fo access element block elements pra total rw read misses sequential accesses 
term numerator add expected number misses cache set referenced random accesses 
accesses cache effectively flushed elements iteration sequential accesses pra 
sets consider contributes kp misses 
ratio total number misses divided total number 
corollary rw lb gamma rw ka gamma rw kp proof 
referenced portion array pra occupies block rw sets cache 
elements array pra displaced cache lines hit random accesses 
lines misses iteration random access plus reload pra block 
remaining gamma rw lines cache hold elements pra misses iteration corollary lb rw ka bl rw bl proof 
elements pra occupy approximately rw bl blocks sets cache 
cache sets accessed gamma rw bl random contribute misses 
cache sets accessed frequently contribute approximately rw bl misses 
corollary rw lb linear interpolation equations 
double calloc fills cache write allocate 
start time clock write tw double start time clock tw double 
pre write kernel determining cache blocks allocated write estimate associativity cache making series measurements tr tr tr tr 
estimate ratio measurement tr follows tr gamma pr take tr pr tr gamma find gives best fit analytic form function series estimates 
best fit estimate associativity cache 
note average read hit access time microbenchmark may somewhat larger estimated microbenchmark 
operations loop index updates overlapped typical optimised code read accesses microbenchmark multiple array read accesses may progress simultaneously 
current microbenchmark th random read proceed th sequential read pra delivered result 
believe typical optimising compiler unroll reschedule loop avoid latency bottleneck dependency associativity testing microbenchmark surprised find experimentation 
read penalty pr microbenchmark substantially larger pr estimated microbenchmark reason 
recall disabled secondary cache 
random read microbenchmark usually incurs latency full ras cas dram cycle sequential read microbenchmark may proceed full bandwidth fast cas dram cy number accesses stride write time 
average write time pc microbenchmark 
cle 
experimental results associativity testing workstations shown figures fit pc pc agreement manufacturer description 

measurement cache write policies written microbenchmarks determine cache write policies estimate write times 
cache write microbenchmark shown designed determine write allocation policy employed 
third loops microbenchmark timed series stride writes array may may cache resident 
previous loops establish cache residency array small fit cache cache write allocation policy 
call microbenchmark pre write kernel write loop precedes timed series array writes 
time write tw reported pre write kernel tw wp constant cache allocate write 
cache allocate write tw dependent rising noticeably referenced portion array cache resident 
assumptions cache behaviour tw lb tw wp array block assumed fifo lru replacement cache lines 
number accesses stride write time nanoseconds 
average write time pc microbenchmark 
read allocate start time clock write tw double start time clock tw double 
pre read kernel determining cache writeback policy 
experimental results tw workstations shown figures 
conclude pc allocate write average write time essentially constant 
average write time pc hand changes markedly indicative write allocate cache capacity kilobytes 
pre read kernel second microbenchmark writes shown 
microbenchmark designed reveal primary cache uses policy 
name kernel taken loops reads 
read establish cache residency minfc rg elements read 
cache uses write policy aver number accesses 
average write time pc microbenchmark 
age write time reported pre read microbenchmark tw essentially invariant cache write policy normally called write back cache 
write back caches average write time reported pre read microbenchmark noticeably dependent reasoning follows array cache resident misses 
case write allocate cache ratio mw benchmark mw minf gamma lg writes don allocate referenced elements cache resident ratio mw gamma 
note background tasks may slowly pollute cache displacing elements know lower bound ratio case 
mw defined previous paragraph equation summarises analysis performance write back cache pre read benchmark 
tw ae wt mw wp wt experimental results pc pc benchmark shown figures 
determined pc employ write allocation expect see fairly gradual rise tw 
rise indicates pc employ write 
cache policies write allocate write commonly termed write back clearly pc indicated step function rise tw near 
written third microbenchmark write accesses evaluate dependence write access time number accesses stride write time nanoseconds 
average write time pc microbenchmark 
stride 
pre read pre write microbenchmark 
kernel code shown similar strided read microbenchmark 
estimate wp results third microbenchmark estimate small results pre read microbenchmark 
experimental results workstations strided write microbenchmark figures 
note pc stride writes particularly efficient comparison stride writes indicating entire cache block just sub block allocated cache write 

summary described series microbenchmarks measure important performance parameters primary cache 
microbenchmarks similar spirit saavedra smith 
microbenchmarks characterise read write times separately determine cache write policies caches set index functions bit selection function commonplace today 
experimental results obtained running microbenchmarks workstations summarised table 
measured capacity blocksize associativity agreement manufacturer description access times plausible systems secondary cache disabled 
analyse performance skewed caches microbenchmarks 
access stride bytes write access time nanoseconds 
average stride write time pc 
caches employ distinct permutations lb define set index function bank believe skewed caches establish complete residency aligned subarrays size single read 
believe skewed caches iterations random sequence repetition behave essentially high associativity approaching 
believe possible principle single kernel variable permutations microbenchmarks measure read accesses 
kernel memory latency iteration start prior iteration read complete 
interested similar latency bound microbenchmarks writes 
intend release microbenchmarks public distribution soon 
hope find time funding porting microbenchmarks unix linux 
bacon graham sharp 
compiler transformations high performance computing 
acm computing surveys december 

skewed associativity improves program performance enhances predictability 
ieee trans 
computers may 

study storage schemes highperformance computer systems 
phd thesis institute computer technology chinese academy sciences 
intel developer intel com design product htm lam rothberg wolf 
cache performance optimizations blocked algorithms 
access stride bytes write access time nanoseconds 
average stride write time pc 
proceedings fourth international conference architectural support programming languages operating systems pages apr 
lebeck wood 
cache profiling spec benchmarks case study 
ieee computer pages october 
patterson hennessy 
computer organization design hardware software interface 
morgan kaufmann second edition 
saavedra smith 
measuring cache tlb performance effect benchmark runtimes 
ieee trans 
computers october 
sites 
memory stupid 
microprocessor report august 
smith 
comparative study set associative memory mapping algorithms cache main memory 
ieee trans 
software engineering se march 
smith 
cache memories 
computer surveys september 
thomborson 
tools randomized experimentation 
lock editors statistical applications expanding computer capabilities proceedings th symposium interface computing science statistics volume pages 
interface foundation north america 
code available www cs auckland ac nz von smith 
performance prediction parallel computers 
technical report ucb csd computer science division eecs university california berkeley may 
name pc pc cpu pentium mmx speed mhz mhz capacity kb kb line size associativity read hit time ns ns pen 
rp ns ns write hit time ns ns pen 
wp ns ns allocate write 
write 
table 
summary experimental results primary caches pentium systems secondary cache disabled 
