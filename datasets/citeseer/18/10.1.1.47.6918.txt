gambling casino adversarial multi armed bandit problem peter auer institute theoretical computer science university technology graz graz austria igi tu graz ac nicol cesa bianchi department computer science universita di milano milano italy dsi unimi yoav freund robert schapire labs park avenue florham park nj research att com internal report dsi universit di milano italy multi armed bandit problem gambler decide arm non identical slot machines play sequence trials maximize reward 
classical problem received attention simple model provides trade exploration trying arm find best exploitation playing arm believed give best payoff 
past solutions bandit problem relied assumptions statistics slot machines 
statistical assumptions whatsoever nature process generating payoffs slot machines 
give solution bandit problem adversary behaved stochastic process complete control payoffs 
sequence plays prove expected round payoff algorithm approaches best arm rate gamma give improved rate convergence best arm fairly low payoff 
prove general matching lower bound best possible performance algorithm setting 
addition consider setting player team experts advising arm play give strategy guarantee expected payoff close best expert 
apply result problem learning play unknown repeated matrix game powerful adversary 
early extended appeared proceedings th annual symposium foundations computer science pages 
draft substantially revised expanded version submitted journal publication 
studied multi armed bandit problem originally proposed robbins gambler choose slot machines play 
time step pulls arm machines receives reward payoff possibly zero negative 
gambler purpose maximize total reward sequence trials 
arm assumed different distribution rewards goal find arm best expected return early possible keep gambling arm 
problem classical example trade exploration exploitation 
hand gambler plays exclusively machine thinks best exploitation may fail discover arms higher average return 
hand spends time trying machines gathering statistics exploration may fail play best arm get high total return 
practically motivated example consider task repeatedly choosing route transmitting packets points communication network 
suppose possible routes transmission cost reported back sender 
problem seen selecting route packet total cost transmitting large set packets larger cost incurred sending single best route 
past bandit problem studied aid statistical assumptions process generating rewards arm 
gambling example instance natural assume distribution rewards arm gaussian time invariant 
costs associated route routing example modeled stationary distribution sophisticated set statistical assumptions required 
general may difficult impossible determine right statistical assumptions domain domains may inherently adversarial nature assumptions appropriate 
variant bandit problem statistical assumptions generation rewards 
model reward associated arm determined time step adversary unbounded computational power benign stochastic process 
assume rewards chosen bounded range 
performance player measured terms regret expected difference total reward scored player total reward scored best arm 
may impossible player stand chance powerful opponent 
deterministic player fare badly adversary assigns low payoff chosen arm high payoff arms 
efficient randomized player algorithm performs adversary 
prove difference expected gain algorithm expected gain best arms tk log number time steps 
note average time step regret approaches zero rate 
refined bounds dependence replaced total reward best arm assumed upper bound thereof 
worst case bounds may appear weaker bounds proved statistical assumptions shown lai robbins form log 
comparing results statistics literature important point important difference asymptotic quantification 
lai robbins assumption distribution rewards associated arm fixed total number iterations increases infinity 
contrast bounds hold finite generality model bounds applicable payoffs randomly adversarially chosen manner depend quantification order adversarial nature framework cause apparent gap 
prove point showing algorithm arm bandit problem exists set reward distributions expected regret algorithm playing arms iterations lower bounded omega gamma kt 
show sequence regret behaved 
precisely show algorithm guarantee actual expected difference gain gain best arm run upper bounded ln high probability 
bound weaker bound expected regret 
clear bound improved dependence number trials 
non stochastic bandit problem considered gittins varaiya 
version bandit problem different assume player compute ahead time exactly payoffs received arm problem optimization exploration exploitation 
algorithm part algorithm freund schapire turn variant littlestone warmuth weighted majority algorithm vovk aggregating strategies 
setting analyzed freund schapire call full information game player trial scores reward chosen arm gains access rewards associated arms just chosen 
situations picking action trials best strategy 
example packet routing problem single route duration message switching routes time time yield better performance 
give variant algorithm combines choices strategies experts recommends actions iteration 
show regret respect best strategy tk ln 
note dependence number strategies logarithmic bound quite reasonable player combining large number strategies 
adversarial bandit problem closely related problem learning play unknown repeated matrix game 
setting player prior knowledge game matrix playing game repeatedly adversary complete knowledge game unbounded computational power 
known matrix games associated value best possible expected payoff playing game adversary 
matrix known randomized strategy achieves value game computed say linear programming algorithm employed player 
case matrix entirely unknown previously considered megiddo proposed different strategies round payoff converges game value 
algorithms extremely inefficient 
problem show algorithm player achieves expected round payoff rounds efficiently approaches value game rate gamma 
convergence faster achieved megiddo 
organized follows 
section give formal definition problem 
section describe freund schapire algorithm full information game state performance 
section describe basic algorithm partial information game prove bound expected regret 
section prove bound regret algorithm typical sequences 
section show adaptively tune parameters algorithm prior knowledge length game available 
section give lower bound regret suffered algorithm partial information game 
section show modify algorithm expert advice 
section describe application algorithm repeated matrix games 
notation terminology formalize bandit problem game player choosing actions adversary choosing rewards associated action 
game parameterized number possible actions action denoted integer assume rewards belong unit interval 
generalization rewards arbitrary straightforward 
game played sequence trials distinguish variants partial information game captures adversarial multi armed bandit problem full information game essentially equivalent framework studied freund schapire 
trial full information game 
adversary selects vector current rewards 
ith component interpreted reward associated action trial 
knowledge adversary choice player chooses action picking number kg scores corresponding reward 

player observes entire vector current rewards 
partial information game corresponds description full information game step replaced player observes reward chosen action ga total reward player choosing actions formally define adversary deterministic function mapping past history play gamma current reward vector 
special case say adversary oblivious independent player actions reward trial function 
results proved adversary hold oblivious adversary 
player algorithms randomized fixing adversary player algorithm defines probability distribution set kg sequences actions 
probabilities expectations considered respect distribution 
oblivious adversary rewards fixed quantities respect distribution adversary reward random variable defined set kg gamma player actions trial gamma 
explicit notation represent dependence refer text appropriate 
measure performance algorithm regret difference total reward algorithm ga total reward best action 
shall concerned expected regret algorithm 
formally define expected total reward algorithm ga expected total reward best action max max jk expected regret algorithm ra max gamma ga definition easiest interpret oblivious adversary case max truly measures gained best loss generality assuming adversary deterministic 
see assume adversary maps past histories distributions values 
defines stochastic strategy adversary step game equivalent distribution deterministic adversarial strategies step game 
assume player algorithm worst case stochastic strategy adversary playing stated equivalence implies deterministic adversarial strategy gain large gain 
argument easily measures performance regret defined shortly 
algorithm hedge parameter real number 
initialization set repeat game ends 
choose action distribution exp jg gamma exp jg gamma 
receive reward vector score gain 

set gamma algorithm hedge full information game 
action played entire sequence 
adversary definition regret bit strange compares total reward algorithm sum rewards associated action iterations action taken rewards chosen adversary different generated variable depends past history plays gamma definition ra looks difficult interpret case section prove bounds regret adversary derive interesting result context repeated matrix games 
shall give bound holds high probability actual regret algorithm actual difference gain algorithm gain best action max gamma ga full information game section describe algorithm called hedge full information game building block design algorithm partial information game 
version hedge variant algorithm introduced freund schapire direct generalization littlestone warmuth weighted majority algorithm 
hedge described 
main idea simply choose action time probability proportional exp jg gamma ff parameter total reward scored action trial actions yielding high rewards quickly gain high probability chosen 
allow rewards larger proving bounds hedge complex freund schapire original algorithm 
extension freund schapire theorem 
modifications enable hedge handle gains rewards losses rewards gamma 
note allow rewards larger 
changes necessary hedge building block partial information game 
function phi defined phi mx gamma gamma mx theorem sequence reward vectors probability vectors computed hedge satisfy gamma ln gamma phi actions special case replace upper bound get lower bound gain hedge 
corollary sequence reward vectors probability vectors computed hedge satisfy delta max gamma ln gamma note delta gamma corollary immediately implies lower bound hedge delta je max gamma ln gamma max gamma ln gamma particular shown choose ln ln hedge suffers regret ln full information game hedge max gamma ln prove theorem inequality 
lemma jx jx phi proof 
suffices show function gamma gamma nondecreasing inequality jx jm immediately implies lemma 
continuous continuous derivatives defining need show derivative sufficient show gamma gamma nonnegative positive nonpositive negative proved noting derivative gamma obviously nonnegative 
proof theorem 
exp jg gamma 
definition algorithm find exp jg gamma exp jx exp jx phi lemma 
logarithms summing yields ln ln ln phi phi observing exp jg get ln jg gamma ln combining equations rearranging obtain statement theorem 
partial information game section move analysis partial information game 
algorithm exp runs algorithm hedge section subroutine 
exp stands exponential weight algorithm exploration exploitation algorithm described 
trial exp receives distribution vector hedge selects action distribution mixture uniform distribution 
intuitively mixing uniform distribution done sure algorithm tries actions gets estimates rewards 
algorithm action initial rewards observes action low large rewards occur observed action selected 
exp receives reward associated chosen action generates simulated reward vector hedge 
hedge requires full information components vector filled algorithm exp parameters reals fl initialization initialize hedge 
repeat game ends 
get distribution hedge 

select action probability gamma fl fl 
receive reward 

feed simulated reward vector back hedge 
algorithm exp partial information game 
actions selected 
chosen action set simulated reward 
dividing actual gain probability action chosen compensates reward actions chosen 
actions receive simulated reward zero 
choice simulated rewards guarantees expected simulated gain associated fixed action equal actual gain action gamma 
give main theorem bounds regret algorithm exp 
theorem fl expected gain algorithm exp exp max gamma fl phi fl max gamma gamma fl ln understand theorem helpful consider simpler bound obtained appropriate choice parameters fl corollary assume max algorithm exp run input parameters fl fl min ln gamma expected regret algorithm exp exp gamma gk ln gk ln proof 
ln gamma bound trivial expected regret theorem expected regret fl phi fl ln gamma gk ln apply corollary necessary upper bound max available tuning fl 
example number trials known advance action payoff greater trial upper bound 
section give technique require prior knowledge upper bound 
rewards range exp rewards translated rescaled range 
applying corollary gives bound gamma gamma tk ln regret 
instance applicable standard loss model rewards fall range gamma 
proof theorem 
definition algorithm fl 
find theorem actions gamma ln gamma phi fl gamma fl gamma fl get actions exp gamma fl gamma gamma fl ln gamma phi fl note take expectation equation respect distribution hi expected value gamma gamma gamma delta gamma delta combining equations find exp gamma fl gamma gamma fl ln gamma phi fl max max max obtain inequality statement theorem 
bound regret holds high probability section showed algorithm exp appropriately set parameters guarantee expected regret gk ln 
case adversarial strategy oblivious rewards associated action chosen regard player past actions compare expected gain player max case actual gain best action 
adversary oblivious notion expected regret weak 
consider example benign adversary assigns reward actions round rounds assigns reward action whichever action played player round actions 
case assuming player chooses action uniformly random algorithms considered expected total gain action gamma means bound get corollary case guarantee expected gain algorithm smaller max gamma weak guarantee run action actual gain gamma 
hand exp clearly perform better promised simple case 
clearly need bound relates player gain actual gain best action run 
section prove bound exp 
specifically define random variable actual total gain action max max actual total gain best action main result section proof bound holds high probability relating player actual gain exp max show dependence difference max gamma exp function high probability appropriate setting exp parameters 
dependence sufficient show average trial gain algorithm approaches best action 
dependence significantly worse dependence bound expected regret proved theorem 
open question gap bounds real closed algorithm 
notational convenience define random variables max max heart proof result section upper bound holds high probability deviation action main difficulty proving bound gains associated single action different trials independent may dependent decisions adversary 
martingale theory prove lemma lemma ffi 
probability gamma ffi action gamma phi fl gamma ln ffi proof 
appendix lemma prove main result section theorem fl ffi 
probability gamma ffi gain algorithm exp exp max gamma fl phi fl phi fl max gamma gamma fl ln gamma ln ffi proof 
note max combining equation gives exp max gamma fl gamma gamma fl ln gamma phi fl max gamma fl gamma phi fl max gamma gamma fl ln gamma fl gamma phi fl gamma gamma fl ln apply lemma implies probability gamma ffi actions exp gamma fl gamma phi fl gamma phi fl gamma ln ffi gamma gamma fl ln gamma fl phi fl phi fl gamma ln ffi gamma gamma fl ln choosing best action gives result 
interpret result give simple corollary 
corollary ffi 
assume max algorithm exp run input parameters fl fl min ln ffi gamma 
probability gamma ffi regret algorithm exp exp ln ffi ln ffi proof 
assume gamma ln ffi bound follows trivial fact regret apply theorem setting ln ffi algorithm exp initialization ln gamma repeat game ends 

restart exp choosing fl corollary fl fl gammar fl 
max gamma fl distribution random action chosen exp 
compute observed reward 
gamma 
algorithm exp partial information game bound max known 
assumed lower bound implies phi plugging bound theorem implies bound regret ln ln ffi bg ln ffi result follows upper bounding ln term ln ffi gb assumed lower bound upper bound holds sequence get dependence regret algorithm 
guessing maximal reward section showed algorithm exp yields regret gk ln upper bound total expected reward max best action known advance 
section describe algorithm exp require prior knowledge bound max regret max ln 
lines bounds corollary achieved prior knowledge max algorithm exp described proceeds epochs epoch consists sequence trials 
index epochs 
epoch algorithm guesses bound total reward best action 
uses guess tune parameters fl exp restarting exp epoch 
usual denote current time step 
note general may differ local variable exp regard subroutine 
section refer total number trials 
exp maintains estimate total reward action estimate unbiased sense estimates algorithm detects approximately actual gain action advanced happens algorithm goes epoch restarting exp larger bound maximal gain 
performance algorithm characterized theorem main result section 
theorem regret suffered algorithm exp exp gamma max ln gamma ln max ln ln proof theorem divided lemmas 
bounds regret suffered epoch second bounds total number epochs 
usual denote total number time steps final value 
define random variables total number epochs final value 
denote time steps completed epoch convenience define tr 
epoch consists trials note degenerate cases epochs may empty case 
max max max max 
lemma action epoch gain exp epoch lower bounded tr sr tr sr gamma gamma ln proof 
trials occur epoch lemma holds trivially summations equal zero 
assume fl fl equation proof theorem tr sr gamma fl tr sr gamma gamma fl ln gamma phi fl tr sr tr sr gamma fl tr sr gamma phi fl tr sr gamma gamma fl ln tr sr gamma fl tr gamma phi fl tr gamma gamma fl ln definition termination condition know gamma gamma fl 
fl exp choice implies tr sr tr sr gamma fl phi fl gamma gamma fl ln choices fl get statement lemma 
lemma gives implicit upper bound number epochs lemma number epochs satisfies gamma max proof 
bound holds trivially 
assume 
gamma epoch gamma completed termination condition max max gamma gamma gamma fl gamma gamma gamma gamma cz gamma kz suppose claim lemma false 
max function cx gamma kx increasing implies cz gamma kz max gamma max max max contradicting equation 
proof theorem 
lemmas exp tr sr max tr sr gamma gamma ln max gamma ln max gamma ln gamma max ln gamma ln max max gamma ln gamma gamma gamma gamma max ln lemma inequality lemma second inequality 
steps follow definitions simple algebra 
gamma gamma gamma ln ln gamma expectations sides equation gives exp max second derivative positive convex jensen inequality max max note max max max max max function increasing 
max max max 
combined equations gives exp max equivalent statement theorem 
hand max nonincreasing max gammab exp theorem follows trivially case 
lower bound section state information theoretic lower bound regret player lower bound holds player unbounded computational power 
precisely show exists adversarial strategy choosing rewards expected regret player algorithm omega gamma tk 
observe match upper bound algorithms exp exp see corollary theorem open problem close gap 
adversarial strategy proof oblivious algorithm simply assigns rewards random distribution similar standard statistical model bandit problem 
choice distribution depends number actions number iterations dependence distribution reason lower bound contradict upper bounds form log appear statistics literature 
distribution rewards fixed 
full information game matching upper lower bounds form theta gammap log delta known 
lower bound shows partial information game dependence number actions increases considerably 
specifically lower bound implies upper bound possible form ff log fi ff fi 
theorem number actions number iterations exists distribution rewards assigned different actions expected regret algorithm minf kt tg proof appendix lower bound expected regret implies course algorithm particular choice rewards cause regret larger expected value 
combining advice experts point considered bandit problem player goal achieve payoff close best single action 
general setting player may set strategies choosing best action 
strategies select different actions different iterations 
strategies computations performed player external advice player experts 
general term expert borrowed cesa bianchi place restrictions generation advice 
player goal case combine advice experts way total reward close best expert best single action 
example consider packet routing problem 
case routing strategies different assumptions regarding network load distribution different data estimate current load 
strategies suggest different routes different times better different situations 
case algorithm combining strategies set packets performs strategy best set 
formally trial assume player prior choosing action provided set probability vectors 
interpret advice expert trial ith component represents recommended probability playing action 
special case distribution concentrated single action represents deterministic recommendation 
adversary chooses payoff vector expected reward expert respect chosen probability vector simply delta 
analogy max define max max jn delta regret ra ga gamma max measures expected difference player total reward total reward best expert 
results hold finite set experts 
formally regard random variable arbitrary function random sequence plays gamma just adversary payoff vector 
definition allows experts advice depends entire past history observed player side information may available 
point view expert meta action higher level bandit problem payoff vector defined trial delta delta 
immediately apply corollary obtain bound gn log player regret relative best expert upper bound max 
bound quite weak player combining experts large 
show algorithm exp section modified yielding regret term form gk log 
bound reasonable number actions small number experts quite large exponential 
algorithm exp shown slightly modified version exp 
exp stands exponential weight algorithm exploration exploitation expert advice hedge subroutine apply hedge problem dimension trial receive probability vector hedge represents distribution strategies 
compute vector weighted average respect strategy vectors 
vector computed action chosen randomly 
define vector fl feed vector fl hedge delta 
define vector components corresponding gains experts delta 
simplest possible expert assigns uniform weight actions round call uniform expert 
prove results need assume uniform expert included family experts 
clearly uniform expert added family experts small expense increasing 
fact slightly weaker sufficient condition uniform expert included convex hull family experts exists nonnegative numbers ff ff ff ff algorithm exp parameters reals fl initialization initialize hedge replaced repeat game ends 
get distribution hedge 

get advice vectors 

select action probability gamma fl fl 
receive reward 

compute simulated reward vector 

feed vector fl hedge delta 
algorithm exp expert advice partial information game 
theorem fl family experts includes uniform expert expected gain algorithm exp exp max gamma fl phi fl max gamma gamma fl ln proof 
prove theorem lines proof theorem 
theorem experts delta gamma ln gamma phi fl delta delta delta gamma fl equation 
similar equation gamma fl equation experts exp gamma fl gamma gamma fl ln gamma phi fl take expectations sides inequality 
note delta max max assumed uniform expert included family experts 
combining facts immediately implies statement theorem 
analogous versions main results proved occurrences ln replaced ln corollary immediate theorem yielding bound regret gamma gk ln analog lemma need prove bound difference expert done exactly replacing ffi ffi proof 
analogs theorems proved need assume uniform expert included family experts 
analog corollary straightforward 
nearly optimal play unknown repeated game bandit problem considered point closely related problem playing unknown repeated game adversary unbounded computational power 
setting game defined theta matrix trial player called row player chooses row matrix 
time adversary column player chooses column player receives payoff ij repeated play player goal maximize expected total payoff sequence plays 
suppose trial player chooses move randomly probability distribution rows represented column vector adversary similarly chooses probability vector expected payoff mq 
von neumann celebrated minimax theorem states max min mq min max mq maximum minimum taken compact set distribution vectors quantity defined equation called value game matrix words says exists mixed randomized strategy row player guarantees expected payoff regardless column player action 
payoff optimal sense column player choose mixed strategy expected payoff regardless row player action 
player knows matrix compute strategy instance linear programming certain bring expected optimal payoff smaller trial 
suppose game entirely unknown player 
precise assume player knows number rows matrix bound magnitude entries main result section proof results section showing player play manner payoff trial rapidly converge optimal maximin payoff result holds adversary knows game knows randomized strategy player 
problem learning play repeated game player gets see column rewards associated choice adversary corresponds full information game 
problem studied hannan blackwell foster vohra fudenberg levin freund schapire 
problem learning play player gets see single element matrix associated choice choice adversary corresponds partial information game emphasis 
problem previously considered megiddo 
previously proposed strategies extremely inefficient 
strategy simpler efficient able prove faster rates convergence 
fact application earlier algorithms problem entirely straightforward 
player actions identified rows matrix chosen randomly trial algorithm exp tune ff fl corollary total number epochs play 
payoff vector simply deltaj th column chosen adversary trial theorem unknown game matrix thetam value suppose player knowing uses algorithm sketched adversary trials 
player expected payoff trial gamma gamma gamma ln proof 
assume extension general case straightforward 
corollary max gamma gamma tn ln maxmin strategy row player max min mq min mq distribution vector th component 
max delta mq vt mq player expected payoff vt gamma gamma tn ln dividing get average trial payoff gives result 
note theorem independent number columns appropriate assumptions theorem easily generalized adversaries infinite number strategies 
matrix large entries small known player algorithm may efficient alternative linear programming 
generality theorem allows handle games outcome plays random variable constant ij 
pointed megiddo result valid non cooperative multi person games average trial payoff player strategy converge rapidly maximin payoff shot game 
known advance methods developed section applied 
acknowledgments express special kurt hornik advice patience listening ideas proofs earlier draft 
yuval peres amir dembo help regarding analysis martingales 
peter auer cesa bianchi gratefully acknowledge support esprit working group ep neural computational learning ii neurocolt ii alfredo 
pseudo games 
annals mathematical statistics 
david blackwell 
controlled random walks 
invited address institute mathematical statistics meeting seattle washington 
cesa bianchi yoav freund david haussler david helmbold robert schapire manfred warmuth 
expert advice 
journal association computing machinery may 
thomas cover joy thomas 
elements information theory 
wiley 
dean foster rakesh vohra 
randomization rule selecting forecasts 
operations research july august 
yoav freund robert schapire 
decision theoretic generalization line learning application boosting 
journal computer system sciences august 
yoav freund robert schapire 
adaptive game playing multiplicative weights 
games economic behavior appear 
drew fudenberg david levine 
consistency cautious fictitious play 
journal economic dynamics control 
gittins 
multi armed bandit allocation indices 
john wiley sons 
james hannan 
approximation bayes risk repeated play 
tucker wolfe editors contributions theory games volume iii pages 
princeton university press 
varaiya 
multi armed bandit problem revisited 
journal optimization theory applications october 
lai herbert robbins 
asymptotically efficient adaptive allocation rules 
advances applied mathematics 
nick littlestone manfred warmuth 
weighted majority algorithm 
information computation 
megiddo 
repeated games incomplete information played non bayesian players 
international journal game theory 

discrete parameter martingales 
north holland 
robbins 
aspects sequential design experiments 
bulletin american mathematical society 
vovk 
aggregating strategies 
proceedings third annual workshop computational learning theory pages 
proof lemma suffices prove fixed action ae gamma phi fl gamma ln ffi oe ffi lemma follows union bound 
fix simplify notation dropping subscripts clear context 
define random variable exp gamma gamma phi fl main claim proof 
claim markov inequality ffig ffi simple algebra seen equivalent equation 
prove induction method lemma vii 
trivially 
prove inductive step gamma gamma exp gamma phi fl exp gamma gamma lemma gamma exp gamma gamma gamma phi gamma gamma phi fl exp phi fl second inequality follows fact gamma gamma gamma gamma gamma gamma fl gamma fl line uses fact fl 
combining equations gives gamma gamma forms gamma inductive hypothesis 
completes proof 
proof theorem construct random distribution rewards follows 
play begins action chosen uniformly random action 
rewards associated action chosen independently random probability ffl small fixed constant ffl chosen proof 
rewards associated actions chosen independently random equal odds 
expected reward best action ffl main part proof derivation upper bound expected gain algorithm distribution rewards 
write deltag denote probability respect random choice rewards write deltag denote probability conditioned action deltag delta ig 
write unif deltag denote probability respect uniformly random choice rewards actions including action 
analogous expectation notation delta delta unif delta 
player strategy 
random variable denoting reward received time denote sequence rewards received trial hr shorthand entire sequence rewards 
randomized playing strategy equivalent priori random choice set deterministic strategies 
adversary strategy defined oblivious actions player suffices prove upper bound expected gain deterministic crucial proof simplifies notation 
formally regard algorithm fixed function step maps reward history gamma action usual ga denotes total reward algorithm max max total reward best action 
note assume oblivious strategy max max 
random variable denoting number times action chosen lemma bounds difference expectations measured delta unif delta 
lemma function defined reward sequences action unif gammae unif ln gamma ffl proof 
apply standard methods instance cover thomas 
distributions kp gamma qk gamma variational distance kl lg kullback liebler divergence relative entropy distributions 
lg denote log notation kl gamma gamma lg gamma gamma conditional relative entropy gamma kl lg gamma lg gamma gamma shorthand relative entropy bernoulli random variables parameters gamma unif frg gamma unif frg unif frg frg gamma unif frg unif frg frg gamma unif frg kp gamma unif cover thomas lemma states kp unif gamma ln kl unif chain rule relative entropy cover thomas theorem gives kl unif kl unif fr gamma fr gamma unif fi ig kl unif fi ig kl ffl jj unif fi ig gamma lg gamma ffl unif gamma lg gamma ffl second equality seen follows regardless past history rewards gamma conditional probability distribution unif fr gamma reward uniform 
conditional distribution fr gamma easily computed gamma action fixed action action conditional distribution uniform probability ffl 
lemma follows combining equations 
ready prove theorem 
specifically show theorem player strategy distribution rewards described expected regret algorithm lower bounded max gamma ga ffl gamma gamma gamma ln gamma ffl proof 
action chosen action clearly expected payoff time ffl ffl fi ig fi ig ffl fi ig expected gain algorithm ga ffl apply lemma function reward sequence actions player strategy determined past rewards 
clearly 
unif gammae unif ln gamma ffl unif gammae unif ln gamma ffl gammat ln gamma ffl fact unif implies unif tk 
combining equation ga ga ffl gamma ln gamma ffl expected gain best action expected gain action max ffl 
get regret lower bounded bound statement theorem 
small ffl bound theorem order theta ffl gamma ffl choosing ffl small constant gives lower bound omega gamma kt 
specifically lower bound theorem obtained theorem choosing ffl minf inequality gamma ln gamma ln 
