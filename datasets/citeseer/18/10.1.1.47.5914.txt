support vector machine approach decision trees bennett mathematical sciences department rensselaer polytechnic institute troy ny rpi edu blue mathematical sciences department rensselaer polytechnic institute troy ny rpi edu key ideas statistical learning theory support vector machines generalized decision trees 
support vector machine decision tree 
optimal decision tree characterized primal dual space formulation constructing tree proposed 
result method generating logically simple decision trees multivariate linear nonlinear decisions 
preliminary results indicate method produces simple trees generalize respect decision tree algorithms single support vector machines 
statistical learning theory powerful theoretical tool practical approach learning problems 
support vector machine svm algorithms successfully applied classification regression problems 
examine key ideas svms extended class decision trees dts 
key ideas formulating problem structural risk minimization solving dual problem nonlinear transformations input space construct nonlinear discriminants 
consider dts decision svm 
making simple changes nonlinear transformation input space decisions tree linear discriminants polynomials radial basis functions neural networks combination 
primary question underlying structure dt number types decisions classification leaves optimal dt 
trade power decisions tree complexity resulting classification tree 
consider limiting cases tree flexible multivariate decision svm versus univariate dt produced cart 
sufficiently general discriminant function th order polynomial svm solve large difficult problem decision 
benefit dts logical interpretable rules produced 
dt algorithm attribute decision requires large number decisions solve problem losing ease interpretability 
due combinatorial nature dts typically recursive algorithm selects locally best decision construct tree tree pruned avoid overfitting 
strategy frequently fails powerful decisions allowed decision potentially solve problem 
decision take account subsequent decisions avoid overfitting 
flexible decisions overfitting occur 
propose single svm univariate dt algorithm 
simple tree nonlinear multivariate decisions techniques apply trees size 
directly address combinatorial aspects dt construction 
structural risk minimization eliminate dt pruning 
ideally resulting dt logically simpler produced dt approaches decisions tree simpler single svm 
review key ideas svms 
show ideas extended dts 
practical algorithms preliminary computational results discussed 
conclude extensions applications 
key ideas support vector machines briefly review key ideas support vector machines 
readers consult details 
svm input vectors mapped nonlinearly high dimensional feature space 
linear discriminant constructed new space resulting nonlinear discriminant original input space 
key idea formulation problem structural risk minimization principle srm 
srm classifier reduce expected classification error confidence interval ensure generalization 
specifically say set labeled training patterns xm dimensional real pattern vector class gamma class points linearly separable exists vector scalar delta gamma delta gamma gamma gamma equivalently delta gamma optimal separating plane delta separates training data maximal margin maximizes delta equivalently minimizes delta subject constraints 
geometrically corresponds pushing apart parallel planes supporting class supporting class wider margin smaller vc dimension resulting classifier 
widening classification margin confidence interval classification error reduced 
primal gop problem dual gop problem min delta gamma max ff ff ff ff delta gamma gamma ff gamma ff primal generalized optimal plane gop problem accepts errors allowing points violate constraints 
minor variant formulations 
problem objective maximizes margin weight minimizes classification error weight gamma 
user select control trade classification rate confidence interval 
second key idea dual problem 
equivalent dual problem constructed efficiently solved problems high input dimension 
lagrangian dual variable ff defined constraint point primal problem 
dual problem variables simple constraints regardless dimensionality feature space 
optimal primal variables ff easily recovered optimal dual variables 
predicted class point determined function sgn ff gamma 
inner product defined delta linear discriminants 
point ff called support vector 
global optimum convex quadratic problem standard mathematical programming techniques 
third key idea convolutions construct nonlinear discriminants 
nonlinear function oe 
maps input vectors new space problem constructs linear discriminant new space 
dual problem replace inner product points general form inner product hilbert space oe delta oe 
example delta resulting discriminant surface polynomial degree consult details convolutions definitions inner products neural networks radial basis functions 
svm formulation decision trees xw gamma gamma xw gamma xw gamma gamma xw gamma xw gamma gamma xw gamma aa xw xw xw logical geometric depiction decision tree optimal margins examine key ideas generalized dts 
dt problem formulated srm 
recall constructing tree svms 
single svm case start linear svms decision 
assume tree exists completely classifies points know sets points reach leaves 
points correctly classified reach leaf set indices points reach leaf similarly define srm want maximize margin separation decision 
geometric depiction tree maximal margins shown 
reach leaf point satisfy linear inequalities 
assumed partitioning points leaves know set inequalities satisfied decision order point reach leaf 
svms separates separates separates generalize problem construct svms 
point define fa gamma fa primal optimal dt problem min delta delta delta gamma delta gamma gamma delta gamma gamma delta gamma gamma practice partitions points known 
combinatorial aspect dt induction 
point reaches leaf error path zero 

calculate errors paths path error zero point correctly classified 
root error left path right path point 
points errors left path decisions respectively 
points errors right path decisions respectively 
point want 
equivalent complementarity constraint prior approach global tree optimization gto uses complementarity constraint error function :10.1.1.48.6290
adding objective term maximize margin gto yields problem min delta delta delta gamma theta delta gamma gamma gamma delta gamma gamma delta gamma gamma delta gamma gamma call primal problem gto svm get original gto 
prior algorithms applied gto readily applied problem 
computational results show gto svm works better gto produces simpler trees conventional dt algorithms 
limitations 
complementarity error function theoretically defined measure error 
second existing computational methods approximately solving problem slow 
third simple equivalent dual problem 
lose benefits svm efficient dual formulation ease generalization nonlinear discriminants notion support vectors 
propose dual space approach 
dual problem problem 
lagrangian variables ff fi fl correspond decisions respectively 
optimality lagrangian variables 
optimal primal variables easily reconstructed needed ff solution depends support vectors points primal constraints positive lagrangian multipliers 
max ff fi fl ff fi fl gamma ff ff fi fi fl fl ff fi fl gamma ff gamma fi gamma fl problem assumed knew remove assumption add constraint allows lagrangian multipliers corresponding path positive 
primal case add lagrangian multipliers point decision possible paths 
decision define ff ff point point may support vector decision left side right side decision 
ff ff positive 
variables corresponding path decisions positive 
ff alternative path ff 
similarly ff ff 
yields complementarity condition ff ff dual problem realizes second key idea max ff ff fi fl ff ff fi fl gamma ff gamma ff ff gamma ff fi fi fl fl ff fi fl ff fi ff fl gamma ff gamma fi gamma fl notice dual problem allows immediately incorporate key idea nonlinear transformations 
just replacing kernel appropriate generalized inner product construct polynomial decisions radial basis function decisions neural network decisions 
practical algorithms computational results algorithms necessary solve primal dual problems quite different 
primal problem gto svm nonconvex objective function minimized respect polyhedral constraints 
shown best solution gto svm terms number points misclassified extreme point polyhedral constraints 
hybrid extreme point tabu search algorithm developed gto solve approximately gto svm 
dual problem known mathematical program equilibrium constraints 
nice convex quadratic objective constraints combinatorial nature 
applications active research topic 
problem larger previously solved 
developing new tabu search algorithm approximately solving problem 
dual problem equilibrium constraints smaller cheaper evaluate structured primal problem conjecture dual algorithm efficient effective 
algorithm solve gto svm hybrid local gradient descent method nonmonotonic heuristic search technique called tabu search ts 
descent method finds local minimum 
ts done extreme points feasible region 
ts searches local minimum uses long term memory diversify new area search space 
cycle table comparison testing set accuracies method data set dim oc gto gto svm svm random random random heart cancer ion diabetes liver house classical descent method ts repeated progress 
details method parameter settings 
primal problem gto corresponds setting 
gto svm data sets cancer house 
types data sets randomly generated real world problems university california irvine uci repository machine learning databases fold cross validation uci data sets 
generated data sets created generating weights thresholds tree linear svms decisions 
points randomly generated unit cube classified generated trees 
dimension training points testing points created 
done times dimension results dimension average runs 
compared gto svm results original gto univariate dt algorithm multivariate dt algorithm oc single svm 
oc standard default settings 
gto parameters gto svm 
single svm experimented polynomials degree various values report results svms degree polynomials performed best data sets attempted 
methods may improved tuning parameters individual data sets 
testing set accuracies data sets table 
oc number decisions parenthesis 
oc recursively selects linear decisions needs 
gto gto svm linear decisions 
computational results promising 
observed adding srm gto formulation consistently improved generalization 
results dramatic tuned individual data sets 
maximizing classification margins improve results empirically 
second gto svm able obtain generalization simple trees comparison produced oc 
surprisingly gto svm performed better methods generated tree data sets 
performance svm real world data sets shows gto svm improved 
real world data sets cleveland heart disease heart wisconsin breast cancer cancer johns hopkins university ionosphere ion pima indians diabetes diabetes bupa liver disorders liver united states congressional voting records house 
comments research primal dual formulation svm approaches dts decisions class problem 
results easily generalized dts number decisions classes 
computational experiments primal method indicate method performs respect dt methods single svm 
method produced simple trees excellent generalization 
initial results highly promising great deal remains 
improve algorithms 
method solving dual formulation currently developed 
dual tested challenging problems character recognition 
second details statistical learning theory optimal dt trade logical simplicity tree complexity decisions needs investigated 
lastly investigating practical applications capability iteratively construct simple trees trees fixed structures 
techniques prune refine existing trees produced dt methods 
developing visualization technique small trees preserve enhance class information map high dimensional data low dimensional space visualization 
acknowledgments supported national science foundation 
bennett blue 
optimal decision trees 
math report rensselaer polytechnic institute troy new york 
revised 
blue bennett 
hybrid extreme point tabu search 
math report rensselaer polytechnic institute troy ny 
appear european journal operations research 
breiman friedman olshen stone 
classification regression trees 
wadsworth international california 
burges scholkopf 
improving accuracy speed support vector machines 
mozer jordan petsche editors neural information processing systems vol 
cambridge ma 
mit press 
press 
cortes vapnik 
support vector networks 
machine learning 
glover 
tabu search part orsa journal computing 
luo pang ralph 
mathematical programs equilibrium constraints 
cambridge university press cambridge england 
murthy kasif salzberg 
system induction oblique decision trees 
journal artificial intelligence research 
quinlan 
programs machine learning 
morgan kaufmann 
vapnik 
nature statistical learning theory 
john wiley sons new york 
