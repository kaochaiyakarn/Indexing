interior point methods optimal control discrete time systems stephen wright 
show developed interior point methods quadratic programming linear complementarity problems put solving discrete time optimal control problems general pointwise constraints states controls 
describe interior point algorithms discrete time linear quadratic regulator problem mixed state control constraints show efficiently incorporated inexact sequential quadratic programming algorithm nonlinear problems 
key efficiency interior point method narrow banded structure coefficient matrix factorized iteration 
key words 
interior point algorithms optimal control banded linear systems 


problem optimal control initial value ordinary differential equation objectives mixed constraints min dt oe init ir ns ir nc ir ns theta ir nc theta ir oe ir ns ir ir ns theta ir ns theta ir ng ir ns ir discrete time counterpart problem min oe xn delta delta delta fixed delta delta delta xn efficient algorithms proposed various special classes problems 
unconstrained case absent newton methods conjugate gradient methods described polak newton method efficient implementation discussed dunn bertsekas 
variety quasi newton approaches applied unconstrained version see example edge powers kelley sachs 
control constrained case absent states appear problem traditionally treated constrained optimization problem pointwise separable nature constraints methods gradient mathematics computer science division argonne national laboratory argonne il 
research supported applied mathematical sciences subprogram office energy research department energy contract eng 
projection class easily implementable see example dunn 
finite dimensional problem methods advantage set currently active constraints change extensively iteration active set methods allow single change active set causes poor performance constraints 
newtonian scaling added gradient projection algorithms see gafni bertsekas dunn improve asymptotic rate convergence resulting methods proven useful control constrained version see section 
mayne described stagewise algorithm control constrained case neighborhood solution produces iterates identical sequential quadratic programming iterates 
making inherent structure level linear algebra computations mayne exploit structure somewhat higher level 
general cases functions nontrivial states controls known significantly difficult solve special cases described 
algorithms nonlinear programming techniques appear promising 
algorithms states controls treated unknowns state equation auxiliary constraints equality inequality constraints respectively 
deals mainly case auxiliary constraints equalities inequalities proposes algorithms reduced gradient type features added ensure near feasibility iterates 
polak yang mayne describe order algorithm barrier functions inequality constraints 
chapter describes variety augmented lagrangian penalty function methods reduced unconstrained problem 
di describe structured quasi newton method particular augmented lagrangian take advantage feature exploit coefficient matrix factored iteration 
focus linear quadratic version discrete problem 
formulated min xn qn xn delta delta delta fixed delta delta delta hn xn sequential quadratic programming algorithms nonlinear problem give rise subproblems form iteration 
algorithm propose may efficient algorithms special cases see example comparisons gradient projection algorithm control bounded problems section claim forms basis efficient general solution procedure 
continuous problem solution problem form similar remains core operation 
numerical results section restrict attention simple discretizations continuous problems 
higher order discretizations possible example collocation gauss points convert nonlinear programming problem 
issues arise discretization process particularly solution contains singular arcs order necessary conditions give rise higher index differential algebraic equation 
discuss point general methodology applicable discretizations local support lead block banded linear systems type described section 
main task show interior point methods may useful tools solving problems form methods embedded inexact sequential quadratic programming algorithms solve problems form 
arises discretization continuous problem alternative algorithms mathematical programming efficient grows large 
example number iterations required active set methods see fletcher chapter reasonably expected proportional number constraints 
iteration involves solution certain narrow banded linear system dimension total complexity probably 
possibility algorithms gradient projection class difficult implement states controls variable complexity feasible set 
observe section number interior point iterations required solve independent better 
main task iteration involves solution linear system banded coefficient matrix dimension total amount practice 
researchers noted cases iteration count practically independent show section formal analyses suggest 
interesting algorithms proposed rockafellar workers extended linear quadratic programming class problems includes discrete time linear quadratic optimal control problems 
aim find saddle point lagrangian multistage problems property decomposable respect primal variables dual variables fixed vice versa 
zhu rockafellar primal dual steepest descent conjugate gradient algorithms take advantage structure linear convergence results proved 
finite termination results proved conjugate gradient algorithm 
algorithms take advantage structure higher level linear algebra complexity iteration similar 
interesting question structured interior point methods type discussed efficiently solve entire extended linear quadratic programming class 
assume convexity condition holds positive semidefinite delta delta delta qn positive semidefinite 
second order sufficiency conditions isolated local solution weaker holds practice problems 
remainder laid follows 
section introduce classes interior point algorithms convex quadratic programming 
algorithms described respect general formulation problem see specific problem 
convergence theory algorithms described 
section discusses practical issues arise implementation algorithms general formulation 
section discuss inexact sequential quadratic programming algorithm general nonlinear programming problem 
convergence analysis derived existing theory mixed nonlinear complementarity problems stopping criterion quadratic subproblem shown easily evaluated 
section show algorithms described preceding sections take special advantage structure inherent problems linear algebra techniques banded linear systems described wright 
deal approach conducive parallel implementation task evaluating functions gradients clearly divided independent processors parallel solution banded linear system carried techniques discussed 
remainder superscripts vector matrix quantities represent iteration numbers subscripts distinguish different components vector distinguish different stages optimal control problem 
subscripts scalars denote iteration numbers 
denotes euclidean norm specified 

interior point algorithms convex quadratic programming 
section give general outline developed interior point algorithms convex quadratic programming 
algorithms usually apply linear complementarity problems descriptions follow connection classes problems 
source frustration years interior point algorithms desirable theoretical properties polynomial complexity superlinear convergence tend slow practice little proved algorithms perform exceptionally 
developments continue occur rapid pace performance gap closing 
outline interior point methods polynomial complexity superlinear convergence tends faster practice nice theoretical properties 
algorithms motivated common simple framework describe stating problem discussing relationship primal dual formulations 
assume convex quadratic program form min qz az cz ir ir ir positive semi definite 
assume optimal solution vector slacks inequality constraints 
dual max gamma qv gamma gamma qv relationship problems outlined proposition see example monteiro adler propositions mangasarian section proposition 
optimal solution exist optimal solution 
conversely optimal solution 
ii optimal solutions respectively 
conversely feasible solution gamma cv feasible gamma cv gamma cv optimal solutions respectively 
assumed primal solution exists follows proposition quartet qz gamma az gammac feasible respect conditions gamma cz gamma gammac gamma qz gamma qz difference primal dual objective function values duality gap 
interior point methods maintain feasibility respect gradually reducing duality gap zero 
fact iterates algorithms remain strictly feasible set defined satisfies fact justifies term interior 
progress algorithm solution gauged examining duality gap potential function constructed example ae ln gamma ln ae fixed barrier parameter 
elements desirable iterate remain vicinity central path defined delta delta delta mg measures closeness discussed 
maintaining closeness central path helps retaining strict property facilitates convergence analysis 
general step method aims retain feasibility respect moving closer central path reducing duality gap 
aims may satisfied performing newton linearization equations delta delta delta succinctly diag delta delta delta delta delta delta ym delta delta delta 
set clearly aiming satisfy complementarity condition regard central path set kt current iterate aiming move closer nearest point central path sense reducing duality gap 
see chosen lie strictly extremes partially satisfies aims 
note linear complementarity problem coefficient matrix gammaa gammac positive semi definite 
linear complementarity point view useful discussing convergence interior point algorithms describing choice initial point section 
interior point framework specified 
suppose initial point satisfies strict inequalities available 
current iterate generate search direction ffi ffiy solving system equations ffiy ffi ffi ffiy gamma elimination ffi yields symmetric indefinite system gamma gamma ffiy gamma gamma set ffi ffiy chosen retain feasibility new iterate respect things 
potential reduction algorithms choose depend duality gap specifically delta ae delta def kt value ae discussed 
chosen ffiy ffi achieves certain constant reduction defined 
result demonstrate polynomial complexity basic potential reduction algorithm 
proved kojima mizuno linear complementarity problem standard form easily extended mixed linear complementarity problem 
theorem 
suppose set nonempty ii ae ae iii iv min delta delta delta gamma delta ae gamma iterates produced interior point algorithm satisfy gamma delta delta delta corollary 
suppose assumptions theorem hold number gamma iterates generated algorithm satisfy delta kt gammal ml 
proof 
result follows immediately theorem fact gamma ml gammal polynomial complexity follows ml time iteration polynomial problem size 
strictly speaking standard assumption data problem rational needed 
algorithm theorem certainly expected yield superlinear convergence slow practice 
researchers analyzed algorithms choices ae ae relaxed 
choices ae yield efficient practical algorithms lie outside scope analysis 
heuristics similar utilized han pardalos ye initially ae min th iteration ae max ae min delta calculate step ffi ffiy set maxf ffiy ffi delta delta delta mg ae min ae min ae min ae min fi take step length go iteration constant fi set 
experience choice ae manipulated large possible allowing steps usually observed produce largest reductions duality gap 
heuristic experimentation quite successful 
noted zhang tapia dennis choice ae delta takes effect iterations ensures quadratic convergence duality gap zero 
second algorithm consider predictor corrector type 
linear programming problems algorithm described mizuno todd ye ye tapia zhang 
analysis extended linear complementarity problems ji huang 
algorithm idea ff neighborhood central path defined follows ff fi fi fi fi fi fl fl fl fl fl gamma fl fl fl fl fl ff ff 
note 
assumed initial point 
complete definition algorithm need specify choices choose largest value ffiy ffi ii odd choose kt 
ji huang show odd main results summarized theorem theorem 
theorem corollary predictor corrector algorithm described delta delta gamma delta delta delta suppose 
strictly complementary solution exactly zero delta delta delta convergence step superlinear lim delta delta main implementation issues algorithm ffl choice odd solve scalar equation fl fl fl fl fl ffiy gamma ffiy ffi fl fl fl fl fl gamma ffiy ffi search interval algorithm local third order convergence 
typically iterations required 
cost step cost solving system 
ffl choice initial point satisfies centrality condition prior information starting point known cold start device discussed section choose point 
circumstances initial estimate solution satisfy available 
case perform corrector iterations setting line search point satisfies encountered 
start phase line search parameter step chosen approximately minimize fl fl fl fl fl ffiy ffiy ffi gamma fl fl fl fl fl retaining ffiy ffi 
implementation details interior point algorithms 
discuss implementation issues arise interior point method discussed previous section choice initial point satisfies efficient stable solution linear system 
situations choice feasible initial point easier form inequality constraints 
common example problems form inequality constraints bounds controls 
controls lie strictly bounds chosen initial values states obtained substituting state adjoint equations respectively 
slacks completely determined choice remaining unknowns chosen multipliers correspond upper lower bounds respectively 
derivative lagrangian respect obtain equation satisfied gamma gamma delta delta delta centrality desirable products close possible delta delta delta choose target value delta products pick satisfy simple optimization problem min gamma deltak gamma deltak delta delta satisfy heuristic produces near central points necessary apply centering steps obtain point satisfies 
choice feasible point easy case control state constraints problem artificially augmented extra component way feasible choice trivially 
monteiro adler kojima mizuno show construct feasible initial points near central path introducing artificial variables convex quadratic programs linear complementarity problems respectively 
describe scheme choosing near central point tailored form 
modification scheme useful initial estimate solution available describe 
introduce large positive number ir actual magnitude discussed 
define quantities delta delta delta gamma gamma consider augmented version qz gamma az gammac ey gammaa gamma gamma little computation shows point feasible respect complementarity condition le le point placed arbitrarily close central path sufficiently large choice delta delta delta viewing linear complementarity problem show solutions augmented problem correspond solutions sufficiently large 
stated mixed linear complementarity problem gammam gammaa gammae clearly positive semi definite 
lemma shows solution triples complementary lemma 
solutions 

proof 
gamma gamma gamma gammam gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma quantity right hand side non negative result follows 
remains specify required magnitude precise result theorem 
suppose optimal solution corresponding dual vectors 
suppose chosen large gamma solution augmented problem 
satisfy 
solution 
proof 
statement easily verified substituting conditions 
prove second statement lemma 
vectors constructed part theorem complementary pair assumed exist second part 
follows satisfies conditions solution 
implementation predictor corrector algorithm start setting 
number iterations multiplied process repeated 
primal dual algorithm place emphasis starting close central path 
start setting max iterations increased suggested theorem problem re initialized 
initial guesses solution available modify strategy take advantage available information 
situation arise linear quadratic problem arises subproblem sequential quadratic programming algorithm nonlinear programming 
arise multilevel method result solving problem coarse grid starting point finer grid 
suppose initial guess gamma gamma gamma gamma choosing threshold criterion ffl ffl define vector ir ffl yk ffl small 
choose similar magnitude kc yk kb gamma zk set gamma gamma gamma choose gamma gamma gamma lm replacing see feasible initial point gamma gamma gamma gamma starting point far central path provided far away 
hot start technique modified case gamma slightly non positive delta delta delta situation arises frequently practice 
occurs usually zero strategy replace zero value positive value chosen turn issues arise solution linear system 
interior point techniques described far applied problem variables equations ordered matrix banded structure system solved operations 
augmentation problem necessary banded structure destroyed 
problem overcome forming factorizing schur complement 
specifically simple row column reordering augmented version yields gammaa gamma gamma gammae gamma ffiy ffi gamma delta ae gamma gamma delta ae gamma elementary argument shows solved cost factorization coefficient matrix block forward resulting factors involving different right hand side plus vector inner products 
turn numerical issue factorizing matrix diagonals gamma unbounded 
problem overcome simple scaling procedure sketch 
define diagonal scaling matrix ii min strict complementarity holds see theorem definition gamma cz lim gamma cz lim suppose loss generality rows partitioned ca da ca da partitioned accordingly sufficiently large matrix factorize gamma gamma ca ca gammai matrix conditioned data equality constrained quadratic program min qz az ca da allow 
similar scaling applied additional diagonal element augmented system 

inexact sequential quadratic programming 
consider general nonlinear programming problem min ir ir ir write lagrangian solution exist order necessary conditions 
loss generality partitioned gamma gamma gamma gamma addition assume strong second order sufficiency conditions hold ffi ffi ffi ffi ffi assume linear independence condition full row rank applies 
robinson theorem shows regular solution mixed nonlinear complementarity problem dl dz regularity referred strong regularity 
common variant sequential quadratic programming algorithm obtains new iterate current iterate solving quadratic program min ffi ffi ffi ffi gammah ffi gammag dl dz set ffi set lagrange multipliers solution 
subproblem restated linear complementarity problem ffi gammac gamma ffi gamma ffi ffi positive semidefinite form coefficient matrix left hand side positive semidefinite 
pang suggests algorithm solving initially choose set iteration find ffi kh ffi fl fl fl fl fl fl fl dl dz min gammag fl fl fl fl fl fl fl ffi ffi ffi min gammac ffi gamma set ffi 
scalar specified 
note ffi exact solution 
positive semidefinite algorithm section obtain approximate solution terminating inner iterations criterion satisfied 
convergence result obtained making straightforward modifications theorem pang 
proof omitted 
theorem 
suppose solution triple twice continuously differentiable neighborhood conditions satisfied 
constant initial point sufficiently close algorithm produces sequence converges 
ii addition assumptions lim convergence superlinear 
iii suppose addition assumptions second derivatives satisfy holder continuity conditions kr gamma kz gamma fl kr gamma kz gamma fl delta delta delta kr gamma kz gamma fl delta delta delta neighborhood fl positive constants 
fl fl fl fl fl fl fl dl dz min gammag fl fl fl fl fl fl fl fl convergence order fl 
remarks 

pang non mixed considered 
mixed equality relations inequalities definition modified described techniques convergence analysis 

value specified precisely 
depends regularity properties point size neighborhood iterates constrained lie 

pang proves result theorem iii case fl 
extension fl immediate 

variant theorem proved case initial accurate kz gamma small ky gamma kw gamma may large 
details tedious pursue option 
noted algorithm section solve making identifications gammah gammag ffi lemma gives alternative expression quantity kh ffi needed lemma 
algorithm section solve identifications described augmentation inner iterate ffi kh ffi fl fl fl fl fl fl fl fl min gammac ffi gamma fl fl fl fl fl fl fl fl proof 
result follows immediately definitions observe iterates algorithm section feasible respect constraints 

tailoring algorithm problems 
turn optimal control problem describe special structure system problem 
suppose ir ns delta delta delta ir nc delta delta delta simplicity exposition assume number auxiliary constraints stage fixed introducing adjoint variables delta delta delta lagrange multipliers state equation slack vectors delta delta delta lagrange multiplier vectors delta delta delta auxiliary constraints arrive set optimality conditions correspond gamma delta delta delta qn xn pn yn gamma delta delta delta gamma gamma delta delta delta delta delta delta hn xn delta delta delta delta delta delta states controls correspond primal variables section correspond order variables system follows ffiu ffiy ffip ffix ffiu ffiy delta delta delta ffiu ffiy ffip ffix ffiy order equations similarly coefficient matrix form gammab gammav gammab 
gammaa gammab gammav gammaa gammab 
qn hn gammav diag delta delta delta 
matrix dimension bandwidth gamma factorized operations 
back forward substitution resulting triangular factors takes operations 
nonlinear problem application sequential quadratic programming sqp gives rise problems form 
described wright case auxiliary constraints absent 
lagrangian xn oe xn gamma current sqp iterate define def def def def def def def gammag def def def def solve resulting linear quadratic problem obtain sqp step 

computational results 
computational results problems literature 
case problems discretizations continuous time problems 
euler discretization problem obtained dividing time interval equal intervals gamma delta delta delta finding solving min oe xn init xn solution accurate gamma obtained 
midpoint trapezoidal rule discretization accurate gamma form circumstances 
discretization example 
inexact sqp algorithm termination criteria consistent theorem ffl def fl fl fl fl fl fl fl fl fl fl fl fl fl fl fl fl fl fl fl fl fl fl fl fl fl fl fl fl fl fl min gammag kmin yn xn gamma quantities evaluated th sqp iterate min function applies componentwise 
problem linear quadratic criterion terminate sqp iteration 
interior point algorithm solve st sqp subproblem terminated iterations kh ffix ffiu mid ffl min ffl ffl max ffl max gamma ffl min gamma defined measure linear quadratic subproblem 
theorem quadratic convergence observed criterion 
tested interior point methods discussed sections 
discussed earlier implementation predictor corrector method guaranteed converge implementation potential reduction method 
faster practice 
motivation algorithms described problems state constraints mixed state control constraints report results tests problems constraints bounds controls 
efficient algorithms problem known metric gradient projection algorithms bertsekas dunn second order ddp algorithms jacobson mayne 
compare interior point algorithms implementation bertsekas algorithm described wright examples literature 
euler discretizations continuous time problems example bertsekas 
min dt gammax ju positive constant specified 
linear quadratic problem sqp iteration 
optimal control tends bound small part interval 
example example objective function min dt problem bang bang solution 
single switching point gamma 
switch gamma occurs 
example jacobson mayne section 
min dt gammax gamma ju gamma gamma specified 
problem nonlinear dynamics sqp iteration usually required 
case feasible starting point quantities chosen discussion section 
apart advantage feasible initial point interior point algorithms exploit special structure constraints factorization coefficient matrix increased efficiency 
preferred codes test examples demonstrate versatility approach customize algorithm class problems 
results problems shown tables 
observations ffl data reported table consists number iterations required algorithm amount cpu time required workstation 
interior point algorithms nonlinear problems number sqp iterations major iterations reported total number interior point iterations minor iterations 
linear quadratic problems sqp iteration needed report just number interior point iterations 
sqp iteration requires evaluation objective constraint functions second derivatives 
interior point iteration requires setup factorization matrix form 
ffl iteration counts predictor corrector algorithm include corrector iterations necessary hot start 
corrector iterations needed move initial point closer central path typically 
ffl nonlinear problems hot start strategy section major iteration 
ffl factor performance difference potential reduction algorithm predictor corrector algorithm typical examples tried 
ffl metric gradient projection algorithm clearly fastest examples worked frequently failed certain values bound believe near degeneracy borderline regions bound active regions inactive components active inactive status difficult determine 
language section zero small 
phenomenon appear affect performance interior point methods 
ffl results metric gradient projection algorithm example 
implementation algorithm exact second derivatives computation second order part step 
global convergence assured complicated strategy implementation failed progress initial point examples 
essentially table results example 
second column shows number active constraints solution 
algorithms number iterations cpu time seconds workstation reported 
metric gradient projection terminated prematurely iterations near optimal objective function value 
metric gp potential reduction predictor corrector active iters time ip time ip time table results example 
problem bang bang solution 
potential reduction predictor corrector ip time ip time bang bang problems metric gradient projection algorithm reduce order gradient projection algorithm 
turn examples state mixed control state inequality constraints interior point algorithms specifically devised 
report results linear quadratic examples example 
min gamma gamma gamma gamma gamma gamma ju versions problem discussed number authors including jacobson mayne page exclude terminal inequality constraints 
problem bang bang solution switching times 
solved discrete nonlinear programming formulation second order method table results example 
number active constraints solution reported second column 
interior point algorithms number sqp major iterations number interior point minor iterations cpu time workstation reported 
metric gradient projections algorithm terminates objective function final value objective function final value metric gp potential reduction predictor corrector active iters time sqp ip time sqp ip time fails 
fails 

table results example different values discretization parameter runge kutta integration final obtain final function value 
cpu times seconds cray mp 
potential reduction predictor corrector final ip time ip time 
problems control bounds bang bang solutions 
discretize problem approximating odes midpoint rule 
example objective function dynamics initial conditions example constraints results shown tables 
computations example performed cray mp observed experiments times faster workstation 
final function value obtained larger version example compares final value obtained required hours cpu time vax compute solution 
comparison solutions appears table 
example cpu time required potential reduction algorithm observed nearly proportional slow growth number iterations predictor corrector algorithm produces slightly greater observed complexity 
table shows number active constraints solution expect 
describe experience nonlinear problems 
example van der pol problem state constraint 

min dt gamma gamma table comparison control profiles computed solutions example 
interior point switching time switching time table results example different values discretization parameter cpu times seconds workstation 
potential reduction predictor corrector ip time ip time 


table active constraints example active components active components table results example different values discretization parameter cpu times seconds workstation 
active sqp iters potential reduction inner iters cpu time 
table results example different values discretization parameter cpu times seconds workstation 
active sqp iters potential reduction inner iters cpu time 
gamma starting point convexity condition hold solution observe convergence 
example di example min dt gamma show results sqp potential reduction inner iterations tables 
examples computation appears scale 


described interior point techniques discretetime optimal control problems 
computational results report section problems appear similar efficiency reported zhu rockafellar performed computations randomly generated examples similar workstation equipment 
approach bears similarity described ohno 
ohno treats firstorder stationarity conditions complementarity conditions system nonlinear equations solved differential dynamic programming method similar newton method 
similar performing predictor iterations predictor corrector algorithm expect algorithm robust 
fact ohno observes algorithm requires initial guess order converge 
discussed algorithm inner loop sophisticated algorithm approximate second derivatives global convergence strategies employed 
bertsekas projected newton methods optimization problems simple constraints siam journal control optimization pp 

simultaneous optimization solution methods batch reactor control profiles computers chem 
engng pp 

approximate methods optimization problems american elsevier new york 
di class structured quasi newton algorithms optimal control problems ifac conference applications nonlinear programming optimization control proceedings 
dunn global asymptotic convergence rate estimates class projected gradient processes siam journal control optimization pp 

convergence projected gradient processes singular critical points journal optimization theory applications pp 

projected newton method minimization problems nonlinear inequality constraints numerische mathematik pp 

dunn bertsekas efficient dynamic programming implementations newton method unconstrained optimal control problems journal optimization theory applications pp 

edge powers function space quasi newton algorithms optimal control problems bounded controls singular arcs journal optimization theory applications pp 

numerical optimization techniques optimization software new york 
fletcher practical methods optimization second edition john wiley sons 
gafni bertsekas metric projection methods constrained optimization siam journal control optimization pp 


han pardalos ye computational aspects interior point algorithm quadratic programming problems box constraints large scale numerical optimization coleman li eds siam publications philadelphia pa pp 

jacobson mayne differential dynamic programming american elsevier new york 
ji huang predictor corrector method linear complementarity problems polynomial complexity superlinear convergence tech 
rep department mathematics university iowa iowa city iowa august 
kelley sachs pointwise quasi newton method unconstrained optimal control problems numerische mathematik pp 

kojima mizuno nl iteration potential reduction algorithm linear complementarity problems tech 
rep research report department information sciences tokyo institute technology 
polynomial time algorithm class linear complementarity problems mathematical programming pp 

efficient determination optimal control profiles differential algebraic systems phd thesis department chemical engineering carnegie mellon university 
mangasarian nonlinear programming mcgraw hill new york 
advances gradient algorithms optimal control problems journal optimization theory applications pp 

mizuno todd ye adaptive step primal dual interior point algorithms linear programming tech 
rep school operations research industrial engineering cornell university ithaca ny 
appear mathematics operations research 
monteiro adler interior path primal dual algorithms 
part ii convex quadratic programming mathematical programming pp 

ohno new approach differential dynamic programming discrete time systems ieee transactions automatic control ac pp 


pang inexact newton methods nonlinear complementarity problem mathematical programming pp 

mayne sequential quadratic programming algorithm discrete optimal control problems control inequality constraints international journal control pp 

polak computational methods optimization academic press new york 
polak yang mayne method centers barrier functions solving optimal control problems continuum state control constraints proceedings th conference decision control december pp 

robinson strongly regular generalized equations mathematics operations research pp 

rockafellar wets generalized linear quadratic problems deterministic stochastic optimal control discrete time siam journal control optimization pp 

wright partitioned dynamic programming optimal control tech 
rep mcs mathematics computer science division argonne national laboratory argonne illinois september 
appear siam optimization 
solution discrete time optimal control problems parallel computers parallel computing pp 

parallel algorithm banded linear systems siam journal scientific statistical computing pp 

ye tapia zhang convergent nl iteration algorithm linear programming tech 
rep tr department mathematical sciences rice university houston tx 
zhang tapia dennis superlinear quadratic convergence interior point linear programming algorithms tech 
rep tr department mathematical sciences rice university 
zhu rockafellar primal dual projected gradient algorithms extended linear quadratic programming tech 
rep department applied mathematics university washington seattle washington august 
