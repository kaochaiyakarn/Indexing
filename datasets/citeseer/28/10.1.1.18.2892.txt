hybrid coordination reinforcement behaviors auv control pag informatics applications institute 
university eia es proposes hybrid coordination method behavior control architectures 
hybrid method takes advantages robustness modularity competitive approaches optimized trajectories cooperative ones 
demonstrate feasibility hybrid method navigation application autonomous underwater vehicle auv 
behaviors learnt online means reinforcement learning 
learning extending step learning popular learning steps 
hand output superposition behavior responses coordinator called cooperative 
programming autonomous underwater vehicle auv navigation mission methodologies disadvantages section 
propose hybrid approach competitive cooperative coordination systems aim advantage 
test feasibility hybrid coordination method behavior control architecture designed tested 
making high capability reinforcement learning robot learning behaviors implemented technique :10.1.1.134.2462
specifically learning algorithm applied extension popular step learning steps 
second purpose explores influence hybrid coordination method learning algorithm considering reinforcements distributed proportionally behavior influences 
stated field application underwater robotics 
corresponds research project behavior robotics reinforcement learning experimenting auvs 
solve difficulty robotic systems included learning techniques 
adaptation needed order able perform different changing environments 
isn established methodology develop adaptive behavior systems 
commonly approach reinforcement learning 
reinforcement learning rl class learning algorithm agent tries maximize scalar evaluation reward punishment interaction environment :10.1.1.32.7692
evaluation generated critic utility function 
rl system tries map states environment actions policy order obtain maximum reward 
rl knowledge database forms machine learning 
reason class learning suitable robotics online learning information environment required 
algorithm uses states perceived actions taken reinforcements received update values table denoted 
appropriate conditions values converge greedy policy maximum value state points optimal action 
learning step learning algorithm reinforcements applied state action pair 
due finite spaces learning considerably large learning time memory requirement 
sophisticated methods implement parameterized function enables generalization states actions :10.1.1.34.9502:10.1.1.137.4324
reinforcement learning applied various behavior systems qlearning 
cases rl algorithm adapt coordination system :10.1.1.160.8786
hand researchers rl learn internal structure behavior mapping perceived states robot actions :10.1.1.17.1220
mahadevan demonstrated decomposition agent learning policy set behaviors behavior robotics proposes simplified increased learning speed 
learning step learning algorithm reinforcements applied state action pair 
due finite spaces learning considerably large learning time memory requirement 
sophisticated methods implement parameterized function enables generalization states actions :10.1.1.34.9502:10.1.1.137.4324
reinforcement learning applied various behavior systems qlearning 
cases rl algorithm adapt coordination system :10.1.1.160.8786
hand researchers rl learn internal structure behavior mapping perceived states robot actions :10.1.1.17.1220
mahadevan demonstrated decomposition agent learning policy set behaviors behavior robotics proposes simplified increased learning speed 
reinforcement learning behaviors feasibility online adaptation robotic systems reinforcement learning algorithm adopted learn independent behaviors control architecture 
main goal test viability rl behaviors proposed hybrid coordination system 
due finite spaces learning considerably large learning time memory requirement 
sophisticated methods implement parameterized function enables generalization states actions :10.1.1.34.9502:10.1.1.137.4324
reinforcement learning applied various behavior systems qlearning 
cases rl algorithm adapt coordination system :10.1.1.160.8786
hand researchers rl learn internal structure behavior mapping perceived states robot actions :10.1.1.17.1220
mahadevan demonstrated decomposition agent learning policy set behaviors behavior robotics proposes simplified increased learning speed 
reinforcement learning behaviors feasibility online adaptation robotic systems reinforcement learning algorithm adopted learn independent behaviors control architecture 
main goal test viability rl behaviors proposed hybrid coordination system 
works competitive coordination system coordination method uses hybrid proposal :10.1.1.17.1220
hand researchers rl learn internal structure behavior mapping perceived states robot actions :10.1.1.17.1220
mahadevan demonstrated decomposition agent learning policy set behaviors behavior robotics proposes simplified increased learning speed 
reinforcement learning behaviors feasibility online adaptation robotic systems reinforcement learning algorithm adopted learn independent behaviors control architecture 
main goal test viability rl behaviors proposed hybrid coordination system 
works competitive coordination system coordination method uses hybrid proposal :10.1.1.17.1220
means proportional learning rate taken depending activation level behavior 
rl algorithm taken learning proposed peng williams 
algorithm incremental multi step learning extends original step learning allowing spreading reinforcements state action pair precedent pairs 
important real robot application dynamics robot slow time response history define behavior 
shows averaged learned table obstacle avoidance behavior 
null state action pairs seen 
number iterations percentage measures activity behavior episode 
proposed hybrid coordinator activity increases consequently learned speed 
reason hybrid coordination method behavior learns continuously faster competitive learning method :10.1.1.17.1220
emphasize performance showed hybrid coordinator assembling behaviors providing robustness behavior fusion 
obstacle avoidance behavior horizontal movements input variables horizontal sonar transducers perceived obstacles critic function rt rt rt behavior activation act act input states states range states range total states output variable normalized set points output states states states total states learning table dim 
entries vertical movement input variables vertical sonar transducers position perceived obstacle critic function rt rt rt behavior activation act act input states states range output variable speed normalized set point output states states learning table dim 
entries table 
proposed hybrid coordination demonstrated behaving robustness competitive coordinators optimized paths cooperative ones 
interesting advantage increase learning speed behavior 
proportional influence behavior robot caused proportional learning doesn appear competitive architectures 
non practical convergence time learning real experiments noted 
stated literature generalization states actions faster algorithms needed :10.1.1.32.7692
continuous spaces parameterized learning functions real experimentation swimming pool constitutes current 
arkin behavior robotics 
mit press 
arkin motor schema mobile robot navigation 

zelinsky qlearning continuous state action spaces 
proc 
th australian joint conference artificial intelligence sydney australia 
kaelbling reinforcement learning survey :10.1.1.134.2462
journal artificial intelligence research vol 
pp 

maes situated agents goals 
