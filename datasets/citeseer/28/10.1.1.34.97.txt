massachusetts institute technology artificial intelligence laboratory memo february memo statistical models occurrence data thomas hofmann jan puzicha publication retrieved anonymous ftp publications ai mit edu 
modeling predicting occurrences events fundamental problem unsupervised learning 
contribution develop statistical framework analyzing occurrence data general setting elementary observations joint occurrences pairs objects nite sets 
main challenge statistical models context overcome inherent data sparseness estimate probabilities pairs rarely observed unobserved sample set 
considerable interest extract grouping structure nd hierarchical data organization 
novel family mixture models proposed explain observed data nite number shared aspects clusters 
obvious partitioning subsets si component sample set si represent empirical distribution nij ni ni 
occurrence data distinctive application 
important example information retrieval may correspond collection documents set keywords 
nij denotes number occurrences word yj document xi 
consider application computational linguistics sets correspond words part binary syntactic structure direct objects nouns corresponding adjectives :10.1.1.14.1729
computer vision may correspond image locations discretized categorical feature values 
local histograms image neighborhood xi utilized subsequent image segmentation 
examples data mining molecular biology preference analysis enumerated stress analyzing occurrences events fact general fundamental problem unsupervised learning 
contribution general statistical framework cod 
prominent techniques example back method simpler lower order models model interpolation held data 
class methods similarity local smoothing techniques proposed essen dagan 
empirical comparison smoothing techniques 
information retrieval essentially proposals overcome sparseness problem 
rst class methods relies cluster hypothesis suggests inter document similarities order improve retrieval performance :10.1.1.36.2325
prohibitive compute pairwise similarities documents methods typically rely random comparisons random fractionation 
second approach focuses index terms derive improved feature representation documents 
far popular technique category salton vector space model di erent variants proposed di erent word weighting schemes 
variant known latent semantics indexing performs dimension reduction singular value decomposition 
mixture models advantage provide sound statistical foundation calculus probability theory powerful inference mechanism 
compared unconstrained table count model mixture models er controllable way reduce number free model parameters 
show signi cantly improves statistical inference generalization new data 
canonical way complexity control vary number components mixture 
introduce di erent technique avoid tting problems relies annealed generalization classical em algorithm :10.1.1.133.4884
argue annealed em additional advantages making important tool tting mix word gram models examples higher order occurrence data 
ture models 
mixture models natural framework unifying statistical inference clustering 
particularly important interested discovering structure typically represented groups similar objects pairwise data clustering :10.1.1.130.3511
introduce di erent technique avoid tting problems relies annealed generalization classical em algorithm :10.1.1.133.4884
argue annealed em additional advantages making important tool tting mix word gram models examples higher order occurrence data 
ture models 
mixture models natural framework unifying statistical inference clustering 
particularly important interested discovering structure typically represented groups similar objects pairwise data clustering :10.1.1.130.3511
major advantage clustering cod compared similarity clustering fact require external similarity measure exclusively relies objects occurrence statistics 
models directly applicable occurrence histogram data necessity pairwise comparisons avoided altogether 
probabilistic models cod investigated titles class gram models distributional clustering aggregate markov models natural language processing :10.1.1.14.1729:10.1.1.13.9919
approaches recovered special cases cod framework clarify relation approach sections 
mixture models natural framework unifying statistical inference clustering 
particularly important interested discovering structure typically represented groups similar objects pairwise data clustering :10.1.1.130.3511
major advantage clustering cod compared similarity clustering fact require external similarity measure exclusively relies objects occurrence statistics 
models directly applicable occurrence histogram data necessity pairwise comparisons avoided altogether 
probabilistic models cod investigated titles class gram models distributional clustering aggregate markov models natural language processing :10.1.1.14.1729:10.1.1.13.9919
approaches recovered special cases cod framework clarify relation approach sections 
particular discuss distributional clustering model major stimulus research section 
rest organized follows section introduces mixture model corresponds probabilistic grouping object pairs 
section focuses clustering models strict sense models partitioning set objects asymmetric models sets simultaneously symmetric models 
xi yj conditionally independent class joint probability distribution smm mixture separable component distri parameterized pij xi yj kx xi kx ij jj number independent parameters smm 
mg signi cantly complete table nm entries 
complexity reduction achieved restricting distribution linear combinations separable component distributions 
fitting smm optimally smm data maximize log likelihood nx mx nij log kx ij jj respect model parameters 
overcome di culties maximizing log sum set unobserved variables introduced corresponding em algorithm derived :10.1.1.133.4884:10.1.1.133.4884
em general iterative technique maximum likelihood estimation iteration composed steps expectation step estimating unobserved data generally averaging complete data log likelihood maximization step involves maximization expected log likelihood computed step iteration 
em algorithm known increase likelihood step converges local maximum mild assumptions cf 
:10.1.1.33.2557:10.1.1.133.4884
denote rr indicator variable represent unknown class observation xi yj generated 
fitting smm optimally smm data maximize log likelihood nx mx nij log kx ij jj respect model parameters 
overcome di culties maximizing log sum set unobserved variables introduced corresponding em algorithm derived :10.1.1.133.4884:10.1.1.133.4884
em general iterative technique maximum likelihood estimation iteration composed steps expectation step estimating unobserved data generally averaging complete data log likelihood maximization step involves maximization expected log likelihood computed step iteration 
em algorithm known increase likelihood step converges local maximum mild assumptions cf 
:10.1.1.33.2557:10.1.1.133.4884
denote rr indicator variable represent unknown class observation xi yj generated 
set indicator variables summarized boolean matrix rr kx rr denotes space boolean assignment matrices 
ectively partitions sample set classes 
treating additional unobserved data complete data log likelihood lx kx rr log log log estimation problems decouple posterior parameter estimate step computed exploiting bayes rule general obtained rr sj rr rr joint probability model starting point distributional clustering algorithm authors fact restricted investigations asymmetric clustering model cf :10.1.1.14.1729
:10.1.1.33.2557:10.1.1.133.4884
denote rr indicator variable represent unknown class observation xi yj generated 
set indicator variables summarized boolean matrix rr kx rr denotes space boolean assignment matrices 
ectively partitions sample set classes 
treating additional unobserved data complete data log likelihood lx kx rr log log log estimation problems decouple posterior parameter estimate step computed exploiting bayes rule general obtained rr sj rr rr joint probability model starting point distributional clustering algorithm authors fact restricted investigations asymmetric clustering model cf :10.1.1.14.1729
section 
smm iteration pk step obtained di erentiation estimate rr imposing normalization constraints method lagrange multipliers 
yields normalized summation respective posterior probabilities ij jj lx iterating step parameters converge local maximum likelihood 
notice unnecessary store posteriors step ciently interleaved 
generates data scheme 
select object xi probability pi 
choose class conditional distribution ji 
select object yj class speci conditional distribution joint probability distribution smm parameterized pij xi yj speci conditional distribution de ned associated object xi understood linear combination prototypical conditional distributions jj weighted probabilities ji cf 
:10.1.1.14.1729
notice ji de nes probabilistic assignment objects classes probabilities induced uncertainty hidden class membership object xi typically case mixture models 
special case smm equivalent word clustering model saul pereira developed parallel 
comparing graphical models fig 
fig simply corresponds arc reversal 
sample set log likelihood nx ni log pi nx kx ii mx maximum likelihood equations ii jj nij log jj pi ni arg min jj ii nij ii ni nx ii ni pn ik nk class conditional distributions jj linear empirical distributions objects xi cluster eq 
simple centroid condition distortion measure cross entropy kullback leibler divergence 
contrast maximum likelihood estimate smm minimizes cross entropy tting ij cross entropy serves distortion measure acm 
update scheme solve likelihood equations structurally similar means algorithm calculate assignments centroids nearest neighbor rule recalculate centroid distributions alternation 
acm fact similar distributional clustering model proposed minimization nx kx ii jj distributional clustering kl divergence distortion measure distributions motivated fact centroid equation satis ed stationary points dropping independent pi parameters data dependent constant arrive nx ni kx ii jj showing choice kl divergence simply follows likelihood principle :10.1.1.14.1729
point non negligible di erence distributional clustering cost function likelihood weights object speci contributions empirical frequencies ni 
implies objects large sample sets si larger uence optimization data partitioning account observations opposed constant uence distributional clustering model 
em algorithm probabilistic acm interpreting cluster memberships ii model parameters may consider unobserved variables 
fact interpretation consistent common mixture models preferred context statistical modeling particular scales consider complete data distribution ij ij ij ny ii speci es prior probability hidden variables ii unobservable variables ii yields em scheme replaces arg min evaluation posterior probabilities nij jj pk jj nij exp jj exp step equivalent posteriors replacing boolean variables 
illustrate di erence consider example 
fraction observed fact unique property kl divergence satis ed euclidean distance 
pairs involving xi best explained assigning xi remaining fraction data best explained assigning fitting smm model approximately results ji ji irrespective number observations posteriors additively collected 
acm contributions rst enter huge product 
particular ni posteriors approach boolean values automatically result hard partitioning compared original distributional clustering model proposed maximum likelihood approach naturally includes additional parameters mixing proportions notice model refer probabilistic acm di erent observations involving object xi independent conditioned parameters :10.1.1.14.1729
consequence considering predictive probability additional observation xi yj requires condition sample set precisely subset si yields sjs kx pi ii kx jj posterior probabilities addition parameters order de ne predictive probability distribution occurrence pairs 
corresponding graphical representation fig 
stresses fact observations identical objects coupled hidden variable ii ii xi graphical representation asymmetric clustering model acm 
notice ni interpreted random variable treated quantity 
supports interpretation pi qj terms marginal probabilities correspond occurrence frequencies objects particular cluster 
constraints nal expression maximum likelihood estimates nx mx nij ii jj interpreted estimate joint probability cluster pair 
notice auxiliary parameters related maximum likelihood estimate quotient joint frequency objects clusters product respective marginal cluster frequencies 
treat functions insert results term represents average mutual information random events xi yj maximizing maximizes mutual information satisfying gives scm precise interpretation terms information theoretic concept 
similar criterion mutual information proposed brown class gram model :10.1.1.13.9919
precisely model special case hard clustering scm formally ii ji bigram model implies word coupled means equations set discrete variables obtained maximizing augmented likelihood deduce ii hi arg max hi jj nij log expression hi simpli es jj ni ni ni constant independent cluster index dropped maximization :10.1.1.13.9919
stressed manipulations fact variables appearing identi ed estimates consistent plausible verifying consistent parameters pij pi holds independently speci choice automatically ensures global normalization pi turn explains global constraint ect optimal choices 
similar equations obtained jj arg nij ii log nature simultaneous clustering suggests alternating minimization procedure partition optimized xed partition vice versa 
update step partition estimators updated order ensure validity update sequence guaranteed increase likelihood step 
constraints nal expression maximum likelihood estimates nx mx nij ii jj interpreted estimate joint probability cluster pair 
notice auxiliary parameters related maximum likelihood estimate quotient joint frequency objects clusters product respective marginal cluster frequencies 
treat functions insert results term represents average mutual information random events xi yj maximizing maximizes mutual information satisfying gives scm precise interpretation terms information theoretic concept 
similar criterion mutual information proposed brown class gram model :10.1.1.13.9919
precisely model special case hard clustering scm formally ii ji bigram model implies word coupled means equations set discrete variables obtained maximizing augmented likelihood deduce ii hi arg max hi jj nij log expression hi simpli es jj ni ni ni constant independent cluster index dropped maximization :10.1.1.13.9919
stressed manipulations fact variables appearing identi ed estimates consistent plausible verifying consistent parameters pij pi holds independently speci choice automatically ensures global normalization pi turn explains global constraint ect optimal choices 
similar equations obtained jj arg nij ii log nature simultaneous clustering suggests alternating minimization procedure partition optimized xed partition vice versa 
update step partition estimators updated order ensure validity update sequence guaranteed increase likelihood step 
notice cluster association parameters ectively decouple interactions assignment variables ii ik di erent xi xk assignment variables jj jl di erent yj yl 
similar equations obtained jj arg nij ii log nature simultaneous clustering suggests alternating minimization procedure partition optimized xed partition vice versa 
update step partition estimators updated order ensure validity update sequence guaranteed increase likelihood step 
notice cluster association parameters ectively decouple interactions assignment variables ii ik di erent xi xk assignment variables jj jl di erent yj yl 
principle insert expression directly likelihood derive local maximization algorithm cf 
result complicated stationary conditions :10.1.1.13.9919
decoupling ect important probabilistic scm derived section 
probabilistic scm acm approach preferred statistics treat discrete hidden variables 
situation essentially acm 
complete data distribution probabilistic scm pscm jj yx ii jj nij classes implicitly utilized di erent ways classes predicting predicted 
simpli ed model computing posteriors observations su ces compute posterior probabilities objects 
result signi cant acceleration example natural language modeling size vocabulary compared size corpus word occurrences typically di er orders magnitude 
const additional speed results simpli ed propagation posteriors tree 
simulation pursued stage strategy degrees freedom incrementally increased 
acm straightforward check predictive probabilities xi yj sjs pi ji ji hierarchies abstraction hierarchical mixture models proposed supervised unsupervised learning shares organization clusters tree structure :10.1.1.136.9119
extracts hierarchical relations clusters breaks permutation symmetry cluster labeling 
important capable perform statistical abstraction 
observations common clusters subtree rooted inner node preferably captured level generality represented node 
observations highly speci explained terminal level 
discuss important problems naturally occur context 
rst problem avoid unfavorable local maxima log likelihood 
second important problem avoid tting maximize performance unseen data 
framework allows improve em procedures aspects known deterministic annealing 
deterministic annealing applied clustering problems including clustering pairwise clustering context cod distributional clustering :10.1.1.14.1729:10.1.1.130.3511
key idea introduce temperature parameter replace minimization combinatorial objective function substitute known free energy 
details topic appendix annealing methods statistical physics 
consider general case maximum likelihood estimation em algorithm 
step de nition computes posterior average complete data log likelihood maximized step 
suggests heuristic procedure start single cluster recursively split clusters 
course annealing keeps track splits uses phase diagram tree topology note merely tree topology successively grown data partition obtained speci temperature may drastically change annealing process 
summarize annealed em solves problems 
avoids unfavorable local minima applying temperature continuation method modi ed likelihood convex high temperature eq 
di ers original formula pereira scales temperature frequency ni includes mixing proportions :10.1.1.14.1729
pointed naturally obtained ml framework distributional clustering cost function weights ni considered 
canceling weights may reasonable approach limit ect frequently observed objects xi organization clusters 
statistical viewpoint implausible observations automatically posterior distributions temperature level jj 
avoids tting discounting likelihood contribution step 
introducing lagrange parameter arrive objective function probability distributions discrete space ft ep ep log solution minimization problem associated generalized free energy gibbs distribution exp step amounts discounting likelihood compared prior th power 
step performs minimization ep ft respect xed gibbs distribution necessarily correspond true posterior 
notice convergence annealed em guaranteed ft necessarily likelihood lyapunov function 
exact calculation posterior step intractable typically higher order correlations hidden variables scm optimization problem restricted factorial dis rr pr suggestive notation pr stress variational parameters pr thought approximation posterior marginals 
variational technique known mean eld approximation successfully applied optimization problems computer vision inference graphical models :10.1.1.130.3511
general solutions mean eld approximation ful ll stationary conditions exp hh rr expectations taken respect 
notice expected costs appear exponent expectation taken respect hidden variables rr xed 
order obtain convergent iteration procedure general case replace synchronous step update sequential update 
scm coupling hidden variables restricted pairs variables ii jj nij allows recompute posteriors variables posteriors sweep vice versa 
breiman friedman olshen stone 
classi cation regression trees 
wadsworth intern 
group belmont california 
brown mercer della pietra lai :10.1.1.13.9919
class gram models natural language 
computational 
buhmann 
vector quantization complexity costs 
neural computation 
deerwester dumais landauer furnas harshman 
indexing latent semantics analysis 
journal american society information science 
dempster laird rubin :10.1.1.133.4884
maximum likelihood incomplete data em algorithm 
journal royal statistical society 
essen 
cooccurrence smoothing stochastic language modeling 
cvgip image understanding 
hindle 
noun classi cation predicate argument structures 
proceedings association computational linguistics pages 
hofmann buhmann :10.1.1.130.3511
pairwise data clustering deterministic annealing 
ieee transactions pattern analysis machine intelligence 
hofmann puzicha buhmann 
deterministic annealing unsupervised texture segmentation 
morgan kaufmann publishers san mateo ca 

minimum property free energy 
physical review 
pereira tishby lee :10.1.1.14.1729
distributional clustering english words 
proceedings association computational linguistics pages 
peters walker 
iterative procedure obtaining maximum likelihood estimates parameters mixture normal distribution 
