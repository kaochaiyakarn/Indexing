support vector machine active learning applications text classification simon tong simon tong cs stanford edu computer science department stanford university stanford ca daphne koller koller cs stanford edu computer science department stanford university stanford ca 
support vector machines met significant success numerous real world learning tasks 
machine learning algorithms generally applied randomly selected training set classified advance 
settings option pool active learning 
randomly selected training set learner access pool unlabeled instances request labels number 
introduce new algorithm performing active learning support vector machines algorithm choosing instances request 
svm dotted line transductive svm solid line 
solid circles represent unlabeled instances 
explosion readily available text data 
approaches achieve goal rocchio dumais sebastiani 
furthermore domain svms shown notable success joachims dumais interest see active learning offer improvement highly effective method :10.1.1.11.6124
remainder structured follows 
section discusses svms terms induction transduction 
section introduces notion version space section provides theoretical motivation methods performing active learning svms 
section experimental results real world text domains indicate active learning significantly reduce need labeled instances practice 
svms find hyperplane maximizes margin feature space way pose optimization task follows maximize min fy delta phi subject kwk delta phi having conditions kwk delta phi cause solution lie version space 
view problem finding point version space maximizes distance min fy delta phi duality feature parameter space phi phi unit normal vector hyperplane parameter space 
constraints delta phi hyperplanes delimit version space 
expression delta phi regarded theta distance point hyperplane normal vector phi done redefining training instances positive regularization constant 
essentially achieves effect soft margin error function cortes vapnik commonly svms :10.1.1.15.9362
permits training data linearly non separable original feature space 
ml long tex support vector machine active learning applications text classification want find point version space maximizes minimum distance hyperplanes 
svms find center largest radius hypersphere center placed version space surface intersect hyperplanes corresponding labeled instances 
normals hyperplanes touched maximal radius hypersphere phi distance delta phi minimal 
procedure repeated times topic results averaged 
considered simple margin maxmin margin ratio margin querying methods random sample method 
random sample method simply randomly chooses query point unlabeled pool 
method reflects happens regular passive learning setting training set random sampling data 
measure performance metrics test set classification error stay compatible previous reuters corpus results precision recall breakeven point joachims :10.1.1.11.6124
precision percentage documents classifier labels relevant really relevant 
recall percentage relevant documents labeled relevant classifier 
altering decision threshold svm trade precision recall obtain precision recall curve test set 
precision recall breakeven point number summary graph point precision equals recall 
columns table interest 
show approximately instances needed ml long tex simon tong daphne koller random achieve level performance ratio active learning method 
instance passive learning average requires times data achieve comparable levels performance active learning methods 
tables indicate active learning provides benefit infrequent classes particularly measuring performance precision recall breakeven point 
observation noted previous empirical tests mccallum nigam :10.1.1.13.8629
noticed approximately half queries active learning methods asked tended turn positively labeled regardless true proportion positive instances domain 
investigated gains active learning methods regular random sampling due biased sampling 
created new querying method called randomly sample equal number positive negative instances pool 
obviously practice ability randomly sample equal number positive negative instances having label entire pool instances may may reasonable depending domain question 

related studies active learning classification 
query committee algorithm seung freund uses prior distribution hypotheses 
general algorithm applied domains classifiers specifying sampling prior distribution natural 
probabilistic models dagan engelson specifically naive bayes model text classification bayesian learning setting mccallum nigam :10.1.1.13.8629:10.1.1.30.6148
naive bayes classifier provides interpretable model principled ways incorporate prior knowledge data missing values 
typically perform discriminative methods svms particularly text classification domain joachims dumais :10.1.1.11.6124
re created mccallum nigam experimental setup newsgroups corpus compared reported results algorithm mn algorithm 
algorithm require positive negative instance seed classifier 
query committee algorithm seung freund uses prior distribution hypotheses 
general algorithm applied domains classifiers specifying sampling prior distribution natural 
probabilistic models dagan engelson specifically naive bayes model text classification bayesian learning setting mccallum nigam :10.1.1.13.8629:10.1.1.30.6148
naive bayes classifier provides interpretable model principled ways incorporate prior knowledge data missing values 
typically perform discriminative methods svms particularly text classification domain joachims dumais :10.1.1.11.6124
re created mccallum nigam experimental setup newsgroups corpus compared reported results algorithm mn algorithm 
algorithm require positive negative instance seed classifier 
seeding active svm positive negative instance give active svm unfair advantage active svm randomly sampled documents queries 
virtually guaranteed training set contained positive instance 
algorithm lt algorithm lacks theoretical justifications query committee algorithm successfully committee active learning method winnow classifiers text domain 
produced emulating experimental setup reuters data set compares reported results 
active svm initialized number randomly selected documents permitted perform active learning 
graph shows active svm accuracy significantly better lt algorithm 
lewis gale introduced uncertainty sampling applied text domain logistic regression companion decision trees lewis catlett :10.1.1.52.2415
simple querying method svm active learning essentially uncertainty sampling method choose instance current classifier uncertain provided substantially justification algorithm effective 
noted performance uncertainty sampling method variable performing quite poorly occasions 
studies campbell cohn independently developed simple method active learning support vector machines provided different formal analyses 
campbell smola extend analysis simple method cover soft margin svms cortes vapnik linearly non separable data :10.1.1.15.9362
lewis gale introduced uncertainty sampling applied text domain logistic regression companion decision trees lewis catlett :10.1.1.52.2415
simple querying method svm active learning essentially uncertainty sampling method choose instance current classifier uncertain provided substantially justification algorithm effective 
noted performance uncertainty sampling method variable performing quite poorly occasions 
studies campbell cohn independently developed simple method active learning support vector machines provided different formal analyses 
campbell smola extend analysis simple method cover soft margin svms cortes vapnik linearly non separable data :10.1.1.15.9362
cohn note interesting behaviors active learning curves presence outliers 

introduced new algorithm performing active learning svms 
advantage duality parameter space feature space arrived algorithms attempt reduce version space possible query 
cost asking query relatively cheap emphasis placed fast feedback simple method may suitable 
case shown methods learning substantially outperform standard passive learning 
furthermore experiments hybrid method indicate possible combine benefits ratio simple methods 
leads directions interest 
studies noted gains computational speed obtained expense generalization performance querying multiple instances time lewis gale mccallum nigam :10.1.1.13.8629:10.1.1.16.3103
viewing svms terms version space gives insight approximations may provide guide multiple instances better query 
instance suboptimal query instances version space hyperplanes fairly parallel 
simple method blindly choosing query instances closest current svm may better query instances close current svm hyperplanes version space fairly perpendicular 
similar tradeoffs ratio maxmin methods 
