structural risk minimization data dependent hierarchies john shawe taylor royal holloway university london peter bartlett australian national university robert williamson australian national university martin anthony london school economics technical report series nc tr october produced part esprit working group neural computational learning coordinating partner department computer science tw ex england information contact john shawe taylor address email dcs ac uk department computer science royal holloway university london tw ex uk 
email dcs ac uk department systems engineering australian national university canberra australia 
email peter bartlett anu edu au department engineering australian national university canberra australia 
email bob williamson anu edu au department mathematics london school economics houghton street london wc ae uk 
email anthony ac uk received september introduces generalizations vapnik method structural risk minimisation srm 
making explicit details srm provides result allows trade errors training sample improved generalization performance 
results reported sections implicit cited treatment serves introduce main results sections explicit assumptions inherent presentations 
considers standard srm provides bounds true error hypothesis lowest empirical error class 
theorem gives error bound decreases twice empirical error roughly linearly ratio vc dimension number examples give error bound decreases empirical error square root ratio 
section onwards address shortcoming srm method vapnik page highlights srm principle structure defined priori training data appear 
algorithm maximally separating hyperplanes proposed vapnik workers violates principle hierarchy defined depends data :10.1.1.103.1189:10.1.1.15.9362
section prove result shows achieves correct classification training data class valued functions thresholded values real valued functions training points away zero bound generalization error better obtained vc dimension thresholded class 
section apply case considered vapnik separating hyperplanes large margin 
section introduce general framework allows large class methods measuring luckiness sample sense large margin lucky 
section show vapnik maximum margin hyperplanes fit general framework allows radius set points estimated data 
result follows 
choice prior dk different affect resulting trade complexity accuracy 
view expectation penalty term choosing large class probably overestimate reasonable give correspondingly large penalty large numbers errors 
possibility exponentially decreasing prior distribution dk gamma rate decrease varied classes 
assuming choice incremental search optimal value reduction number classification errors class ln em note tradeoff errors sample generalization error discussed :10.1.1.15.9362
classifiers large margin classifiers large margin standard methods structural risk minimization require decomposition hypothesis class chosen advance seeing data 
section introduce variant srm effectively decomposition data seen 
main tool dimension introduced problems learning 
show classifier correctly classifies training set large margin fat shattering function scale related margin small generalization error small 
proof making lemma move double sample union bound suffices show ffi er fl fl min jf gamma ery ffi largest value need consider greater number points xy 
sufficient ffi ffi large margin hyperplanes consider note lemma function fl bounds fat fl 
probability distribution theta second component determined target value component 
note point misclassified maxf xg fl theta maxf xg fl gamma fl fi fi fi phi psi fi fi fi ffi replacing fl fl lemma lemma obtain ffi ffl ffi log em log log ffi condition lemma satisfied linking ffl substituting ffi gives result 
related result gives bounds misclassification probability thresholded functions terms error estimate involving margin corresponding real valued functions :10.1.1.50.4654
result bounds fat shattering dimension sigmoidal neural networks gives bounds generalization performance networks depend size parameters independent number parameters 
large margin hyperplanes consider particular case results previous section applicable class linear threshold functions euclidean space 
vapnik page suggested choosing maximal margin hyperplane hyperplane maximises minimal distance points assuming correct classification improve generalization resulting classifier :10.1.1.103.1189:10.1.1.15.9362
give evidence indicate generalization performance frequently significantly better predicted vc dimension full class linear threshold functions 
note point misclassified maxf xg fl theta maxf xg fl gamma fl fi fi fi phi psi fi fi fi ffi replacing fl fl lemma lemma obtain ffi ffl ffi log em log log ffi condition lemma satisfied linking ffl substituting ffi gives result 
related result gives bounds misclassification probability thresholded functions terms error estimate involving margin corresponding real valued functions :10.1.1.50.4654
result bounds fat shattering dimension sigmoidal neural networks gives bounds generalization performance networks depend size parameters independent number parameters 
large margin hyperplanes consider particular case results previous section applicable class linear threshold functions euclidean space 
vapnik page suggested choosing maximal margin hyperplane hyperplane maximises minimal distance points assuming correct classification improve generalization resulting classifier :10.1.1.103.1189:10.1.1.15.9362
give evidence indicate generalization performance frequently significantly better predicted vc dimension full class linear threshold functions 
section show large margin help case give explicit bound generalization error terms margin achieved training sample 
bounding appropriate fat shattering function applying theorem 
margin arises proof perceptron convergence theorem see example page alternate motivation large margin noise 
give evidence indicate generalization performance frequently significantly better predicted vc dimension full class linear threshold functions 
section show large margin help case give explicit bound generalization error terms margin achieved training sample 
bounding appropriate fat shattering function applying theorem 
margin arises proof perceptron convergence theorem see example page alternate motivation large margin noise 
margin occurs winnow algorithms variants developed littlestone :10.1.1.130.9013
connection uses explored 
large margin hyperplanes consider hyperplane defined weight vector threshold value 
subset euclidean space limit point hyperplane min wi say hyperplane canonical form respect min wi delta denote euclidean norm 
theorem basis argument 
position apply theorem value theorem taken 
fat fl fl fl fl substituting bound theorem gives required bound 
section give analogous result special case general framework derived section 
sample size bound result weaker additional log factor allow cope slightly general situation estimating radius ball knowing advance 
fact bound theorem depend dimension input space particularly important light vapnik construction support vector machines :10.1.1.15.9362
method implementing quite complex decision rules defined polynomials neural networks terms linear hyperplanes dimensions 
clever part technique algorithm dual space maximizes margin training set 
vapnik algorithm bound theorem allow posteriori bounds generalization error range applications 
important note explanation performance maximum margin hyperplanes different vapnik page 
andrew barron approximation estimation bounds artificial neural networks machine learning 
andrew barron complexity regularization applications artificial neural networks pages ed 
nonparametric functional estimation related topics kluwer academic publishers 
andrew barron thomas cover minimum complexity density estimation ieee transactions information theory 
peter bartlett sample complexity pattern classification neural networks size weights important size network technical report department systems engineering australian national university may :10.1.1.50.4654
peter bartlett philip long prediction learning uniform convergence scale sensitive dimensions preprint department systems engineering australian national university november 
peter bartlett philip long robert williamson learnability real valued functions journal computer system sciences 
michael manfred opper perceptron learning largest version space neural networks statistical perspective 
proceedings workshop theoretical physics world scientific 
michael manfred opper perceptron learning largest version space neural networks statistical perspective 
proceedings workshop theoretical physics world scientific 
available brain ac kr compressed ps bernhard boser isabelle guyon vladimir vapnik training algorithm optimal margin classifiers pages proceedings fifth annual workshop computational learning theory pittsburgh acm 
kevin kumar learning canonical smooth estimation part simultaneous estimation ieee transactions automatic control 
cortes vladimir vapnik support vector networks machine learning :10.1.1.15.9362
thomas cover joy thomas elements information theory wiley new york 
richard duda peter hart pattern classification scene analysis john wiley sons new york :10.1.1.15.9362
girosi michael jones poggio regularization theory neural networks architecture neural computation pages 
leonid pascal approximation learning convex pages paul ed proceedings lecture notes artificial intelligence springer berlin 
available brain ac kr compressed ps bernhard boser isabelle guyon vladimir vapnik training algorithm optimal margin classifiers pages proceedings fifth annual workshop computational learning theory pittsburgh acm 
kevin kumar learning canonical smooth estimation part simultaneous estimation ieee transactions automatic control 
cortes vladimir vapnik support vector networks machine learning :10.1.1.15.9362
thomas cover joy thomas elements information theory wiley new york 
richard duda peter hart pattern classification scene analysis john wiley sons new york :10.1.1.15.9362
girosi michael jones poggio regularization theory neural networks architecture neural computation pages 
leonid pascal approximation learning convex pages paul ed proceedings lecture notes artificial intelligence springer berlin 
fundamentals artificial neural networks mit press cambridge ma 
david haussler decision theoretic generalizations pac model neural net learning applications information computation 
