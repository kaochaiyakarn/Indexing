multimodal interfaces alex waibel minh tue vo paul stefan school computer science carnegie mellon university pittsburgh pa university karlsruhe computer science department am karlsruhe germany overview research laboratories multimodal human computer interfaces 
goal interfaces free human computer interaction limitations acceptance barriers due rigid operating commands keyboards main device 
move involve available human communication modalities 
human modalities include speech gesture pointing eye gaze lip motion facial expression handwriting face recognition face tracking sound localization 

developments computer communication industries accelerating pace variety forms information delivered users worldwide 
turn multiplies problems associated managing interacting new wealth data information 
combination sound images text available multimedia pc publishing companies advancing goal delivering multimedia information information super highway unfolds 
multiplicity amount information expands ways access communicate remain limited 
relatively primitive input devices interfaces keyboard mouse dominate interface 
contrast human human communication takes advantage wealth hints signals explicit implicit lost human computer interaction 
meeting face face having eye contact reading lips pointing finger plain talk human communication richer simple text 
speech writing represent direct expressions language routinely complemented visual modalities body language 
need processed hopes achieve truly natural human computer interaction 
increase effectiveness human computer interfaces include combine visual spoken textual language represent full spectrum human communication 
efforts developing richer human computer computer mediated human human interfaces attempt embrace take advantage communication modalities 
interact project involves number research projects progress labs carnegie mellon university pittsburgh usa university karlsruhe germany 
aimed interpreting visual acoustic instantiations language communicate day day 
modalities interest labs speech understanding translation sound source localization gesture recognition lipreading handwriting recognition eye face tracking 
research involves improving recognition accuracies modality specific component processors development optimal combination multiple input signals deduce user intent reliably cross modal speech acts 
specifically aim combine visual acoustic textual cues including speech recognition lipreading robust recognition gesture speech multimodal interpretation speech handwriting flexible redundant input face sound source localization robust speech extraction adverse acoustic environments cocktail party effect face speech recognition focus attention user talking 
referring 
begun attacking advanced goals broad front activity 
reviews currently stand subtopics lipreading complement speech recognition gesture recognition complement speech line handwriting character recognition 
possible develop learning strategies connectionist statistical ensure scalability portability larger different application domains 
discuss efforts report initial results cross modal integration 

automatic lip reading speech recognition lip movement visual information source tightly synchronously coupled acoustic speech act naturally viewed integral part speech recognition effort 
contrast communication modalities described article gestures handwriting may invoked independently speech 
approaches automated speech recognition asr consider solely acoustic information sensitive background noise fail totally voices simultaneously cocktail party effect happens offices conference rooms outdoors real world environments 
humans deal distortions considering additional sources directional contextual visual information primarily lip movements 
source involved recognition process extensively hearing impaired individuals contributes significantly normal hearing recognition 
usefulness lip movement stems large part rough acoustic signal reliable distinguishing place articulation ex 
conveys robustly manner voicing information ex 

task exploiting lip reading automatic speech recognition system requires solution conceptually distinct independent problems suitable representation recognition visual signal integration obtained visual evidence acoustic side 
clearly type visual pre processing constrain options available combination sources 
related research significant attempt supplement acoustic asr lip reading system built petajan applied speaker dependent isolated word vocabulary words recognition task 
static features extracted image frame linear time warping procedure identify probable word 
combining output optical recognizer commercial asr system recognition rate improved percent 
follow effort simplified optical processing achieve near real time performance 
image speaker mouth area captured camera lighting system installed head mounted harness circumventing image processing problems 
combination optical acoustic decisions achieved set heuristic rules 
performance similar earlier system 
pentland mase chose parameterize oral cavity image computing average optical flow vectors regions picture designed capture movement particular facial muscles 
regions selected manually experimenters 
template matching optical data recognize strings digits speakers 
average word recognition rate roughly 
neural networks optical acoustic input distinguish vowel phonemes varying acoustic signal noise ratio snr 
static images sequences optical input 
furthermore relative contribution visual acoustic information adjusted snr omniscient controller value snr explicitly 
visual input shown compensate noise induced performance drop purely acoustic recognition 
stork measured position reflective markers placed lips talker significantly simplifying issue optical data capture 
measurements derived parameters visual input 
separate time delay neural networks tdnn processed acoustic optical data render decision english letters spoken isolation 
visual acoustic recognition percent respectively 
combining outputs bayesian framework achieved performance 
visual features extracted processed image frames acquired head mounted camera identify possible timit sentences spoken single talker 
appears sentences treated essentially long words setup 
vector quantization input allowed discrete hidden markov models hmm recognition process 
system generalized models achieved recognition rate visual information 
initial system integrated acoustic visual continuous speech asr system reported 
developed spelling task german alphabet 
training test utterances comprise spelled pauses names nonsense letter sequences arbitrary unknown recognizer lengths 
task equivalent continuous recognition small highly confusable vocabulary 
system description record acoustic visual data parallel pre process illustrated 
acoustic signal sampled khz bit resolution 
fairly standard front computes fourier coefficients hamming windowed speech segments msec frame rate 

bimodal data acquisition speech recognition lip reading letter hypotheses german letters dtw layer acoustic tdnn combined layer visual tdnn phoneme viseme state layer hidden layer input layer 
network architecture audio visual asr system visual data initially captured pixel images bit gray level resolution pixel 
smaller area pixels centered mouth manually extracted low pass filtering downsampled pixel image 
resulting gray level values normalized frame lie constituted visual evidence available classification algorithms 
modular multi state time delay neural network ms tdnn perform actual recognition 
shows architecture 
layers input hidden phoneme viseme acoustic visual inputs processed separately 
third layers produces activations phoneme states acoustic side viseme states visual side 
viseme rough correlate phoneme smallest visually distinguishable unit speech 
general purposes visemes defined mapping set phonemes 
reflects fact phonemes instance essentially indistinguishable visual information 
system phoneme phoneme viseme transitions included separate phone viseme states 
outputs phoneme viseme layers integrated units combined layer layer exist basic ms tdnn 
activation combined phone state weighted sum activations corresponding phoneme state viseme state units 
visemes influence combined layer units 
final layer copies activations combined layer stage dynamic time warping algorithm applied find optimal path phone hypotheses corresponds sequence letter models 
network training done phases 
acoustic visual sub nets trained separately fit phoneme viseme targets 
second complete network trained fit letter targets 
error backpropagation find connection weights resulting optimal recognition training data 
weights determining combination sub nets trained way computed dynamically recognition reflect apparent reliability sub net outputs 
specifically activations phoneme viseme layers normalized represent probabilities entropy sub net output computed 
low entropy signifies probability concentrated units relative confidence respective sub net identification 
conversely high entropy corresponds near equal probability phonemes visemes 
accordingly symmetrically weight acoustic visual contributions combined layer inverse proportion respective entropies 
results table shows recognition performance originally achieved speaker dependent task 
training data consisted training testing sequences speaker speaker acoustic visual combined msm clean msm noisy mcb clean mcb noisy table 
word accuracy speech lip system msm mcb 
misclassified omitted inserted letters counted errors 
noisy experiments acoustic data corrupted broadband noise acoustic performance significantly reduced 
results show adding visual information significantly boost recognition rate despite relatively poor performance visual input 
improvement naturally evident acoustic speech noisy 
experiments carried database speakers female male test performance task 
sequences speaker training testing 
visual mode achieved word accuracy 
adding acoustic mode reduced error rate clean speech db snr 
current development directions seeking improve performance system letter spelling task view extending continuous speech recognition 
concentrating visual side system acoustic technology mature point 
robustness practical system manual extraction mouth region face image acceptable 
step away method recorded new data speaker asked position lips visible rectangle shown workstation screen 
image frame directly 
experimenting system understand principal weaknesses sources fragility 
contrary initial suspicion processing appears relatively insensitive reasonable variation lighting conditions 
implemented different version gray level normalization procedure protects performance varying average image brightness 
severe illumination gradients pose problem alleviated adaptive histogram equalization 
significantly increase computational load 
shift lip image frame cause serious degradation shown experiment 
trained network newly recorded sequences speaker 
images training sequences network recognizes perfectly diagonally shifted frame pixels 
direction shift chosen random successive image sequence 
shift equivalent speaker moving face millimeters 
word accuracy dropped 
pixel shift recognition 
severe losses observed shift effected constant direction 
investigating approaches counteract effect 
designing detector automatically precisely locate lips picture 
addition compensating shifts obviate need speaker hold head constant orientation 
initial speaker dependent results indicate neural network detector reliably locate lip region varying image size lighting backgrounds 
increase robustness training visual tdnn copies training images artificially shifted scaled 
idea network learn patterns may occur different locations sizes frame 
training sequences created artificial image translation constant size system shows insensitivity image shifts approaching performance levels observed originally hand excised frames 
investigating different parameterizations input exhibit shift invariance 
magnitude fourier transform image representation 
parameterization certainly irrelevant redundant information gray level values currently visual input 
large parameter count increases significantly number network weights need estimated 
smaller parameter set lead better generalization particularly speaker independent recognition computational load reduction 
reduce number visual parameters invoking heuristics image segmentation feature extraction tdnn expected form internal representation relevant features 
preliminary experiments show represent images principal components reducing data factor visibly performance 
noted representation relying correlations original data points sensitive image shifts studies ex 

investigating linear discriminant analysis related technique prove better image classification opposed representation 
acoustic visual integration evidence humans combine acoustic visual information classification making separate decisions modality 
automatic system benefit integration low level availability cross modal features instance temporal relationships events modalities 
course contingent availability sufficient training data robustly train magnified network results increasing size input vector 
preliminary experiments suggest approach modality integration fact feasible visual data reduction 
observation supplies motivation described 
low level modality integration allows avoid tricky problem viseme specification 
reasonably straightforward specify phoneme viseme mapping discrete syllables true continuous speech especially considering segmentation coarticulation effects 
lead maintain integration phoneme viseme level combination scheme expanded 
units combined layer benefit drawing inputs just corresponding phoneme viseme 
instance identity second guess sub net prove relevant 

line cursive handwriting recognition recognition cursive continuous handwriting written touch screen graphics tablet scientific significant practical value note pad computers integration multi modal systems 
different cessing recognition approaches optical character recognition ocr online character recognition developed decades 
main advantage temporal information handwriting recorded recognition 
general dynamic writing information coordinate sequence available ocr input consists scanned text bitmaps 
contrast systems spatial context proximity strokes characters distorted lost merely retains uses pen coordinates function time 
developed input representation combines advantages bitmaps ocr dynamic writing information 
input representation characters words represented sequence called context bitmaps basically low resolution descriptions coordinate neighborhood 
input representation connectionist recognizer suited handling temporal sequences patterns provided kind input representation 
recognizer multi state time delay neural network ms tdnn integrates segmentation recognition words single network architecture 
ms tdnn originally proposed continuous speech recognition tasks combines shift invariant high accuracy pattern recognition capabilities tdnn non linear time alignment procedure dynamic time warping aligning strokes character sequences 
shows basic architecture line handwriting recognition system 
recognition system integrated example application shown 
sections describe preprocessing techniques ms tdnn architecture recognition results writer independent single character recognition tasks large vocabulary writer dependent cursive handwriting recognition tasks vocabulary sizes words 

system overview 
example application 
differences cursive characters hard detect local information preprocessing preprocessing time ordered coordinate sequence provided digitizer performed successive steps normalization feature extraction 
normalization normalization performed remove irrelevant variability occurring raw coordinate sequence 
compensate varying writing speeds different writers single writer single word character coordinate sequence resampled successive coordinates equally spaced 
resampled coordinate sequence smoothed moving average window mainly removes undesired sampling noise 
baseline normalization word performed 
stage process word rotated linear regression points get rough correction word orientation 
second fine adjustment word rotated linear regression respect local minima 
word character linearly rescaled fixed height 
feature extraction second step preprocessing extraction features pen trajectory yielding sequence time ordered feature vectors 
basic idea feature extraction refer low level topological features trajectory leave extraction high level features connectionist recognizer 
started set strictly local features similar 
frame consisted information pen position coordinates directional features curvature speed pen pen indicator 
inspection confusion matrices networks trained features revealed significant problems discriminating cursive letters look similar differ small regions characters see examples 
problems arise due fact features strictly local means local space time 
inadequate modeling temporal long range context dependencies occurring pen trajectory 
basis new set features bitmap representation digitizer input 
normalization input map sequence points grey scale bitmap indicates number points falling pixel 

calculation context bitmaps contrast limitations optical character recognition bitmaps source information know time sequence points 
leads method combining spatial temporal sources assume falls bitmap pixel 
consider local section bitmap centered derive grey scale bitmap averaging section 
means derive temporal sequence low resolution bitmaps centered 
bitmaps plus directional information pen pen feature form new set input features recognition 
features local space longer local time 
point trajectory visible point trajectory small neighborhood 
call local bitmaps context bitmaps 
appropriate modeling temporal long range spatial short range phenomena observed pen trajectories 
multi state time delay neural network architecture input representation described previous section connectionist recognizer single character cursive handwriting recognition tasks 
recognizer integrates recognition segmentation words single network architecture multi state time delay neural network ms tdnn 
words represented sequence characters character modeled states 
results character modeled states representing initial middle final sections character 
basic ms tdnn architecture shown 
layers constitute standard tdnn sliding input windows certain sizes 
tdnn computes scores state time frame states layer 
dynamic time warping layer dtw layer word vocabulary modeled sequence states corresponding scores simply copied states layer word models dtw layer 
optimal alignment path dtw algorithm word output scores zero 
dynamic time warping dtw layer 
time delay neural network states layer hidden layer input layer input features state units 
rel 
changes context bitmaps sum activations optimal path taken score word output unit 
network trained particular vocabulary vocabularies varying sizes retraining just replacing word models dtw layer 
network parameters number states character size input windows number hidden units optimized manually results optimized automatically automatic structure optimization aso algorithm proposed 
aso algorithm time consuming manual tuning network parameters particular handwriting tasks training set sizes necessary achieving optimal recognition performance 
time 
multi state time delay neural network architecture experiments results tested input representation ms tdnn architecture single character recognition tasks cursive continuous handwriting recognition tasks 
handwriting databases training testing ms tdnn collected university karlsruhe 
data recorded pressure sensitive graphics tablet cordless stylus produces sequence time ordered dimensional vectors maximum report rate dots second consisting coordinates pressure value dot 
subjects write set single words word vocabulary covering lower case letters set isolated lower case letters upper case letters digits 
continuous handwriting results data authors 
data preprocessed described 
task training patterns test patterns recognition rate writers writers writers table 
single character recognition results writer independent table shows results different writer independent single character recognition tasks isolated characters 
writer dependent recognition results cursive handwriting isolated words table 
network results table trained 
training patterns word vocabulary msm tested retraining different vocabularies sizes words 
vocabularies msm msm msm msm completely different vocabulary network trained selected randomly word vocabulary wall street journal vocabulary 
experiments writer independent cursive handwriting databases shown recognition rates word vocabulary 
task vocabulary size words test patterns recognition rate msm msm msm msm msm table 
results different writer dependent cursive handwriting tasks 
results show proposed input representation ms tdnn architecture single character recognition cursive handwriting recognition tasks high recognition performance 
ms tdnn performs vocabulary trained see task msm vocabularies seen see task msm larger vocabularies see tasks msm msm msm 

gesture recognition investigating pen gestures drawn stylus digitizing tablet 
kind gesture simpler handle hand gestures captured camera allows rich powerful expressions editor mark manuscripts knows 
pen gestures popular hand held computers focus research mainly gestures effectively combined input modalities gestures sole input channel clumsy way issuing commands computers 
order pursue direction investigation developed multimodal text editor capable recognizing speech gesture commands 
initial multimodal editor developed currently uses editing gestures see table 
inspired standard mark symbols human editors 
delete symbols people automatically correcting written text normal pencil 
select delete delete paste selection selection transpose split line table 
text editing gestures input representation preprocessing temporal representation gestures 
gesture captured sequence coordinates tracking stylus moves tablet surface opposed static bitmapped representation shape gesture 
dynamic representation motivated successful handwritten character recognition section 
results experiments described suggest time sequential signal contains information relevant classification static image leading better recognition performance 
current implementation stream data digitizing tablet goes preprocessing phase 
coordinates normalized resampled regular intervals eliminate differences size drawing speed resampled coordinates extract local geometric information point direction pen movement curvature trajectory 
features believed hold discriminatory information help recognition process give neural network recognizer appropriate information find temporal regularities input stream 
gesture classification neural networks tdnn see classify preprocessed time sequential signal gesture predefined set gestures 
gesture set represented output unit 
data point input stream represented input units corresponding features extracted preprocessing phase include pen coordinates pressure local geometric information mentioned 
network trained set manually classified gestures modified backpropagation algorithm 
output layer hidden layer hidden layer input layer 
tdnn architecture gesture recognition training units hidden layer essentially feature detectors extract low level patterns input hidden units layer learn spot features contribute recognition gestures 
output unit integrates time evidence corresponding unit second hidden layer 
output unit highest activation level determines recognized gesture 
data samples train evaluate gesture recognizer collected single collected samples samples gesture form training set samples gesture form independent test set seen network training 
gesture recognizer achieves recognition rate training data set test set 
learning gesture handwriting recognition usefulness gesture handwriting recognition depends largely ability adapt new users great range variability way individuals write gestures 
matter tokens put training database cover different gestures mean delete text example may totally different gestures part gesture vocabulary 
particularly troublesome neural network systems usually network retrained old training data mixed large number new examples order able recognize new patterns forgetting previously learned patterns 
large number examples needed long retraining time clearly done line way enable user continue productively 
system able query user correction remember particular input pattern caused error order intelligent guesses similar inputs occur 
fallback method offer reasonable level performance network retrained line 
excitatory connection weight extra unit 
incremental tdnn architecture developed method accomplish incremental tdnn architecture 
start training regular tdnn available data obtain base network 
recognition error occurs system queries user correct output creates template matching hidden units influence output units excitatory inhibitory connections see 
template matching accomplished making weight matrix extra unit proportional activation matrix hidden layer deemed better matching input layer directly training backpropagation units hidden layer learned spot input features relevant classification 
order retain time shift invariant property tdnn powerful classifier time sequential patterns assemble extra units subunits having weights matching different section activation template activation matrix hidden layer 
extra units fact thought extra hidden layers 
purpose enable subunits slide time dimension just regular tdnn units 
consecutive subunits extra unit tend high activations consecutive time slices employ technique compute match scores see 
subsequent input pattern similar template create extra unit extra unit turned able influence corresponding output unit 
extra units fix recognition errors lowering outputs incorrectly high inhibitory negative weight connections boosting outputs incorrectly low excitatory positive weight connections 
time time score average match 
activation trace extra unit composed subunits tested incremental learning capability series experiments involving simple handwritten digit recognition 
task chosen simple easily eliminate influence factors extraneous want measure degradation performance old input patterns trained new input patterns 
development motivated gesture recognition research handwriting recognition similar poses problems gesture recognition results experiments described relevant 
trained base network examples handwritten digits written consistent way 
tested network different variation digit digit written clockwise direction counterclockwise training set 
base tdnn unable recognize new examples 
single extra unit added resulting able correctly classify new examples forgetting old training examples 
experiments show capable quickly adding coverage new input variation forgetting previously learned information candidate systems requiring line immediate recognition improvements gesture handwriting recognizers pen computers 
systems capable incremental learning able adapt quickly new user reasonable level performance allowing productive continue 
subsequent sessions new data unobtrusively collected line training full network regular architecture 
presumably superior network replace patched 
language speech gesture shows block diagram multimodal interpreter module speech gesture text editor 
gesture recognizer word spotter parser gesture frame speech frame frame merger unified frame command interpreter 
joint interpretation gesture speech tdnn gesture recognizer described 
speech component alternative speech recognition strategies include keyword spotter developed full scale continuous speech recognition modules sphinx janus 
speech recognition module coupled rtn parser semantic grammar developed editing task 
keyword spotting version word spotter trained spot keywords representing editing commands move delete textual units character word 
effect user speak naturally having worry grammar vocabulary long utterance contains relevant keywords 
example utterance please delete word equivalent delete word 
case con speech recognition semantic fragment parser achieves essentially effect matching fragments recognized speech predefined templates find semantically meaningful parts text 
creates frame consisting slots representing various components plausible semantic interpretation fills slot semantic fragments hypothesized sentence 
interpretation multimodal inputs semantic frames slots representing parts interpretation user intent 
speech gesture recognizers produce partial hypotheses form partially filled frames 
output interpreter obtained unifying information contained partial frames 
system frame slots named action scope operate 
scope named type textual unit 
possible scope types include point box textual units include character word line 
consider example user draws circle says please delete word 
gesture processing subsystem recognizes circle fills coordinates box scope gesture frame specified position size circle 
word spotter produces delete word parser fills action textual unit slot speech frame 
frame merger outputs unified frame action delete scope type box scope textual unit word 
command interpreter constructs editing command delete word circled user 
important advantage frame approach flexibility facilitate integration modalities acoustic visual linguistic ones 
define general frame interpretation specify ways slots filled input modality 

research aimed producing natural robust redundant efficient human computer interfaces exploring combination different human communication modalities 
combinations naturally involve acoustic visual gestural expressions human intent form multimodal language seek decode 
shown robust recognition achieved combining speech lipreading visual acoustic modalities 
shown line handwritten character recognizer combined speech gesture 
demonstrated speech gesture joined provide natural robust interpretation user intent speech gesture deliver complementary cues complete semantics multimodal speech act 
research currently progress includes exploring eye sound source localization deliver multimodal cues accurately person moving room determine focus attention human interaction 

authors gratefully acknowledge support nsf arpa basic neural network modeling speech gesture recognition combination speech gesture language 
state baden germany neuroinformatik supporting character recognition lip reading 
research impossible sponsors support 
special herman hild chris bregler arthur mcnair torsten michael finke wayne ward invaluable help code 

austin schwartz 
comparison approximate algorithms finding best hypotheses 
proc 
icassp 
baluja pomerleau 
non intrusive gaze tracking artificial neural networks 
appear advances neural information processing systems morgan kaufmann 
waibel 
connectionist architectural learning high performance character speech recognition 
proc 
icassp 

automatically structured neural networks handwritten character word recognition 
proc 
icann 

integration identification consonant segments 
quarterly journal experimental psychology pp 

bregler hild waibel 
improving connected letter recognition lipreading 
proc 
icassp 
bregler 
als zur 
thesis 
fuer informatik universitaet karlsruhe 

continuous automatic speech recognition lipreading 
ph dissertation 
george washington university 
guyon albrecht lecun denker hubbard 
design neural network character recognizer touch terminal 
pattern recognition 
haffner waibel 
integrating time alignment neural networks high performance continuous speech recognition 
proc 
icassp 
haffner waibel 
multi state time delay neural networks continuous speech recognition 
advances neural network information processing systems nips 
hauptmann 
speech gestures graphic image manipulation 
proc 
chi 
hild waibel 
connected letter recognition multi state time delay neural network 
neural information processing systems nips 
huang hon hwang lee rosenfeld 
sphinx ii speech recognition system overview 
computer speech language press 
jackson 
theoretical minimal unit visual speech perception visemes coarticulation 
review sept pp 


connectionist recognizer line cursive handwriting recognition 
proc 
icassp 
miller nicely 
analysis perceptual confusions english consonants 
journal acoustical society america mar pp 

ney 
stage dynamic programming algorithm connected word recognition 
proc 
icassp 
nodine toto 
recording analyzing eye position data microcomputer workstation 
behavior research methods instruments computers pp 

mase pentland 
automatic lipreading optical flow analysis 
systems computers japan pp 

petajan 
automatic lipreading enhance speech recognition 
proc 
ieee communications society global telecommunications conference atlanta ga nov 
petajan bischoff 
improved automatic lipreading system enhance speech recognition 
acm sigchi pp 

pomerleau 
neural network perception mobile robot guidance 
ph thesis carnegie mellon university cmu cs 
rose paul 
hidden markov model keyword recognition systems 
proc 
icassp 
guyon henderson 
line cursive script recognition time delay neural networks hidden markov models 
proc 
icassp 

lvq model speaker adaptive speech recognition 
proc 
icassp 
stork greg wolff levine 
neural network lipreading system improved speech recognition 
proc 
ijcnn 
summerfield 
audio visual speech perception lipreading artificial stimulation 
hearing science hearing disorders eds new york academic press 
waibel 
performance consistency ms large vocabulary continuous speech recognition 
advances neural information processing systems morgan kaufmann 
turk pentland 
eigenfaces recognition 
journal cognitive neuroscience pp 

vo waibel 
multimodal human computer interface combination speech gesture recognition 
adjunct proc 
interchi 
vo 
incremental learning time delay neural network 
proc 
icassp 
waibel hinton shikano lang 
phoneme recognition time delay neural networks 
ieee transactions acoustics speech signal processing 
waibel jain mcnair saito hauptmann 
janus speech speech translation system connectionist symbolic processing strategies 
proc 
icassp 
ward 
understanding spontaneous speech phoenix system 
proc 
icassp 
ware 
evaluation eye tracker device computer input 
human factors computing systems iv 
advances janus speech translation system 
proc 
eurospeech 
goldstein jr sejnowski 
integration acoustic visual speech signals neural networks 
ieee communications magazine nov 
waibel 
hybrid neural network dynamic programming word spotter 
proc 
icassp 
houghton waibel 
improving ms tsnn word spotting 
proc 
icassp 
biographies alex waibel received degree massachusetts institute technology 
electrical engineering computer science ph 
computer science carnegie mellon university 
computer science faculty carnegie mellon serves senior research scientist directing janus speech translation project interact multimodal interfaces project 
holds joint appointments center machine translation robotics institute computational linguistics department carnegie mellon 
university professor informatik karlsruhe university germany directs laboratory interactive systems 
professor waibel published extensively areas speech recognition synthesis neurocomputing machine learning machine speech translation multimodal interfaces 
founders star directs verbmobil large aimed international cooperation multilingual human human communication 
time delay neural networks awarded ieee signal processing society senior award atr best award 
minh tue vo born originally south vietnam moved montreal canada 
currently working ph computer science carnegie mellon university pittsburgh obtained bachelor degree computer engineering university waterloo waterloo canada master degree computer science carnegie mellon 
research interests include neural network systems multimodal human computer interfaces 
stefan born bonn germany 
currently ph student computer science department university karlsruhe germany got master degree computer science 
research interests line handwriting recognition integration speech recognition lipreading 
interested multi modal systems neural networks 
paul born warsaw poland united states 
received bachelor master doctor science degrees electrical engineering massachusetts institute technology respectively 
sc thesis investigated new structure automatic speech recognition motivated models human speech cue integration 
currently conducting post doctoral research laboratory interactive systems university karlsruhe germany 
professional purposes interested automatic speech recognition speech processing human machine multimodal communication probabilistically describable phenomena 

