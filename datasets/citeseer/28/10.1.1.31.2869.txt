machine learning proceedings fourteenth international conference 
boosting margin new explanation effectiveness voting methods robert schapire yoav freund labs mountain avenue murray hill nj usa research att com peter bartlett dept systems engineering aust 
national university canberra act australia peter bartlett anu edu au sun lee electrical engineering department university college australian defence force academy canberra act australia lee ee oz au 
surprising recurring phenomena observed experiments boosting test error generated hypothesis usually increase size large observed decrease training error reaches zero 
show phenomenon related distribution margins training examples respect generated voting classification rule margin example simply difference number correct votes maximum number votes received incorrect label 
show techniques analysis vapnik support vector classifiers neural networks small weights applied voting methods relate margin distribution test error 
show techniques analysis vapnik support vector classifiers neural networks small weights applied voting methods relate margin distribution test error 
show theoretically experimentally boosting especially effective increasing margins training examples 
compare explanation bias variance decomposition 
years growing interest learning algorithms achieve high accuracy voting predictions classifiers 
example researchers reported significant improvements performance decision tree learning algorithms cart voting methods :10.1.1.133.1040
refer hypotheses combined vote base hypothesis final voted hypothesis combined hypothesis 
examples effectiveness methods consider results experiments letter dataset 
experiment breiman bagging method top 
times random bootstrap subsamples combined computed trees simple voting 
shown training test error curves lower upper curves respectively combined hypothesis function number trees combined 
test error dataset run just 
test error bagging trees significant improvement 
error rates indicated horizontal grid lines 
second experiment freund schapire adaboost algorithm top dataset :10.1.1.32.8918
algorithm similar bagging subsamples chosen manner concentrates hardest examples 
results experiment shown 
note boosting drives test error just 
error curves reveal remarkable phenomenon observed drucker cortes quinlan breiman 
addition bagging boosting variant dietterich bakiri method error correcting output codes viewed voting method 
carefully constructing error correcting codes simply random output codes highly similar properties 
base learning algorithm simple algorithm finding best binary split decision tree decision 
algorithm weak pseudoloss versions boosting bagging 
see freund schapire details :10.1.1.133.1040
explained learning curve figures shows training error bottom test error top curves 
indicated horizontal grid lines error rate base hypothesis run just error rate combined hypothesis iterations 
note log scale figures 
margin distribution graphs shown iterations indicated short dashed long dashed barely visible solid curves respectively 
explanation evident margin distribution graphs see boosting far fewer training examples margin close zero 
stumps boosting unable achieve large margins consistent theorem base hypotheses higher training errors 
presumably low margins adversely affect generalization error complexity decision stumps smaller full decision trees 
relation bias variance theory main explanations improvements achieved voting classifiers separating expected error classifier bias term variance term 
details definitions differ author author attempts capture quantities bias term measures persistent error learning algorithm words error remain infinite number independently trained hypotheses :10.1.1.57.5909
variance term measures error due fluctuations part generating single hypothesis 
idea averaging hypotheses reduce variance term way reduce expected error 
section discuss strengths weakness bias variance theory explanation performance voting methods especially boosting 
bias variance decomposition classification 
bias variance nonnegative averaging decreases variance term changing bias term 
naturally hope beautiful analysis carry quadratic regression classification 
unfortunately observed majority vote classification rules result increase expected classification error 
simple observation suggests may inherently difficult impossible find decomposition classification natural satisfying quadratic regression case 
difficulty reflected myriad definitions proposed bias variance :10.1.1.57.5909
discussing separately remainder section noted follow definitions kong dietterich referred definition breiman :10.1.1.57.5909
bagging variance reduction 
notion variance certainly helpful understanding bagging empirically bagging appears effective learning algorithms large variance 
fact idealized conditions variance definition amount decrease error effected bagging large number base hypotheses 
naturally hope beautiful analysis carry quadratic regression classification 
unfortunately observed majority vote classification rules result increase expected classification error 
simple observation suggests may inherently difficult impossible find decomposition classification natural satisfying quadratic regression case 
difficulty reflected myriad definitions proposed bias variance :10.1.1.57.5909
discussing separately remainder section noted follow definitions kong dietterich referred definition breiman :10.1.1.57.5909
bagging variance reduction 
notion variance certainly helpful understanding bagging empirically bagging appears effective learning algorithms large variance 
fact idealized conditions variance definition amount decrease error effected bagging large number base hypotheses 
ideal situation bootstrap samples bagging faithfully approximate truly independent samples 
favorable case hope bagging get close error bayes optimal predictor 
training set examples generalization error achieved bagging iterations 
results averaged runs 
reason poor performance random sample coordinates slightly correlated bagging tends pick coordinates 
case kong dietterich definitions breiman definitions stumps stumps error pseudo loss error error pseudo loss error name boost bag boost bag boost bag boost bag boost bag boost bag waveform bias var error bias var error bias var error bias var gamma gamma error kong bias dietterich var error table bias variance experiments boosting bagging synthetic data :10.1.1.57.5909
columns labeled dash indicate base learning algorithm run just 
behavior bagging different expected behavior truly independent training sets 
boosting data achieved test error 
boosting variance reduction 
breiman argued boosting primarily variance reducing procedure 
evidence comes observed effectiveness boosting cart algorithms known empirically high variance 
error algorithms due variance surprising reduction error primarily due reduction variance 
experiments show boosting highly effective learning algorithms error tends dominated bias variance 
ran boosting bagging artificial datasets described breiman artificial problem studied kong dietterich :10.1.1.57.5909
previous authors training sets size problem 
base learning algorithm tested 
simple base learning algorithm essentially finds single node decision tree decision minimizes training error pseudoloss see freund schapire details :10.1.1.133.1040
estimated bias variance average error algorithms times 
experiments show boosting highly effective learning algorithms error tends dominated bias variance 
ran boosting bagging artificial datasets described breiman artificial problem studied kong dietterich :10.1.1.57.5909
previous authors training sets size problem 
base learning algorithm tested 
simple base learning algorithm essentially finds single node decision tree decision minimizes training error pseudoloss see freund schapire details :10.1.1.133.1040
estimated bias variance average error algorithms times 
experiments bias variance definitions kong dietterich proposed breiman :10.1.1.57.5909
multi class problems freund schapire tested error pseudoloss versions bagging boosting :10.1.1.133.1040
class problems error versions 
previous authors training sets size problem 
base learning algorithm tested 
simple base learning algorithm essentially finds single node decision tree decision minimizes training error pseudoloss see freund schapire details :10.1.1.133.1040
estimated bias variance average error algorithms times 
experiments bias variance definitions kong dietterich proposed breiman :10.1.1.57.5909
multi class problems freund schapire tested error pseudoloss versions bagging boosting :10.1.1.133.1040
class problems error versions 
results summarized table 
clearly boosting doing reducing variance 
base learning algorithm tested 
simple base learning algorithm essentially finds single node decision tree decision minimizes training error pseudoloss see freund schapire details :10.1.1.133.1040
estimated bias variance average error algorithms times 
experiments bias variance definitions kong dietterich proposed breiman :10.1.1.57.5909
multi class problems freund schapire tested error pseudoloss versions bagging boosting :10.1.1.133.1040
class problems error versions 
results summarized table 
clearly boosting doing reducing variance 
instance boosting decreases error fact original goal boosting reduce error called weak learning algorithms tend large bias 
hand optimal margin method suitable case magnitudes base hypotheses decay number increases dual spaces vapnik 
related optimal margin method uses quadratic programming optimization boosting algorithm seen method approximate linear programming 
vapnik showed constraint set points set normalized linear combinations points margin fl points vc dimension decreases fl result implies bounds generalization error terms expected margin test points typically known 
shawe taylor techniques theory learning real valued functions give bounds generalization error terms margins training examples linear combinations bound coefficients 
shawe taylor gave related results arbitrary real classes bartlett gave bounds type neural networks bounded weights :10.1.1.33.8995
boosting neural network learning algorithms attempt find functions large margins 
vapnik gave alternative analysis optimal margin classifiers number support vectors number examples define final hypothesis 
analysis preferable analysis depends size margin training examples support vectors 
previous suggested boosting method selecting small number informative examples training set 
extended appeared 
ron kohavi david wolpert 
bias plus variance decomposition zero loss functions 
machine learning proceedings thirteenth international conference pages 
bae kong thomas dietterich :10.1.1.57.5909
error correcting output coding corrects bias variance 
proceedings twelfth international conference machine learning pages 
quinlan 
bagging boosting 
proceedings thirteenth national conference artificial intelligence pages 
john shawe taylor peter bartlett robert williamson martin anthony 
framework structural risk minimisation 
proceedings ninth annual conference computational learning theory pages 
john shawe taylor peter bartlett robert williamson martin anthony :10.1.1.33.8995
structural risk minimization hierarchies 
technical report nc tr 
robert tibshirani 
bias variance prediction error classification rules 
