journal artificial intelligence research submitted published hierarchical reinforcement learning maxq value function decomposition thomas dietterich cs orst edu department computer science oregon state university corvallis presents new approach hierarchical reinforcement learning decomposing target markov decision process mdp hierarchy smaller mdps decomposing value function target mdp additive combination value functions smaller mdps 
decomposition known maxq decomposition procedural semantics subroutine hierarchy declarative semantics representation value function hierarchical policy 
maxq unifies extends previous hierarchical reinforcement learning singh kaelbling dayan hinton 
assumption programmer identify useful subgoals define subtasks achieve subgoals 
defining subgoals programmer constrains set policies need considered reinforcement learning 
maxq value function decomposition represent value function policy consistent hierarchy 
concludes comparison related discussion design tradeoffs hierarchical reinforcement learning 
fl ai access foundation morgan kaufmann publishers 
rights reserved 
dietterich 
area reinforcement learning bertsekas tsitsiklis sutton barto studies methods agent learn optimal near optimal plans interacting directly external environment :10.1.1.124.1600
basic methods reinforcement learning classical dynamic programming algorithms developed late bellman howard 
reinforcement learning methods offer important advantages classical dynamic programming 
methods online 
permits focus attention parts state space important ignore rest space 
