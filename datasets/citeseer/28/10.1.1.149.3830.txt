benchmarking attribute selection techniques data mining mark hall geoffrey holmes department computer science university waikato hamilton new zealand data engineering generally considered central issue development data mining applications 
success learning schemes attempts construct models data hinges reliable identification small set highly predictive attributes 
inclusion irrelevant redundant noisy attributes model building process phase result poor predictive performance increased computation 
attribute selection generally involves combination search attribute utility estimation plus evaluation respect specific learning schemes 
leads large number possible permutations led situation benchmark studies conducted 
presents benchmark comparison attribute selection methods 
methods produce attribute ranking useful devise isolating individual merit attribute 
attribute selection achieved cross validating rankings respect learning scheme find best attributes 
results reported selection standard data sets learning schemes naive bayes 
factors affect success data mining algorithms task 
quality data factor information irrelevant redundant data noisy unreliable knowledge discovery training difficult 
attribute subset selection process identifying removing irrelevant redundant information possible 
learning algorithms differ amount emphasis place attribute selection 
extreme algorithms simple nearest neighbour learner classifies novel examples retrieving nearest stored training example available features distance computations 
extreme algorithms explicitly try focus relevant features ignore irrelevant ones 
decision tree inducers examples approach 
testing values certain attributes decision tree algorithms attempt divide training data subsets containing strong majority class 
necessitates selection small number highly predictive features order avoid fitting training data 
regardless learner attempts select attributes ignores issue attribute selection prior learning beneficial 
reducing dimensionality data reduces size hypothesis space allows algorithms operate faster effectively 
cases accuracy classification improved result compact easily interpreted representation target concept 
attribute selection preprocessing step learning generally involves combination search attribute utility estimation 
evaluation selected features respect learning algorithms considered leads large number possible permutations 
fact computational cost attribute selection techniques led situation benchmark studies conducted 
helps fill void providing benchmark comparison attribute selection techniques produce ranked lists attributes 
methods useful improving performance learning algorithms rankings produce provide data miner insight data clearly demonstrating relative merit individual attributes 
section describes attribute selection techniques compared benchmark 
section outlines experimental methodology briefly describes weka experiment editor powerful java system run benchmarking experiments 
section presents results 
section summarizes findings 
attribute selection techniques attribute selection techniques categorized number criteria 
popular categorization coined terms filter wrapper describe nature metric evaluate worth attributes :10.1.1.30.525:10.1.1.30.525
wrappers evaluate attributes accuracy estimates provided actual target learning algorithm 
filters hand general characteristics data evaluate attributes operate independently learning algorithm 
useful taxonomy drawn dividing algorithms evaluate rank individual attributes evaluate rank subsets attributes 
group split basis search technique commonly employed method explore space attribute subsets attribute selection techniques handle regression problems class numeric discrete valued variable 
provides dimension categorize methods 
methods compared capable handling regression problems study restricted discrete class data sets methods capable handling sort problem 
focusing techniques rank attributes simplified matter important note search technique method evaluates attribute subsets possible permutations leads explored reducing number possible permutations 
say ignored methods evaluate subsets attributes contrary possible obtain ranked lists attributes methods simple hill climbing search forcing continue far side search space 
example forward selection hill climbing search starts empty set evaluates attribute individually find best single attribute 
tries remaining attributes conjunction best find best pair attributes 
process continues single attribute addition improves evaluation subset 
forcing search continue best attribute added step may decrease evaluation subset noting attribute added list attributes ranked incremental improvement subset obtained 
rest section devoted brief description methods compared benchmark 
methods evaluate individual attributes produce ranking methods evaluate subsets attributes 
forward selection search method described methods produce ranked lists attributes 
methods cover major developments attribute selection machine learning decade 
include classical statistical technique dimensionality reduction 
information gain attribute ranking simplest fastest attribute ranking methods text categorization applications sheer dimensionality data precludes sophisticated attribute selection techniques :10.1.1.161.6020
attribute class equations give entropy class observing attribute 
log log 
amount entropy class decreases reflects additional information class provided attribute called information gain 
attribute ai assigned score information gain class igi ai ai ai ai ai 
data sets numeric attributes discretized method fayyad irani 
relief relief instance attribute ranking scheme introduced kira rendell enhanced kononenko :10.1.1.51.6297
relief works randomly sampling instance data locating nearest neighbour opposite class 
values attributes nearest neighbours compared sampled instance update relevance scores attribute 
process repeated user specified number instances rationale useful attribute differentiate instances different classes value instances class 
relief originally defined class problems extended relieff handle noise multi class data sets :10.1.1.51.6297
relieff smoothes influence noise data averaging contribution nearest neighbours opposite class sampled instance single nearest neighbour 
multi class data sets handled finding nearest neighbours class different current sampled instance weighting contributions prior probability class 
principal components principal component analysis statistical technique reduce dimensionality data product transforming original attribute space 
transformed attributes formed computing covariance matrix original attributes extracting eigenvectors 
eigenvectors principal components define linear transformation original attribute space new space attributes uncorrelated 
eigenvectors ranked amount variation original data account 
typically transformed attributes account variation data retained remainder discarded 
worth noting attribute selection techniques compared principal components unsupervised method class attribute 
implementation principal components handles valued discrete attributes converting binary attributes 
disadvantage increasing dimensionality original space multi valued discrete attributes 
cfs cfs correlation feature selection methods evaluate subsets attributes individual attributes :10.1.1.149.3848
heart algorithm subset evaluation heuristic takes account usefulness individual features predicting class level 
heuristic equation assigns high scores subsets containing attributes highly correlated class low 
merits rff merits heuristic merit feature subset containing features rcf average feature class correlation rff average 
numerator thought giving indication predictive group features denominator redundancy 
heuristic handles irrelevant features poor predictors class 
redundant attributes discriminated highly correlated features 
order apply equation necessary compute correlation dependence attributes 
cfs discretizes numeric features technique fayyad irani uses symmetrical uncertainty essentially equation normalized entropy attributes involved estimate degree association discrete features 
consistency subset evaluation approaches attribute subset selection class consistency evaluation metric :10.1.1.48.2488
methods look combinations attributes values divide data subsets containing strong single class majority 
usually search biased small feature subsets high class consistency 
consistency subset evaluator uses liu setiono consistency metric di mi attribute subset number distinct combinations attribute values di number occurrences ith attribute value combination mi cardinality majority class ith attribute value combination total number instances data set 
data sets numeric attributes discretized method fayyad irani 
wrapper subset evaluation described start section wrapper attribute selection uses target learning algorithm estimate worth attribute subsets 
crossvalidation provide estimate accuracy classifier novel data attributes subset 
implementation uses repeated fold cross validation accuracy estimation 
cross validation repeated long standard deviation runs greater percent mean accuracy repetitions completed :10.1.1.30.525
wrappers generally give better results filters interaction search learning scheme inductive bias 
improved performance comes cost computational expense result having invoke learning algorithm attribute subset considered search 
experimental methodology benchmark experiment applied attribute selection techniques sixteen standard machine learning data sets uci collection 
data sets characteristics summarized table 
order compare effectiveness attribute selection attribute sets chosen technique tested learning algorithms decision tree learner release probabilistic learner naive bayes 
algorithms chosen represent quite different approaches learning relatively fast state art algorithms data mining applications 
table data sets 
data set instances num 
nom 
classes glass anneal breast credit diabetes horse colic heart heart stat ionosphere labor lymph segment soybean vote zoo percentage correct classifications averaged fold cross validation runs calculated algorithm data set combination attribute selection 
train test split dimensionality reduced attribute selector passed learning algorithms 
dimensionality reduction accomplished cross validating attribute rankings produced attribute selector respect current learning algorithm 
fold cross validation training part train test split estimate worth highest ranked attribute highest ranked attributes highest ranked attributes highest ranked attributes best cross validated accuracy chosen best subset 
attribute selection techniques require data pre processing copy training split operate 
folds attribute selector learning scheme combination 
final accuracy induced models reduced feature sets primary interest recorded statistics number attributes selected time taken select attributes size decision trees induced 
weka experiment editor perform benchmark experiment weka waikato environment knowledge analysis powerful open source java machine learning workbench run computer java run time environment installed 
weka brings machine learning algorithms tools common framework intuitive graphical user interface 
weka primary modes data exploration mode experiment mode 
explorer provides easy access weka data preprocessing learning attribute selection data visualization modules environment encourages initial exploration data 
experimenter allows large scale experiments run results stored database retrieval analysis 
shows configuration panel experimenter 
results table results attribute selection naive bayes data set nb ig rlf cns pc cfs wrp zoo heart ionosphere soybean glass vote heart stat lymph labor diabetes breast credit segment horse colic anneal statistically significant improvement degradation www cs waikato ac nz ml weka experimenter 
table wins versus losses accuracy attribute selection naive bayes 
scheme wins wins losses losses wrp cfs cns ig rlf nb pc table shows results attribute selection naive bayes 
table shows method performs significantly better denoted worse denoted performing feature selection column 
speak results significantly different difference statistically significant level paired sided test 
table seen best result wrapper improves naive bayes data sets degrades 
cfs second best improvement datasets degradation 
simple information gain technique ig results improvements degradations 
consistency method cns improves naive bayes data sets degrades 
relieff gives better performance data sets degrades performance 
principal components comes worst improvement data sets degradation 
table ranks attribute selection schemes 
pairwise comparison scheme 
number times scheme significantly accurate recorded schemes ranked total number wins minus losses 
table seen wrapper clearly best wins losses schemes 
cfs consistency method schemes wins losses 
table results attribute selection data set ig cfs cns rlf wrp pc zoo heart stat ionosphere diabetes vote credit soybean heart glass labor lymph breast segment anneal horse colic statistically significant improvement degradation table wins versus losses accuracy attribute selection scheme wins wins losses losses rlf cns cfs wrp ig pc table shows results attribute selection table shows wins minus losses ranking scheme compared 
results somewhat different naive bayes 
best scheme relieff improves performance data sets degrades 
top ranking wins losses schemes 
consistency scheme ranked higher feature selection improves performance data sets degrades performance data sets 
cfs wrapper tied fourth ranking 
cfs improves performance data sets scheme degrades performance datasets 
wrapper improves performance datasets degrades performance 
success relieff consistency attributable ability identify attribute interactions dependencies 
including strongly interacting attributes reduced subset increases likelihood discover interactions early tree construction data fragmented 
naive bayes hand unable interacting attributes attribute independence assumption 
reasons account poorer performance wrapper 
nature search forward selection generate ranking fail identify strong attribute interactions early result attributes involved ranked highly 
second reason wrapper attribute evaluation fold cross validation training data 
cross validation entails setting aside training data evaluation result data available building model 
table size trees produced attribute selection 
data set ig cfs cns rlf wrp pc zoo heart stat ionosphere diabetes vote credit soybean heart glass labor lymph breast segment anneal horse colic statistically significant improvement degradation table wins versus losses tree size scheme wins wins losses losses pc cfs ig rlf cns wrp table compares size number nodes trees produced attribute selection scheme size trees produced attribute selection 
smaller trees preferred easier 
table ranking table seen principal components produces smallest trees accuracy generally degraded clear models transformed attributes necessarily fit data 
cfs second ranking produces smaller trees data sets larger tree dataset 
information gain relieff wrapper produce smaller trees data sets large produce larger trees cfs 
consistency produces smaller trees data sets produces larger tree 
appears quite low ranking generally produces slightly larger trees schemes 
table number features selected naive bayes 
figures brackets show percentage original features retained 
data set orig ig rlf cns pc cfs wrp zoo heart ionosphere soybean glass vote heart stat lymph labor diabetes breast credit segment horse colic anneal table wins versus losses number features selected naive bayes 
scheme wins wins losses losses cfs ig wrp rlf cns pc table shows average number attributes selected scheme naive bayes table shows wins versus losses ranking 
table shows schemes exception principal components reduce number features average 
principal components increases number features 
table seen cfs chooses fewer features compared schemes retaining attributes average 
wrapper clear winner accuracy third ranking retaining just attributes average 
table shows average number features selected scheme table shows wins versus losses ranking 
expected fewer features retained schemes naive bayes 
cfs wrapper retain features average 
relieff winner accuracy retains features average 
case naive bayes cfs chooses fewer features schemes table 
relieff bottom ranking table larger feature set sizes justified higher accuracy schemes 
table number features selected 
figures brackets show percentage original features retained 
data set orig ig cfs cns rlf wrp pc zoo heart stat ionosphere diabetes vote credit soybean heart glass labor lymph breast segment anneal table wins versus losses number features selected scheme wins wins losses losses cfs wrp cns ig pc rlf interesting compare speed attribute selection techniques 
measured time taken milliseconds select final subset attributes 
includes time taken generate ranking time taken cross validate ranking determine best set features 
table shows wins versus losses ranking time taken select attributes naive bayes 
cfs information gain faster schemes 
expected wrapper far slowest scheme 
principal components slow probably due extra data set pre processing fact initial dimensionality increases multi valued discrete attributes 
rough measure 
obtaining true cpu time java program quite difficult 
table wins versus losses time taken select attributes naive bayes 
scheme wins wins losses losses cfs ig cns rlf pc wrp table wins versus losses time taken select attributes scheme wins wins losses losses cns ig cfs rlf pc wrp table ranks schemes time taken select attributes 
interesting note consistency method fastest case 
consistency rank attributes fast information gain speed gains product quality ranking produced faster cross validate ranking poor 
smaller trees produced pruning performed early ranking best attributes 
poorer attributes ranked near top may harder produce tree 
effect naive bayes model induction speed affected attribute quality 
relieff produces best attribute rankings fast information gain 
instance nature algorithm quite slow produce attribute ranking 
benchmark comparison attribute selection techniques produce ranked lists attributes 
benchmark shows general attribute selection beneficial improving performance common learning algorithms 
shows learning algorithms single best approach situations 
needed data miner understanding different attribute selection techniques strengths weaknesses target learning algorithm background knowledge data available 
factors considered choosing attribute selection technique particular application 
example wrapper forward selection search suited naive bayes backward elimination search better identifying attribute interactions suitable 
results suggest general recommendations 
wins versus losses tables show accuracy wrapper best attribute selection scheme speed issue 
cfs consistency relieff performers 
cfs chooses fewer features faster produces smaller trees strong attribute interactions learning scheme consistency relieff better choice 
almuallim dietterich :10.1.1.111.7659
learning irrelevant features 
proceedings ninth national conference artificial intelligence pages 
aaai press 
blake keogh merz 
uci repository machine learning data bases 
university california department information computer science irvine ca 
www 
ics uci edu mlearn mlrepository html 
dumais platt heckerman sahami :10.1.1.161.6020
inductive learning algorithms representations text categorization 
proceedings international conference information knowledge management pages 
fayyad irani 
multi interval discretisation attributes 
proceedings thirteenth international joint conference artificial intelligence pages 
morgan kaufmann 
hall :10.1.1.149.3848
correlation feature selection machine learning 
phd thesis department computer science university waikato hamilton new zealand 
kira rendell 
practical approach feature selection 
proceedings ninth international conference machine learning pages 
morgan kaufmann 
ron kohavi george john :10.1.1.30.525
wrappers feature subset selection 
artificial intelligence 
kononenko :10.1.1.51.6297
estimating attributes analysis extensions relief 
proceedings seventh european conference machine learning pages 
springer verlag 
liu setiono 
probabilistic approach feature selection filter solution 
proceedings th international conference machine learning pages 
morgan kaufmann 
quinlan 
programs machine learning 
morgan kaufmann san mateo ca 
