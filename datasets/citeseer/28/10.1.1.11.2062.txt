ieee transactions signal processing vol 
october online learning kernels kivinen alexander smola robert williamson member ieee kernel algorithms support vector machines achieved considerable success various problems batch setting training data available advance 
support vector machines combine called kernel trick large margin idea 
little methods online setting suitable real time applications 
consider online learning reproducing kernel hilbert space 
considering classical stochastic gradient descent feature space straightforward tricks develop simple computationally efficient algorithms wide range problems classification regression novelty detection 
addition allowing exploitation kernel trick online setting examine value large margins classification online setting drifting target 
derive worst case loss bounds show convergence hypothesis risk functional 
experimental results support theory illustrating power new algorithms online novelty detection 
index terms reproducing kernel hilbert spaces stochastic gradient descent large margin classifiers tracking novelty detection condition monitoring classification regression 
kernel methods proven successful batch settings support vector machines gaussian processes regularization networks :10.1.1.11.2062
whilst apply batch algorithms utilising sliding buffer better online algorithm 
extension kernel methods online settings data arrives sequentially proven provide hitherto unsolved challenges 
challenges online kernel algorithms standard online settings linear methods danger overfitting applied estimator hilbert space method high dimensionality weight vectors 
handled regularisation exploitation prior probabilities function space gaussian process view taken 
october function algorithm similar obtain similar loss bounds obtained 
advantages large margin classifier allows track changing distributions efficiently 
context gaussian processes alternative theoretical framework develop kernel algorithms related 
key difference algorithm opper repeatedly project low dimensional subspace computationally costly requiring matrix multiplication 
considered tracking arbitrary linear classifiers variant winnow warmuth studied tracking small set experts posterior distributions :10.1.1.41.3139
note whilst originally developed online algorithm sequential minimal optimization smo algorithm closely related especially bias term case effectively perceptron algorithm 
outline section ii develop idea stochastic gradient descent hilbert space 
provides basis algorithms 
subsequently show general form algorithm applied problems classification novelty detection regression section iii 
establish mistake bounds moving targets linear large margin classification algorithms section iv 
proof stochastic gradient algorithm converges minimum risk functional section conclude experimental results discussion sections vi vii 
ii 
stochastic gradient descent hilbert space consider problem function estimation goal learn mapping sequence 
xm ym examples xt yt assume exists loss function deviation estimates observed labels common loss functions include soft margin loss function logistic loss classification novelty detection quadratic loss absolute loss huber robust loss insensitive loss regression :10.1.1.41.3139
shall discuss section iii 
reason allowing range allows refinement evaluation learning result 
example classification interpret sgn prediction class confidence classification 
call output learning algorithm hypothesis denote set possible hypotheses assume reproducing kernel hilbert space rkhs :10.1.1.11.2062
xm ym examples xt yt assume exists loss function deviation estimates observed labels common loss functions include soft margin loss function logistic loss classification novelty detection quadratic loss absolute loss huber robust loss insensitive loss regression :10.1.1.41.3139
shall discuss section iii 
reason allowing range allows refinement evaluation learning result 
example classification interpret sgn prediction class confidence classification 
call output learning algorithm hypothesis denote set possible hypotheses assume reproducing kernel hilbert space rkhs :10.1.1.11.2062
means exists kernel dot product reproducing property closure span words linear combinations kernel functions 
inner product induces norm usual way interesting special case normal dot product corresponds learning linear functions varied function classes learned different kernels 
risk functionals batch learning typically assumed examples immediately available drawn independently distribution natural measure quality case expected risk 
unknown drawn standard approach minimise empirical risk xt yt :10.1.1.11.2062:10.1.1.11.2062
call output learning algorithm hypothesis denote set possible hypotheses assume reproducing kernel hilbert space rkhs :10.1.1.11.2062
means exists kernel dot product reproducing property closure span words linear combinations kernel functions 
inner product induces norm usual way interesting special case normal dot product corresponds learning linear functions varied function classes learned different kernels 
risk functionals batch learning typically assumed examples immediately available drawn independently distribution natural measure quality case expected risk 
unknown drawn standard approach minimise empirical risk xt yt :10.1.1.11.2062:10.1.1.11.2062
minimising may lead overfitting complex functions fit training data generalise unseen data 
way avoid complex functions minimising risk measure complexity sensible way :10.1.1.11.2062
constant needs chosen appropriately problem 
parameters example see write 
inner product induces norm usual way interesting special case normal dot product corresponds learning linear functions varied function classes learned different kernels 
risk functionals batch learning typically assumed examples immediately available drawn independently distribution natural measure quality case expected risk 
unknown drawn standard approach minimise empirical risk xt yt :10.1.1.11.2062:10.1.1.11.2062
minimising may lead overfitting complex functions fit training data generalise unseen data 
way avoid complex functions minimising risk measure complexity sensible way :10.1.1.11.2062
constant needs chosen appropriately problem 
parameters example see write 
interested online algorithms deal example time define instantaneous approximation instantaneous risk single example 
online setting interested online learning examples available desired learning algorithm produces sequence hypotheses 
applications discussed section necessary introduce additional parameters need updated 
refer somewhat loosely family algorithms norma 
iii 
applications general idea norma applied wide range problems 
utilise standard addition constant offset function expansion update bt bt xt yt classification ft bt binary classification :10.1.1.11.2062:10.1.1.11.2062
obvious loss function context yf 
loss incurred sgn correct prediction say mistake charge unit loss 
mistake loss function drawbacks fails take account margin yf considered measure confidence correct prediction non positive margin meaning actual mistake mistake loss discontinuous non convex unsuitable gradient algorithms 
order deal drawbacks main loss function classification soft margin loss max yf margin parameter 
estimate progress trial ft ft new hypothesis truncation 
write ft ft ft ft estimate write ft ft ft ft 
ft ft ft ft ft ft combining estimate get ft ft xt yt ft xt yt notice similarity 
rest follows proof theorem 
sch smola learning kernels :10.1.1.11.2062
cambridge ma mit press 
support vector machine techniques nonlinear equalization ieee transactions signal processing vol 
pp 
november 

tracking linear threshold concepts winnow proceedings th annual conference computational learning theory kivinen sloan eds 
berlin springer lnai july pp 

littlestone learning quickly irrelevant attributes abound new linear threshold algorithm machine learning vol :10.1.1.41.3139
pp 

warmuth tracking small set experts mixing past posteriors journal machine learning research vol 
pp 
