feature subset selection genetic algorithm yang vasant honavar artificial intelligence research group department computer science hall iowa state university ames ia cs iastate edu practical pattern classification knowledge discovery problems require selection subset attributes features larger set represent patterns classified 
due fact performance classifier usually induced learning algorithm cost classification sensitive choice features construct classifier 
exhaustive evaluation possible feature subsets usually infeasible practice large amount computational effort required 
genetic algorithms belong class randomized heuristic search techniques offer attractive approach find near optimal solutions optimization problems 
presents approach feature subset selection genetic algorithm 
advantages approach include ability accommodate multiple criteria accuracy cost classification feature selection process find feature subsets perform particular choices inductive learning algorithm construct pattern classifier 
conversely learned rules small number relevant features concise easier understand humans 
presents feature subset selection problem automated design pattern classifiers 
feature subset selection problem refers task identifying selecting useful subset features represent patterns larger set mutually redundant possibly irrelevant features different associated measurement costs risks 
example scenario significant practical interest task selecting subset clinical tests different financial cost diagnostic value associated risk performed part medical diagnosis task 
examples feature subset selection problem include large scale data mining applications power system control zhou construction user interest profiles text classification yang sensor subset selection design autonomous robots balakrishnan honavar :10.1.1.53.2895
rest organized follows section summarizes various approaches feature subset selection 
section describes approach uses genetic algorithm neural network pattern classifiers 
section explains implementation details experiments 
section presents results various experiments designed evaluate performance approach benchmark classification problems document classification task 
see sklansky langley dash liu surveys 
approaches involve searching optimal subset features criteria interest 
feature subset selection problem viewed special case feature weighting problem 
involves assigning real valued weight feature 
weight associated feature measures relevance significance classification task cost salzberg punch wettschereck :10.1.1.35.8359
restrict weights binary valued feature weighting problem reduces feature subset selection problem 
focus feature subset selection 
performance measure evaluate feature subset respect criteria interest cost accuracy resulting classifier 
feature subset selection problem essentially optimization problem involves searching space possible feature subsets identify optimal near optimal respect feature subset selection algorithms broadly classified categories characteristics search strategy employed 
example addition irrelevant features social security numbers medical records diagnosis task significantly generalization accuracy decision tree classifier quinlan 
furthermore feature subset selection techniques rely monotonicity performance criterion appear reasonably linear classifiers exhibit poor performance non linear classifiers neural networks ripley 
systematic search find feature subset consistent training data forward selection reliability measure reported schlimmer 
greedy hillclimbing procedures different sequential search methods obtaining generalization decision tree construction algorithms id quinlan proposed caruana freitag 
related john forward selection backward elimination minimize cross validation error decision tree classifiers quinlan kohavi kohavi hillclimbing best search feature subset selection decision tree classifiers :10.1.1.30.3875
koller koller sahami koller sahami forward selection backward elimination select feature subsumed remaining features determined markov blanket set features render selected feature conditionally independent remaining features constructing naive bayesian duda hart mitchell decision tree classifiers quinlan :10.1.1.155.2293
preset algorithm employs rough set theory pawlak select feature subset rank ordering features generate minimal decision tree 
class techniques feature subset selection probability error correlation features reported 
feature subset selection randomized search randomized algorithms motwani raghavan randomized probabilistic opposed deterministic steps sampling processes 
furthermore feature subset selection techniques rely monotonicity performance criterion appear reasonably linear classifiers exhibit poor performance non linear classifiers neural networks ripley 
systematic search find feature subset consistent training data forward selection reliability measure reported schlimmer 
greedy hillclimbing procedures different sequential search methods obtaining generalization decision tree construction algorithms id quinlan proposed caruana freitag 
related john forward selection backward elimination minimize cross validation error decision tree classifiers quinlan kohavi kohavi hillclimbing best search feature subset selection decision tree classifiers :10.1.1.30.3875
koller koller sahami koller sahami forward selection backward elimination select feature subsumed remaining features determined markov blanket set features render selected feature conditionally independent remaining features constructing naive bayesian duda hart mitchell decision tree classifiers quinlan :10.1.1.155.2293
preset algorithm employs rough set theory pawlak select feature subset rank ordering features generate minimal decision tree 
class techniques feature subset selection probability error correlation features reported 
feature subset selection randomized search randomized algorithms motwani raghavan randomized probabilistic opposed deterministic steps sampling processes 
researchers explored algorithms feature subset selection 
readily lend multiple selection criteria classification accuracy feature measurement cost 
particularly attractive design pattern classifiers practical scenarios 
filter wrapper approaches feature subset selection feature subset selection algorithms classified categories feature selection done independently learning algorithm construct classifier 
feature selection performed independently learning algorithm technique said follow filter approach 
said follow wrapper approach john :10.1.1.30.3875
filter approach generally computationally efficient wrapper approach major drawback optimal selection features may independent inductive representational biases learning algorithm construct classifier 
wrapper approach hand involves computational overhead evaluating candidate feature subsets executing selected learning algorithm dataset represented feature subset consideration 
feasible learning algorithm train classifier relatively fast 
summarizes filter wrapper approaches 
feasible learning algorithm train classifier relatively fast 
summarizes filter wrapper approaches 
approach feature subset selection proposed instance wrapper approach 
utilizes genetic algorithm feature subset selection 
feature subsets evaluated computing generalization accuracy optionally cost features neural network classifier constructed computationally efficient neural network learning algorithm called distal yang :10.1.1.53.2895
feature selection genetic algorithm neural network pattern classifiers feature subset selection context practical problems diagnosis presents instance multi criteria optimization problem 
multiple criteria optimized include accuracy classification cost risk associated classification turn depends selection features describe patterns 
genetic algorithms offer particularly attractive approach multi criteria optimization 
neural networks offer attractive framework design trainable pattern classifiers real world real time pattern classification tasks account potential parallelism feature subset selection genetic algorithm feature subset selection features wrapper approach algorithm learning feature subset features generation features evaluation feature subset selection filter approach performance algorithm learning feature subset optimal feature subset optimal performance approaches feature subset selection incorporation learning algorithm 
computational capabilities pattern classification abilities neural network depend architecture connectivity functions computed individual neurons setting parameters weights 
known multi layer networks non linear computing elements threshold neurons realize classification function oe oe finite set classes finite number discrete real valued attributes set real numbers finite set discrete values 
attributes non numeric nominal mapped numeric values appropriate coding scheme 
function computed neural network determined topology computations performed individual neurons designing neural network particular pattern classification task reduces determination network architecture number neurons connectivity types neurons linear sigmoid threshold feature subset selection genetic algorithm parameter weight values 
typically accomplished combination design priori knowledge inductive learning may modify things weights network architecture gallant honavar uhr honavar parekh honavar :10.1.1.16.9105
genetic algorithm wrapper approach feature subset selection neural network pattern classifiers practical considerations genetic algorithms offer attractive technique feature subset selection neural network pattern classifiers reasons mentioned 
faced difficulties approach practice 
traditional neural network learning algorithms backpropagation perform error gradient guided search suitable setting weights weight space determined user specified network architecture 
ad hoc choice network architecture inappropriately constrains search appropriate setting weights 
gradient learning algorithms mathematically founded unimodal search spaces get caught local minima error function 
complicate evaluation feature subset employed represent training patterns train neural networks 
due fact poor performance classifier due failure learning algorithm feature subset 
fortunately constructive neural network learning algorithms gallant honavar uhr honavar eliminate need ad hoc inappropriate priori choices network architectures potentially discover near minimal networks size commensurate complexity classification task implicitly specified training data 
new provably convergent relatively efficient constructive learning algorithms multi category real discrete valued pattern classification tasks begun appear literature yang parekh parekh yang honavar :10.1.1.53.2895:10.1.1.16.9105
algorithms demonstrated performance terms reduced network size learning time generalization number experiments artificial fairly large real world datasets 
honavar uhr parekh yang :10.1.1.53.2895:10.1.1.16.9105
exception distal yang time consuming iterative training algorithms setting weights neurons :10.1.1.53.2895
genetic algorithms feature subset selection design neural network pattern classifiers involves running genetic algorithm generations 
due fact poor performance classifier due failure learning algorithm feature subset 
fortunately constructive neural network learning algorithms gallant honavar uhr honavar eliminate need ad hoc inappropriate priori choices network architectures potentially discover near minimal networks size commensurate complexity classification task implicitly specified training data 
new provably convergent relatively efficient constructive learning algorithms multi category real discrete valued pattern classification tasks begun appear literature yang parekh parekh yang honavar :10.1.1.53.2895:10.1.1.16.9105
algorithms demonstrated performance terms reduced network size learning time generalization number experiments artificial fairly large real world datasets 
honavar uhr parekh yang :10.1.1.53.2895:10.1.1.16.9105
exception distal yang time consuming iterative training algorithms setting weights neurons :10.1.1.53.2895
genetic algorithms feature subset selection design neural network pattern classifiers involves running genetic algorithm generations 
generation evaluation individual feature subset requires training corresponding neural network computing accuracy cost 
evaluation performed individuals population 
fortunately constructive neural network learning algorithms gallant honavar uhr honavar eliminate need ad hoc inappropriate priori choices network architectures potentially discover near minimal networks size commensurate complexity classification task implicitly specified training data 
new provably convergent relatively efficient constructive learning algorithms multi category real discrete valued pattern classification tasks begun appear literature yang parekh parekh yang honavar :10.1.1.53.2895:10.1.1.16.9105
algorithms demonstrated performance terms reduced network size learning time generalization number experiments artificial fairly large real world datasets 
honavar uhr parekh yang :10.1.1.53.2895:10.1.1.16.9105
exception distal yang time consuming iterative training algorithms setting weights neurons :10.1.1.53.2895
genetic algorithms feature subset selection design neural network pattern classifiers involves running genetic algorithm generations 
generation evaluation individual feature subset requires training corresponding neural network computing accuracy cost 
evaluation performed individuals population 
feasible computationally expensive iterative weight update algorithms training neural network classifiers evaluating candidate feature subsets 
generation evaluation individual feature subset requires training corresponding neural network computing accuracy cost 
evaluation performed individuals population 
feasible computationally expensive iterative weight update algorithms training neural network classifiers evaluating candidate feature subsets 
background distal offers attractive approach training neural networks 
distal fast algorithm constructing neural network pattern classifiers distal yang simple relatively fast constructive neural network learning algorithm pattern classification :10.1.1.53.2895
results experiments neural networks constructed distal 
key idea distal add hidden neurons time greedy strategy ensures hidden neuron correctly classifies maximal subset training patterns belonging single class 
correctly classified examples eliminated consideration 
process terminates pattern set empty network correctly classifies entire training set 
fact possible set weights hidden output neuron generate pool apply initial population genetic operators evaluate feature candidate subsets feature new pool rank selection individual best candidate subsets values fitness distal feature subset selection genetic algorithm distal 
starting initial population candidates having different feature subsets new populations generated repeatedly previous ones applying genetic operators crossover mutation selected parents evaluating fitness values offsprings distal ranking fitness values 
best individual obtained generation 
connections going iterative process 
straightforward show distal guaranteed converge classification accuracy finite training set time polynomial number training patterns yang :10.1.1.53.2895
experiments reported yang show distal despite simplicity yields classifiers compare quite favorably generated sophisticated substantially computationally demanding learning algorithms :10.1.1.53.2895
distal attractive choice experimenting evolutionary approaches feature subset selection neural network pattern classifiers 
key steps approach shown 
implementation details explained earlier genetic algorithm search optimization problem requires choice representation encoding candidate solutions manipulated genetic algorithm definition fitness function evaluate candidate solutions definition selection scheme fitness proportionate selection definition suitable genetic operators transform candidate solutions explore search space setting user controlled parameters probability applying particular genetic operator size population experiments run genetic algorithm goldberg mitchell rank selection strategy 
starting initial population candidates having different feature subsets new populations generated repeatedly previous ones applying genetic operators crossover mutation selected parents evaluating fitness values offsprings distal ranking fitness values 
best individual obtained generation 
connections going iterative process 
straightforward show distal guaranteed converge classification accuracy finite training set time polynomial number training patterns yang :10.1.1.53.2895
experiments reported yang show distal despite simplicity yields classifiers compare quite favorably generated sophisticated substantially computationally demanding learning algorithms :10.1.1.53.2895
distal attractive choice experimenting evolutionary approaches feature subset selection neural network pattern classifiers 
key steps approach shown 
implementation details explained earlier genetic algorithm search optimization problem requires choice representation encoding candidate solutions manipulated genetic algorithm definition fitness function evaluate candidate solutions definition selection scheme fitness proportionate selection definition suitable genetic operators transform candidate solutions explore search space setting user controlled parameters probability applying particular genetic operator size population experiments run genetic algorithm goldberg mitchell rank selection strategy 
probability selection highest ranked individual user specified parameter second highest ranked individual gamma third highest ranked individual gamma ranked individual gamma sum probabilities selection individuals 
document datasets 
abstracts chosen different sources ieee expert magazine journal artificial intelligence research neural computation 
news articles obtained reuters dataset 
document represented form vector numeric weights words terms vocabulary 
weights correspond term frequency inverse document frequency tfidf salton mcgill yang values corresponding words :10.1.1.53.2895
training sets abstracts generated classification corresponding documents classes interesting interesting different individuals resulting different data sets classifications news articles topics classes koller sahami resulting different datasets reuters reuters feature subset selection genetic algorithm reuters respectively 
datasets summarized table 
datasets measurement costs features experiments document datasets focused identifying minimal subset features yield high accuracy neural network classifiers 
experimental results different sets experiments run explore performance 
clearly outperformed plain distal features parity problem sense successfully selected important features giving generalization 
remaining datasets improvement generalization ranged modest cases marginal 
best individual generated outperformed distal datasets 
number features selected significantly smaller total number features original data representation datasets 
table compares results results ga non ga approaches available literature liu setiono liu setiono kohavi kohavi koller sahami koller sahami :10.1.1.155.2293
indicates result reported corresponding 
results indicate gave higher generalization accuracy techniques comparable accuracy cases vehicle dataset occasionally selected features 
produced feature subsets larger number features approach koller sahami koller sahami reuters datasets :10.1.1.155.2293
explained feature subsets genetic algorithm datasets relatively large number features set number features select priori 
number features selected significantly smaller total number features original data representation datasets 
table compares results results ga non ga approaches available literature liu setiono liu setiono kohavi kohavi koller sahami koller sahami :10.1.1.155.2293
indicates result reported corresponding 
results indicate gave higher generalization accuracy techniques comparable accuracy cases vehicle dataset occasionally selected features 
produced feature subsets larger number features approach koller sahami koller sahami reuters datasets :10.1.1.155.2293
explained feature subsets genetic algorithm datasets relatively large number features set number features select priori 
noted generally feasible completely fair thorough comparison different approaches complete knowledge parameters set experiments 
table comparison neural network pattern classifiers constructed distal entire set features best network constructed ga selected subsets features randomly partitioned datasets 
features number features accuracy generalization accuracy obtained neural networks 
summary discussion approach feature subset selection genetic algorithm neural network pattern classifiers proposed 
fast inter pattern distance constructive neural network algorithm distal employed evaluate fitness terms generalization accuracy candidate feature subsets genetic algorithm 
results indicate genetic algorithms offer attractive approach solving feature subset selection problem different cost performance constraints inductive learning pattern classifiers general neural network pattern classifiers particular 
table comparison various approaches feature subset selection 
column non ga shows best performance non ga approaches cited section liu setiono liu setiono kohavi kohavi koller sahami koller sahami second column adhoc shows performance reported column shows performance approach :10.1.1.155.2293
non ga adhoc dataset features accuracy features accuracy features accuracy annealing cancer crx glass heart hepatitis horse pima sonar vehicle votes reuters reuters reuters table comparison performance neural network pattern classifiers constructed features selected accuracy vs features selected accuracy cost randomly partitioned datasets 
accuracy accuracy cost dataset features accuracy cost features accuracy cost hepatitis pima table comparison performance neural network pattern classifiers constructed features selected accuracy vs features selected accuracy cost datasets arranged fold cross validation 
accuracy accuracy cost dataset features accuracy cost features accuracy cost hepatitis pima ga approach feature subset selection rely monotonicity assumptions traditional approaches feature selection limits applicability real world classification knowledge acquisition tasks 
offers natural approach feature subset selection account distribution available data 
