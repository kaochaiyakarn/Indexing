probabilistic visual learning object representation moghaddam alex pentland institute technology unsupervised technique visual learning density estimation high dimensional spaces eigenspace decomposition 
types density estimates derived modeling training data multivariate gaussian unimodal distributions mixture gaussians model multimodal distributions 
probability densities formulate maximum likelihood estimation framework visual search target detection automatic object recognition coding 
learning technique applied probabilistic visual modeling detection recognition coding human faces non rigid objects hands 

visual attention process restricting higher level processing subset visual field referred focus attention foa 
computer vision eigenvector analysis imagery characterization human faces automatic face recognition eigenfaces 
principal component analysis imagery applied robust target detection nonlinear image interpolation visual learning object recognition visual servoing robotics 
specifically murase nayar low dimensional parametric eigenspace recovering object identity pose matching views spline 
nayar extended technique visual feedback control servoing robotic arm peg hole insertion tasks 
pentland proposed view multiple eigenspace technique face recognition varying pose detection description facial features :10.1.1.47.3791
similarly burl bayesian classification object detection feature vector derived principal component images 
weng proposed visual learning framework klt conjunction optimal linear discriminant transform learning recognition objects views 
authors exception eigenvector analysis primarily dimensionality reduction technique subsequent modeling interpolation classification 
contrast method uses eigenspace decomposition integral part efficient technique probability density estimation high dimensional data 
longer simple mahalanobis distance interpreted distance relating gamma log 
density estimated parametric mixture model 
specifically model arbitrarily complex densities mixture gaussians yj theta nc sigma sigma dimensional gaussian density mean vector covariance sigma mixing parameters components satisfying 
mixture completely specified parameter theta sigma nc training set fy mixture parameters esti 
probabilistic visual learning mated ml principle theta argmax theta estimation problem best solved expectation maximization em algorithm consists step iterative procedure ffl step sigma nc sigma ffl step nc sigma gamma gamma step computes posteriori probabilities expectations missing component labels denote membership th component :10.1.1.133.4884
expectations computed step maximizes joint likelihood data missing variables 
em algorithm monotonically convergent likelihood guaranteed find local maximum total likelihood training set 
details em algorithm estimation mixture densities 
operating assumptions training data dimensional resides solely principal subspace exception perturbations due white gaussian measurement noise equivalently space component data separable gaussian density estimate complete likelihood function xj omega gamma xj omega gamma yj theta xj omega gamma xj omega gamma gaussian component density dffs 

probabilistic visual learning darrell pentland space time gestures proc 
ieee conf 
computer vision pattern recognition new york ny june 
dempster laird rubin maximum likelihood incomplete data em algorithm journal royal statistical society vol :10.1.1.133.4884

golub van loan matrix computations johns hopkins press 
hu visual pattern recognition moment invariants ieee trans 
information theory vol 
