comparative study feature selection text categorization yiming yang school computer science carnegie mellon university pittsburgh pa usa yiming cs cmu edu jan pedersen verity ross dr ca usa verity com comparative study feature selection methods statistical learning text categorization 
focus aggressive dimensionality reduction 
methods evaluated including term selection document frequency df information gain ig mutual information mi test chi term strength ts 
ig chi effective experiments 
ig thresholding neighbor classifier reuters corpus removal removal unique terms yielded improved classification accuracy measured average precision df thresholding performed similarly 
strong correlations df ig chi values term 
contrast mi relatively poor performance due bias favoring rare terms sensitivity probability estimation errors 
text categorization problem automatically assigning predefined categories free text documents 
textual information available online effective retrieval difficult indexing summarization document content 
documents categorization solution problem 
growing number statistical classification methods machine learning techniques applied text categorization years including multivariate regression models nearest neighbor classification bayes probabilistic approaches decision trees neural networks symbolic rule learning inductive learning algorithms :10.1.1.54.6608:10.1.1.49.860:10.1.1.49.860:10.1.1.39.6139:10.1.1.18.9415
major characteristic difficulty text categorization problems high dimensionality feature space 
native feature space consists unique terms words phrases occur documents tens hundreds thousands terms moderate sized text collection 
prohibitively high learning algorithms 
neural networks example handle large number input nodes 
bayes belief models example computationally intractable independence assumption true features imposed 
highly desirable reduce native space sacrificing categorization accuracy 
desirable achieve goal automatically manual definition construction features required 
automatic feature selection methods include removal non informative terms corpus statistics construction new features combine lower level features terms higherlevel orthogonal dimensions 
lewis ringuette information gain measure aggressively reduce document vocabulary naive bayes model decision tree approach binary classification :10.1.1.49.860
wiener mutual information statistic select features input neural networks :10.1.1.21.466:10.1.1.54.6608
yang schutze principal component analysis find orthogonal dimensions vector space documents :10.1.1.159.7380
yang wilbur document clustering techniques estimate probabilistic term strength reduce variables linear regression nearest neighbor classification 
moulinier inductive learning algorithm obtain features tive normal form news story categorization 
highly desirable reduce native space sacrificing categorization accuracy 
desirable achieve goal automatically manual definition construction features required 
automatic feature selection methods include removal non informative terms corpus statistics construction new features combine lower level features terms higherlevel orthogonal dimensions 
lewis ringuette information gain measure aggressively reduce document vocabulary naive bayes model decision tree approach binary classification :10.1.1.49.860
wiener mutual information statistic select features input neural networks :10.1.1.21.466:10.1.1.54.6608
yang schutze principal component analysis find orthogonal dimensions vector space documents :10.1.1.159.7380
yang wilbur document clustering techniques estimate probabilistic term strength reduce variables linear regression nearest neighbor classification 
moulinier inductive learning algorithm obtain features tive normal form news story categorization 
lang minimum description length principle select terms netnews categorization 
desirable achieve goal automatically manual definition construction features required 
automatic feature selection methods include removal non informative terms corpus statistics construction new features combine lower level features terms higherlevel orthogonal dimensions 
lewis ringuette information gain measure aggressively reduce document vocabulary naive bayes model decision tree approach binary classification :10.1.1.49.860
wiener mutual information statistic select features input neural networks :10.1.1.21.466:10.1.1.54.6608
yang schutze principal component analysis find orthogonal dimensions vector space documents :10.1.1.159.7380
yang wilbur document clustering techniques estimate probabilistic term strength reduce variables linear regression nearest neighbor classification 
moulinier inductive learning algorithm obtain features tive normal form news story categorization 
lang minimum description length principle select terms netnews categorization 
feature selection techniques tried thorough evaluations rarely carried large text categorization problems 
classifiers scaled target space thousands tens thousands categories 
seek answers questions empirical evidence ffl strengths weaknesses existing feature selection methods applied text categorization ffl extend feature selection improve accuracy classifier 
document vocabulary reduced losing useful information category prediction 
section describes term selection methods 
due space limitations include phrase selection approaches principal component analysis :10.1.1.54.6608:10.1.1.159.7380
section describes classifiers document corpus chosen empirical validation 
section presents experiments results 
section discusses major findings 
section summarizes 
re examine assumption respect text categorization tasks 
information gain ig information gain frequently employed criterion field machine learning 
measures number bits information obtained category prediction knowing presence absence term document 
fc denote set categories target space 
information gain term defined gamma log jt log jt log definition general employed binary classification models :10.1.1.49.860
general form text categorization problems usually ary category space may tens thousands need measure goodness term globally respect categories average 
training corpus unique term computed information gain removed feature space terms information gain predetermined threshold 
computation includes estimation conditional probabilities category term entropy computations definition 
probability estimation time complexity space complexity number training documents vocabulary size 
training corpus unique term computed information gain removed feature space terms information gain predetermined threshold 
computation includes estimation conditional probabilities category term entropy computations definition 
probability estimation time complexity space complexity number training documents vocabulary size 
entropy computations time complexity 
mutual information mi mutual information criterion commonly statistical language modelling word associations related applications :10.1.1.54.6608
considers contingency table term category number times occur number time occurs number times occurs total number documents mutual information criterion defined log theta estimated log theta theta natural value zero independent 
measure goodness term global feature selection combine scores term alternate ways avg max max fi mi computation time complexity similar ig computation 
weakness mutual information score strongly influenced marginal probabilities terms seen equivalent form log gamma log terms equal conditional probability rare terms higher score common terms 
scores comparable terms widely differing frequency 
category belongs multiple neighbors sum similarity scores neighbors weight category output 
category ranking llsf method regression model words document predict weights categories 
regression coefficients determined solving squares fit mapping training documents training categories 
properties knn llsf suitable experiments systems top performing state art classifiers 
evaluation classification methods reuters newswire collection section break point values knn llsf outperforming systems evaluated collection including symbolic rule learning ripper swap decision approach inductive learning sleeping experts typical information retrieval approach named rocchio :10.1.1.39.6139
variation reuters collection training set test set partitioned differently knn break point result neural networks llsf break point :10.1.1.54.6608
systems scale large classification problems 
large mean input output classifier thousands dimensions higher 
want examine degrees feature selection reduction removing standard words extremely aggressive reduction observe effects accuracy classifier entire target space 
category ranking llsf method regression model words document predict weights categories 
regression coefficients determined solving squares fit mapping training documents training categories 
properties knn llsf suitable experiments systems top performing state art classifiers 
evaluation classification methods reuters newswire collection section break point values knn llsf outperforming systems evaluated collection including symbolic rule learning ripper swap decision approach inductive learning sleeping experts typical information retrieval approach named rocchio :10.1.1.39.6139
variation reuters collection training set test set partitioned differently knn break point result neural networks llsf break point :10.1.1.54.6608
systems scale large classification problems 
large mean input output classifier thousands dimensions higher 
want examine degrees feature selection reduction removing standard words extremely aggressive reduction observe effects accuracy classifier entire target space 
examination need scalable system 
allows straight forward global evaluation document categorization performance measuring goodness category ranking document category performance standard applying binary classifiers problem 
classifiers context sensitive sense independence assumed input variables terms output variables categories 
llsf example optimizes mapping document categories treat words separately 
similarly knn treats document single point vector space 
context sensitivity distinction context free methods explicit independence assumptions naive bayes classifiers regression methods :10.1.1.49.860
context sensitive classifier better information provided features context free classifier enabling better observation feature selection 
classifiers differ statistically 
llsf linear parametric model knn non parametric non linear classifier assumptions input data 
evaluation classifiers reduce possibility classifier bias results 
evaluation classifiers reduce possibility classifier bias results 
data collections corpora study reuters collection ohsumed collection 
reuters news story collection commonly corpora text categorization research documents full collection half documents human assigned topic labels 
documents topic divided randomly training set test set documents 
partition similar employed differs full collection including newly revised version reuters available www research att com lewis :10.1.1.49.860:10.1.1.39.6139
documents stories mean length words standard deviation 
considered categories appear training set 
categories cover topics commodities interest rates foreign exchange 
documents fourteen assigned categories mean categories document 
displays performance curves llsf reuters 
training part llsf resource consuming approximation llsf number features unique words 
average precision knn vs unique word count ig df ts chi max mi max number features unique words 
average precision llsf vs unique word count ig df ts chi max mi max complete solution save computation resources experiments 
computed largest singular values solving llsf best results similar performance knn appeared singular values :10.1.1.159.7380
simplification llsf invalidate examination feature selection focus experiments 
observation merges categorization results knn llsf reuters 
ig df chi thresholding similar effects performance classifiers 
eliminate unique terms improvement loss categorization accuracy measured average precision 
ig chi computation quadratic measures expensive 
availability simple effective means aggressive feature space reduction may significantly ease application powerful computationally intensive learning methods neural networks large text categorization problems intractable 
jaime carbonell ralf brown cmu pointing implementation error computation information gain providing correct code 
tom mitchell machine learning group cmu fruitful discussions clarify definitions information gain mutual information literature 
apte damerau weiss :10.1.1.39.6139
language independent automated learning text categorization models 
proceedings th annual acm sigir conference 
kenneth ward church patrick hanks 
word association norms mutual information lexicography 
david lewis robert schapire james callan ron papka 
training algorithms linear text classifiers 
sigir proceedings th annual international acm sigir conference research development information retrieval 

lewis ringuette :10.1.1.49.860
comparison learning algorithms text categorization 
proceedings third annual symposium document analysis information retrieval 
tom mitchell 
machine learning 
th ann int acm sigir conference research development information retrieval sigir pages 
hartman 
automatic indexing bayesian inference networks 
proc th ann int acm sigir conference research development information retrieval sigir pages 
wiener pedersen weigend :10.1.1.54.6608
neural network approach topic spotting 
proceedings fourth annual symposium document analysis information retrieval 
wilbur 
automatic identification words 
sci 
yang 
expert network effective efficient learning human decisions text categorization retrieval 
th ann int acm sigir conference research development information retrieval sigir pages 
yang :10.1.1.159.7380
noise reduction statistical approach text categorization 
proceedings th ann int acm sigir conference research development information retrieval sigir pages 
yang 
sampling strategies learning efficiency text categorization 
