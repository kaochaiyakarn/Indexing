mistake driven learning text categorization ido dagan dept math 
cs bar ilan university israel dagan cs 
ac 
il learning problems text processing domain map text space dimensions measured fea tures text words 
characteristic properties domain high dimensionality learned concepts instances reside sparsely feature space high variation number active features instance 
study mistake driven learning algo rithms typical task nature text categorization 
functions ex ist real weights wl wn real threshold sl sn iff 
particular functions include boolean tions conjunctions variables threshold functions 
win guaranteed find perfect separator exists appears fairly successful perfect separator 
algorithm independence assumptions features contrast parametric estimation techniques typically bayesian predictors commonly statistical nlp 
theoretical analysis shown algorithm exceptionally behavior presence irrelevant features noise target func tion changing time littlestone littlestone littlestone warmuth warmuth cal support claims littlestone gold ing roth blum :10.1.1.37.1595
key feature winnow mistake bound grows linearly number relevant features log total number features 
second important property mistake driven 
intuitively algorithm sensitive relationships features relation ships may go unnoticed algorithm counts accumulated separately attribute 
crucial analysis algo rithm empirically littlestone gold ing roth 
crucial analysis algo rithm empirically littlestone gold ing roth 
discussion holds versions winnow studied bal 
theoretical results differ slightly mistake bounds flavor 
major difference algorithms positive weights allowing negative weights plays sig role applied current domain discussed section 
winnow closely related served motivation collection works combining advice different experts littlestone warmuth cesa bianchi cesa bianchi :10.1.1.37.1595
features experts learning gorithm viewed algorithm learns combine classifications different experts optimal way 
additive update algorithm evaluate perceptron goes back rosenblatt 
algorithm known learn target linear function exists bounds perceptron convergence theorem duda hart may exponential opti mal mistake bound fairly simple functions kivinen warmuth 
refer warmuth thorough analysis multiplicative update algorithms versus additive update algorithms 
idea having wide separation clear separator appeal basic intuition 
thick separator impor tant documents ranked sim ply classified actual score pro duced classifier decision process 
reason fc score produced classifier fc evaluated document assumptions dependencies features probability doc ument relevant category prob function known sigmoid function decision region way scores far apart threshold value indicate decision significant probability 
formally weight vectors choose hyper plane largest sep parameter separating parameter defined largest value exists classifier defined weight vector positive examples negative fc 
implementation try find optimal done cortes vapnik determine heuristically :10.1.1.15.9362
order find thick separator modify gorithms update rule training phase follows single thresh old separate thresholds 
training say algorithm predicts mistake ex ample labeled positive score assigns example 
similarly say algorithm predicts score exceeds 
examples scores range con mistakes 
size improved results due reduction noise introduced irrelevant features 
fur ther investigation issue long version 
typically thirds features filtered category significantly reducing output representation size 
summary experimental results study described section determined version performs best experimented 
eventually selected version algorithm experts unigram cohen singer neural network wiener pedersen weigend rocchio rocchio ripper cohen singer decision trees lewis ringuette bayes lewis ringuette swap apte damerau weiss apte split lewis split na na table break points comparison :10.1.1.49.860
data split training set test set lewis split lewis documents training testing apte split apte damerau weiss training testing omitting documents topical category 
algorithm incorporates range mod square root occurrences fea ture strength discard features modification table 
compared version algorithms appeared literature complete reuters corpus 
table presents break points algorithms defined section 
rows table compare formance algo rithms resemble approach ex algorithm cohen singer neural network approach wiener weigend 
see section 
rocchio algorithm classical algo rithms tasks performs compared newly developed techniques lewis 
compared ripper algorithm cohen singer best results task negative tests simple decision tree learning sys tem bayesian classifier 
taken lewis ringuette evaluated lewis split :10.1.1.49.860
com learning system apte damerau weiss swap eval apte split 
results significantly outperform results appear table set features single words 
results know literature version experts algorithm cohen singer uses richer feature set sparse word trigrams outperforms result lewis split break point compared unigram na na version achieves apte split compared 
long version plan results algorithm richer feature set 
