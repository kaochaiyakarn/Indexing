ieee transactions neural networks vol pp 
subspace information criterion non quadratic regularizers model selection sparse klaus robert muller gmd 
berlin germany computational biology research center ku tokyo japan tokyo institute technology ku tokyo japan university am germany go jp og cs ac jp klaus gmd de non quadratic regularizers particular norm regularizer yield sparse solutions generalize 
propose generalized subspace information criterion gsic allows predict generalization error useful family regularizers 
show technical assumptions gsic asymptotically unbiased estimator generalization error 
gsic demonstrated performance experiments norm regularizer compare network information criterion cross validation relatively large sample cases 
give comparison method nic cross validation 
sec 
gives concluding remarks 
preliminaries linear regression problem target function approximated parametric model linear parameters 
assume target function contained parametric model principle select parameters feature selection techniques :10.1.1.123.8151
stage scheme complex algorithmic structure harder analyze 
regularizer appears simple 
subspace information criterion non quadratic regularizers nonlinear basis function parameter vector 
describe true parameter 
subspace information criterion non quadratic regularizers smola mangasarian scholkopf sparse kernel feature analysis tech 
rep university wisconsin data mining institute madison 
smola scholkopf sparse greedy matrix approximation machine learning proceedings icml pp 

jain mao statistical pattern recognition review ieee trans :10.1.1.123.8151
patt 
anal 
mach 
intell vol 
