learning lateral interactions feature binding sensory segmentation honda europe gmbh carl str main germany rd honda jp new approach supervised learning lateral interactions competitive layer model clm dynamic feature binding architecture 
method consistency conditions shown characterize attractor states linear threshold recurrent network 
set training examples learning problem formulated convex quadratic optimization problem lateral interaction weights 
efficient dimension reduction learning problem achieved linear superposition basis interactions 
show successful application method medical image segmentation problem microscope cell images 
feature binding proposed provide elegant solution strategies segmentation problem perception :10.1.1.21.3670
set training examples learning problem formulated convex quadratic optimization problem lateral interaction weights 
efficient dimension reduction learning problem achieved linear superposition basis interactions 
show successful application method medical image segmentation problem microscope cell images 
feature binding proposed provide elegant solution strategies segmentation problem perception :10.1.1.21.3670
lot feature binding models tried reproduce mechanisms gestalt laws visual perception connectedness continuation temporal synchronization spatial binding :10.1.1.21.3670
quite generally models grouping lateral interactions feature representing neurons characterize degree compatibility features 
currently approaches lateral interaction scheme chosen heuristically experimental data corresponding connection patterns visual cortex insufficient 
complex feature spaces heuristic approach infeasible raising question systematic learning methods lateral interactions 
mozer suggested supervised learning dynamic feature binding model complex valued directional units connections hidden units guiding grouping dynamics adapted recurrent backpropagation learning 
pelillo suggested supervised learning method compatibility coefficients relaxation labeling algorithms minimizing distance target labeling vector output iterating fixed number relaxation steps 
main problem multiple local minima arising highly nonlinear optimization problem 
results shown linear threshold lt networks provide interesting architectures combining properties digital selection analogue context sensitive amplification efficient hardware implementation options 
demonstrated properties learn winner take competition groups neurons lt network lateral inhibition 
clm binding model implemented large scale organized lt network shown leads consistency conditions characterizing binding states :10.1.1.21.3670
contribution show conditions formulate learning approach clm quadratic optimization problem 
section briefly introduce competitive layer binding model 
learning approach elaborated section 
section show application results approach cell segmentation problem give discussion final section 
contribution show conditions formulate learning approach clm quadratic optimization problem 
section briefly introduce competitive layer binding model 
learning approach elaborated section 
section show application results approach cell segmentation problem give discussion final section 
clm architecture clm consists set layers feature selective neurons see fig :10.1.1.21.3670

activity neuron position layer denoted column denotes set neuron activities sharing common position column particular feature associated described set parameters local edge elements characterized position orientation 
binding features represented columns expressed simultaneous activities share common layer neurons column equally driven external input represents significance detection feature preprocessing step 
input fed activities connection weight 
activity neuron position layer denoted column denotes set neuron activities sharing common position column particular feature associated described set parameters local edge elements characterized position orientation 
binding features represented columns expressed simultaneous activities share common layer neurons column equally driven external input represents significance detection feature preprocessing step 
input fed activities connection weight 
layer activities coupled lateral connections rr characterize degree compatibility features symmetric function feature parameters rr purpose layered arrangement clm enforce assignment input features layers dynamics contextual information stored lateral interactions 
unique assignment single layer realized winner take wta circuit uses mutual symmetric inhibitory interactions absolute strength neural activities share common column due wta coupling stable equilibrium state clm neuron layer active column :10.1.1.21.3670
number layers number active groups sufficiently layers active carry salient group 
combination inputs lateral vertical interactions combined standard linear threshold additive activity dynamics rr max 
large compared lateral weights rr single active neuron column reproduces input shown stable states satisfy consistency conditions rr rr express assignment feature layer highest lateral support :10.1.1.21.3670
rl lateral interaction vertical wta interaction input layer layer layer competitive layer model architecture see text description 
layer activities coupled lateral connections rr characterize degree compatibility features symmetric function feature parameters rr purpose layered arrangement clm enforce assignment input features layers dynamics contextual information stored lateral interactions 
unique assignment single layer realized winner take wta circuit uses mutual symmetric inhibitory interactions absolute strength neural activities share common column due wta coupling stable equilibrium state clm neuron layer active column :10.1.1.21.3670
number layers number active groups sufficiently layers active carry salient group 
combination inputs lateral vertical interactions combined standard linear threshold additive activity dynamics rr max 
large compared lateral weights rr single active neuron column reproduces input shown stable states satisfy consistency conditions rr rr express assignment feature layer highest lateral support :10.1.1.21.3670
rl lateral interaction vertical wta interaction input layer layer layer competitive layer model architecture see text description 
learning clm lateral interactions formulation learning problem 
task learning algorithm adapt lateral interactions interaction coefficients rr clm architecture performs appropriate segmentation labeled training data generalizes new test data 
assume training data consists set labeled training patterns pattern consists subset fr different features corresponding labels 
shows corresponding images tissue section containing cells courtesy schubert 
bottom row corresponding image patches displayed individual cell regions manually labeled obtain training data learning process 
image patches training vector consists list labeled edge features parameterized position image unit local edge orientation vector computed intensity gradient 
pixel image amounts set labeled edge features 
ground separating mechanism implemented clm cell segmentation application features labeled part cell obtain corresponding background label :10.1.1.21.3670
training pattern contains additional free layer enable learning algorithm generalize number layers 
lateral interaction adapted decomposed weighted basis components constant negative interaction features facilitates group separation ii self coupling interaction background layer determines background ground segmentation iii angular interaction limited range decomposed templates capturing interaction particular combination relative angles edges 
angular decomposition done discretization space orientations turning unit vector representation angular orientation variable 
achieve rotation invariance interaction dependent edge orientations relative manually labelled training patterns grouping results learning original images manually labeled training patterns 
compares optimized interaction field earlier heuristic lateral interactions contour grouping 
see detailed discussion 
performance learning approach investigated choosing small number manually labeled patterns training patterns 
training examples resulting inequalities fact incompatible rendering direct solution infeasible 
training completed minimizing new image patch selected test pattern clm grouping performed lateral interaction learned dynamical model described :10.1.1.21.3670
quadratic consistency optimization performed described previous section exploring free margin parameter 
set training patterns shown fig 
total features learning sweep takes minutes standard desktop computer 
typical segmentation results obtained quadratic consistency optimization approach shown margin 
neuron 
wang 
image segmentation oscillatory correlation 
neural computation 
:10.1.1.21.3670
ritter 
dynamical stability conditions recurrent neural networks piecewise linear transfer functions 
neural computation 
ritter :10.1.1.21.3670
:10.1.1.21.3670
ritter 
dynamical stability conditions recurrent neural networks piecewise linear transfer functions 
neural computation 
ritter :10.1.1.21.3670
competitive layer model feature binding sensory segmentation 
neural computation 

spatial feature binding learning competitive neural layer architectures 
