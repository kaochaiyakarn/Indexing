partially labeled classification markov random walks martin mit ai lab cambridge ma ai mit edu jaakkola mit ai lab cambridge ma ai mit edu classify large number unlabeled examples combine limited number labeled examples markov random walk representation unlabeled examples 
random walk representation exploits low dimensional structure data robust probabilistic manner 
develop compare estimation criteria algorithms suited representation 
includes particular multi way classification average margin criterion permits closed form solution 
time scale random walk representation set margin criterion favoring unambiguous classification 
extend basic regularization adapting time scales individual examples 
discuss techniques estimating unknown parameters maximum likelihood em maximum margin subject constraints 
em estimation estimation criterion conditional log likelihood labeled points log jk log ji jt ijk jt ijk fixed specific objective function jointly concave free parameters unique maximum value 
concavity guarantees optimization easily performed em algorithm 
ijk soft assignment component ijk ji jt ijk 
em algorithm iterates step ijk recomputed current estimates step update ijk ijk see :10.1.1.28.2163
margin estimation alternative discriminative formulation possible sensitive individual classification decisions product likelihoods 
define margin classifier labeled point class kd post jk post 
correct classification margin nonnegative classes kd zero correct class yk 
training find parameters maximize average margin labeled points forcing correctly classified 
case classes global margin parameter labeled points 
algorithm focuses attention site minimum margin unfortunately outlier 
tackled noisy non separable problems adding linear slack variable constraint arrive average margin criterion linearity 
average min margin training yields hard parameters 
risk overfitting controlled smooth representation regularized increasing time parameter regularization desired applied maximum entropy discrimination framework bias solution uniform values :10.1.1.18.3303
additional regularization resulted similar classification performance adds computational cost 
examples consider example classification markov random walks 
labeled unlabeled points pattern 
pattern manifold structure distances locally globally euclidean due curved arms 
local scale parameter trades emphasis shortest paths low effectively ignores distant points versus volume paths high 
smoothness random walk representation depends number transitions 
regularization parameter akin kernel width density estimator 
limiting case employ local neighborhood graph 
special case obtain kernel expansion representation squared euclidean distance :10.1.1.28.2163
points labeled obtain nearest neighbors classifier 
limiting case representation node flat distribution points connected component 
choose unsupervised heuristics mixing time reach stationary distribution mutual information 
appropriate depends classification task 
done maintaining choices explicitly including time scales parametric form ta 
unclear exponential decay desirable 
facilitate continuum limit analysis establish better correspondence underlying density construct neighborhood graph basis balls nearest neighbors 
authors gratefully acknowledge support telephone ntt nsf itr iis 
jaakkola :10.1.1.28.2163
kernel expansions unlabeled examples 
nips 
jaakkola jebara 
maximum entropy discrimination 
