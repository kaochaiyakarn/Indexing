minimally supervised morphological analysis multimodal alignment david yarowsky richard department computer science johns hopkins university baltimore md email yarowsky cs jhu edu presents corpus algorithm capable inducing inflectional morphological analyses regular highly irregular forms brought bring distributional patterns large monolingual text direct supervision 
algorithm combines original alignment models relative corpus frequency contextual similarity weighted string similarity incrementally retrained inflectional transduction probabilities 
starting paired inflection root examples training prior seeding legal morphological transformations accuracy induced analyses past tense test cases english exceeds set currently accuracy highly irregular forms accuracy forms exhibiting non concatenative 
task definition presents original successful algorithm nearly unsupervised induction inflectional morphological analyzers focus highly irregular forms typically handled morphology induction algorithms 
useful consider task separate steps estimate probabilistic alignment inflected forms root forms language train supervised morphological analysis learner weighted subset aligned pairs 
result step standalone analyzer probabilistic scoring component iteratively refine alignment step 
target output step inflection root mapping shown table optional columns giving hypothesized stem change suffix analysis part speech 
stem root change suffix inflection pos take ake ook took vbd take ing vbg take takes vbz take en taken vbn skip ed skipped vbd defy ed vbd defy vbz defy ing vbg gar vpi gar vpi ar amos vpi ien en vpi table transformational model sufficient languages morphologies 
remarkably productive indo european languages current form extended schema appropriate 
applications vocabulary list achieves sufficiently broad coverage alignment table effectively morphological analyzer simply table lookup independent necessary contextual ambiguity resolution 
generative analytical system step remains useful previously unseen words words typically quite regular difficult substance lemmatization problem captured large inflection pos root mapping table simple transducer handle residual forms 
course case agglutinative languages turkish finnish highly inflected languages czech sparse data issue 
languages quite practical degree inflectional morphological analysis generation viewed alignment task broad coverage wordlist 
discuss implementation stand probabilistic analyzer retraining process steps challenge large coverage inflection root alignment expressed step core 
required optional resources clarification task description morphological induction described assumes limited set optional available resources table table inflectional parts speech language list canonical suffixes part speech 
suffixes serve mnemonic tags pos labels obtain noisy set candidate examples part speech 
large unannotated text corpus 
list candidate noun verb adjective roots language typically obtainable dictionary rough mechanism identifying candidate parts speech remaining vocabulary aggregate models context tag sequence morphological analysis 
concurrent yarowsky focuses problem bootstrapping approximate tag probability distributions modelling relative word form occurrence probabilities indicative lexical contexts noun vbg predictive variables goal training models 
necessary select part speech word context provide estimate candidate tag distributions full corpus 
source candidate tag estimates unimportant lists quite noisy 
major function partially limit potential alignment space unrestricted word word alignments entire vocabulary 
current implementation assumes list consonants vowels language 
lists need exhaustive missing irregular suffixes captured stem change null suffix send sent similar representation take ake ook took 
essential execution algorithm list common function words language useful extraction context similarity features 
available various distance similarity tables generated algorithm previously studied languages useful seed information especially languages closely related spanish italian 
related rich tradition supervised unsupervised learning domain morphology 
rumelhart mcclelland pinker prince sproat performed early studies fully supervised learning english past tense paired training data approximate models phonological distance 
brent de marcken kazakov goldsmith focused problem unsupervised learning morphological systems essentially segmentation task yielding morphologically plausible statistically motivated partition stems affixes 
brent de marcken minimum description length framework primary goal inducing lexemes speech streams 
goldsmith specifically sought induce suffix paradigm classes null ed ing vs ed ing vs ed es ing vs ted tion raw text 
handling irregular words largely excluded goldsmith assumed strictly concatenative morphology models stem changes 
morphology induction languages turkish finnish presents problem similar parsing segmenting sentence long strings allowed relatively free affix order 
voutilainen approached problem finitestate framework 
done trigram tagger assumption concatenative model 
level model morphology koskenniemi extremely successful manually capturing morphological processes world languages 
context sensitive stem change models current partially inspired framework 
example level equivalent capturing happy er happier quite similar spirit function probabilistic model app er 
sought learn level rule set english english spanish part speech vb vbd vbz vbg vbn ed en canonical ing ed suffixes examples jump jumped jumps jumping jumped announce announced announces announcing announced training take took takes taken part speech vpi vpi vpi vpi vpi vpi canonical ar amos suffixes er es en ir table supervision aligned inflection root pairs extracted dictionaries 
single character insertion deletions allowed learned rules supported 
supervised learning approach applied directly aligned pairs induced 
nirenburg developed framework learn level morphological analyzers interactive supervision elicit build test loop boas project 
humans provide needed feedback regarding errors omissions 
applied polish model assumes concatenative morphology treats non concatenative irregular forms table lookup 
notable gap research literature induction analyzers irregular morphological processes including significant stem changing 
algorithm described directly addresses gap successfully inducing regular analyses supervision 
lemma alignment frequency similarity motivating dilemma approach morphological alignment question determines past tense sing sang 
pairing sing requires simple concatenation canonical suffix ed legal word vocabulary past tense 
irregular verbs true word occupying slot generated regular morphological rule large corpus filled spelling mistakes observed frequency errors havoc na alignment methods 
overcome problem 
relative corpus frequency useful evidence source 
observe table word collection newswire text relative frequency distribution sang sing indicates reasonably close frequency match sing ratio substantial disparity 
vbd vb vbd vb sang sing sing sang vbd vb ln vbd vb table simply looking close relative frequencies inflection candidate root inappropriate inflections relatively rare expected occur frequently root form 
order able rank sang sing sing candidates effectively necessary able quantify fits deviates expected frequency distributions 
simple non parametric statistics calculate probability particular vbd vb ratio examining frequently ratios similar range seen corpus 
illustrates histogram log ratios focus attention extrema 
histogram smoothed normalized approximation probability density function estimator quantify extent candidate log vbd vb log sang sing fits empirically motivated expectations 
relative position candidate pairings graph suggests estimator informative task ranking potential root inflection pairings 
estimating distributions presents problem true alignments frequency ratios inflections assumed known advance 
approximate distribution automatically log vbd vb take sing took take sang sing sang log vbd vb log vbd vb estimator rank potential vbd vb pairs english simplifying assumption frequency ratios inflections roots largely issue tense usage significantly different regular irregular morphological processes 
table illustrate simplifying assumption supported empirically 
despite large lemma frequency differences regular irregular english verbs relative tense ratios vbd vbg vb vb quite similar terms means density functions 
vbd vb vbg vb avg 
lemma freq regular irregular table regular verbs irregular verbs ln vbd vb distributional similarity regular irregular forms vbd vb initially approximate vbd vb ratios automatically extracted noisy set verb pairs exhibiting simple canonical ed suffix 
distribution re estimated alignments improve single function continues predict frequency ratios unaligned largely irregular word pairs observed frequency previously aligned largely regular ones 
furthermore just limited ratio posi predict expected frequency posi corpus 
expected frequency viable past tense candidate sing frequency inflectional variants 
assuming earlier iterations algorithm filled sing lemma slots vbg vbz table regular inflections vbd vbg vbd vz estimators 
shows histogram estimator log vbd vbg lemma vb vbd vbg vbz vbn word sing sing singing freq singing table took sang singing sang log vbd vbg log vbd vbg estimator rank potential vbd vbg matches english limited single estimator 
fact considerable robustness advantages gained average estimators especially highly inflected languages observed frequency counts may relatively small 
accomplish general framework estimate hidden variable total lemma frequency lf average observed posi frequency lf globally estimated model 
posi subsequent posi frequency estimations relative posi lf somewhat posi variant log distribu lf posi tion illustrated 
advantage consensus approach requires estimators especially important inflectional tagset grows quite large languages 
alternately conduct frequency distribution ranking experiments estimate predict frequency vbd overestimate relative true 
contrast distribution vbd vbz considerably noisy problems vbz forms confused plural nouns 
measure yields underestimate 
log vbd lf vbd log vbd estimator rank vbd potential vbd lemma matches english suffixes tags 
example log ed vbd ing yields similar estimator log vbg somewhat higher variance 
frequency alignment models informative highly inflected languages 
illustrates estimate empirical distribution vpi partof speech frequency ratios spanish estimator strongly favoring correct irregular alignment orthographically similar competitors 
log vpi log vpi estimator rank potential pairs spanish measure frees need part speech distribution estimation 
optional variant suffixes ed en exist canonical suffix set performance improved modeling distribution separately verbs observed distinct en forms relative distribution log ed ing log ed change somewhat substantially root cases 
know advance test verb belongs set 
initial frequency similarity score average estimators presence absence distinct variant form lemma subsequent iterations 
lemma alignment context similarity second powerful measure ranking potential alignments morphologically related forms contextual similarity candidate forms 
measure computed traditional cosine similarity vectors weighted filtered context features 
measure gives relatively high similarity semantically similar words sip drink rare synonyms exhibit similar idiosyncratic argument distributions selectional preferences inflectional variants word sip 
primary goal clustering inflectional variants verbs give predominant vector weight head noun objects subjects verbs 
maximally language independent approximated positions small set extremely simple regular expressions parts speech initially including pos residual content words cw aux neg det cw 
expressions clearly extract significant noise fail match legitimate contexts applied potentially unlimited monolingual corpus noise ratio tolerable 
ideally aim identify automatically set patterns appropriate language accomplished subsequent iterations algorithm previously extracted inflection root pairs testing set extraction patterns effective maximizing mean context similarity inflection root relative non pairs 
similar techniques weight relative importance contextual positions 
similar reasons useful subsequent iterations algorithm apply current analysis modules contextual feature sets 
effect condensing contextual signal removing potentially distracting correlations inflectional forms context 
important concept context similarity measures morphology differs word clustering measures need eliminate context words subject pronouns strongly correlate inflectional forms 
giving words weight cause different verbs person number appear similar different inflections verb 
filtering high distributional entropy context word help eliminate counter productive features 
lemma alignment weighted levenshtein distance third alignment similarity function considers stem edit distance weighted levenshtein measure 
morphological systems worldwide vowels vowel clusters relatively mutable morphological processes consonants general tend lower probability change inflection 
treating string edits equal cost matrix form shown table utilized initial distance costs initially set relatively arbitrary assignment reflecting tendency 
subsequent algorithm iterations proceed matrix reestimated empirically observed character stem change probabilities algorithm current best weighted alignments 
ue ue 
table optimally initial state matrix seeded values partially borrowed previously trained matrices related languages 
alternately initial distances set partially sensitive phonological similarities dist dist example particular distinction emerges readily iterative re estimation baseline model 
lemma alignment morphological transformation probabilities goal research extract accurate table inflection root alignments generalize mapping function generative probabilistic model 
section describes creation model context sensitive probability morphological transformation fourth alignment similarity measure 
iteration algorithm probabilistic mapping function trained table output previous iteration equivalent information table root inflection pairs optional part speech tags confidence scores suffix analysis 
output cluster observed stem changes variable length root context applied illustrated table 
root stem matching context change suffix count examples ray ed spray stray 
ay ed play spray 
oy ed annoy enjoy 
ey ed obey key 
fy ed 
ry ed carry 
dy ed 
ed carry 
ed spray 
ing carry spray 
ed dance 
ing dance take 
ing ke ake ook take shake 
ke ake oke wake ke ke de ay id lay pay id lay pay table note triple root suffix uniquely determine resulting inflection effectively compute inflection root suffix pos root suffix pos root suffix inflection os pos 
statistics shown table possible compute generation alignment probability inflection root suffix simple interpolated backoff model function relative sample size conditioning event root indicates final characters root 
inflection root suffix pos root suffix pos root suffix pos root suffix pos root suffix pos suffix pos backoff extent necessary 
furthermore note english inflections spanish stem changes observed adding suffixes independent part speech behaves pairs suffix analysis analysis generated deterministically removing longest matching canonical suffix inflection generating minimal transformation capturing remaining stem difference 
nouns verbs probabilities simplified deleting conditioning variable pos illustrated 
ed vbd ed vbd ed ify ed fy ed ed ed generalized variablelength context models full trie architecture allowing robust specialization long root contexts sample sizes sufficient 
baseline model morphological transformation probabilities iteration inflection root pairs available estimating models 
prior knowledge available regarding stem change probabilities assumption cost proportional previously described levenshtein distance cost change increasing geometrically distance root increases 
rate cost increase ultimately depends tendency language allow word internal spelling changes spanish arabic strongly favor changes point english 
model improvement iterative re estimation primary goal iterative retraining refine core morphological transformation model serves similarity models primary deliverable learning process 
subsequent iterations proceed probability models retrained output prior iteration weighting training example alignment confidence filtering changes minimum level support help reduce noise 
final stem change probabilities interpolation trained model pj initial baseline model described section root suffix pos suffix pj root suffix pos levenshtein distance models reestimated observed section context similarity model improved better self learned lemmatization modelled context words 
lemma alignment model combination pigeonhole principle shown empirically model sufficiently effective 
applied traditional classifier combination techniques merge model scores scaling achieve compatible dynamic range 
frequency levenshtein context similarity models retain equal relative weight training proceeds morphological transformation similarity model increases relative weight better trained 
table demonstrates combined measures action showing relative rankings candidate roots inflections took similarity models iteration columns 
consensus similarity measure iteration shown column 
note estimators independently ranked shake root iteration consensus choice correct 
final column table shows retrained similarity measure convergence 
training evidence confidently aligned pairs took take shake undertook undertake previous iterations probability ake ook increased significantly increasing confidence alignments convergence shown changing previously correct ranking cases 
final alignment constraint pursued pigeonhole principle 
principle suggests part speech root inflection multiple inflections part speech share root 
course exceptions tendency travelled traveled observed variant forms respected roots 
addition consensus similarity score shows average ranks candidate root inflection ranks candidate inflection root 
bidirectional average ranking score favors cases attraction root inflection mutual cases higher ranked competition exists root attentions effectively capturing weak form pigeonhole principle 
primary ranking criteria raw similarity score 
candidate roots english inflection took st iteration similarity context frequency levenshtein iteration similarity similarity similarity similarity similarity take take take toot toot take turn turn tell tool tool toot tell turn toe tong tool test tide talk take tone tong talk tower test top 
tone tie touch teach take tout candidate roots english inflection st iteration similarity context frequency levenshtein iteration similarity similarity similarity similarity similarity shake shake share shoot shake shoot shave ship shoot shoot ship shape shift shoe shock shatter shore shop shake short shock shop shower shake shop shout short shut shoot shut shout 
shove shun shock shoot show shake shore candidate roots spanish inflection st iteration similarity context frequency levenshtein iteration similarity similarity similarity similarity extent overlaps penalized depends course likelihood variant forms morphology spanish english probability seeing variant forms relatively small 
exploited pigeonhole principle ways simultaneously 
greedy algorithm candidates aligned order decreasing score choice root inflection taken inflection part speech algorithm continues free slot 
exception highest ranking free form orders magnitude lower choice choice alignment assumed correct variant form 
empirical evaluation current empirical evaluation focuses accuracy analyzing highly irregular past tense english verbs 
consistent prior empirical studies field evaluation performed test set inflected words including highly irregular inflections cases past tense formed simple concatenative inflections exhibiting non concatenative stem change elision 
execution test inflected form table analysis algorithm free consider alignment word corpus identified potential root verb part speech tagging process occurrence just roots test set 
challenging evaluation testing simple alignment accuracy clean extraneous entry free 
table shows performance investigated similarity measures 
frequency similarity fs enhanced levenshtein ls context similarity cs achieve accuracy respectively 
hypothesis measures model independent complementary evidence sources supported roughly additive combined accuracy 
final performance full converged cs fs ls ms model accuracy full test set accuracy inflections requiring analysis simple concatenative quite remarkable algorithm absolutely inflection root examples training data prior inventory stem changes available slight sta fact cases consensus ranking choice correct independent model choice wrong yielding small synergistic super additivity 
combination highly simple similarity iter words irregular concat 
concat 
models ations fs frequency sim iter ls levenshtein sim iter cs sim iter cs fs iter cs fs ls iter cs fs ls ms iter cs fs ls ms tistical bias favor shorter stem changes smaller levenshtein distance minimal search simplifying assumption models candidate alignments prefix 
starting point single character changes point equally iteration processes context shift context preceding consonant vowel emerge high probability appropriate contexts low probability 
inducing analyses highly irregular inflections course difficult context frequency models cs fs perform relatively typically higher frequency irregular forms cause context signatures better estimated observed frequency discrepancies typically significant 
table shows models perform randomly selected highly irregular forms correctly selected roots identified bold 
residual errors primarily types inflections went ate correct roots due different character 
largest class errors due pigeonhole principle strongly inflections sharing root 
despite relatively lower performance scores essentially unsupervised learning outperforms fully supervised rumelhart mcclelland past tense learning algorithm achieved accuracy forms trained verbs pairs 
previously noted case dream burned burnt higher probability analysis typically occupying root slot lower probability form typically forced seek alignment 
pigeonhole principle problematic alignment principles creates nearly problems fixes 
performance advan table remaining errors typically due sparse statistics lower frequency irregular forms 
mappings particularly difficult corpus frequency little data estimate context profile effectively discriminatory frequency profile 
enlarging raw corpus size improve performance cases 
original algorithm capable inducing accurate morphological analysis highly irregular verbs starting paired inflection root examples training prior seeding legal morphological transformations 
treating morphological analysis predominantly alignment task large corpus performing effective collaboration original similarity measures expected frequency distributions context morphologically weighted levenshtein similarity iteratively bootstrapped model stem change probabilities 
constitutes significant achievement previous approaches morphology acquisition focused unsupervised learning concatenative morphology added coverage irregular forms required fully supervised learning 
essentially unsupervised learning highly irregular forms quite novel 
collectively algorithm achieves accuracy highly irregular forms accuracy analyses requiring stem change outperforming prior fully supervised learning algorithm measures 
tage slightly favor misalignments avoided problems created cost approach borne heavily irregular verbs probabilistic model variant forms expected allowed necessary fix cases preserving advantages principle analyses regular verbs 
true cs fs ls ms cs fs ls cs fs ls word root score itr itr itr itr got get go go go go gut knew know know know know know know took take take take take take toot blow blow blow blow blow blow mate drew draw draw draw draw draw draw store wore wear wear wear wear wear wire came come come come come come come thought think think think think think think brought bring bring bring bring bring strive strive strive strive straddle strive stuck stick stick stick stick stabilize stock swept sweep sweep sweep sweep sweep swap shine shine shine shine shine shine wake wake wake wake wind wake close bore bear bear bear bar bear bare meant mean mean mean mean manage mount lent lend lend lend lend lend lend slit slit slight slight slow struck strike strike strike strike strike bought buy buy buy buy buy bound bit bite bite bite bite bet dove dive dive dive dive dash dive burnt burn burn went go want want want want want caught catch catch catch cut catch cough dealt deal deal deal deal disagree deal brent 
minimal generative models middle ground neurons triggers 
proceedings th annual conference cognitive science society pages 
brent 
efficient probabilistically sound algorithm segmentation word discovery 
machine learning pages 
yarowsky 
language independent minimally supervised induction lexical probabilities 
proceedings acl hong kong 
de marcken 
unsupervised language acquisition 
phd dissertation 
mit 
goldsmith 
unsupervised learning morphology natural language 
humanities uchicago edu faculty goldsmith linguistica html 

statistical morphological disambiguation agglutinative languages 
proceedings coling 
karttunen 
finite state constraints 
john goldsmith ed 
phonological rule pages 
chicago university chicago press 
kazakov 
unsupervised learning naive morphology genetic algorithms 
daelemans van den bosch eds 
table workshop notes ecml workshop empirical learning nlp tasks 
koskenniemi 
general computation model word form recognition production 
publication dept general linguistics 
helsinki university helsinki 
nirenburg 
practical bootstrapping morphological analyzers 
proceedings conference natural language learning bergen norway 
pinker prince 
language connectionism analysis parallel distributed processing model language acquisition 
pinker mehler eds connections symbols 
mit press 
rumelhart mcclelland 
learning past tense english verbs 
mcclelland rumelhart pdp research group parallel distributed processing explorations microstructure cognition volume 
mit press 

automatic acquisition level morphological rules 
proceedings fifth conference applied natural language processing washington pp 

voutilainen 
morphological disambiguation 
karlsson voutilainen eds 
constraint grammar language independent system parsing unrestricted text 
