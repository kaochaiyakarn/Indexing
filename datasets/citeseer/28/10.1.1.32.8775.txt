boosting applied word sense disambiguation gerard german research center lsi department 
universitat de upc 
barcelona 
lsi upc es 
schapire singer adaboost mh boosting algorithm applied word sense disambiguation wsd problem 
initial experiments set selected polysemous words show boosting approach surpasses naive bayes exemplar approaches represent state art accuracy supervised wsd 
despite wide range approaches investigated large effort devoted tackling problem fact date large scale broad coverage highly accurate word sense disambiguation system built 
successful current line research corpus approach statistical machine learning ml algorithms applied learn statistical models classifiers corpora order perform wsd 
generally supervised approaches learn previously semantically annotated corpus obtained better results unsupervised methods small sets selected highly ambiguous words artificial pseudo words 
research partially funded spanish research department project tic research department consolidated research group wordnet project fi 
standard ml algorithms supervised learning applied naive bayes exemplar learning decision lists neural networks mooney compared previously cited methods restricted domain including decision trees rule induction algorithms :10.1.1.14.5763:10.1.1.11.5417:10.1.1.14.9674:10.1.1.134.3024
unfortunately direct comparisons alternative methods identical test data 
commonly accepted naive bayes neural networks exemplar learning represent state art accuracy supervised wsd 
supervised methods suffer lack widely available semantically tagged corpora construct really broad coverage systems 
known knowledge acquisition bottleneck 
known knowledge acquisition bottleneck 
ng estimates manual annotation effort necessary build broad coverage semantically annotated corpus man years 
extremely high overhead supervision additionally serious overhead learning testing commonly algorithms scaling real size wsd problems explain supervised methods seriously questioned 
due fact works focused reducing acquisition cost need supervision corpus methods wsd 
consequently lines research design efficient example sampling methods lexical resources wordnet www search engines automatically obtain internet arbitrarily large samples word senses unsupervised em algorithms estimating statistical model parameters :10.1.1.14.13:10.1.1.14.1677:10.1.1.44.6158
belief body particular second line provides evidence opening acquisition bottleneck near 
reason worth investigating application new supervised ml methods better resolve wsd problem 
boosting algorithms 
main idea boosting algorithms combine simple moderately accurate hypotheses called weak classifiers single highly accurate classifier task hand 
reason worth investigating application new supervised ml methods better resolve wsd problem 
boosting algorithms 
main idea boosting algorithms combine simple moderately accurate hypotheses called weak classifiers single highly accurate classifier task hand 
weak classifiers trained sequentially conceptually trained examples difficult classify preceding weak classifiers 
adaboost mh algorithm applied generalization freund schapire adaboost algorithm theoretically experimentally studied extensively shown perform standard machine learning tasks standard machine learning algorithms weak learners :10.1.1.133.1040:10.1.1.33.353:10.1.1.49.2457:10.1.1.156.2440:10.1.1.131.1931
regarding natural language nl problems adaboost mh successfully applied part speech pos tagging prepositional phrase attachment disambiguation text categorization especially results :10.1.1.156.2440:10.1.1.134.3024
text categorization domain shares properties usual settings wsd high dimensionality typical features consist testing presence absence concrete words presence irrelevant highly dependent features fact learned concepts examples reside sparsely feature space 
application adaboost mh wsd promising choice 
noted apart excellent results obtained nl problems adaboost mh advantages theoretically founded easy implement 
boosting algorithms 
main idea boosting algorithms combine simple moderately accurate hypotheses called weak classifiers single highly accurate classifier task hand 
weak classifiers trained sequentially conceptually trained examples difficult classify preceding weak classifiers 
adaboost mh algorithm applied generalization freund schapire adaboost algorithm theoretically experimentally studied extensively shown perform standard machine learning tasks standard machine learning algorithms weak learners :10.1.1.133.1040:10.1.1.33.353:10.1.1.49.2457:10.1.1.156.2440:10.1.1.131.1931
regarding natural language nl problems adaboost mh successfully applied part speech pos tagging prepositional phrase attachment disambiguation text categorization especially results :10.1.1.156.2440:10.1.1.134.3024
text categorization domain shares properties usual settings wsd high dimensionality typical features consist testing presence absence concrete words presence irrelevant highly dependent features fact learned concepts examples reside sparsely feature space 
application adaboost mh wsd promising choice 
noted apart excellent results obtained nl problems adaboost mh advantages theoretically founded easy implement 
organized follows section devoted explain detail adaboost mh algorithm 
section describes domain application initial experiments performed reduced set words 
section alternatives explored accelerating learning process reducing feature space 
best alternative fully tested section 
section concludes outlines directions 
boosting algorithm adaboost mh section describes schapire singer adaboost mh algorithm multiclass multi label classification exactly notation authors :10.1.1.156.2440:10.1.1.134.3024
said purpose boosting find highly accurate classification rule combining weak hypotheses weak rules may moderately accurate 
assumed exists separate procedure called acquiring weak hypotheses 
boosting algorithm finds set weak hypotheses calling weak learner repeatedly series rounds 
weak hypotheses combined single rule called combined hypothesis 
implementation algorithm runs procedure adaboost mh set training examples initialize distribution mk get weak hypothesis theta update distribution exp gammay normalization factor chosen distribution return combined hypothesis adaboost mh fig 

adaboost mh algorithm exactly way explained sets reduced unique label combined hypothesis forced output unique label maximizes 
remains defined form 
schapire singer prove hamming loss adaboost mh algorithm training set normalization factor computed round upper bound guiding design algorithm attempts find weak hypothesis minimizes exp gammay weak hypotheses wsd simple weak hypotheses test value boolean predicate prediction value :10.1.1.156.2440
predicates described section form feature value previous word hospital 
formally predicate interest lies weak hypotheses predictions form ae holds jl real numbers 
predicate bearing minimization mind values jl calculated follows 
subset examples fraction training examples labels sign differs 
predicate bearing minimization mind values jl calculated follows 
subset examples fraction training examples labels sign differs 
predicate holds subset examples predicate hold 
predicate holds 
current distribution real numbers calculated possible label gamma jl jl jl gamma weight respect distribution training examples partition labelled shown minimized particular predicate choosing jl ln jl jl gamma settings imply jl jl gamma predicate chosen value smallest :10.1.1.156.2440
small zero values parameters jl cause jl predictions large infinite magnitude 
practice large predictions may cause numerical problems algorithm increase tendency overfit 
suggested smoothed values jl :10.1.1.156.2440:10.1.1.134.3024
applying boosting wsd corpus experiments boosting approach evaluated dso corpus containing semantically annotated occurrences nouns verbs 
predicate holds 
current distribution real numbers calculated possible label gamma jl jl jl gamma weight respect distribution training examples partition labelled shown minimized particular predicate choosing jl ln jl jl gamma settings imply jl jl gamma predicate chosen value smallest :10.1.1.156.2440
small zero values parameters jl cause jl predictions large infinite magnitude 
practice large predictions may cause numerical problems algorithm increase tendency overfit 
suggested smoothed values jl :10.1.1.156.2440:10.1.1.134.3024
applying boosting wsd corpus experiments boosting approach evaluated dso corpus containing semantically annotated occurrences nouns verbs 
correspond frequent ambiguous english words 
dso corpus collected ng colleagues available linguistic data consortium ldc experiments group words nouns verbs frequently appear related wsd literature selected :10.1.1.14.4304
words described left hand side table 
practice large predictions may cause numerical problems algorithm increase tendency overfit 
suggested smoothed values jl :10.1.1.156.2440:10.1.1.134.3024
applying boosting wsd corpus experiments boosting approach evaluated dso corpus containing semantically annotated occurrences nouns verbs 
correspond frequent ambiguous english words 
dso corpus collected ng colleagues available linguistic data consortium ldc experiments group words nouns verbs frequently appear related wsd literature selected :10.1.1.14.4304
words described left hand side table 
goal acquire classifier word row represents classification problem 
number classes senses ranges number training examples number attributes 
mfs column right hand side table shows percentage frequent sense word accuracy naive frequent sense classifier obtain 
mfs column right hand side table shows percentage frequent sense word accuracy naive frequent sense classifier obtain 
binary valued attributes describing examples correspond binarization features referring narrow linguistic context 
gamma gamma context consecutive words examples tagged set labels correspond minor changes senses wordnet 
ldc mail address ldc cis upenn edu word disambiguated 
features mentioned exactly gamma gamma gamma gamma gamma correspond collocations consecutive words :10.1.1.11.5417:10.1.1.11.5417
benchmark algorithms experimental methodology adaboost mh compared algorithms naive bayes nb 
naive bayesian classifier classical setting 
avoid effect zero counts estimating conditional probabilities model simple smoothing technique proposed :10.1.1.11.5417:10.1.1.11.5417
exemplar learning eb 
ldc mail address ldc cis upenn edu word disambiguated 
features mentioned exactly gamma gamma gamma gamma gamma correspond collocations consecutive words :10.1.1.11.5417:10.1.1.11.5417
benchmark algorithms experimental methodology adaboost mh compared algorithms naive bayes nb 
naive bayesian classifier classical setting 
avoid effect zero counts estimating conditional probabilities model simple smoothing technique proposed :10.1.1.11.5417:10.1.1.11.5417
exemplar learning eb 
implementation examples stored memory classification new example nn algorithm hamming distance measure closeness doing examples examined 
greater resulting sense weighted majority sense nearest neighbours example votes sense strength proportional closeness test example 
ties resolved favour frequent sense tied 
numerical information corresponding experiment included table 
table shows accuracy results detailed word nb eb eb ab ab sc best result word printed boldface 
seen cases best results correspond boosting algorithms 
comparing global results accuracies ab ab sc significantly greater methods 
note accuracies corresponding nb eb comparable suggested greater crucial making exemplar learning competitive wsd :10.1.1.11.5417:10.1.1.11.5417
making boosting practical wsd seen adaboost mh simple competitive algorithm wsd task 
achieves accuracy performance superior naive bayes exemplar algorithms tested 
adaboost mh drawback computational cost algorithm scale properly real wsd domains thousands words 
space time round requirements adaboost mh mk recall number training examples number senses including call weak learner 
