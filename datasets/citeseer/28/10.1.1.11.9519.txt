re examination text categorization methods reports controlled study statistical signi cance tests text categorization methods support vector machines svm nearest neighbor knn classi er neural network nnet approach linear fit llsf mapping naive bayes nb 
focus robustness methods dealing skewed category distribution performance function training set category frequency 
results show svm knn llsf signi cantly outperform nnet nb number positive training instances category small methods perform comparably categories su ciently common instances 
automated text categorization tc supervised learning task de ned assigning category labels pre de ned new documents likelihood suggested training set labelled documents 
raised open challenges statistical learning methods requiring empirical examination ectiveness solving real world problems high dimensional category distribution labelled documents 
topic spotting newswire stories example commonly investigated application domains tc literature 
results show svm knn llsf signi cantly outperform nnet nb number positive training instances category small methods perform comparably categories su ciently common instances 
automated text categorization tc supervised learning task de ned assigning category labels pre de ned new documents likelihood suggested training set labelled documents 
raised open challenges statistical learning methods requiring empirical examination ectiveness solving real world problems high dimensional category distribution labelled documents 
topic spotting newswire stories example commonly investigated application domains tc literature 
increasing number learning approaches including regression models nearest neighbor classi cation bayesian probabilistic approaches decision trees inductive rule learning neural networks line learning support vector machines :10.1.1.49.860:10.1.1.32.9956:10.1.1.109.2516:10.1.1.43.9670:10.1.1.39.6139:10.1.1.54.6608:10.1.1.11.6124:10.1.1.11.6124:10.1.1.46.1529:10.1.1.25.4340:10.1.1.14.6535
rich literature provides valuable information individual methods clear comparison di cult published results directly comparable 
example tell performance nnet wiener statistically signi cantly better worse permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage copies bear notice full citation page 
copy republish post servers redistribute lists requires prior specific permission fee 
sigir ca usa copyright acm 
rich literature provides valuable information individual methods clear comparison di cult published results directly comparable 
example tell performance nnet wiener statistically signi cantly better worse permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage copies bear notice full citation page 
copy republish post servers redistribute lists requires prior specific permission fee 
sigir ca usa copyright acm 
yiming yang xin liu school computer science carnegie mellon university pittsburgh pa usa www cs cmu edu yiming performance svm joachims di erent data collections evaluations methods statistical signi cance analysis conducted verify impact di erence data performance variation classi ers :10.1.1.11.6124
naive bayes classi ers exhibited relatively poor performance previous studies hand papers claimed nb methods perform surprisingly gaining popularity lately 
di cult understand claimed strengths naive bayes methods evaluations non comparable performance measures tested selected subsets benchmark collections top common categories total 
clear claimed precisely surprisingly mean statistically signi cant better methods statistically di erent top performing classi ers published worse best ones better expectation authors 
claims speci ed empirical evidence far 
commonly methods compared single score accuracy error rate averaged measure see section de nition category assignments documents 
single valued performance measure dominated classi er performance common categories rare categories depending performance computed micro averaging versus macro averaging 
matter performance average computed single score prohibits ne grained analysis respect training set frequencies categories 
address evaluation problems conducting controlled study known text categorization methods nnet svm nb knn llsf 
methods published relatively strong performance scores previous evaluations partial comparison directly compared controlled study thorough statistical signi cance analysis focus :10.1.1.109.2516:10.1.1.11.6124
speci cally contains new contributions provides directly comparable results methods new benchmark corpus reuters 
currently published results llsf nnet nb corpus full set categories available 
knn published results available lower results previous version collection :10.1.1.109.2516:10.1.1.11.6124
svm published results contain su cient details statistical signi cance analysis 
address evaluation problems conducting controlled study known text categorization methods nnet svm nb knn llsf 
methods published relatively strong performance scores previous evaluations partial comparison directly compared controlled study thorough statistical signi cance analysis focus :10.1.1.109.2516:10.1.1.11.6124
speci cally contains new contributions provides directly comparable results methods new benchmark corpus reuters 
currently published results llsf nnet nb corpus full set categories available 
knn published results available lower results previous version collection :10.1.1.109.2516:10.1.1.11.6124
svm published results contain su cient details statistical signi cance analysis 
proposes variety statistical signi cance tests di erent standard performance measures including measures category assignments average precision category ranking suggests way jointly tests cross method comparison 
observes performance classi er function training set category frequency analyzes robustness classi ers dealing skewed category distribution 
task corpus performance measures evaluation results comparable published results tc evaluations chose topic spotting newswire stories task reuters corpus data 
svm published results contain su cient details statistical signi cance analysis 
proposes variety statistical signi cance tests di erent standard performance measures including measures category assignments average precision category ranking suggests way jointly tests cross method comparison 
observes performance classi er function training set category frequency analyzes robustness classi ers dealing skewed category distribution 
task corpus performance measures evaluation results comparable published results tc evaluations chose topic spotting newswire stories task reuters corpus data 
corpus new benchmark lately tc evaluations re ned version older versions reuters reuters tc methods evaluated results older versions may directly comparable results new version :10.1.1.49.860:10.1.1.109.2516:10.1.1.39.6139:10.1.1.54.6608
version reuters obtained eliminating unlabelled documents selecting categories document training set test set 
process resulted categories training test sets 
eliminating documents belong categories obtained training set documents test set documents vocabulary unique words stemming word removal 
number categories document average 
decision line maximal margin 
data points dashed lines support vectors 
linearly separable data 
letting yi xi denote training set yi classi cation positive example negative example class svm problem nd satis es constraints xi yi xi yi vector norm minimized 
svm problem solved quadratic programming techniques :10.1.1.15.9362
algorithms solving linearly separable cases extended solving linearly non separable cases introducing soft margin hyperplanes mapping original data vectors higher dimensional space new features contains interaction terms original features data points new space linearly separable :10.1.1.15.9362
relatively cient implementations svm include sv light system joachims sequential minimal optimization smo algorithm platt :10.1.1.11.6124
interesting property svm decision surface determined data points ex distance decision plane 
points wk called support vectors ective elements training set points removed algorithm learn decision function 
data points dashed lines support vectors 
linearly separable data 
letting yi xi denote training set yi classi cation positive example negative example class svm problem nd satis es constraints xi yi xi yi vector norm minimized 
svm problem solved quadratic programming techniques :10.1.1.15.9362
algorithms solving linearly separable cases extended solving linearly non separable cases introducing soft margin hyperplanes mapping original data vectors higher dimensional space new features contains interaction terms original features data points new space linearly separable :10.1.1.15.9362
relatively cient implementations svm include sv light system joachims sequential minimal optimization smo algorithm platt :10.1.1.11.6124
interesting property svm decision surface determined data points ex distance decision plane 
points wk called support vectors ective elements training set points removed algorithm learn decision function 
property svm theoretically unique di erent methods knn llsf nnet nb data points training set optimize decision function 
linearly separable data 
letting yi xi denote training set yi classi cation positive example negative example class svm problem nd satis es constraints xi yi xi yi vector norm minimized 
svm problem solved quadratic programming techniques :10.1.1.15.9362
algorithms solving linearly separable cases extended solving linearly non separable cases introducing soft margin hyperplanes mapping original data vectors higher dimensional space new features contains interaction terms original features data points new space linearly separable :10.1.1.15.9362
relatively cient implementations svm include sv light system joachims sequential minimal optimization smo algorithm platt :10.1.1.11.6124
interesting property svm decision surface determined data points ex distance decision plane 
points wk called support vectors ective elements training set points removed algorithm learn decision function 
property svm theoretically unique di erent methods knn llsf nnet nb data points training set optimize decision function 
interesting know theoretical distinction leads signi cant performance di erences svm methods practice 
knn result reported lower knn results 
addressing points decided re test svm sv light system joachims version knn 
knn knn stands nearest neighbor classi cation wellknown statistical approach intensively studied pattern recognition decades 
knn applied text categorization early stages research 
top performing methods benchmark reuters corpus version apte set top performing methods include llsf yang decision trees boosting apte neural networks wiener :10.1.1.54.6608
knn algorithm quite simple test document system nds nearest neighbors training documents uses categories neighbors weight category candidates 
similarity score neighbor document test document weight categories neighbor document 
nearest neighbors share category neighbor weights category added resulting weighted sum likelihood score category respect test document 
sorting scores candidate categories ranked list obtained test document 
sense nnet approach exactly previous reported ones leave comparison research 
decided hidden layer nodes empirically chosen section 
implemented nnet system cient handling sparse document vectors 
nb naive bayes nb probabilistic classi ers commonly studied machine learning 
increasing number evaluations nb methods reuters published :10.1.1.21.988:10.1.1.49.860:10.1.1.46.1529
basic idea nb approaches joint probabilities words categories estimate probabilities categories document 
naive part nb methods assumption word independence conditional probability category assumed independent conditional probabilities words category 
assumption computation nb classi ers far cient exponential complexity non naive bayes approaches word combinations predictors 
versions nb classi ers 
basic idea nb approaches joint probabilities words categories estimate probabilities categories document 
naive part nb methods assumption word independence conditional probability category assumed independent conditional probabilities words category 
assumption computation nb classi ers far cient exponential complexity non naive bayes approaches word combinations predictors 
versions nb classi ers 
studies multinomial mixture model reported improved performance scores version commonly versions nb data collections including reuters :10.1.1.46.1529
improved model evaluated common categories common ones total categories reuters results allow complete comparison previously reported nb methods methods full set reuters categories 
confusing aspect evaluations nb non conventional accuracy measure proportion correct category assignments total assignments number test documents document assigned category 
narrowly de ned accuracy equivalent tothe standard precision category document assumption classi ers equivalent standard recall assuming document correct category 
equivalent standard de nition accuracy text categorization literature proportion correct assignments binary decisions category document pairs 
second annual conference innovative applications intelligence 
tokunaga 
cluster text categorization comparison category search strategies 
proceedings th ann int acm sigir conference onresearch development information retrieval sigir pages 
thorsten joachims :10.1.1.11.6124
text categorization support vector machines learning relevant features 
european conference machine learning ecml 
koller sahami 
hierarchically classifying documents words 
