machine learning kluwer academic publishers boston 
manufactured netherlands 
strength weak learnability robert schapire rs theory lcs mit edu mit laboratory computer science technology square cambridge ma 
addresses problem improving accuracy hypothesis output learning algorithm distribution free pac learning model 
concept class learnable strongly learnable access source examples unknown concept learner high probability able output hypothesis correct arbitrarily small fraction instances 
concept class weakly learnable learner produce hypothesis performs slightly better random guessing 
shown notions learnability equivalent 
method described converting weak learning algorithm achieves arbitrarily high accuracy 
construction may practical applications tool efficiently converting mediocre learning algorithm performs extremely 
addition construction interesting theoretical consequences including set general upper bounds complexity strong learning algorithm function allowed error keywords 
machine learning learning examples learnability theory pac learning polynomial time identification 
valiant pioneering interest called approximately correct pac model learning 
model learner tries identify unknown concept randomly chosen examples concept 
examples chosen fixed unknown arbitrary distribution space instances 
learner task find hypothesis prediction rule correctly classifies new instances positive negative examples concept 
high probability hypothesis correct arbitrarily small fraction instances 
inference task includes requirement output hypothesis specified form 
concerned representation independent model learning learner may output hypothesis classify instances polynomial time 
class concepts learnable strongly learnable exists polynomial time algorithm achieves low error high confidence concepts class 
weaker model learnability called weak learnability drops requirement learner able achieve arbitrarily high accuracy weak learning algorithm need output hypothesis performs slightly better inverse polynomial random guessing 
notion weak learnability introduced kearns valiant left open question notions strong weak learnability equivalent 
question termed hypothesis boosting problem showing notions equivalent requires method boosting low accuracy weak learning algorithm hypotheses 
schapire kearns considering hypothesis boosting problem gives convincing argument natural approach trying boost accuracy weak learning algorithm running procedure times majority vote output hypotheses 
kearns valiant show uniform distribution instance space monotone boolean functions weakly strongly learnable 
shows strong weak learnability equivalent certain restrictions placed instance space distribution 
implausible strong weak learning models prove inequivalent unrestricted distributions 
hypothesis boosting question answered affirmative 
main result proof surprising equivalence strong weak learnability 
result may significant applications tool proving concept class learnable suffice find algorithm correct say instances distributions 
alternatively negative form result says concept class learned accuracy hope slightly better guessing class distribution 
proof constructive explicit method described directly converting weak learning algorithm achieves arbitrary accuracy 
construction uses filtering modify distribution examples way force weak learning algorithm focus harder learn parts distribution 
distribution free nature learning model fully exploited 
immediate corollary main result equivalence strong group learnability 
group learning algorithm need output hypothesis capable classifying large groups instances positive negative 
notion group learnability considered kearns li pitt valiant shown equivalent weak learnability kearns valiant 
result extends haussler kearns littlestone warmuth prove equivalence numerous relaxations variations basic pac learning model weak group learnability added class equivalent learning models 
relevance main result number learning models considered 
interesting unexpected consequence construction proof strong learning algorithm outputting hypotheses length time evaluate depends allowed error modified output hypotheses length polynomial log 
learning algorithm converted hypotheses significantly complex error tolerance lowered 
put terms bound implies sequence labeled examples learnable concept sense efficiently compressed far compact form rule hypothesis consistent labels examples 
particular shown sample size compressed rule size poly logarithmic fact discrete case size output hypothesis entirely independent provides partial converse occam razor result blumer ehrenfeucht haussler warmuth stating existence compression algorithm implies learnability concept class 
complements results board pitt provide partial converse occam razor somewhat different flavor 
result yields strong bound sample size needed learn discrete concept class 
strength weak learnability bound size output hypothesis implies hardness learning concept class family small circuits 
example shows pattern languages class languages considered previously angluin assuming np poly poly 
hardness result cryptographic assumptions 
bound shows function computable polynomial size circuits exists distribution function domain function roughly approximated family small circuits 
addition bound hypothesis size construction implies set general bounds dependence time sample space complexity needed efficiently learn learnable concept class 
surprising proof exists learnable concept class efficient algorithm requiring space poly logarithmic size sample needed learn accuracy general means example far space required learn necessary store entire sample 
known learning algorithms exactly manner storing large sample finding hypothesis consistent implies dramatic savings memory class algorithms possibly cost requiring larger sample 
general complexity bounds implications line learning model 
model learner instance time series trials 
received learner tries predict true classification new instance attempting minimize number mistakes prediction errors 
translating bounds described line model shown learnable concept class exists line algorithm space requirements quite modest comparison number examples seen far 
particular space needed trials poly logarithmic space efficient line algorithms particular interest capture notion incremental algorithm forced limited memory explicitly generalize data observed 
results space efficiency batch line algorithms extend interested problem including floyd haussler 
particular results solve open problem proposed haussler littlestone warmuth 
interesting bound derived expected number mistakes trials 
shown concept class learnable exists line algorithm class expectation bounded polynomial log large expect extremely small fraction predictions incorrect 
result answers open question haussler littlestone warmuth significantly improves similar bound kearns constant ot 
preliminaries description distribution free learning model 
concept boolean function domain instances 
concept class family concepts 
decomposed subclasses cn indexed parameter un schapire concepts cn common domain xn 
assume instance xn encoded length bounded polynomial 
associate concept size typically measure length representation encoding scheme concepts example concept class consist functions computed boolean formulas 
case cn set functions computed boolean formula variables xn set assignments variables size concept length shortest boolean formula computes function learner assumed access source ex examples 
time oracle ex called instance randomly independently chosen fixed unknown arbitrary distribution oracle returns chosen instance label indicating value instance unknown target concept cn 
labeled instance called example 
assume ex runs unit time 
access ex learning algorithm runs time outputs hypothesis prediction rule 
restrictions exist possibly probabilistic polynomial time algorithm instance computes prediction write tr indicate probability predicate holding instances drawn xn distribution accommodate probabilistic hypotheses find useful regard bernoulli random variable 
example pr chance hypothesis may randomized misclassify particular instance contrast quantity probability misclassify instance chosen random distribution note probability taken random choice random bits general assuming independence pr tr pr tr ved probability instance chosen 
technically formula valid discrete 
handle general domains summation need replaced appropriate integral probability measure domain 
simplify presentation assume xn discrete omit extension results general domains extension simply mimics discrete case 
probability called error error say close target concept quantity accuracy say concept class learnable strongly learnable exists algorithm target concepts cn distributions xn algorithm parameters size access oracle ex runs time polynomial outputs hypothesis probability close equivalent notions learnability including polynomial predictability haussler kearns littlestone warmuth 
note authors term learnable mean slightly different 
strength weak learnability kearns valiant introduced weaker form learnability error necessarily arbitrarily small 
concept class weakly learnable exists polynomial algorithm target concepts cn distributions xn algorithm parameters size access oracle ex runs time polynomial outputs hypothesis probability close words weak learning algorithm produces prediction rule performs just slightly better random guessing 

equivalence strong weak learnability main result proof strong weak learnability equivalent notions 
theorem 
concept class weakly learnable strongly learnable 
strong learnability implies weak learnability trivial 
remainder section devoted proof converse 
assume concept class weakly learnable show build strong learning algorithm weak 
description technique accuracy algorithm boosted small significant amount 
show mechanism applied recursively error arbitrarily small 

hypothesis boosting mechanism algorithm produces high probability hypothesis close target concept sketch algorithm simulates different distributions outputs hypothesis significantly closer ex examples oracle distribution xn induced ex 
algorithm begins simulating original distribution oracle ex ex 
hi hypothesis output intuitively weak advantage original distribution advantage expressed hi 
force learn harder parts distribution destroy advantage 
creates new distribution instance chosen roughly equal chance correctly incorrectly classified 
distribution simulated filtering examples chosen ex 
simulate new examples oracle ex constructed 
asked instance ex flips fair coin result heads ex requests examples ex chosen hi ex waits instance chosen 
show prevent ex having wait long loops desired instance 
algorithm simulated time providing examples chosen ex 
output hypothesis 
schapire yj 
graph function 
constructed filtering instances hi agree 
third oracle ex simulates choice instance requesting instances ex hi 
show limit time spent waiting loop desired instance 
third time algorithm simulated examples drawn time ex producing hypothesis 
outputs hypothesis instance hz predicts agreed value predicts 
words takes majority vote 
show error bounded quantity significantly smaller original error seen graph depicted 
solid curve function comparison dotted line shows graph identity function 

strong learning algorithm idea follows naturally treat previously described procedure subroutine recursively boosting accuracy weaker hypotheses 
procedure desired error bound confidence parameter constructs close hypothesis weaker recursively computed hypotheses 
assumed weak learning algorithm find desired hypothesis close hypothesis computed recursively calling subroutine set 
unfortunately scheme quite due technical difficulty way ex ex constructed examples may required small portion original distribution 
happens time spent waiting example chosen region may great 
see strength weak learnability learn ex input error parameter confidence parameter examples oracle ex implicit size parameters return hypothesis close target concept probability procedure return ex ex ex hi learn ex rl estimate el hl choose sample sufficiently large lal rl probability return hi defun ex flip coin heads return instance ex ha return instance ex hi learn ex estimate choose sample sufficiently large probability return defun ex return instance ex hi learn ex defun hi bx return bl return return 
strong learning algorithm learn 
difficulty overcome explicitly checking errors hypotheses small 
shows detailed sketch resulting strong learning algorithm lea rn 
procedure takes error parameter confidence parameter provided examples oracle ex 
procedure required return hypothesis error probability polynomial weak lea ex assumed weak learning procedure outputs hypothesis close target concept probability 
schapire function ix variable set value 
quantities estimates errors hi distribution estimates error tolerances defined computed obvious manner samples drawn ex required size samples determined instance chernoff bounds 
parameters assumed known globally 
note lea procedure inputs function ex returning output function hypothesis treated procedure 
furthermore simulate new example oracles lea rn means dynamically defining new procedures allowed instance lisp languages 
somewhat nonstandard keyword defun denote definition new function syntax calls name procedure followed parenthesized list arguments body indented braces 
static scoping assumed 
lea works recursively boosting accuracy hypotheses 
lea typically calls times simulated example oracles described preceding section 
recursive call required error bound constructed hypotheses comes closer bound reaches weak learning algorithm rn 
procedure takes measures limit run time simulated oracles provides recursive calls 
lea calls second time find expected number iterations ex find example depends error hi estimated 
hi desired accuracy need find ha sufficiently hypothesis fl shown ex loop long find instance 
similarly lea rn calls find ha expected number iterations ex depends hi disagree see turn function error original distribution error estimated small hypothesis returned learn 
shown ex run long 

correctness show section algorithm correct sense theorem 
hypothesis returned calling lea rn ex close target concept probability 
proof 
proving theorem find useful assume goes wrong execution lea specifically say lea run hypothesis returned rn close target concept statistical estimate quantities obtained required accuracy 
argue inductively depth recursion lea rn run output hypothesis close target concept furthermore probability run facts clearly imply theorem statement 
strength weak learnability base case trivially handled assumptions 
general case inductive hypothesis fewer recursive calls lea rn runs probability 
estimates desired accuracy probability 
chance run chance events occur 
remains show run output hypothesis error easy special case fil smaller respectively 
case follows immediately due accuracy assumed estimated returned hypothesis close target concept 
general case sub hypotheses combined 
error distribution provided oracle ex distribution induced oracle ex ith recursive call 
inductive hypothesis 
special case hypotheses deterministic distributions depicted schematically shown 
shows portion distribution hypotheses agree target concept distribution top bar represents relative fraction instance space hi agrees bottom striped bar represents instances agrees valid deterministic hypotheses may helpful motivating intuition follows 
pi pr hi chance fixed instance misclassified 
recall hypotheses may randomized necessary consider probability particular fixed instance misclassified 
similarly pr chance classified differently hx 
define follows 
distributions 
schapire pr hi ved pr hl ved pr hl pr hl ved clearly pr hl boolean pr hl 
terms variables express explicitly chance ex returns instance di pl equation trivial 
see equation holds note chance initial coin flip comes tails chance instance instance misclassified hi 
case coin comes heads handled similar fashion derivation equation 
equation dz pz vex vc pl pz xn pl strength weak learnability note equation derived case deterministic hypotheses ifb shown hard see alb 
imply equation 
combining equations see values solved written explicitly terms values equation ready compute error output hypothesis pr red pr ved vex aa ce ot ot ot ot oz ot oo desired 
inequalities follow facts ai equation completes proof 

analysis section argue lea rn runs polynomial time 
section stated polynomial refers polynomial 
approach derive bound expected running time procedure part confidence bound high probability actual running schapire time algorithm 
shown procedure probably fast correct completing proof theorem 
technically show lea rn halts probabilistically techniques described haussler kearns littlestone warmuth procedure easily converted learning algorithm halts deterministically polynomial time 
interested bounding quantities 
course interested bounding expected running time lea rn ex 
running time turn depends time evaluate hypothesis returned lea rn expected number examples needed lea rn 
addition analogous quantities ex 
assumption polynomially bounded 
functions depend implicitly technical point note expectations denoted taken runs lea expectations computed assumption sub hypothesis estimator successfully computed desired accuracy 
theorem learn run probability 
important point respectively expected running time lea rn rn called oracle ex provides examples unit time 
analysis take account fact simulated oracles supplied lea rn rn lower levels recursion general run unit time 
see exponential depth recursion induced calling lea bounding depth 
smallest integer gi lip recursive call replaced 
depth recursion bounded 
lemma 
depth recursion induced calling learn ex log log log 
proof say gb ll lip gc clearly gi 
gc fig log similarly 
implies gi assuming gi 
gb ll lip flog remainder analysis clear context letb 
note fore lip 
show polynomially bounded 
important require returned hypothesis polynomially 
lemma 
time evaluate hypothesis returned lea rn ex sb 
proof ife lip learn returns hypothesis computed weaklearn 
case 
hypothesis returned lea rn involves computation sub hypotheses 
strength weak learnability positive constant straightforward induction argument shows recurrence implies bound bu 
example requested simulated oracle lea recursive calls oracle draw examples oracle ex 
instance third recursive call simulated oracle draw instances finds ha disagree 
naturally running time lea rn depends examples drawn manner simulated oracle 
lemma bounds quantity 
lemma 
expected number examples drawn ex oracle ex simulated lea rn run asked provide single example 
proof lea rn calls time find examples oracle ex passed left unchanged 
case 
second time lea rn calls constructed oracle exz loops time called receives desirable example 
depending result initial coin flip expect ex loop times 
note ra estimate aa lea rn simply returned ha making second third recursive call 
assume aa case 
lea rn calls third time expect constructed oracle ex loop times finding suitable example 
variables defined proof theorem 
remains show 
note error original distribution fact equations solve explicitly terms aa find aa aa aac aa aa aa regarding ot fixed refer function right hand side inequality asf 
lower bound find minimum interval 
derivative aa ot aa ao denominator derivative clearly zero aa numerator parabola centered line aa zero 
critical point interval oo 
furthermore oo aa oo single critical point range possibly schapire minimal 
means thatf minimum closed subinterval oo achieved endpoint subinterval 
particular subinterval interest function achieves minimum 
min 
assume od smaller quantity lea rn returned going compute 
bound fact cd 

conclude completing proof 
bound number examples needed estimate bounds tails binomial distribution angluin valiant hoeffding 
lemma 
chernoff bounds consider sequence independent bernoulli trials succeeding probability random variable describing total number successes 
hold pr rap pr mp mp pr mp mp 
lemma 
run expected number examples needed learn ex log proof base case learn simply calls weaklearn 
recursive calls simulated oracle required provide examples 
provide example simulated oracle draw average examples ex 
recursive call demands examples average 
strength weak learnability addition lea rn requires examples making estimates 
bound lemma follows sample size log suffices estimate 
note ot ct 
choice estimates log examples 
arrive recurrent inequality 
cp log positive constant complete proof argue inductively implies bound base case clearly satisfies bound 
general case equation implies inductive hypothesis 
og cp log 
og cp log log cp log log clearly implies equation 
inequality follows fact ot 
lemma 
run expected execution time learn ex log proof previous lemmas base case easily handled 
case 
lea rn takes time recursive calls 
addition lea rn spends time drawing examples estimates overhead time spent simulated examples oracles passed recursive calls 
typical example drawn learn oracle ex evaluated zero previously computed sub hypotheses 
instance example drawn purpose estimating evaluated hi example drawn simulated oracle ex evaluated 
lea rn overhead time proportional product schapire total number examples needed ea rn time takes evaluate examples 
recurrence holds ts applying lemmas implies ts log positive constant straightforward induction argument shows implies 
ts og ts 
main result section follows immediately theorem 

probability execution ea rn ex halts polynomial time outputs hypothesis close target concept 
proof 
theorem chance earn run 
markov inequality lemma chance ea rn run fails halt time 
probability ea rn run outputs close hypothesis halts polynomial time 

space complexity immediate consequence proof theorem worth pointing ea space requirements relatively modest proved section 
space earn ex space needed store output hypothesis space needed evaluate hypothesis 
analogous quantities ea rn ex 
lemma 
space required store hypothesis output lea ex 
space needed evaluate hypothesis 
total space required lea rn 
proof 
bounds trivial 
bound note hypothesis returned ea rn composite fewer hypotheses 

strength weak learnability evaluate composite hypothesis sub hypotheses evaluated time 

bound note space required lea rn dominated storage sub hypotheses recursive computation space needed evaluate 
sub hypotheses computed time 
solutions recurrences straightforward imply stated bounds 

improving lea rn time sample complexity section describe modification construction section significantly improves lea time sample complexity 
particular improve complexity measures roughly factor giving bounds linear ignoring log factors 
improved bounds interesting consequences described sections 
original construction lea rn time examples simulated oracles exi waiting desirable instance drawn 
lemma showed expected time spent waiting 
modification described reduce ot ee 

recall running time oracle ex depends error ha 
original construction ensured small estimating value smaller returning ha continuing normal execution subroutine 
approach guarantees way ensuring ex run time 
improve ex running time modify deliberately increasing error 
ironically intentional injection error effect improving lea rn worst case running time limiting time spent ex ex waiting suitable instance 
modifications specifically lea rn modified 
call new procedure lea rn 
recursive computation ha lea rn estimates error hi accurately learn 
aa estimate choose sample large lal hal probability 
oz assume loss generality od fi od 
lea rn defines new hypothesis follows instance hi flips coin biased turn heads probability exactly schapire ha ha outcome tails hi evaluates hi returns result 
heads predicts wrong answer 
training phase assume correct classification available simulated 
new hypothesis place ofh ex ex 
rest subroutine unmodified aside minor modification described lemma 
particular final returned hypothesis unchanged ha hi 
correctness see lea rn correct assume proof theorem run occurs case probability 
note error exactly chance error heads 
choice verified 
hypothesis lieu ha 
note related exactly way ha related original proof theorem 
imagine returned recursive call original procedure lea rn impossible returned second third recursive calls case returned hypothesis 
put way proof error ot identical copy proof theorem occurrences ha replaced 
shown error ofh 
pr pi theorem 
xn pr pa pr 
inequality follows observation thatp pl pa 
implies error error bounded 
analysis show lea rn runs faster fewer examples lea rn 
essentially analysis section 
lemmas modified versions lemmas 
proofs lemmas apply immediately learn little modification 
strength weak learnability lemma 
expected number examples drawn ex oracle exi simulated learn run asked provide single example 
proof original proof ex 
expect second oracle loop times average 
cd case 
bound number iterations ex shown equation original proof 
lower bound find minimum equation replaced course interval od 
noted previously achieve minimum endpoint interval 
assume original proof previously shown thatf od similar argument od op od 
completes proof 
lemma 
run expected number examples needed lea rn ex proof proof nearly lemma 
addition incorporating superior bound lemma number examples needed simulated oracles consider number examples needed estimate estimated sample size log log derived bound lemma noting 
estimating slightly different manner achieve better bound sample size needed 
specifically choose sample large probability ife ands ife estimate properties needed lea rn requires sample size log derived second third bound lemma 
see haussler littlestone warmuth detailed example sort calculation 
arrive recurrence implies stated bound argument similar proof lemma 
lemma 
run expected execution time lea rn ex log proof bound follows equation superior bound 
schapire 
variations learning model consider main result relates learning models 
group learning immediate consequence theorem concerns group learnability 
group learning model learner produces hypothesis need correctly classify large groups instances positive negative examples 
kearns valiant prove equivalence group learning weak learning 
theorem group learning equivalent strong learning 

miscellaneous pac models haussler kearns littlestone warmuth describe numerous variations basic pac model show equivalent 
instance consider randomized versus deterministic algorithms algorithms size target concept known unknown 
hard see equivalence proofs apply weak learning algorithms exception described weak learning models equivalent theorem basic pac learning model 
reduction hold weak learning algorithms concerns equivalence oracle learning models 
oracle model exclusively learner access single source positive negative examples 
oracle model learner access oracle returns positive examples returning negative examples 
authors show models equivalent strong learning algorithms 
proof apparently adapted show oracle weak learnability implies oracle weak learnability proof converse easily validly adapted 
proof assumes error arbitrarily small clearly bad assumption weak learning algorithms 
problem shown oracle weak learnability implies oracle strong learnability turn implies oracle strong weak learnability 
despite haussler original proof learning models equivalent 

fixed hypotheses pac learning research concerned form representation hypotheses output learning algorithm 
clearly construction described section general preserve form hypotheses weak learning algorithm 
natural ask exists construction preserving form 
concept class weakly learnable algorithm hypotheses strength weak learnability class representations exist strong learning algorithm outputs hypotheses 
general answer question modulo relatively weak complexity assumptions 
simple example consider problem learning term dnf formulas hypotheses represented term dnf 
formula disjunctive normal form dnf written disjunction terms conjunction literals literal variable complement 
pitt valiant show learning problem infeasible rp np small 
weak learning problem solved algorithm sketched 
similar algorithm kearns 
choose large sample 
significantly half examples sample negative positive output predict negative positive hypothesis halt 
assume distribution roughly evenly split positive negative examples 
select output disjunction fewer literals misclassifies positive examples fewest negative examples 
briefly argue hypothesis high probability fl ne close target concept 
note target term dnf formula equivalent cnf formula pitt valiant 
formula conjunctive normal form cnf written conjunction clauses clause disjunction literals 
clause consists literals formula observe clause satisfied assignment satisfies entire cnf formula 
formula clauses averaging argument clause satisfied fl assignments weighted target distribution satisfy entire formula 
exists disjunction literals correct nearly positive examples flo negative examples 
particular output hypothesis property 
distribution roughly evenly divided positive negative examples implies output hypothesis roughly nk close target formula 

queries number researchers considered learning scenarios learner able passively observe randomly selected examples able ask teacher various sorts questions queries target concept 
instance learner allowed ask particular instance positive negative example 
angluin describes kinds query useful learner 
purpose section simply point construction section applicable presence kinds query 
weak learning algorithm depends availability certain kinds query converted construction strong learning algorithm query types 

valued concepts considered boolean valued concepts concepts classify instance positive negative example 
course real world schapire learning tasks require classification categories instance character recognition 
result generalize handle valued concepts 
learning valued concept immediately clear define notion weak learnability 
hypothesis guesses randomly instance correct time natural definition require weak learning algorithm classify instances correctly slightly time 
unfortunately definition strong weak learnability inequivalent small 
informal example consider learning concept values suppose easy predict concept value hard predict concept value 
weakly learn concept suffices fmd hypothesis correct concept guesses randomly 
distribution hypothesis correct half time achieving weak learning criterion accuracy significantly better 
boosting accuracy clearly infeasible 
better definition weak learnability requiring hypothesis correct slightly half distribution regardless definition construction section easily modified handle valued concepts 

general complexity bounds pac learning construction derived sections yields unexpected relationships allowed error various complexity measures applied strong learning algorithm 
surprising proof learnable concept class exists efficient algorithm output hypotheses evaluated time polynomial log 
furthermore algorithm space requirements poly logarithmic far instance needed store entire sample 
addition time sample size requirements grow linearly disregarding log factors 
theorem 
learnable concept class exists efficient learning algorithm requires sample size pl log log halts time log log uses space log log outputs hypotheses size log time ps log polynomials pl ps proof strong learning algorithm hard wire converting weak learning algorithm outputs hypotheses close target concept 
strength weak learnability procedure obtained applying construction lea rn plugged ea rn 
remarked previously assume loss generality halts deterministically polynomial time 
note lemmas sections achieves resource bounds theorem problem bounds attained polynomial log desired 
problem alleviated applying construction haussler kearns littlestone warmuth converting learning algorithm running time polynomial log 
essentially construction works follows inputs simulate times time setting accuracy parameter confidence parameter 
save computed hypotheses 
draw sample log di examples output misclassifies fewest examples sample 
haussler argue resulting procedure outputs close hypothesis probability 
applying construction obtain final procedure verify achieves stated bounds 
remainder section discussion consequences theorem 

improving performance known algorithms bounds applied immediately number existing learning algorithms yielding improvements time space complexity terms 
instance computation time blumer ehrenfeucht haussler warmuth algorithm learning half spaces involves solution linear programming problem size proportional sample improved polynomial factor true baum algorithm learning unions half spaces involves finding convex hull significant fraction sample 
algorithms theorem implies improved space efficiency 
especially true known pac algorithms choosing large sample finding hypothesis consistent 
instance rivest decision list algorithm works algorithms described blumer helmbold sloan warmuth construction learning nested differences learnable concepts 
entire sample stored algorithms terribly space efficient dramatically improved applying theorem 
course improvements typically come cost requiring somewhat larger sample polynomial factor log 
appears tradeoff space sample size time complexity 

data compression blumer ehrenfeucht haussler warmuth considered relationship learning data compression 
shown sample compressed represented prediction rule significantly smaller original sample compression algorithm converted pac learning algorithm 
schapire sense bound theorem size output hypothesis implies converse 
particular suppose cn learnable concept class examples vl vl 
vm vm concept cn size examples need chosen random 
data compression problem find small representation data hypothesis significantly smaller original data set property vi vi hypothesis property said consistent sample 
theorem implies existence efficient algorithm outputs consistent hypotheses poly logarithmic size sample 
proved theorem theorem 
learnable concept class 
exists efficient algorithm distinct examples concept cn size outputs probability deterministic hypothesis consistent sample size polynomial log proof pitt valiant show convert learning algorithm finds hypotheses consistent set data points 
idea choose run learning algorithm simulated uniform distribution data set 
weight placed element sample output hypothesis error zero 
applying technique learning algorithm satisfying conditions theorem see output hypothesis size polynomial log far smaller original sample large technically technique requires learning algorithm output deterministic hypotheses 
probabilistic hypotheses handled choosing somewhat smaller value hard wiring computed probabilistic hypothesis sequence random bits 
precisely set run distribution 
assume run 
note output hypothesis regarded deterministic function instance sequence random bits chance randomly chosen sequence misclassifies instances sample 
chance certainly instance chosen simulated uniform distribution sample 
error choice implies words probability random sequence chosen correctly classifies examples 
choosing testing random sequences quickly find deterministic hypothesis consistent sample 
note size output hard wired hypothesis bounded ih poly logarithmic rl irl bounded time takes evaluate naturally notion size preceding theorem depends underlying model computation left unspecified 
theorem immediate corollaries learning problem discrete instance domain xn encoded finite alphabet string bounded polynomial concept size encoded finite alphabet string length bounded polynomial strength weak learnability corollary 
learnable discrete concept class 
exists efficient algorithm sample theorem outputs probability deterministic consistent hypothesis size polynomial independent proof assume loss generality points sample distinct size exceed xn 
log ix bounded polynomial corollary follows immediately 
applying occam razor blumer 
implies strong general bound sample size needed efficiently learn bound better theorem terms pointed improvement requires sacrifice space efficiency entire sample stored 
theorem 
learnable discrete concept class 
exists efficient learning algorithm requiring sample size polynomial proof blumer 
describe technique converting called occam algorithm property described corollary efficient learning algorithm stated sample complexity bound 
essentially conversion simply draws sample stated size appropriately runs sample find consistent hypothesis 
authors argue computed hypothesis simply virtue small size consistency sample close target concept high probability 
technically approach needs minor modifications handle instance randomized occam algorithm modifications straightforward 

hard functions hard learn theorem bound size output hypothesis implies hard evaluate concept class 
result sound surprising previously unclear proved learning algorithm hypotheses technically permitted grow polynomially learnability classes question 
result yields representation independent hardness results cryptographic assumptions 
instance assuming poly np poly class nondeterministic boolean circuits learnable 
set poly np poly consists languages accepted family polynomial size deterministic nondeterministic circuits 
furthermore learning pattern languages shown schapire hard learning np poly result shows pattern languages relatively weak structural assumption 
schapire theorem 
suppose learnable assume exists polynomial concepts size exists circuit size exactly computing proof 
consider set pairs xn 
corollary exists algorithm positive probability output hypothesis consistent set elements size polynomial hypothesis polynomially converted standard techniques circuit required size 

hard functions hard approximate similar argument bound hypothesis size implies function computable small circuits weakly approximated family small circuits distribution inputs 
boolean function distribution circuit variables 
cis said probability assignment chosen randomly theorem 
suppose function computed family circuits 
exists family distributions set polynomials exist infinitely exists variable circuit size approximates dn 
proof 
proof assume loss generality integer 
suppose exists distribution exists circuit size nk sense weakly learned 
precisely exists procedure searching exhaustively set circuits size find nk distribution theorem fis strongly learnable similar sense exponential time 
applying theorem validity depends size output hypothesis running time implies exactly computed family polynomial size circuits contradicting theorem hypothesis 
exists integer distribution circuit size able nk approximate complete proof suffices show implies theorem 
set distributions circuit size nk easy verify dff function computed exponential size circuits exist constant smallest 
preceding argument exist 
furthermore implies set finite cardinality 
strength weak learnability eliminate repeated elements ki di defined follows kj di distribution empty definition define arbitrarily 
desired ky family hard distributions 
integer ki dn ki iki ki 
proves theorem 
informally theorem states language complexity class poly weakly approximated language poly hard family distributions 
fact theorem easily modified apply circuit classes including monotone poly monotone non monotone nc fixed 
class nc consists languages accepted polynomial size circuits depth log monotone circuit negated variables appear 
general theorem applies circuit classes closed transformation hypotheses resulting construction section 

line learning consider implications theorem line learning algorithms 
online learning model learner randomly selected instance time series trials 
told correct classification learner try predict instance positive negative example 
incorrect prediction called mistake 
model learner goal minimize number mistakes 
previously haussler littlestone shown concept class learnable exists line learning algorithm properties probability mistake th trial worst linear constant equivalently expected number mistakes trials worst linear constant 
result described kearns 
noting examples learning algorithms second bound grows poly logarithmically authors ask learnable concept class algorithm attaining bound 
theorem answers open question affirmatively showing general expected number mistakes trials need grow polynomial log expect minute fraction predictions incorrect 
result confused haussler littlestone warmuth 
authors describe general algorithm applicable wide collection concept classes show expected number mistakes algorithm trials linear log algorithm requires exponential computation time known concept class schapire learnable 
contrast theorem states concept class learnable exists efficient algorithm making poly logarithmic mistakes average trials 
haussler littlestone warmuth consider space efficiency line learning algorithms 
space efficient learning algorithm space requirements trials exceed polynomial log space efficient algorithm far memory required store explicitly preceding observations 
authors describe number space efficient algorithms unable find learning unions axis parallel rectangles plane led ask exist space efficient algorithms learnable concept classes 
surprisingly open question answered proved theorem 
lastly theorem gives bound computational complexity line learning terms 
particular total computation time required process examples proportional log constant sense amortized average computation time th trial poly logarithmic 
fact careful analysis show true worst case computation time th trial 
theorem 
learnable concept class 
exists efficient line learning algorithm properties probability mistake th trial log expected number mistakes trials log total computation time required trials log space trials log polynomials proof learnable exists efficient batch algorithm satisfying properties theorem 
algorithm el substituted 
chance output hypothesis incorrectly classifies randomly chosen instance 
technique haussler kearns littlestone warmuth 
fix re number examples needed theorem re constant value implicitly bounded polynomial 
verified 
examples suffice find hypothesis chance error 
convert line learning algorithm manner preserves time space efficiency imagine breaking sequence trials blocks increasing size block consists trials new block twice size 
general th block size si ip consists trials ai bi strength weak learnability trials th block algorithm simulated compute th hypothesis specifically simulated set si bounds probability misclassifies new instance 
note instances available block compute hypothesis desired accuracy 
block st hypothesis computed predictions block discarded hi takes place 
th trial occurs th block bi probability mistake bounded si error rate hi definition implies desired bound probability mistake th trial turn expected number mistakes trials 
note th block space needed store hypothesis block hi simulate computation block hypothesis 
theorem quantities grow polynomially log 
choice implies desired bound algorithm space efficiency 
time complexity procedure bounded similar fashion 

open problems shown model learnability learner required perform slightly better guessing strong model learner error arbitrarily small 
proof result filtering distribution manner causing weak learning algorithm eventually learn nearly entire distribution 
shown proof implies set general bounds complexity pac learning batch line discussed applications bounds 
hoped results open way new method algorithm design pac learning 
previously mentioned vast majority currently known algorithms finding hypothesis consistent large sample 
alternative approach suggested main result seek hypothesis covering slightly half distribution 
hypothesis easier find point view algorithm designer 
approach leads algorithms flavor similar described term dnf section possible find similar algorithms number concept classes known learnable example decision lists rivest rank decision trees ehrenfeucht haussler 
extent approach fruitful classes presently known learnable 
open question 
open problem concerns robustness construction described 
intuitively close relationship reducing error hypothesis overcoming noise data 
valid intuition 
construction modified handle noise 
turning away theoretical side machine learning ask construction perform practice 
learning problem instance neural network designed implemented empirically achieve error rate way seen improving program enable achieve great error schapire rate 
suppose construction implemented top learning program 
help 
theoretical question answered experimentally obviously depends domain underlying learning program 
plausible construction cases give results practice 
acknowledgments prepared support aro daal darpa contract siemens 
sally goldman michael kearns ron rivest helpful comments suggestions 
anonymous referees careful reading thoughtful comments 
angluin 

finding patterns common set strings 
computer system sciences 
angluin 

queries concept learning 
machine learning 
angluin valiant 

fast probabilistic algorithms hamiltonian circuits matchings 
computer system sciences 
baum 

learning union half spaces 
unpublished manuscript 
blumer ehrenfeucht haussler warmuth 

occam razor 
information processing letters 
ehrenfeucht haussler warmuth 

learnability vapnik chervonenkis dimension 
association computing machinery 
board pitt 

necessity occam algorithms 
press proceedings second annual acm symposium theory computing 
new york ny acm press 


remarks space complexity learning circuit complexity recognizing 
proceedings workshop computational learning theory pp 

san mateo ca morgan kaufman 
haussler 

learning decision trees random examples 
information computation 
floyd 

space bounded learning vapnik chervonenkis dimension 
proceedings second annual workshop computational learning theory pp 

san mateo ca morgan kaufman 


space efficient learning algorithms technical report ucsc crl 
santa cruz ca university california center computer engineering information sciences 
haussler kearns littlestone warmuth 

equivalence models polynomial learnability 
proceedings workshop computational learning theory pp 

san mateo ca morgan kaufman 
haussler littlestone warmuth 

expected mistake bounds line learning algorithms 
unpublished manuscript 
haussler littlestone warmuth 

predicting functions randomly drawn points 
proceedings ninth annual symposium foundations computer science pp 

washington dc ieee computer society press 
helmbold sloan warmuth 

learning nested differences intersection closed concept classes 
machine learning xxx xxx 
hoeffding 

probability inequalities sums bounded random variables 
american statistical association 
strength weak learnability kearns 

thoughts hypothesis boosting 
unpublished manuscript 
kearns 

computational complexity machine learning 
doctoral dissertation department computer science harvard university cambridge 
kearns li pitt valiant 
learnability boolean formulae 
proceedings nineteenth annual acm symposium theory computing pp 

new york ny acm press 
valiant 

learning boolean formulae finite automata hard factoring technical report tr 
cambridge ma harvard university aiken computation laboratory 
kearns valiant 

cryptographic limitations learning boolean formulae finite automata 
proceedings annual acm symposium theory computing pp 

new york ny acm press 
pitt valiant 

computational limitations learning examples 
association computing machinery 
rivest 
learning decision lists 
machine learning 
schapire 

pattern languages learnable 
unpublished manuscript 
valiant 

theory learnable 
communications acm 
