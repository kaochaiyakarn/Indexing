mdps semi mdps framework temporal abstraction reinforcement learning richard sutton precup university massachusetts amherst singh university colorado boulder learning planning representing knowledge multiple levels temporal abstraction key challenges ai 
consider challenges addressed mathematical framework reinforcement learning markov decision processes mdps 
extend usual notion action framework include options closed loop policies action period time 
examples options include picking object going lunch traveling distant city primitive actions muscle joint torques 
show options enable temporally knowledge action included reinforcement learning framework natural general way 
particular show options may interchangeably primitive actions planning methods dynamic programming learning methods learning 
human decision making routinely involves choice courses action broad range time scales 
understand automate ability flexibly multiple overlapping time scales 
temporal abstraction explored ai early primarily context strips style planning fikes hart nilsson newell simon sacerdoti korf laird rosenbloom newell minton iba etzioni tambe nilsson 
temporal abstraction focus appealing aspect qualitative modeling approaches ai kuipers de kleer brown dejong say brafman tennenholtz 
explored robotics control engineering maes brooks grossman grupen colombetti dorigo sastry :10.1.1.160.8786
consider temporal abstraction framework reinforcement learning markov decision processes mdps 
framework popular ai ability deal naturally stochastic environments integration learning planning barto bradtke singh dean boutilier brafman simmons koenig geffner bonet 
reinforcement learning methods proven effective number significant applications haykin appear singh bertsekas tesauro 
mdps conventionally conceived involve temporal abstraction temporally extended action 
mdps conventionally conceived involve temporal abstraction temporally extended action 
discrete time step unitary action taken time affects state reward time 
notion course action variable period time 
consequence conventional mdp methods unable take advantage efficiencies available higher levels temporal abstraction 
hand temporal abstraction introduced reinforcement learning possible ways sutton watkins ring schmidhuber mahadevan connell singh lin dayan dayan hinton kaelbling singh chrisman sutton thrun schwartz uchibe asada ar szepesv ari dietterich huber grupen schmidhuber precup sutton singh sutton parr russell drummond :10.1.1.105.1944:10.1.1.9.313:10.1.1.17.1220:10.1.1.44.1621
generalize simplify previous works form compact unified framework temporal abstraction reinforcement learning mdps 
answer question minimal extension reinforcement learning framework allows general treatment temporally knowledge action 
second part new framework develop new results generalizations previous results 
keys treating temporal abstraction minimal extension reinforcement learning framework build theory decision processes smdps see puterman 
section focuses link smdp theory illustrates speedups planning learning possible temporal abstraction 
rest concerns ways going smdp analysis options change learn internal structure terms mdp 
section considers problem effectively combining set options single primitive temporally extended actions similarly prefer name refer 
policy 
example robot may pre designed controllers servoing joints positions picking objects visual search face difficult problem coordinate switch behaviors mahadevan connell matari uchibe sastry maes brooks koza rice dorigo colombetti ar :10.1.1.160.8786:10.1.1.17.1220
sections concern intra option learning looking inside options learn simultaneously options consistent fragment experience 
section define notion subgoal shape learn new options 
reinforcement learning mdp framework section briefly review standard reinforcement learning framework discrete time finite markov decision processes mdps forms basis extensions temporally extended courses action 
framework learning agent interacts environment discrete lowest level time scale time step agent perceives state environment basis chooses primitive action response action environment produces step numerical reward state convenient suppress differences available actions states possible denote union action sets 
possible markov options termination decisions solely basis current state long option executing 
handle cases interest consider generalization semi markov options policies termination conditions may choices dependent prior events option initiated 
general option initiated time say determines actions selected number steps say terminates intermediate time decisions markov option may depend decisions semi markov option may depend entire sequence events prior 
call sequence history denote tt denote set histories omega gamma semi markov options policy termination condition functions possible histories omega theta fi omega 
semi markov case useful cases options detailed state representation available policy selects options hierarchical machines parr parr russell :10.1.1.105.1944
note hierarchical structures options select options give rise higher level options semi markov options markov 
semi markov options include general range possibilities 
set options initiation sets implicitly define set available options state sets available actions unify kinds sets noting actions considered special case options 
action corresponds option available available fs lasts exactly step fi selects 
transit times options discrete simply special case arbitrary real intervals permitted smdps 
pi relationship mdps options smdps provides basis theory planning learning methods options 
sections discuss limitations theory due treatment options indivisible units internal structure section focus establishing benefits provides 
establish theoretical foundations survey smdp methods planning learning options 
formalism slightly different results essence taken adapted prior including classical smdp bradtke parr parr russell singh sutton precup sutton precup sutton singh sutton :10.1.1.105.1944
result similar theorem proved detail parr 
sections new methods improve smdp methods 
planning options requires model consequences 
fortunately appropriate form model options analogous ss defined earlier actions known existing smdp theory 
note strict inequality holds history gamma ends trajectory generated non zero probability 
pi application result consider case optimal policy set markov options discussed planning learning determine optimal value functions optimal policy achieves 
best done changing smdp defined best possible achievable mdp course typically wish directly primitive options computational expense 
interruption theorem gives way improving little additional computation stepping outside step interrupt current option switch new option valued highly checking options typically done vastly expense time step involved combinatorial process computing sense interruption gives nearly free improvement smdp planning learning method computes intermediate step 
extreme case interrupt step switch greedy option option state highly valued polling execution dietterich :10.1.1.9.313
case options followed step superfluous 
options play role determining basis greedy switches recall multi step options may enable quickly section 
multi step options followed step provide substantial advantages computation theoretical understanding 
shows simple example 
nonterminating options applied option time option executing time 
markov options special temporal difference methods learn usefully model option option terminates 
call intra option methods learn experience single option 
intra option methods learn model option executing option long selections consistent option 
intra option methods examples policy learning methods sutton barto learn consequences policy behaving potentially different policy :10.1.1.32.7692
intra option methods simultaneously learn models different options experience 
intra option methods introduced sutton prediction problem single unchanging policy full control case consider 
just bellman equations value functions bellman equations models options 
consider intra option learning model markov option hi fii 
fundamental changing policies consider briefly section 
natural think options achieving subgoals kind adapt option policy better achieve subgoal 
example option open door natural adapt policy time effective efficient opening door may generally useful 
subgoals options relatively straightforward design policy intra option learning methods adapt policies better achieve subgoals 
example may possible simply apply learning learn independently subgoal option singh lin dorigo colombetti thrun schwartz :10.1.1.44.1621
hand clear best way formulate subgoals associate options basis evaluation 
important considerations extent models options constructed achieve subgoal transferred aid planning achieve 
long lived learning agent face continuing series subtasks result capable 
section briefly simple approach associating subgoals suffices illustrate possibilities problems arise 
problems arise new subgoal involves states may passed option executed 
may feasible detect prevent problems 
idea keep track states option passes invalidate options models pass subgoal states 
idea alter subgoal formulation subgoal states passed stopping collecting subgoal value optional required 
note require models accurate just non precup sutton predict correct outcome just outcome equal expected value correct outcome :10.1.1.37.2027
may enable important special cases handled simply 
example new subgoal involving states subgoal value singleton probably safely transferred 
sort problem shown occur cases 
representing knowledge flexibly multiple levels temporal abstraction potential greatly speed planning learning large problems 
