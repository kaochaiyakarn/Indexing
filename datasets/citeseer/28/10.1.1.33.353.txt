machine learning fl kluwer academic publishers boston 
manufactured netherlands 
appears machine learning volume nos 
july august pages final draft prior line editors kluwer 
slight differences page numbers exist empirical comparison voting classification algorithms bagging boosting variants eric bauer cs stanford edu computer science department stanford university stanford ca ron kohavi ronnyk cs stanford edu blue software campus dr suite san mateo ca received oct revised sept editors philip chan stolfo david wolpert 
methods voting classification algorithms bagging adaboost shown successful improving accuracy certain classifiers artificial realworld datasets 
compare mean squared error voting methods non voting methods show voting methods lead large significant reductions mean squared errors 
practical problems arise implementing boosting algorithms explored including numerical instabilities 
graphically show adaboost instances emphasizing hard areas outliers noise 
keywords classification boosting bagging decision trees naive bayes mean squared error 
methods voting classification algorithms bagging adaboost shown successful improving accuracy certain classifiers artificial real world datasets breiman freund schapire quinlan :10.1.1.133.1040
voting algorithms divided types adaptively eric bauer ron kohavi change distribution training set performance previous classifiers boosting methods bagging 
algorithms adaptively change distribution include option decision tree algorithms construct decision trees multiple options nodes buntine buntine kohavi kunz averaging path sets sets extended sets alternatives pruning oliver hand voting trees different splitting criteria human intervention kwok carter error correcting output codes dietterich bakiri kong dietterich 
wolpert discusses stacking classifiers complex classifier simple uniform weighting scheme bagging 
ali provides review related algorithms additional chan stolfo wolpert 
voting algorithms divided types adaptively eric bauer ron kohavi change distribution training set performance previous classifiers boosting methods bagging 
algorithms adaptively change distribution include option decision tree algorithms construct decision trees multiple options nodes buntine buntine kohavi kunz averaging path sets sets extended sets alternatives pruning oliver hand voting trees different splitting criteria human intervention kwok carter error correcting output codes dietterich bakiri kong dietterich 
wolpert discusses stacking classifiers complex classifier simple uniform weighting scheme bagging 
ali provides review related algorithms additional chan stolfo wolpert 
algorithms adaptively change distribution include adaboost freund schapire arc breiman :10.1.1.32.8918
drucker cortes quinlan applied boosting decision tree induction observing error significantly decreases generalization error degrade classifiers combined 
elkan applied boosting simple inducer performs uniform discretization achieved excellent results real world datasets artificial dataset failed achieve significant improvements artificial datasets 
review voting algorithms including bagging adaboost arc describe large empirical study purpose improve understanding algorithms affect classification error 
ensure study reliable dozen datasets fewer instances instances 
perturbation causes different classifiers built inducer unstable neural networks decision trees breiman performance improve induced classifiers correlated bagging may slightly degrade performance stable algorithms nearest neighbor effectively smaller training sets training classifier breiman 

boosting boosting introduced schapire method boosting performance weak learning algorithm 
improvements freund expanded freund adaboost adaptive boosting introduced freund schapire 
concentrate adaboost called adaboost freund schapire :10.1.1.133.1040
bagging adaboost algorithm generates set classifiers votes 
algorithms differ substantially 
adaboost algorithm shown generates classifiers sequentially bagging generate parallel 
adaboost changes weights training instances provided input inducer classifiers previously built 
