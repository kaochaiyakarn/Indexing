prop ii global optimization multilayer perceptrons gas computer science dept 
madrid spain el es dept architecture computer technology campus de spain general problem model selection obtain right parameters model fit observed data 
multilayer perceptron mlp trained backpropagation bp means finding right hidden layer size appropriate initial weights learning parameters 
proposes method prop ii attempts solve problem combining genetic algorithm ga bp train mlps single hidden layer 
ga selects initial weights learning rate network changes number neurons hidden layer application specific genetic operators 
prop ii combines advantages global search performed ga mlp parameter space local search bp algorithm 
prop ii combines advantages global search performed ga mlp parameter space local search bp algorithm 
application prop ii algorithm real world benchmark problems shows mlps evolved prop ii smaller achieve higher level generalization perceptron training algorithms algorithms lvq 
shows improvement previous versions algorithm 
state art application artificial neural networks anns designed structured layers correctly connected initial parameters established trained 
bp different versions widely training mechanism examples include qp evolutionary approaches :10.1.1.1.6150
whichever method chosen training mechanism iterative gradient descent algorithm designed step step minimize difference actual output vector network desired output vector 
method successfully fields especially pattern recognition due learning ability encounter certain difficulties practice convergence tends extremely slow convergence global optimum guaranteed learning constants guessed heuristically 
method got set parameters adjusted depending problem solve type network number hidden units training test sets 
leaves problem automatic mlp parameter setting optimization open 
method successfully fields especially pattern recognition due learning ability encounter certain difficulties practice convergence tends extremely slow convergence global optimum guaranteed learning constants guessed heuristically 
method got set parameters adjusted depending problem solve type network number hidden units training test sets 
leaves problem automatic mlp parameter setting optimization open 
ways approaching optimization problem parameters incremental see review genetic algorithms see yao review 
incremental algorithms cascade correlation fahlman lebiere tiling perceptron cascade parekh methods proposed zhang adding hidden neurons network minimum size reaches required precision :10.1.1.125.6421
algorithms pruning methods pelillo big network eliminating hidden layer obtain smaller network 
approaches set weights zero done optimal brain damage le cun optimal brain surgeon 
proposes method construction neural classifiers regularization pruning 
evolutionary neural networks provide alternative task controlling complexity adjusting number weights ann 
example hybrid ga bp train ann 
gas design ann applied ways search optimal set weights pre established topology net ga evolve population ann encoding parameters hidden layer binary strings 
de propose method evolutionary approach provide optimal set synaptic weights network 
authors search topology space white miller proposes couple gas weight elimination 
methods proposed yao liu combine search optimal set weights search optimal topology ga bp :10.1.1.1.6150
de method evolutionary approach face optimization design neural network architecture choice best learning method 
approaches search optimal learning parameters having pre established number neurons connectivity approach simulated annealing bp 
incremental algorithms gradient descent optimization methods suffer problem may reach closest local minimum search space point method began 
evolutionary neural networks efficient way searching search subset possible parameters 
table shows average error rate average size nets number hidden units learning parameter 
learning rate obtain results qp value prop ii best net 
hidden layer size expressed enclosed parentheses terms number parameters net number weights net 
dna error size params 
learning qp prop ii prop sa prop generations lvq generations generations table results evaluating qp prop ii classification different views large virus error test set hidden layer size learning parameter results obtained lvq prop sa prop :10.1.1.1.6150
hidden layer size expressed enclosed parentheses terms number parameters net number weights net 
general prop ii obtains mlps lower generalization error methods sa prop cancer presents similar results account mention standard prop ii obtains smaller sized networks neurons similar classification error 
results obtained prop ii similar obtained prop particular problem noticed prop ii searches learning constant prop needs extra effort guess suitable learning constant train networks :10.1.1.1.6150
dna problem ga executed generations population individuals probability mutation crossing points crossover operator 
dna error size params 
learning qp prop ii prop sa prop generations lvq generations generations table results evaluating qp prop ii classification different views large virus error test set hidden layer size learning parameter results obtained lvq prop sa prop :10.1.1.1.6150
hidden layer size expressed enclosed parentheses terms number parameters net number weights net 
general prop ii obtains mlps lower generalization error methods sa prop cancer presents similar results account mention standard prop ii obtains smaller sized networks neurons similar classification error 
results obtained prop ii similar obtained prop particular problem noticed prop ii searches learning constant prop needs extra effort guess suitable learning constant train networks :10.1.1.1.6150
dna problem ga executed generations population individuals probability mutation crossing points crossover operator 
generation population replaced new individuals 
individuals mlp inputs outputs number hidden units evaluated epochs initial learning coefficient 
execution parameters took minutes 
strategy attempts avoid individuals inherit trained weights parents 
step full automation mlp design sets value learning constant 
benchmarks cancer dna test proposed algorithm compare 
results show ga obtains mlp classification accuracy better obtained training mlp conventional procedures 
extend method include development new genetic operators improvement described perform incremental learning pruning addition operators approach :10.1.1.125.6421
qp algorithm operator ga suggested algorithm applied solve real world problems 
supported part project bio spain project pb feder project fd tel 
bibliography fahlman 
faster learning variations backpropagation empirical study 
optimization competitive learning neural network genetic algorithms 

lectures notes computer science vol 

gonzalez :10.1.1.1.6150
prop global optimization multilayer perceptrons gas 
submitted neurocomputing 
xin yao yong liu 
designing artificial neural networks evolution 
international journal pattern recognition artificial intelligence 
xin yao 
review evolutionary artificial neural networks 
international journal intelligent systems april 
fahlman lebiere :10.1.1.125.6421
cascade correlation learning architecture 
neural information systems 
touretzky 
ed morgan kauffman 
