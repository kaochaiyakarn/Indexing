journal arti cial intelligence research submitted published wrap trainable discourse module information extraction stephen soderland cs umass edu wendy lehnert lehnert cs umass edu department computer science university massachusetts amherst ma vast amounts line text available led renewed interest information extraction systems analyze unrestricted text producing structured representation selected information text 
presents novel approach uses machine learning acquire knowledge higher level processing 
wrap trainable discourse component inferences identi es logical relations information extracted text 
previous approaches limited lower level processing part speech tagging lexical disambiguation dictionary construction 
wrap fully trainable automatically decides classi ers needed derives feature set classi er automatically 
performance equals partially trainable discourse module requiring manual customization domain 
ai access foundation morgan kaufmann publishers 
rights reserved 
soderland lehnert describes wrap soderland lehnert rst system automatically acquire domain knowledge higher level processing associated discourse analysis 
wrap uses supervised learning induce set classi ers training corpus representative texts text accompanied hand coded target output 
implemented wrap id decision tree algorithm quinlan machine learning algorithms selected :10.1.1.167.3624:10.1.1.167.3624
wrap fully trainable system unique decides classi ers needed domain automatically derives feature set classi er 
user supplies de nition objects relationships interest domain training corpus hand coded target output 
wrap rest hand coding needed tailor system new domain 
section discusses task detail introduces microelectronics domain gives overview circus sentence analyzer 
muc microelectronics domain explained section required decision trees slot filtering stage slot merging link creation object splitting inferring missing objects inferring missing slot values 
example link creation stage tree determines pointers lithography objects equipment objects 
pair lithography equipment objects text encoded instance sent lithography equipment link tree 
classi er returns positive wrap adds pointer objects output indicate equipment lithography process 
id decision tree algorithm quinlan experiments machine learning classi er plugged wrap architecture :10.1.1.167.3624:10.1.1.167.3624
vector space approach appropriate performance depend weights assigned feature salton 
hard see principled way assign weights heterogeneous features wrap classi ers see section features encode attributes domain objects encode linguistic context relative position text 
look example section stepper see wrap discourse decision add pointer uv lithography equipment object 
wrap encodes instance lithography equipment link decision tree features representing attributes lithography equipment objects extraction patterns relative position text 
id feature value associated positive negative training instance encapsulates statistics node tree builds 
shows portion lithography equipment link tree showing path classify instance uv lithography stepper positive 
numbers tree node show number positive negative training instances represented node 
priori probability pointer lithography equipment training corpus positive negative training instances 
id uses information gain metric select ective feature partition training instances quinlan case choosing equipment type test root tree :10.1.1.167.3624
feature su cient classify instances modular equipment radiation source system negative instances 
apparently types equipment lithography processes useful bit domain knowledge 
branch equipment type stepper leads node tree representing positive negative training instances raising probability link 
id pos neg pos neg soderland lehnert pos neg equipment type stepper pos neg pos neg pos neg lithography type line beam line uv pos neg pos neg distance pos neg pos neg 
