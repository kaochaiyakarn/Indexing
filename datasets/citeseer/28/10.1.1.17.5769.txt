knowledge acquisition examples multiple models pedro domingos dept information computer science university california irvine irvine california ics uci edu www ics uci edu qualify knowledge learner output accurate stable comprehensible 
learning multiple models improve significantly accuracy stability single models cost losing comprehensibility possess example simple decision trees rule sets 
proposes evaluates cmm meta learner seeks retain accuracy gains multiple model approaches producing single comprehensible model 
cmm base learner recover frontiers implicit multiple model ensemble 
done giving base learner new training set composed large number examples generated classified ensemble plus original examples 
cmm evaluated rules base learner bagging multiple model methodology 
turney working industrial applications decision tree learning engineers disturbed different batches data process result radically different decision trees 
engineers lose confidence decision trees demonstrate trees high predictive accuracy 
turney researchers noted negative impact instability learners ability produce knowledge dietterich approach problem object research see example chan stolfo wolpert 
consists learning say different models means variations learner data combining models way predictions 
different forms multiple models approach include bagging breiman boosting freund schapire stacking wolpert bayesian averaging buntine error correcting output coding kong dietterich combiner trees chan stolfo :10.1.1.133.1040
approach quite effective practice drucker cortes jackel lecun vapnik quinlan substantial theoretical foundations madigan raftery friedman focus line research reducing instability means improving accuracy point view knowledge acquisition fact represents gives essential goal output comprehensibility :10.1.1.49.2457
example single decision tree easily understood human long large trees individually simple exceed capacity patient user fact understand ensemble prediction necessary addition understand keep track trees interact performance time 
significant progress separately achieving goals accuracy stability comprehensibility overarching attaining simultaneously remains elusive 
aims move closer ideal explore space trade offs subgoals proposing learning method combines accuracy stability gains multiple models comprehensibility single model 
engineers lose confidence decision trees demonstrate trees high predictive accuracy 
turney researchers noted negative impact instability learners ability produce knowledge dietterich approach problem object research see example chan stolfo wolpert 
consists learning say different models means variations learner data combining models way predictions 
different forms multiple models approach include bagging breiman boosting freund schapire stacking wolpert bayesian averaging buntine error correcting output coding kong dietterich combiner trees chan stolfo :10.1.1.133.1040
approach quite effective practice drucker cortes jackel lecun vapnik quinlan substantial theoretical foundations madigan raftery friedman focus line research reducing instability means improving accuracy point view knowledge acquisition fact represents gives essential goal output comprehensibility :10.1.1.49.2457
example single decision tree easily understood human long large trees individually simple exceed capacity patient user fact understand ensemble prediction necessary addition understand keep track trees interact performance time 
significant progress separately achieving goals accuracy stability comprehensibility overarching attaining simultaneously remains elusive 
aims move closer ideal explore space trade offs subgoals proposing learning method combines accuracy stability gains multiple models comprehensibility single model 
proposed approach described section 
partitioning produced combining models complex produced component models meta learned model complex necessarily extent rendered incomprehensible especially approximation exact partitioning produced combined models 
hand fact meta learned model accurate combined especially representational power language combined models greater component model language 
crucially accuracy stability learned models tend increase training set size due decreasing variance kohavi wolpert training set size meta learning step large desired subject computational resource constraints possible obtain meta learned model accurate stable base models 
procedure just described called cmm combined multiple models shown pseudo code table 
significant points instances different weights boosting freund schapire :10.1.1.133.1040
limit different learners typically done stacking wolpert 
variations variations may 
note procedure quite different stacking wolpert final output level classifier explicitly includes models produced plus meta classifier combine predictions performance time 
table 
rules quinlan release base learner 
rules produces propositional rule sets believe easily understood representations currently 
rules advantage widely constituting standard empirical comparisons 
system rules extracted previously learned decision tree ordered rule applies appearing ordering prevails 
bagging breiman multiple model methodology account simplest available effectiveness decision trees established breiman quinlan bagging procedure training set size bootstrap replicate constructed samples replacement training set :10.1.1.49.2457:10.1.1.49.2457
new training set size produced original examples may appear 
average sufficiently large original examples appear bootstrap sample 
learner applied training set 
procedure repeated times resulting models aggregated uniform voting test example class predicted greatest number models predicted tie occurs lowest ordered class chosen breiman 
