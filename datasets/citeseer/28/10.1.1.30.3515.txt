additive logistic regression statistical view boosting jerome friedman trevor hastie robert tibshirani august boosting freund schapire schapire singer important developments classification methodology 
performance classification algorithms dramatically improved sequentially applying versions input data weighted majority vote sequence classifiers produced 
show seemingly mysterious phenomenon understood terms known statistical principles additive modeling maximum likelihood 
class problem boosting viewed approximation additive modeling logistic scale maximum bernoulli likelihood criterion 
develop direct approximations show exhibit nearly identical results boosting 
direct multi class generalizations multinomial likelihood derived exhibit performance comparable proposed multi class generalizations boosting situations far superior 
interestingly test error consistently decrease level classifiers added ultimately increase 
reason adaboost immune overfitting 
shows performance discrete adaboost synthetic classification task adaptation cart tm breiman friedman olshen stone base classifier 
adaptation grows fixed size trees best manner see section page 
included bagged tree breiman averages trees grown bootstrap resampled versions training data :10.1.1.32.9399
bagging purely variance reduction technique trees tend high variance bagging produces results 
early versions adaboost resampling scheme implement step algorithm weighted importance sampling training data 
suggested connection bagging major component success boosting variance reduction 
boosting performs comparably ffl weighted tree growing algorithm step weighted resampling training observation assigned weight removes randomization component essential bagging 
imbalance inhibit learning 
imbalance occurs logitboost near decision boundary correctly misclassified observations appear roughly equal numbers 
example illustrates large reductions computation boosting achieved simple trick 
variety examples shown exhibit similar behavior boosting methods 
note committee approaches classification bagging breiman randomized trees dietterich admitting parallel implementations take advantage approach reduce computation :10.1.1.32.9399:10.1.1.131.1931
concluding remarks order understand learning procedure statistically necessary identify important aspects structural model error model 
important determines function space approximator characterizing class functions accurately approximated 
error model specifies distribution random departures sampled data structural model 
defines criterion optimized estimation structural model 
natural choice especially appropriate observations belong class schapire singer 
usual setting unique class label observation symmetric multinomial distribution appropriate error model 
develop multi class logitboost procedure maximizes corresponding log likelihood quasi newton stepping 
show simulated examples exist settings approach leads superior performance situations encountered set real data examples illustration performance approaches quite similar performance examples 
concepts developed suggest little connection deterministic weighted boosting randomized ensemble methods bagging breiman random ized trees dietterich :10.1.1.32.9399:10.1.1.131.1931
language squares regression purely variance reducing procedures intended mitigate instability especially associated decision trees 
boosting hand fundamentally different 
appears purely bias reducing procedure intended increase flexibility stable highly biased weak learners incorporating jointly fitted additive expansion 
distinction clear boosting implemented finite random importance sampling weights 
