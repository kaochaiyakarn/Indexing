chapter information theoretic learning jose xu john fisher iii chapter seeks extend ubiquitous mean square error criterion mse cost functions include information training data 
learning process ultimately transfer information carried data samples system parameters natural goal find cost functions directly manipulate information 
name information theoretic learning itl 
order useful itl independent learning machine architecture require solely availability data require priori assumptions data distributions 
chapter presents current efforts develop itl criteria integration nonparametric density estimators renyi quadratic entropy definition 
motivation start application mse manipulate infor mation nonlinear characteristics learning machine 
notice mapper linear non linear criterion may may exploit external input normally called desired response information theoretic learning includes special cases unsupervised supervised frameworks 
want learning criterion external independent mapper 
briefly review done area 
xw analogy optimization euclidean space adapt parameters mapper manipulating output distribution maximizing output entropy maxent minimizing cross entropy outputs output signals 
bell sejnowski blind source separation bss example application gw bp algorithm mapping network maxent principle :10.1.1.110.696
neural network literature barlow atick entropy concepts learning 
optimization principle potentially useful solve engineering particular learning problems 
comon cardoso amari utilized principle formulate solve bss problem demonstrated previous chapters 
solution bss obtained minimizing mutual information redundancy outputs mapper formulated divergence joint pdf factorized marginals iy yn hy problem arises estimating joint output density 
