probabilistic hierarchical clustering labeled unlabeled data larsen hansen informatics mathematical modeling technical university denmark richard build 
dk denmark web imm dk emails jl asz imm dk 
presents hierarchical probabilistic clustering methods unsupervised supervised learning datamining applications supervised learning performed labeled unlabeled examples 
probabilistic clustering previously suggested generalizable gaussian mixture model extended modified expectation maximization procedure learning unlabeled labeled examples 
proposed hierarchical scheme agglomerative probabilistic similarity measures 
compare dissimilarity measure error confusion similarity accumulated posterior cluster probability measure 
compare dissimilarity measure error confusion similarity accumulated posterior cluster probability measure 
unsupervised supervised schemes successfully tested artificially data mails segmentation 
hierarchical methods unsupervised supervised datamining provide multilevel description data relevant applications related information extraction retrieval navigation organization information see 
different approaches hierarchical analysis divisive agglomerative clustering schemes suggested developments include 
focus agglomerative probabilistic clustering gaussian density mixtures earlier extended suggesting comparing various similarity measures connection cluster merging :10.1.1.20.7748:10.1.1.29.3111
advantage probabilistic clustering scheme automatic detection final hierarchy level new data training 
order provide meaningful description clusters suggest interpretation techniques listing prototypical data examples cluster listing typical features associated cluster 
generalizable gaussian mixture model ggm soft generalizable gaussian mixture model basic model supervised unsupervised learning :10.1.1.40.8941:10.1.1.29.3111
extend framework research supported danish research distributed multimedia technologies applications program center multimedia signal image processing telemedicine site program 
different approaches hierarchical analysis divisive agglomerative clustering schemes suggested developments include 
focus agglomerative probabilistic clustering gaussian density mixtures earlier extended suggesting comparing various similarity measures connection cluster merging :10.1.1.20.7748:10.1.1.29.3111
advantage probabilistic clustering scheme automatic detection final hierarchy level new data training 
order provide meaningful description clusters suggest interpretation techniques listing prototypical data examples cluster listing typical features associated cluster 
generalizable gaussian mixture model ggm soft generalizable gaussian mixture model basic model supervised unsupervised learning :10.1.1.40.8941:10.1.1.29.3111
extend framework research supported danish research distributed multimedia technologies applications program center multimedia signal image processing telemedicine site program 
supervised learning combined sets labeled unlabeled data modified version approach called unsupervised supervised generalizable gaussian mixture model 
supervised learning combined sets relevant practical applications due fact labeled examples hard expensive obtain instance document categorization medical applications 
models estimate parameters gaussian clusters modified em procedure disjoint data sets prevent notorious infinite overfit problems ensuring generalization ability 
extend framework research supported danish research distributed multimedia technologies applications program center multimedia signal image processing telemedicine site program 
supervised learning combined sets labeled unlabeled data modified version approach called unsupervised supervised generalizable gaussian mixture model 
supervised learning combined sets relevant practical applications due fact labeled examples hard expensive obtain instance document categorization medical applications 
models estimate parameters gaussian clusters modified em procedure disjoint data sets prevent notorious infinite overfit problems ensuring generalization ability 
optimum number clusters mixture determined automatically minimizing estimate generalization error :10.1.1.40.8941
focuses applications objective categorizing text topic spotting new topics providing short easy understandable interpretation larger text blocks broader sense create intelligent search engines provide understanding documents content webpages yahoo ontologies 
section various ggm models supervised unsupervised learning discussed particular introduce algorithm 
hierarchical clustering scheme discussed section introduces similarity measures cluster merging 
section provide numerical experiments segmentation mails 
focuses applications objective categorizing text topic spotting new topics providing short easy understandable interpretation larger text blocks broader sense create intelligent search engines provide understanding documents content webpages yahoo ontologies 
section various ggm models supervised unsupervised learning discussed particular introduce algorithm 
hierarchical clustering scheme discussed section introduces similarity measures cluster merging 
section provide numerical experiments segmentation mails 
generalizable gaussian mixture model step approach probabilistic clustering flexible universal extension gaussian mixture density model generalizable gaussian mixture model aim supervised learning unlabeled labeled data :10.1.1.40.8941:10.1.1.20.7748:10.1.1.29.3111
define dimensional input feature vector associated output cg class labels assuming mutually exclusive classes 
joint input output density modeled gaussian mixture xj kx xjk xjk exp kj number components xjk component gaussians mixed non negative priors pk class cluster posteriors pc 
th gaussian component described mean vector covariance matrix vector model parameters fp yg 
gaussian mixture universal approximator model eq 
update covariance matrices dl xn dl xn du du xn xn perform regularization see text 

update cluster priors dl xn nl nu du 
update class cluster posteriors dl yn xn dl xn algorithm 
algorithm model parameters estimated iterative modified em algorithm means covariance matrices estimated independent data sets combined set :10.1.1.40.8941:10.1.1.40.8941
approach prevents overfitting problems standard approach 
designated generalizable gaussian mixture model labeled unlabeled data may viewed extension em algorithm suggested 
ggm implemented hard soft assignments data components em iteration step 
hard ggm approach data example assigned cluster selecting highest 
designated generalizable gaussian mixture model labeled unlabeled data may viewed extension em algorithm suggested 
ggm implemented hard soft assignments data components em iteration step 
hard ggm approach data example assigned cluster selecting highest 
means covariances estimated classical empirical estimates data assigned component 
soft version means covariances estimated weighted quantities xn :10.1.1.29.3111
ggm provides biased estimate gives better results small data sets general soft version preferred :10.1.1.29.3111
algorithm summarized fig 
soft approach 
main iteration loop aborted change example cluster assignment noticed 
ggm implemented hard soft assignments data components em iteration step 
hard ggm approach data example assigned cluster selecting highest 
means covariances estimated classical empirical estimates data assigned component 
soft version means covariances estimated weighted quantities xn :10.1.1.29.3111
ggm provides biased estimate gives better results small data sets general soft version preferred :10.1.1.29.3111
algorithm summarized fig 
soft approach 
main iteration loop aborted change example cluster assignment noticed 
labeled examples assigned clusters kn arg xn dl unlabeled kn arg du 
practical simulations show proper choice cases 
convergence criteria changes negative formulated 
unsupervised ggm model input data available perform unsupervised learning 
case object modeling input density eq 
trained algorithm :10.1.1.29.3111
supervised ggm model clearly case unlabeled examples 
choice separate ggm models class conditional input densities xjy xjy xjy defined eq 
ky number components 
bayes optimal rule assuming loss function classification done maximizing yjx xjy pc xjy 
consequently consider different measures express similarity probability space models xjy cf 
sec 
computed exactly levels hierarchy fig 
illustrates hierarchical clustering gaussian distributed toy data 
dissimilarity measure densities defined pj xj pj xjm dx index different clusters :10.1.1.36.5462
due inequality distance measure referred dissimilarity 
kg set cluster indices define disjoint ii contain indices clusters constitute clusters level respectively 
density cluster pj xj ip zero 
pj xjm ip obtains similar definition 

freitag machine learning information extraction informal domains machine learning vol 
pp 

hansen nielsen larsen modeling text generalizable gaussian mixtures proc :10.1.1.40.8941
ieee icassp vol 
pp 

hansen supervised learning labeled unlabeled data submitted 

nigam mccallum thrun mitchell text classification labeled unlabeled documents em machine learning vol 
pp 

larsen hansen hierarchical clustering datamining proc :10.1.1.29.3111
th int 
conf 
knowledge intelligent information engineering systems allied technologies osaka nara japan sept 
lippmann learning mixture hierarchies advances nips pp 
