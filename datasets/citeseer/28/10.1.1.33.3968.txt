appear proceedings ai berlin springer verlag stochastic attribute selection committees zheng geo rey webb school computing mathematics university victoria australia edu au 
classi er committee learning methods generate multiple classi ers form committee repeated application single base learning algorithm 
committee members vote decide nal classication 
methods bagging boosting shown great success decision tree learning 
create di erent classi ers modifying distribution training set 
studies different approach stochastic attribute selection committee learning decision trees 
empirical evaluation variant method sasc representative collection natural domains shows sasc method signi cantly reduce error rate decision tree learning 
average sasc accurate bagging accurate boosting tailed sign test fails show di erences signi cant 
addition bagging sasc stable boosting terms frequently obtaining signi cantly higher error rates error raised producing lower error rate increases 
bagging sasc amenable parallel distributed processing boosting 
classi er committees focus attention freund freund schapire quinlan breiman dietterich kong ali chan stolfo wolpert ali pazzani schapire freund bartlett lee bauer kohavi :10.1.1.33.353
type approach set classi ers generated single base learning algorithm form committee 
committee members vote decide nal classi cation 
bagging breiman boosting schapire freund freund schapire schapire representative methods type signi cantly decrease error rate decision tree learning quinlan freund schapire bauer kohavi :10.1.1.33.353:10.1.1.31.2869
repeatedly build di erent classi ers base learning algorithm decision tree generator changing distribution training set 
bagging sasc amenable parallel distributed processing boosting 
classi er committees focus attention freund freund schapire quinlan breiman dietterich kong ali chan stolfo wolpert ali pazzani schapire freund bartlett lee bauer kohavi :10.1.1.33.353
type approach set classi ers generated single base learning algorithm form committee 
committee members vote decide nal classi cation 
bagging breiman boosting schapire freund freund schapire schapire representative methods type signi cantly decrease error rate decision tree learning quinlan freund schapire bauer kohavi :10.1.1.33.353:10.1.1.31.2869
repeatedly build di erent classi ers base learning algorithm decision tree generator changing distribution training set 
bagging learns constituent classi ers bootstrap samples drawn training set 
boosting learns constituent classi ers sequentially 
training examples creating classi er modi ed performance previous classi ers away generation classi er concentrate training examples misclassi ed previous classi ers 
bagging learns constituent classi ers bootstrap samples drawn training set 
boosting learns constituent classi ers sequentially 
training examples creating classi er modi ed performance previous classi ers away generation classi er concentrate training examples misclassi ed previous classi ers 
major di erence bagging boosting adaptively changes distribution training set performance previously created classi ers uses function performance classi er weight stochastically changes distribution training set uses equal weight voting 
boosting produces committees average accurate produced bagging performance boosting variable bagging quinlan bauer kohavi :10.1.1.33.353
integer committee size boosting bagging need approximately times long base learning algorithm learning single classi er 
bagging advantage boosting 
amenable parallel distributed processing boosting generation committee member classi er independent occur sequentially 
bagging appropriate parallel machine learning datamining 
integer committee size boosting bagging need approximately times long base learning algorithm learning single classi er 
bagging advantage boosting 
amenable parallel distributed processing boosting generation committee member classi er independent occur sequentially 
bagging appropriate parallel machine learning datamining 
attention focused boosting bagging classi er committee learning approaches developed including generating multiple trees manually changing learning parameters kwok carter error correcting output codes dietterich bakiri generating di erent classi ers randomizing base learning process dietterich kong ali learning option trees buntine kohavi kunz training committee neural networks manually selecting attribute subsets ghosh learning committees randomly choosing attribute subsets zheng creating committees rst order learning adding random selection conditions foil ali pazzani :10.1.1.72.7289
di erent base learning algorithms learning di erent classi ers committees wolpert 
collection research area reviews related methods chan stolfo wolpert dietterich ali 
contrast bagging boosting studies alternative approach generating di erent classi ers form committee sasc stochastic attribute selection committees 
sasc builds di erent classi ers stochastically modifying set attributes considered induction distribution training set kept unchanged 
rst probabilistic predictions produced ts voting weights 
second categorical predictions provided ts weights 
voting method corresponds method original bagging breiman 
methods weight voting 
alternatives weighted voting categorical predictions corresponds original adaboost schapire freund schapire freund schapire :10.1.1.31.2869
rst method set default boost bag performs better similarly average 
empirically compare voting methods section 
experimental domains methods natural domains uci machine learning repository merz murphy 
include domains quinlan studying boosting bagging 
boost bag sasc reduce average error rate respectively 
relative error reductions committee learning algorithms domains respectively 
tailed pairwise sign test shows error reductions signi cant level better 
boost accurate bag average bag stable boost terms frequently obtaining signi cantly higher error rates 
consistent previous ndings quinlan bauer kohavi :10.1.1.33.353
note tailed sign test fails show frequency error reductions boost bag domains signi cant 
average sasc accurate bag accurate boost 
sasc achieves average relative error reduction bag domains 
obtains signi cantly lower error rates domains signi cantly higher error rates domains 
results indicate bag sasc stable boost terms frequently obtaining signi cantly higher error rates obtaining lower error rate increases 
analysis suggests stochastic component bag contributes stable behavior 
voting weights mentioned boost bag voting methods nal classi cation 
table presents average error rates average error rate ratios boost bag sasc di erent voting methods domains 
previous research boosting uses categorical predictions voting weights voting method method table freund freund schapire quinlan bauer kohavi probabilistic predictions voting weights rst method table method outperforms respect lower average error rate higher average relative error reduction :10.1.1.33.353
average relative error reduction domains 
sign test fails show error reduction signi cant 
note probabilistic predictions voting weights method achieves lowest average error rate domains voting methods boost 
average error rate ratio probabilistic predictions voting weights method probabilistic predictions voting weights method domains 
categorical predictions voting weights method obtains highest average error rate lowest average relative error reduction domains voting methods boost 
performs signi cantly worse probabilistic predictions voting weights method tailed sign test voting weight tree training errors set big value boost bag experiments reported subsection 

substituting probabilistic predictions voting weights results lowest average error rate boost boost results table signi cantly ect comparative outcomes respect sasc error ratio 
bagging uses categorical predictions voting weights method voting previous research breiman quinlan bauer kohavi :10.1.1.33.353
experiments show voting method achieves lowest average error rate domains methods bag 
probabilistic predictions voting weights method obtains average error rate just percentage points higher method bag 
tailed sign test shows di erences error rate signi cant domains 
addition average relative error reductions domains average error ratio 
