department computer science hamilton correlation feature selection machine learning mark hall thesis submitted partial fulfilment requirements degree doctor philosophy university waikato 
april mark hall ii central problem machine learning identifying representative set features construct classification model particular task 
thesis addresses problem feature selection machine learning correlation approach 
central hypothesis feature sets contain features highly correlated class uncorrelated 
feature evaluation formula ideas test theory provides operational definition hypothesis 
cfs correlation feature selection algorithm couples evaluation formula appropriate correlation measure heuristic search strategy 
cfs evaluated experiments artificial natural datasets 
machine learning algorithms decision tree learner ib instance learner naive bayes 
experiments artificial datasets showed cfs quickly identifies screens irrelevant redundant noisy features identifies relevant features long relevance strongly depend features 
natural domains cfs typically eliminated half features 
cases classification accuracy reduced feature set equaled accuracy complete feature set 
feature selection degraded machine learning performance cases features eliminated highly predictive small areas instance space 
experiments compared cfs wrapper known approach feature selection employs target learning algorithm evaluate feature sets 
cases cfs gave comparable results wrapper general outperformed wrapper small datasets 
cfs executes times faster wrapper allows scale larger datasets 
methods extending cfs handle feature interaction experimentally evaluated 
considers pairs features second incorporates iii feature weights calculated relief algorithm 
experiments artificial domains showed methods able identify interacting features 
natural domains pairwise method gave reliable results weights provided relief 
iv foremost acknowledge prompt help supervisor lloyd smith 
lloyd allowed complete freedom define explore directions research 
proved difficult somewhat bewildering come appreciate wisdom way encouraged think unfortunately easy avoid undergraduate 
lloyd department computer science provided appreciated financial support degree 
kindly provided teaching positions travel funds attend conferences 
geoff holmes ian witten bill teahan providing valuable feedback reading parts thesis 
stuart super combo len trigg eibe frank deserve technical assistance helpful comments 
len convinced ms word writing thesis 
go richard david kindly providing university waikato thesis style assistance latex 
special go family partner 
provided unconditional support encouragement lows time graduate school 
vi evaluating cfs ml algorithms artificial domains 
irrelevant attributes 
redundant attributes 
monk problems 
discussion 
natural domains 
chapter summary 
comparing cfs wrapper wrapper feature selection 
comparison 
chapter summary 
extending cfs higher order dependencies related 
joining features 
incorporating relief cfs 
evaluation 
discussion 
summary 


appendices graphs chapter curves concept added redundant attributes results cfs uc cfs mdl cfs relief natural domains cv paired test results ix cfs merit versus accuracy cfs applied uci domains bibliography list figures decision tree golf dataset 
branches correspond values attributes leaves indicate classifications 
filter wrapper feature selectors 
feature subset space golf dataset 
effects correlation outside variable composite variable number components inter correlations components rii correlations components outside variable 
effects varying attribute class level symmetrical uncertainty symmetrical relief normalized symmetrical mdl attributes informative graphs left noninformative graphs right 
curves shown classes 
effect varying training set size symmetrical uncertainty symmetrical relief normalized symmetrical mdl attributes informative non informative 
number classes curves shown valued attributes 
components cfs 
training testing data reduced contain features selected cfs 
dimensionally reduced data passed machine learning algorithm induction prediction 
effect cfs feature selection accuracy naive bayes classification 
dots show results statistically significant 
learning curve ib dataset added irrelevant attributes 
xi number relevant attributes selected concept added redundant features cfs uc cfs mdl cfs relief function training set size 
number multi valued attributes selected concept added redundant features cfs uc cfs mdl cfs relief function training set size 
number noisy attributes selected concept added redundant features cfs uc cfs mdl cfs relief function training set size 
learning curves nbayes naive bayes cfs uc nbayes cfs cfs relief nbayes concept added redundant features 
number redundant attributes selected concept added redundant features cfs uc cfs mdl cfs relief function training set size 
number relevant attributes selected concept added redundant features cfs uc cfs mdl cfs relief function training set size 
learning curves nbayes naive bayes cfs uc nbayes cfs cfs relief nbayes concept added redundant features 
learning curves nbayes naive bayes cfs uc nbayes cfs cfs relief nbayes concept added redundant features 
number natural domains cfs improved accuracy left degraded accuracy right naive bayes ib 
effect feature selection size trees induced natural domains 
bars zero line indicate feature selection reduced tree size 
dots show statistically significant results 
original number features natural domains left average number features selected cfs right 
xiii heuristic merit cfs uc vs actual accuracy naive bayes randomly selected feature subsets chess game horse colic audiology soybean 
point represents single feature subset 
absolute difference accuracy cfs uc merged subsets cfs uc naive bayes left ib middle right 
dots show statistically significant results 
wrapper feature selector 
comparing cfs wrapper naive bayes average accuracy naive bayes feature subsets selected cfs minus average accuracy naive bayes feature subsets selected wrapper 
dots show statistically significant results 
number features selected wrapper naive bayes left cfs right 
dots show number features original dataset 
comparing cfs wrapper average accuracy feature subsets selected cfs minus average accuracy feature subsets selected wrapper 
dots show statistically results 
average change size trees induced features selected wrapper left cfs right 
dots show statistically significant results 
effect varying training set size symmetrical uncertainty symmetrical relief normalized symmetrical mdl attributes informative non informative 
number classes curves shown valued attributes 
number redundant attributes selected concept cfs uc cfs mdl cfs relief function training set size 
number relevant attributes selected concept cfs uc cfs mdl cfs relief function training set size 
number multi valued attributes selected concept cfs uc cfs mdl cfs relief function training set size 
xiv number noisy attributes selected concept cfs uc cfs mdl cfs relief function training set size 
mushroom mu 
vote vo 
vote 
australian credit screening cr 
lymphography ly 
primary tumour pt 
breast cancer bc 
dna promoter dna 
audiology au 
soybean large sb 
horse colic hc 
chess game kr 
average number features selected cfs uci domains 
dots show original number features 
effect feature selection size trees induced uci domains 
bars zero line indicate feature selection reduced tree size 
xv xvi list tables golf dataset 
contingency tables compiled golf data 
computed distance values golf data 
greedy hill climbing search algorithm 
best search algorithm 
simple genetic search strategy 
valued non informative attribute valued attribute derived randomly partitioning larger number values 
attribute appears predictive class attribute information gain measure 
feature correlations calculated golf dataset 
relief calculate correlations 
forward selection search correlations table 
search starts empty set features merit 
subsets bold show local change previous best subset resulted improvement respect evaluation function 
domain characteristics 
datasets horizontal line natural domains artificial 
missing column shows percentage data set entries number features number instances missing values 
average feature vals max min feature vals calculated nominal features data sets 
training test set sizes natural domains monk problems 
feature class correlation assigned features symmetrical uncertainty mdl relief concept 
xvii feature class correlations assigned measures features dataset containing redundant features 
columns measure lists attribute original features number values attribute level redundancy 
average number features selected cfs uc cfs mdl cfs relief monk problems 
comparison naive bayes feature selection monk problems 
comparison ib feature selection monk problems 
comparison feature selection monk problems 
naive bayes ib feature selection natural domains 
comparison learning algorithms feature selection merged subsets 
top feature class correlations assigned cfs uc cfs mdl chess game dataset 
comparison naive bayes feature selection naive bayes feature selection wrapper cfs 
time taken cpu units wrapper cfs single trial dataset 
comparison feature selection feature selection wrapper cfs 
performance enhanced cfs cfs cfs relief compared standard cfs uc artificial domains ib induction algorithm 
figures braces show average number features selected 
xviii example effect redundant attribute relief distance calculation domain 
table shows instances domain table shows instances domain added redundant attribute 
column marked dist shows far particular instance instance 
performance enhanced cfs cfs cfs relief compared standard cfs uc artificial induction algorithm 
performance enhanced cfs cfs cfs relief compared standard cfs uc artificial naive bayes induction algorithm 
performance enhanced cfs cfs cfs relief compared standard cfs uc natural domains ib induction algorithm 
performance enhanced cfs cfs cfs relief compared standard cfs uc natural domains induction algorithm 
performance enhanced cfs cfs cfs relief compared standard cfs uc natural naive bayes induction algorithm 
accuracy naive bayes feature selection cfs uc compared feature selection cfs mdl cfs relief 
accuracy ib feature selection cfs uc compared feature selection cfs mdl cfs relief 
accuracy feature selection cfs uc compared feature selection cfs mdl cfs relief 
naive bayes ib feature selection natural domains 
cv test significance applied 
xix comparison naive bayes feature selection naive bayes feature selection wrapper cfs 
cv test significance applied 
comparison feature selection feature selection wrapper cfs 
cv test significance applied 
comparison learning algorithms feature selection cfs uc 
xx chapter live information age accumulating data easy storing inexpensive 
alleged amount stored information doubles months psf 
unfortunately amount machine readable information increases ability understand keep pace growth 
machine learning provides tools large quantities data automatically analyzed 
fundamental machine learning feature selection 
feature selection identifying salient features learning focuses learning algorithm aspects data useful analysis prediction 
hypothesis explored thesis feature selection supervised classification tasks accomplished basis correlation features feature selection process beneficial variety common machine learning algorithms 
technique correlation feature selection ideas test theory developed evaluated common machine learning algorithms variety natural artificial problems 
feature selector simple fast execute 
eliminates irrelevant redundant data cases improves performance learning algorithms 
technique produces results comparable state art feature selector literature requires computation 
motivation machine learning study algorithms automatically improve performance experience 
heart performance prediction 
algorithm data exemplifies task improves ability predict key elements task said learned 
machine learning algorithms broadly characterized language represent learned knowledge 
research shown single learning approach clearly superior cases fact different learning algorithms produce similar results ls 
factor enormous impact success learning algorithm nature data characterize task learned 
data fails exhibit statistical regularity machine learning algorithms exploit learning fail 
possible new data may constructed old way exhibit statistical regularity facilitate learning complexity task fully automatic method intractable tho 
data suitable machine learning task discovering regularities easier time consuming removing features data irrelevant redundant respect task learned 
process called feature selection 
process constructing new input data feature selection defined potential fully automatic computationally tractable process 
benefits feature selection learning include reduction amount data needed achieve learning improved predictive accuracy learned knowledge compact easily understood reduced execution time 
factors particular importance area commercial industrial data mining 
data mining term coined describe process sifting large databases interesting patterns relationships 
declining cost disk storage size corporate industrial databases grown point analysis parallelized machine learning algorithms running special parallel hardware infeasible jl 
approaches enable standard machine learning algorithms applied large databases feature selection sampling 
reduce size database feature selection identifying salient features data sampling identifying representative examples jl 
thesis focuses feature selection process benefit learning algorithms regardless amount data available learn 
existing feature selection methods machine learning typically fall broad categories evaluate worth features learning algorithm ultimately applied data evaluate worth features heuristics general characteristics data 
referred wrappers filters koh kj 
categories algorithms differentiated exact nature evaluation function space feature subsets explored 
wrappers give better results terms final predictive accuracy learning algorithm filters feature selection optimized particular learning algorithm 
learning algorithm employed evaluate set features considered wrappers prohibitively expensive run intractable large databases containing features 
furthermore feature selection process tightly coupled learning algorithm wrappers general filters re run switching learning algorithm 
author opinion advantages filter approaches feature selection outweigh disadvantages 
general filters execute times faster wrappers stand better chance scaling databases large number features wrappers 
filters require re execution different learning algorithms 
filters provide benefits learning wrappers 
improved accuracy particular learning algorithm required filter provide intelligent starting feature subset wrapper process result shorter faster search wrapper 
related scenario wrapper applied search filtered feature space reduced feature space provided filter 
methods help scale wrapper larger datasets 
reasons filter approach feature selection machine learning explored thesis 
filter algorithms previously described machine learning literature exhibited number drawbacks 
algorithms handle noise data require level noise roughly specified user priori 
cases subset features selected explicitly features ranked final choice left user 
cases user specify features required manually set threshold feature selection terminates 
algorithms require data transformed way increases initial number features 
case result dramatic increase size search space 
thesis statement thesis claims feature selection supervised machine learning tasks accomplished basis correlation features 
particular thesis investigates hypothesis feature subset contains features highly correlated predictive class uncorrelated predictive 
evaluation hypothesis accomplished creating feature selection algorithm evaluates worth feature sets 
implementation correlation feature selection cfs described chapter 
cfs measures correlation nominal features numeric features discretized 
general concept correlation feature selection depend particular data transformation supplied means measuring correlation variables 
principle technique may applied variety supervised classification problems including class variable predicted numeric 
cfs fully automatic algorithm require user specify thresholds number features selected simple incorporate desired 
cfs operates original albeit discretized feature space meaning knowledge induced learning algorithm features selected cfs interpreted terms original features terms transformed space 
importantly cfs filter incur high computational cost associated repeatedly invoking learning algorithm 
cfs assumes features conditionally independent class 
experiments chapter show cfs identify relevant features moderate feature dependencies exist 
features depend strongly class cfs fail select relevant features 
chapter explores methods detecting feature dependencies class 
thesis overview chapter defines terms provides overview concepts supervised machine learning 
reviews common machine learning algorithms techniques discretization process converting continuous attributes nominal attributes 
feature selectors including implementation cfs machine learning algorithms best suited handle problems attributes nominal 
chapter surveys feature selection techniques machine learning 
broad categories algorithms discussed involve machine learning scheme estimate worth features 
advantages disadvantages approaches discussed 
chapter begins presenting rationale correlation feature selection ideas borrowed test theory 
methods measuring association nominal variables reviewed empirically examined section 
behaviour measures respect attributes values number available training examples discussed emphasis suitability correlation feature selector 
section describes cfs implementation correlation feature selector rationale section incorporating correlation measures discussed section 
operational requirements assumptions algorithm discussed computational expense simple optimizations employed decrease execution time 
chapter describes datasets experiments discussed chapters 
outlines experimental method 
half chapter empirically tests variations cfs employing correlation measures examined chapter artificial problems 
shown cfs effective eliminating irrelevant redundant features identify relevant features long strongly depend features 
correlation measures shown inferior cfs 
second half chapter evaluates cfs machine learning algorithms applied natural learning domains 
results analyzed detail variations cfs 
shown cases cfs improves performance reduces size induced knowledge structures machine learning algorithms 
shortcoming cfs revealed results datasets 
cases cfs fail select locally predictive features especially overshadowed strong globally predictive ones 
method merging feature subsets shown partially mitigate problem 
chapter compares cfs known implementation wrapper approach feature selection 
results show cases cfs gives results comparable wrapper general outperforms wrapper small datasets 
cases cfs inferior wrapper attributed shortcoming algorithm revealed chapter presence strong class conditional feature dependency 
cfs shown execute significantly faster wrapper 
chapter presents methods extending cfs detect class conditional feature dependency 
considers pairwise combinations features second incorporates feature weighting algorithm sensitive higher order including higher pairwise feature dependency 
methods compared results show improve results datasets 
second method shown reliable 
chapter presents suggests 
chapter supervised machine learning concepts definitions field artificial intelligence embraces approaches artificial learning hut 
motivated study mental processes says artificial learning study algorithms embodied human mind 
aim discover algorithms translated formal languages computer programs 
second approach motivated practical computing standpoint aims 
involves developing programs learn past data branch data processing 
sub field machine learning come second approach artificial learning grown rapidly birth mid seventies 
machine learning concerned concept learning classification learning 
simply generalization tho 
classification task learning classify objects pre specified set categories classes characteristic intelligence keen interest researchers psychology computer science 
identifying common core characteristics set objects representative class enormous focusing attention person computer program 
example determine animal zebra people know look stripes examine tail ears 
stripes strongly concept generalization zebras 
course stripes sufficient form class description zebras tigers certainly important characteristics 
ability perform classification able learn classify gives people computer programs power decisions 
efficacy decisions affected performance classification task 
machine learning classification task described commonly referred supervised learning 
supervised learning specified set classes example objects labeled appropriate class example program told zebra 
goal generalize form class descriptions training objects enable novel objects identified belonging classes 
contrast supervised learning unsupervised learning 
case program told objects zebras 
goal unsupervised learning decide objects grouped words learner forms classes 
course success classification learning heavily dependent quality data provided training learner input learn 
data inadequate irrelevant concept descriptions reflect misclassification result applied new data 
data representation typical supervised machine learning task data represented table examples instances 
instance described fixed number measurements features label denotes class 
features called attributes typically types nominal values members unordered set numeric values real numbers 
table qui shows fourteen instances suitable unsuitable days play game golf 
instance day described terms nominal attributes outlook humidity temperature wind class label indicates day suitable playing golf 
typical application machine learning algorithms requires sets examples training examples test examples 
set training examples produce instance features class outlook temperature humidity wind sunny hot high false don play sunny hot high true don play overcast hot high false play rain mild high false play rain cool normal false play rain cool normal true don play overcast cool normal true play sunny mild high false don play sunny cool normal false play rain mild normal false play sunny mild normal true play overcast mild high true play overcast hot normal false play rain mild high true don play table golf dataset 
learned concept descriptions separate set test examples needed evaluate accuracy 
testing class labels algorithm 
algorithm takes input test example produces output class label predicted class example 
learning algorithms learning algorithm induction algorithm forms concept descriptions example data 
concept descriptions referred knowledge model learning algorithm induced data 
knowledge may represented differently algorithm 
example qui represents knowledge decision tree naive bayes mit represents knowledge form probabilistic summaries 
thesis machine learning algorithms basis comparing effects feature selection feature selection 
naive bayes ib represents different approach learning 
algorithms known machine learning community proved popular practice 
sophisticated algorithm induces knowledge arguably easier interpret 
ib naive bayes proved popular simple implement shown perform competitively complex algorithms cn cs ls sections briefly review algorithms indicate conditions feature selection useful 
naive bayes naive bayes algorithm employs simplified version bayes formula decide class novel instance belongs 
posterior probability class calculated feature values instance instance assigned class highest probability 
equation shows naive bayes formula assumption feature values statistically independent class 
ci 
vn ci vj ci 
vn left side equation posterior probability class ci feature values 
vn observed instance classified 
denominator right side equation omitted constant easily computed requires posterior probabilities classes sum 
learning naive bayes classifier straightforward involves simply estimating probabilities right side equation training instances 
result probabilistic summary possible classes 
numeric features common practice assume normal distribution necessary parameters estimated training data 
tables contingency tables showing frequency distributions relationships features class golf dataset table 
tables easy calculate probabilities necessary apply equation 
imagine morning wished determine day suitable game golf 
noting outlook sunny temperature hot humidity play don play sunny overcast rain outlook play don play hot mild cool temperature play don play high norm humidity play don play true false wind table contingency tables compiled golf data 
mal wind wind false apply equation calculate posterior probability class probabilities derived tables don play sunny hot normal false don play sunny don play hot don play normal don play false don play 
play sunny hot normal false play sunny play hot play normal play false play 
day play golf 
due assumption feature values independent class naive bayesian classifier predictive performance adversely affected presence redundant attributes training data 
example feature perfectly correlated second feature treating independent means twice affect equation 
langley sage ls naive bayes performance improves redundant features removed :10.1.1.43.3692
domingos pazzani dp strong correlations features degrade performance naive bayes perform moderate dependencies exist data :10.1.1.144.7475
explanation moderate dependencies result inaccurate probability estimation probabilities far wrong result increased mis classification 
version naive bayes experiments described thesis provided mlc utilities 
version probabilities nominal features estimated frequency counts calculated training data 
probabilities numeric features assumed come normal distribution necessary parameters estimated training data 
zero frequencies replaced probability number training examples 
decision tree generator qui predecessor id qui algorithms summarise training data form decision tree 
systems induce logical rules decision tree algorithms proved popular practice 
due part robustness execution speed fact explicit concept descriptions produced users interpret 
shows decision tree summarises golf data 
nodes tree correspond features branches associated values 
leaves tree correspond classes 
classify new instance simply examines features tested nodes tree follows branches corresponding observed values instance 
reaching leaf process terminates class leaf assigned instance 
decision tree classify example day sunny hot normal false initially involves examining feature root tree outlook 
value outlook sunny overcast rain humidity play wind high normal true false don play play don play play decision tree golf dataset 
branches correspond values attributes leaves indicate classifications 
outlook new instance sunny left branch followed 
value humidity evaluated case new instance value normal right branch followed 
brings leaf node instance assigned class play 
build decision tree training data id employ greedy approach uses information theoretic measure guide 
choosing attribute root tree divides training instances subsets corresponding values attribute 
entropy class labels subsets entropy class labels full training set information gained see section chapter splitting attribute 
uses gain ratio criterion qui select attribute attribute root tree 
gain ratio criterion selects attributes average better gain attribute ratio gain divided entropy 
algorithm applied recursively form sub trees terminating subset contains instances class 
main difference id prunes decision trees 
pruning simplifies decision trees reduces probability overfitting training data qui 
prunes upper bound confidence interval resubstitution error 
node replaced best leaf estimated error leaf standard deviation estimated error node 
proven benchmark performance machine learning algorithms measured 
algorithm robust accurate fast added bonus produces comprehensible structure summarising knowledge induces 
deals remarkably irrelevant redundant information feature selection generally resulted little improvement accuracy 
removing irrelevant redundant information reduce size trees induced kj 
smaller trees preferred easier understand 
version experiments thesis original algorithm implemented quinlan qui 
ib instance learner instance learners represent knowledge form specific cases experiences 
rely efficient matching methods retrieve stored cases applied novel situations 
naive bayes algorithm instance learners usually computationally simple variations considered models human learning 
instance learners called lazy learners learning delayed classification time power residing matching scheme 
ib aka implementation simplest similarity learner known nearest neighbour 
ib simply finds stored instance closest euclidean distance metric instance classified 
new instance assigned retrieved instance class 
equation shows distance metric employed ib 
xj yj equation gives distance instances xj yj refer jth feature value instance respectively 
numeric valued attributes xj yj xj yj symbolic valued attributes feature values xj yj differ 
table shows distance example day sunny hot normal false instances golf data set equation 
case instances equally close example day arbitrary choice 
extension nearest neighbour algorithm called nearest neighbours uses prevalent class closest cases novel instance parameter set user 
instance distance instance distance table computed distance values golf data 
simple nearest neighbour algorithm known adversely affected presence irrelevant features training data 
nearest neighbour learn presence irrelevant information requires training data fact amount training data needed sample complexity reach maintain accuracy level shown grow exponentially number irrelevant attributes aka ls ls 
possible improve predictive performance nearest neighbour training data limited removing irrelevant attributes 
furthermore nearest neighbour slow execute due fact example classified compared stored training cases turn 
feature selection reduce number training cases fewer features equates fewer distinct instances especially features nominal 
reducing number training cases needed maintaining acceptable error rate dramatically increase speed algorithm 
version ib experiments thesis version implemented david aha aka 
equation compute similarity instances 
attribute values linearly normalized ensure attribute affect similarity function 
performance evaluation evaluating performance learning algorithms fundamental aspect machine learning 
important order compare competing algorithms cases integral part learning algorithm 
estimate classification accuracy new instances common performance evaluation criterion information theory suggested kb 
thesis classification accuracy primary evaluation criterion experiments feature selection machine learning algorithms 
feature selection considered successful dimensionality data reduced accuracy learning algorithm improves remains 
case size number nodes induced trees important smaller trees preferred easier interpret 
classification accuracy defined percentage test examples correctly classified algorithm 
error rate measure commonly statistics algorithm minus accuracy 
measuring accuracy test set examples better training set examples test set induce concept descriptions 
training set measure accuracy typically provide optimistically biased estimate especially learning algorithm overfits training data 
strictly speaking definition accuracy sample accuracy algorithm 
sample accuracy estimate true accuracy algorithm probability algorithm correctly classify instance drawn unknown distribution examples 
data limited common practice resample data partition data training test sets different ways 
learning algorithm trained tested partition accuracies averaged 
doing provides reliable estimate true accuracy algorithm 
random subsampling fold cross validation common methods resampling gei sch 
random subsampling data randomly partitioned disjoint training test sets multiple times 
accuracies obtained partition averaged 
fold cross validation data randomly split mutually exclusive subsets approximately equal size 
learning algorithm trained tested times time tested folds trained remaining folds 
cross validation estimate accuracy number correct classifications divided number examples data 
random subsampling method advantage repeated indefinite number times 
disadvantage test sets independently drawn respect underlying distribution examples test paired differences random subsampling lead increased chance type error identifying significant difference exist die 
test accuracies produced fold fold cross validation lower chance type error may give stable estimate accuracy 
common practice repeat fold cross validation times order provide stable estimate 
course renders test sets non independent increases chance type error 
unfortunately satisfactory solution problem 
alternative tests suggested dietterich die low chance type error high chance type ii error failing identify significant difference exist 
stratification process applied random subsampling fold crossvalidation 
stratification ensures class distribution dataset preserved training test sets 
stratification shown help reduce variance estimated accuracy especially datasets classes koh 
stratified random subsampling paired test evaluate accuracy 
appendix reports results major experiments cv paired test recommended dietterich die 
stated test decreased chance type error increased chance type ii error see appendix details 
plotting learning curves way machine learning algorithms compared 
learning curve plots classification accuracy learning algorithm function size training set shows quickly algorithm accuracy improves access training examples 
situations training data limited preferable learning algorithm achieves high accuracy small training sets 
attribute discretization classification tasks machine learning involve learning distinguish nominal class values may involve features ordinal continuous nominal 
machine learning algorithms developed deal mixed data sort research tin dks shows common machine learning algorithms instance learners naive bayes benefit treating features uniform fashion 
common methods accomplishing called discretization 
discretization process transforming continuous valued attributes nominal 
fact decision tree algorithm qui accomplishes internally dividing continuous features discrete ranges construction decision tree 
feature selection algorithms described chapter require continuous features discretized give superior results discretization performed outset ad ks ls 
discretization preprocessing step correlation approach feature selection thesis requires features type 
section describes discretization approaches machine learning literature 
cart ww ct machine learning algorithms capable dealing continuous class data :10.1.1.51.4098
methods discretization dougherty kohavi sahami dks define axes discretization methods categorised 
supervised versus 
unsupervised 
global versus 
local 
static versus 
dynamic 
supervised methods class label discretizing features 
distinction global local methods discretization performed 
global methods discretize features prior induction local methods carry discretization induction process 
local methods may produce different discretizations particular local regions instance space 
discretization methods require parameter indicating maximum number intervals divide feature 
static methods perform discretization pass data feature determine value feature independently 
hand dynamic methods search space possible values features simultaneously 
allows inter dependencies feature discretization captured 
global methods discretization relevant feature selection algorithm thesis feature selection generally global process single feature subset chosen entire instance space 
kohavi sahami ks compared static discretization dynamic methods cross validation estimate accuracy different values report significant improvement employing dynamic discretization static methods 
sections discuss methods unsupervised supervised global discretization numeric features common usage 
unsupervised methods simplest discretization method called equal interval example may split continuous feature differently different branches decision tree width 
approach divides range observed values feature equal sized bins parameter provided user 
dougherty dks point method discretization sensitive outliers may drastically skew range 
example observed feature values setting gives bin width resulting discrete ranges reasonably distribution examples bins 
suppose outlying value 
cause ranges formed 
case examples example value fall bin 
simple discretization method equal frequency intervals requires feature values sorted assigns values bin 
wong chiu wc describe variation equal frequency intervals called maximal marginal entropy iteratively adjusts boundaries minimise entropy interval 
unsupervised methods class setting interval boundaries dougherty dks note classification information lost result placing values strongly associated different classes interval 
section discusses methods supervised discretization overcome problem 
supervised methods holte hol presents simple supervised discretization method incorporated level decision tree algorithm 
method sorts values feature attempts find interval boundaries interval strong majority particular class 
method constrained form intervals minimal size order avoid having intervals instances 
setiono liu sl statistically justified heuristic method supervised discretization called chi 
numeric feature initially sorted placing observed value interval 
step uses chi square statistic determine relative frequencies classes adjacent intervals similar justify merging 
formula computing value adjacent intervals aij eij eij number classes aij number instances th interval class ri number instances th interval cj number instances class intervals total number instances intervals eij expected frequency aij ri cj extent merging process controlled automatically set threshold 
threshold determined attempting maintain fidelity original data 
catlett cat fayyad irani fi minimum entropy heuristic discretize numeric features 
algorithm uses class entropy candidate partitions select cut point discretization 
method applied recursively intervals previous split stopping conditions satisfied creating multiple intervals feature 
set instances feature cut point class information entropy partition induced ent ent intervals bounded cut point ent class entropy subset ent ci log ci 
feature cut point minimises equation selected conditionally binary discretization boundary 
catlett cat employs ad hoc criteria terminating splitting procedure 
include stopping number instances partition sufficiently small stopping maximum number partitions created stopping entropy induced possible cut points set equal 
fayyad irani fi employ stopping criterion minimum description length principle ris 
stopping criterion prescribes accepting partition induced cost encoding partition classes instances intervals induced cost encoding classes instances splitting 
partition induced cut point accepted iff gain log number instances set gain ent log cent ent ent 
equation number distinct classes respectively 
qui qui uses equation locally nodes decision tree determine binary split numeric feature 
kohavi sahami ks perform global discretization numeric features 
applied numeric feature separately build tree contains binary splits test single feature 
internal pruning mechanism applied determine appropriate number nodes tree number discretization intervals 
number studies dks ks comparing effects various discretization techniques common machine learning domains algorithms method fayyad irani superior 
reason method discretization experiments described chapters 
chapter feature selection machine learning factors affect success machine learning task 
representation quality example data foremost 
theoretically having features result discriminating power 
practical experience machine learning algorithms shown case 
learning algorithms viewed making biased estimate probability class label set features 
complex high dimensional distribution 
unfortunately induction performed limited data 
estimating probabilistic parameters difficult 
order avoid overfitting training data algorithms employ occam razor gl bias build simple model achieves acceptable level performance training data 
bias leads algorithm prefer small number predictive attributes large number features proper combination fully predictive class label 
irrelevant redundant information data noisy unreliable learning training phase difficult 
feature subset selection process identifying removing irrelevant redundant information possible 
reduces dimensionality data may allow learning algorithms operate faster effectively 
cases accuracy classification improved result compact easily interpreted representation target concept 
research shown common machine learning algorithms adversely affected irrelevant redundant training information 
simple nearest neighbour algorithm sensitive irrelevant attributes sample complexity number training examples needed reach accuracy level grows exponentially number irrelevant attributes ls ls aka 
sample complexity decision tree algorithms grow exponentially concepts parity 
naive bayes classifier adversely affected redundant attributes due assumption attributes independent class ls :10.1.1.43.3692
decision tree algorithms qui qui overfit training data resulting large trees 
cases removing irrelevant redundant information result producing smaller trees kj 
chapter begins highlighting common links feature selection pattern recognition statistics feature selection machine learning 
important aspects feature selection algorithms described section 
section outlines common heuristic search techniques 
sections review current approaches feature selection machine learning literature 
feature selection statistics pattern recognition feature subset selection long research area statistics pattern recognition dk mil 
surprising feature selection issue machine learning pattern recognition fields share common task classification 
pattern recognition feature selection impact economics data acquisition accuracy complexity classifier dk 
true machine learning added concern distilling useful knowledge data 
fortunately feature selection shown improve comprehensibility extracted knowledge kj 
machine learning taken inspiration borrowed pattern recognition statistics 
example heuristic search technique sequential backward elimination section introduced marill green mg kittler kit introduced different variants including forward method stepwise method 
cross validation estimating accuracy feature subset backbone wrapper method machine learning suggested allen applied problem selecting predictors linear regression 
statistical methods evaluating worth feature subsets characteristics training data applicable numeric features 
furthermore measures monotonic increasing size feature subset decrease performance condition hold practical machine learning algorithms search algorithms dynamic programming branch bound nf rely monotonicity order prune search space applicable feature selection algorithms attempt match general bias machine learning algorithms 
characteristics feature selection algorithms feature selection algorithms notable exceptions perform search space feature subsets consequence address basic issues affecting nature search lan 
starting point 
selecting point feature subset space search affect direction search 
option features successively add attributes 
case search said proceed forward search space 
conversely search features successively remove 
case search proceeds backward search space 
alternative middle move outwards point 

search organisation 
exhaustive search feature subspace prohibitive small initial number features 
initial features exist possible subsets 
heuristic search strategies feasible exhaustive measures residual sum squares rss mallows cp separability measures ratio generalisations described miller mil parsons par respectively 
example decision tree algorithms qui discover regularities training data partitioning data basis observed feature values 
maintaining statistical reliability avoiding overfitting necessitates small number strongly predictive attributes 
ones give results guarantee finding optimal subset 
section discusses heuristic search strategies feature selection 

evaluation strategy 
feature subsets evaluated single biggest differentiating factor feature selection algorithms machine learning 
paradigm dubbed filter koh kj operates independent learning algorithm undesirable features filtered data learning begins 
algorithms heuristics general characteristics data evaluate merit feature subsets 
school thought argues bias particular induction algorithm taken account selecting features 
method called wrapper koh kj uses induction algorithm statistical re sampling technique cross validation estimate final accuracy feature subsets 
illustrates filter wrapper approaches feature selection 

stopping criterion 
feature selector decide searching space feature subsets 
depending evaluation strategy feature selector adding removing features alternatives improves merit current feature subset 
alternatively algorithm continue revise feature subset long merit degrade 
option continue generating feature subsets reaching opposite search space select best 
heuristic search searching space feature subsets reasonable time constraints necessary feature selection algorithm operate data large number features 
simple search strategy called greedy hill climbing considers local changes current feature subset 
local change simply addition deletion single feature subset 
algorithm considers additions feature subset testing data training data search training data feature set dimensionality reduction training data feature set heuristic merit ml algorithm hypothesis feature evaluation final evaluation estimated accuracy filter testing data training data feature set feature set cv fold search feature evaluation cross validation ml algorithm wrapper estimated accuracy hypothesis training data feature set dimensionality reduction ml algorithm final evaluation training data hypothesis estimated accuracy filter wrapper feature selectors 
known forward selection considering deletions known backward elimination kit mil 
alternative approach called stepwise bi directional search uses addition deletion 
variations search algorithm may consider possible local changes current subset select best may simply choose change improves merit current feature subset 
case change accepted reconsidered 
shows feature subset space golf data 
scanned top bottom diagram shows local additions node scanned bottom top diagram shows possible local deletions node 
table shows algorithm greedy hill climbing search 
best search rk ai search strategy allows backtracking search path 
greedy hill climbing best moves search space making local temp hum wind temp hum wind temp hum temp wind hum wind temp hum temp wind hum wind temp hum wind temp hum wind feature subset space golf dataset 

start state 

expand making possible local change 

evaluate child 
child highest evaluation 

goto 
return table greedy hill climbing search algorithm changes current feature subset 
hill climbing path explored begins look promising best search back track promising previous subset continue search 
time best search explore entire search space common stopping criterion 
normally involves limiting number fully expanded subsets result improvement 
table shows best search algorithm 

open list containing start state closed list empty best start state 

arg max get state open highest evaluation 

remove open add closed 

best best 
child open closed list evaluate add open 

best changed set expansions goto 
return best 
table best search algorithm genetic algorithms adaptive search techniques principles natural selection biology hol 
employ population competing solutions evolved time converge optimal solution 
effectively solution space searched parallel helps avoiding local optima 
feature selection solution typically fixed length binary string representing feature subset value position string represents presence absence particular feature 
algorithm iterative process successive generation produced applying genetic operators crossover mutation members current generation 
mutation changes values adding deleting features subset randomly 
crossover combines different features pair subsets new subset 
application genetic operators population members determined fitness feature subset respect evaluation strategy 
better feature subsets greater chance selected form new subset crossover mutation 
manner subsets evolved time 
table shows simple genetic search strategy 
fully expanded subset possible local changes considered 

randomly generating initial population 
calculate member 
define probability distribution members 

select population members respect 
apply crossover produce new population members 

apply mutation 

insert generation 

goto 


generations process goto 

return highest 
table simple genetic search strategy 
feature filters earliest approaches feature selection machine learning filter methods 
filter methods heuristics general characteristics data learning algorithm evaluate merit feature subsets 
consequence filter methods generally faster wrapper methods practical data high dimensionality 
consistency driven filters almuallim ad describe algorithm originally designed boolean domains called focus 
focus exhaustively searches space feature subsets finds minimum combination features divides training data pure classes combination feature values associated single class 
referred min features bias 
feature selection final feature subset passed id qui constructs decision tree 
main difficulties focus pointed freitag cf 
firstly focus driven attain consistency training data exhaustive search may intractable features needed attain consistency 
secondly strong bias consistency statistically unwarranted may lead overfitting training data algorithm continue add features repair single inconsistency 
authors address problems ad 
algorithms consisting forward selection search coupled heuristic approximate min features bias methods focus computationally feasible domains features 
algorithm evaluates features information theoretic formula entropy pi ni sample pi pi ni log pi pi ni ni pi ni log ni pi ni 
feature subset possible truth value assignments features 
feature set divides training data groups instances truth value assignments features equation measures entropy class values groups pi ni denote number positive negative examples th group respectively 
stage feature minimises equation added current feature subset 
second algorithm chooses discriminating feature add current subset stage search 
pair positive negative examples feature discriminating value differs 
stage feature chosen discriminates greatest number positive negative pairs examples discriminated existing feature subset 
third algorithm second positive negative example pair contributes weighted increment score feature discriminates 
increment depends total number features discriminate pair 
liu setiono ls describe algorithm similar focus called lvf 
fo cus lvf consistency driven focus handle noisy domains approximate noise level known priori 
lvf generates random subset feature subset space round execution 
contains fewer features current best subset inconsistency rate dimensionally reduced data described compared inconsistency rate best subset 
consistent best subset replaces best subset 
inconsistency rate training data prescribed feature subset defined groups matching instances 
group matching instances inconsistency count number instances group minus number instances group frequent class value 
inconsistency rate sum inconsistency counts groups matching instances divided total number instances 
liu setiono report results lvf applied artificial domains mixed results applied commonly natural domains 
applied lvf large data sets having instances described attributes second having instances described attributes 
report lvf able reduce number attributes data sets half 
note due random nature lvf longer allowed execute better results measured inconsistency criterion 
feature selection rough sets theory mod paw uses notions consistency similar described 
rough sets theory information system tuple finite universe instances 
finite set features 
set possible feature values 
information function 
instance feature maps value subset features relation ind defined ind feature relation equivalence relation partitions instances equivalence classes sets instances respect features partition classification denoted ind 
supervised machine learning sets instances respect class attribute contain obviously instances class 
subset instances subset features lower upper approximations defined follows ind ind exact set definable feature subset rough set respect instances classified equivalence classes ind feature set called positive region respect defined follows ind 
degree consistency afforded feature subset respect equivalence classes ind totally consistent respect 
feature selection rough sets theory achieved identifying reduct set features 
set reduct independent ind ind 
independent exist strict subset ind ind 
reduct property feature removed changing relation 
rough sets lvf algorithm assign higher consistency attributes values 
extreme example attribute values instances 
attribute little power generalize training data 
attribute class attribute easy show contains instances 
similarly lvf feature guarantees inconsistency data 
feature selection discretization setiono liu sl note discretization potential perform feature selection numeric features 
numeric feature justifiably discretized single value safely removed data 
combined discretization feature selection algorithm chi discussed section uses chi square statistic perform discretization 
numeric attributes initially sorted placing observed value interval 
numeric attribute repeatedly discretized test determine adjacent intervals merged 
extent merging process controlled automatically set threshold 
threshold determined attempting maintain original fidelity data inconsistency measured way lvf algorithm described controls process 
authors report results natural domains containing mixture numeric nominal features qui qui discretization 
conclude chi effective improving performance eliminating features 
clear improvement due entirely features having removed discretization plays role 
learning algorithm filter researchers explored possibility particular learning algorithm pre processor discover useful feature subsets primary learning algorithm 
cardie car describes application decision tree algorithms task selecting feature subsets instance learners 
applied natural language data sets features appeared final decision trees element ind set containing exactly unique instance element ind subset equivalence classes ind 
nearest neighbour classifier 
hybrid system resulted significantly better performance nearest neighbour algorithm 
similar approach singh provan sp greedy oblivious decision tree algorithm select features construct bayesian network 
oblivious decision trees differ constructed algorithms nodes level oblivious decision tree test attribute 
feature subsets selected oblivious decision tree algorithms employing different information theoretic splitting criterion evaluated bayesian network classifier machine learning datasets 
results showed bayesian networks features selected oblivious decision tree algorithms outperformed bayesian networks feature selection bayesian networks features selected wrapper 
holmes nevill manning holte system hol estimate predictive accuracy individual features 
builds rules single features called rules 
data split training test sets possible calculate classification accuracy rule feature 
classification scores ranked list features obtained 
experiments choosing select number highest ranked features common machine learning algorithms showed average top features accurate original set 
approach unusual due fact search conducted 
relies user decide features include ranked list final subset 
pfahringer pfa uses program inducing decision table majority classifiers select features 
dtm decision table majority classifiers simple type nearest neighbour classifier similarity function restricted returning stored instances exact matches instance classified 
instances returned prevalent class training data predicted class majority class matching instances 
best features nominal 
induction dtm achieved greedily searching space possible decision tables 
decision table defined features includes induction simply rules thought single level decision trees 
feature selection 
pfahringer approach minimum description length mdl principle ris guides search estimating cost encoding decision table training examples misclassifies respect feature subset 
features appearing final decision table learning algorithms 
experiments small selection machine learning datasets showed feature selection dtm induction improve accuracy cases 
dtm classifiers induced mdl compared induced cross validation wrapper approach estimate accuracy tables feature sets 
mdl approach shown efficient perform cross validation 
information theoretic feature filter koller sahami ks introduced feature selection algorithm ideas information theory probabilistic reasoning pea 
rationale approach goal induction algorithm estimate probability distributions class values original feature set feature subset selection attempt remain close original distributions possible 
formally set classes set features subset assignment values 
vn features vx projection values variables goal feature selector choose pr vx close possible pr 
achieve goal algorithm begins original features employs backward elimination search remove stage feature causes change distributions 
reliable estimate high order probability distributions limited data approximate algorithm uses pair wise combinations features 
cross entropy measure difference distributions user specify features removed algorithm 
cross entropy class distribution pair features pr vi vi vj vj pr vj vj vi vi vj vj vi vi vj vj log 
vj vj feature algorithm finds set mi containing attributes remain subsume information feature class values 
mi contains features remaining features value equation smallest 
expected cross entropy distribution class values mi vi distribution class values just mi calculated feature feature quantity minimal removed set 
process iterates user specified number features removed original set 
experiments natural domains artificial domains naive bayes final induction algorithm showed feature selector gives best results size conditioning set set 
domains containing features algorithm able reduce number features half improving accuracy percent 
problem algorithm requires features values encoded binary order avoid bias entropic measures features values 
greatly increase number features original data introducing dependencies 
furthermore meaning original attributes obscured making output algorithms hard interpret 
instance approach feature selection kira rendell kr describe algorithm called relief uses instance learning assign relevance weight feature 
feature weight reflects ability distinguish class values 
features ranked weight exceed user specified threshold selected form final subset 
algorithm works randomly sampling instances training data 
instance sampled nearest instance class nearest hit opposite class nearest 
attribute weight updated values distinguish sampled instance nearest hit nearest attribute receive high weight differentiates instances different classes value instances class 
equation shows weight updating formula mi approximation markov blanket pea feature relief wx wx diff diff wx weight attribute randomly sampled instance nearest hit nearest number randomly sampled instances 
function diff calculates difference instances attribute 
nominal attributes defined values different values continuous attributes difference actual difference normalised interval 
dividing guarantees weights interval 
relief operates class domains 
kononenko kon describes enhancements relief enable cope multi class noisy incomplete domains 
kira rendell provide experimental evidence shows relief effective identifying relevant features interact example parity problems 
relief handle redundant features 
authors state features relevant concept relief select features small number necessary concept description brauer sb describe similar instance approach assigning feature weights developed independently relief 
relief strives reinforce similarities instances class simultaneously decrease similarities instances different classes 
gradient descent approach employed optimize feature weights respect goal 
feature wrappers wrapper strategies feature selection induction algorithm estimate merit feature subsets 
rationale wrapper approaches induction method interacting features values dependent values features class provide information class 
hand redundant features values dependent values features irrespective class provide information class 
ultimately feature subset provide better estimate accuracy separate measure entirely different inductive bias lan 
feature wrappers achieve better results filters due fact tuned specific interaction induction algorithm training data 
tend slower feature filters repeatedly call induction algorithm re run different induction algorithm 
wrapper defined process variation application due method estimate sample accuracy target induction algorithm target induction algorithm organisation search 
section reviews focused wrapper approach methods reduce computational expense 
wrappers decision tree learners john kohavi pfleger advocate wrapper general framework feature selection machine learning 
formal definitions degrees feature relevance claim wrapper able discover relevant features 
feature xi said strongly relevant target concept probability distribution class values full feature set changes xi removed 
feature xi said weakly relevant strongly relevant probability distribution class values subset containing xi full feature set changes xi removed 
features strongly weakly relevant irrelevant 
experiments conducted artificial natural domains id qui qui induction algorithms 
accuracy estimated fold cross validation training data disjoint test set reporting final accuracies 
forward selection backward elimination search 
exception artificial domain results showed feature selection significantly change id generalisation performance 
main effect feature selection reduce size trees 
john freitag cf test number greedy search methods id calendar scheduling domains 
backward elimination ward selection test variants stepwise bi directional search starting features 
results showed bi directional searches slightly outperformed forward backward searches little difference various search strategies respect computation time 
feature selection able improve performance id calendar scheduling domains 
vafaie de jong vj shavlik cs applied genetic search strategies wrapper framework improving performance decision tree learners 
vafaie de jong vj describe system genetic algorithm driven modules performs feature selection second performs constructive induction mic 
modules able significantly improve performance id texture classification problem 
shavlik cs algorithm called set gen strives improve decision trees accuracy 
set gen genetic search uses fitness function linear combination accuracy term simplicity term fitness feature subset average cross validation accuracy average size trees produced normalized number training examples number features subset normalized total number available features 
equation ensures fittest population members feature subsets lead induce small accurate decision trees 
wrappers instance learning wrapper approach proposed approximately time independently constructive induction process creating new attributes applying logical mathematical operators original features 
john langley sage ls ls investigation simple nearest neighbour algorithm sensitivity irrelevant attributes 
scaling experiments showed nearest neighbour sample complexity number training examples needed reach accuracy increases exponentially number irrelevant attributes data aka ls ls 
algorithm called performs backward elimination features oblivious decision tree induction algorithm 
experiments fold cross validation artificial domains showed able remove redundant features learn faster domains features interact 
moore lee ml take similar approach augmenting nearest neighbour algorithms system uses leave fold cross validation concentrates improving prediction numeric discrete classes :10.1.1.108.8901
aha ab leave cross validation pair beam search hill climbing 
results show feature selection improve performance ib nearest neighbour classifier sparse instances cloud pattern domain features 
moore hill johnson encompass feature selection wrapper process number nearest neighbours prediction space combination functions 
leave cross validation achieve significant improvement control problems involving prediction continuous classes 
similar vein skalak ska combines feature selection prototype selection single wrapper process random mutation hill climbing search strategy 
experimental results showed significant improvement accuracy nearest neighbour natural domains drastic reduction algorithm storage requirement number instances retained training 
domingos dom describes context sensitive wrapper approach feature selection instance learners 
motivation approach may features relevant restricted area instance space irrelevant original features included tree number assumptions classification time langley sage note structure functionally equivalent simple nearest neighbour fact implemented 
beam search limited version best search remembers portion search path backtracking relevant certain values weakly interacting features irrelevant 
case features estimated globally entire instance space irrelevant aspects sorts features may overwhelm useful aspects instance learners 
true backward search strategies wrapper domingos presents algorithm called rc detect context sensitive features 
rc works selecting potentially different set features instance training set 
backward search strategy cross validation estimate accuracy 
instance training set rc finds nearest neighbour class removes features differ 
accuracy entire training dataset estimated cross validation 
accuracy degraded modified instance question accepted instance restored original state deactivated feature selection attempted 
feature selection process continues instances inactive 
experiments selection machine learning datasets showed rc outperformed standard wrapper feature selectors forward backward search strategies instance learners 
effectiveness context sensitive approach shown artificial domains engineered exhibit restricted feature dependency 
features globally relevant irrelevant rc advantage standard wrapper feature selection 
furthermore examples available data noisy standard wrapper approaches detect globally irrelevant features easily rc 
domingos noted wrappers employ instance learners including rc unsuitable databases containing instances quadratic number instances 
kohavi kf koh uses wrapper feature selection explore potential decision table majority dtm classifiers 
appropriate data structures allow fast incremental cross validation dtm classifiers 
experiments showed dtm classifiers appropriate feature subsets compared favourably sophisticated algorithms wrapper approach backward search strategies generally effective forward search strategies domains feature interactions 
backward search typically begins features removal strongly interacting feature usually detected decreased accuracy cross validation 

wrappers bayes classifiers due naive bayes classifier assumption class probability distributions attributes independent langley sage ls note performance domains redundant features improved removing features :10.1.1.43.3692
forward search strategy employed select features naive bayes opposed backward strategies decision tree algorithms instance learners 
rationale forward search immediately detect dependencies harmful redundant attributes added 
experiments showed improvement increased learning rate natural domains change remaining 
pazzani paz combines feature selection simple constructive induction wrapper framework improving performance naive bayes 
forward backward hill climbing search strategies compared 
case algorithm considers addition single features current subset creating new attribute joining unselected features selected features subset 
case algorithm considers deleting individual features replacing pairs features joined feature 
results selection machine learning datasets show approaches improve performance naive bayes 
forward strategy better job removing redundant attributes backward strategy 
starts full set features considers possible pairwise joined features backward strategy effective identifying attribute interactions forward strategy 
improvement naive bayes wrapper feature selection reported kohavi sommerfield ks kohavi john kj 
provan singh ps applied wrapper select features construct bayesian networks 
results showed feature selection improve accuracy networks constructed full set features networks con structed feature selection considerably smaller faster learn 
methods improving wrapper criticism wrapper approach feature selection concerned computational cost 
feature subset examined induction algorithm invoked times fold cross validation 
wrapper prohibitively slow large data sets features 
drawback led researchers investigate ways mitigating cost evaluation process 
freitag cf devised scheme caches decision trees 
substantially reduce number trees grown feature selection allow larger spaces searched 
moore lee ml method race competing models feature subsets :10.1.1.108.8901
point leave cross validation subset deemed lowest estimated error evaluation terminated 
effect reducing percentage training examples evaluation reduces computational cost fully evaluating subset 
algorithm blocks near identical feature subsets race 
prevents having run feature subsets nearly identical predictions right 
racing blocking bayesian statistics maintain probability distribution estimate mean leave cross validation error competing subset 
algorithm uses forward selection sequentially trying local changes best subset changes raced 
race finishes competing subset remains cross validation ends 
kohavi john ks introduce notion compound search space operators attempt backward best search strategies computationally feasible 
local changes additions deletions single features feature subset evaluated compound operator created combining best local changes 
operator applied feature subset creating new subset away search space 
compound operator leads subset improved estimate second compound operator constructed combines best local changes forth 
compound operators search quickly strongly relevant features 
experiments compound operators forward best search showed significant change accuracy id naive bayes 
compound operators combined backward best search accuracy degraded slightly id improved 
poor results id suggest best search get stuck local maxima 
improvement due pruning form feature selection allows best search overcome local maxima 
moore lee ml describe search variant called schemata search takes interacting features account speeds search process :10.1.1.108.8901
starting empty full set features search begins features marked unknown 
iteration feature chosen raced subset excluded 
combinations unknown features equal probability 
due probabilistic nature search feature subset win race dependent feature 
experiments artificial domains showed schemata search effective identifying relevant features raced versions forward backward selection faster raced backward selection 
feature weighting algorithms feature weighting viewed generalisation feature selection 
feature selection feature weights restricted feature 
feature weighting allows finer differentiation features assigning continuous valued weight 
algorithms nearest neighbour normally treat feature equally easily modified include feature weighting calculating similarity cases 
thing note general feature weighting algorithms reduce dimensionality data 
features low weight removed data initially assumed feature useful induction degree usefulness reflected magnitude weight 
continuous weights features involves searching larger space involves greater chance overfitting :10.1.1.49.3282
salzberg sal incorporates incremental feature weighting instance learner called 
correct classification weight matching feature incremented global feature adjustment rate 
mismatching features weights decremented amount 
incorrect classifications opposite occurs mismatching features incremented weights matching features decremented 
salzberg reported value needs tuned different data sets give best results 
wettschereck aha wa note weighting scheme insensitive skewed concept descriptions 
ib aha extension nearest neighbour algorithm addresses problem calculating separate set feature weights concept 
weight feature computed wi max 
expected approach half apparently irrelevant attributes 
incrementally updated learning 
higher observed frequencies classes instances instance classified similar neighbour concept description 
incremented diff xi yi class diff xi yi 
incremented 
experiments ib showed tolerant irrelevant features nearest neighbour algorithm 
relief kr algorithm uses instance approach assign weights features 
wettschereck aha wa relief calculate weights relief originally feature selection described section nearest neighbour algorithm report significant improvement standard nearest neighbour domains 
kohavi langley yun describe approach feature weighting considers small set discrete weights continuous weights :10.1.1.49.3282
approach uses wrapper coupled simple nearest neighbour estimate accuracy feature weights best search explore weight space 
experiments vary number discrete weights considered algorithm results showed advantage increasing number non zero discrete weights fact exception carefully crafted artificial domains non zero weight equivalent feature selection difficult outperform 
methods feature weighting feedback nearest neighbour algorithm incrementally learning special stage prior induction adjust weights 
non feedback methods setting weights include category feature importance sets weight feature conditional probability class feature cross category feature importance wa category feature importance averages classes mutual information sw feature class 
approaches require numeric features discretized 
chapter summary practical machine learning algorithms assumptions apply heuristics trade accuracy resulting model speed execution comprehensibility result 
assumptions heuristics reasonable yield results presence irrelevant redundant information fool resulting reduced accuracy understandable results 
feature subset selection help focus learning algorithm important features particular problem 
reduce dimensionality data allowing learning algorithms operate faster effectively 
known information gain feature class 
see chapter details 
main approaches feature subset selection described literature 
wrapper tuned specific interaction induction algorithm training data shown give results practise may slow practical large real world domains containing features 
filter methods faster involve repeatedly invoking learning algorithm 
existing filter solutions exhibit number drawbacks 
algorithms unable handle noise rely user specify level noise particular problem 
cases subset features selected explicitly features ranked final choice left user 
algorithms handle redundant irrelevant features 
algorithms require features transformed way increases initial number features search space 
case result loss meaning original representation turn impact interpretation induced models 
feature weights easily incorporated learning algorithms nearest neighbour advantage feature weighting feature selection minimal best due increased chance overfitting data 
general feature weighting reduce dimensionality original data 
chapter correlation feature selection thesis claims feature selection classification tasks machine learning accomplished basis correlation features feature selection procedure beneficial common machine learning algorithms 
chapter presents correlation feature selector cfs claim subsequent chapters examine behaviour cfs various conditions show cfs identify useful features machine learning 
section outlines rationale motivation correlation approach feature selection ideas borrowed psychological measurement theory 
various machine learning approaches measuring correlation nominal variables discussed section respective biases implications cfs discussed section 
section presents cfs algorithm variations experimental purposes 
rationale glf state features relevant values vary systematically category membership term correlation general sense thesis 
intended refer specifically classical linear correlation refer degree dependence predictability variable 
words feature useful correlated predictive class irrelevant 
kohavi john kj formalize definition definition feature vi said relevant iff exists vi vi vi vi vi 
empirical evidence feature selection literature shows irrelevant features redundant information eliminated ls kj ks 
feature said redundant features highly correlated 
definitions relevance redundancy lead hypothesis feature selection method thesis feature subset contains features highly correlated predictive class uncorrelated predictive 
test theory ghi principle design composite test predicting external variable interest 
situation features individual tests measure traits related variable interest class 
example accurate prediction person success mechanics training course composite number tests measuring wide variety traits ability learn ability comprehend written material manual dexterity forth individual test measures restricted scope traits 
states develop composite intend basis predicting outside variable components select form composite relatively low inter correlations 
seek predict variable variables try select predictor variables measure different aspects outside variable correlation components test outside variable known inter correlation pair components cor relation composite test consisting summed components outside variable predicted rii correlation summed components outside variable number components average correlations components outside variable rii average inter correlation components ghi hog 
equation fact pearson correlation coefficient variables standardized 
shows correlation composite outside variable function number component variables composite magnitude inter correlations magnitude correlations components outside variable 
entering illustrative values equation allowing values rii vary formula solved values plotted 
drawn higher correlations components outside variable higher correlation composite outside variable 
lower inter correlations components higher correlation composite outside variable 
number components composite increases assuming additional components original components terms average components outside variable correlation composite outside variable increases 
seen increasing number components substantially increases correlation composite outside variable 
group components highly correlated outside avg 
avg 
avg 
rii effects correlation outside variable composite variable number components inter correlations components rii correlations components outside variable 
variable time bear low correlations ghi 
furthermore hog notes addition additional component considered low inter correlation selected components may predominate high correlation outside variable 
equation thesis heuristic measure merit feature subsets supervised classification tasks 
situation external variable class problem remaining develop suitable ways measuring correlation feature feature inter correlation 
supervised learning tasks involve different data features may continuous ordinal nominal binary 
order common basis computing correlations equation desirable uniform way treating different types features 
discretization method fayyad irani fi applied pre processing step convert continuous features nominal 
prediction clear redundant attributes eliminated feature predictive ability covered safely removed 
learning algorithms naive bayes require order maximise predictive formance ls :10.1.1.43.3692
data mining applications comprehensible results paramount importance clear redundancy eliminated 
example rule may sense user attribute replaced highly correlated 
cfs described section accommodates situation providing report generation facility 
attribute final subset cfs list close substitutes terms merit final subset attribute question replaced substitutes simply correlation attribute question 
correlating nominal features features class treated uniform manner feature class correlation feature feature inter correlations equation may calculated 
research decision tree induction provided number methods estimating quality attribute predictive attribute 
measures attribute quality characterize variability collections instances corresponding values particular attribute 
reason known impurity functions bre cb 
collection instances considered pure instance respect value second attribute collection instances impure degree instances differ respect value second attribute 
decision tree induction typically involves measuring predictive attributes class 
corresponds feature class correlations equation 
calculate merit feature subset equation feature feature inter correlations ability feature predict vice versa measured 
decision tree learners perform greedy simple complex hill climbing search space possible trees general inductive bias favour smaller trees larger ones mit 
factor impact size tree generalizes new instances bias inherent attribute quality measure select attributes test nodes tree 
quality measures known unfairly favour attributes values fewer values qui wl kon 
result construction larger trees may overfit training data generalize poorly 
similarly measures correlations equation feature subsets containing features values may preferred situation lead inferior performance decision tree learner restricted subset 
kononenko kon examines biases eleven measures estimating quality attributes :10.1.1.51.3072:10.1.1.51.3072
relief mdl acceptable biases respect attribute level number values described section 
inter correlation features measure needed characterizes predictive ability attribute vice versa 
simple symmetric versions relief mdl purpose 
third measure tested kononenko symmetrical uncertainty bias similar relief mdl 
section reconstructs experiments done kononenko analyze bias attribute quality measures 
behaviour symmetrical uncertainty mdl relief respect attribute level may affect feature selection discussed 
experimental scenario extended examine behaviour measures respect number available training examples implications feature selection discussed 
versions cfs feature selector relief mdl symmetric uncertainty empirically compared chapter 
symmetrical uncertainty probabilistic model nominal valued feature formed estimating individual probabilities values training data 
model estimate value novel sample drawn distribution training data entropy model attribute number bits take average correct output model 
entropy measure uncertainty unpredictability system 
entropy log 
observed values training data partitioned values second feature entropy respect partitions induced entropy prior partitioning relationship features equation gives entropy observing log 
amount entropy decreases reflects additional information provided called information gain qui alternatively mutual information sw 
information gain gain 
information gain symmetrical measure amount information gained observing equal amount information gained observing symmetry desirable property measure feature feature 
unfortunately information gain biased favour features values 
furthermore correlations equation normalized ensure comparable affect 
symmetrical uncertainty compensates information gain bias attributes values normalizes value range gain symmetrical uncertainty coefficient 
relief relief kr feature weighting algorithm sensitive feature interactions see chapters details 
kononenko kon notes relief attempts approximate difference probabilities weight feature wx different value nearest instance different class different value nearest instance class :10.1.1.51.3072:10.1.1.51.3072
removing context sensitivity provided nearest instance condition attributes treated independent equation kon kon relief different value different class different value class reformulated relief gini class variable gini 
gini modification attribute quality measure called gini index bre 
gini gini index similar information gain biased favour attributes values 
relief symmetrically features measure calculated twice feature treated turn class results averaged 
relief mentioned subsequent chapters symmetrical version referred 
difference equation gini index uses place mdl roughly speaking minimum description length mdl principle ris states best theory infer training data minimizes length complexity theory length data encoded respect theory 
mdl principle taken operational definition occam razor formally theory inferred data total description length dl dl dl 
equation description lengths measured bits 
data observed values feature values partitioned values second feature description length data theory second term equation approximated multiplying average entropy number observed instances 
problem just entropy measure quality model attribute possible construct model predicts data perfectly zero entropy 
model necessarily 
example consider attribute distinct values instances data 
data partitioned values exactly value probability partitions entropy respect zero 
model generalize new data overfitted training data overly sensitive statistical idiosyncrasies training data 
term equation deals just sort problem 
model just described complex take bits describe 
model reduced description length data zero value equation large due high cost describing model 
best models mdl principle predictive data time captured underlying structure occam razor principle commonly attributed william occam early th century states entities multiplied necessity principle generally interpreted choice theories equally consistent observed phenomena prefer simplest 
compact fashion 
quinlan qui discusses mdl principle coding decision trees kononenko kon defines mdl measure attribute quality mdl prior mdl log prior mdl post mdl :10.1.1.51.3072:10.1.1.51.3072
nc log post mdl log 
log number training instances number class values ni 
number training instances class ci number training instances th value attribute nij number training instances class ci having th value attribute 
equation gives average compression instance class afforded attribute 
prior mdl description length class labels prior partitioning values attribute 
post mdl performs calculation prior mdl partitions induced attribute sums result 
term equation equation encodes class labels respect model encoded respective second term 
model prior mdl simply probability distribution class labels instances class model post mdl probability distribution class labels partitions induced attribute 
obtain measure lies equation normalized dividing prior mdl gives fraction average description length class labels reduced partitioning values attribute 
equation non symmetric measure exchanging roles attribute class give result 
measure symmetrically features calculated twice treating feature turn class results averaged 
mdl measure mentioned subsequent chapters normalized symmetrical version referred 
bias correlation measures nominal features section examines bias methods discussed measuring correlation nominal features 
measures information gain tend overestimate worth multi valued attributes 
problem known decision tree community 
quinlan qui shows gain attribute measured respect class feature equal gain attribute formed randomly partitioning larger number values 
means general derived attribute analogy attributes values appear predictive correlated class original 
example suppose attribute values possible classes shown table 
instances shown table entropy class bit entropy class attribute bit gain calculated equation attribute provides information class 
second attribute formed converting values attribute value probability examples shown table may occur 
case entropy class respect attribute bits gain bits 
additional partitioning produced randomly reasonably considered correlated class approach eliminating bias decision tree induction construct binary decision trees 
entails dividing values attribute mutually exclusive subsets 
bias eliminated virtue features having values 
quinlan qui notes process results large increase computation feature values node tree possible ways subsetting values evaluated order select best 
feature selection algorithms described koller sa class class table valued non informative attribute valued attribute derived randomly partitioning larger number values 
attribute appears predictive class attribute information gain measure 
ks avoid bias favour multi valued features boolean encoding 
value attribute represented binary indicator attribute 
value attribute dataset appropriate indicator attribute set indicator attributes corresponding possible values set 
greatly increase number features size search space introduce dependencies data originally 
furthermore subsetting decision trees boolean encoding feature selection result intelligible decision trees loss meaning original attributes 
sections bias symmetrical uncertainty relief mdl examined 
purpose exploring bias measures obtain indication measure affect heuristic merit calculated equation get feel measures exhibit bias similar employed common learning algorithms 
measure behaviour respect irrelevant attributes particular interest feature selection algorithm thesis 
experimental measurement bias test bias various measures monte carlo simulation technique white liu adopted wl 
technique approximates distributions various measures differing conditions number attribute values class values 
estimated parameters derived distributions compared see effects conditions measures 
white liu examined effects attribute class level various measures random irrelevant attributes generated independently class 
kononenko kon extended scenario including attributes predictive class :10.1.1.51.3072:10.1.1.51.3072
section examines bias symmetrical uncertainty normalized symmetrical mdl symmetrical relief experimental methodology kononenko kon :10.1.1.51.3072:10.1.1.51.3072
section explores effect varying sample size behaviour measures 
method experiments section artificial data generated properties equiprobable classes attribute values attributes irrelevant values drawn uniform distribution independently class informative kononenko method kon training instances experiments section number training instances allowed vary experiments section :10.1.1.51.3072
multi valued attributes informative joining values attribute subsets 
attribute values subsets 
div div 
formed 
probability attribute value subsets depends class selection particular value inside subset random uniform distribution 
probability attribute value subsets 
kc mod kc mod number class values integer indexing possible class values 
ci parameter controlling level association attribute class higher values attribute informative 
equation seen attributes informative higher numbers classes 
experiments section 
merit features calculated symmetrical uncertainty mdl relief 
results measure averaged trials 
varying level attributes section explores effects varying number attribute values bias measures fixed number training instances 
show results informative non informative attributes classes 
estimates informative attributes measures decrease exponentially number values 
effect extreme symmetrical uncertainty compared 
behaviour comparable occam razor states things equal simplest explanation usually best 
practical terms feature selection measures prefer features fewer values values furthermore probability estimation reliable attributes fewer values especially data limited risk overfitting training data generalizing poorly novel cases 
non informative attributes mdl measure best 
estimates zero clearly distinguishing informative attributes 
symmetrical uncertainty relief exhibit linear bias favour non informative attributes values 
relief estimates lower relative informative attributes symmetrical uncertainty 
curves corresponding number classes compared informative non informative attributes symmetrical uncertainty relief seen scale graphs clear separation estimates informative non informative attributes informative attributes informative number classes see equation 
possibility non informative attribute symmetrical 
coeff 
informative informative informative symmetrical 
coeff 
non informative non informative non informative number attribute values number attribute values informative informative informative non informative non informative non informative symmetrical relief symmetrical relief number attribute values number attribute values normalized symmetrical mdl informative informative informative normalized symmetrical mdl non informative non informative non informative number attribute values number attribute values effects varying attribute class level symmetrical uncertainty symmetrical relief normalized symmetrical mdl attributes informative graphs left non informative graphs right 
curves shown classes 
values estimated useful slightly informative attribute symmetrical uncertainty relief 
section examines behaviour measures sample size varied shows danger occurring greater fewer training examples 
varying sample size experiments described section vary number training examples examine effect behaviour correlation measures 
training data sets containing instances generated 
results measure averaged trials training set size 
graphs number classes set curves generated attribute values 
curves classes show similar extreme tendencies shown appendix shows results correlation measures 
behaviour measures stable large training set sizes 
estimates symmetrical uncertainty symmetrical relief show tendency increase exponentially fewer training examples 
effect marked attributes informative noninformative values 
number training examples problem typically fixed increase value measure smaller training set pose problem informative attributes increase attributes differing levels 
seen graphs increase constant respect attribute level applies non informative attributes 
symmetrical uncertainty relief show greater increase informative non informative attributes greater numbers values 
graph symmetrical uncertainty coefficient worth informative attribute values greater informative attribute values training sets examples 
informative attributes values overtake informative attribute values training examples respectively 
furthermore non informative attribute values appears useful informative attribute values fewer training examples 
relief slightly better behaved symmetrical uncertainty coefficient estimate informative attribute values overtake informative attribute values estimates irrelevant attributes exceed informative attributes 
informative attributes behaviour mdl measure exact opposite symmetrical uncertainty relief 
whilst stable large numbers training examples mdl measure exhibits tendency decrease exponentially fewer training examples 
similar tendency observed noninformative attributes 
fewer training examples trend reverses measure begins increase 
effect prominent attributes greater numbers values 
training examples noninformative attributes values appear slightly informative 
general mdl measure pessimistic requires data order ascertain quality feature 
discussion preceding section empirically examined bias attribute quality measures respect attribute level training sample size 
informative attributes measures exhibit behaviour spirit occam razor preferring attributes fewer values choice equally informative attributes varying level 
calculating heuristic merit feature subsets equation bias result preference subsets containing predictive features fewer values situation conducive induction smaller models machine learning schemes prefer simple hypotheses 
respect irrelevant attributes mdl measure best 
training examples mdl measure clearly identifies irrelevant attribute returning negative value 
symmetrical uncertainty relief symmetrical 
coeff 
informative informative informative non informative non informative non informative symmetrical 
coeff 
informative informative informative non informative non informative non informative number examples number examples informative informative informative non informative non informative non informative informative informative informative non informative non informative non informative symmetrical relief symmetrical relief number examples number examples normalized symmetrical mdl informative informative informative non informative non informative non informative normalized symmetrical mdl informative informative informative non informative non informative non informative number examples number examples effect varying training set size symmetrical uncertainty symmetrical relief normalized symmetrical mdl attributes informative non informative 
number classes curves shown valued attributes 
linearly biased favour irrelevant attributes greater numbers values 
undesirable measuring attribute ability predict class irrelevant attribute values may appear useful informative attribute values 
experiments section show danger occurring greater small training set sizes 
bias multi valued irrelevant attributes advantageous respect feature feature inter correlations denominator equation 
equation feature acceptable low correlation features multi valued irrelevant feature appear correlated included subset 
symmetrical uncertainty relief optimistic measures little training data mdl measure pessimistic 
training sets small mdl measure equation may result preference smaller feature subsets containing strong correlations class 
section introduces cfs correlation feature selection algorithm uses attribute quality measures described heuristic evaluation function 
correlation feature selector cfs simple filter algorithm ranks feature subsets correlation heuristic evaluation function 
bias evaluation function subsets contain features highly correlated class uncorrelated 
irrelevant features ignored low correlation class 
redundant features screened highly correlated remaining features 
acceptance feature depend extent predicts classes areas instance space predicted features 
cfs feature subset evaluation function equation repeated slightly modified notation ease ms rff ms heuristic merit feature subset containing features rcf mean feature class correlation rff average feature feature 
numerator equation thought providing indication predictive class set features denominator redundancy features 
equation forms core cfs imposes ranking feature subsets search space possible feature subsets 
addresses issue evaluation strategy langley lan characterization search feature selection algorithms chapter section 
exhaustive enumeration possible feature subsets prohibitive cases issues concerning organization search start point stopping criterion addressed 
implementation cfs experiments described thesis allows user choose heuristic search strategies forward selection backward elimination best 
forward selection begins features greedily adds feature time possible single feature addition results higher evaluation 
backward elimination begins full feature set greedily removes feature time long evaluation degrade 
best start features features 
search progresses forward search space adding single features search moves backward search space deleting single features 
prevent best search exploring entire feature subset search space stopping criterion imposed 
search terminate consecutive fully expanded subsets show improvement current best subset 
shows stages cfs algorithm conjunction machine learning scheme 
copy training data discretized method fayyad irani fi passed cfs 
cfs calculates feature class feature feature correlations measures described section cnf searches feature subset space 
subset highest merit measured equation search reduce dimensionality original training data testing data 
reduced datasets may passed machine learning scheme training testing 
important note general concept correlation feature selection depend module 
sophisticated method measuring correlation may discretization unnecessary 
similarly conceivable search strategy may cfs 
cfs training data data pre processing discretisation calculate feature correlations feature class class feature feature feature set search feature evaluation merit feature set ml algorithm testing data dimensionality reduction final evaluation estimated accuracy components cfs 
training testing data reduced contain features selected cfs 
dimensionally reduced data passed machine learning algorithm induction prediction 
variations cfs employing attribute quality measures described previous section estimate correlations equation evaluated experiments described chapter 
cfs uc uses symmetrical uncertainty measure correlations cfs mdl uses normalized symmetrical mdl measure correlations cfs relief uses symmetrical relief measure correlations 
unknown missing data values treated separate value calculating correlations 
best way deal unknowns depends meaning domain 
unknown special meaning example blank entry particular symptom disease may mean patient exhibit symptom treating separate value best approach 
unknowns represent truly missing information sophisticated scheme distributing counts associated missing entries values attribute proportion relative frequencies may appropriate 
table table give example cfs applied golf data set table 
table shows feature correlation matrix data set relief calculate correlations 
table illustrates forward selection search feature subset space merit subset calculated equation 
search begins empty set features zero merit 
single feature addition empty set evaluated humidity added subset highest score 
step involves trying remaining features humidity choosing best outlook 
similarly stage wind added subset 
step tries single remaining feature temperature current subset improve merit search terminates 
best subset outlook humidity wind returned 
outlook temperature humidity wind class outlook temperature humidity wind table feature correlations calculated golf dataset 
relief calculate correlations 
computational expense time complexity cfs quite low 
requires operations computing pairwise feature correlation matrix number instances initial number features 
feature selection search requires operations worst case forward selection backward elimination search 
best search pure form exhaustive stopping criterion probability exploring entire search space small 
evaluating equation feature subset containing features additions required numerator feature class correlations additions required denominator feature feature inter correlations 
search algorithm imposes partial ordering search space numerator denominator equation calculated incrementally requires addition subtraction feature set rcf rff merit outlook temperature humidity wind outlook humidity temperature humidity humidity wind outlook temperature humidity outlook humidity wind outlook temperature humidity wind table forward selection search correlations table 
search starts empty set features merit 
subsets bold show local change previous best subset resulted improvement respect evaluation function 
backward search numerator additions subtractions denominator 
forward search necessary pre compute entire feature correlation matrix feature correlations calculated needed search 
unfortunately applied backward search backward search begins features 
independence assumption naive bayesian classifier cfs assumes features conditionally independent class 
experiments naive bayes real data sets shown perform assumption moderately violated dp dp expected cfs identify relevant features moderate feature dependencies exist 
strong feature interactions occur cfs may fail select relevant features 
extreme example parity problem feature isolation appear better feature relevant 
chapter explores additional methods detecting feature dependencies class 
chapter summary chapter presents new feature selection technique discrete class supervised learning 
technique dubbed cfs correlation feature selection assumes useful feature subsets contain features predictive class uncorrelated 
cfs computes heuristic measure merit feature subset pair wise feature correlations formula adapted test theory 
heuristic search traverse space feature subsets reasonable time subset highest merit search reported 
cfs treats features uniformly discretizing continuous features training data outset 
supervised discretization method fayyad irani fi employed method perform best pre processing step machine learning algorithms dks ks 
measures association nominal variables reviewed task quantifying feature class feature feature correlations necessary calculate merit feature subset equation 
symmetrical uncertainty mdl relief prefer predictive features fewer values values 
bias measures promote choice feature subsets give results machine learning algorithms especially prefer compact predictive theories bias measures information gain favour multi valued attributes 
measures may report irrelevant attributes values predictive degree 
chance irrelevant attribute preferred predictive greatest training examples 
chapter datasets experiments order evaluate effectiveness cfs selecting features machine learning thesis takes empirical approach applying cfs pre processing step common machine learning algorithms 
chapter reviews datasets general methodology experiments chapters 
domains twelve natural domains artificial domains evaluating cfs machine learning algorithms 
natural domains artificial monk domains tbb drawn uci repository machine learning databases mm 
domains chosen predominance literature prevalence nominal features reducing need discretize feature values 
addition artificial domains increasing difficulty borrowed langley sage ls test wrapper feature selector nearest neighbour classification 
artificial domains useful allow complete control parameters attribute level predictive ability attributes number irrelevant redundant attributes noise 
varying parameters allows conjectures tested behaviour algorithms extreme conditions examined 
table summarises characteristics domains 
default accuracy accuracy predicting majority class dataset 
artificial domains default accuracy default accuracy limit example equally infinite sample available 
variations artificial domains added irrelevant redundant features test cfs ability screen types features 
average max min default domain instances features missing feature vals feature vals class vals accuracy mu vo cr ly pt bc dna au sb hc kr table domain characteristics 
datasets horizontal line natural domains artificial 
missing column shows percentage data set entries number features number instances missing values 
average feature vals max min feature vals calculated nominal features data sets 
brief description datasets 
mushroom mu dataset contains records drawn society field guide north american mushrooms lin 
task distinguish edible poisonous mushrooms basis nominal attributes describing characteristics mushrooms shape cap gill spacing 
large dataset containing instances 
ib achieve accuracy dataset naive bayes suggesting attributes may redundant 
vote vo dataset party affiliation house representatives characterised voted key issues education spending immigration 
democrats instances features binary 
original data different types vote 
collapsed related voting categories 
version dataset single predictive attribute physician fee freeze removed 
australian credit screening cr dataset contains instances australian credit 
task distinguish credit worthy non credit worthy customers 
attributes names values converted meaningless symbols ensure confidentiality data 
continuous features nominal 
nominal features range values 
lymphography ly small medical dataset containing instances 
task distinguish healthy patients malignant lymphoma 
features nominal 
medical domains primary tumour breast cancer provided university medical centre institute oncology ljubljana yugoslavia 
primary tumour pt dataset involves predicting location tumour body patient basis nominal features 
classes corresponding body locations lung liver forth 
instances provided 
breast cancer bc task predict cancer recur patients 
nominal attributes describing characteristics tumour size location 
instances 
dna promoter dna small dataset containing positive examples coli promoter gene sequences negative examples 
nominal attributes representing gene sequence 
attribute dna nucleotide base pair having possible values 
audiology au task diagnose ear 
instances de scribed nominal features 
classes 
dataset provided professor college medicine 
soybean large sb task diagnose diseases soybean plants 
examples described nominal features 
features measure properties leaves various plant abnormalities 
classes diseases 
horse colic hc instances dataset provided mary matt university 
attributes continuous 
features include horse young old surgery pulse respiratory rate level abdominal number attributes serve class commonly lesion surgical 
chess game kr dataset contains chess game board descriptions 
game king rook versus king pawn square away king rook side white move 
task predict white win basis features describe board 
feature values binary 
boolean domains borrowed langley sage ls 
exhibit increasing level feature interaction 
irrelevant redundant attributes added domains test cfs ability deal sorts features 
simple conjunction features exhibits amount feature interaction 
concept class value class 
disjunct conjuncts known concept 
case concept class bits set problem difficult due higher degree interaction features 
exhibits highest degree feature interaction similar parity problem single feature isolation appear useful 
concept domain included example situation cfs fail select relevant features due fact assumption attribute independence class completely incorrect 
monk problems monk problems artificial domains representation compare machine learning algorithms tbb 
monk domains contain instances robots described nominal features head shape round square body shape round square smiling holding sword balloon flag jacket colour red yellow green blue tie monk problems instances total 
problem standard training test set 
monk concept head shape body shape jacket colour red problem difficult due interaction features 
note value jacket colour feature useful 
monk concept exactly features value 
hard problem due pairwise feature interactions fact value feature useful 
note features relevant concept 
better predicting default class problem 
monk concept jacket colour green holding sword jacket colour blue body shape standard training set problem class noise added training examples label reversed 
monk problem noise free 
possible achieve approximately accuracy jacket colour blue body shape disjunct 
experimental methodology experiments described thesis compare runs machine learning algorithms feature selection datasets described previous section 
accuracy algorithms measured random subsampling performs multiple random splits dataset disjoint train test sets 
trial algorithm trained training dataset induced theory evaluated test set 
algorithms compared applied training test sets 
testing accuracy algorithm percentage test examples classifies correctly 
table shows train test set sizes natural domains monk problems 
natural domains thirds training third testing split cases 
split vote datasets third thirds split credit eighth instances training mushroom largest dataset 
case monk problems testing performed full dataset done originally thrun tbb 
various different train test set sizes artificial domains see chapter details 
domain train size test size mu vo cr ly pt bc dna au sb hc kr table training test set sizes natural domains monk problems 
exception learning curve experiments described accuracy averaged train test trials dataset 
furthermore train test set split stratified 
stratification ensures class distribution dataset preserved training test sets 
stratification shown help reduce variance estimated accuracy especially datasets classes koh 
cfs requires datasets discretized discretization method fayyad irani fi applied copy training dataset passed cfs 
effects feature selection interest induction performed original training datasets 
test cfs dataset example dataset discretized features selected machine learning algorithm run selected features original form 
tailed paired tests determine difference algorithms significant 
difference accuracy considered significant value confidence level greater 
algorithms compared table accuracies summarising results 
symbols denote algorithm statistically significantly better worse 
discussion results algorithm stated better worse significantly better worse level 
bar graph showing absolute accuracy difference algorithms 
example shows absolute difference accuracy naive bayes cfs feature selection naive bayes feature selection 
bar zero line indicates cfs managed improve naive performance bar zero line indicates cfs degraded naive bayes performance 
stars bar graph show differences different 
tables graphs summarising induced tree sizes reported 
accuracy difference mu vo cr ly pt bc dna au sb hc kr dataset effect cfs feature selection accuracy naive bayes classification 
dots show results statistically significant experiments described section chapter examine cfs ability deal irrelevant redundant attributes 
interesting examine learning curves algorithms naive bayes ib adversely affected kinds attributes 
learning curve shows quickly algorithm performance improves access training examples 
random subsampling described generate training datasets size instances added successive iteration 
testing performed remaining instances 
accuracy training set size averaged trials 
example shows learning curve ib artificial domain added irrelevant attributes 
error bars represent confidence interval 
accuracy training set size learning curve ib dataset added irrelevant attributes 
variations cfs attribute correlation measures previous chapter evaluated experiments cfs uc uses symmetrical uncertainty measure attribute correlations 
cfs mdl uses normalized symmetrical mdl measure attribute correlations 
cfs relief uses symmetrical relief measure attribute correlations 
forward best search variations cfs initial experiments showed search strategy performed slightly better forward selection backward elimination 
forward best search evaluated fewer subsets backward elimination 
best search stops consecutive fully expanded nodes showing improvement heuristic merit function evaluated 
stopping criterion default mlc implementation wrapper feature selector 
chapter compares cfs wrapper 
execution times algorithms mentioned reported cpu units sun sparc server 
chapter evaluating cfs ml algorithms chapter describes evaluation cfs artificial natural machine learning datasets 
artificial domains relevant features known advance cfs performance directly ascertained 
natural domains relevant features known advance performance cfs measured indirectly 
way compare learning algorithm performance feature selection cfs 
section examines cfs ability deal irrelevant redundant features artificial domains 
section cfs select features natural domains performance machine learning algorithms feature selection compared 
artificial domains purpose experiments described section empirically test claim cfs feature evaluation heuristic equation filter irrelevant redundant features 
boolean domains sections discuss performance irrelevant redundant features added artificial boolean domains borrowed langley sage ls 
concepts concepts exhibit increasing level feature interaction provide opportunity test cfs behaviour assumption feature independence class violated 
domain dataset randomly generated containing examples irrelevant attributes added second dataset randomly generated containing examples redundant attributes added 
experiments number relevant irrelevant redundant attributes selected cfs plotted function number training examples shown cfs 
allows behaviour different attribute correlation measures variations cfs cfs uc cfs mdl cfs relief compared 
training sets increasing size examples iteration selected random subsampling method described previous chapter 
number relevant irrelevant redundant attributes averaged trials training set size 
ib sensitive irrelevant attributes learning curves illustrating impact feature selection ib accuracy shown datasets added irrelevant attributes 
similarly naive bayes affected redundant attributes learning curves shown naive bayes datasets added redundant attributes 
monk problems monk problems tbb challenging artificial domains compare performance machine learning algorithms 
domains involve irrelevant features noise high degrees feature interaction 
section tests cfs concepts 
irrelevant attributes versions boolean domains fifteen uniformly random boolean attributes uniformly random valued attribute uniformly random valued attribute added total attributes seventeen irrelevant 
concept shows number irrelevant attributes selected variations cfs concept 
results show average number irrelevant attributes included versions cfs decreases rapidly training examples seen 
cfs mdl decreases faster cfs uc cfs relief 
agrees results chapter showed mdl measure effective identifying irrelevant attributes 
cfs uc cfs mdl cfs relief irrelevant features training set size number irrelevant attributes selected concept added irrelevant features cfs uc cfs mdl cfs relief function training set size 
cfs relief selects fewer irrelevant attributes cfs uc training sets examples requires training examples methods number irrelevant attributes drops zero 
training examples variations cfs select average irrelevant attribute 
shows number relevant attributes selected variations cfs concept 
best results exhibited cfs uc selects relevant features training examples seen 
average number relevant features selected cfs mdl starts low increases rapidly training examples seen 
consistent results chapter show relevant features cfs uc cfs mdl cfs relief training set size number relevant attributes selected concept added irrelevant features cfs uc cfs mdl cfs relief function training set size 
mdl measure pessimistic compared gain ratio relief estimates relevant attributes training examples 
results cfs relief worse methods 
average number relevant features selected cfs relief increases training instances seen rapidly methods variation curve stable 
cfs relief reliably include relevant features time training examples seen curious result bears investigation 
understanding poor performance cfs relief relative methods begins consideration feature class correlations assigned methods 
table shows feature class correlations assigned relevant features symmetrical uncertainty mdl relief full instances dataset concept 
values assigned features symmetrical uncertainty mdl close estimate slightly lower expect 
fact examples concept drawn uniformly random experiment continued training examples 
training examples cfs relief selecting relevant features consistently 
distribution limit sample size approaches infinity correlation values features equal 
relief estimates attribute lower relative symmetrical uncertainty mdl 
symmetrical attribute uncertainty mdl relief table feature class correlation assigned features symmetrical uncertainty mdl relief concept 
conjunctive concept splitting instances basis relevant attributes produce pure subset instances class corresponding value attribute 
hol produce rules relevant attributes full set instances gives rule class 
covers examples class 
covers examples rule class 
covers examples class 
covers examples rule class 
covers examples class 
covers examples proportion class examples covered attribute value attributes indicates differentiated basis size respective pure nodes 
value covers examples covers examples covers examples exactly ranking assigned measures table 
relief modification gini impurity measure bre 
breiman notes gini criterion prefers splits put largest class pure node 
relief appears sensitive size pure node symmetrical uncertainty mdl 
shows learning curves ib feature selection 
see chapter section 
accuracy ib cfs uc ib cfs mdl ib cfs relief ib training set size learning curves ib cfs uc ib cfs mdl ib cfs relief ib concept added irrelevant features cfs uc cfs mdl cfs relief irrelevant features training set size number irrelevant attributes selected concept added irrelevant features cfs uc cfs mdl cfs relief function training set size 
note cfs uc cfs relief produce result 
accuracy ib feature selection increases slowly number training examples increases due high number irrelevant features 
ib accuracy starts default accuracy concept finishes training examples contrast accuracy ib feature selection cfs uc cfs mdl increases rapidly reaching training examples respectively 
accuracy ib feature selection cfs relief better ib feature selection reach due fewer relevant features included method 
shape learning curves ib feature selection correspond closely shape curves number relevant features shown 
concept shows number irrelevant attributes selected variations cfs concept 
results show variations cfs filter irrelevant features quickly domain fewer irrelevant features included compared results concept 
concept uniform distribution classes concept large majority class 
differentiation informative non informative attributes concept done primarily basis small percentage instances class 
relevant attributes value class training examples irrelevant attribute may match relevant attribute small percentage class cases chance making just informative 
concept relevant attribute value class cases similarly relevant attribute value class cases 
case irrelevant attribute distribution values class match relevant attribute chance 
concept cfs mdl screens irrelevant features faster methods 
shows number relevant attributes selected variations cfs concept 
variations rapidly identify relevant attributes 
cfs uc cfs relief consistently choose relevant attributes training examples seen cfs mdl training examples 
shows learning curves ib feature selection concept 
concept ib accuracy feature selection improves slowly training examples seen examples 
feature selection allows ib learn rapidly achieve accuracy concept small number training examples 
expected shape learning curves ib feature selection correspond closely shape curves number relevant features selected variations cfs 
concept concept highest degree feature interaction expected cfs unable distinguish relevant features irrelevant features 
show number irrelevant relevant features selected variations cfs respectively concept 
graphs show cfs unable distinguish relevant irrelevant features 
cfs uc cfs relief select features especially irrelevant features cfs mdl exhibit tendency include features training examples seen 
conversely cfs mdl appears favour fewer features training examples seen 
concept cfs assumption features independent class means assuming ideal correlation measure infinitely large sample correlations zero equation merit feature subsets zero 
training examples limited features may appear slightly correlated class chance 
training instances feature may stand somewhat better 
number training examples increases features homogeneous correlations class similar exactly zero 
chapter shown feature accepted subset increase merit subset correlation class average inter correlation features subset features subset 
explains small increasing tendency features included cfs uc cfs relief 
cfs mdl hand small penalty associated size model feature decreasing correlation class resulting value zero 
shown graphically plots average number relevant features cfs uc cfs mdl cfs relief training set size number relevant attributes selected concept added irrelevant features cfs uc cfs mdl cfs relief function training set size 
accuracy ib cfs uc ib cfs mdl ib cfs relief ib training set size learning curves ib cfs uc ib cfs mdl ib cfs relief ib concept added irrelevant features 
cfs uc cfs mdl cfs relief irrelevant features training set size number irrelevant attributes selected concept added irrelevant features cfs uc cfs mdl cfs relief function training set size 
cfs uc cfs mdl cfs relief relevant features training set size number relevant attributes selected concept added irrelevant features cfs uc cfs mdl cfs relief function training set size 
cfs uc cfs mdl cfs relief irrelevant features training set size number irrelevant multi valued attributes selected concept added irrelevant features cfs uc cfs mdl cfs relief function training set size 
multi valued irrelevant attributes valued valued irrelevant attribute selected variations cfs 
cfs mdl selects multivalued irrelevant attributes 
mdl measure increased model penalty features values compensates slight chance correlations class 
shows learning curves ib feature selection concept 
ib feature selection learns slowly consistently ib feature selection shows clear trend 
interestingly feature selection cfs mdl results consistent performance cfs uc cfs relief 
cfs mdl performance roughly default accuracy version dataset 
tendency cfs uc cfs relief include irrelevant features leads erratic performance worse default accuracy dataset 
redundant attributes versions boolean domains redundant attributes added ib cfs uc ib cfs mdl ib cfs relief ib accuracy training set size learning curves ib cfs uc ib cfs mdl ib cfs relief ib concept added irrelevant features 
total twelve attributes 
redundant attributes copies attribute domains boolean redundant attributes valued redundant attributes valued redundant attributes 
level redundant attribute boolean valued valued attributes match attribute time exact copies attribute matches time uniformly random remaining 
multi valued attributes redundant joining values attribute subsets 
attribute values subsets 
div div 
formed 
attribute value redundant attribute value selected random subset value selected random second subset 
attributes match attribute time subsets contain exactly relevant features redundant features 
attributes match attribute time treated relevant noisy adding subsets quite 
concept shows number redundant attributes selected combinations redundant attributes example tried 
results similar reported 
variations cfs concept 
average number redundant attributes decreases rapidly cfs uc cfs mdl including redundant attribute seeing training examples 
cfs relief selects redundant attributes average variations 
corresponding dataset concept added irrelevant attributes sizes pure nodes relevant attributes vary version 
time attribute covers examples value attribute covers examples attribute covers examples 
table shows feature class correlations assigned features symmetrical uncertainty mdl relief full instances version concept 
measures rank attribute binary copies highest 
symmetrical uncertainty mdl measure rank attribute highest list 
relief ranks valued attributes match attribute time higher attribute relief sensitivity size attribute pure node domain outweighed bias attributes values 
examination feature subsets selected cfs relief shows attributes valued attributes 
full dataset correlation attributes assigned relief lower relative features assigned symmetrical uncertainty mdl measure helps explain cfs relief includes features 
smaller training set sizes examples shows average cfs mdl selects slightly redundant features cfs uc 
examining feature feature correlation redundant boolean features attribute mdl reveals value value symmetrical uncertainty assigns expect correlation features exact copies 
explanation due mdl measure extra cost model complexity 
mdl measure achieve upper bound due fact post mdl equation zero 
shows number relevant attributes selected variations cfs concept 
case irrelevant attributes cfs uc cfs mdl quickly asymptote selecting relevant features cfs relief takes longer 
cfs uc cfs mdl cfs relief redundant features training set size number redundant attributes selected concept added redundant features cfs uc cfs mdl cfs relief function training set size 
cfs uc cfs mdl cfs relief symm 
attr vals red 

attr vals red 
mdl attr vals red 
relief table feature class correlations assigned measures features dataset containing redundant features 
columns measure lists attribute original features number values attribute level redundancy 
relevant features cfs uc cfs mdl cfs relief training set size number relevant attributes selected concept added redundant features cfs uc cfs mdl cfs relief function training set size 
cfs uc cfs mdl cfs relief multi valued features training set size number multi valued attributes selected concept added redundant features cfs uc cfs mdl cfs relief function training set size 
cfs uc cfs mdl cfs relief noisy features training set size number noisy attributes selected concept added redundant features cfs uc cfs mdl cfs relief function training set size 
shows average number multi valued features selected variations cfs 
results show cfs uc cfs mdl effective filtering multi valued features preferring boolean equivalents 
cfs relief due higher preference valued features averages multi valued features 
shape graph cfs relief similar indicating redundant features selected multi valued 
shows average number noisy features selected variations cfs copies attribute noise 
results show variations cfs prefer include noisy features 
noisy feature included average variations cfs uc cfs mdl perform better cfs relief 
shows learning curves naive bayes feature selection concept 
sake clarity confidence intervals omitted cfs relief wider cfs uc cfs mdl 
variations cfs enable naive bayes learn faster feature selection 
points cfs uc nbayes cfs mdl nbayes dip slightly accuracy accuracy nbayes cfs uc nbayes cfs mdl nbayes cfs relief nbayes training set size learning curves nbayes naive bayes cfs uc nbayes cfs cfs relief nbayes concept added redundant features 
correspond roughly peaks redundant attributes included selected subsets 
larger dips curve cfs relief nbayes correspond points cfs relief average selects fewer relevant attributes 
concept case added irrelevant features variations cfs perform approximately equally concept 
average redundant feature included subsets chosen cfs seeing training examples 
cfs mdl starts including slightly redundant attributes methods 
variations cfs perform similarly identifying relevant attributes relevant features consistently identified training examples seen 
reflected learning curves naive bayes feature selection 
feature selection naive bayes improve performance training examples seen 
feature selection naive bayes accuracy starts near rapidly increases training examples 
graphs multi valued noisy attributes shown zero training examples attributes selected concept cfs uc cfs mdl cfs relief redundant features training set size number redundant attributes selected concept added redundant features cfs uc cfs mdl cfs relief function training set size 
relevant features cfs uc cfs mdl cfs relief training set size number relevant attributes selected concept added redundant features cfs uc cfs mdl cfs relief function training set size 
accuracy nbayes cfs uc nbayes cfs mdl nbayes cfs relief nbayes training set size learning curves nbayes naive bayes cfs uc nbayes cfs cfs relief nbayes concept added redundant features 
variations cfs 
concept results cfs concept added redundant features similar results cfs added irrelevant features 
cfs unable distinguish attributes fails include relevant features 
case added irrelevant features cfs uc cfs relief show tendency include features sample size increases cfs mdl selects fewer features features 
full set graphs appendix shows learning curves naive bayes feature selection 
naive bayes improve default accuracy version concept due extreme feature interaction 
accuracy naive bayes feature selection cfs mdl reaches maximum faster versions due small number features average selected 
features naive bayes predicts prior probabilities class values observed training nbayes cfs uc nbayes cfs mdl nbayes cfs relief nbayes accuracy training set size learning curves nbayes naive bayes cfs uc nbayes cfs cfs relief nbayes concept added redundant features 
data training test sets stratified achieve default accuracy 
monk problems section tests cfs monk problems 
problem uses representation features 
relevant features uses features 
examples problem pre defined training set 
training sets contain examples respectively full datasets testing tbb 
experiments training sets size pre defined sets generated random subsampling method described previous chapter uses full dataset done thrun tbb 
results averaged trials 
table shows average number features selected variations cfs monk problems 
variations unable select relevant features due high order feature interactions 
jacket colour feature consistently selected relevant features concept 
domain cfs uc cfs mdl cfs relief table average number features selected cfs uc cfs mdl cfs relief monk problems 
features relevant interact 
cfs assigns feature class correlations close zero features results approximately half features selected cfs uc cfs relief 
cfs mdl assigns feature class correlations zero accounts selecting fewer features 
cfs uc cfs mdl consistently choose body shape jacket colour give second conjunction concept jacket colour blue body shape 
cfs relief occasionally omits features 
tables show results machine learning algorithms feature selection monk problems 
domain naive bayes cfs uc nbayes cfs mdl nbayes cfs relief nbayes statistically significant improvement degradation table comparison naive bayes feature selection monk problems 
domain ib cfs uc ib cfs mdl ib cfs relief ib statistically significant improvement degradation table comparison ib feature selection monk problems 
cfs able improve accuracy naive bayes monk problems eliminating interacting features concepts 
removal interacting features head shape body shape yield conjunct concept allows accuracy achieved just feature 
removing features allows naive bayes approach default domain cfs uc cfs mdl cfs relief statistically significant improvement degradation table comparison feature selection monk problems 
accuracy dataset cfs mdl achieves best result removes features average variations 
cfs unable improve accuracy indicating holding feature conjunct concept naive bayes 
cfs degrades performance ib monk problems 
naive bayes ib able strongly interacting relevant features removing features results worse performance 
cfs degrades ib accuracy ib performance feature selection affected totally irrelevant features concept features removed ib achieve close accuracy 
features removed cfs closer ib accuracy default dataset 
cfs improves accuracy ib dramatically 
due removal totally irrelevant features 
accuracy improved approximately holding feature omitted cfs included 
cfs degrades performance 
able interacting relevant features 
removing features results accuracy maximum achievable just jacket colour feature 
unable learn concept achieves default accuracy 
cfs mdl cfs relief able increase performance match default accuracy 
discussion experiments cfs artificial domains concluded feature selection correlation specifically hypothesis proposed chapter feature subsets contain features correlated class uncorrelated select relevant features furthermore conditions moderate feature interaction long relevant features individually predictive class time 
expected cfs fails select relevant features cases strong feature interactions features predictive ability apparent context features 
strong feature interaction certainly possible natural domains results section appendix suggest common datasets similar uci collection 
results show cfs handles irrelevant redundant features noise avoids attributes values traits improve performance learning algorithms sensitive conditions 
correlation measures tested cfs symmetrical uncertainty mdl superior relief 
cases relief underestimates worth relevant attributes relative situation lead relevant features omitted subsets selected cfs 
cases attribute estimation relief causes cfs include noise redundancy feature subsets 
section presents experiments designed show cfs performance artificial domains carries natural domains 
natural domains section describes results testing cfs twelve natural domains 
relevant features known advance domains performance learning algorithms feature selection taken indication cfs success selecting useful features 
experiments artificial domains stratified random subsampling create training test sets results reported average trials algorithm dataset 
number datasets number datasets cfs uc cfs mdl cfs relief feature selector cfs uc cfs mdl cfs relief feature selector naive bayes ib number datasets cfs uc cfs mdl cfs relief feature selector number natural domains cfs improved accuracy left degraded accuracy right naive bayes ib 
machine learning algorithm shows natural domains accuracy improved degraded significantly variations cfs 
graphs show cfs uc cfs mdl perform better cfs relief 
cfs mdl improves accuracy datasets followed cfs uc cfs relief 
cfs uc degrades accuracy fewer datasets cfs mdl cfs relief 
accuracies variations cfs compared cfs uc better average cfs relief datasets worse average datasets 
compared cfs mdl cfs uc better average datasets worse average datasets 
case artificial domains cfs uc cfs mdl clearly outperform cfs relief 
performance cfs uc cfs mdl similar artificial domains cfs uc slightly better cfs mdl natural domains 
datasets cfs mdl cfs uc tend fewer training instances 
results suggest cfs uc preferred standard version cfs 
remainder section results cfs uc analyzed discussed detail 
full tabulated results comparing variations cfs appendix table shows performance naive bayes ib feature selection cfs uc 
results cv test dietterich die appendix cfs maintains improves accuracy naive bayes datasets degrades accuracy 
ib cfs maintains improves accuracy datasets degrades 
shows cfs reduces size trees induced twelve domains 
cfs appears difficulty domains highest number classes especially au sb 
worst performance audiology 
cfs reduces accuracy naive bayes dataset worst results 
domain classes instances cfs performance worse primary tumour domain pt similar characteristics audiology 
indicates possibly factors apart number classes dataset size affecting performance cfs 
interestingly cfs resulted worse performance ib chess game domain kr contrast naive bayes improves performance 
similar situation occurs mushroom domain mu lesser extent chess game 
improvement naive bayes indicates redundant features domains 
chess game domain cfs finds features give accuracy regardless learning algorithm 
ib able achieve accuracy respectively feature selection 
audiology domain classes features binary 
furthermore examples domain 
adds dom naive bayes cfs nbayes ib cfs ib cfs mu vo cr ly pt bc dna au sb hc kr statistically significant improvement degradation table naive bayes ib feature selection natural domains 
tree size difference mu vo cr ly pt bc dna au sb hc kr dataset effect feature selection size trees induced natural domains 
bars zero line indicate feature selection reduced tree size 
dots show statistically significant results 
support notion factor dataset size number classes affecting cfs performance 
possibility best search getting trapped local maximum chess game domain domains cfs degraded accuracy 
see case cfs re run audiology soybean chess endgame best search stopping condition number consecutive non improving subsets increased 
results show significant improvement datasets indicating algorithm getting trapped local maxima 
second possibility interacting features chess game domain 
examination cfs mdl output domain show trial fourth feature included resulting accuracy learning algorithms including naive bayes 
may interacting features fact naive bayes able shows fourth feature strongly interacting 
third possibility cfs heuristic merit function heavily biased favour small feature subsets resulting feature selection overly aggressive chess game domain domains cfs degraded accuracy 
shows number features original datasets number features selected cfs 
primary tumour dataset pt cfs reduced number features half 
case audiology chess game number features reduced strong indication feature selection aggressive datasets 
explore bias cfs merit function get idea merit corresponds actual accuracy learning algorithm merit versus accuracy plotted randomly selected feature subsets natural domains 
subsets features randomly selected single training split dataset fewer subsets possible size generated 
subset heuristic merit measured cfs training data actual accuracy estimated training learning algorithm features subset evaluating performance test set shows plots cfs uc merit versus naive bayes accuracy selection datasets chess game horse colic audiology soybean 
plots remaining datasets ib measure accuracy appendix thing apparent correspondence merit actual accuracy exist domains feature selection cfs degrades accuracy audiology soybean 
thing apparent number feature subsets naive bayes result close accuracy chess game domain 
examining size feature subsets represented points graphs reveals cfs bias subsets fewer features 
example horse colic dataset bias effective rightmost highest merit point graph horse colic represents subset containing features give accuracy test set 
subsets close accuracy contain features features assigned lower merit feature subset 
hand audiology soybean cfs favours smaller moderately accurate subsets larger subsets higher accuracy 
graphs audiology soybean clear subsets high accuracy merit close rightmost highest merit points 
analysis concluded cfs poor performance datasets traced merit formulation search 
aggressive bias favouring small feature subsets may result loss accuracy 
see performance improved datasets cause cfs difficulty method merging subsets introduced aim increasing feature set size incorporating subsets merit close best subset 
method works follows simply returning best subset search top subsets sorted order merit discovered search recorded 
averaging runs multiple training testing splits give reliable estimates merit accuracy feature subsets time consuming 
single training test set provides rough idea merit accuracy may generate outliers feature subsets chance predictive test set high merit respect training set 
features mu vo cr ly pt bc dna au sb hc kr dataset original number features natural domains left average number features selected cfs right 
search second best subset merged best subset merit new composite subset recalculated 
merit merit best subset composite accepted 
third best subset merged composite previous step forth 
continues long merit new composite best subset 
table shows accuracy naive bayes ib feature selection cfs uc subset merging scheme 
naive bayes eleven datasets accuracy maintained significantly improved dataset accuracy significantly degraded 
merging feature subsets result lymphography better significant difference results soybean primary tumour different worse 
ib merging feature subsets changed result audiology domain significant difference accuracy domain worse 
merging subsets degraded performance 
main difference horse colic domain gone improved degraded 
surprising merging subsets improves cfs performance naive bayes dataset 
naive bayes accuracy naive bayes accuracy merit merit naive bayes accuracy naive bayes accuracy merit merit heuristic merit cfs uc vs actual accuracy naive bayes randomly selected feature subsets chess game horse colic audiology soybean 
point represents single feature subset 
turns attribute selection heuristic responsible degraded performance 
chooses attribute split selecting attributes average better information gain attribute maximises gain ratio 
full feature set chooses attribute surgery split root tree attribute average gain highest gain ratio information gain normalised entropy attribute 
cfs chooses attribute type lesion high gain ratio 
merging subsets adds large number distinct values reasonably high gain ratio actuality poor predictor 
attribute far highest information gain due values 
features average information gain low due poor attributes low gain surgery average information gain preferred 
case reduced subset provided merging average information gain higher making attribute average information gain 
consequently chosen root tree low accuracy results 
dom naive bayes cfs nbayes ib cfs ib cfs mu vo cr ly pt bc dna au sb hc kr statistically significant improvement degradation table comparison learning algorithms feature selection merged subsets 
shows difference accuracy cfs merged subsets cfs merged subsets learning algorithms 
large improvements accuracy clearly seen lymphography audiology soybean chess game datasets 
apparent merging feature subsets degraded results datasets notably vote vote dna horse colic 
apparent naive bayes significant degradations mushroom vote vote shown 
results suggest useful features included merging subsets harmful redundancy included 
question immediately springs mind merging feature subsets necessary 
subsets contain features useful increase accuracy best subset search features included best subset 
shed light question chess game dataset examined 
recall features selected cfs give accuracy significantly degraded merging subsets results significantly better naive bayes feature selection see table accuracy difference mu vo cr ly pt bc dna au sb hc kr dataset absolute difference accuracy cfs uc merged subsets cfs uc naive bayes left ib middle right 
dots show statistically significant results 
learning algorithms trial cfs mdl included fourth feature gives accuracy 
indicates fourth feature may close borderline acceptance best subset fact included trials cfs uc cfs mdl merged subsets 
cfs uc cfs mdl symmetrical attribute uncertainty attribute mdl table top feature class correlations assigned cfs uc cfs mdl chess game dataset 
table shows top feature class correlations calculated symmetrical uncertainty mdl entire chess game dataset 
features consistently selected cfs 
highest correlation class low inter correlation features respectively 
feature feature added gives accuracy 
seen table cfs mdl assigns higher correlation comparative features feature cfs uc explain feature included trial cfs mdl cfs uc 
produce rule attribute full dataset gives rule attribute class attribute 
covers examples class won attribute 
covers examples clause rule highly predictive small number examples achieves accuracy examples 
examples gets correct account dataset 
possible clause rule attribute value true responsible improvement learning schemes accuracy dataset 
see case predictions naive bayes feature subset feature subset compared random training testing split dataset 
results show improvement corresponds entirely instances attribute value true 
furthermore new errors introduced inclusion attribute 
correlations attribute attributes low indicating little redundancy introduced feature 
analyzing attribute reveals similar pattern 
rule feature shows value covers examples small percentage instance space 
examining predictions naive bayes feature included manner shows improvement accuracy solely attributable single value attribute new errors introduced correlations attribute low 
attribute different story 
including attribute results accuracy degrading 
analyzing feature manner previous shows value covers examples 
value responsible test set originally misclassified classified correctly unfortunately causes test set misclassified 
correlations attributes higher attribute indicating redundancy introduced turn affects accuracy naive bayes 
analysis indicates datasets may contain features values strongly predictive locally small area instance space remaining values may low predictive power irrelevant partially correlated features 
cfs measures feature merit globally entire training instance space bias small feature subsets may prevent features included especially overshadowed strong globally predictive features 
number features cumulatively cover significant proportion dataset cfs large improvement audiology dataset merged feature sets supports conjecture 
inspection rules produced audiology dataset reveals number features values highly predictive small number instances 
features may useful cover instances covered stronger features 
experiments section show cfs ability select useful features carry artificial natural domains 
learning algorithms cfs improves naive bayes 
due fact cfs deals effectively redundant attributes naive bayes treats features independent class 
analysis results natural domains revealed weakness cfs 
attributes locally predictive small areas instance space may selected especially overshadowed strongly predictive features 
concluded subsets selected cfs treated definitive represent indication important features dataset 
appendix presents results cfs uc applied suite uci domains evaluation part weka waikato environment knowledge analysis workbench 
results show similar pattern smaller set domains analyzed 
subdivides training instance space constructs decision tree greater chance identifying features explains superior performance chess game dataset 
chapter summary chapter presents experiments test claim cfs method redundant irrelevant features removed learning data cfs common machine learning algorithms 
experiments artificial domains show cfs effective screening irrelevant redundant features long extreme feature interactions cfs able quickly identify relevant features 
furthermore results section show cfs prefer relevant features fewer values noise 
learning curves show ml algorithms increase accuracy slowly artificial domains presence irrelevant redundant information accuracy dramatically improved cfs select features 
experiments selection natural domains show cfs ml algorithms terms improving accuracy case improving comprehensibility induced model 
dimensionality data reduced learning algorithms execute noticeably faster 
examination cases cfs select features results worse performance reveals shortcoming approach 
correlations estimated globally training instances cfs tends select core subset features highly predictive class may fail include subsidiary features locally predictive small area instance space 
experiments show version cfs merges subsets include features subsets merit close best subset able incorporate subsidiary features 
merging feature subsets may allow harmful redundancy included 
versions cfs tested versions symmetrical uncertainty coefficient mdl measure perform best 
selecting fewer features cfs mdl tends cautious cfs uc training instances 
cfs uc performs slightly better cfs mdl 
larger datasets cfs mdl performs slightly better cfs uc 
chapter comparing cfs wrapper chapter compares cfs performance wrapper approach feature selection 
wrapper simplest feature selectors conceptually computationally generally perform filter methods cf ab 
comparing cfs wrapper challenging test wrapper driven estimated performance target learning algorithm tuned inductive bias 
wrapper feature selection rationale wrapper feature selectors induction method ultimately feature subset provide better estimate accuracy separate measure different inductive bias 
possible optimal feature subset induction algorithm contain relevant features advocates wrapper approach claim target learning algorithm feature evaluation function easiest way discover koh kj 
undoubtedly true short designing feature evaluation measure mimics behaviour particular induction algorithm induction algorithm measure stands best chance identifying optimal feature subset 
wrapper feature selectors fault cross validation accuracy estimates highly variable number instances small indicative overfitting koh cross validation prohibitively slow large datasets cf ml 
kohavi koh gives example withholding relevant attribute naive bayes concept results better performance including 
mlc machine learning library provide results wrapper library kohavi feature selection experiments reported literature 
version mlc utilities implementation naive bayes learner built provides support scripts 
unfortunately built version ib support nominal features unsuitable datasets chosen evaluating cfs easy support calling external learning algorithm apart 
reason ib experiments described chapter 
naive bayes represent diverse approaches learning results algorithms provide indication feature selection methods generalize algorithms 
order allow fair comparison cfs wrapper training testing splits generate results shown chapter mlc 
furthermore search strategy stopping condition 
accuracy estimation wrapper achieved fold cross validation training set 
shows stages wrapper feature selector 
testing data training data search training data dimensionality reduction feature set estimated accuracy feature set training data feature evaluation cross validation ml algorithm feature set cv fold hypothesis ml algorithm final evaluation estimated accuracy wrapper wrapper feature selector 
version naive bayes mlc implementation experiments described chapter comparison table shows accuracy naive bayes feature selection naive bayes feature selection wrapper cfs uc domains 
training set sizes artificial datasets added irrelevant redundant features respectively chosen examining learning curves chapter 
concepts point relevant irrelevant redundant features selected cfs noted training sets size instances respectively 
cfs unable select relevant features training set size set exactly half dataset 
table seen wrapper improves naive bayes thirteen datasets degrades 
cfs improves naive bayes fourteen datasets degrades 
clear wrapper difficulty datasets fewer training instances lymphography primary tumour breast cancer dna promoter 
especially noticeable artificial domains added irrelevant features degrades naive bayes performance 
equivalent domains added redundant attributes performance better 
datasets features relevant suggesting combination training instances irrelevant features blame wrapper performance 
wrapper tends outperform cfs datasets training examples mushroom soybean chess game domains features locally predictive small areas instance space chess game audiology 
wrapper better selects features cfs 
features common cfs wrapper fairly stable trials datasets 
suggests wrapper able detect additional locally predictive features cfs 
wrapper cfs need reserve part training data evaluation purposes general tends better smaller datasets wrapper 
shows difference accuracy cfs wrapper 
bars zero line show naive average accuracy feature subsets selected cfs higher average accuracy feature subsets selected wrapper 
cfs better wrapper domains worse domains 
wrapper outperforms cfs domain discovers best performance naive bayes eliminating features 
chapter showed cfs mdl measure cfs mdl achieves result 
dom naive bayes wrapper cfs mu vo cr ly pt bc dna au sb hc kr statistically significant improvement degradation table comparison naive bayes feature selection naive bayes feature selection wrapper cfs 
table shows cpu time taken measured sparc server complete trial dataset wrapper cfs seen cfs faster wrapper 
shows number features selected wrapper cfs 
cfs generally selects similar sized feature set wrapper 
cases number features reduced half methods 
cpu times reported cfs non optimized version chapter suggests simple methods optimization 
accuracy difference mu vo cr ly pt bc dna au sb hc kr dataset comparing cfs wrapper naive bayes average accuracy naive bayes feature subsets selected cfs minus average accuracy naive bayes feature subsets selected wrapper 
dots show statistically significant results 
domain wrapper cfs mu vo cr ly pt bc dna au sb hc kr table time taken cpu units wrapper cfs single trial dataset 
features mu vo cr ly pt bc dna au sb hc kr dataset number features selected wrapper naive bayes left cfs right 
dots show number features original dataset 
table shows accuracy feature selection feature selection wrapper cfs uc 
wrapper improves datasets degrades 
cfs improves datasets degrades 
wrapper successful cfs artificial datasets interacting features 
able improve performance cfs 
interesting note wrapper better cfs chess endgame dataset degrades performance 
possible explanation high order higher pairwise feature interactions dataset 
wrapper forward best search stands chance discovering pairwise interactions backward searches needed discover higher pairwise feature interactions ls 
shows difference accuracy cfs wrapper 
bars zero line show average accuracy feature subsets selected cfs higher average accuracy feature subsets selected wrapper 
cfs better wrapper domains worse domains 
case naive bayes cfs superior wrapper artificial domains added dom wrapper cfs mu vo cr ly pt bc dna au sb hc kr statistically significant improvement degradation table comparison feature selection feature selection wrapper cfs 
irrelevant features 
wrapper consistently identifies interacting features domain interacting features domain time resulting superior performance cfs domains 
wrapper fails small sample size combined presence irrelevant features 
cpu times wrapper similar naive bayes 
soybean dataset took longest just hours complete trial took amount cpu time half minutes 
cfs independent learning algorithm execution time remains 
shows feature selection wrapper cfs affects size trees induced 
bars zero line indicate feature selection reduced size trees 
graph shows feature selectors reduce size trees induced 
cfs affords similar reductions tree size wrapper 
wrapper particularly successful lymphography domain accuracy difference mu vo cr ly pt bc dna au sb hc kr dataset comparing cfs wrapper average accuracy feature subsets selected cfs minus average accuracy feature subsets selected wrapper 
dots show statistically results increase accuracy outperform subsets chosen cfs resulted smallest average tree size interestingly wrapper poorest performer dataset naive bayes 
wrapper tends select slightly smaller feature subsets cfs cfs subsets course naive bayes 
chapter summary chapter compares cfs wrapper feature selector 
cfs implementation wrapper share search strategy represent completely different paradigms feature selection wrappers evaluate feature subsets statistical estimation accuracy respect learning algorithm cfs filter evaluates feature subsets heuristic measure correlation 
wrappers generally considered superior filters tuned specific interaction learning algorithm training data stand best chance tree size difference mu vo cr ly pt bc dna au sb hc kr dataset average change size trees induced features selected wrapper left cfs right 
dots show statistically significant results 
finding optimal feature subset 
experiments chapter show cfs competitive wrapper cases 
cases wrapper performs cfs generally cfs assumption attribute independence class grossly violated contain features locally predictive small areas instance space shown chapter 
cfs training data give better results wrapper small datasets especially irrelevant features 
results show methods select similar sized feature subsets reduce trees similar manner 
argue backward search give better results wrapper 
true backward searches slow wrappers 
kohavi koh kj discusses compound search space operators propel search quickly relevant features backward searches possible experiments show little improvement forward searches artificial domains high order feature interactions 
compound operators reduce execution time wrapper cfs times faster 
chapter extending cfs higher order dependencies experiments artificial domains chapter showed cfs able detect relevant features moderate levels interaction relevant features individually predictive class time 
features ability predict class dependent appear irrelevant cfs assumes feature independence class 
detecting high order feature dependencies difficult probabilities question apt small 
risk overfitting probabilities may reliably represented data limited 
furthermore research area bayesian networks shown inducing optimal bayesian classifier np hard feature node network constrained dependent features sah 
reasons limited pairwise approach detecting feature interactions method explored chapter 
second approach instance attribute estimation method relief see chapter replacement entropy feature class correlation measure cfs uc 
relief potential sufficient data detect higher pairwise feature interactions 
related feature selection method koller sahami ks greedily eliminates features disrupt original conditional class distribution 
reliable estimate high order probability distributions limited data approach assumes features involved high order dependencies exhibit pairwise dependency see chapter details 
cfs naive bayesian classifier assumes features independent class 
assumption efficient simple learning algorithm 
furthermore naive bayes classification performance competitive sophisticated learning schemes dp :10.1.1.144.7475
qualities prompted number attempts reduce algorithm improve performance domains class conditional feature dependencies 
methods described improving naive bayes taken pairwise approach computational reasons detecting incorporating feature dependencies 
pazzani paz combines dependence detection feature selection wrapper approach improving naive bayes 
forward backward hill climbing searches stage search considers adding subtracting feature joining pair features 
manner algorithm join features multiple steps 
joining features occur result increase accuracy 
joining attributes possible combination values attributes considered jointly kononenko kon argues allowing just combinations attributes values considered jointly remain independent flexible 
semi naive bayesian classifier uses exhaustive search determine pairs attribute values worth considering jointly probabilistic reliability measure screen joined values values joined algorithm doing requires multiple iterations 
kdb sah ks algorithm constructing limited bayesian networks allows order feature dependencies 
dependencies specified network greedy fashion feature network arcs added features dependent dependency measured pairwise fashion metric class conditional mutual information 
kdb requires features binary best results kdb mutual information threshold prevents spurious dependencies included network 
tan tree augmented naive bayes algorithm fg constructs bayesian network restricts node feature network additional parent class 
allows optimal classifier quadratic time 
kdb tan uses class conditional mutual information measure dependency attributes 
joining features straightforward computationally feasible extension cfs detecting higher order dependencies consider pairs features 
joining features results derived attribute possible value corresponding combination values example attribute values attribute values joined attribute xy values ay az bz cy cz 
algorithm considers possible pairwise combinations features manner quadratic original number features 
new attributes created corresponding possible pairwise combination features feature class correlations calculated normal fashion measures described chapter 
derived feature candidate feature selection correlation class higher constituent features discarded 
derived features screened fashion feature feature inter correlations calculated new feature space feature selection proceeds original algorithm 
important note extension cfs perform constructive induction alter input space machine learning algorithm fashion discarding number original features 
best feature subset cfs contains derived features passed learning algorithm individual features comprise derived features 
initial experiments enhancement cfs dubbed cfs showed derived features candidates feature selection number training instances small leading larger final feature subsets inferior performance compared chosen standard cfs 
experiments chapter showed correlation measures tended increase number training examples decreased attributes greater number values fewer values 
situation probability estimates reliable attributes values may appear useful warranted amount available data 
counter trend overfitting small training sets reliability constraint applied chi square tests independence 
equation shows statistic chi square distribution eij oij eij oij observed number training instances class ci having th value attribute eij expected number instances null hypothesis association attributes true eij jni 
equation number training instances th value attribute ni 
number training instances class ci total number training instances 
equation unreliable optimistic detecting association expected frequencies small 
recommended chi square test expected frequencies sie ww 
cfs derived feature screened subjecting expected frequencies values possible classes constraint derived feature candidate selection association class respect particular correlation measure overestimated 
incorporating relief cfs drawback considering pairs features feature subset space enlarged considerably turn take longer explore 
second approach detecting feature interactions expand feature subset space course incur computational expense incorporate relief kr kon algorithm estimating feature relevance 
feature estimates provided relief replace feature class correlations cfs uc feature feature inter correlations calculated normal symmetrical uncertainty coefficient 
version cfs called cfs relief opposed cfs relief context insensitive simplification relief calculate feature correlations described chapters 
relief instance algorithm imposes ranking features assigning weight 
weight particular feature reflects relevance distinguishing classes 
relief feature selection right explicitly select subset relevance threshold set domain domain basis number features discarded 
furthermore relief attempt deal redundant features 
advantage relief sensitive feature interactions detect higher pairwise interactions data 
kononenko kon notes relief assigns weight feature approximating difference probabilities wx different value nearest instance different class different value nearest instance class relief incrementally updates weights features repeatedly sampling instances training data 
instance similarity metrics find nearest instances sampled metrics take features account weight feature estimated context features 
original relief algorithm kr operates class domains 
relief set feature weights randomly select instance find nearest hit class nearest different class attribute diff diff function diff calculates difference values attribute instances 
nominal attributes difference attribute value instances value attribute differs instances 
continuous attributes difference squared arithmetic difference normalized interval 
diff calculate difference instances finding nearest hits misses 
difference instances simply sum attribute differences 
version relief incorporated cfs extended version relief described kononenko kon generalizes relief multiple classes handles noise 
increase reliability relief weight estimation relief finds nearest hits misses instance 
multiple class problems relief finds nearest misses different class respect instance averages contribution updating 
average weighted prior probability class 
version relief runs outer loop relief algorithm available training instances randomly sampling number 
results variation relief estimation feature weights cost increased computation 
evaluation section evaluates performance extended forms cfs cfs cfs relief described 
particular interest performance artificial domains strong feature interactions 
results feature selection ib algorithm sensitive interacting features achieve higher accuracy artificial domains correct feature set naive bayes 
table compares performance enhanced cfs cfs cfs relief standard cfs uc artificial domains ib induction algorithm 
results show cfs consistently identifies relevant features selects relevant features domain averages relevant features domain 
incorporating derived features cfs resulted worse performance artificial domains 
domains added redundant features cfs selects average redundant feature 
examination subsets selected cfs show selects joined feature correctly captures dependencies converted back constituent features results inclusion redundant feature 
example feature set ab bc ce correctly captures pairwise dependencies dataset resolves feature set feature copy feature difficulty derived feature ac equivalent ce represent dependency algorithm tell appropriate necessary 
inclusion occasional redundant feature domains affect accuracy ib 
cfs selects correct subsets domains accepts derived features candidates selection necessary 
chapter showed symmetrical uncertainty relief correlation favour non informative attributes values values 
derived attribute comprised non informative attributes appears predictive symmetrical uncertainty relief constituent attributes 
happen mdl measure assigns value zero non informative attributes sufficient data 
results cfs relief show effective cfs 
improves standard cfs uc datasets cfs degrades datasets cfs degrade 
irrelevant features included cfs relief domain resulting lower accuracy cfs standard ib domain cfs uc cfs cfs relief statistically significant improvement degradation table performance enhanced cfs cfs cfs relief compared standard cfs uc artificial domains ib induction algorithm 
figures braces show average number features selected 
cfs uc 
cfs relief selects fewer relevant features average cfs resulting lower accuracy domains 
interesting relief ability detect high order interactions cfs relief cfs especially feature interaction 
kira rendell kr note amount feature interaction increases amount training data increase order relief reliably estimate feature relevance 
tested cfs relief increasing number instances dataset 
size dataset cfs relief able reliably select relevant features 
cfs relief effective fails select relevant features 
problem lies relief algorithm examination feature relevances assigned relief datasets show feature exact copies domain consistently assigned relevance 
selection nearest neighbours important relief attempts find nearest neighbours respect important attributes 
averaging contribution nearest neighbours improves relief attribute estimates helping counteract effect irrelevant attributes redundant attributes noise nearest neighbour selection 
helped case see consider weight update attribute domain instance 
furthermore assume copies instance dataset small sam ples 
table shows possible instances domain distance instance 
nearest neighbour class copy 
value instances change weight nearest instances differing feature values equally close opposite class 
value differs results weight incremented average times instance sampled training data 
fourth feature see table exact copy nearest neighbour opposite class 
instance value results change weight situation nearest instance opposite class having value feature occurs instance domain copy feature 
result weight copy changed initial value 
example simplicity 
increasing value help case increased proportion number training instances remedy restrict instance appearing list nearest neighbours 
tables compare performance enhanced cfs cfs cfs relief standard cfs uc artificial domains naive bayes induction algorithms 
case cfs improves standard cfs uc 
cfs result worse performance dataset compared cfs uc 
cfs relief improves standard cfs cfs select relevant features 
case naive bayes enhancements cfs result degraded results compared standard cfs uc effect dramatic cfs 
training instances 
assuming instances uniformly distributed set excess order start detecting relevance attribute inst 
class dist inst 
class dist table example effect redundant attribute relief distance calculation domain 
table shows instances domain table shows instances domain added redundant attribute 
column marked dist shows far particular instance instance 
occasional redundant feature responsible slight decrease performance cfs domains 
inclusion interacting features cfs cfs relief results slightly worse leaving 
domain cfs uc cfs cfs relief statistically significant improvement degradation table performance enhanced cfs cfs cfs relief compared standard cfs uc artificial induction algorithm 
cfs cfs relief natural domains 
tables compare performance cfs cfs relief standard cfs uc natural domains ib naive bayes induction algorithms 
subsets selected cfs practically identical selected standard cfs uc datasets exception mushroom 
result significant naive bayes domain cfs uc cfs cfs relief statistically significant improvement degradation table performance enhanced cfs cfs cfs relief compared standard cfs uc artificial naive bayes induction algorithm 
accuracy difference mushroom dataset induction algorithms subsets selected cfs 
derived features considered cfs natural domains concluded little pairwise dependency datasets cases training data reliable estimations 
exception chess game dataset derived features considered average 
derived features clearly irrelevant strongest pairwise dependencies features selected standard cfs dataset 
possible strong dependencies useful small area instance space problem occured normal features standard cfs chapter 
cfs relief better standard cfs cfs mushroom domain ib naive bayes 
suggests detected feature interaction cfs dataset 
result audiology better standard cfs ib naive bayes 
subsets provided cfs relief resulted worse performance standard cfs datasets datasets naive bayes ib 
datasets cfs relief degraded accuracy fewer instances 
suggests relief attribute estimation reliable small datasets 
comparison cfs mdl measure considered average derived attributes chess game dataset 
ib domain cfs uc cfs cfs relief mu vo cr ly pt bc dna au sb hc kr statistically significant improvement degradation table performance enhanced cfs cfs cfs relief compared standard cfs uc natural domains ib induction algorithm 
domain cfs uc cfs cfs relief mu vo cr ly pt bc dna au sb hc kr statistically significant improvement degradation table performance enhanced cfs cfs cfs relief compared standard cfs uc natural domains induction algorithm 
naive bayes domain cfs uc cfs cfs relief mu vo cr ly pt bc dna au sb hc kr statistically significant improvement degradation table performance enhanced cfs cfs cfs relief compared standard cfs uc natural naive bayes induction algorithm 
discussion chapter presents methods extending cfs detect feature interaction cfs considers pairs features cfs relief replaces standard cfs feature class correlation attribute estimates provided relief algorithm 
experiments comparing enhancements standard cfs drawn cfs cfs relief improve accuracy standard cfs domains pairwise feature interactions 
general cfs degrade accuracy compared standard cfs 
cfs relief perform cfs 
relief estimates attributes reliable fewer training instances cases affected presence redundant attributes 
factors impact feature subsets selected cfs relief 
considering pairs features degrade performance cfs cfs preferred cfs relief suspected dataset contains feature interactions 
larger datasets mdl measure preferred correlation measure cfs ability clearly identify non informative attributes result fewer spurious derived features considered selection 
noted cfs detect interaction higher order pairwise 
relief hand shown parity concepts able detect higher pairwise interactions sufficient training data kr 
cfs cfs relief computationally expensive standard cfs 
worst case cfs may square number features consideration pairwise combination original features accepted candidate selection happen practice 
finds nearest neighbours training instance version relief cfs relief quadratic number training instances 
enhanced versions cfs faster wrapper 
example single trial mushroom dataset took units cpu time cfs derived features considered candidates units cpu time cfs relief trial took units cpu time wrapper 
chapter summary central claim thesis feature selection supervised machine learning accomplished basis correlation features 
feature selection algorithm implemented empirically tested support claim 
chapter outlined rationale correlation approach feature selection ideas evaluation formula adapted test theory 
evaluation formula awards high merit feature subsets contain features predictive class measured average correlations individual features class low level redundancy measured average inter correlation features 
implementation correlation feature selection algorithm cfs incorporating evaluation function described 
methods measuring association nominal features reviewed candidates feature correlations required evaluation function 
experiments artificial data showed measures prefer predictive features fewer values bias compatible decision tree algorithms prefer smaller trees larger ones 
measures relief symmetrical uncertainty give optimistic estimates may estimate multi valued attributes data limited 
mdl measure gives pessimistic estimates data limited situation may result preference smaller feature subsets evaluation function 
cfs empirically tested artificial natural machine learning datasets 
experiments artificial datasets showed cfs effectively screen irrelevant redundant noisy features 
cfs selects relevant features long strongly interact features 
correlation measures reviewed chapter symmetrical uncertainty mdl measure superior relief cfs 
cases attributes divide training data pure subsets relief shown sensitive size pure subset symmetrical uncertainty mdl situation lead underestimation omission relevant features 
experiments common machine learning algorithms natural domains showed cases cfs improves performance reduces size induced knowledge structures 
symmetrical uncertainty mdl correlation measures give better results relief 
symmetrical uncertainty chosen preferred correlation measure cfs gave slightly better results small datasets cautious mdl 
results datasets showed cfs overly aggressive feature selection 
particular cfs may fail select features locally predictive small areas instance space especially overshadowed strong globally predictive features 
method merging top ranked feature subsets partially mitigates problem method completely satisfactory allows redundant features re included final feature set 
tests compared cfs wrapper approach feature selection 
cases cfs gives results comparable wrapper generally outperforms wrapper small datasets 
datasets wrapper clearly outperforms cfs contain strong feature interactions features locally predictive small numbers instances 
cfs faster wrapper orders magnitude 
chapter investigated methods extending cfs detect feature interaction 
improved results datasets 
method cfs considers pairwise combinations features gives reliable results second method cfs relief uses weights estimated relief algorithm correlations features classes 
cfs relief potential data detect higher pairwise feature interactions 
single learning algorithm superior problems 
research machine learning attempts provide insight strengths limitations different algorithms 
armed insight background knowledge particular problem practitioners choose algorithms apply 
case cfs cases cfs enhance degrade performance machine learning algorithms time achieving reduction number features learning 
cfs may fail select relevant features data contains strongly interacting features features values predictive small area instance space 
cfs component weka workbench part ongoing research university waikato produce high quality process model machine learning 
cfs applied number problems notably select features musical compression system bi 
greatest limitation cfs failure select features locally predictive values overshadowed strong globally predictive features 
single feature may account small proportion dataset number features may cumulatively cover significant proportion dataset 
merging feature subsets allows redundancy re introduced 
redundancy affect algorithms ib detrimental effect naive bayes 
ideal solution naive bayes mind identify attributes locally predictive instances covered selected attributes low correlation selected attributes 
course attributes locally predictive low correlation irrelevant values number features degrade performance instance learners 
domingos dom addresses problem specifically instance learners wrapper select different feature set instance 
cfs global filters general case able please people time people time 
interesting apply boosting technique problem detecting locally predictive features 
boosting methods fs bre improve classification performance combining predictions knowledge induced multiple runs learning algorithm 
iteration learning algorithm focused areas training instance space learner previous iteration difficult predict 
approach necessitates particular learning algorithm combined cfs result hybrid system wrapper filter 
standard cfs select initial set features 
learning algorithm selected features applied predict weight training instances 
cfs applied weighted training instances select secondary set features forth 
locally predictive features genuinely useful help predicting instances learner previous iteration difficulty 
features selected cfs generally represent core subset features 
interesting see wrapper feature selector fare started feature subset selected cfs 
case bidirectional search considers additions deletions features appropriate wrapper forward backward search 
search initiated intelligent start point computational expense wrapper reduced fewer subsets evaluated 
approach may improve wrapper performance smaller datasets reliable accuracy estimates cause trapped local maxima 
area trying developing measures correlation cfs 
measures correlation operate nominal variables explored thesis 
justification desirable treat different types features uniform manner order provide common basis computing correlation discretization shown improve significantly degrade performance learning algorithms dks 
ting tin describes method converting nominal attributes numeric attributes opposite discretization 
method replaces nominal value attribute estimated prior probability training data 
attributes including class numeric pearson linear correlation cfs 
experiments evaluate cfs domains attributes numeric learning algorithms ct ww capable predicting continuous class variables :10.1.1.51.4098
appendix graphs chapter shows behaviour symmetrical uncertainty relief mdl number training instances vary classes 
symmetrical 
coeff 
informative informative informative non informative non informative non informative symmetrical 
coeff 
informative informative informative non informative non informative non informative number examples number examples informative informative informative non informative non informative non informative informative informative informative non informative non informative non informative symmetrical relief symmetrical relief number examples number examples normalized symmetrical mdl informative informative informative non informative non informative non informative normalized symmetrical mdl informative informative informative non informative non informative non informative number examples number examples effect varying training set size symmetrical uncertainty symmetrical relief normalized symmetrical mdl attributes informative non informative 
number classes curves shown valued attributes 
appendix curves concept added redundant attributes figures show curves cfs uc cfs mdl cfs relief concept added redundant attributes 
cfs uc cfs mdl cfs relief redundant features training set size number redundant attributes selected concept cfs uc cfs mdl cfs relief function training set size 
cfs uc cfs mdl cfs relief relevant features training set size number relevant attributes selected concept cfs uc cfs mdl cfs relief function training set size 
cfs uc cfs mdl cfs relief multi valued features training set size number multi valued attributes selected concept cfs uc cfs mdl cfs relief function training set size 
cfs uc cfs mdl cfs relief noisy features training set size number noisy attributes selected concept cfs uc cfs mdl cfs relief function training set size 
appendix results cfs uc cfs mdl cfs relief natural domains domain cfs uc nbayes cfs mdl nbayes cfs relief nbayes mu vo cr ly pt bc dna au sb hc kr statistically significant result better worse table accuracy naive bayes feature selection cfs uc compared feature selection cfs mdl cfs relief 
domain cfs uc ib cfs mdl ib cfs relief ib mu vo cr ly pt bc dna au sb hc kr statistically significant result better worse table accuracy ib feature selection cfs uc compared feature selection cfs mdl cfs relief 
domain cfs uc cfs mdl cfs relief mu vo cr ly pt bc dna au sb hc kr statistically significant result better worse table accuracy feature selection cfs uc compared feature selection cfs mdl cfs relief 
appendix cv paired test results dietterich die shown common approach paired differences test random subsampling elevated chance type error incorrectly detecting difference difference exists 
recommends cv test warns test increased chance type ii error failing detect difference exist 
test iterations fold cross validation uses modified statistic overcome type problem 
cv statistic difference accuracy fold replication fold cross validation variance computed th replication 
table shows results naive bayes ib feature selection cfs uc 
cv test applied 
results similar pattern chapter cfs improves performance naive bayes datasets ib 
seen fewer significant results indicating cfs safely removes attributes adversely affecting accuracy learning algorithms 
table shows accuracy naive bayes feature selection naive bayes feature selection wrapper cfs uc domains 
cv test applied 
similarly table shows accuracy feature dom naive bayes cfs nbayes ib cfs ib cfs mu vo cr ly pt bc dna au sb hc kr statistically significant improvement degradation table naive bayes ib feature selection natural domains 
cv test significance applied 
selection wrapper cfs uc 
results follow pattern similar chapter cfs better job naive bayes wrapper wrapper better job cfs 
datasets cfs degrades accuracy strong attribute interactions worse results wrapper expected cases 
dom naive bayes wrapper cfs mu vo cr ly pt bc dna au sb hc kr statistically significant improvement degradation table comparison naive bayes feature selection naive bayes feature selection wrapper cfs 
cv test significance applied 
dom wrapper cfs mu vo cr ly pt bc dna au sb hc kr statistically significant improvement degradation table comparison feature selection feature selection wrapper cfs 
cv test significance applied 
appendix cfs merit versus accuracy naive bayes accuracy ib accuracy accuracy merit naive bayes merit ib merit mushroom mu 
naive bayes accuracy ib accuracy accuracy merit naive bayes merit ib merit vote vo 
naive bayes accuracy ib accuracy accuracy merit naive bayes merit ib merit vote 
naive bayes accuracy ib accuracy accuracy merit naive bayes merit ib merit australian credit screening cr 
naive bayes accuracy ib accuracy accuracy merit naive bayes merit ib merit lymphography ly 
naive bayes accuracy ib accuracy accuracy merit naive bayes merit ib merit primary tumour pt 
naive bayes accuracy ib accuracy accuracy merit naive bayes merit ib merit breast cancer bc 
naive bayes accuracy ib accuracy accuracy merit naive bayes merit ib merit dna promoter dna 
naive bayes accuracy ib accuracy accuracy merit naive bayes merit ib merit audiology au 
naive bayes accuracy ib accuracy accuracy merit naive bayes merit ib merit soybean large sb 
naive bayes accuracy ib accuracy accuracy merit naive bayes merit ib merit horse colic hc 
naive bayes accuracy ib accuracy accuracy merit naive bayes merit ib merit chess game kr 
appendix cfs applied uci domains table shows results machine learning algorithms feature selection cfs uc uci domains 
domains test suite experiments weka workbench representative available uci repository 
accuracy table average train test trials training testing split data 
colic colic orig hc versions horse colic dataset 
colic hc attributes respectively surgical lesion class 
colic orig attributes pathology cp data class 
version naive bayes experiments part weka workbench 
shows average number features selected cfs uc domains 
shows effect feature selection cfs uc size trees induced 
domain naive bayes cfs uc ib cfs uc cfs uc anneal audiology autos balance scale breast cancer breast colic colic orig hc credit credit diabetes glass heart heart heart statlog hepatitis hypothyroid ionosphere iris kr vs kp letter lymph mushroom primary tumor promoters segment sick sonar soybean splice vehicle vote vote vowel waveform zoo average statistically significant improvement degradation table comparison learning algorithms feature selection cfs uc 
features dataset average number features selected cfs uci domains 
dots show original number features 
tree size difference dataset effect feature selection size trees induced uci domains 
bars zero line indicate feature selection reduced tree size 
bibliography ab ad ad aha 
feature selection case classification cloud types 
working notes th aaai workshop case reasoning pages 
almuallim dietterich 
learning irrelevant features 
proceedings ninth national conference artificial intelligence pages 
mit press 
almuallim dietterich 
efficient algorithms identifying relevant features 
proceedings ninth canadian conference artificial intelligence pages 
morgan kaufmann 
aha aha 
tolerating noisy irrelevant novel attributes instancebased learning algorithms 
international journal man machine studies 
aka bi aha kibler albert 
instance learning algorithms 
machine learning 
allen 
relationship variable selection data augmentation method prediction 
technometrics 
breiman friedman olshen stone 
classification regression trees 
wadsworth international group 
bainbridge 
musical image compression 
proceedings ieee data compression conference pages 
bre breiman 
bias variance arcing classifiers 
technical report university california berkeley ca 
bre car cat breiman 
note properties splitting criteria 
machine learning 
cardie 
decision trees improve cased learning 
proceedings international conference knowledge discovery data mining 
aaai press 
catlett 
changing continuous attributes ordered discrete attributes 
proceedings european working session learning pages berlin 
springer verlag 
cb brodley 
qualitative impurity splitting rules minima free property 
technical report electrical computer engineering purdue university 
cf caruana freitag 
greedy attribute selection 
machine learning proceedings eleventh international conference 
morgan kaufmann 
cleary witten 
mdl estimate significance rules 
proceedings isis information statistics induction science 
cunningham witten 
applications machine learning information retrieval 
technical report university waikato 
masand smith waltz 
trading mips memory knowledge engineering 
communications acm 
cn cs cs ct clark niblett :10.1.1.51.4098
cn induction algorithm 
machine learning 
cost salzberg 
weighted nearest neighbor algorithm learning symbolic features 
machine learning 
shavlik 
growing simpler decision trees facilitate knowledge discovery 
proceedings second international conference knowledge discovery data mining 
aaai press 
cleary trigg 
instance learner entropic distance measure 
machine learning proceedings international conference 
morgan kaufmann 
die dietterich 
approximate statistical tests comparing supervised classification algorithms 
neural computation 
dk devijver kittler 
pattern recognition statistical approach 
prentice hall international 
dks dougherty kohavi sahami 
supervised unsupervised discretisation continuous features 
machine learning proceedings international conference 
morgan kaufmann 
dom domingos 
context sensitive feature selection lazy learners 
artificial intelligence review 
dp domingos pazzani :10.1.1.144.7475
independence conditions optimality simple bayesian classifier 
machine learning proceedings thirteenth international conference machine learning 
morgan kaufmann 
dp domingos pazzani 
optimality simple bayesian classifier zero loss 
machine learning 
fg fi fs gei ghi gl glf hog hol friedman goldszmidt 
building classifiers bayesian networks 
proceedings national conference artificial intelligence pages 
fayyad irani 
multi interval discretisation attributes classification learning 
proceedings thirteenth international join conference artificial intelligence 
morgan kaufmann 
freund schapire 
experiments new boosting algorithm 
machine learning proceedings thirteenth international conference machine learning 
morgan kaufmann 

predictive sample reuse method applications 
journal american statistical association 

theory psychological measurement 
mcgrawhill new york 
lavrac 
conditions occam razor applicability noise elimination 
proccedings ninth european conference machine learning 
gennari langley fisher 
models incremental concept formation 
artificial intelligence 
holmes witten 
weka machine learning workbench 
proceedings second australia new zealand conference intelligent information systems 
holmes nevill manning 
feature selection discovery simple classification rules 
proceedings symposium intelligent data analysis baden baden germany 

methods aggregating opinions 
de editors decision making change human affairs 
reidel publishing dordrecht holland 
holland 
adaption natural artificial systems 
university michigan press ann arbor mi 
hol holte 
simple classification rules perform commonly datasets 
machine learning 
hut hutchinson 
algorithmic learning 
clarendon press oxford 
jl kb kf john kohavi pfleger 
irrelevant features subset selection problem 
machine learning proceedings eleventh international conference 
morgan kaufmann 
john langley 
static versus dynamic sampling data mining 
proceedings second international conference knowledge discovery data mining 
aaai press 
kononenko bratko 
information evaluation criterion classifier performance 
machine learning 
kohavi 
useful feature subsets rough sets reducts 
proceedings third international workshop rough sets soft computing 
kit kittler 
feature set search algorithms 
chen editor pattern recognition signal processing 
netherlands 
kj kohavi john 
wrappers feature subset selection 
artificial intelligence special issue relevance 
kohavi john long manley pfleger 
mlc machine learning library 
tools artificial intelligence pages 
ieee computer press 
kohavi langley yun :10.1.1.49.3282
utility feature weighting nearest neighbor algorithms 
proceedings ninth european conference machine learning prague 
springer verlag 
koh koh kon kon kon kohavi :10.1.1.51.3072
power decision tables 
european conference machine learning 
kohavi 
wrappers performance enhancement oblivious decision graphs 
phd thesis stanford university 
kononenko 
semi naive bayesian classifier 
proceedings sixth european working session learning pages 
kononenko 
estimating attributes analysis extensions relief 
proceedings european conference machine learning 
kononenko 
biases estimating multi valued attributes 
ijcai pages 
kr ks ks ks ks lan lin ls kira rendell :10.1.1.43.3692
practical approach feature selection 
machine learning proceedings ninth international conference 
kohavi sommerfield 
feature subset selection wrapper method overfitting dynamic search space topology 
proceedings international conference knowledge discovery data mining 
aaai press 
kohavi sahami 
error entropy discretisation continuous features 
proceedings second international conference knowledge discovery data mining 
aaai press 
koller sahami 
optimal feature selection 
machine learning proceedings thirteenth international conference machine learning 
morgan kaufmann 
koller sahami 
classifying documents words 
machine learning proceedings fourteenth international conference 
morgan kaufmann 
langley 
selection relevant features machine learning 
proceedings aaai fall symposium relevance 
aaai press 

society field guide north american mushrooms 
alfred new york 
langley sage 
induction selective bayesian classifiers 
proceedings tenth conference uncertainty artificial intelligence seattle 
morgan kaufmann 
ls langley sage 
oblivious decision trees cases 
working notes aaai workshop case reasoning seattle 
aaai press 
ls langley sage 
scaling domains irrelevant features 
greiner editor computational learning theory natural learning systems volume 
mit press 
ls langley simon 
applications machine learning rule induction 
acm 
ls mg liu setiono 
probabilistic approach feature selection filter solution 
machine learning proceedings thirteenth international conference machine learning 
morgan kaufmann 
marill green 
effectiveness receptors recognition systems 
ieee transactions information theory 
mic mil moore hill johnson 
empirical investigation brute force choose features smoothers function approximators 
hanson judd petsche editors computational learning theory natural learning systems volume 
mit press 
michalski 
theory methodology inductive learning 
artificial intelligence 
miller 
subset selection regression 
chapman hall new york 
mit mitchell 
machine learning 
mcgraw hill new york 
ml moore lee :10.1.1.108.8901
efficient algorithms minimizing cross validation error 
machine learning proceedings eleventh international conference 
morgan kaufmann 
mm merz murphy 
uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
mod nf 
feature selection rough sets theory 
proceedings european conference machine learning pages 
narendra fukunaga 
branch bound algorithm feature subset selection 
ieee transactions computers 
par parsons 
voice speech processing 
mcgraw hill 
paw 
rough sets theoretical aspects reasoning data 
kluwer 
paz pea pazzani 
searching dependencies bayesian classifiers 
proceedings fifth international workshop ai statistics 
pearl 
probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann san mateo california 
pfa pfahringer 
compression feature subset selection 
proceedings ijcai workshop data engineering inductive learning pages 
press flannery teukolsky vetterling 
numerical recipes cambridge university press cambridge 
ps provan singh 
learning bayesian networks feature selection 
fisher lenz editors learning data lecture notes statistics pages 
springer verlag new york 
psf piatetsky shapiro frawley 
knowledge discovery databases 
mit press cambridge mass 
qui quinlan 
induction decision trees 
machine learning 
qui qui qui quinlan 
simplifying decision trees 
international journal man machine studies 
quinlan 
inferring decision trees minimum description length principle 
information computation 
quinlan 
programs machine learning 
morgan kaufmann los altos california 
ris rissanen 
modeling shortest data description 
automatica 
rk rich knight 
artificial intelligence 
mcgraw hill 
sah sal sb sch sahami 
learning limited dependence bayesian classifiers 
proceedings second international conference knowledge discovery data mining 
aaai press 
salzberg 
nearest hyperrectangle learning method 
machine learning 
brauer 
feature selection means feature weighting approach 
technical report technische universit nchen 
schaffer 
selecting classification method cross validation 
machine learning 
schapire freund bartlett lee 
boosting margin new explanation effectiveness voting methods 
machine learning proceedings fourteenth international conference 
morgan kaufmann 
sie siegel 
nonparametric statistics 
mcgraw hill new york 
ska sl sp skalak 
prototype feature selection sampling random mutation hill climbing algorithms 
machine learning proceedings eleventh international conference 
morgan kaufmann 
setiono liu 
chi feature selection discretization numeric attributes 
proceedings seventh ieee international conference tools artificial intelligence 
singh provan 
efficient learning selective bayesian network classifiers 
machine learning proceedings thirteenth international conference machine learning 
morgan kaufmann 
sw tbb tho tin vj wa wc wl shannon weaver 
mathematical theory 
university illinois press urbana ill 
thrun bala bloedorn bratko cestnik cheng de dzeroski fahlman fisher hamann kaufman keller kononenko michalski mitchell reich vafaie van de wenzel zhang 
monk problems performance comparison different learning algorithms 
technical report cmu cs carnegie mellon university 
thornton 
techniques computational learning 
chapman hall london 
ting 
common issues instance naive bayesian classifiers 
phd thesis university sydney nsw australia 
vafaie de jong 
genetic algorithms tool restructuring feature space representations 
proceedings international conference tools ieee computer society press 
wettschereck aha 
weighting features 
international conference cased reasoning portugal 
springer 
wong chiu 
synthesizing statistical knowledge incomplete mixed mode data 
ieee transactions pattern analysis machine intelligence 
white liu 
bias information measures decision tree induction 
machine learning 
ww wonnacott wonnacott 
introductory statistics 
wiley 
ww wang witten 
inducing model trees continuous classes 
proceedings poster papers ninth european conference machine learning 

note group judgements group size 
human relations 

