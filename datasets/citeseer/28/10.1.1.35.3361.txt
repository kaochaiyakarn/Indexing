portability tuning supervised word sense disambiguation systems gerard german research center software department lsi technical university upc barcelona 
lsi upc es report describes set experiments carried explore portability alternative supervised word sense disambiguation algorithms 
aim threefold firstly studying performance algorithms tested different corpus trained secondly exploring ability tune new domains thirdly demonstrating empirically lazyboosting algorithm outperforms state art supervised wsd algorithms previous situations 
keywords word sense disambiguation machine learning natural language processing portability tuning nlp systems 
word sense disambiguation wsd problem assigning appropriate meaning sense word text discourse meaning distinguishable senses potentially attributable word 
example table shows definition senses word age example sentence sense 
sense definitions age length time existed age replaced age age historic period age live age corpora examples age mad stars age age years ago ice age ended table sense definitions corpora examples 
problem fact date large scale broad coverage highly accurate wsd system built 
successful current lines research corpus approach statistical machine learning ml algorithms applied learn statistical models classifiers corpora order perform wsd 
generally supervised approaches learn previously semantically annotated corpus obtained better results unsupervised methods small sets selected ambiguous words artificial pseudo words 
standard ml algorithms supervised learning applied wsd including decision lists yarowsky neural networks towell voorhees bayesian learning bruce wiebe exemplar learning ng fujii boosting :10.1.1.11.5417
mooney previously cited methods compared jointly decision trees rule induction algorithms restricted domain 
performance supervised ml systems usually calculated testing algorithm separate part set annotated examples say fold cross validation set examples partitioned disjoint sets folds training test procedure repeated times combinations gamma folds training fold testing 
cases test examples different training belong corpus expected quite similar 
methodology valid certain nlp problems english part ofspeech tagging think exists reasonable evidence say wsd accuracy results simply extrapolated domains ffl wsd domain application 
performance supervised ml systems usually calculated testing algorithm separate part set annotated examples say fold cross validation set examples partitioned disjoint sets folds training test procedure repeated times combinations gamma folds training fold testing 
cases test examples different training belong corpus expected quite similar 
methodology valid certain nlp problems english part ofspeech tagging think exists reasonable evidence say wsd accuracy results simply extrapolated domains ffl wsd domain application 
gale idea sense discourse suggested polysemous word appears times written discourse extremely share sense 
ffl see ng lee ng quite different accuracy figures obtained testing exemplar wsd classifier different corpora wall street journal brown corpus corpora belong different domains :10.1.1.11.5417
ffl reasonable think training material large representative cover potential types examples 
think study domain dependence wsd style studies devoted parsing needed assess validity supervised approach determine extent tuning process necessary real wsd systems portable 
order corroborate previous hypotheses explores portability tuning different ml algorithms training testing different corpora 
supervised wsd really viable 
order corroborate previous hypotheses explores portability tuning different ml algorithms training testing different corpora 
supervised wsd really viable 
known supervised methods suffer lack widely available semantically tagged corpora construct really broad coverage wsd systems 
known knowledge acquisition bottleneck gale 
ng estimated manual annotation effort necessary build broad coverage semantically annotated english corpus man years :10.1.1.11.5417
extremely high overhead supervision greater costly tuning procedure required applying existing system new domain additionally serious learning overhead common ml algorithms scale real size wsd problems explain supervised methods seriously questioned 
due fact works focused reducing acquisition cost need supervision computational requirements corpus methods wsd 
consequently lines research explored 
design efficient sampling methods engelson dagan fujii 
classical setting duda hart 
assuming independence features classifies new example assigning class maximizes conditional probability class observed sequence features example 
fc cm set classes set feature values test example 
naive bayes method tries find class maximizes arg max arg max estimated training process relative frequencies 
avoid effects zero counts estimating conditional probabilities model simple smoothing technique proposed ng :10.1.1.11.5417
consists replacing zero counts number training examples 
despite simplicity naive bayes claimed obtain state art accuracy supervised wsd papers mooney ng leacock :10.1.1.11.5417
exemplar classifier eb exemplar learning stanfill waltz aha generalization training examples performed 
examples stored memory classification new examples classes similar stored examples 
fc cm set classes set feature values test example 
naive bayes method tries find class maximizes arg max arg max estimated training process relative frequencies 
avoid effects zero counts estimating conditional probabilities model simple smoothing technique proposed ng :10.1.1.11.5417
consists replacing zero counts number training examples 
despite simplicity naive bayes claimed obtain state art accuracy supervised wsd papers mooney ng leacock :10.1.1.11.5417
exemplar classifier eb exemplar learning stanfill waltz aha generalization training examples performed 
examples stored memory classification new examples classes similar stored examples 
implementation examples stored memory classification new example nn nearest neighbours algorithm hamming distance measure closeness doing examples examined 
greater resulting sense weighted majority sense nearest neighbours example votes sense strength proportional closeness test example 
exemplar classifier eb exemplar learning stanfill waltz aha generalization training examples performed 
examples stored memory classification new examples classes similar stored examples 
implementation examples stored memory classification new example nn nearest neighbours algorithm hamming distance measure closeness doing examples examined 
greater resulting sense weighted majority sense nearest neighbours example votes sense strength proportional closeness test example 
experiments explained section eb algorithm run times different number nearest neighbours results corresponding best choice reported exemplar learning said best option wsd ng :10.1.1.11.5417
authors daelemans point exemplar methods tend superior language learning problems forget exceptions 
snow winnow classifier snow golding roth stands sparse network 
line learning system winnow algorithm basic component 
method applied wsd 
line learning system winnow algorithm basic component 
method applied wsd 
reason described detail 
winnow algorithm winnow littlestone linear threshold algorithm class problems binary valued input features 
classifies new example positive class order construct real eb system wsd parameter estimated cross validation training set ng :10.1.1.11.5417
negative class 
formulation set active features value weight associated input feature threshold parameter 
winnow online algorithm accepts examples time updates weights necessary 
winnow initializes weights 
averaged results nouns verbs best results case printed boldface 
observed lb outperforms methods cases 
additionally superiority statistically significant comparing lb eb approach cases marked asterisk 
note surprisingly lb achieve substantial improvement results fact variation statistically significant second slightly significant 
knowledge second third column correspond train test sets ng lee ng acquired single corpus covers knowledge combining corpora :10.1.1.11.5417
effect observed methods specially cases snow vs joining training corpora counterproductive 
accuracy method pos nouns mfc verbs total nouns nb verbs total nouns eb verbs total nouns snow verbs total nouns lb verbs total table accuracy results methods training test combinations regarding portability systems disappointing results obtained 
restricting lb results observe accuracy obtained accuracy considered upper bound lb corpus difference points 
furthermore slightly better frequent sense corpus 
senses feature rule state court rule state court table example referred collocation state court 
noted rules completely coherent senses assigned examples containing state court corpora contradiction comes different information characterize examples corpora 
pointed difficulties regarding portability supervised wsd systems important issue paid little attention 
main extracted assure portability systems process tuning new domain required learning testing corpora differ bc wsj 
result contradiction idea robust broad coverage wsd introduced ng supervised system trained large corpora say examples word provide accurate disambiguation corpora significantly better mfs :10.1.1.11.5417
consequently belief number issues regarding portability tuning knowledge acquisition studied stating supervised ml paradigm able resolve realistic wsd problem 
regarding ml algorithms tested contribution consist empirically demonstrating lazyboosting algorithm outperforms state art supervised ml methods wsd 
furthermore algorithm proven better properties applied new domains 
planned done directions ffl studying problem obtaining representative training corpus clarifying exactly meant representative 
