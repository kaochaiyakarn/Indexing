crawling infinite web levels ricardo baeza yates carlos castillo center web research dcc universidad de chile dcc uchile cl 
large amount publicly available web pages generated dynamically request contain links dynamically generated pages 
usually produces web sites create arbitrarily pages 
article probabilistic models browsing infinite web sites proposed studied 
models estimate deep crawler go download significant portion web site content visited 
proposed models validated real data page views web sites showing theory practice crawler needs download just levels clicks away start page reach pages users visit 
studies web refer publicly indexable portion excluding portion web called hidden web characterized pages normal users eventually access automated agents crawlers search engines 
certain pages indexable require special authorization 
dynamic pages generated request 
dynamic pages indexable parameters creating links 
case typical product catalogs web stores links navigate catalog user having pose query 
amount information web certainly finite dynamic page leads dynamic page number pages potentially infinite 
take example dynamic page implements calendar click month point data items calendar humans reasonably sure find events scheduled years advance crawler 
examples crawler traps involve loops near duplicates detected want avoid downloading 
deal problem capturing relevant portion dynamically generated content known parameters avoiding download pages 
interested knowing user see dynamically generated page 
probability low search engine retrieve page 
clearly web site point view answer search engine point view answer 
case results relevant 
answer case user point view clear priori depend result 
main contributions models propose random surfing inside web site number pages unbounded 
take tree induced web graph site study levels 
analyze models focusing question deep users go inside web site validate models actual data web sites link analysis pagerank 
results help decide crawler evaluate important non crawled pages 
section outlines prior topic rest organized follows section models random surfing dynamic web sites analyzed section models compared actual data access log web sites 
section concludes final remarks recommendations practical web crawler implementations 
previous crawlers important component web search engines internals kept business secrets 
descriptions web crawlers include mercator wire parallel crawler general crawler architecture described chakrabarti 
models random surfers studied diligenti page ranking pagerank algorithm sampling web 
studies web crawling focused crawling policies capture high quality pages keep search engine copy web date 
link analysis web currently active research topic concise summary techniques see survey henzinger 
log file analysis number restrictions arising implementation specially caching proxies noted haigh 
caching implies re visiting page recorded re visiting pages common action account activity users measuring directly browser 
proxies implies users accessing web site ip address 
process log file data careful data preparation done including detection sessions automated agents 
visits web site modeled sequence decisions huberman obtain model number clicks follows zipf law 
levene proposed absorbing state represent user leaving web site analyzed lengths user sessions probability link increases session length 
lukose huberman analysis markov chain model user clicking web site focus designing algorithm automatic browsing topic liu 
random surfer models web site infinite number pages consider web site set pages host name user session finite sequence page views web site 
starting point user session need page located root directory server users may enter web site link internal page 
page depth page session shortest path start page pages seen session 
function web site structure perceived depth particular session 
session depth maximum depth page session 
random surfing model page state system hyperlink possible transition simpler model collapse multiple pages level single node shown left center 
web site graph collapsed sequential list 
fig 

left web site modeled tree 
center web site modeled sequence levels 
right representation different actions random surfer 
step walk surfer perform actions consider atomic go level action go back previous level action back stay level action stay go different previous level action prev go different higher level action fwd go start page action start jump outside web site action jump 
action jump add extra node exit signal user session closing browser going different web site shown right 
regarding web site leaving users option start page depth action start 
node exit single going link probability affect results nodes remove node exit change transitions going start level 
way understand process memory going back start page starting new session equivalent actions jump start indistinguishable terms resulting probability distribution nodes 
set atomic actions start jump back stay prev fwd 
probability action level action 
probabilities action action 
probability distribution level time vector 
exists limit call limt article study models probability advancing level constant levels 
purpose predict far real user go dynamically generated web site 
know crawler decide crawl just levels 
models analyze chosen simple intuitive possible sacrificing correctness 
seek just fitting distribution user clicks want understand explain user behavior terms simple operations 
model back level time model probability user advance deeper probability user go back level shown 
back stay start jump prev fwd fig 

model user go forward backward level time 
stable state characterized xi xi xi solution recurrence xi 
solution xi absorbing state framework means depth ensure certain proportion pages visited users 
impose normalization constraint geometric distribution xi cumulative probability levels 
xi model back level model user go back start page session probability shown 
back 
stay start jump prev fwd 
fig 

model user go forward level time go back level going start page starting new session 
stable state characterized xi xi xi geometric distribution xi cumulative probability levels 
xi note cumulative distribution obtained model back level parameter qa model back home parameter qb equivalent qa qb distribution session depths equal qb transformation parameter consider model charting fitting distributions 
model back previous level model user discover new level probability go back previous visited level probability decides go back previously seen level choose uniformly set visited levels including current shown 
stable state characterized xi xk xi xk back stay start jump prev fwd 
fig 

model user go forward level time go back previous levels uniform probability 
take solution form xi imposing normalization constraint yields xi cumulative probability levels 
xi comparison models terms cumulative probability visiting different levels models produce equivalent results transformation parameters 
plotting cumulative distributions models yields 
see models need crawler go past depth capture pages random surfer visit larger say crawler go depth capture amount page views 
cumulative probability level cumulative probability level fig 

cumulative probabilities models left right data user sessions web sites studied real user sessions different web sites spain italy chile including commercial educational non governmental organizations collection fit code type country recorded average root best error sessions page views entry model educational chile educational spain educational commercial chile commercial chile chile chile organization italy organization ob organization blog chile ob organization blog chile blog chile blog chile table 
characteristics studied web sites results fitting models 
number user sessions reflect relative traffic web sites data obtained different time periods 
root entry fraction sessions starting home page 
web logs sites collaborative forums play major role known blogs characteristics sample results fitting models data summarized table 
obtained access logs anonymous ip addresses web sites processed obtain user sessions considering session sequence get requests user agent minutes requests 
processed log files discard hits web applications mail content management systems respond logic page browsing usually accessible web crawlers 
expanded sessions missing pages referrer field requests considering frames multi frame page single page 
discarded sessions web robots known user agent fields accesses robots txt file discarded requests searching buffer overflows software bugs 
re visits recorded caching data log files overestimates depth users spent time 
shows cumulative distribution visits page depth web sites 
visits occur depth sessions include start page 
average session length pages case web logs sessions tend longer 
reasonable web postings short blog users view session 
fitted models data web sites shown table 
general curves produced model model better cumulative fraction visits ob ob level fig 

distribution visits level access logs web sites 
educational commercial non governmental organization ob organization line forum blog web log line forum 
approximation user sessions distribution produced model blogs 
approximation characterizing session depth error general lower 
studied empirical values distribution different actions different levels web site 
averaged distribution studied web sites different depths 
results shown table consider web sites blogs 
level observations start jump back stay prev fwd table 
average distribution different actions user sessions considering blogs 
transitions values greater shown bold face 
see table actions jump back important ones favor models back level model back start level 
note doesn vary lies 
increases grows reasonable user seen pages follow link 
jump higher back levels higher start 
half user sessions involve page web site 
start stay fwd common actions 
models empirical data lead characterization user sessions modeled random surfer advances level probability leaves web site probability general levels 
simplified model representing data web sites consider model back level time equivalent terms cumulative probability level change parameters 
empirical data observe users just leave web site browsing model clicks go back level model 
complex model derived empirical data particularly considers depends 
considered purposes related web crawling simple model 
model appears better blogs 
similar study focused access logs blogs reasonable thing blogs represent growing portion line pages 
cases models data show evidence distribution visits strongly biased levels web site 
distribution visits closer clicks away entry page web sites 
blogs observed deeper user sessions visits clicks away entry page 
theory internal pages starting points concluded web crawlers download entire web sites 
practice case consider physical page depth directory hierarchy web site observe distribution surfing entry points level rapidly decreases number pages crawl finite shown left 
link analysis specifically pagerank provides evidence 
asked fraction total pagerank score captured pages levels web sites 
answer crawled large portion web cl obtaining pages april seed pages web sites 
right shows cumulative pagerank score sample 
levels frequency entry pages web site pages level directory depth cumulative pagerank pagerank level links structure fig 

left fraction different web pages seen depth fraction entry pages depth studied web sites considering directory structure 
right cumulative pagerank page levels large sample web 
capture best pages 
note levels obtained terms global web structure considering internal external links user sessions study najork wiener 
models observations search engine expect area 
instance search engine crawler performs breadth crawling measure ratio new urls web site adding queue vs seen urls able infer deep crawl specific web site 
article provides framework kind adaptivity 
interesting enhancement models shown consider contents pages detect duplicates near duplicates 
model downloading duplicate page equivalent going back level visited page time 
detailed analysis consider distribution terms web pages link text user browses web site 
amount line content people organizations business willing publish grows web sites built web pages dynamically generated pages ignored search engines 
aim generate guidelines crawl new practically infinite web sites 

raghavan garcia molina crawling hidden web 
proceedings seventh international conference large databases vldb rome italy morgan kaufmann 
heydon najork mercator scalable extensible web crawler 
world wide web conference 
burke guided crawling personal digital libraries 
proceedings acm ieee cs joint conference digital libraries virginia 
baeza yates castillo balancing volume quality freshness web crawling 
soft computing systems design management applications santiago chile ios press amsterdam 
cho garcia molina parallel crawlers 
proceedings eleventh international conference world wide web honolulu hawaii usa acm press 
chakrabarti mining web 
morgan kaufmann publishers 
diligenti gori unified framework web page scoring systems 
ieee transactions knowledge data engineering 
page brin motwani winograd pagerank citation algorithm bringing order web 
proceedings seventh conference world wide web brisbane australia 
henzinger heydon mitzenmacher najork near uniform url sampling 
proceedings ninth conference world wide web amsterdam netherlands elsevier science 
najork wiener breadth crawling yields high quality pages 
proceedings tenth conference world wide web hong kong elsevier science 
cho garcia molina synchronizing database improve freshness 
proceedings acm international conference management data sigmod dallas texas usa 
henzinger hyperlink analysis web 
ieee internet computing 
haigh measuring web site usage log file analysis 
network notes 
tauscher greenberg revisitation patterns world wide web navigation 
proceedings conference human factors computing systems chi 

advanced data preprocessing web usage mining 
ieee intelligent systems 
tan kumar discovery web robots session navigational patterns 
data mining knowledge discovery 
huberman pirolli pitkow lukose strong regularities world wide web surfing 
science 
adar huberman economics web surfing 
poster proceedings ninth conference world wide web amsterdam netherlands 
levene borges zipf law web surfers 
knowledge information systems 
lukose huberman surfing real option 
proceedings international conference information computation economies acm press 
liu zhang yang characterizing web usage regularities information foraging agents 
ieee transactions knowledge data engineering 
cooley mobasher srivastava data preparation mining world wide web browsing patterns 
knowledge information systems 
catledge pitkow characterizing browsing behaviors world wide web 
computer networks isdn systems educ 
educ 
educ 
actual data model err model err actual data model err model err actual data model err model err com 
com 
ref 
actual data model err model err actual data model err model err actual data model err model err ref 
org 
org 
actual data model err model err actual data model err model err org blog org blog actual data model err model err actual data model err model err actual data model err model err blog blog actual data model err model err actual data model err model err fig 

fit models actual data terms cumulative page views level 
model back start level smaller errors web sites blogs 
asymptotic standard error fit model worst case consistently cases 
note zoomed upper portion graph starting cumulative page views 
