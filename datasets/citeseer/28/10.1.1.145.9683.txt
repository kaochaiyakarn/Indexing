learning dimensionality hidden variables gal nir friedman school computer science engineering hebrew university nir cs huji ac il serious problem learning probabilistic models presence hidden variables 
variables observed interact observed variables 
detecting hidden variables poses problems determining relations variables model determining number states hidden variable 
address problem context bayesian networks 
describe approach utilizes score agglomerative state clustering 
show approach allows efficiently evaluate models range cardinalities hidden variable 
show extend procedure deal multiple interacting hidden variables 
demonstrate effectiveness approach evaluating synthetic real life data 
show approach learns models hidden variables generalize better better structure previous approaches 
decade great deal research focused problem learning bayesian networks data :10.1.1.112.8434
important issue existence hidden latent variables observed interact observed variables 
hidden variables play important role improving quality learned model understanding nature interactions domain 
crucial problem question determine dimensionality hidden variable 
issue relevant learning fixed structure assessed expert cases learning algorithm attempts introduce new variables 
number states hidden variable significant effect performance model complexity 
example demonstrates common phenomenon states parent variable merged children may longer conditionally independent consequence complicated networks edges children needed describe domain 
phenomenon pronounced variable parents 
illustration change network results merging states parent variable 
child variables longer separated ancestors additional edges needed 
see correct determination cardinality hidden variable affect complexity learned network turn important ramifications robustness learned parameters complexity inference 
propose agglomerative score approach determining cardinality hidden variables 
approach starts maximal number states possible merges states greedy fashion 
iteration algorithm maintains training instance hard assignment hidden variable 
score data complete data scoring functions orders magnitude efficient standard em scores incomplete data 
procedure progresses choosing states merger lead best improvement decrease score 
steps repeated states merged state 
scores intermediate stages choose cardinality hidden variable 
show networks learned intermediate stages initial starting points em runs fine tune parameters 
move consider networks multiple hidden variables 
show combine multiple invocations single variable procedure learn interactions hidden variable 
combine method structural detection hidden variables show leads learning better performing models test real life data 
background learning bayesian networks consider finite set discrete random variables variable may take states finite set denoted bayesian network annotated directed acyclic graph encodes joint probability distribution nodes graph correspond random variables node annotated conditional probability distribution cpd represents denotes parents bayesian network specifies unique joint probability distribution graph represents conditional independence properties distribution 
markov independencies variable independent parents implication markov independencies variable interacts directly markov blanket 
blanket includes parents children spouses additional parents children 
denote variables markov blanket interested learning bayesian networks examples 
assume training set unknown distribution 
want find network best matches structure network maximum likelihood approach estimate parameters 
challenging problem learn structure network 
common approach problem introduce scoring function evaluates candidate networks respect training data search best network score 
commonly scoring function learn bayesian networks bayesian scoring bde metric denote :10.1.1.156.9918
scoring metric uses balance likelihood gain learned model complexity network structure representation 
important characteristic score function data complete training instance assigns values variables score decomposable 
precisely score rewritten sum instances sampled contribution variable score total network score depends states training instances 
assuming 
asr ut giv flk hyper parameters prior distributions parameterizations 
terms flk counts number occurrences fyg wh terms event data 
vector counts called sufficient statistic vector family specify scoring function structure learning task reduces problem searching combinatorial space structures structure maximizes score 
standard approach local search procedure greedy hill climbing changes edge time 
learning problem different training data incomplete states training data missing learn network contains hidden variables observed 
situation task computationally conceptually harder 
order learn parameters network structure expectation maximization em algorithm search local maximum likelihood maximum posteriori parameter assignment 
presence incomplete data scoring candidate structures complex 
efficiently evaluate marginal likelihood need resort approximations 
commonly approximation cheeseman stutz cs score combines likelihoods parameters em estimate penalty term associated structure 
structural em algorithm friedman extends idea em realm structure search 
roughly speaking algorithm uses step part structure search 
current model structure parameters computing expected sufficient statistics candidate structures 
candidate structures scored expected sufficient statistics 
search algorithm moves new candidate structure 
apply em new structure get desired expected sufficient statistics score new candidate structures 
algorithm converges local maximum 
search space algorithm contains convergence points care taken choosing initialization point 
detecting hidden variables bayesian networks mentioned interested cases hidden variable dimensionality unknown constructing new hidden variables 
purpose method detecting hidden variables suggested 
briefly review method 
general idea method detect hidden variables finding structural signatures bayesian network learned observed variables 
show signature formed removing hidden variable clique children reconstructing network data edges 
searching perfect cliques algorithm searches approximate cliques relaxation number neighbors called semi cliques 
semi clique set variables ut variable edge half variables set 
semi clique new hidden variable proposed 
evaluate variable algorithm constructs network variable new variable parent variables addition edges variables removed 
algorithm applies constrained version structural em adapt structure estimate parameters new network 
score learned network compared score original 
change score reflects utility introducing hidden variables 
results show algorithm successful introducing hidden variables improves performance test data 
choosing cardinality hidden variable address problem 
training data samples network structure variable additional need determine cardinality leads best scoring network 
straightforward way solve problem follows examine possible cardinalities certain point 
cardinality apply em algorithm learn parameters network ing contain states 
em get stuck local maxima perform em runs different random starting points 
parameters network approximate score network states say cheeseman stutz approximation 
process return cardinality received best score 
approach common probabilistic clustering algorithms 
central problem approach exhaustiveness 
em algorithm time consuming requires inference bayesian network 
simple naive bayes networks clustering cost prohibitive 
network structures cost multiple em runs high 
strive find method finds best scoring cardinality approximation significantly faster 
suggest approach works hard assignments states hidden variables 
approach motivated agglomerative clustering methods bayesian model merging techniques hmm literature 
general outline approach follows 
iteration maintain hard assignment training data 
represent assignment mapping set assignment state holds instance 
initialize algorithm variable states describe details 
evaluate score network respect dataset trace agglomeration process simple synthetic example 
sampled instances alarm network hid observations variable data 
attempted reconstruct cardinality 
leaf tree annotated values variables markov blanket 
nodes correspond states result merging operations 
numbered order merging operations annotated change score incurred merge operation 
note stage merge chosen produces largest increase smallest decrease score 
nodes correspond final cardinality chosen 
completed merge states form variable smaller cardinality 
leads new assignment function 
reevaluate network respect new assignment 
steps repeated single state 
return number states received highest score 
shows concrete example tree built agglomeration process 
consider detail steps process 
start initialization point algorithm setting initial variable states recall markov blanket separates variables 
implies instances state identical perspective 
largest number states relevant data sets number distinct assignments initialize data 
state assignment 
example assignments possible observed data 
augment training data assignments assignment state instance set state consistent markov blanket assignment instance contain assign set need evaluate usefulness 
assigns specific state instance completes training data apply standard complete data score function bde completed data set 
recall data complete evaluated efficiently closed form formula 
score depends sufficient statistics vectors 
vector counts number occurrences assignment variable parents 
denote sufficient statistics correspond family node parents denote evaluate need consider families child iteration algorithm choose merge states resulting set states best 
suppose states want merge 
means instances assigned new state say formally define new function evaluate compare score score difference improvement loss merge operation 
note merging states need modify training data 
simply apply merging operation sufficient statistics correspond children 
set assignment parents similarly compute sufficient statistics children families 
determine best merge operation algorithm considers pairs states potentially lead cubic running time iteration require quadratic amount computation 
suitable choice prior show bde score mdl score locally decomposable 
precise suppose result merging states define score score score locally decomposable depend states compute change score result merging need recompute successive iterations 
closer look properties score reveals behavior expect see applying procedure 
recall scoring function trades likelihood data complexity model 
consider plots score vs cardinality observe effects come play 

merging states number parameters network reduced 
gives positive contribution score complexity model reduced 

fewer states probability state parents larger 
likelihood term score agglomeration results em agglomeration starting point multiple starting point em number states typical behavior score function number states agglomeration run 
bde score agglomeration method cs score em run starts agglomeration output cs score best em run multiple starting points 
results shown recovering variable alarm network 
associated improves merge operation 

states provide better prediction children 
fact initialization point children deterministically determined state state joint assignment markov blanket 
number states reduced predictions children stochastic likelihood reduced 
merge likelihood children decrease 
suggests score increase rapidly due contribution effects slow increase due steady contribution effect decrease approach single state plunge due third effect 
shows example graph get track score iterations algorithm 
shows relations score algorithm assigns cardinality assigned standard traditional method runs em cardinality 
section analyze detail methods 
deciding cardinality hidden variables previous section examined problem deciding cardinality single hidden variable 
happens network contains hidden variables 
start noting cases decouple problem variable hidden separated hidden variables observed variables learn independent rest 
precisely consists observable variables need worry interactions hidden variables 
hidden variables interact problem complex 
decision cardinality hidden variable effect decisions hidden variables 
need consider joint decision interacting variables 
standard em approach mentioned section problematic cardinality space grows exponentially number hidden variables 
describe simple heuristic approach attempts approximate cardinality assignment multiple variables 
ideas motivated similar approach multi variable discretization :10.1.1.29.4992
basic idea apply agglomerative procedure previous section round robin fashion 
iteration fix number states state assignment instances hidden variables 
apply agglomerative algorithm respect hidden variable 
iteration select variable repeat procedure 
easy check reexamine hidden variable variables markov changed 
continue procedure hidden variable changed cardinality state assignment 
crucial issue initialization procedure 
suggest start network hidden variables state 
initial rounds procedure hidden variable trained respect observable neighbors 
iterations interactions hidden variables start play role 
easy see iteration procedure improve score completed data set specified state assignment functions hidden variables 
immediately follows converge 
experimental results evaluation set evaluate applicability approach various learning tasks 
start evaluating algorithm determines variable cardinality synthetic datasets know cardinality variable hid 
sampled instances alarm network manually hid variable dataset 
gave algorithm original network evaluated ability reconstruct variable cardinality 
shows typical behavior vs number states 
repeated procedure variables alarm network 
consider variables leafs neighbors 
training sets instances predictions cardinality broken follows number variables correct cardinality missing single state collapsed single state number instances deviations predicted cardinality agglomeration method true cardinality variables alarm network function number instances 
shown curves true cardinality collapse single states single missing state deviations rare 
variables agglomerative procedure recovered correct cardinality 
variables estimated cardinality state true cardinality 
variables estimated cardinality additional state 
examining network cpds suggest children variables stochastic states parents uniform probability 
initial steps agglomeration attempted model distribution lead sub optimal aggregate states phases agglomeration 
variables agglomerative procedure suggested complete collapse single state 
equivalent removing variable 
close look probabilities network shows variables little effect children redundant 
order confirm claim variables cardinality ran em multiple starting points find best scoring network 
variables best score achieved variable collapsed single state 
summarize variables got correct near perfect prediction cardinality 
variables characteristics data weak reach statistically significant results 
tested effect training set size decisions 
applied agglomeration method variable training sets different sizes 
shows deviation true cardinality function training set size 
see small sample original network learned agglomeration learned binary states performance agglomeration algorithm network interacting hidden variables 
comparison model learned agglomeration model learned binary values demonstrates importance determining cardinality hidden variables 
dashed light edges edges removed thin edges edges added sizes predictions variables perfect underestimates cardinality 
expected training set manifest rare assignments markov blanket variable states needed explain data 
compared approach standard method evaluating different cardinalities em 
compared variants em 
performed multiple em runs different random starting points 
second variant performed single em run starting parameters learn completed data agglomeration step 
compares scores assigned different cardinalities agglomerative approach em variants variable 
note methods case correct cardinality received highest score 
note em variants give similar scores 
suggests agglomerative approach finds useful starting points em 
terms running time em run cardinality example takes seconds 
agglomeration procedure takes little second initial states 
claim determining cardinality suffices run iterations em computationally cheaper 
test run em early stopping rule 
reduced running time em seconds run 
resulted worse estimates cardinality worse agglomerative method 
conclude significant time saved method set number states apply em fine tuning 
typical behavior observed similar comparisons hid variables alarm network 
wanted evaluate performance algorithm dealing multiple hidden variables 
constructed synthetic network shown hidden variables generated matching data set appropriate variables hidden 
true structure starting point applied agglomerative algorithm followed structural em 
strawman apply structural em binary values hidden variables 
flexibility structural em challenging structure network expect learning algorithm precise quickly deviate true structure 
results summarized states respectively visible nodes binary 
evident agglomeration method able effectively handle interacting hidden variable 
cardinality close original cardinality extra states introduces better explain stochastic relations look stochastic training data 
structure learned binary model emphasizes importance determining cardinality hidden variables suggested example 
terms log loss score test data model learned agglomeration superior original model better model learned binary values 
turn incorporation cardinality determining algorithm hidden variable discovery algorithm see section 
candidate network searches semi cliques offers candidate hidden variables 
applies method candidate network determine cardinality hidden variable 
allow structural em candidate network 
applied variables synthetic alarm network 
experimented real life data sets stock data dataset traces daily change major technology stocks years trading days 
states discretized categories change 
tb dataset records information tuberculosis patients san francisco county courtesy dr peter small stanford medical center 
data set contains demographic information gender age ethnic group medical information hiv status tb infection type test results 
news data set contains log loss bits instance original hr vent lung st agglomeration log loss performance test data find hidden algorithm agglomeration synthetic real life data 
base line performance original network input messages newsgroups :10.1.1.22.6286
represent message vector containing attribute newsgroup attributes word vocabulary 
removed common words sorted words frequency data set 
data set included group designator common words 
trained messages randomly selected total data set 
shows log loss performance networks test data 
base line original network learned hidden variable supplied input find hidden 
solid diamonds score network hidden variable agglomeration hidden variable arbitrarily set states squares network hidden variable agglomeration method applied 
see cases network suggested hidden variable outperformed original network 
network learned agglomeration performed better learned network agglomeration excluding cases agglomeration suggested exactly states equivalent agglomeration run 
interesting look structures procedure 
interesting model tb patient dataset 
state hidden variable captures highly dominant segments population older hiv negative foreign born younger hiv positive born blacks 
hidden variable children distinguished aggregated subpopulations hiv result variable ancestor 
noted possible additional states hidden states separated populations 
compares model learned algorithm model news learned integration agglomerative method 
model perform better test data see define separate populations born hiv negative born years higher probability hiv foreign born probability hiv foreign born hiv negative 
discussion proposed agglomerative score approach determining cardinality hidden variables 
compared method exhaustive approach setting cardinality multiple em runs showed generating competing learning models 
importance plausibility agglomeration method pre processing step learning algorithm important consequence saving significant computational effort 
algorithm proved robust number instances training set 
able deal effectively interacting hidden variables 
evaluated method part hidden variable detection algorithm synthetic real life data showed improved performance appealing structures 
works related approach 
authors examined operations value abstraction refinement bayesian networks 
works concerned ramifications operations inference decision making 
decisions cardinality appear context discretization 
data observable discretized variable modeled adding hidden variable 
example friedman goldszmidt incorporated discretization process learning bayesian networks :10.1.1.29.4992
approach decomposable score trade likelihood gain complexity penalty resulting particular discretization 
approach discretizing multiple interacting variables similar 
context learning hidden variables relevant works stolcke omohundro 
works learn hidden markov models probabilistic grammar performing bottom 
similar method start spanning possible states iteratively merging states information vs complexity measures 
viewed generalization applying general bayesian networks combining hidden variable detection algorithm 
structural em algorithm friedman followed aimed learning non trivial structures hidden variables data 
incorporation hidden variables essential improving prediction new examples gain understanding underlying interactions domain 
plan continue research project xray hidden hidden ethnic pob clustered xray disease site clustered ethnic disease site gender pob agglomeration gender agglomeration improvement structure tb network due incorporation cardinality determining algorithm 
hidden variables states captures distinct populations improves predictive ability model 
directions 
intend explore additional methods detecting dimensionality hidden variables estimating information theoretic measures situations similar 
order deal effectively sparse data domains structural signatures weak methods discovery hidden variable need developed 
direction extend methods learning hidden structure expressive models probabilistic relational models :10.1.1.101.3165
noam nachman comments earlier drafts 
supported part israel science foundation number 
nir friedman supported alon fellowship harry abe sherman senior computer science 
experiments reported run equipment funded isf basic equipment 
wilson gill salamon rand small 
comparative genomics bcg genome dna microarray 
science 
beinlich suermondt chavez cooper 
alarm monitoring system euro 
conf 
ai med 

boyen friedman koller 
learning structure complex dynamic systems uai 
chang fung 
refinement coarsening bayesian networks 
uai pp 


cheeseman kelly self stutz taylor freeman 
autoclass bayesian classification system 
ml 

chickering heckerman 
efficient approximations marginal likelihood bayesian networks hidden variables 
mach 
learning 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
royal stat 
soc 
duda hart 
pattern classification scene analysis 

friedman koller 
discovering hidden variables structure approach 
nips 
friedman 
bayesian structural em algorithm 
uai 

friedman goldszmidt :10.1.1.29.4992
discretization continuous attributes learning bayesian networks 
ml pp 


getoor friedman koller pfeffer :10.1.1.101.3165
learning probabilistic relational models 
ijcai 

heckerman :10.1.1.112.8434
tutorial learning bayesian networks 
learning graphical models 
heckerman geiger chickering :10.1.1.156.9918
learning bayesian networks combination knowledge statistical data 
mach 
learning 
lang :10.1.1.22.6286
learning filter netnews 
ml pp 

lauritzen 
em algorithm graphical association models missing data 
comp 
stat 
data ana 
horvitz 
dynamic construction refinement utility categorization models 
ieee trans 
sys 
man cyb 
horvitz 
reasoning value decision model refinement methods application 
uai pp 


stolcke omohundro 
hidden markov model induction bayesian model merging 
nips pages 

stolcke omohundro 
inducing probabilistic grammars bayesian model merging 
inter 
conf 
grammatical inference 
wellman 
liu 
state space abstraction anytime evaluation probabilistic networks 
uai pp 

