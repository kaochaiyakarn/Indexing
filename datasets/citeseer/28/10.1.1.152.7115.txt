birch efficient data clustering method large databases tian zhang raghu ramakrishnan miron livny sciences dept computer sciences dept computer sciences dept 
wisconsin 
wisconsin 
wisconsin zhang cs 
wise edu cs wise edu cs 
wise finding useful patterns large datasets attracted considerable interest widely st problems area identification clusters populated regions multi dir 
prior adequately address problem large datasets minimization costs 
presents data clustering method named balanced iterative reducing clustering hierarchies demonstrates especially suitable large databases 
birch incrementally clusters incoming multi dimensional metric data points try produce best quality clustering available resources available memory time constraints 
birch typically find clustering single scan data improve quality scans 
birch clustering algorithm database area handle noise data points part underlying pattern effectively 
evaluate birch time space efficiency data input order sensitivity clustering quality experiments 
performance comparisons bir versus clara ns clustering method proposed large datasets ow birch consistently superior 
examine data clustering particular kind mining problem 
large set data points data spare usually uniformly occupied 
data clustering identifies sparse crowded places discovers distribution patterns dataset 
derived clusters visualized efficiently effectively original dataset lee 
research supported nsf iri nasa ec 
permission copy part personal classroom fee provided copies distributed pro tt commercial advantage notice title publication date appear notice copying permission acm copy republish post servers redistribute lists requires prior specific permission fee 
sigmod montreal canada iq acm generally types attributes involved data clustered 
ln consider metric attributes statistics literature clustering prol formalized follows clusters dataset measurement average tn clusters rue asked find dataset rrl value measurement 
kr optimization problem 
due abundance local minima typically way find global minimal solution trying possible partitions 
adopt problem definition statistics additional database oriented constraint amount memory available smaller data set sw ule ze required 
related point desirable lre able take account amount user willing wait results clustering algorithm 
clustering method named birch demonstrate especially suitable large databases 
cost linear size dataset scan dataset yields clustering additional passes optionally improve quality 
evaluating birch time space data order sensitivity clustering quality comparing existing algorithms experiments argue birch best available clustering method large databases 
architecture offers opportunities parallelism interactive dynamic performance tuning knowledge gained course execution 
birch clustering informally attribute values satisfy requirements space self identity triangular inequality exists distance definition xi xz xi 
proposed area addresses outl rs intuitively data points regarded proposes plausible solution 
outline rest organized follows 
sec 
surveys relat ed summarizes birch sec 
presents background material 
ser 
introduces concepts clustering feature au tree central birch 
details bill algorithm described sec 
preliminary performance study birch sec 
directions fw ture sec 
summary relevant research data clustering studied statistics lee mur machine learning cks fis fis lel database nh ix communities different methods different emphases previous approaches probability approaches machine learning statistics adequately consider case large fit main memory 
particular problem viewed terms limited resources memory mu smaller size dataset clustering accurately possible keeping costs low 
probability approaches typically fish kst assumption distributions separate attributes statistically independent reality far true 
correlation attributes exists kind correlation exactly looking 
probability representations updating storing clusters expensive attributes large number values complexities dependent number attributes number values attribute 
related problem fis probability tree identify clusters height balanced input data may cause performance distance assume data points advance scanned 
totally partially ignore fact points respect clustering purpose dat points close nse considered collectively individually global se methods granularity data points 
clustering decision inspect data currently existing clusters matter close far away measurements require scanning data points currently existing clusters 
linear time scalability stal le quality example ee approximately iin dh ways partitioning set data points subsets 
practice find global minimum infeasible iv extremely small 
dh kr starts initial partition tries possible moving swapping data points group see moving swapping improves value measurement function find local minimum hut local minimum initially selected partition worst case complexity exponential 
hc dh kr mur try find clusters keeps merging closest pair splitting farthest pair objects form clusters reasonable distance measurement best time complexity practical hc algorithm 
scj unable scale large clustering recognized useful spatial data mining method 
nh presents ban search proposes outperforms traditional clustering algorithms statistics 
clarans represented centrally loc ated data point cluster clustering process formalized searching graph ii partition set ii ls nodes neighbors differ medoid clarans starts randomly node 
current node checks om number neighbors randomly better neighbor moves neighbor records current node restarts new randomly selected node search local stops number called returns best ya ra suffers ch shove io method wrt 
efficiency addition may find real local minimum due searching trimming controlled 
aj ek propose focusing techniques trees improve clara ability deal data objects may reside clustering sample aset drawn tree data page focusing relevant data points clist ance quality updates experiments show time improved small loss quality cent birch important contribution formulation clustering problem way appropriate large making time memory constraints explicit 
addition birch advantages previous distance approaches 
birch local opposed global clustering decision scanning data points currently existing clusters 
uses reflect natural closeness points time incrementally maintained clustering process 
birch exploits observation data space usually uniformly occupied data point equally important clustering purposes 
dense region points treated collectively single cluster 
points sparse regions treated removed optionally 
full available memory derive finest possible subclusters ensure accuracy minimizing costs ensure efficiency 
clustering reducing process organized characterized memory highly occupied tree structure 
due features running time linearly scalable 
optional phase birch incremental method require advance scans 
background assume readers familiar terminology vector spaces defining centroid radius diameter 
dimensional data points cluster xi 
centroid xo radius diameter cluster defined 
jx mt average distance member points centroid 
average pairwise distance cluster 
alternative measures tightness cluster centroid 
clusters define alternative distances measuring closeness 
centroids clusters centroid euclidian distance centroid manhattan distance clusters defined xi xb ni dimensional data points cluster 
data points cluster nl average er clust er average cluster distance variance increase distance clusters defined merged cluster 
sake clarity treat properties single cluster properties clusters state separately 
optionally preprocess data weighting shifting different dimensions affecting relative placement 
clustering feature cf tree concepts clustering feature cf tree core birch incremental clustering 
clustering feature triple summarizing information maintain cluster 
definition dimensional data points cluster ii 
clustering feature cf vector cluster defined triple cf number data points cluster linear sum data points square sum data points iz 
theorem cf additivity theorem ne cf nl cf cf vectors dw oint clusters 
cf vector cluster formed merging disjoint clusters cf cf nl proof consists straightforward algebra 
cf definition additivity theorem know cf vectors clusters stored calculated incrementally accurately clusters merged 
easy prove cf vectors clusters corresponding xo usual quality rnet rics weighted total average diameter clusters calculated easily 
think cluster set data points cf vector stored summary 
cf summary efficient stores data points cluster hut accurate sufficient calculating measurements need making clustering decisions birch 
cf tree cf tree height balanced tree ers branching factor threshold nonleaf node contains entries form cfi pointer th child node cf cf er represented child 
nonleaf node represents cluster subclusters represented entries 
leaf node contains entries form cfi 
addition leaf node pointers prev chain leaf nodes efficient scans 
leaf node represents cluster subclusters represented entries 
entries leaf node satisfy requirement respect value tht tree size function larger tree require node pa size dimension data space sizes leaf nonleaf entries known determined varied performance tuning 
cf tree built dynamically new data inserted 
guide new insertion correct clustering purposes just tree usd guide new insertion il tu position sorting purposes 
cf tree compact representation dat aset entry leaf node single data oint hut subcluster absorbs data points diameter radius specific threshold 
insertion cf tree algorithm inserting entry cf tree 
entry ent proceeds appropriate leaf starting root recursively descends cf tree choosing tile closest child node chosen distance metric defined sec 

leaf reaches leaf node leaf entry say tests 
ent threshold 
cf vector li ul dated reflect new entry ent leaf 
space leaf new entry done sp leaf node 
splitting done choosing farthest pair entries seeds redistributing remaining entries closest criteria 
th mth leaf inserting ent mt leaf update cf information tl cluster ent ust satisfy threshold condition 
note cf vector new cal cf vectors 
ent 
nonleaf entry path leaf 
absence split simply involves adding cf vectors reflect addition ent 
leaf split requires insert new nonleaf entry parent node describe newly created leaf parent space entry higher levels need update cf vectors reflect addition ent 
general may split parent root root split tree height increases 
tt splits caused hy page size independent clustering properties data 
presence skewed data input order affect clustering quality space utilization 
simple additional merging step helps ameliorate problems suppose leaf split propagation split stops nonleaf nj accommodate additional entry resulting split 
scan node find closest entries 
pair corresponding split try merge corresponding child nodes 
entries child nodes page hold split merging result 
case seed attracts merged entries fill page just put rest entries seed 
summary entries fit single page free node space create entry space node nj increasing space utilization postponing splits improve distribution entries closest children 
node hold limited number entries clue size correspond natural cluster 
occasionally subclusters cluster split nodes 
depending order data input degree skew possible ers cluster kept node 
infrequent anomalies caused page size remedied global semi glo algorithm arranges leaf entries nodes phase discussed sec 
undesirable artifact data point inserted twice hut different times copies entered distinct leaf entries 
word occasionally skewed input order point enter leaf entry entered 
problem addressed refinement passes data phase discussed ser 
birch clustering algorithm fig 
presents overview birch 
main task phase scan data build initial inmemory cf tree amount memory data initial cf tree phase optional condense tit 
desirable rang 
sm cf tree tree better base global clu clusters birch recycling space disk 
cf tree tries reflect clustering information dataset fine possible memory limit 
crowded data points grouped fine subclusters sparse data points removed outliers phase creates inmemory summary data 
details phase discussed sec 

phase subsequent computations phases fast operations needed problem clustering original data reduced smaller problem clustering subclusters leaf entries 
accurate lot outliers eliminated remaining data reflected finest granularity achieved available 
order sensitive leaf entries initial tree form input order containing better data locality compared arbitrary original data input order 
phase optional 
observed existing global semi global clustering methods applied phase different input size ranges perform terms speed quality 
potentially gap size phase results input range phase 
phase serves cushion bridges gap similar phase scans leaf entries initial cf tree rebuild smaller cf tree removing outliers grouping crowded subclusters larger ones 
undesirable effect skewed input order splitting triggered page size sec 
causes actual clustering patterns data 
remedied phase global semi global algorithm cluster leaf entries 
observe existing clustering algorithms set data points readily adapted set subclusters described cf vector 
example cf vectors known naively calculating centroid representative subcluster treat subcluster single point existing algorithm modification little sophisticated treat subcluster data points cent repeating times modify existing algorithm slightly take counting information account general accurate apply existing algorithm directly subclusters information cf vectors usually sufficient calculating distance quality metrics 
adapted agglomerative hierarchical clustering algorithm applying directly subclusters represented cf vectors 
uses accurate distance metric calculated cf vectors clustering complexity lv 
provides flexibility allowing user specify desired number clusters desired diameter radius threshold clusters 
phase obtain set clusters captures major distribution pattern data minor localized inaccuracies exist rare problem mentioned sec 
fact phase applied coarse summary data 
phase optional entails cost additional passes data correct inaccuracies refine clusters 
note point original data scanned tree outlier information may scanned multiple times 
phase uses centroids clusters produced ly phase seeds redistributes data points closest seed obtain set new clusters 
allow points belonging cluster ensures copies data point go cluster 
phase extended additional passes desired user proved converge minimum 
bonus pass data point labeled cluster belongs wish identify data points cluster 
phase provides option discarding outliers 
point far closest seed treated outlier included result 
phase revisited fig 
shows details phase 
starts initial threshold value scans data inserts points tree 
runs memory finishes scanning data increases threshold value rebuilds new smaller cf tree re inserting leaf entries old tree 
old leaf entries re inserted scanning data insertion new tree resumed point interrupted 
proceeds scanning data insert mem data 
result 
increase 
lf new tree tl 
leaf entry tl potential disk space write disk 
tl tz disk space re tl re tl phase old tree new tree reducibility tree assume cf tree threshold 
height size er nodes want leaf entries tz rebuild cf tree 
threshold size tt larger 
ifi rebuilding algorithm consequent theorem 
node cf tree entries labeled contiguously nk number entries node path entry root level leaf node level uniquely il ll label th level entry path 
naturally path tl zl re pat ifi ij 
obvious leaf node corresponds path uniquely path leaf node interchangeably 
algorithm illustrated fig 

natural path order defined scans frees old tree path path time creates new tree path hy path 
new tree starts ol starts leftmost path old tree 
old thr ig tn new tree nodes added new tree exactly old tree chance new tree larger old tree 
insert leaf tn thy neti tree new threshold leaf entry tested new tree see fit newc pat top closest criteria new tree 
new inserted space left available inserted new ath creating new node 
spare iit ne ur path leaf entries ath processed un needed nodes path freed 
nodes newc empty leaf entries originally correspond path pushed forward 
case empty nodes freed 

set thr pdh old tf ther repeat stq 
rebuilding steps old leaf entries reinserted new tree larger old tree 
nodes corresponding path new need exist simultaneously maximal extra space needed tree pages 
hy increasing threshold rebuild smaller cf tree limited extra memory 
theorem theorem cf ti rr threshold oft ext pages ory oft 
threshold values choice threshold value greatly reduce number rebuilds 
initial threshold value increased dynamically adjust tc low 
initial high obtain detailed cf tree feasible available memory 
set conservatively 
bir sets zero default knowledgeable user change eit er absorbed existing leaf entry created leaf entry splitting 
suppose turns lx small subsequently run memory nt data points leaf entries hem formed satisfying threshold condition wrt 
fi 
portion data scanned tree built far need estimate threshold value estimation full solution scope 

try choose min 
known choose proportion data seen far 

want increase threshold measure 
distinct notions volume estimating threshold average volume rd average radius root cluster cf tree space 
intuitively measure space portion oft data seen far footprint seen data 
second notion volume packed defined vp number leaf entries tt maximal volume leaf entry 
intuitively measure actual volume occupied leaf clusters 
essentially run memory fixed amount memory approximate vp ti assumption grows number data points ni 
maintaining number points ni estimate ri squares linear regression 
define factor maz heuristic measure data footprint growing 
max motivated observation large datasets footprint constant quite quickly input order skewed 
similarly making assumption vt grows linearly ni estimate ti squares linear regression 

traverse path root leaf cf tree going child points greedy attempt find crowded leaf node 
calculate distance nin entries leaf 
want build condensed tree reasonable increase threshold value zn entries merged 

multiplied ti value obtained linear regression expansion factor follows tt 
ensure threshold value grows monotonically case ti obtained choose tt 

equivalent assuming aii iar points uniformly distributed dimensional sphere really just crude ti iti ll rarely called 
outlier handling option optionally bytes disk space handling leaf entries low density judged unimportant wrt 
ering pattern 
rebuild cf tree re inserting old leaf entries size new tree reduce ways 
increase threshold value allowing leaf entry points 
second treat leaf entries potential outliers write disk 
old leaf entry considered potential outlier far fewer data points average 
far fewer course heuristics 
periodically disk space may run potential outliers scanned see current tree causing tree grow size 
increase threshold value change distribution due new iat read potential outlier written mean potential outlier longer qualities outlier 
data potential outliers left disk space scanned verify outliers 
potential outlier ran absorbed chance real outlier removed 
note entire cycle insufficient memory triggering rebuilding tree insufficient disk spare triggering re absorbing outliers repeated times scanned 
effort considered lit ion cost scanning data order assess ost phase accurately 
delay split option run main memory may case data points fit current cf tree changing threshold 
data points read may require sl lit node tree simple idea writ data points disk manner similar outliers written proceed reading data run disk space 
advantage approach general data points fit tree rebuild 
performance studies complexity analysis discuss experiments conducted synthetic real dataset analysis analyze cpu cost phase 
maximal size tree 
insert point need follow path root leaf touching logb nodes 
node examine entries looking closest cost entry proportional dimension cost inserting data points logb 
case rebuild tree es cf entry size 
leaf entries re insert cost leaf entries logb 
number times re tree depends threshold heuristics 
currently logz value arises fact estimate farther twice cm rent size number data points loaded memory threshold 
total cpu cost phase logb log logb 
analysis phase cpu cost similar omitted 
scan data phase phase 
outlier handling delay split options cost associated writing outlier entries disk reading back rebuilt 
considering amount disk available outlier handling delay split log re builds cost phase significantly different cost reading dataset 
analysis pessimistic light experimental results cost phases scale linearly phase 
input phase bounded cpu cost phase hy constant depends maximum input size global algorithm chosen phase 
phase scans puts data point proper cluster time taken proportional iv 
newest nearest neighbor techniques improved linear wrt 
synthetic dataset generator study sensitivity birch characteristics wide range input datasets collection synthetic datasets generated generator developed 
data generation controlled set parameters summarized table 
dataset consists clusters data points 
cluster characterized number data points radius center 
range nk range rl rh 
placed clusters cover range values note wl en ll tl points fixed rl radius fixed eter values 

number clusters 
nt lower 
lh higher 
lower 
table data generation parameters values ranges experimented dimension 
refer ranges overview dataset 
location center cluster determined pattern parameter 
patterns random currently supported generator 
gnd pattern cluster centers placed grid 
distance centers neighboring clusters row column controlled kg set 
leads overview kj dimensions 
pattern places cluster centers curve sine function 
clusters divided groups different cycle sine function 
location center cluster ni location sine ni 
overview sine dataset nc mk directions respectively 
random pattern places cluster centers randomly 
overview dataset dimensions locations centers randomly distributed range 
characteristics cluster determined data points cluster generated independent normal distribution mean center variance dimension 
note due properties normal distribution maximum distance point cluster center unbounded 
words point may arbitrarily far belonging cluster 
data point belongs cluster may closer center cluster center refer points outsiders 
addition clustered data points noise form data points uniformly distributed overview dataset added dataset 
parameter rn controls percentage data points dataset considered noise 
placement data points dataset controlled order parameter randomized option data points clusters noise randomized entire scope parameter default value memory ox bytes disk lc clef 
quality clef 
threshold clef 
initial delay split age size bytes outlier handling outlier clef 
leaf entry average er points leaf euclidian distance closest seed larger radius cluster table bir parameters dataset 
ordered option selected data points cluster placed placed order generated noise placed 
parameters default setting 
ir capable working various settings 
table lists parameters birch effecting scopes default values 
specified explicitly 
experiments conducted default setting 
flf selected kbytes dataset size base workload experiments 
space just outliers assume set experiments effects distance metrics phases zr indicate phases results higher en ling threshold produces clusters poorer quality distinctive performance difference ot hms 
decided choose default 
statistics tradition choose weighted average diameter denoted quality measurement 
smaller het ter quality threshold defined threshold cluster diameter default 
phase initial threshold default study page size affects zr selected 
delay split option threshold cf tree accepts data points reaches higher capacity 
option remove outliers concentrate dense places amount resources 
simplicity treat leaf entry number points quarter average outlier 
phase global algorithms handle ol 
default input range 
chosen algorithm 
phase refine clusters option iat points quality measurement fair comparisons 
base workload performance set experiments evaluate ability cluster various lar datasets times second 
datasets pattern 
table presents generator settings 
weight wi average diameters actual clusters table 
fig 
visualizes actual clusters hy plotting cluster circle center centroid radius cluster ra anti label number points cluster bir clusters ds fig 

observe bir clusters similar actual clusters terms location number points 
maximal average difference centroids actual cluster corresponding birch cluster respectively 
number points bill cluster han correspon ling actual cluster 
radii bir clusters ranging average close actual 
note bir smaller actual radii 
birch assigns outsiders art ual clusters proper birch cluster 
similar reached analyzing visual presentations ds omitted clue lack space 
summarized table took seconds hp workstation cluster data points dataset 
pattern dataset impact clustering time table presents performance results additional ds ds ds correspon ds ds anti respectively parameter generator set ordered table changing order data points impact performance birch 
sensitivity parameters studied sensitivity birch performance change values parameters 
due lack space major details see rl 
refer clusters generated tile generator actual clusters clusters birch ch clusters 
dataset setting act dsi grid nt rh kg randomized ds sine lh rn ra ut dx random nt rt rh rn randomized table datasets base workload initial threshold birch performance stable long initial threshold excessively high wrt 
dataset 
works little extra running time 
user know rewarded saving time 
page size phase smaller larger tends increase running time requires higher lower threshold produces coarser finer leaf entries degrades improves quality 
refinement phase experiments suggest qualities phase different final qualities refinement 
outlier options birch tested noisy datasets outlier options 
results show outlier options birch slower faster time quality better 
memory size phase memory size maximal tree size increases running time increases processing larger tree rebuilt slightly clone memory finer subclusters generated feed phase results better quality inaccuracy caused hy insufficient memory compensated extent phase refinements 
word birch tradeoff memory time achieve similar final quality 
time scalability distinct ways increasing size test scalability birch 
increasing number points cluster ds ds ds create range keeping generator settings changing nk change running time phases phases plotted dataset size fig 

shown grow linearly wrt 
consistently patterns 
increasing number ers ds ds ds create range datasets keeping generator settings changing change running time phases phases plotted dataset size fig 

growing phase complexity improved linear total dataset time dataset time ds ds ds ds ds ds table birch performance base workload wrt 
time arid put order dataset time dat 
ii 
ii ds ds ii ds ds ds table cla rans performance base workload wrt 
input order time exactly linear wrt 
running time phases confirmed grow linearly wrt 
consistently patterns 
comparison birch clarans experiment compare performance birch base workload 
clarans assumes memory holding needs memory birch 
order acceptable running time set value larger newly enforced upper limit recommended ng 
value 
fig 
visualizes cla rans clusters ds 
comparing actual clusters dsi observe pattern location cluster centers distorted 
number data points cluster different number actual cluster 
radii cla rans clusters varies largely average larger actual clusters 
similar behaviors observed visualization clusters ds ds omitted due lack space 
table summarizes performance la ran 
datasets base workload times slower birch sensitive pattern dataset 
value la rans clusters larger birch clusters 
results ds ds ds show data points ordered time quality degrade dramatically 
base workload birch uses memory hut faster accurate order sensitive compared ds phase base ds phase ds phase phase ds phase 
number tuples scalability wrt 
ill ds phase base ds phase ds phase ok base ds phase birch clusters number tuples mt 
application real datasets filtering real images fig 
similar images trees partly cloudy sky background taken different 
top near infrared hand nir bottom visible wavelength band vis 
image contains xl pixels pixel act pair brightness values corresponding nir vis 
soil scientists receive hundreds image pairs try filter trees background filter trees leaves shadows branches statistical analysis 
applied birch nir vis value pairs pixels image tuples hy dataset size disk space size weighting ni vis values equally 
obtained clust ers correspond bright part sl ordinary part sky clouds leaves tree branches shadows trees 
step took seconds 
branches shadows distinguished cou separate categories pulled part data corresponding tuples birch time nir weighted times heavier vis observed branches shadows easier tell apart nir image vis image birch ended finer threshold processed smaller dataset wit amount memory 
clusters corresponding branches shadows obtained ls fig 
shows parts image correspond studying reasonable ways increasing threshold dynamically dynamic adjustment outlier criteria accurate quality nts data parameters indicators birch perform 
explore birch architecture opportunities parallel executions interactive 
incremental birch able read data directly tape drive network matching clustering speed data reading speed 
study clustering information obtained help solve problems storage query optimization data compression 
ima ges taken nir vis cks peter james kelly matthew self auto class bayesian proc 
tb int 
machine learning morgan jun 
dh richard duda peter hart sce ze analysis wiley 
dj 
dubes ing methodologies data advances edited vol 
academic new york 
dt 
martin ester peter kriegel aud xu database interface clustering spatial databases proc 
st int conf 
ou aud data 
ester peter kriegel xu knowl 
spatial focusing techniques proc 
tb int symposium ou large spatial databases maine 
fis douglas fisher knowledge uia tone clustering machine leaves branches shadows leaves tree branches shadows trees obtained hy clustering birch 
visually see original image user intention 
summary research clustering method large datasets 
large clustering problem tractable hy concentrating densely occupied portions compact summary 
utilizes measurements capture natural closeness data 
measurements stored updated incrementally tree 
birch amount complexity little scan data 
experimentally birch shown perform large significantly superior clarans terms quality speed order sensitivity 
proper parameter setting important bir 
near concentrate fis douglas fisher iterative optimization simp hierarchical clusterings technical report dept computer science vanderbilt sity nashville tn 
gray vector quantization signal compression boston ma kluwer academic 
kr leonard kaufman peter rousseeuw finding groups data analysis wiley series mathematical statistics 
michael lebowitz experiment 
incremental formation machine 
lee lee clustering ar application information systems science edited vol 
pp 
plenum press new york mur survey advance 
hierarchical 
nh raymond ng jiawei hau methods spatial data mining roe vldb 
clark son parallel algorithms hierarchical technical report computer 
california berkeley dec 
tian rau akt aud liv birch ef cient data clustering databases technical report computer dept 
madison 
