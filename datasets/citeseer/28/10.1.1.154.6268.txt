detecting short passages similar text large document collections caroline lyon james malcolm bob dickerson department computer science university hertfordshire hertfordshire ab uk lyon malcolm dickerson ac uk presents statistical method fingerprinting text 
large collection independently written documents text associated fingerprint different 
fingerprints close suspected passages copied similar text occur documents 
method exploits characteristic distribution word trigrams measures determine similarity set theoretic principles 
system developed corpus broadcast news reports successfully detect plagiarism students 
find small sections similar identical 
method simple effective lexical patterns text automatically captured give fingerprint piece writing 
set documents detect similar passages documents comparing fingerprints seeing close 
introduce simple novel system extends scope textual pattern analysis applies copy detection tasks 
tool detect plagiarism scripts submitted large classes students 
method applied purposes finding relationships large numbers documents order detect copyright 
developed corpus tv news reports 
corpus contains examples varying degrees similarity issues addressed consecutive reports 
corpus just words long see table 
smaller corpus federalist papers texts words investigated extensively hamilton establish benchmarks 
describe plagiarism detector works students similar usually identical passages sentences long detected 
tool flag pairs documents contain similar passages 
case student correctly quotes cites source material 
determined inspection 
rest organised way 
section introduces principles underlying method discusses word trigrams suitable features extract 
section looks context theoretical point view gives outline metrics employed 
section looks related field section describes domain develop prototype 
sections describe experiments undertaken results 
section concludes 
principle fingerprint extraction fundamental issue language modelling automated speech recognition sparse data problem ney page gibbon page 
phenomenon turned head put fingerprint text 
principle underlying system identifying fingerprint associated piece text large number small easily extracted lexical features word trigrams 
text converted set overlapping word sequences composed 
consider separate texts subject certain common words bigrams trigrams 
compound noun phrases provide typical examples 
phenomenon exploit fact common trigrams constitute small proportion trigrams derived independent texts see examples table table 
consider documented distribution single words english languages shannon manning schutze 
find characteristic zipfian distribution small number words significant number rarely 
brown corpus classrooms centres health workers try spread disease 
pupils staff injected today combat described public health emergency 
morning children queuing injections lessons school centre outbreak 
health teams begun pupils staff attempt cases meningitis bring public health emergency control 
table examples news text independent reports subject 
semantic content similar matching trigrams 
trigrams overlap consecutive matching words produce matching trigrams 
lot pressure put people various capacities suddenly find pressures coming impossible job 
lot pressure people various capacities find pressures impossible job 
table examples similar news reports common source suspected 
matching trigrams take grams matches grams matches 
words word forms occur kupiec 
distribution words empirical observation understood theoretical grounds certain assumptions english language production bell chapter 
distinctive distribution words pronounced word bigrams pronounced trigrams 
probability word occuring low word occurring conjunction lower 
usually case trigrams turning new texts occurred large training corpora documents subject author 
table shows high percentage trigrams occur tv news corpus 
gibbon give figures large corpora wall street journal gibbon 
corpus defined limited domain expect recurrent lexical features quite common corpus size increased 
table shows words trigrams occurred 
particular article majority trigrams probably belong article 
set trigrams derived article distinguishing feature set 
investigated grams lexical features various single words word pairs distinguishing power trigrams effective demonstrated 
reduce sensitivity tool ability detect similar exactly copied text 
illustrated sample texts shown table matching trigrams trigrams text matching grams grams text matching grams grams text 
context statistical pattern recognition overviews vast field include special issue journal pattern analysis machine intelligence january jain :10.1.1.123.8151
dominant approach pattern recognition take data analysed tv news corpus number texts total number words maximum file size words number files words average file size words average number distinct trigrams file average singleton trigrams file table statistics tv news corpus source corpus size distinct trigrams singleton trigrams trigrams words singletons tv news federalist papers wsj table statistics tv news corpus federalist papers wall street journal corpora gibbon page significant features 
resultant features lined feature vector characterises data 
feature vectors processed achieve objective classifying task 
associated approach need appropriate features get associated weights find effective methods processing 
method successfully sound image processing tasks 
approach works certain limits 
relationships number elements feature vector amount data needs set parameters system large number features need large amount training data 
instance neural network branch statistical pattern recognition guidelines number training examples needed certain size feature vector typically quote minimum ratio jain page bishop page :10.1.1.123.8151
language processing tasks addressed limitations main stream statistical pattern recognition approach 
instance parsing indefinite number words may mapped limited number parts ofspeech 
lyon frank 
detecting semantic similarities texts linguistic indicators extracted feature vector hatzivassiloglou 
tasks reduced number significant features losing necessary information 
need lexical information number words unrestricted natural language usually prohibitive 
standard pattern recognition approach processing linear feature vectors document processing large vocabulary texts cited problem remaining addressed jain page :10.1.1.123.8151
copy detection need lexical information 
trigrams prima facie comparable value abstracted reduced set started processing 
take different approach classifying text set theoretic concepts 
lining features linear vector large numbers mini features grouped set 
approach weight different mini features model dependencies 
greater range lexical information 
spite advances statistical pattern recognition years comparatively little attention paid approach 
comparison documents set theoretic concepts objective compare documents classify independent similar 
comparison take place modes may compare items comparable length case looking resemblance texts 
secondly piece text compared large body material source 
case texts unequal size looking containment 
significant portion smaller text contained larger indicates material lifted suspected source 
method set theoretic concepts transform piece text set trigrams 
documents considered sets trigrams compared 
measures come broder broder :10.1.1.24.779
concept resemblance informally number matches elements sets trigrams scaled joint set size 
set trigrams documents respectively 
resemblance concept containment suppose measuring extent set contained set set derived concatenated potential source material web large set 
set derived single student essay small set 
informally containment number matches elements trigram sets scaled size set words proportion trigrams known jaccard coefficient feature vector analysis manning schutze page 
related lexical statistics widely speech language processing years kukich gibbon 
find record way propose 
copy detection text approaches copy detection task include methods concept searching matching strings typically longer trigrams 
known system scam stanford copy analysis mechanism shivakumar garcia molina web objectives 
check internet copyright second filter duplicates near duplicates information retrieval 
process tens millions web pages 
large scale evaluation inevitably presents problems limited 
scale current puts objectives different class similar concept fingerprinting text extracting sets chunks 
chunk string words non overlapping sequences various length distribution 
sampling mechanism chosen reduce number fingerprints stored 
non overlapping chunks reduces storage requirements loses useful information 
chunks match word match 
problems arise similar chunks get phase appear match 
broder broder addressed task clustering documents groups closely resembled :10.1.1.24.779
approach concept matching shingles shingle sequence words 
length shingle determined 
reduce storage demands shingles selected sampling process 
similarity measures resemblance containment defined 
similar approach employed heintze heintze developed system analysing documents 
uses character strings word strings basis fingerprint quotes effective lengths characters 
focuses methods selection produce reduced size fingerprint full set 
web prototype available 
systems address copy detection large scale tens millions documents 
problems scale evalu ation presents difficulties 
instance scam system evaluated just classifying sample texts subjective basis shivakumar garcia molina section 
methods longer chunks shingles character strings sensitive tool detecting similar texts exact copies illustrated table 
insertion deletion substitution small number words undermine matching process 
trigram method underlying similarities better detected superficial variations text 
related hatzivassiloglou 
hatzivassiloglou developed method detecting semantically similar texts extracting heterogenous collection morphological syntactic semantic features processing resultant feature vector rule induction system 
example traditional feature vector approach described section 
approach detecting similar sections code clones large programs 
successful example neural processor detects clones lines code product telecommunications barton 
case feature vector heterogenous collection items derived information keyword frequency counts numbers parameters formatting factors 
limited number features neural approach appropriate 
phelps wilensky phelps wilensky shown texts identified small signatures words long 
producing robust urls web page moved traced url augmented content signature 
different task copy detection documents different signatures contain copied material 
tv news corpus principal corpus develop prototype set tv news reports taken periods 
statistics corpus table 
usually texts news programmes day corpus 
semantic content described different words illustrated table 
language similar illustrated table 
verbatim copying 
ran diagnostic tests corpus examples expected copied text came light 
clear instances independent similar text definitive objective boundary borderline cases 
eventually classification documents similar independent subjective judgement 
boundary set depends purpose tool 
possible copies flagged false positives tolerated 
avoid threshold set higher false positives short similar passages may go undetected 
decided texts table count sufficiently close flagged 
experiments results preprocessing experiments pre processing text done 
words starting sentences initial letter providing proper noun 
punctuation removed 
numbers replaced symbol typical word patterns inflation rose percent preserved 
text transformed associated set trigrams 
experiments resemblance compared pair files original corpus 
scores resemblance 
result discussed number matches programmes day typically range giving scores news programmes day 
inspection texts included sections copied verbatim similar copies varying words length 
cases sections copied 
exceptional case indicated news resembled programme 
inspection turned item words long date re broadcast verbatim programme words long second date copied 
item environmental pollution particularly topical 
closer inspection pairs texts copied sections varying lengths varying degrees similarity 
taken preliminary threshold copying indicated subsequently revised downwards 
identifying known copying blind test identifying known copying done files put aside experimenter 
half contents replace text file main body corpus put back main corpus 
experimenter ran number files compared original files pairs maximum max files minimum files counterpart min files passages copied table scores resemblance original tv news corpus files compared scores files 
number files compared maximum containment files rest corpus minimum containment files rest corpus table scores containment files original tv news corpus rest corpus compared scores texts 
fingerprinting programs corpus including files 
easily identified see table 
similar tests carried containment metric 
compared single files large body material source 
body texts concatenated apart reserved assessed 
taken time compared rest corpus 
results table show clear distinction scores files exceptional case compared normal ones 
experiments files show gross plagiarism easily detected 
establishing thresholds federalist corpus federalist papers texts completely different domain 
celebrated collection papers written proposed american constitution 
easily accessible extensively studied mosteller wallace 
considered papers written madison hamilton totalling words 
average length words 
corpus subjects addressed repeatedly 
papers written authors pseudonym 
threshold ran prototype papers maximum score 
confirmed view reasonable level similar text 
reduced investigations students copying false positives may arise 
threshold investigated containment scores individual files concatenated rest 
took files spaced corpus assess 
averaged words words left rest corpus 
resulting scores containment ranged 

conducted similar experiment files extracted news corpus chosen news programmes day rest corpus 
consecutive days 
news reports day frequently treat subjects copied sections embedded 
reports different days reduces eliminate risk 
results table obtained showing number words concatenated corpus number files assessed average number words assessed file range scores table results experiment establish base lines thresholds comparing text large potential source propose provisional threshold aware false positives may slip 
detecting plagiarism students plagiarism detector students submitted electronically 
vast majority scripts clearly independently written clearly flagged potential copies 
assignments loaded processing time quick instance seconds files preprocessing simplified letters converted lower case non alphabetic characters ignored 
example investigate reports 
students submitted reports single text file averaging words sentences 
submitted reports sets smaller html files 
averaged words 
robust system handles html ordinary text non alphabetic symbols ignored parts embedded html commands left words undermine system 
cases matching sections text 
significant paragraphs long 
matching sentences sprinkled scripts 
students cut paste fragments web 
matching passages similar identical occasional words phrases deleted substituted inserted 
course way telling direction copying students 
significant cases matching trigrams files words 
similar paragraphs 
matching trigrams files words 
similar paragraphs 
cases matching sentences ranged 
threshold reduced false positives arise 
assignment small number matching trigrams plagiarism instance table contents 
threshold cases find matching passages scripts threshold 
case reports analysed averaging words 
highest reading 
looked cases matching sentences time students quoted assignment brief 
interface developed plagiarism detection tool 
display names pairs texts ranked order include similar text coming 
contents files displayed required similar passages text highlighted side side 
point subjective decision copied similar 
introduced way lexical information language processing produced application 
method simple effective 
mainstream textual pattern analysis restricted focus linear feature vectors described section examples section 
different approach set theoretic principles 
system developed material subtle similarities distinction independent non independent text hard determine 
basis develop robust tool 
system evaluated extent proven effectiveness detecting copied material students 
similar passages sentences long files words 
passages similar identical identified slight editing mask plagiarism 
field section ambitious addressing comparison millions web pages 
evaluation difficult large scale tasks 
smaller system demonstrably works trigram fingerprint detect similar passages identical 
need compare system methods issue evaluation large scale language processing tools needs attention 
integrated focus scaling system particular material web 
plan investigate approach extended comparing programs case non alphanumeric symbols candidates fingerprinting 
start investigating data particular distribution symbol tuples 
illustrates empirical investigations lead new data 
claim carried known wittgenstein pages acquiring knowledge language don think look 
problems solved arranging known 
barton davey frank 

dynamic competitive learning applied clone detection problem 
international workshop applications neural networks telecommunications stockholm 
bell cleary witten 

text compression 
prentice hall 
bishop 

neural networks pattern recognition 
oup 
broder 

resemblance containment documents 
compression complexity sequences ieee computer society 
gibbon moore 

handbook standards resources spoken language systems 
mouton de gruyter 
hamilton madison jay 
federalist papers 
www mcs net fed htm 
hatzivassiloglou klavans eskin 

detecting text similarity short passages exploring linguistic feature combinations machine learning 
proc 
emnlp 
jain duin mao 

statistical pattern recognition review 
ieee trans 
pattern analysis machine intelligence 
kukich 

techniques automatically correcting words text 
acm computing surveys december 
kupiec 

robust part speech tagging hidden markov model 
computer speech language 
lyon frank 

single layer networks discrete sequential data example natural language processing 
neural computing applications 
manning schutze 

foundations statistical natural language processing 
mit 
mosteller wallace 

applied bayesian classical inference case federalist papers 
springer verlag 
ney martin wessel 

statistical language modelling leaving 
young editors corpus methods language speech processing 
kluwer academic publishers 
heintze 
scalable document fingerprinting 
bell laboratories www cs cmu edu afs cs user nch www 
thomas phelps robert wilensky 
robust hyperlinks cheap 
proc 
digital documents electronic publishing www cs berkeley edu phelps robust papers html 
shannon 

prediction entropy printed english 
bell system technical journal pages 
shivakumar garcia molina 

building scalable accurate copy detection mechanism 
proc 
rd international conference theory practice digital libraries 
wittgenstein 

investigations 
blackwell 
