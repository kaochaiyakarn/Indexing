probabilistic latent semantic analysis appear artificial intelligence uai stockholm thomas hofmann eecs department computer science division university california berkeley international computer science institute berkeley ca hofmann cs berkeley edu probabilistic latent semantic analysis novel statistical technique analysis mode occurrence data applications information retrieval filtering natural language processing machine learning text related areas 
compared standard latent semantic analysis stems linear algebra performs singular value decomposition occurrence tables proposed method mixture decomposition derived latent class model 
results principled approach solid foundation statistics 
order avoid overfitting propose widely applicable generalization maximum likelihood model fitting tempered em 
approach yields substantial consistent improvements latent semantic analysis number experiments 
learning text natural language great challenges artificial intelligence machine learning 
presents statistical view lsa leads new model called probabilistic latent semantics analysis plsa 
contrast standard lsa probabilistic variant sound statistical foundation defines proper generative model data 
detailed discussion numerous advantages plsa subsequent sections 
latent semantic analysis count data occurrence tables lsa principle applied type count data discrete dyadic domain cf 
:10.1.1.46.2857
prominent application lsa analysis retrieval text documents focus setting sake concreteness 
suppose collection text doc uments fd dn terms vocabulary fw wm ignoring sequential order words occur document may summarize data theta occurrence table counts ij denotes term occurred document particular case called matrix rows columns referred document term vectors respectively 
key assumption simplified bag words vector space representation documents cases preserve relevant information tasks text retrieval keywords 
latent semantic analysis svd mentioned key idea lsa map documents symmetry terms vector space reduced dimensionality latent semantic space 
obtains approximation notice document document inner products approximation sigma think rows sigma defining coordinates documents latent space 
original high dimensional vectors sparse corresponding low dimensional latent vectors typically sparse 
implies possible compute meaningful association values pairs documents documents terms common 
hope terms having common meaning particular synonyms roughly mapped direction latent space 
probabilistic lsa aspect model starting point probabilistic latent semantic analysis statistical model called aspect model :10.1.1.46.2857
aspect model latent variable model occurrence data associates unobserved class variable fz observation 
joint probability model theta graphical model representation aspect model asymmetric symmetric parameterization 
defined mixture wjd wjd virtually statistical latent variable models aspect model introduces conditional independence assumption independent conditioned state associated latent variable corresponding graphical model representation depicted 
cardinality smaller number documents words collection acts bottleneck variable predicting words 
posterior probabilities classes different occurrences segment indicate factors pair generated observation 
displayed estimates conditional word probabilities segment jd see correct meaning word segment identified cases 
implies segment occurs frequently document overlap factored representation low identified polysemous word relative chosen resolution level dependent context explained different factors 
aspects versus clusters worth comparing aspect model statistical clustering models cf 
:10.1.1.46.2857
clustering models documents typically associates latent class variable document collection 
closely related approach distributional clustering model thought unsupervised version naive bayes classifier 
shown conditional word probability probabilistic clustering model wjd pfc pfc zg posterior probability document having latent class simple implication bayes rule posterior probabilities concentrate probability mass certain value increasing number observations length document 
means algebraically equivalent conceptually different yield fact different results 
communications acm 
hofmann 
probabilistic latent semantic indexing 
proceedings sigir 
hofmann puzicha jordan :10.1.1.46.2857
unsupervised learning dyadic data 
advances neural information processing systems volume 
mit press 
landauer dumais 
