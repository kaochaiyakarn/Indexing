presenting analyzing results ai experiments data averaging data snooping proceedings fourteenth national conference artificial intelligence aaai aaai press menlo park california pp 

copyright aaai 
presenting analyzing results ai experiments data averaging data snooping lee giles steve lawrence nec research institute independence way princeton nj experimental results reported machine learning ai literature misleading 
investigates common processes data averaging reporting results terms mean standard deviation results multiple trials data snooping context neural networks popular ai machine learning models 
processes result misleading results inaccurate 
demonstrate easily happen propose techniques avoiding important problems 
data averaging common presentation assumes distribution individual results gaussian 
investigate distribution common problems find approximate gaussian distribution may symmetric may multimodal 
show assuming gaussian distributions significantly affect interpretation results especially comparison studies 
controlled task find distribution performance skewed better performance smoother target functions skewed worse performance complex target functions 
propose new guidelines reporting performance provide information actual distribution box whiskers plots 
data snooping demonstrate optimization performance experimentation multiple parameters lead significance assigned results due chance 
suggest precise descriptions experimental techniques important evaluation results need aware potential data snooping biases formulating experimental techniques selecting test procedure 
additionally important rely appropriate statistical tests ensure assumptions tests valid normality distribution 
known analysis presentation ai machine learning simulation results needs done carefully 
specific case neural networks recognized experimental evaluation needs improvement prechelt 
current recommendations include copyright american association artificial intelligence www aaai org 
rights reserved 
reporting mean standard deviation results number trials called data averaging computation statistical tests test performance comparisons 
recommendations assume distribution results multiple trials gaussian potential biases data snooping taken account 
part shows distribution results may differ significantly gaussian distribution common procedure reporting mean standard deviation number trials lead misleading incorrect 
second part investigates data snooping shows lead incorrect 
experiments contained done neural networks issues raised guidelines broader relevant ai machine learning paradigms 
data averaging presenting results multiple trials part investigate common practice reporting neural network simulation results terms mean standard deviation multiple trials 
provide background neural network training problem descriptive statistics 
complexity neural network training performance neural network simulation result training process interest consider properties training problem 
general training problem multi layer perceptron mlp neural network np complete lugosi blum rivest general algorithm capable finding optimal set parameters computation time bounded polynomial input dimension 
typical compromise iterative optimization technique backpropagation bp 
cases techniques guaranteed find local minimum cost function 
problem training algorithm hard find globally irq st optimal solution may difficult predict expected quality distribution solutions 
cases typically reason expect distribution results gaussian actual distribution interest 
performance measures typical method assessing performance running multiple simulations different starting point weight space reporting mean standard deviation results suitable distribution results gaussian 
example particular network training algorithm distribution results skewed multimodal observed mean standard deviation 
case alternative method describing results provide accurate understanding true nature performance network algorithm 
descriptive statistics median interquartile range median interquartile range iqr sections 
median interquartile range iqr simple statistics sensitive outliers commonly mean standard deviation weiss 
median value middle arranging distribution order smallest largest value 
divide data equal groups median iqr difference medians groups 
iqr contains points 
comparing mean median advantages disadvantages 
median preferred distributions outliers mean takes account numerical value point median 
example student wishes average exam results mean appropriate 
ai machine learning performance distributions interested distribution individual performance results mean performance number trials 
specifically may interested probability trial meet performance criterion 
median iqr minimum maximum values provide information distribution results consequently nature optimization process 
plots incorporate median iqr minimum maximum values distribution tukey 
kolmogorov smirnov test kolmogorov smirnov test order test normality distributions 
statistic press 
estimator cumulative distribution function distribution test cg cumulative distribution function case normal distribution erf mean variance 
distribution statistic approximated null hypothesis distributions 
determine significance level value null hypothesis distributions 
formula adc lk number data points ranges small values indicate distributions significantly different case small values indicate distribution represented significantly different gaussian 
results reported 
created distribution results normalization zero mean unit variance 
empirical result distributions primarily interested distribution results practical problems resulting implications results 
results number experiments problems commonly neural network literature 
case plot analyze distribution network error training test data 
training details standard backpropagation stochastic update update training point 
specified networks mlps 
inputs normalized zero mean unit variance 
quadratic cost function haykin 
learning rate reduced linearly zero training period initial value 
performance reported terms percentage examples incorrectly classified classification problem normalized mean squared error nmse 
phoneme data experiments database esprit project 
aim task distinguish nasal oral vowels verleysen 
training patterns test patterns inputs provided cochlear spectra outputs 
hidden nodes iterations trial distribution results shown 
observed distributions skewed better performance gaussian symmetric 
test case underlying distribution classification error discrete continuous 
mackey glass mackey glass equation time delay differential equation proposed model white blood cell production mackey glass br mg bw result similar performance search converge learning rate schedules proposed darken moody darken moody 
train test 
distribution classification performance networks trained phoneme problem 
left hand graph shows training distribution right hand graph shows test distribution 
abscissa corresponds percentage examples correct ordinate represents percentage individual results falling section histogram 
distribution created individual simulations random starting points 
note scales change graphs 
constants commonly chosen delay parameter determines behavior system farmer 
stable fixed point attractor 
stable limit cycle attractor 
period doubling begins system produces chaotic attractor 
experiments re subsampled series consider predicting series step ahead 
problem results number architectures compared mlp fir mlp iir mlp back 
fir iir mlp networks similar standard mlp synapse replaced fir iir filters respectively 
fir iir filters layer synapses contained taps second layer synapses contain fir iir filters mlp networks input window 
network hidden nodes trained updates 
training patterns test patterns 
fir iir networks tested synaptic gains back 
interesting observe difference distribution results case 
synaptic gains extra parameter inserted synapse multiplies weighted sum individual filter outputs 
altering synaptic gain equivalent altering weights corresponding filter taps 
addition synaptic gains affect representational power networks affect error surface extra degrees freedom may optimization easier back 
continues ported chosen shows distribution normalized mean squared error nmse results 
observed distribution varies significantly various models distributions highly skewed multimodal 
shows box whiskers plots usual mean standard deviation plots models 
mean minus standard deviation lower best individual error iir test cases 
observer interpreting results mean standard deviation assumption distributions approximately gaussian may un fir finite impulse response iir infinite impulse response 
der impression percentage networks obtained performance better mean minus standard deviation points 
trials results performance iir test cases 
considering fir iir synaptic gains networks significant differences evident distributions plots distributions multimodal iir case significantly skewed better performance 
differences clear mean standard deviation similar cases 
interesting significantly different distributions fir iir mlp networks synaptic gains 
expected observed general box whiskers plots informative mean plus standard deviation plots informative actual distributions 
training test sets respec values tively fir gains fir gains iir gains iir gains mlp 
distributions significantly different gaussian mlp case 
train train train train train test test test test test fir gains fir gains iir gains iir gains mlp 
distribution nmse results mlp fir mlp iir mlp networks trained mackey glass problem 
left hand graphs show distribution training errors right hand graphs shows distribution test errors 
abscissa corresponds mean squared error ordinate represents percentage individual results falling section histogram 
distribution created individual simulations 
scales small distinguish see 
artificial task order conduct controlled experiment vary complexity target function artificial task 
mlp input nodes hidden nodes output node initialized random weights uniformly selected specified range range weights network biases constant 
bias weights initialized small random values range 
task similar procedure crane training nmse mlp fir gains fir gains iir gains iir gains train train train train test test test test test nmse mlp fir gains fir gains iir gains iir gains 
distribution errors networks trained artificial task 
top bottom graphs correspond values 
left hand graphs show distribution training errors right hand graphs shows distribution test errors 
abscissa corresponds mean squared error ordinate represents percentage individual results falling section histogram 
distribution created individual simulations 
scales small distinguish see 

box whiskers plots tukey left case mackey glass task shown mean plus minus standard deviation right case 
plots box corresponds iqr bar represents median whiskers extend minimum maximum values 
mlp fir gains cases compressed graph due relative poor performance cases 
results cases plotted right hand scale 
plots distinguished dotted line box 
notice means fir iir synaptic gains cases similar median iir mlp networks lower mean minus standard deviation iir mlp networks lower best individual networks lower zero synaptic gains case 
train nmse test nmse maximum random weight general increased complexity function mapping increased 

data points created selecting random inputs zero mean unit variance propagating network find corresponding outputs 
dataset forms training data subsequent simulations 
procedure repeated create test dataset points 


training data set train new mlps 
initial weights new networks set standard procedures equal weights network create dataset 
initialized node node basis uniformly distributed random numbers range fan neuron haykin 
network trained updates 
shows histograms distribution results cases observed distribution performance skewed better performance smoother target functions lower skewed worse performance complex target functions higher general trend maximum random weight 
box whiskers plots artificial task left case mean plus minus standard deviation right case 
left right observe mean minus standard deviation lower best individual networks median moves mean mean increased 
observed higher frequency trials resulted relatively worse performance increased 
note significant multimodality high shows box whiskers plots usual mean standard deviation plots cases 
note mean minus standard deviation lower best individual error trials 
values training test sets respectively 
distributions significantly different gaussian 
data snooping previous section presents case providing information distribution results information account drawing 
section presents case providing information experimental process information formulating experimental process commonly done 
researchers aware forms experimental bias ceiling floor effects regression effects order effects cohen 
investigation machine learning ai literature shows data snooping biases ignored 
data snooping commonly refers practice assigning meaning spurious correlations patterns 
example running simulation different parameters reporting best result sample test set result may partially completely due chance 
order demonstrate data snooping created purely random problem uniformly distributed random inputs random class classification output 
training details follows training test sets consisted points networks contained hidden nodes stochastic backpropagation updates learning rate reduced linearly zero initial value linearly reducing learning rate 
simulations performed setting parameters 
shows results repeating problem times different random training test sets 
observe performance varies correct classification expected 
compare results null hypothesis gaussian distribution mean variance commonly rec test find results significantly different ran simulations chance results significantly different random level significance 
note different doing tests selecting significant course valid comparing multiple groups requires different tests anova tukey hsd 
wrong 
tests appropriate distributions significantly different normal time shapiro wilks tests pairwise variance distributions varies significantly time tests 
clearly care taken interpreting results formulating tests 
shows results testing range learning rates procedure done seen summarized literature optimized learning rate 
clearly selection optimal value allow results significantly better chance 
prime example data snooping principle recommendation known result optimization optimal learning rate tested additional test set unseen data possible tune learning algorithm sample test set 
shows results comparison study compares performance different neural networks 
case modify random task slightly assuming problem temporal perform training variety recurrent networks standard tdnn network order networks details models important 
overabundance statistically significant differences algorithms gamma network better elman np wz fir level significance tests 
problems drawing gamma network better include fact concentrating particular comparison number possible comparisons potential tests 
precise description experimental techniques important evaluation results need aware potential data snooping biases formulating experimental techniques 
additionally important rely appropriate statistical tests anova tukey hsd comparing multiple groups ensure assumptions tests valid normality distribution 
note observed difficulties example reduced greatly larger test set size 
test error data selection 
results random problem different random data sets 
publications commonly report performance neural networks mean variance number simulations different starting conditions 
papers recommend reporting confidence intervals gaussian distributions testing significance comparisons test 
test error learning rate 
results random problem learning rate varied 
test error elman fgs tdnn np wz fir gamma model 
results random problem different network types 
sume symmetric distributions 
distribution results neural network simulations vary widely depending architecture data training algorithm 
comparisons mean standard deviation simulation results misleading observer assumes distributions gaussian 
alternative means presenting results informative 
example possible obtain indication particular network algorithm produce acceptable result 
practical situation distribution results affect desirable number trials results multiple trials vary greatly may reasonable smaller number trials 
recommendations 
plot distribution results visual inspection 
distributions significantly multimodal mean plus standard deviation box whisker plots show complete picture 

median interquartile range minimum maximum values mean standard deviation interpreting results 
plotting results plots 

certain cases may possible approximate normal distribution removing outliers 
case relatively small number trials result comparatively poor convergence practice removing trials statistics reporting percentage failed trials appear reasonable 
may difficult perform simulations order accurately characterize distribution performance reasonable time 
may possible follow recommendations 
distribution results taken account presenting analyzing results avenues presentation misleading results inaccurate 
considered possibility data snooping biases 
demonstrated common procedures performing experiments analyzing results result incorrect due data snooping 
suggest precise description experimental techniques important evaluation results need aware possible data snooping biases formulating experimental techniques 
additionally important rely appropriate statistical tests ensure assumptions tests valid normality distribution 
back 
new techniques nonlinear system identification neural networks linear systems 
ph dissertation department electrical engineering university queensland 
blum rivest 
training node neural network np complete 
neural networks 
cohen 
empirical methods artificial intelligence 
cambridge massachussets mit press 
crane pearson 
characterizing neural network error surfaces sequential quadratic programming algorithm 
machines learn 
darken moody 
note learning rate schedules stochastic optimization 
lippmann moody touretzky eds advances neural information processing systems volume 
san mateo ca morgan kaufmann 

lugosi 
strong universal consistency neural network classifiers 
ieee transactions information theory 
farmer 
chaotic attractors infinite dimensional dynamical system 
physica 

statistical evaluation neural network experiments minimum requirements current practice 
technical report tr austrian research institute artificial intelligence vienna austria 
haykin 
neural networks comprehensive foundation 
new york ny macmillan 
mackey glass 
oscillation chaos physiological control systems 
science 
prechelt 
quantitative study experimental evaluations neural network learning algorithms 
neural networks 
press teukolsky vetterling flannery 
numerical recipes 
cambridge cambridge university press second edition 
tukey 
exploratory data analysis 
reading ma addison wesley 
verleysen 
statistical neural network high dimensional vector classification 
proceedings ieee international conference neural networks icnn 
perth western australia ieee 
weiss 
introductory statistics 
reading massachusetts addison wesley second edition 
