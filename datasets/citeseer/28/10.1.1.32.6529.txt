inference learning hybrid bayesian networks kevin murphy report 
ucb csd january computer science division eecs university california berkeley california survey literature methods inference learning bayesian networks composed discrete continuous nodes continuous nodes multivariate gaussian distribution mean variance depends values discrete nodes 
brie consider hybrid dynamic bayesian networks extension switching kalman lters 
report meant summarize known sucient level detail enable implement algorithms 
update algorithm described report due lau see cdls ch 
implemented part bayes net toolbox numerically unstable 
material added appropriate 
discuss bayesian networks bns jen node discrete continuous scalar vector valued joint distribution nodes conditional gaussian cg lw lau instantiation discrete nodes distribution continuous nodes form xjy represents multivariate gaussian normal density 
note discrete nodes continuous parents model 
general kind bn exact inference algorithms known 
related review article rg :10.1.1.30.5555
start discussing represent conditional probability distribution node joint distribution encodes 
give example hybrid bn discussing inference learning techniques 
representing local conditional probability distributions discrete nodes conditional distribution discrete node parents pa speci ed means table called conditional probability table cpt entries ijk pr 
denotes th possible value instantiation pa 
rst pass called collect evidence phase second pass called distribute evidence phase 
denote evidence subtree rooted denote rest evidence rst pass clique potential contains pr second pass clique potential contains pr pr 
passes recover posterior marginal family nding clique contains variables clique 
compute marginal set variables contained clique see xu 
tree chain structure algorithm identical forwards backwards algorithm hmms :10.1.1.150.82
things remain speci ed initialize update clique potentials 
initialize potential clique multiplying cpts variables assigned example assign get pr pr pr bja pr pr 
similarly assign get pr pr 
separators initialized 
hybrid case exact posterior distribution hybrid potential mixture gaussians 
approximated single gaussian performing weak 
general arbitrarily bad approximation may replacing multimodal distribution unimodal 
suppose error introduced step 
results bk bk show hybrid dbn total error function mixing rate markov chain independent alternative approach learning hybrid dbns taken gh maximize exact lower bound likelihood produced considering tractable approximation original structure :10.1.1.30.5334
partially observable case observe value node training case longer closed form expression mle 
section investigate methods learning circumstances 
methods passes training data update parameters pass reach local maximum likelihood space batch methods 
easy convert incremental online versions update parameters seeing subset training set see nh incremental em bc incremental gradient descent 
uai 
gh geiger heckerman 
learning gaussian networks 
uai volume pages 
gh ghahramani hinton :10.1.1.30.5334
switching state space models 
technical report crg tr dept comp 
sci univ toronto 
jen jensen 
morgan kaufmann 
ps peot shachter 
fusion multiple observations belief networks 
arti cial intelligence 
rg roweis ghahramani :10.1.1.30.5555
unifying review linear gaussian models 
neural computation 
smyth heckerman jordan :10.1.1.150.82
probabilistic independence networks hidden markov probability models 
arti cial intelligence 
rg roweis ghahramani :10.1.1.30.5555
unifying review linear gaussian models 
neural computation 
smyth heckerman jordan :10.1.1.150.82
probabilistic independence networks hidden markov probability models 
technical report msr tr microsoft research 
sk shachter 
gaussian uence diagrams 
