review communicated steven nowlan unifying review linear gaussian models sam roweis computation neural systems california institute technology pasadena ca zoubin ghahramani department computer science university toronto toronto canada factor analysis principal component analysis mixtures gaussian clusters vector quantization kalman filter models hidden markov models unified variations unsupervised learning single basic generative model 
achieved collecting disparate observations derivations previous authors introducing new way linking discrete continuous state models simple nonlinearity 
nonlinearities show independent component analysis variation basic generative model 
show factor analysis mixtures gaussians implemented neural networks learned squared error plus regularization term 
introduce new model static data known sensible principal component analysis novel concept spatially adaptive observation noise 
review literature involving global local mixtures basic models provide pseudocode inference learning basic models 
hand rotate axes measure data easily fix things noise constrained axis aligned covariance diagonal 
em factor analysis criticized quite slow rubin thayer 
standard method fitting factor analysis model quasi newton optimization algorithm fletcher powell empirically converge faster em 
em algorithm efficient way fitting factor analysis model wish emphasize factor analysis latent variable models reviewed em provides unified approach learning 
online learning shown possible derive family em algorithms faster convergence rates standard em algorithm kivinen warmuth bauer koller singer :10.1.1.30.7849
pca 
restricting merely diagonal require multiple identity matrix words covariance ellipsoid spherical model call sensible principal component analysis roweis 
columns span principal subspace subspace pca call scalar value diagonal global noise level 
note uses pk free parameters model covariance 
