machine learning proceedings fourteenth international conference 
output codes boost multiclass learning problems robert schapire labs mountain avenue room murray hill nj schapire research att com 
describes new technique solving multiclass learning problems combining freund schapire boosting algorithm main ideas dietterich bakiri method error correcting output codes ecoc 
boosting general method improving accuracy base weak learning algorithm 
ecoc robust method solving multiclass learning problems reducing sequence class problems 
show new hybrid method advantages ecoc method requires base learning algorithm binary labeled data 
boosting algorithms repeatedly reweighting examples training set weak learning algorithm examples 
boosting effectively forces weak learning algorithm concentrate hardest examples 
typically final combined hypothesis weighted vote weak hypotheses 
boosting algorithms discovered schapire freund 
freund schapire boosting algorithm called adaboost shown effective experiments conducted drucker cortes jackson craven freund schapire quinlan breiman :10.1.1.49.2457:10.1.1.133.1040:10.1.1.32.8918
labs planning move murray hill 
new address park avenue florham park nj 
simplest form adaboost requires accuracy weak hypothesis classification rule produced weak learner exceed 
binary classification problems example labeled value requirement minimal hoped random guessing achieve accuracy 
simplest form adaboost requires accuracy weak hypothesis classification rule produced weak learner exceed 
binary classification problems example labeled value requirement minimal hoped random guessing achieve accuracy 
multiclass problems labels possible accuracy may harder achieve random guessing accuracy rate fairly powerful weak learners decision tree algorithms problem 
experimentally cart capable producing hypotheses accuracy difficult distributions examples produced boosting 
accuracy requirement difficulty powerful weak learners simple attribute value tests studied holte jackson craven freund schapire boosting experiments :10.1.1.133.1040
error rate better powerful weak learners expressive weak learners advantage final combined hypothesis usually complicated computation time may reasonable especially large datasets 
freund schapire provide solution problem modifying form weak hypotheses refining goal weak learner :10.1.1.32.8918
approach predicting single class example weak learner chooses set plausible labels example 
instance character recognition task weak hypothesis may predict particular example choosing just single label 
multiclass problems labels possible accuracy may harder achieve random guessing accuracy rate fairly powerful weak learners decision tree algorithms problem 
experimentally cart capable producing hypotheses accuracy difficult distributions examples produced boosting 
accuracy requirement difficulty powerful weak learners simple attribute value tests studied holte jackson craven freund schapire boosting experiments :10.1.1.133.1040
error rate better powerful weak learners expressive weak learners advantage final combined hypothesis usually complicated computation time may reasonable especially large datasets 
freund schapire provide solution problem modifying form weak hypotheses refining goal weak learner :10.1.1.32.8918
approach predicting single class example weak learner chooses set plausible labels example 
instance character recognition task weak hypothesis may predict particular example choosing just single label 
weak hypothesis evaluated pseudoloss measure example penalizes weak hypothesis failing include correct label predicted plausible label set incorrect label included plausible set 
final combined hypothesis example chooses single label occurs frequently plausible label sets chosen weak hypotheses possibly giving weight weak hypotheses 
ffl train weak learner examples xm ym weighted ffl get weak hypothesis 
compute coefficients ff ff output final hypothesis final arg max ff generic algorithm combining boosting ecoc 
exact form pseudoloss control boosting algorithm weak learning algorithm designed handle changes form loss measure 
design gives boosting algorithm freedom focus weak learner hard predict examples labels hardest distinguish correct label 
approach works experimentally suffers certain drawbacks :10.1.1.133.1040
requires design weak learner responsive pseudoloss defined boosting algorithm hypotheses generate predictions form plausibility sets 
shelf learning algorithms error may demand extra effort creativity part programmer may completely impossible source code weak learning algorithm unavailable 
second drawback pseudoloss approach fairly slow 
typically running time weak learner times slower algorithm class problem 
combined hypothesis final computed 
hypothesis viewed kind weighted vote weak hypotheses 
example interpret binary classification weak hypothesis vote labels labels color selected vote weighted real number ff label receiving weighted votes chosen final classification 
ties broken arbitrarily analysis counted errors 
complete description algorithm need derive reasonable choice coloring coefficients ff reduce pseudoloss method freund schapire development adaboost multiclass versions boosting algorithm :10.1.1.32.8918
reduction lead analysis resulting algorithm 
review adaboost review freund schapire pseudoloss method boosting algorithm adaboost :10.1.1.32.8918
shown 
round boosting algorithm computes distribution mg theta words viewed distribution pairs examples incorrect labels 
example interpret binary classification weak hypothesis vote labels labels color selected vote weighted real number ff label receiving weighted votes chosen final classification 
ties broken arbitrarily analysis counted errors 
complete description algorithm need derive reasonable choice coloring coefficients ff reduce pseudoloss method freund schapire development adaboost multiclass versions boosting algorithm :10.1.1.32.8918
reduction lead analysis resulting algorithm 
review adaboost review freund schapire pseudoloss method boosting algorithm adaboost :10.1.1.32.8918
shown 
round boosting algorithm computes distribution mg theta words viewed distribution pairs examples incorrect labels 
idea enable boosting algorithm concentrate weak learner hard examples incorrect labels hardest distinguish correct label 
distribution weak learner computes soft hypothesis power set explained interpret set plausible labels example intuitively easier weak learner identify set labels may plausibly correct selecting single label 
shown 
round boosting algorithm computes distribution mg theta words viewed distribution pairs examples incorrect labels 
idea enable boosting algorithm concentrate weak learner hard examples incorrect labels hardest distinguish correct label 
distribution weak learner computes soft hypothesis power set explained interpret set plausible labels example intuitively easier weak learner identify set labels may plausibly correct selecting single label 
freund schapire allow soft hypotheses take general form functions mapping theta :10.1.1.32.9399:10.1.1.32.8918
soft consider equivalent restricting theirs range 
simplifying restriction special case theirs problem applying results 



letter adaboost oc adaboost ecoc bagging arc comparison learning methods weak learner 
goal weak learner minimize pseudoloss ffl delta gamma delta loss measure penalizes weak hypothesis failing include correct label plausible set associated example penalizes incorrect label included plausible set 
recall correct labels contribute sum 
note pseudoloss pseudoloss obtained trivially setting freund schapire adaboost algorithm works increasing round weight placed examples incorrect labels contribute pseudoloss :10.1.1.32.9399:10.1.1.32.8918
combined hypothesis chooses single label occurs largest number plausible label sets chosen weak hypotheses votes weak hypotheses count 
ffl gamma fl freund schapire theorem show training error combined hypothesis final adaboost bounded gamma gamma fl gamma exp gamma fl fl bounded away training error goes zero exponentially fast 
note weak hypotheses evaluated respect pseudoloss final hypothesis final analyzed respect usual error measure 
freund schapire give method bounding generalization error combined hypothesis schapire come better analysis voting methods adaboost 
pseudoloss ffl gamma gamma ffl definition ffl definition pseudoloss ffl expressed simply terms error ffl setting ffl gamma fl eq 
ffl gamma fl error ffl slightly better random guessing error rate pseudoloss slightly better provided 
resulting algorithm called adaboost oc shown 
method derivation algorithm fact special case adaboost weak soft hypothesis particular form 
immediately apply results freund schapire obtain theorem main theoretical result theorem sequence ht sequence weak hypotheses returned weak learner :10.1.1.32.8918
ffl gamma fl error respect data trained eq 


training error final hypothesis final algorithm adaboost oc bounded gamma gamma fl gamma exp gamma fl 
weak learners similar spirit studied holte 
second weak learner called outputs hypothesis tests conjunction attribute value comparisons 
rule built entropic potential pruned back held data 
method loosely rule formation part cohen ripper algorithm furnkranz widmer irep algorithm 
algorithms described detail freund schapire :10.1.1.133.1040
note url www ics uci edu mlearn mlrepository html soybean small iris 

glass 

advances neural information processing systems pages 
yoav freund 
boosting weak learning algorithm majority 
information computation 
yoav freund robert schapire :10.1.1.133.1040
experiments new boosting algorithm 
machine learning proceedings thirteenth international conference pages 
yoav freund robert schapire :10.1.1.32.8918
decisiontheoretic generalization line learning application boosting 
information computation 
yoav freund robert schapire :10.1.1.133.1040
experiments new boosting algorithm 
machine learning proceedings thirteenth international conference pages 
yoav freund robert schapire :10.1.1.32.8918
decisiontheoretic generalization line learning application boosting 
journal computer system sciences appear 
extended appeared 
johannes furnkranz gerhard widmer 
karp 
reducibility combinatorial problems 
miller thatcher editors complexity computer computations pages 
plenum press 
quinlan :10.1.1.49.2457
bagging boosting 
proceedings thirteenth national conference artificial intelligence pages 
ross quinlan 
programs machine learning 
