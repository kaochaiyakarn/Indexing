equivalence knowledge representation automata recurrent neural networks dynamical fuzzy systems christian omlin lee giles department computer science university south africa nec research institute princeton nj umiacs maryland college park md mail omlin cs sun ac za research nj nec com neuro fuzzy systems combination artificial neural networks fuzzy logic increasingly popular 
neuro fuzzy systems need extended applications require context speech handwriting control 
applications modeled form finite state automata 
previously proved deterministic finite state automata dfas synthesized mapped second order recurrent neural networks sigmoidal discriminant functions sparse interconnection topology programming networks weights gammah results proposes synthesis method mapping fuzzy finite state automata ffas recurrent neural networks suitable implementation vlsi encoding ffas generalization encoding dfas 
synthesis method requires ffas undergo transformation prior mapped recurrent networks 
neurons slightly enriched functionality order accommodate fuzzy representation ffa states state occupied fuzzy membership takes values range fuzzy states occupied time enriched neuron functionality allows fuzzy parameters ffas directly represented parameters neural network 
contrast stochastic finite state automata exists ambiguity automaton current state 
automaton exactly state time choice successor state determined probability distribution 
discussion relation probability fuzziness see instance 
increased interest hybrid systems applications hybrid models emerge 
definitions hybrid systems :10.1.1.46.8335
example hybrid systems combining artificial neural networks fuzzy systems see 
fuzzy logic provides mathematical foundation approximate reasoning fuzzy logic proven successful variety applications 
parameters adaptive fuzzy systems clear physical meanings facilitates choice initial values 
furthermore rule information incorporated fuzzy systems systematic way 
stable encoding knowledge means model give correct answer independent system long 
lead robustness noise independent 
extraction knowledge trained neural networks methods potentially applied incorporating refining priori fuzzy knowledge recurrent neural networks 
computational capabilities recurrent neural networks quite powerful 
capable modeling learning state processes automata :10.1.1.29.8351
appropriate tools modeling learning diverse dynamical problems control signal processing abbreviation ffa denote fuzzy finite state automaton remainder 
variety implementations ffas proposed digital systems 
knowledge proof implementations sigmoid activation stable guaranteed convergence correct prespecified membership 
proof previous mapping deterministic finite state automata dfas recurrent neural networks reported 
knowledge proof implementations sigmoid activation stable guaranteed convergence correct prespecified membership 
proof previous mapping deterministic finite state automata dfas recurrent neural networks reported 
contrast dfas set ffa states occupied varying degrees point time states generally reduces size model dynamics system modeled accessible direct interpretation 
control theory point view fuzzy finite state automata shown useful modeling fuzzy dynamical systems particular recurrent neural networks system identification 
computational capabilities recurrent neural networks appropriate tools modeling refining learning systems current state depends previous states inputs lot interest learning synthesis extraction finite state automata recurrent neural networks :10.1.1.117.1928:10.1.1.29.8351
variety neural network implementations ffas proposed 
previously shown fuzzy finite state automata ffas mapped recurrent neural networks second order weights crisp representation ffa states :10.1.1.22.7691
encoding required transformation ffa deterministic finite state automaton dfa computes membership functions strings applicable restricted class ffas final states 
transformation fuzzy automaton equivalent deterministic acceptor generally increases size automaton network size 
contrast dfas set ffa states occupied varying degrees point time states generally reduces size model dynamics system modeled accessible direct interpretation 
control theory point view fuzzy finite state automata shown useful modeling fuzzy dynamical systems particular recurrent neural networks system identification 
computational capabilities recurrent neural networks appropriate tools modeling refining learning systems current state depends previous states inputs lot interest learning synthesis extraction finite state automata recurrent neural networks :10.1.1.117.1928:10.1.1.29.8351
variety neural network implementations ffas proposed 
previously shown fuzzy finite state automata ffas mapped recurrent neural networks second order weights crisp representation ffa states :10.1.1.22.7691
encoding required transformation ffa deterministic finite state automaton dfa computes membership functions strings applicable restricted class ffas final states 
transformation fuzzy automaton equivalent deterministic acceptor generally increases size automaton network size 
furthermore fuzzy transition memberships original ffa undergo modifications transformation original ffa equivalent dfa suitable implementation second order recurrent neural network 
direct correspondence system network parameters lost may obscure natural fuzzy description systems modeled 
direct correspondence system network parameters lost may obscure natural fuzzy description systems modeled 
existence crisp recurrent network encoding ffas raises question recurrent networks trained compute fuzzy membership function represent ffa states internally 
theoretical analysis know ability represent ffas form equivalent deterministic acceptors 
reported addresses questions 
augmenting second order network linear output layer computing fuzzy string membership suggested authors chose assign fuzzy string memberships occurring training set individual output neurons :10.1.1.22.7691
transformed fuzzy inference problem binary inference problem multiple classes 
approach burden training improves accuracy robustness string membership computation 
apart multiple classes training networks compute fuzzy string membership identical training networks behave dfas 
authors empirically verified information extraction recurrent networks trained fuzzy strings develop crisp internal representation ffas represent ffas form equivalent deterministic acceptors 
mapping fuzzy automata recurrent network requires ffas transformed equivalent ffas eliminate ambiguities fuzzy transitions memberships lead single ffa state transformation algorithm section 
recurrent network architecture representing ffas described section 
stability encoding derived section 
discussion simulation results section summary results possible directions research section conclude 
reasons completeness included main results laid foundations papers :10.1.1.22.7691
necessity overlap 
example fuzzy finite state automaton fuzzy finite state automaton shown weighted state transitions 
state automaton start state 
transition state input symbol weight represented directed arc labeled 
significant change observed comparing histograms existence significant neuron output errors suggests internal ffa representation highly unstable 
internal ffa state representation stable 
discontinuous change explained observing exists critical value number stable fixed points changes respectively see 
smooth transition large output errors small errors recurrent state neurons figures explained observing recurrent state neurons receive number residual inputs neurons may receive residual input input symbol time step case low signals neurons strengthened note strong low signals imply strong high signals lemma 
previously shown possible deterministically encode fuzzy finite state automata ffa recurrent neural networks transforming ffa deterministic acceptor assign string membership :10.1.1.22.7691
deterministic encoding network classification strings fuzzy representation states crisp 
correspondence ffa network parameters fuzzy transition memberships network weights respectively lost transformation 
demonstrated analytically empirically possible encode ffas recurrent networks transforming deterministic acceptors 
constructed network directly represents ffa states desired fuzziness 

omlin giles stable encoding large finite state automata recurrent neural networks sigmoid discriminants neural computation vol 
pp 

omlin giles fuzzy finite state automata deterministically encoded recurrent neural networks ieee transactions fuzzy systems vol :10.1.1.22.7691
pp 

mamdani fuzzy logic controller traffic junction ieee transactions systems man cybernetics vol :10.1.1.22.7691
smc pp 

omlin giles fuzzy finite state automata deterministically encoded recurrent neural networks ieee transactions fuzzy systems vol :10.1.1.22.7691
pp 

mamdani fuzzy logic controller traffic junction ieee transactions systems man cybernetics vol :10.1.1.22.7691
smc pp 

pollack induction dynamical recognizers machine learning vol :10.1.1.29.8351
pp 

mamdani fuzzy logic controller traffic junction ieee transactions systems man cybernetics vol :10.1.1.22.7691
smc pp 

pollack induction dynamical recognizers machine learning vol :10.1.1.29.8351
pp 

santos automata information control vol 
pp 
