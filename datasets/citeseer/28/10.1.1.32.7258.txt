hierarchical memory reinforcement learning hernandez arti cial intelligence lab massachusetts institute technology cambridge ma ai mit edu sridhar mahadevan department computer science michigan state university east mi cse edu key challenge reinforcement learning scaling large partially observable domains 
show hierarchy behaviors create select variable length short term memories appropriate task 
higher levels hierarchy agent abstracts lower level details looks back variable number high level decisions time 
formalize idea framework called hierarchical sux memory hsm 
hsm uses memory smdp learning method rapidly propagate delayed reward long decision sequences 
describe detailed experimental study comparing memory vs hierarchy hsm framework realistic corridor navigation task 
hsm uses memory smdp learning method rapidly propagate delayed reward long decision sequences 
describe detailed experimental study comparing memory vs hierarchy hsm framework realistic corridor navigation task 
reinforcement learning encompasses class machine learning problems agent learns experience interacts environment 
fundamental challenge faced reinforcement learning agents real world problems state space large consequently may long delay reward received 
previous addressed issue breaking large task hierarchy subtasks behaviors :10.1.1.29.4106:10.1.1.76.7134:10.1.1.9.313
dicult issue problem perceptual aliasing di erent real world states generate observations 
strategy deal perceptual aliasing add memory past percepts 
short term memory consisting linear tree sequence primitive actions observations shown useful strategy 
considering short term memory uniform resolution primitive actions scale poorly tasks long decision sequences 
attempting choose actions greedy history match usm tries explicitly determine memory useful predicting reward 
agent builds tree structure state representation online selectively adding depth tree additional history distinction helps predict reward 
usm learns model hsm updates values doing sweep value iteration leaves tree states 
implement hierarchy behaviors principle hierarchical reinforcement learning method may 
implementation hierarchy machines ham framework proposed parr russell :10.1.1.29.4106
executed machine executes partial policy returns control caller termination 
ham architecture uses learning rule modi ed smdps 
context state represented history sux 
instance state instance incoming history matches sux representing state 
autonomous robots journal special issue learning autonomous robots 
andrew mccallum 
reinforcement learning selective perception hidden state 
phd thesis university rochester 
ron parr :10.1.1.29.4106
hierarchical control learning markov decision processes 
phd thesis university california berkeley 
dana ron yoram singer naftali tishby 
power learning probabilistic automata variable mem ory length 
