ects training set size decision tree complexity presents experiments datasets decision tree pruning algorithms show increasing training set size results linear increase tree size additional complexity results signi cant increase classi cation accuracy 
said di erently removing randomly selected training instances results trees substantially smaller just accurate built available training instances 
implies decreases tree size obtained sophisticated data reduction techniques decomposed parts reduction training set size remainder due method selects instances discard 
perform decomposition data reduction technique john robust john show large percentage ect tree size attributable fact simply reduces size training set 
conclude random data reduction baseline sophisticated data reduction techniques compared 
examine possible cause pathological relationship tree size training set size 
observe ltering unanticipated side ect leads substantially smaller trees brodley 
argue broad range circumstances data reduction techniques result decrease tree size little impact accuracy 
section ers detailed empirical evidence validity claim intuitive feeling grasped looking 
gure shows plots tree size accuracy function training set size uc irvine australian dataset 
generate trees quinlan plot corresponds di erent pruning mechanism error default quinlan reduced error rep quinlan minimum description length mdl quinlan rivest cost complexity se rule ccp se breiman cost complexity se rule ccp se :10.1.1.167.3624
left hand side graphs training instances available best test instances assign class label random 
right hand side graph entire dataset excluding test instances available tree building process 
movement left right corresponds addition randomly selected instances training set 
alternatively moving right left corresponds removing randomly selected instances training set 
general additional tree structure welcome long improves classi cation accuracy 
ideally correlation tree size training set size classi cation accuracy peaks 
linear regression tree size training set size indicates probability making error rejecting null hypothesis correlation slope regression line zero amount tree size accounted training set size signi cant andr high changes training set size strong predictable ects tree size 
relationship training set size tree size explored pruning methods datasets taken uc irvine repository 
pruning methods error default quinlan reduced error rep quinlan minimum description length mdl quinlan rivest cost complexity se rule ccp se breiman cost complexity se rule ccp se :10.1.1.167.3624
majority extant pruning methods take general approaches accuracy estimates training set pruning accuracy estimates pruning set rep creating set pruned trees di erent values parameter selecting appropriate parameter value pruning set cross validation ccp se ccp se managing tradeo accuracy complexity mdl 
pruning methods selected representative approaches 
ccp se included determine impact se rule cost complexity pruning 
plots tree size accuracy function training set size generated combination dataset pruning algorithm follows 
despite fact adding training instances little ect accuracy doing large ect tree size 
trees built training instances larger accurate trees built small subset training instances 
strong relationship tree size training set size technique removes training instances prior tree construction result smaller trees just reducing size training set 
realization small numbers training instances su ce build small accurate trees addition yielding useful tree simpli cation tool frees data previously tree construction purposes 
example pruning techniques divide training set disjoint subsets building tree pruning quinlan cestnik bratko mingers :10.1.1.167.3624
larger pruning sets result better estimates classi cation accuracy ective pruning 
random data reduction simultaneously produces smaller trees data available pruning 
random data reduction serve method evaluating new pruning techniques 
continued growth tree size associated increase accuracy points problem tting experiments described section determine extent ofthe problem pruning method 
