multi conditional learning generative discriminative training clustering classification andrew mccallum chris pal greg wang department computer science governors drive university massachusetts amherst ma mccallum pal cs umass edu presents multi conditional learning mcl training criterion product multiple conditional likelihoods 
combining traditional conditional probability label input generative probability input label acts surprisingly effective regularizer 
applied models latent variables mcl combines structure discovery capabilities generative topic models latent dirichlet allocation exponential family accuracy robustness discriminative classifiers logistic regression conditional random fields 
results standard text data sets showing significant reductions classification error due mcl regularization substantial gains precision recall due latent structure discovered mcl 
conditional probability training form maximum entropy classifiers berger conditional random fields crfs lafferty sutton mccallum dramatic growing impact natural language processing information retrieval computer vision bioinformatics related fields 
discriminative models tend overfit training data prior parameters typically provides limited relief 
fact shown cases generative na bayes classifiers provide higher accuracy conditional maximum entropy classifiers ng jordan 
consider alternative training criteria reduced reliance parameter priors combine generative discriminative learning 
presents multi conditional learning family parameter estimation objective functions product multiple conditional likelihoods 
configuration approach objective function weighted product discriminative probability label input generative probability input label 
aims find decision boundary aims model density input single set parameters na bayes structured model strives 
regularizers provide additional copyright american association artificial intelligence www aaai org 
rights reserved 
constraints parameter estimation 
experimental results variety standard text data sets show density estimation constraint effective regularizer shrinkage zero basis traditional regularizers gaussian prior reducing error nearly cases 
improving accuracy inclusion density estimation criterion helps improve confidence prediction 
addition simple conditional models growing interest conditionally trained models latent variables jebara pentland mccallum 
simultaneously immense interested generative topic models latent dirichlet allocation undirected analogues including models welling xing smolensky 
demonstrate multi conditional learning applied latent variable models 
mcl discovers latent space projection captures cooccurrence features input generative models provides ability accurately predict designated outputs discriminative models 
find mcl robust conditional criterion purposeful generative latent variable models 
document retrieval task introduced welling 
find mcl doubles precision recall comparison generative 
latent variable models mcl seen form semi supervised clustering flexibility operate relational structured crf models principled way 
mcl aims combine strengths crfs handling auto correlation non independent input features making predictions strengths topic models discovering occurrence patterns useful latent projections 
sets stage various interesting multi conditional learning 
configurations multi conditional learning possible including ones conditional probabilities 
example transfer learning naturally configured product conditional probabilities labels task latent variables parameters shared 
semi supervised learning configured product conditional probabilities predicting label predicting input 
configurations subject ongoing 
multi conditional learning mrfs exposition general framework multi conditional learning 
derive equations multi conditional learning structured markov random field mrf models 
introduce discrete hidden sub class variables na mrf models creating multi conditional mixtures discuss multi conditional methods derived 
construct binary word occurrence models coupled hidden continuous variables exponential family demonstrating advantages multi conditional learning models 
mcl framework consider data set consisting instances 
construct probabilistic models consisting discrete observed random variables discrete hidden variables continuous hidden variables denote outcome random variable define ns pairs disjoint subsets observations xa ij xb ij indices denote ith instance variables subset construct multi conditional objective product different conditional probabilities involving subsets weight contributions different conditionals 
definitions optimal parameter settings multi conditional criterion argmax xa ij xb ij ij derive marginal conditional likelihoods single underlying joint probability model parameters 
underlying joint probability model may normalized locally globally combination 
experiments partition observed variables set labels set features define pairs subsets xa xb xa xb 
construct objective functions lmc form lmc log 
configuration think objective having generative component discriminative component 
attractive definition pairs xa xb xa xb giving rise objectives form log represents way restructuring joint likelihood concentrate modeling power conditional distribution interest 
objective similar approach advocated minka 
na mrfs documents graphical descriptions na bayes model text documents nigam multinomial logistic regression maximum entropy berger model written similar na graphical structures 
consider na mrfs represented similar graphical structure define joint distribution terms unnormalized potential functions 
consider data yn xj mn instances instance mn realizations discrete random variables 
yn denote single discrete random variable class label 
model parameters denoted 
collection documents mn word events document 
joint distribution data modeled set na mrfs observation xmn mn xj 
xmn mn xj 
define potential functions consist exponentiated linear functions multinomial variables sparse vectors single dimensions labels wj word na mrf written exp mn wj 
simplify presentation consider combining multinomial word variables mn wj 
combine exp yt model optimize lmc exp yt exp yt exp yt 
mn exp exp 
wmn gradients log conditional likelihoods contained objective computed ly exp yt xn exp yt xn xy xy denotes expectation respect distribution denote empirical distribution data distribution obtained placing delta labels xmn word events labels hidden topics xmn word events left factor graph kschischang na mrf 
right factor graph mixture na mrfs 
models word occurrence draw discrete random variable mn random variables document function data point normalized compute observe mn mn wj mixtures na mrfs exp wj exp yt mn wj wj np wj yn 
extend basic na mrf model shown left adding hidden subclass variable illustrated right 
mixture na mrfs joint distribution data observation modeled mn xj potential encodes sparse compatibility function relating labels classes subset states hidden discrete variable optimize mixture na mrfs expected gradient algorithm 
model compute gradient complete log likelihood gradient decomposes respect expectation computation efficiently performed lx lnp lnp 
example gradient weights xe zs comprising elements potential function parameters computed mn lx zn yn zs xj zn xe zs zn zn yn zs xj zn zn zs binary feature functions evaluating state xe state zs 
updates potentials function parameters take form similar standard maximum entropy gradient computations augmented hidden variable 
term mixture models trained multi conditional learning multi conditional mixtures mcm 
structured models model smolensky layer markov random field mrf consisting observed variables hidden variables 
mrfs model defined terms globally normalized product unnormalized potential functions defined subsets variables 
described type restricted boltzmann machine hinton 
new type exponential family multi attribute extending models welling 
dual wing xing 

exponential family structured model written exp xi fj zj ij xi zj vector continuous valued hidden variables vector observations represents parameter vectors weights ij represents parameter vector cross product states fi denotes feature functions ij set parameters function normalization constant 
model factorizes third term ij xi zj fi xi zj wt ij parameter matrix dimensions rows equal number states fi xi columns equal number states fj zj 
models construct binary word occurrence vectors dimension mv size vocabulary 
contrast models previous section different number discrete word events mn document denote observed input variables xd discrete label denoted 
illustrates multi attribute model factor graph 
represents factorization joint distribution observed hidden variables zk zn binary word occurrences hidden topics observed variables domain labels factor graph multi attribute model layer mrf 
globally normalized product local functions 
experiments shall factorization structure define mrf define sets marginal conditionals distributions observed variables particular interest form multi conditional objective 
importantly globally normalized joint distribution construction possible derive consistent conditional models hidden variables observed variables observed variables hidden variables welling 
conditional distributions defined models implement sampling schemes various probabilities underlying joint model 
important remember original model parameterization defined terms conditional distributions 
experiments joint model form defined wt wt wt exponential family conditional distributions consistent joint model zn zn xb xb wb xd xd wd represent normal bernoulli discrete distributions respectively 
equation represent marginal distribution exp combines labels model discrete random variable xd features binary variables 
exponential family model exponential function easy verify gradient log marginal likelihood observed data expressed denotes expectation empirical distribution expectation models marginal distribution number data elements 
compute gradient log likelihood respect weight matrix nd ns wt nd xi ns nd number vectors observed data samples indexed ns number samples data vector computed gibbs sampling conditionals 
experiments possible small number markov chain monte carlo mcmc andrieu steps initialized data vector contrastive divergence approach hinton 
standard mcmc approximations expectations possible 
straightforward gradient optimization model parameters learning rate momentum term 
conditional likelihood multi conditional likelihood learning gradient values obtained lmc xb xd xb xd xb xd xd xb xb xd xb xd relationships xb xd theoretical empirical results ng jordan supported notion discriminative model may lower asymptotic error data error rate classifications analogous generative model approach asymptotically higher error rate faster 
hybrids methods combining generative discriminative methods appealing potential draw strengths approaches 
example 
high dimensional subset parameters trained joint likelihood objective smaller subset parameters trained conditional likelihood objective 
contrast approach parameters optimized number conditional objectives 
jaakkola method characterized information regularization formulated information marginal density unlabeled data constrain free conditional distribution 
approach thought method penalizing decision boundaries occur areas high marginal density 
terms regularization perspective approach uses additional auxiliary conditional distributions derived underlying joint probability model regularizers 
furthermore approach defined context underlying joint model 
belief additional conditional distributions objective function serve regularizer conditional distributions primarily care probability labels 
weight conditional distributions differently objective 
equal weighting conditionals appropriate definition subsets variables method seen type pseudo likelihood besag 
goals quite different trying approximate joint likelihood wish explicitly optimize conditional distributions objective 
mixtures na mrfs resemble multiple mixture components class approach nigam 

conditional distributions arising labels data related mixtures experts jordan jacobs conditional mixture models jebara pentland simple mixtures maximum entropy models pavlov mixtures conditional random fields mccallum 
continuous latent variable model similar dual wing layer random field xing 
mining text images 
approach lower dimensional representation image text data obtained optimizing joint likelihood model 
experimental results section experimental results objective functions context models described 
apply na markov random fields document classification show multi conditional training provides better regularization traditional gaussian prior 
demonstrate mixture forms model real synthetic data including example topic discovery 
show models multi conditional objective provides quantitatively better latent space 
na mrfs mcl regularization objective function na mrfs compare generative na bayes model discriminative maximum entropy model document classification 
extensive experiments common text data sets briefly described 
newsgroups corpus approximately newsgroup messages 
entire corpus abbreviated news subsets talk comp 
industry sector corpus collection corporate webpages split categories 
entire corpus sector subsets healthcare financial technology 
movie review corpus movie collection user movie reviews internet movie database compiled bo pang cornell university 
polarity data set task classify sentiment review positive negative 
data set consists usenet articles discussion groups simulated auto racing simulated aviation real autos real aviation 
web knowledge base webkb data set consists webpages universities classified faculty student course project discard categories staff department 
determine weights component objective function gaussian prior variance cross validation 
specifically fold crossvalidation folds choosing parameters folds testing 
models tend quite sensitive values 
additionally longer guarantee convexity thoughtful initialization parameters required 
hope thoroughly understand control engineering issues 
preprocessing remove words occur corpus stopwords html email message headers 
test small vocabulary versions data set vocabulary size reduced information gain 
results table 
parenthesized values standard deviations test accuracy cross validation folds 
data sets show improvements maximum entropy na bayes 
differences accuracy small cases trend data sets illustrates potential mcl regularization 
fact difference mean accuracy maximum entropy mcl larger difference mean accuracies na bayes maximum entropy 
data sets mean mcl accuracy significantly greater mean accuracies naive bayes maximum entropy tailed paired test 
data sets calculated area accuracy coverage curve mcl provided better confidence estimates 
mixtures na mrfs order demonstrate ability multi conditional mixtures successfully classify data linearly separable perform synthetic data experiments 
class labels associated dimensional gaussians having means variances uniformly sampled 
positions data points generated gaussians rounded integer values 
samples gaussian means variances xor configuration significant portion data misclassified best linear separator 
learn combine multiple linear decision boundaries 
mcm hidden subclasses class attains accuracy na bayes maximum entropy non mixture multi conditional na mrfs accuracies respectively 
explicitly constructed xor positioning mcm attains yield 
running talk data set yields topics similar latent dirichlet allocation lda blei parameter estimation driven discover topics re generate words help predict class label mcm understood semi supervised topic model 
furthermore mcm topics data naive bayes maxent mcl news news comp comp talk talk sector sector tech tech health health movie movie webkb webkb mean topic gun control topic incident guns nra texas assault gun enforcement compound 
president employer peace cult years terrorists matthew table mcm discovered topics associated politics guns label run talk data set 
left discussion gun control texas 
words prominent classes including politics misc 
right discussion gun rights david federal agents compound tx 
aspects cult discussed religion misc 
table document classification accuracies naive bayes maximum entropy mcl 
defined positive word associations prominent negative word associations 
words positive negative shown table 
lower variance conditional mixture estimation consider data generated classes sub classes drawn isotropic gaussians similar example jebara pentland 
data illustrated red blue 
joint conditional multi conditional likelihood fit mixture models diagonal covariance na subclasses conditional expected gradient optimization 
depicts parameters best models objectives ellipses constant probability model 
illustrative example see parameters estimated joint likelihood completely fail classify versus location 
contrast conditional objective focuses completely decision boundary random initializations produced parameters high variance little interpretability 
multi conditional objective optimizes class label prediction class conditioned density yielding classification accuracy sensible parameter estimates 
multi conditional interested quality latent representations obtained optimizing multi attribute structured models standard joint maximum likelihood ml conditional likelihood cl multi conditional likelihood mcl objectives 
similar testing strategy welling 
focus comparing different latent spaces obtained various optimization objectives 
welling 
left joint likelihood optimization 
middle near optimal solutions conditional likelihood optimization 
right optimal solution objective 
reduced newsgroups data set prepared matlab sam roweis 
data set documents represented word vocabulary binary occurrences labeled domains 
evaluate quality latent space retrieve documents domain label test document cosine coefficient latent space observing binary occurrences 
randomly split data training set documents test set documents 
joint model corresponding full rank multi variate bernoulli conditional binary word occurrences discrete conditional domains 
shows precision recall results 
ml model domain label information 
ml optimized domain label information 
cl optimized predict domains words mcl optimized predict words domains domains words 
see latent space captured model relevant domain classification model optimized cl mcl objectives 
mcl doubles precision recall reasonable values counterparts 
discussion multi conditional learning context na mrfs mixtures na mrfs models 
naive mrfs show multi conditional learning provides improved precision recall ml ml cl mcl precision recall curves newsgroups data ml cl mcl latent variables 
random guessing horizontal line 
flexible robust mixtures 
context structured models experiments show multi conditional contrastive divergence optimization procedures lead latent document spaces superior quality 
multi conditional learning suited multi task semi supervised learning multiple prediction tasks easily naturally defined mcl framework 
ando zhang semisupervised multi task learning methods combined 
approach involves auxiliary prediction problems defined unlabeled data model structures arising tasks useful classification problem particular interest 
approach involves finding principal components parameters space auxiliary tasks 
similarly mcl approach define auxiliary conditional distributions features 
way mcl natural framework semi supervised learning 
presently exploring mcl multitask semi supervised settings 
supported part center intelligent information retrieval part central intelligence agency national security agency national science foundation nsf iis part defense advanced research projects agency darpa department interior nbc acquisition services division contract number 
ando zhang 

framework learning predictive structures multiple tasks unlabeled data 
journal machine learning research 
andrieu de freitas doucet jordan 

mcmc machine learning 
machine learning 
berger pietra pietra 

maximum entropy approach natural language processing 
computational linguistics 
besag 

statistical analysis non lattice data 
statistician 
blei ng jordan 

latent dirichlet allocation 
journal machine learning research 
jaakkola 

information regularization 
proceedings uncertainty artificial intelligence 
hinton 

training products experts minimizing contrastive divergence 
neural computation 
jebara pentland 

maximum conditional likelihood bound maximization cem algorithm 
neural information processing systems nips 
jebara pentland 

reversing jensen inequality 
nips 
jordan jacobs 

hierarchical mixtures experts em algorithm 
neural computation 
kschischang frey loeliger 

factor graphs sum product algorithm 
ieee transactions information theory 
lafferty mccallum pereira 

conditional random fields probabilistic models segmenting labeling sequence data 
proc 
icml 
mccallum bellare pereira 

conditional random field discriminatively trained finite state string edit distance 
conference uncertainty ai uai 
minka 

discriminative models discriminative training 
msr tr 
ng jordan 

discriminative vs generative classifiers comparison logistic regression naive bayes 
nips 
nigam mccallum thrun mitchell 

text classification labeled unlabeled documents em 
machine learning 
pavlov popescul pennock ungar 

mixtures conditional maximum entropy models 
nec research institute technical report neci 
collins darrell 

conditional random fields object recognition 
nips 
shen ng mccallum 

classification hybrid generative conditional models 
nips 
roweis ghahramani 

optimization em expectation conjugate gradient 
proc 
icml 
smolensky 

information processing dynamical systems foundations harmony theory 
mcclelland eds parallel distributed processing explorations microstructure cognition 
volume foundations 
mit press 
sutton mccallum 

conditional random fields relational learning 
getoor taskar eds statistical relational learning 
mit press 
appear 
welling rosen zvi hinton 

exponential family application information retrieval 
nips 
xing yan hauptmann 

mining associated text images dual wing 
proc 
uncertainty artificial intelligence 
