inductive bias case reasoning systems griffiths bridge july order learn behaviour case reasoners learning systems formalise simple case learner pac learning algorithm case representation hcb oei 
consider naive case learning algorithm cb oeh learns collecting available cases case base calculates similarity counting number features problem descriptions agree 
results concerning consistency learning algorithm give partial results regarding sample complexity 
able characterise cb oeh weak general learning algorithm 
consider sample complexity case learning reduced specific classes target concept application inductive bias prior knowledge class target concepts 
demonstrating case learning improved choosing similarity measure appropriate concept learnt define second case learning algorithm cb learns best possible similarity measure inferred chosen target concept 
equation gives oe oe conclude oe definite similarity measure definition 
having established precise conditions cb oe consistent learner result follows trivially 
corollary similarity measure oe predictive concept space sufficient ensure cb oe pac learning algorithm proof learning algorithm consistent respect concept space learns finite hypothesis space pac learning algorithm concept space ab 
result follows theorem number distinct binary functions defined dn indicating hypothesis space cb oe finite 
pac learnability results case classifiers aka aa hold concepts defined real valued attributes :10.1.1.138.635
result dealing uncountable example space albert aha modify pac learning framework introducing constraints probability distribution example space pac learnability proven 
finite example spaces considered current mean consistent learning algorithm inconsistent ones satisfy classical definition pac learnability definition additional constraints 
pac learnability important basic result establishes samples learning algorithm eventually converge arbitrarily approximations target concept 
addition results corollaries property consistency corollary similarity measure oe predictive concept space target concept ffl case base cb hcb oei proof oe oe predictive take ffl training sample contains exemplar point example space dn theorem guarantees output cb oe consistent clearly function hcb oei output cb oe exactly corollary significance establishing universality case representation clear total function ffl bn case representation 
proposition hcb oe hcb oe proposition shows availability knowledge relevant irrelevant features collapses hypothesis space cb irrelevant hypotheses output cb oeh avoided entirely 
fixed representation size size hypothesis space increase monotonically direct corollary proposition 
gives explanation kind contrast learning curves figures 
result highlight fact knowledge encoded oe learning behaviour cb independent representation size shown results section 
sample complexity cb independent representation size claim easily established sample complexity cb increases number relevant attributes independent size representation easily done considering covering net technique applied case learning algorithms albert aha aa :10.1.1.138.635
definition ffl net cb 
set instances ffl net cb target concept iff set exceptions occurring probability ffl ffl instance ffl element net ffl agree bits relevant target concept 
ffl net iff fx ffl xj ffl delta oe ffl elaborate method constructing bound sample complexity shown aa aka necessary case finite example space dn current context definition covering net sufficient guarantee hypothesis bound sample complexity established assumptions :10.1.1.138.635
definition equivalent requiring equivalence class partition oe hit exemplar training sample apart exceptions occurring sum total probability ffl 
result highlight fact knowledge encoded oe learning behaviour cb independent representation size shown results section 
sample complexity cb independent representation size claim easily established sample complexity cb increases number relevant attributes independent size representation easily done considering covering net technique applied case learning algorithms albert aha aa :10.1.1.138.635
definition ffl net cb 
set instances ffl net cb target concept iff set exceptions occurring probability ffl ffl instance ffl element net ffl agree bits relevant target concept 
ffl net iff fx ffl xj ffl delta oe ffl elaborate method constructing bound sample complexity shown aa aka necessary case finite example space dn current context definition covering net sufficient guarantee hypothesis bound sample complexity established assumptions :10.1.1.138.635
definition equivalent requiring equivalence class partition oe hit exemplar training sample apart exceptions occurring sum total probability ffl 
members classes equivalent purposes classification clearly sufficient ensure correct classifications majority example space 
proposition elements contained sample ffl im fx ffl net cb target concept error hypothesis output cb ffl 
ffl net er cb ffl proof sample ffl assume im fx ffl net cb fx ffl xj ffl delta oe ffl fx ffl xj ffl delta oe gamma ffl 
additionally negative exemplar disagree relevant bit giving neg ffl cb delta oe oe neg 
hcb oe 
ffl delta ffl delta oe cb 
fx ffl xj ffl delta oe fx ffl fx ffl gamma ffl er cb fx ffl ffl 
probability drawing ffl net training sample fixed size calculated exactly aa :10.1.1.138.635
proposition aa lemma aka lemma probability drawing sample ffl net cb wrt ffl mn delta gamma fx ffl je ffl net wrt delta gamma proof oe partition induced dn similarity measure oe contains regions descriptions ffl agree relevant bits description 
subset ffl delta ffl subset ffl delta ffl ffl example sample hits ffl delta delta oe 
long example sample hits ffl probability having example ffl delta oe ffl delta ffl ffl 
probability drawing sample ffl net cb wrt probability drawing sample ffl ffl delta delta ffl specific ffl probability example sample taken gamma ffl probability ffl hit sample jg delta gamma ffl jg jgj result follows immediately inequality gamma corollary sample complexity cb respect concept space mn literal monomials ffl log ffi mn ffi ffl ffl log ffi proof proposition ffl net er cb ffl proposition fx ffl je ffl net wrt delta gamma fx ffl cb fflg fx ffl je ffl net wrt delta gamma large fx ffl cb fflg ffi value ffi specifically ffl log ffi straightforward establish upper bound sample complexity cb independent size representation increasing function ffl ffi corollary poor bound increasing tighter bound section considers better definition covering net derivation sample complexity results blumer 
feature counting approach oe oe disadvantage learning monomial functions small numbers examples hypotheses represented measures contain disjunction 
consider modification vs cbr definition infers hypothesis hcb oe hcb oe case algorithm exactly congruent standard learning algorithm monomial functions val hypothesis output algorithm identical training sample monomial target concept ffl mn difference remains approaches standard algorithm produces hypotheses representation efficiently evaluated implicit case representation variant vs cbr 
clear continuity case learning forms inductive learning especially kinds learning introduced case memory system kind modelled appropriate manipulation similarity measure 
believe reported contributed showing questions problems learning case reasoning systems essentially systems frequently studied field machine learning questions usefully dealt existing techniques field 
aa albert aha :10.1.1.138.635
analyses instance learning algorithms 
aaai proceedings ninth national conference artificial intelligence pages 
ab anthony 
computational learning theory 
