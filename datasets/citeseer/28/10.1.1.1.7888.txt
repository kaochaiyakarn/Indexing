soft margins adaboost gmd gmd klaus 
muller gmd technical report series nc tr august produced part esprit working group neural computational learning ii information see website www com email com gmd berlin germany jp shi tokyo japan klaus gmd de gmd berlin germany received aug ensemble methods adaboost successfully applied character recognition tasks seemingly problems overfitting 
shows adaboost rarely low noise regime clearly higher noise levels 
central understanding fact margin distribution find adaboost achieves doing gradient descent error function respect margin asymptotically hard margin distribution algorithm concentrates resources hard learn patterns interesting overlap emerge support vectors 
clearly sub optimal strategy noisy case regularization data introduced algorithm alleviate distortions difficult pattern outliers cause margin distribution 
propose regularization methods generalizations original adaboost algorithm achieve soft margin concept known support vector learning 
extensive simulations demonstrate proposed regularized adaboost type algorithms useful competitive noisy data 
keywords adaboost arcing large margin hard margin soft margin classification support vectors ensemble collection neural networks types classifiers hypotheses trained task 
boosting ensemble learning methods great success applications ocr 
far reduction generalization error adaboost completely understood 
low noise cases lines explanation proposed candidates explaining functioning boosting methods :10.1.1.31.2869
studies noisy patterns shown clearly myth boosting methods overfit 
try understand adaboost exhibits virtually overfitting low noise strong overfitting high noise data 
propose improvements adaboost achieve noise robustness avoid overfitting 
section analyze adaboost asymptotically 
studies noisy patterns shown clearly myth boosting methods overfit 
try understand adaboost exhibits virtually overfitting low noise strong overfitting high noise data 
propose improvements adaboost achieve noise robustness avoid overfitting 
section analyze adaboost asymptotically 
due similarity refer adaboost unnormalized arcing exponential function adaboost type algorithms ata :10.1.1.116.4582
especially focus error function find function written terms margin iteration adaboost tries minimize function stepwise maximizing margin :10.1.1.30.3515:10.1.1.40.4427
analysis function introduce hard margin concept 
show connections vapnik maximum margin classifiers support vector sv learning linear programming lp :10.1.1.103.1189
bounds size margin 
try understand adaboost exhibits virtually overfitting low noise strong overfitting high noise data 
propose improvements adaboost achieve noise robustness avoid overfitting 
section analyze adaboost asymptotically 
due similarity refer adaboost unnormalized arcing exponential function adaboost type algorithms ata :10.1.1.116.4582
especially focus error function find function written terms margin iteration adaboost tries minimize function stepwise maximizing margin :10.1.1.30.3515:10.1.1.40.4427
analysis function introduce hard margin concept 
show connections vapnik maximum margin classifiers support vector sv learning linear programming lp :10.1.1.103.1189
bounds size margin 
noisy patterns shown adaboost overfit holds boosted decision trees rbf nets kinds classifiers 
section analyze adaboost asymptotically 
due similarity refer adaboost unnormalized arcing exponential function adaboost type algorithms ata :10.1.1.116.4582
especially focus error function find function written terms margin iteration adaboost tries minimize function stepwise maximizing margin :10.1.1.30.3515:10.1.1.40.4427
analysis function introduce hard margin concept 
show connections vapnik maximum margin classifiers support vector sv learning linear programming lp :10.1.1.103.1189
bounds size margin 
noisy patterns shown adaboost overfit holds boosted decision trees rbf nets kinds classifiers 
section explain property adaboost enforce hard analysis adaboost learning process margin necessarily lead overfitting presence noise case overlapping class distributions 
hard margin plays central role causing overfitting propose relax hard margin section allow misclassifications soft margin concept successfully applied support vector machines cf 
bounds size margin 
noisy patterns shown adaboost overfit holds boosted decision trees rbf nets kinds classifiers 
section explain property adaboost enforce hard analysis adaboost learning process margin necessarily lead overfitting presence noise case overlapping class distributions 
hard margin plays central role causing overfitting propose relax hard margin section allow misclassifications soft margin concept successfully applied support vector machines cf 
:10.1.1.15.9362
view margin concept key understanding svms 
far know margin distribution look learner achieve optimal classification noise case large hard margin clearly best choice 
noisy data trade believing data data point mislabeled outlier 
leads regularization reflects prior knowledge problem 
furthermore propose qp adaboost show connections svms 
section numerical experiments artificial realworld data sets show validity competitiveness regularization approach 
concluded brief discussion 
analysis adaboost learning process algorithm fh tg ensemble hypotheses defined input vector weights satisfying 
consider binary classification case results transfered easily classification classes :10.1.1.31.2869
binary classification case output class labels sigma 
ensemble generates label weighted majority votes sign order train ensemble find appropriate hypotheses fh weights convex combination algorithms proposed windowing bagging boosting arcing adaboost 
bagging weighting simply boosting arcing weighting scheme complicated known ensemble learning algorithms 
sequel focus boosting arcing adaboost type algorithms 
binary classification case output class labels sigma 
ensemble generates label weighted majority votes sign order train ensemble find appropriate hypotheses fh weights convex combination algorithms proposed windowing bagging boosting arcing adaboost 
bagging weighting simply boosting arcing weighting scheme complicated known ensemble learning algorithms 
sequel focus boosting arcing adaboost type algorithms 
omit detailed description ata give pseudo code details see :10.1.1.116.4582
binary classification case define margin pair mg analysis adaboost learning process algorithm adaboost oe input examples initialize 
train neural network respect weighted sample set fz wg obtain hypothesis 
sigma 
calculate training error ffl ffl abort ffl ffl oe gamma delta delta small constant 
sigma 
calculate training error ffl ffl abort ffl ffl oe gamma delta delta small constant 
set log ffl gamma oe oe gamma ffl 
update weights exp gammab normalization constant 
output final hypothesis jbj jbj jb adaboost type algorithm ata :10.1.1.40.4427
oe retrieve original adaboost algorithm 
ata specialization unnormalized arcing exponential function 
analysis adaboost learning process denotes number training patterns 
margin positive right class pattern predicted 

error function adaboost important question analysis kind error function optimized 
algorithmic formulation cf 
straight forward understand aim algorithm consider weights hypotheses patterns manner equation remember facts 
weights th iteration chosen previous hypothesis exactly weighted training error ffl :10.1.1.31.2869

weight hypothesis chosen minimizes functional introduced breiman :10.1.1.116.4582
essentially functional depends rate incorrect classification patterns defined gamma exp phi gamma psi oe constant 
functional minimized analytically gets explicit form equation solution 
algorithmic formulation cf 
straight forward understand aim algorithm consider weights hypotheses patterns manner equation remember facts 
weights th iteration chosen previous hypothesis exactly weighted training error ffl :10.1.1.31.2869

weight hypothesis chosen minimizes functional introduced breiman :10.1.1.116.4582
essentially functional depends rate incorrect classification patterns defined gamma exp phi gamma psi oe constant 
functional minimized analytically gets explicit form equation solution 
train th hypothesis step bootstrap replicates training set sampled minimize weighted error function base learning algorithm 
observed convergence ata faster weighted error function 
gradient descent compute gradient error function respect parameters optimized 
corresponds computing gradient respect margins 
second step size direction determined usually line search 
comparable minimization mentioned point list 
adaboost related gradient descent method aims minimize functional constructing ensemble classifiers :10.1.1.30.3515:10.1.1.40.4427
explains point list gradient descent method new search direction perpendicular previous 
analogy perfect 
gap having pattern distribution having classifier 
difficult find classifier minimizes knowing pattern distribution 
mentioned ways incorporating sample distribution 
way create bootstrap replicate sampled pattern distribution 
usually lot random effects hide true information contained distribution 
information lost gap larger 
direct way weighted error function employ weighted minimization breiman need iterations bootstrap weighted minimization fastest convergence obtained uses directly finding hypothesis cf :10.1.1.116.4582
:10.1.1.30.3515
considerations explain point list 
friedman mentioned randomized version shows better performance version weighted minimization :10.1.1.30.3515
connection discussion section clearer randomized version show overfitting effect possibly overfitting observed observed efficient weighted minimization 
way create bootstrap replicate sampled pattern distribution 
usually lot random effects hide true information contained distribution 
information lost gap larger 
direct way weighted error function employ weighted minimization breiman need iterations bootstrap weighted minimization fastest convergence obtained uses directly finding hypothesis cf :10.1.1.116.4582
:10.1.1.30.3515
considerations explain point list 
friedman mentioned randomized version shows better performance version weighted minimization :10.1.1.30.3515
connection discussion section clearer randomized version show overfitting effect possibly overfitting observed observed efficient weighted minimization 
analysis adaboost learning process adaboost annealing process definition equation written exp gamma gamma mg delta jb exp gamma gamma mg delta jb inspecting equation closely see adaboost uses softmax function parameter jbj interpret annealing parameter :10.1.1.40.4427
information lost gap larger 
direct way weighted error function employ weighted minimization breiman need iterations bootstrap weighted minimization fastest convergence obtained uses directly finding hypothesis cf :10.1.1.116.4582
:10.1.1.30.3515
considerations explain point list 
friedman mentioned randomized version shows better performance version weighted minimization :10.1.1.30.3515
connection discussion section clearer randomized version show overfitting effect possibly overfitting observed observed efficient weighted minimization 
analysis adaboost learning process adaboost annealing process definition equation written exp gamma gamma mg delta jb exp gamma gamma mg delta jb inspecting equation closely see adaboost uses softmax function parameter jbj interpret annealing parameter :10.1.1.40.4427
temperature jbj high system state high energy patterns relevant high weights 
temperature goes patterns smallest margin get higher higher weights 
:10.1.1.30.3515
considerations explain point list 
friedman mentioned randomized version shows better performance version weighted minimization :10.1.1.30.3515
connection discussion section clearer randomized version show overfitting effect possibly overfitting observed observed efficient weighted minimization 
analysis adaboost learning process adaboost annealing process definition equation written exp gamma gamma mg delta jb exp gamma gamma mg delta jb inspecting equation closely see adaboost uses softmax function parameter jbj interpret annealing parameter :10.1.1.40.4427
temperature jbj high system state high energy patterns relevant high weights 
temperature goes patterns smallest margin get higher higher weights 
limit arrive maximum function 
pattern highest rate smallest margin considered get non zero weight 
shows margin yf pattern coordinate shows monotone loss pattern loss solid squared error dashed kullback leibler error dash dotted jbj 
left panel oe right plot oe 
oe controls position step loss ae gamma oe asymptotically approximated adaboost loss function 
analysis large margin 
main point explanation ata generalization performance size hard margin achieved :10.1.1.116.4582:10.1.1.31.2869
low noise case hypothesis largest margin generalization performance 
interesting see large margin depending 
generalizing theorem freund case oe get theorem assume ffl ffl weighted classification errors generated running ata oe max ffl inequality holds gamma gamma gamma ffl gamma gamma ffl final hypothesis oe proof appendix corollary ata asymptotically generate margin distributions margin ae bounded ae ln gamma gamma delta ln gamma gamma oe gamma ffl gamma delta ln gamma gamma ln gamma oe gamma ffl gamma ffl max ffl ffl gamma ae satisfied 
analysis adaboost learning process proof maximum ffl gamma gamma ffl respect ffl reached gamma ffl gamma increasing monotonically ffl replace ffl ffl equation ae yf ii gamma gamma ffl gamma gamma ffl basis right hand side smaller asymptotically yf 
analysis adaboost learning process proof maximum ffl gamma gamma ffl respect ffl reached gamma ffl gamma increasing monotonically ffl replace ffl ffl equation ae yf ii gamma gamma ffl gamma gamma ffl basis right hand side smaller asymptotically yf 
asymptotically example smaller margin 
biggest possible margin max max gamma gamma max ffl gamma max gamma ffl max solve equation max get max ln gamma gamma delta ln gamma gamma oe gamma ffl gamma delta ln gamma gamma ln gamma oe gamma ffl gamma get assertion ae bigger equal max equation see interaction oe ffl difference ffl oe small right hand side small 
smaller oe important difference 
theorem weaker bound ae gamma oe oe small ae large choosing small oe results larger margin training patterns :10.1.1.116.4582
increase complexity basis algorithm leads increased ae error ffl decrease 
support patterns decrease functional jbj jbj predominantly achieved improvements margin mg 
margin mg negative error jbj takes clearly big value additionally amplified jbj 
adaboost tries decrease negative margin efficiently improve error jbj 
figures apparent margin distribution asymptotically step fixed size margin training patterns 
see influence noise data strength base hypotheses margin ae 
noise level high complexity low gets higher training errors ffl smaller value ae 
numerical results support theoretical asymptotic analysis 
interestingly margin distributions resembles support vector machines svms separable case cf :10.1.1.103.1189:10.1.1.15.9362

example cf 
patterns support vectors svs lie step part margin distribution adaboost 
adaboost achieves hard margin asymptotically svms separable case 
illustrated shows typical ata margin distributions iterations 
findings section 
adaboost type algorithms aim minimize functional depends margin distribution 
minimization done approximate gradient descent respect margin cf 
:10.1.1.30.3515

annealing part algorithm 
depends annealing parameter jbj controls loss gamma oe approximated 
size margin decided certain annealing process 
training patterns area decision boundary asymptotically margin 
call patterns support patterns 
large overlap svs svm 

asymptotically hard margin achieved comparable original sv approach :10.1.1.103.1189

larger hard margins achieved ffl oe small cf 
corollary 
low noise case choice lead better generalization performance shown ocr :10.1.1.40.4427
asymptotically hard margin achieved comparable original sv approach :10.1.1.103.1189

larger hard margins achieved ffl oe small cf 
corollary 
low noise case choice lead better generalization performance shown ocr :10.1.1.40.4427
hard margin overfitting mg typical margin distribution graphs original adaboost dotted dash dotted dashed solid iterations 
toy example patterns oe rbf networks centers 
iterations convergence reached 
mg typical margin distribution graphs normalized svm hard margin solid soft margin gamma dashed gamma dash dotted 
give examples hard margin approach fail general noise 
understanding noisy data properties overlapping class probability distributions outliers mislabeled patterns 
kinds noise appear data analysis 
development noise robust version adaboost important 
theoretical analysis adaboost connection margin distributions done schapire :10.1.1.31.2869
main result bound generalization error mg depending base hypotheses class margin distribution training set 
probability gamma ffi mg zz mg log log ffi satisfied denotes number patterns 
stated reason success adaboost compared ensemble learning methods bagging maximization margin 
experimentally observed adaboost maximizes margin patterns difficult smallest margin 
increasing minimum margin patterns adaboost reduces margin rest patterns 
hard margin overfitting number iterations typical overfitting behavior generalization error smoothed function number iterations left typical decision line right generated adaboost iterations rbf networks centers case noisy data patterns oe 
positive negative training patterns shown respectively support patterns marked 
approximation bayes decision line plotted dashed 
breiman connection smallest margin generalization error analyzed experimentally confirmed noisy data :10.1.1.116.4582
grove linear programming lp approach freund breiman extended maximize smallest margin existing ensemble classifiers :10.1.1.116.4582
experiments uci benchmarks noisy data unexpectedly observed lp adaboost performs cases worse original adaboost algorithm smallest margins larger 
experiments shown margin increases generalization performance better datasets noise ocr noisy data observed adaboost moderate number combined hypotheses :10.1.1.40.4427
example overlapping classes left shows typical overfitting behavior generalization error adaboost data section 
hard margin overfitting number iterations typical overfitting behavior generalization error smoothed function number iterations left typical decision line right generated adaboost iterations rbf networks centers case noisy data patterns oe 
positive negative training patterns shown respectively support patterns marked 
approximation bayes decision line plotted dashed 
breiman connection smallest margin generalization error analyzed experimentally confirmed noisy data :10.1.1.116.4582
grove linear programming lp approach freund breiman extended maximize smallest margin existing ensemble classifiers :10.1.1.116.4582
experiments uci benchmarks noisy data unexpectedly observed lp adaboost performs cases worse original adaboost algorithm smallest margins larger 
experiments shown margin increases generalization performance better datasets noise ocr noisy data observed adaboost moderate number combined hypotheses :10.1.1.40.4427
example overlapping classes left shows typical overfitting behavior generalization error adaboost data section 
adaboost iterations best generalization performance achieved 
approximation bayes decision line plotted dashed 
breiman connection smallest margin generalization error analyzed experimentally confirmed noisy data :10.1.1.116.4582
grove linear programming lp approach freund breiman extended maximize smallest margin existing ensemble classifiers :10.1.1.116.4582
experiments uci benchmarks noisy data unexpectedly observed lp adaboost performs cases worse original adaboost algorithm smallest margins larger 
experiments shown margin increases generalization performance better datasets noise ocr noisy data observed adaboost moderate number combined hypotheses :10.1.1.40.4427
example overlapping classes left shows typical overfitting behavior generalization error adaboost data section 
adaboost iterations best generalization performance achieved 
equation clear adaboost asymptotically achieve positive margin oe training patterns classified possibly wrong labels cf 
right complexity combined hypotheses increases 
larger second term smaller 
mason similar bound optimize margin distribution piecewise linear approximation directly 
approach successful noisy data maximization smallest margin 
introduce possibility parts data leads soft margin concept 
improvements soft margin original sv algorithm similar problems ata respect hard margins :10.1.1.103.1189
sv approach training errors data overlapping classes allowed generalization performance poor noisy data 
soft margins gave new algorithm achieved better results compared original algorithm cf :10.1.1.15.9362

improvements soft margin sequel show soft margin idea 
approach successful noisy data maximization smallest margin 
introduce possibility parts data leads soft margin concept 
improvements soft margin original sv algorithm similar problems ata respect hard margins :10.1.1.103.1189
sv approach training errors data overlapping classes allowed generalization performance poor noisy data 
soft margins gave new algorithm achieved better results compared original algorithm cf :10.1.1.15.9362

improvements soft margin sequel show soft margin idea 
section change error function introducing new term controls importance pattern compared achieved margin 
section show soft margin idea built lp adaboost algorithm section show extension quadratic programming qp adaboost connections support vector approach 
margin vs influence pattern propose improvement original adaboost regularization term analogy weight decay 
define influence pattern combined hypotheses weighted average weight pattern computed ata learning process cf 
pseudo code 
pattern misclassified difficult classify high average weight high influence 
definition influence clearly depends base hypotheses space corollary theorem training patterns get margin mg larger equal gamma oe iterations cf :10.1.1.116.4582
discussion section 
asymptotically get inequalities mg ae ae gamma oe better bounded equation 
see relation ae sufficient large value jbj equation minimized ae maximized 
iterations inequalities satisfied long oe hard margin ae achieved lead overfitting case noise :10.1.1.31.2869
definition influence clearly depends base hypotheses space corollary theorem training patterns get margin mg larger equal gamma oe iterations cf :10.1.1.116.4582
discussion section 
asymptotically get inequalities mg ae ae gamma oe better bounded equation 
see relation ae sufficient large value jbj equation minimized ae maximized 
iterations inequalities satisfied long oe hard margin ae achieved lead overfitting case noise :10.1.1.31.2869
consider case oe generalizations straight forward 
define soft margin pattern mg trade margin influence pattern final hypothesis follows mg mg fixed constant fixed exponent 
modify trade 
reformulate adaboost optimization process terms soft margins 
solved linear programming maximize ae subject ae linear program achieves larger hard margin original adaboost algorithm 
reasoning section lp adaboost generalize noisy data stronger difficult patterns outliers 
define soft margin pattern mg mg introduce regularization lp adaboost 
technically approach equivalent slack variables lp adaboost arrive algorithm lp reg adaboost solves linear program improvements soft margin maximize ae gamma subject ae gamma modification allows patterns smaller margins ae especially lower 
trade margins bigger ae maximize ae gamma trade controlled constant quadratic programming connection support vector machines section extend lp reg adaboost algorithm quadratic programming similar techniques support vector machines :10.1.1.103.1189:10.1.1.15.9362
gives interesting insights connection svms adaboost 
start transforming lp reg adaboost algorithm maximizes ae jcj kept fixed linear program ae fixed jbj minimized 
unfortunately equivalent linear program 
taylor expansions get linear program compare linear programming approaches related sv learning minimize subject gamma essentially algorithm slack variables acting differently taylor expansion jbj 
partial funding ec storm project number gratefully acknowledged 
proof lemma proof define exp gammab definition get mg mg exp gamma gamma mg delta exp gamma gamma mg delta 
definition gamma exp gammab get gamma exp gammab gamma gamma exp gammab gamma exp gammab gamma cf 
step 
proof theorem proof theorem proof follows theorem :10.1.1.31.2869
theorem generalization oe proof yf exp gamma yf exp gamma exp exp gamma exp gamma exp gamma gamma exp gamma ht exp gamma gamma gammab ht exp gamma gamma bt exp gamma gamma gamma ffl gammab ffl bt ffl ht exp get recursively yf exp gamma ffl gammab ffl rbf nets adaptive centers plugging definition get yf gamma ffl ffl oe gamma oe delta delta oe gamma oe gamma oe oe gamma ffl ffl oe gamma oe oe gamma oe oe delta delta gamma ffl ffl gamma gamma gamma ffl gamma gamma ffl rbf nets adaptive centers rbf nets experiments extension method moody darken centers variances adapted see 
output network computed linear superposition basis functions denotes weights output layer 
gaussian basis functions defined exp gamma kx gamma oe oe denote means variances respectively 
step means initialized means clustering variances oe determined distance closest kg 
springer 
bishop 
neural networks pattern recognition 
clarendon press oxford 
boser guyon vapnik :10.1.1.103.1189
training algorithm optimal margin classifiers 
haussler editor th annual acm workshop colt pages pittsburgh pa 
acm press 
breiman 
technical report statistics department university california june 
breiman 
bias variance arcing classifiers 
technical report statistics department university california july 
breiman :10.1.1.116.4582
prediction games arcing algorithms 
technical report statistics department university california december 
cortes vapnik :10.1.1.15.9362
support vector networks 
technical report statistics department university california july 
breiman :10.1.1.116.4582
prediction games arcing algorithms 
technical report statistics department university california december 
cortes vapnik :10.1.1.15.9362
support vector networks 
machine learning 
freund schapire 
decision theoretic generalization line learning application boosting 
conf 
comput 
learning theory pages 
acm press new york ny 
friedman hastie tibshirani :10.1.1.30.3515
additive logistic regression statistical view boosting 
technical report department statistics sequoia hall stanford july 
harrison 
perceptrons kernel feature space 
support vector machines time series prediction 
scholkopf burges smola editors advances kernel methods support vector learning 
mit press cambridge ma 
appear 
:10.1.1.40.4427
muller 
asymptotic analysis adaboost binary classification case 
proc 
int 
ensemble learning methods classification 
april 
diploma thesis german 
www gmd de ps gz 
schapire singer :10.1.1.31.2869
improved boosting algorithms confidence rated predictions 
proceedings colt march 
schapire freund bartlett lee :10.1.1.31.2869
boosting margin new explanation effectiveness voting methods 
www gmd de ps gz 
schapire singer :10.1.1.31.2869
improved boosting algorithms confidence rated predictions 
proceedings colt march 
schapire freund bartlett lee :10.1.1.31.2869
boosting margin new explanation effectiveness voting methods 
proc 
th international conference machine learning pages 
morgan kaufmann 
