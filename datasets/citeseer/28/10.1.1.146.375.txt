ieee transactions neural networks vol 
september improvements smo algorithm svm regression keerthi bhattacharyya murthy points important source inefficiency smola sch lkopf sequential minimal optimization smo algorithm support vector machine svm regression caused single threshold value 
clues kkt conditions dual problem threshold parameters employed derive modifications smo regression 
modified algorithms perform significantly faster original smo datasets tried 
index terms quadratic programming regression sequential minimal optimization smo algorithm support vector machine svm 
support vector machine svm elegant tool solving pattern recognition regression problems 
past years attracted lot researchers neural network mathematical programming community main reason ability provide excellent generalization performance 
svms demonstrated valuable real world applications 
address svm regression problem 
smola sch lkopf proposed iterative algorithm called sequential minimal optimization smo solving regression problem svm :10.1.1.11.2062
algorithm extension smo algorithm proposed platt svm classifier design :10.1.1.11.2062
computational speed ease implementation noteworthy features smo algorithm 
improvements platt smo algorithm svm classifier design suggested 
extend ideas smola sch lkopf smo algorithm regression 
improvements suggested enhance value smo regression 
particular point important source inefficiency caused way smo maintains updates single threshold value 
getting clues optimality criteria associated karush kuhn tucker kkt conditions dual problem suggest threshold parameters devise modified versions smo regression efficient original smo 
computational comparison datasets show modifications perform significantly better original smo 
organized follows 
section ii briefly discuss svm regression problem formulation dual problem associated kkt optimality conditions 
point conditions lead proper criteria terminating algorithms designing svm regression 
section iii gives brief overview smo algorithm regression 
section iv point inefficiency associated way smo uses single threshold value describe modified algorithms section computational comparison done section vi 
needed implementing modified algorithms 
ii 
svm regression problem optimality conditions basic problem addressed regression problem 
tutorial smola sch lkopf gives overview solution problem svm denote input vector svm denote feature space vector related transformation training set consist data points th input pattern corresponding target value goal svm regression estimate function close possible target values time flat possible generalization :10.1.1.11.2062
function represented linear function feature space denotes bias 
svm designs define kernel function denotes inner product space 
computations done kernel function 
inner product kernel helps dot product vectors feature space having construct feature space explicitly 
mercer theorem tells conditions kernel operator useful svm designs 
svm regression purposes vapnik suggested insensitive loss function error penalized long assumed known priori 
error function regularizing term letting optimization problem solved support vector machine formulated manuscript received december revised may 
bhattacharyya murthy department computer science automation indian institute science bangalore india mail csa ernet csa ernet murthy csa ernet 
keerthi department mechanical production engineering national university singapore singapore mail guppy mpe nus edu sg 
publisher item identifier 
problem referred primal problem 
constant determines tradeoff smoothness ieee ieee transactions neural networks vol 
september amount deviations larger tolerated 
define refer lagrange multipliers 
wolfe duality theory shown obtained solving dual problem 
obtained primal variables easily determined kkt conditions primal problem 
feature space infinite dimensional 
computationally difficult solve primal problem 
numerical approach svm design solve dual problem finite dimensional optimization problem 
note derive proper stopping conditions algorithms solve dual important write optimality conditions dual 
lagrangian dual case case case case case define index sets define definitions rewrite necessary conditions mentioned kkt conditions dual problem easily seen optimality conditions hold iff exists satisfying 
define optimality conditions hold iff conditions necessary sufficient optimality 
refer optimality conditions 
optimality conditions simplified considering cases 
easy check optimality nonzero time 
cases corresponding left 
worth noting smo regression algorithm modifications discussed condition maintained 
exists close relationship threshold parameter primal problem multiplier kkt conditions primal dual problem imply optimality identical 
seen setting derivative lagrangian respect zero gives equality constraint lagrangian multiplier equality constraint optimality coincide 
rest denote quantity 
say index pair defines violation sets conditions holds ieee transactions neural networks vol 
september note optimality condition hold iff exist index pair defines violation 
numerical solution usually possible achieve optimality exactly need define approximate optimality conditions 
condition replaced positive tolerance parameter 
parameter referred tol 
correspondingly definition violation altered replacing respectively optimality mentioned mean approximate optimality 
easy check optimality holds iff exists hold conditions special choice check example violates kkt conditions :10.1.1.11.2062
choice turns right conditions checking optimality incorrect 
say section iv brief discussion smola sch lkopf smo algorithm section 
possible give alternate approximate optimality condition closeness dual objective function optimal value estimated duality gap ideas 
fine care needed choosing tolerance see related discussion 
criterion disadvantage single global condition involving hand consists individual condition pair indexes better suited methods discussed 
particular satisfies strict improvement dual objective function achieved optimizing lagrange multipliers corresponding examples 
true iii 
smola sch lkopf smo algorithm regression number algorithms suggested solving dual problem 
smola sch lkopf give detailed view algorithms implementations :10.1.1.11.2062
traditional quadratic programming algorithms interior point algorithms suitable large size problems reasons 
require kernel matrix computed stored memory 
requires extremely large memory 
second methods involve expensive matrix operations cholesky decomposition large submatrix kernel matrix 
third coding algorithms difficult 
attempts develop methods overcome problems 
method chunking 
idea operate fixed size subset training set time 
subset called working set optimization subproblem solved respect variables corresponding examples working set set support vectors current working set 
current support vectors determine new working set 
new set set worst input data points violate optimality conditions current estimator 
new optimization subproblem solved process repeated optimality conditions satisfied examples 
platt proposed algorithm called smo svm classifier design :10.1.1.11.2062
algorithm puts chunking extreme iteratively selecting working sets size optimizing target function respect 
advantage working sets size optimization subproblem solved analytically 
chunking process repeated till training examples satisfy optimality conditions 
smola sch lkopf extended ideas solving regression problem svm describe algorithm briefly :10.1.1.11.2062
details pseudocode :10.1.1.11.2062
assume reader familiar 
convey ideas compactly employ notations :10.1.1.11.2062
basic step smo algorithm consists choosing pair indexes optimizing dual objective function varying lagrange multipliers corresponding 
important comment role threshold parameter section ii denote output error th pattern :10.1.1.11.2062
call indices multipliers chosen joint optimization step take step varying lagrange multipliers examples need know knowledge value needed take step 
method followed choose step crucial finding solution problem efficiently 
smo algorithm employs loop approach outer loop chooses chosen inner loop chooses outer loop iterates patterns violating optimality conditions lagrange multipliers upper lower boundary smola sch lkopf pseudocode looping indicated satisfied patterns violating optimality conditions ensure problem solved 
efficient implementation cache maintained updated indices corresponding lagrange multipliers 
remaining computed needed 
see smo algorithm chooses aim large increase objective function 
ieee transactions neural networks vol 
september expensive try possible choices choose gives best increase objective function index chosen maximize depending multipliers available cache multiplier indexes indexes initially choice choice yield sufficient progress steps taken 
starting randomly chosen index indexes corresponding multipliers tried choice 
sufficient progress possible indexes tried choices starting randomly chosen index 
choice random seed affects running time smo 
value needed take step needed employed checking optimality 
smo algorithm updated step 
value chosen satisfy step involving takes value exploited update value rare case happen exists interval say admissible thresholds 
situation smo simply chooses midpoint interval 
iv 
inefficiency smo algorithm smo algorithm regression discussed simple easy implement 
inefficient way computing maintaining single threshold value 
instant smo algorithm fixes current indexes joint optimization 
checking remaining examples violate optimality quite possible different shifted choice may better job 
smo algorithm quite possible reached value optimality satisfied smo detected identified correct choice quite possible particular index may appear violate optimality conditions employed incorrect value index may able pair progress objective function 
situation smo algorithm expensive wasteful search looking second index take step 
believe major source inefficiency smo algorithm 
simple alternate way choosing involves indices 
duality theory objective function value primal feasible solution greater equal objective function value dual feasible solution 
difference values referred duality gap 
duality gap zero optimality 
suppose term chosen optimally function 
result duality gap expressed function 
possible way improving smo algorithm choose minimize duality gap 
corresponds subproblem solution subproblem easy 
denote number examples 
increasing order arrangement th th values 
function optimized minimization problem convex function slope convex function negative values smaller positive values bigger zero interval minimizer 
determination done efficiently median finding technique 
typically available stage algorithm appropriate apply idea subset available 
set contains implemented idea tested benchmark problems 
idea led reasonable improvement original smo 
modifications suggest section gave greater improvements 
see section vi performances examples 
modifications smo algorithm section suggest modified versions smo algorithm regression overcomes problems mentioned section 
see computational evaluation section vi modifications better original smo algorithm regression situations give quite remarkable improvement efficiency 
short modifications avoid single threshold value checking optimality 
threshold parameters maintained employed checking optimality 
assuming reader familiar smo give set pointers describe changes smola sch lkopf smo algorithm regression :10.1.1.11.2062
fully describe 
suppose instant available indices checking particular optimality easy 
example suppose check condition holds violation case smo procedure applied index pair similar steps indexes sets 
approach checking optimality index choice second index go hand hand original smo algorithm 
see compute efficient updating process 
efficient smo algorithm spend effort altering cache maintained updated efficiently 
optimality holds indices examined optimality 
ieee transactions neural networks vol 
september procedure modified 
successful step pair indices compute partially 
note extra steps inexpensive cache available updates easily done 
careful look shows just involved successful step sets nonempty partially computed null elements 
take values choices subsequent step see item keep values cache 
working loop note holds point implies optimality holds far concerned 
mentioned item choice influenced indexes gives easy way exiting loop 
ways implementing loop involving indexes method similar done smo 
loop check optimality violated choose appropriately 
example violation case choose method worst violating pair choose depending methods call resulting modification smo smo modification smo modification 
smo smo modification identical way optimality tested 
hand smo modification thought improvement smo modification cache effectively choose violating pair optimality holds said come back check optimality indexes loop indexes 
partially computed update quantities examined 
computed optimality checked current violation update quantities 
example violation case take step hand violation modified suppose described 
happens violation loop having conclude optimality holds answer question affirmative 
easy see argument 
suppose contradiction exist pair define violation satisfy 
say satisfied optimality check described implementation earlier seen affected calculation settings 
words mistakenly taken having satisfied optimality earlier loop detected violating optimality analyzed 
holds possible indices satisfy fig 

toy data cpu time seconds shown function optimality checks 
furthermore holds loop indices completed true values defined computed indices encountered 
final choice doing inference appropriate set vi 
computational comparison section compare performance modifications smola sch lkopf smo algorithm regression smo algorithm duality gap ideas threshold see section iv datasets 
implemented methods ran gcc mhz linux machine 
value experiments 
dataset experiments added independent gaussian noise mean zero standard deviation 
dataset value manually set random way depending amount noise added data 
similar experimental approaches include 
input output normalized zero mean unit variance 
dataset toy dataset function approximated cubic polynomial domain function fixed 
value 
training samples chosen randomly 
performance algorithms polynomial kernel chosen degree polynomial data generated shown fig 

second dataset boston housing dataset standard benchmark testing regression algorithms 
dataset available uci repository 
dimension input 
training set size 
value 
fig 
shows performance algorithms dataset 
third dataset gaussian kernel ieee transactions neural networks vol 
september value employed 
value chosen randomly 
svm insensitive different choices third dataset comp activ available delve website :10.1.1.46.8538
dataset contains data points 
implemented involves attributes predict fraction time percentage cpu runs user mode 
dataset 
performance algorithms dataset shown fig 

clear modifications outperform original smo algorithm 
situations improvement ef remarkable 
particular large values improvement order magnitude 
modifications difficult say better 
primary purpose experiments examine differences training times methods 
studying generalization abilities methods important task currently investigating 
results comparison reported 
fig 

boston housing data cpu time seconds shown function vii 
pointed important source inefficiency smola sch lkopf smo algorithm caused operation single threshold value 
suggested modifications smo algorithm overcome problem efficiently maintaining updating threshold parameters 
computational experiments show modifications speed smo algorithm significantly situations 
modifications easily implemented 
fig 

comp activ data cpu time seconds shown function blake merz 
uci repository machine learning databases 
univ california dept inform 
comput 
sci irvine ca 
online 
available www ics uci edu mlearn ml repository html burges tutorial support vector machines pattern recognition data mining knowledge discovery vol 

delve data evaluating learning valid experiments online :10.1.1.46.8538
available www cs utoronto ca delve mangasarian 
aug massive support vector regression 
online available ftp ftp cs wisc edu pub dmi tech reports ps keerthi bhattacharyya murthy fast iterative nearest point algorithm support vector machine classifier design ieee trans 
neural networks vol 
pp 
jan 
aug improvements platt smo algorithm svm classifier design accepted publication neural computation 
control division dept mechanical production engineering national university singapore singapore 
online 
available guppy mpe nus edu sg platt advances kernel methods support vector machines sch lkopf burges smola eds :10.1.1.11.2062
cambridge ma mit press dec 
fast training support vector machines sequential minimal optimization 
keerthi bhattacharyya murthy 
aug improvements smo algorithm regression 
control division dept mechanical production engineering national university singapore singapore 
online 
available guppy mpe nus edu sg smola learning kernels ph dissertation gmd birlinghoven germany :10.1.1.11.2062
smola sch lkopf tutorial support vector regression royal holloway college london neurocolt tech 
rep tr 
vapnik nature statistical learning theory 
new york springer verlag 
