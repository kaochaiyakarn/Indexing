information preserving multi objective feature selection unsupervised learning ingo artificial intelligence unit department computer science university dortmund ingo uni dortmund de propose novel sound framework evolutionary feature selection unsupervised machine learning problems 
show unsupervised feature selection inherently multi objective behaves differently supervised feature selection number features maximized minimized 
sound surprising supervised learning point view exemplify relationship problem data clustering show existing approaches pose optimization problem appropriate way 
important consequence paradigm change method segments pareto sets produced approach 
inspecting prototypical points segments drastically reduces amount selecting final solution 
compare methods existing approaches data sets 
track learning classifier systems genetics machine learning categories subject descriptors computing methodologies pattern recognition general terms algorithms experimentation keywords multi objective feature selection unsupervised learning pareto front segmentation 
feature selection unsupervised learning challenging new application field genetics machine learning 
show rigorous definition competing criteria leads novel sound theoretical framework multi objective optimization 
approach creates pareto sets share interesting property depending number inherent patterns front shows 
allow interpretable segmentation user select prototypes drastically reduces effort selecting final solution 
turns unsupervised feature selection permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage copies bear notice full citation page 
copy republish post servers redistribute lists requires prior specific permission fee 
gecco july seattle washington usa 
copyright acm 
michael artificial intelligence unit department computer science university dortmund ls cs uni dortmund de case study automatic segmentation highly complex pareto sets 
today machine learning consists paradigms supervised unsupervised learning 
supervised learning set labeled data points 
learning method merely find function predicts label unseen data points 
supervised learning methods applied information known 
unsupervised machine learning describe data set 
task automatically find inherent natural patterns data 
natural patterns express useful information decision maker 
typical application areas include customer segmentation information retrieval image analysis 
search proper supervised prediction function usually formulated optimization problem number wrong predictions known data points minimized 
similar criterion validity exist unsupervised setting 
optimization function unsupervised algorithm rely patterns order decide pattern correct wrong 
validity discovered patterns depends background knowledge intention user 
desirable unsupervised learning methods solution user 
main problems supervised unsupervised learning algorithms decide dimensions data space taken account 
dimensions data space called features corresponding selection problem called feature selection 
prediction accuracy learned decision function dramatically increased redundant noisy features omitted learning 
problem learning paradigms consequences different 
supervised feature selection problem solved minimizing number features prediction accuracy preserved 
search best feature subset possible subsets usually requires heuristics larger dimensions genetic algorithms demonstrated ability solve problem times 
addition minimizing number features maximizing prediction accuracy multi objective optimization problem removing necessary features data set decrease accuracy 
problem exists unsupervised learning 
existence noisy redundant features cover inherent data clusters omitting features reveal actual natural patterns 
approaches try directly identify promising feature subsets clustering 
approaches reflect multi objective character problem setting 
state art feature selection approaches unsupervised learning multi objective optimization 
transfer idea minimizing number features clustering optimization criterion preserved :10.1.1.1.4533
defining optimization problem way appropriate 
weak assumptions show pareto set collapse singular point 
population collapse tends cover small fraction solution space 
trade number features cluster validity 
number features maximized minimized order achieve competition 
sound surprising change optimization direction natural origin aim unsupervised learning 
order describe data set hand amount information derived feature set preserved feature selection process 
solve corresponding multi objective optimization problem nsga ii 
resulting pareto sets beneficial users provide larger coverage possible candidate solutions 
pareto fronts produced selection approach segmented meaningful regions 
eases selection final solution set pareto optimal points 
enable feature selection density clustering schemes 
contrast combinatorial clustering algorithms means clustering algorithms able find non gaussian clusters rings spirals 
propose improved feature selection approach applicable wide variety clustering algorithms provides interpretable segmentations complex pareto set 
outline section introduce problem data clustering corresponding optimization criteria unsupervised learning 
section discuss existing approaches unsupervised feature selection 
transfer supervised learning appealing idea show approaches lead complete pareto fronts type problem 
section discuss simply changing optimization direction criteria leads natural multi objective optimization problem solved nsga ii 
furthermore introduce segmentation procedure resulting pareto fronts 
section presents results artificial real world data sets compares discussed approaches 
section concludes 

data clustering important approaches unsupervised learning data clustering 
aim cluster analysis group data points sets similar data points 
data set individual data point denoted xi cluster subset data points ck principle clusters may overlap 
clustering algorithms designed produce partitions data points set clusters 
ck ck cl ck cl clusters overlap sk ck data point covered cluster 
combinatorial clustering algorithms cluster analysis aims assigning items clusters elements cluster similar elements clusters 
notion expressed optimization problem distance measure xi xj set data points wd kx xi ck xj ck xi xj 
shown minimizing wd mean pairwise difference pairs points different clusters maximized 
efficient approach optimizing function means clustering 
squared euclidean distance 
assume data points represented set real valued features xi ir xim value th feature data point xi 
euclidean distance points xi xj xi xj xim xjm 
shown optimizing wd respect squared euclidean distance equivalent optimizing function kx mx xim ckm xi ck ckm th value centroid cluster ck 
centroid point smallest distance points ck 
calculated ckm ck xim 
means algorithm uses relationship applying alternating optimization procedure 
step data point assigned cluster nearest centroid 
centroids recalculated cluster data points newly assigned clusters new centroids 
alternation stops change cluster assignment maximal number steps 
centroids initialized random data points drawn means guarantee find optimal solution sensitive choice initial points 
common strategy start means times different random initializations 
simplicity efficiency means popular clustering algorithms 
natural choice evaluating set clusters produced means function optimizes 
measure drawbacks 
important normalized respect feature values number clusters 
increasing number clusters decreases monotonically 
decreases decreasing number features 
reasons suited criterion unsupervised feature selection problems 
evaluation measures means proposed 
probably important davies db index 
calculated db kx ff sk sl max ck cl sk sl average cluster distances cluster ck cl respectively defined sk xi ck 
ck db index takes account relative separation clusters worst separated 
value normalized divided distance corresponding centroids 
sensitive number clusters number features 
focusing clusters worst separated allows fine grained optimization parts clustering separated important parts 
please note db order evaluate cluster structure 
efficient algorithm optimizes db directly 
gaussian mixtures gaussian mixture clustering assumes underlying data generating process consists mixture different overlapping gaussian distributions distribution represents individual cluster 
clustering achieved estimating parameters underlying distributions assigning data point distribution produced 
notion formalized log xi kx xi gk multivariate gaussian distribution representing cluster ck mixture parameters probability pk apriori probability data point belonging cluster ck 
expectation maximization em approach employs alternating optimization procedure similar applied means clustering 
data point assigned distribution generated parameters distributions clusters recalculated data points cluster 
procedure repeated change occurs maximum number steps reached 
graph clustering assigning data points clusters directly impose graph points 
data points xi xj connected sufficiently similar 
maximum distance threshold dmax xi xj connected xi xj dmax 
far common clustering criterion regard clusters connected components distance graph 
approach denoted single link clustering 
finding connected components graph usually achieved graph search 
value dmax determines coherence resulting clusters 
fixed number clusters find maximal value dmax graph contains connected components clusters 
case resulting dmax evaluation measure spiral data set created single link clustering 
denotes strength weakest link cluster structure 
weakest link point cluster split components dmax decreased 
graph clustering popular combined advanced connectivity criteria density clustering support vector clustering 
contrast means gaussian mixtures algorithms detect clusters shape means limited find spherical clusters boundaries 
shows example structure clustered em means 
structures play important role applications image recognition astronomical data analysis 

multi objective feature selection clustering multi objective optimization natural choice selecting appropriate feature subsets clustering problems 
current state art represented described 
describe approaches show limited ways 
limitations result way multiobjective optimization problem posed 
kim street menczer introduce performance criteria means clustering 
variant normalized number features 
variant cluster distance second measure 
measure behaves essentially way normalized minimizing cluster distance equivalent maximizing cluster distance 
third measure represents number clusters minimized 
measure captures number features nf minimized 
theorem show number clusters minimizing number features leads exactly pareto optimal point 
optimal point selects single feature dataset original criteria normalized constant 
influence pareto optimality 
particular leads minimal loss respect clustering performance criterion 
theorem 
minimizing number features nf leads single pareto optimal point 
proof denote loss individual feature kx am xim ckm xi ck order minimize number features selecting feature optimal 
show min am 
means performance decrease adding feature optimizes am 
easily seen mp mp min am am min am optimization suited approach feature selection clustering problems leads trivial solutions 
similar proof normalized cluster distance 
normalized variant db proposed alternative performance criterion db 
approach better suited db normalized respect feature space 
criterion sensitive 
feature set contains example real valued feature takes discrete values choosing feature pareto optimal 
feature certainly represent complete dataset descriptive sense mentioned 
section see examples pareto set collapses single trivial solution 
furthermore normalized db lead competition number features cluster quality measure db 
clustering random data produces pareto front exhibits relationship see 
major problem approaches pose problem correctly point view multi objective optimization 
section give alternative problem formulation solves described difficulties 

information preserving feature selection sections discussed quality measurements different clustering schemes 
assume criteria maximized feature selection 
criteria minimized original problem definition multiplied 
contrast existing approaches discussed section minimize number features maximize nf 
prevents algorithm selecting trivial solutions leads complete pareto sets natural clusterings 
fitness evaluation done performing clustering scheme reduced feature sets 
depending scheme db equation means clustering dmax single link clustering 
natural competition maximizing number features nf selected cluster criterion need apply artificial normalization factor 
feature selection problem inherently multi objective solved single objective evolutionary algorithms 
clustering setting user idea criteria weights furthermore exist simple decision correct wrong clusterings 
decision totally depend amount information user obtain different clusterings 
try maintain information possible aim finding solutions optimal arbitrary criteria weight vectors 
solutions called pareto optimal 
evolutionary algorithms optimize target function introducing special selection operators 
due population approach evolutionary algorithms broad selection pareto optimal solutions run 
user select solutions optimization 
additionally multi objective evolutionary algorithms strongly depend form continuity pareto optimal set :10.1.1.48.3504
see clustering non normalized optimization criteria pareto front nicely shaped continuous 
nsga ii multi objective feature selection wrapper 
nsga ii employs selection technique sorts individuals levels non domination 
individuals levels added generation desired population size reached 
adding individuals possible level level sorted respect crowding distance order preserve diversity population 
individuals bit vectors length indicating feature selected 
population size set maximal number generations 
bit flip mutation performed probability uniform crossover probability 
finding interesting points pareto front pareto plots derived multi objective optimization procedure described section show clear structure 
structure accidental reflects structure underlying data 
exploit pareto plot structure order discover patterns underlying data set 
exploiting structure drastically reduces effort selecting final solution pareto set 
basic idea find points trade number features cluster quality significantly changes 
trade represented slope pareto plot point want find points change slope maximal 
notion formalized 
assume pareto optimal points sorted th point denoted pair dbp nfp 
value represents slope point dbp dbp arctan 
nfp nfp interested points slope significantly changes calculate 
value greater indicates adding additional feature significant smaller negative influence cluster coherence proceeding features 
value smaller indicates additional feature stronger negative influence 
points strong increase strong decrease slope represent redundant features coherent sets features vertical parts 
adding individual feature change performance 
features decrease increase represent areas noisy incoherent features horizontal parts 
adding features direct negative influence cluster quality features added 
example please refer section 
evaluation section discuss results approach proposed normalized minimization approach discussed section 
applied algorithms synthetic real world data sets 
data sets order measure effect artificial normalization factor necessary feature set minimization applied algorithms grid data set grid random data set random containing white noise 
artificial data set gm consisting gaussian clusters random standard deviations dimensions created 
data set enriched additional single gaussian noise features standard deviation 
applied algorithms clustering benchmark datasets iris data set wisconsin prognostic breast cancer data set 
especially interesting redundant features 
allows check approaches able cope redundancy 
order evaluate feature selection non standard cluster boundaries generated artificial data set consisting spirals apply single link clustering 
clusterings combinatorial clustering algorithms means 
shows dimensions created data set 
table summarizes properties data sets 
experiments performed freely available machine learning environment yale 
complete data sets programs experiment configurations available web site www ai cs uni dortmund de results shows pareto sets simultaneous optimization cluster criterion feature set size 
noted cases population converges final front generations 
nsga ii selection able sustain solution optimization 
clearly seen cases pareto sets provided information preserving approach contain points results normalized minimization approach 
feature relative small standard deviation pareto set minimization approach collapse grid iris nn spiral 
normalization factor introduces convex front optimize 
effect seen random data set minimization approach finds convex pareto front front provided approach linear 
normal iris data set iris gn data set enriched noise features proposed approach finds complete pareto set minimization approach able find small number feature subsets 
clear clustering correct user able select complete pareto front 
applies real world data set shows results 
focus simultaneous optimization number clusters course possible 
approach differ existing approaches respect concentrated usability information preservation 
real world data set simultaneously optimize range leads dimensional pareto set 
pareto plot shows influence number clusters pareto fronts 
regions distinct deviations convex hull pareto set 
small rear region bigger front region 
totally covered structure normalization approach possible detect structure 
furthermore redundant features easily determined vertical parts front 
pareto front segmentation fact pareto sets complete advantage non normalized maximization covered structure 
contrast easily discovered result approach 
applied segmentation algorithm described section pareto set delivered 
neighbors clearly seen number features maximized minimized clustering criterion normalized 
selecting points highest absolute leads interpretable segmentation result 
shows selected points segments 

novel multi objective evolutionary framework feature selection unsupervised machine learning settings 
exemplified framework task data clustering plays important role wide variety abbr 
properties noise results grid equidistant values dimensions random uniformly distributed values gm gaussian mixture clusters iris iris data set noise features iris nn iris data set nominal noise iris gn iris data set gaussian noise data set noise features 
spiral spirals origin table data sets 
column summarizes abbreviations text second summarizes properties data set 
total number examples total number features 
column noise defines features explicitly added noise features 
columns define mean standard deviation original features noise features 
column indicates number clusters known 
column indicates pareto sets data set approaches 
number features number features number features number features db norm grid min vs ndb db grid max vs db db norm random min vs ndb db random max vs db number features number features number features number features db norm gm min vs ndb db gm max vs db db norm iris min vs ndb db iris max vs db number features noise feature number features number features number features noise feature db norm iris nn min vs ndb iris nn max vs db db db norm iris gn min vs ndb db iris gn max vs db number features number features number features number features db norm min vs ndb db max vs db dmax spiral min vs dmax dmax spiral max vs dmax pareto fronts data sets 
left result dataset achieved approach discussed section normalized value ndb 
clearly seen results complete covered artificial structure 
results right achieved information preserving maximization approach 
applied information preserving feature selection real world data set 
number features davies clustering criterion db number clusters simultaneously optimized 
result dimensional pareto set containing necessary information allowing decision best clustering 
segment pareto set ease analysis front 
applications ranging pattern recognition customer relationship management web search 
clustering ideal test case evolutionary computation methods reasons 
inherently multi objective problem 
usually correct result supervised learning 
users explore space results interactively gain insight natural patterns data set 
second approach proposed yields pareto sets show significant inner structure 
structure accidental reflects patterns underlying data 
generic method automatic pareto set segmentation showed discovered segments interpreted respect unsupervised feature selection 
turns clustering application automatic pareto set analysis 
argued benefits achieved optimization problem posed sound way 
maximizing number features feature selection sound surprising paradigm change motivated aim unsupervised learning search descriptive natural patterns 
particular shown existing approaches multi objective unsupervised feature selection minimization appropriate produce trivial incomplete solution sets 
contribution approach applicability clustering algorithms em means 
essential applications clusters shape means density clustering algorithms 
plan incorporate additional state art clustering algorithms unsupervised learning techniques association rule learning 
plan explore possibilities evolutionary multiobjective optimization problem unsupervised feature construction 
approach runtime existing approaches large scale unsupervised feature selection feasible 
impact sampling analyzed order reduce total runtime loosing information data set 
opinion evolutionary computation promising solution overcome essential limitations current unsupervised machine learning approaches 

acknowledgments supported deutsche forschungsgemeinschaft dfg collaborative research center reduction complexity multivariate data structures 
points highest absolute deviation marked perpendicular lines 
leads interpretable segmentation pareto front eases process selecting final solution pareto set 

ben hur horn siegelmann vapnik 
support vector clustering 
journal machine learning research 
coello coello :10.1.1.48.3504
comprehensive survey evolutionary multiobjective optimization techniques 
knowledge information systems 
cormen leiserson rivest stein 
algorithms 
mit press 
davies 
cluster separation measure 
ieee transactions pattern analysis machine intelligence 
deb agarwal 
fast elitist multi objective genetic algorithm nsga ii 
technical report kanpur genetic algorithms laboratory indian institute technology 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society series 
hunter macintyre 
multiobjective evolutionary setting feature selection commonality crossover operator 
proc 
congress evolutionary computation cec pages 
ester 
kriegel sander xu 
density algorithm discovering clusters large spatial databases noise 
proc 
international conference knowledge databases kdd pages 
fischer klinkenberg 
yale learning environment tutorial 
technical report ci collaborative research center university dortmund dortmund germany 
fisher 
multiple measurements taxonomic problems 
annals 
hartigan wong 
means clustering algorithm 
applied statistics 
hastie tibshirani friedman 
elements statistical learning data mining inference prediction 
springer series statistics 
springer 
jain murty flynn 
data clustering review 
acm computing surveys 
kim street menczer 
feature selection unsupervised learning evolutionary search 
proc 
th acm sigkdd international conference knowledge discovery data mining pages new york ny usa 
acm press 
kim street menczer 
evolutionary model selection unsupervised learning 
intelligent data analysis 
morita sabourin suen 
unsupervised feature selection multi objective genetic algorithms handwritten word recognition 
proc 
th international conference document analysis recognition icdar 
murtagh 
clustering massive data sets pages 
kluwer academic publishers 
punch goodman kuhn jain 
dimensionality reduction genetic algorithms 
ieee transactions evolutionary computation 
roth lange 
feature selection clustering problems 
proc 
neural information processing systems nips 
sander ester 
kriegel xu 
density clustering spatial databases algorithm applications 
data knowledge discovery 
wolberg street mangasarian 
computer derived nuclear grade breast cancer prognosis 
analytical quantitative 
yang honavar 
feature subset selection genetic algorithm 
ieee intelligent systems 
zitzler thiele 
multiobjective evolutionary algorithms comparative case study strength pareto approach 
ieee transactions evolutionary computation 

