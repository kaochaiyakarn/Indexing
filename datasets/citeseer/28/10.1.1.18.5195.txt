unbiased evaluation retrieval quality clickthrough data thorsten joachims cornell university department computer science ithaca ny usa tj cs cornell edu draft 
proposes new method evaluating quality retrieval functions 
traditional methods require relevance judgements experts explicit user feedback entirely clickthrough data 
key advantage clickthrough data collected low cost overhead user 
approach experiment design proposes experiment setup generates unbiased feedback relative quality search results explicit user feedback 
theoretical analysis shows method gives results evaluation traditional relevance judgements mild statistical assumptions 
especially large thorsten joachims evaluation presentation hand tuned hand tuned table 
average retrieval functions hand tuned strategy uses different weights html tags implemented laser 
rows correspond retrieval method laser query time columns hold values subsequent evaluation methods 
figures reported means standard errors 
table taken dynamic document collections intractable get accurate recall estimates require relevance judgements full document collection :10.1.1.41.9172
extend focused sampling pooling method trec reduce assessment cost 
idea focus manual assessment top documents retrieval systems contain relevant documents 
attempts evaluate retrieval functions human judgements statistics document collection evaluation schemes give approximate solutions may fail capture users preferences 
retrieval systems www typically evaluated recall 
idea focus manual assessment top documents retrieval systems contain relevant documents 
attempts evaluate retrieval functions human judgements statistics document collection evaluation schemes give approximate solutions may fail capture users preferences 
retrieval systems www typically evaluated recall 
precision measured 
decrease amount manual relevance assessment method focusses evaluation documents observed user :10.1.1.111.4364
need manual relevance judgements experts limits scale frequency evaluations 
usefulness measure frei sch uses different form human relevance assessment 
respect relative performance criterion similar method proposed 
usefulness measure designed compare retrieval strategies absolute relevance judgements 
method relies relative preference statements users 
sets retrieved documents query user asked judge relative usefulness pairs documents 
user preferences compared orderings imposed retrieval functions respective number violation score 
technique eliminates need relevance judgements document collection relies manual relevance feedback user 
unbiased evaluation retrieval quality clickthrough data attempts implicit feedback observing clicking behavior retrieval systems browsing assistants :10.1.1.41.9172
semantics data unclear argued 
presentation bias clickthrough data search engine provides better results google msnsearch 
evaluating hypothesis tests basic statistical inference 
unfortunately regular clickthrough data suited answer question principled way 
unfortunately regular clickthrough data suited answer question principled way 
consider setup experiment setup regular clickthrough data user types query unified interface query sent search engines returned rankings selected random user 
ranks links user clicked recorded 
example observation experiment user types query support vector machine receives ranking search clicks links ranked 
data collected boyan shows setup leads strong presentation bias making results difficult interpret :10.1.1.41.9172
consider average rank clicks performance measure example 
conclude type clickthrough data 
table shows average retrieval strategies averaged queries 
rows correspond retrieval method user columns show average subsequent evaluation retrieval functions 
