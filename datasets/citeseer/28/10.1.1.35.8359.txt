weighted nearest neighbor algorithm learning symbolic features scott cost steven salzberg department computer science johns hopkins university baltimore md internet cost cs jhu edu salzberg cs jhu edu past nearest neighbor algorithms learning examples worked best domains features numeric values 
domains examples treated points distance metrics standard definitions 
symbolic domains sophisticated treatment feature space required 
introduce nearest neighbor algorithm learning domains symbolic features 
algorithm calculates distance tables allow produce real valued distances instances attaches weights instances modify structure feature space 
show technique produces excellent classification accuracy problems studied machine learning researchers predicting protein secondary structure identifying dna promoter sequences english text 
comparisons show algorithm accuracy comparable back propagation decision trees learning algorithms 
results support claim nearest neighbor algorithms powerful classifiers features symbolic 
instance learning versus models power instance methods demonstrated number important real world domains prediction cancer recurrence diagnosis heart disease classification congressional voting records aha kibler salzberg 
experiments demonstrate instance learning ibl applied effectively domains features unordered symbolic values prediction protein secondary structure word pronunciation prediction dna promoter sequences 
domains received considerable attention connectionist researchers employed back propagation learning algorithm sejnowski rosenberg qian sejnowski towell :10.1.1.54.4913
addition word pronunciation problem subject number comparisons machine learning algorithms stanfill waltz shavlik dietterich :10.1.1.75.5676
domains represent problems considerable practical importance symbolic feature values difficult conventional nearest neighbor algorithms 
show nearest neighbor algorithm pebls stanfill waltz value difference method produce highly accurate predictive models domains 
intent compare ibl learning methods respects classification accuracy speed training ease algorithm representation understood 
results support claim nearest neighbor algorithms powerful classifiers features symbolic 
instance learning versus models power instance methods demonstrated number important real world domains prediction cancer recurrence diagnosis heart disease classification congressional voting records aha kibler salzberg 
experiments demonstrate instance learning ibl applied effectively domains features unordered symbolic values prediction protein secondary structure word pronunciation prediction dna promoter sequences 
domains received considerable attention connectionist researchers employed back propagation learning algorithm sejnowski rosenberg qian sejnowski towell :10.1.1.54.4913
addition word pronunciation problem subject number comparisons machine learning algorithms stanfill waltz shavlik dietterich :10.1.1.75.5676
domains represent problems considerable practical importance symbolic feature values difficult conventional nearest neighbor algorithms 
show nearest neighbor algorithm pebls stanfill waltz value difference method produce highly accurate predictive models domains 
intent compare ibl learning methods respects classification accuracy speed training ease algorithm representation understood 
comparable performance respect superiority argue ibl preferable learning algorithms types problem domains considered 
instance entered weight immediately influential classifiers space 
better strategy initialize new instance weight equal matching exemplar 
adopted weighting strategy experiments described 
weighting scheme completes modified value difference metric 
domains chose comparisons domains received considerable attention machine learning research community word pronunciation task sejnowski rosenberg shavlik prediction protein secondary structure qian sejnowski prediction dna promoter sequences towell :10.1.1.75.5676
domain symbolic valued features mvdm applicable standard euclidean distance 
sections describe databases problems learning 
protein secondary structure accurate techniques predicting folded structure proteins exist despite increasingly numerous attempts solve problem 
techniques depend part prediction secondary structure primary sequence amino acids 
recall overlap metric measures distance number features different values 
worth noting test runs pebls instances caused errors negative source variation classification accuracy training test sets 
proteins structurally similar algorithm may accurate predicting structure protein trained 
table promoter sequence prediction algorithm error rate pebls kbann pebls unweighted back propagation id nearest neighbor overlap neill instances 
towell notes negative examples database data derived selecting substrings fragment coli believed contain promoter sites towell :10.1.1.54.4913
suggest results examples re examined 
examples interesting exceptions general patterns dna promoters 
english text pronunciation english pronunciation task training set defined rosenberg nettalk program 
set consists instances drawn brown corpus commonly words english language 
suggest results examples re examined 
examples interesting exceptions general patterns dna promoters 
english text pronunciation english pronunciation task training set defined rosenberg nettalk program 
set consists instances drawn brown corpus commonly words english language 
unable discern difference training set somewhat restricted set shavlik shavlik experimental design :10.1.1.75.5676
training brown corpus pebls tested entire word webster pocket dictionary 
results table weighted unweighted versions pebls algorithm 
comparison table english text pronunciation algorithm phoneme accuracy phoneme stress pebls pebls unweighted back propagation give results nettalk program back propagation learning algorithm 
shavlik 
