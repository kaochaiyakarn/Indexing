markovian models sequential data bengio dept informatique recherche op universit de montr eal montreal qc canada hidden markov models hmms statistical models sequential data successfully machine learning applications especially speech recognition 
furthermore years new promising probabilistic models related hmms proposed 
summarize basics hmms review related learning algorithms extensions hmms including particular hybrids hmms artificial neural networks input output hmms conditional hmms neural networks compute probabilities weighted transducers variable length markov models markov switching state space models 
discuss challenges research active area 
hidden markov models hmms statistical models sequential data successfully applications artificial intelligence pattern recognition speech recognition modeling biological sequences 
focus learning algorithms developed hmms related models hybrids hmms artificial neural networks input output hmms weighted transducers variable length markov models markov switching models switching state space models :10.1.1.29.8423:10.1.1.126.6810:10.1.1.30.5334:10.1.1.30.5334:10.1.1.33.1845:10.1.1.56.7115
remind reader joint probability distribution sequence observations fy factored jy gamma supported national science engineering research council canada institute robotics intelligent systems 
performed bengio laboratories updates corrections comments sent bengio ca 
neural computing surveys www icsi berkeley edu jagota ncs notation mean probability random variable takes value context notation ambiguous 
neural computing surveys www icsi berkeley edu jagota ncs intractable general model sequential data conditional distribution jy gamma observed variable time depends details previous values gamma models discussed share property assume past sequence summarized concisely unobserved random variable called state variable carries information gamma useful describe distribution observation common models hmms best known contribution advances automatic speech recognition decades 
tutorial hmms context speech recognition :10.1.1.131.2084
see book focusses applications bioinformatics 
algorithms estimating parameters hmms developed 
application hmms speech independently proposed popularized :10.1.1.131.2084
early review alternative methods hmms related hmms speech recognition collection papers 
neural computing surveys www icsi berkeley edu jagota ncs intractable general model sequential data conditional distribution jy gamma observed variable time depends details previous values gamma models discussed share property assume past sequence summarized concisely unobserved random variable called state variable carries information gamma useful describe distribution observation common models hmms best known contribution advances automatic speech recognition decades 
tutorial hmms context speech recognition :10.1.1.131.2084
see book focusses applications bioinformatics 
algorithms estimating parameters hmms developed 
application hmms speech independently proposed popularized :10.1.1.131.2084
early review alternative methods hmms related hmms speech recognition collection papers 
hmms applied variety applications outside speech recognition handwriting recognition pattern recognition molecular biology fault detection 
variants extensions hmms discussed include language models econometrics time series signal processing 
analysis sample computational complexity approximating distribution hmm probabilistic automaton done tools pac learning :10.1.1.116.3824
application hmms speech independently proposed popularized :10.1.1.131.2084
early review alternative methods hmms related hmms speech recognition collection papers 
hmms applied variety applications outside speech recognition handwriting recognition pattern recognition molecular biology fault detection 
variants extensions hmms discussed include language models econometrics time series signal processing 
analysis sample computational complexity approximating distribution hmm probabilistic automaton done tools pac learning :10.1.1.116.3824
see analysis case hidden markov chains deterministic emissions shows classes markovian learning problems hard polynomial number samples required 
case probabilistic finite state automata show algorithm polynomial time complexity sample size learning subclass acyclic probabilistic finite state automata 
learning problem type algorithms discussed framed follows 
training set fd dn sequences data criterion quality model set data mapping model real valued scalar choose model certain set models way maximize minimize expected value criterion new data assumed sampled unknown distribution training data sampled 
remaining sections describe extensions hmms markovian models related hmms hybrids artificial neural networks section input output hmms section including markov switching models section asynchronous input output hmms section generalizations hmms called weighted transducers section useful combine markovian models state space models markovian models continuous state section 
hidden markov models section remind reader basic definition hmm tutorial way 
formalize assumptions describe basic elements algorithms hmms 
algorithms estimate parameters hmms discussed details section generalized hmms input output conditional hmms 
markov model order probability distribution sequence variables fq neural computing surveys www icsi berkeley edu jagota ncs :10.1.1.133.4884
bayesian network representing independence assumptions markov model order jq gamma jq gamma gamma gamma fq gamma conditional independence property jq gamma jq gamma gammak gamma gammak summarizes relevant past information generally called state variable 
conditional independence property joint distribution sequence decomposed product jq gamma gammak special case markov model order models described 
case distribution simpler jq gamma completely specified called initial state probabilities transition probabilities jq gamma 
illustrate concept markov model order consider simple example 
bayesian network markov model order shown 
shows directed acyclic graph dag node corresponds random variable 
edge variable variable implies causal direct influence absence edge implies conditional independence variables may exist path conditioning intermediate variables paths independent 
specifically joint probability distribution set random variables fa represented graph arbitrary connectivity product parents fb edge ag 
see formal definitions pointers related literature graphical probabilistic models inference algorithms :10.1.1.52.696:10.1.1.112.8434
see neural computing surveys www icsi berkeley edu jagota ncs relations hmms graphical models markov random fields kalman filters see application ideas decoding 
formulations hmms graphical models proposed applied successfully speech recognition 
exploited review graphical model formulation lends naturally extensions context variables factoring state variable 
note probabilistic models described cast framework bayesian networks 
case discrete variable jq generally taken multinomial 
model homogeneous output distributions parameters matrix elements 
applications interest multivariate continuous 
obtain discrete distribution approaches common 
perform vector quantization order map vector valued discrete value quantize quantize jq emission probability generally :10.1.1.116.3824

multiple codebooks split vector variable sub vectors ti assumed approximately independent quantize separately maps quantize ti quantize ti jq emission probability 
example speech recognition systems represents spectral information time represents changes spectrum represents local average signal energy time time derivative 
continuous emissions continuous hmms commonly emission distributions gaussian distribution gaussian mixture jq ji ij sigma ij ji ji sigma probability observing vector gaussian distribution mean vector covariance matrix sigma 

multiple codebooks split vector variable sub vectors ti assumed approximately independent quantize separately maps quantize ti quantize ti jq emission probability 
example speech recognition systems represents spectral information time represents changes spectrum represents local average signal energy time time derivative 
continuous emissions continuous hmms commonly emission distributions gaussian distribution gaussian mixture jq ji ij sigma ij ji ji sigma probability observing vector gaussian distribution mean vector covariance matrix sigma 
variant continuous hmm gaussian mixtures called semi continuous hmm gaussians shared parameters specific state mixture weights jq ji sigma mixture weights play role similar multinomial coefficients discrete emission hmms described :10.1.1.45.5793
modeling approaches discrete features hmm selected modeler prior knowledge data number values state variable topology hmm forcing transition probabilities zero type distribution emissions includes choices number gaussians mixture 
basically address model selection question restrict discussion general prior knowledge topology speech recognition hmms numerical free parameters chosen numerically learning parameter estimation algorithm 
neural computing surveys www icsi berkeley edu jagota ncs parameter estimation point discussed hmm model 
briefly discuss issue learning parameters model 
basically address model selection question restrict discussion general prior knowledge topology speech recognition hmms numerical free parameters chosen numerically learning parameter estimation algorithm 
neural computing surveys www icsi berkeley edu jagota ncs parameter estimation point discussed hmm model 
briefly discuss issue learning parameters model 
details learning algorithms sections 
distributions em expectation maximization algorithm estimate parameters hmm order maximize likelihood function dj tp set training sequences tp length letter :10.1.1.133.4884
em algorithm introduced discussed formally section 
em algorithm graphical models bayesian networks 
training em algorithm addition emission distributions described previous subsection emission distributions exponential family mixtures thereof 
speech sequence recognition applications algorithm hmm conditioned sequence correct labels fw wl chooses maximizes product class conditional likelihoods jw training sequences 
re estimation formulae incorrect parameter values posterior weights new parameters may exactly maximize likelihood shown bring likelihood closer maximum 
iterating procedure rapidly converge maximum likelihood 
viterbi algorithm model class defined parameters model learned things done model 
subsection discuss infer state sequence observation sequence robot example infer trajectory robot sequence actions 
applications hmms speech recognition molecular biology applications example hidden state variable associated particular meaning phonemes words speech recognition :10.1.1.131.2084
state corresponds classification label states grouped label 
state sequence corresponds sequence classification labels words characters phonemes 
useful observed sequence infer state sequence corresponding 
achieved algorithms perform neural computing surveys www icsi berkeley edu jagota ncs 
constraints topology sharing parameters states associated different instances speech unit represent strong assumptions relation speech signal neural computing surveys www icsi berkeley edu jagota ncs dog example constrained hmm representing conditional distribution jw 
acoustic sequence sequence words dog 
word sequence assumptions known strong 
focus current research field build faithful models keeping tractable sure imperfections model hurt final decision 
assumptions useful practice order build current state art speech recognition systems applications bioinformatics :10.1.1.131.2084
see prior topology discover sequential clusters data 
generally research learning language models addresses issue learning structure model addition parameters 
see example polynomial time algorithms constructively learn probabilistic structure language merging states learnability acyclic probabilistic finite automata 
learning criteria learning criterion choose parameters hmm 
idea training set modules separately respect global criterion gradient algorithms proposed years ago 
way integrate anns hmms mathematically clear way idea neural computing surveys www icsi berkeley edu jagota ncs bayesian network representing graphically independence assumptions synchronous hidden markov model 
state sequence output sequence input sequence 
input output hmms described section 
input output hmms input output hidden markov models iohmms conditional hmms simply hmms emission transition distributions conditional sequence called input sequence noted case observations modeled emission distributions called outputs model represents distribution sequences conditional distribution jx :10.1.1.29.8423
simpler models input output sequences length extension section allows input output sequences different lengths 
transducers section seen generalizations conditional distributions allow input output sequences different lengths 
conditional independence assumption synchronous represented bayesian network 
say hmms homogeneous sense transition emission probability distributions depend iohmms inhomogeneous sense transition emission probability distribution change input change time 
reason explain em algorithm train hmms iohmms section section 
ordinary hmms emission distributions homogeneous model jq iohmms timevarying conditional model jq 
similarly time invariant transition probabilities jq gamma iohmms jq gamma 
generally values inputs gammak different time steps condition distributions 
hmms pattern recognition trained choosing parameters maximize likelihood observations correct classification sequence jw iohmms may trained maximize likelihood jx decision variables observed variables literature learning algorithms iohmms proposed sequence processing tasks complex emission transition models anns :10.1.1.29.8423
control reinforcement learning literature similar models called partially observable markov decision processes :10.1.1.56.7115
neural computing surveys www icsi berkeley edu jagota ncs case objective model output sequence input sequence find strategy sequence actions seen inputs model minimizes cost function defined sequence outputs observed 
case represents probabilistic relationship actions observations hidden state variable 
hmms described section speech recognition represent conditional probability jw observation sequence classification sequence achieved deterministically attaching symbolic meaning different values state variable 
ordinary hmms emission distributions homogeneous model jq iohmms timevarying conditional model jq 
similarly time invariant transition probabilities jq gamma iohmms jq gamma 
generally values inputs gammak different time steps condition distributions 
hmms pattern recognition trained choosing parameters maximize likelihood observations correct classification sequence jw iohmms may trained maximize likelihood jx decision variables observed variables literature learning algorithms iohmms proposed sequence processing tasks complex emission transition models anns :10.1.1.29.8423
control reinforcement learning literature similar models called partially observable markov decision processes :10.1.1.56.7115
neural computing surveys www icsi berkeley edu jagota ncs case objective model output sequence input sequence find strategy sequence actions seen inputs model minimizes cost function defined sequence outputs observed 
case represents probabilistic relationship actions observations hidden state variable 
hmms described section speech recognition represent conditional probability jw observation sequence classification sequence achieved deterministically attaching symbolic meaning different values state variable 
iohmms state variable stochastically related output variable classification problems view output sequence classification sequence input observed sequence conditioning sequence 
section describe particular kinds iohmms proposed econometrics literature 
section em algorithm training hmms synchronous hmms 
neural computing surveys www icsi berkeley edu jagota ncs markov switching models markov switching models introduced econometrics literature modeling non due abrupt changes regime economy 
point view taken extend time series regression models addition discrete hidden state variable allows changing parameters regression models state variable changes value 
consider example time series regression model fi observation output variable time random variable zero mean gaussian distribution vector input variables past values past values observed variables :10.1.1.30.5334
different sets parameters fi different discrete values hidden state variable basically specifies particular form emission distribution jq gaussian distribution mean linear function different parameters different values conditional mean non linear function neural network applications iohmms modeling distribution returns described shows non linear models yield improved modeling distribution stock market indices returns 
obtain complete picture joint distribution past observed values needs specify distribution state variable 
cases described econometrics literature distribution assumed time invariant specified matrix transition probabilities ordinary hmms complicated specifications suggested 
representation variance equation complex single constant parameter variance function state variable input variables 
filtering algorithm compute estimate current distribution jy state past inputs outputs 
smoothing algorithm compute posteriori estimate distribution jy state path sequence inputs outputs 
prediction algorithm allows compute distribution states outputs past input output observations 
section consider state space models hidden state variable continuous hybrids discrete continuous state variables similar time series modeling applications 
em hmms iohmms section sketch application em expectation maximization algorithm hmms iohmms :10.1.1.133.4884
papers baum special case em algorithm applied discrete emissions hmms written general version em algorithm described :10.1.1.133.4884
basic idea em algorithm hidden variable joint distribution observed variable simpler marginal distribution observed variable 
hmms iohmms hidden variable state path seen simpler compute represent 
hidden variable em algorithm looks expectation values hidden variable log probability joint distribution 
smoothing algorithm compute posteriori estimate distribution jy state path sequence inputs outputs 
prediction algorithm allows compute distribution states outputs past input output observations 
section consider state space models hidden state variable continuous hybrids discrete continuous state variables similar time series modeling applications 
em hmms iohmms section sketch application em expectation maximization algorithm hmms iohmms :10.1.1.133.4884
papers baum special case em algorithm applied discrete emissions hmms written general version em algorithm described :10.1.1.133.4884
basic idea em algorithm hidden variable joint distribution observed variable simpler marginal distribution observed variable 
hmms iohmms hidden variable state path seen simpler compute represent 
hidden variable em algorithm looks expectation values hidden variable log probability joint distribution 
neural computing surveys www icsi berkeley edu jagota ncs expectation called auxiliary function conditioned previous values parameters training observations 
neural computing surveys www icsi berkeley edu jagota ncs expectation called auxiliary function conditioned previous values parameters training observations 
step algorithm consists forming conditional expectation eq log eq expectation fy tn set output sequences similarly respectively sets input state sequences 
em algorithm iterative algorithm successively applying step step 
step consists finding parameters maximize auxiliary function 
th iteration argmax shown increase brings increase likelihood algorithm converges local maximum likelihood yjx :10.1.1.133.4884
maximization done exactly increases iteration gem generalized em algorithm 
maximization general done solving system equations hmms iohmms state space models simple emission transition distributions done analytically 
discuss case discrete states expectation equation corresponds sum values state variable solution equation obtained efficiently recursive algorithms 
see rewrite joint probability states observations introducing indicator variables value log jx log jq gamma log gamma joint log probability training set sum training sequences sum 
asynchronous iohmms proposed speech recognition applications map input sequences output sequences different length 
represent particular type probabilistic transducers discussed section 
acceptors transducers way view hmm way weigh various hypotheses 
example speech recognition hmms different sequences speech units corresponding subset possible state sequences associated different weights fact joint probability state sequences acoustic sequence 
generally weighted acceptors transducers assign weight sequence pair input output sequences :10.1.1.126.6810
weighted acceptors transducers attractive applications speech recognition language processing conveniently uniformly represent integrate different types knowledge sequence processing task 
advantage framework deals easily sequences different lengths 
furthermore algorithms transducers acceptors applied weight structures include limited probabilities useful sequence processing task involves processing numbers necessarily probabilistic interpretation 
weighted acceptor maps sequence scalar may probability example 
represents joint distribution speech unit label sequences acoustic observations sequences 
transducers convenient represent hierarchical structure linguistic units designers speech recognition systems usually embed hmms 
language model speech recognition fact acceptor assigns probability possible sequence labels certain language 
acoustic transducer ju assigns probability speech unit phoneme particular context subsequence acoustic data intermediate transducers represent mapping sequences speech units sequences words jw 
generic composition operation allows combine cascade transducers acceptors joint distribution acoustics phonetic speech units words conditional independence neural computing surveys www icsi berkeley edu jagota ncs different levels ju jw integrates different levels knowledge data hierarchical representation speech shown :10.1.1.126.6810
search algorithms viterbi algorithm beam search 
look sequence values intermediate variables states hmms speech units words 
generalized transducers way generalize transducers proposed allows kind data structure labels discrete symbols sequences processed generalizes transducers parameterized operations weighted graphs allows cascade generalized transducers acceptors jointly trained respect global criterion 
framework data processing viewed transformation directed acyclic weighted graphs directed acyclic weighted graphs call hypotheses graphs 
line applications em algorithm backward pass equivalent equations 
hybrids discrete continuous state disadvantage discrete representation state inefficient representation comparison distributed representation multiple state variables 
state variable take values logn bits information past observation sequence carried value 
example binary variables exponentially bits available 
types algorithms require exponentially computation called factorial hmms proposed properties approximations em algorithm cost grow exponentially :10.1.1.16.2929
models factorial distributed state appealing expressive power lot research trying computationally efficient learning algorithms see example 
hand models continuous valued state typically restricted model reasons computational tractability learning algorithm 
case problem em algorithm requires computing integrals sums done analytically gaussian case done numerically general 
model abrupt gradual changes time series researchers fact proposed hybrids state space models discrete state hmms iohmms known state space models switching jump linear systems 
models factorial distributed state appealing expressive power lot research trying computationally efficient learning algorithms see example 
hand models continuous valued state typically restricted model reasons computational tractability learning algorithm 
case problem em algorithm requires computing integrals sums done analytically gaussian case done numerically general 
model abrupt gradual changes time series researchers fact proposed hybrids state space models discrete state hmms iohmms known state space models switching jump linear systems 
see review models :10.1.1.30.5334
early models assume parameters distribution known priori approximate em algorithm heuristic step require exponential computations :10.1.1.131.2084
expensive monte carlo simulations address problem 
function lower bound log likelihood maximized tractable algorithm :10.1.1.30.5334
uses idea variational approximation proposed intractable models 
hand models continuous valued state typically restricted model reasons computational tractability learning algorithm 
case problem em algorithm requires computing integrals sums done analytically gaussian case done numerically general 
model abrupt gradual changes time series researchers fact proposed hybrids state space models discrete state hmms iohmms known state space models switching jump linear systems 
see review models :10.1.1.30.5334
early models assume parameters distribution known priori approximate em algorithm heuristic step require exponential computations :10.1.1.131.2084
expensive monte carlo simulations address problem 
function lower bound log likelihood maximized tractable algorithm :10.1.1.30.5334
uses idea variational approximation proposed intractable models 
simpler version idea physics mean field approximation statistical mechanics systems 
model abrupt gradual changes time series researchers fact proposed hybrids state space models discrete state hmms iohmms known state space models switching jump linear systems 
see review models :10.1.1.30.5334
early models assume parameters distribution known priori approximate em algorithm heuristic step require exponential computations :10.1.1.131.2084
expensive monte carlo simulations address problem 
function lower bound log likelihood maximized tractable algorithm :10.1.1.30.5334
uses idea variational approximation proposed intractable models 
simpler version idea physics mean field approximation statistical mechanics systems 
note kind hybrid model trade may occur choice model class fits data distribution efficiency training model 
similar trade offs generality model intractability learning algorithm described variants hmms finite state learning algorithms 
state variable keeps informations past sequence discards 
captures temporal dependencies 
shown markovian models including hmms iohmms markov switching models partially observable markov decision processes learning long term dependencies sequential data exponentially difficult span dependencies increases 
problem bad conditional models iohmms conditional markov switching models partially observable markov decision processes state state transformation conditioned extra information generally deterministic 
promising direction proposed manage problem split state variable multiple sub state variables may operate different time scales slow variables easily represent longer term context :10.1.1.16.2929:10.1.1.16.2929
see related bayesian networks factor state variable represent different types contexts 
ffl models raise general problem intractability computation likelihood step em algorithm 
address problems introduced promising methodology variational approximation tractable substructures bayesian network 
idea applied hybrids continuous discrete state variables :10.1.1.30.5334
promising direction proposed manage problem split state variable multiple sub state variables may operate different time scales slow variables easily represent longer term context :10.1.1.16.2929:10.1.1.16.2929
see related bayesian networks factor state variable represent different types contexts 
ffl models raise general problem intractability computation likelihood step em algorithm 
address problems introduced promising methodology variational approximation tractable substructures bayesian network 
idea applied hybrids continuous discrete state variables :10.1.1.30.5334
ffl transducers offer generalization markovian models applied wide range learning tasks complex priori structural knowledge task smoothly integrated learning examples 
local probabilistic assumptions interpretations numbers processed learning algorithm may wrong inconsistent data normalization imposed probabilities may correspond strong assumptions correct solution 
difficulties inherent making probabilistic assumptions interpretations avoided removing local probabilistic assumptions delaying probabilistic interpretation final level decision 
ffl problem non stationary time series addressed certain extent iohmms markov switching models long new regimes time series resemble seen regimes 
baldi bioinformatics machine learning approach 
mit press 
chrisman reinforcement learning perceptual aliasing perceptual distinctions approach proceedings th national conference artificial intelligence pp 

nowlan mixtures controllers jump linear non linear plants advances neural information processing systems cowan tesauro alspector eds san mateo ca morgan kaufmann :10.1.1.29.8423
bengio frasconi input output hmm architecture advances neural information processing systems tesauro touretzky leen eds pp :10.1.1.29.8423
mit press cambridge ma 
jordan learning fine motion markov mixtures experts advances neural information processing systems mozer touretzky perrone eds mit press cambridge ma 
riley pereira weighted finite automata tools applications speech language processing tech 
mit press 
chrisman reinforcement learning perceptual aliasing perceptual distinctions approach proceedings th national conference artificial intelligence pp 

nowlan mixtures controllers jump linear non linear plants advances neural information processing systems cowan tesauro alspector eds san mateo ca morgan kaufmann :10.1.1.29.8423
bengio frasconi input output hmm architecture advances neural information processing systems tesauro touretzky leen eds pp :10.1.1.29.8423
mit press cambridge ma 
jordan learning fine motion markov mixtures experts advances neural information processing systems mozer touretzky perrone eds mit press cambridge ma 
riley pereira weighted finite automata tools applications speech language processing tech 
rep technical memorandum tm bell laboratories 
ron singer tishby power learning probabilistic automata variable memory length machine learning vol 

singer adaptive mixtures probabilistic transducers neural computation vol 

hamilton new approach economic analysis non stationary time series business cycle econometrica vol :10.1.1.30.5334
pp 
march 
dynamic linear models switching amer :10.1.1.131.2084
stat 

hamilton new approach economic analysis non stationary time series business cycle econometrica vol :10.1.1.30.5334
pp 
march 
dynamic linear models switching amer :10.1.1.131.2084
stat 
assoc vol 
pp 

stat 
assoc vol 
pp 

ghahramani hinton switching state space models tech :10.1.1.30.5334
rep technical report university toronto 
rabiner tutorial hidden markov models selected applications speech recognition proceedings ieee vol :10.1.1.131.2084
pp 

pp 

ghahramani hinton switching state space models tech :10.1.1.30.5334
rep technical report university toronto 
rabiner tutorial hidden markov models selected applications speech recognition proceedings ieee vol :10.1.1.131.2084
pp 

baum inequality applications statistical prediction functions markov processes model ecology bull 
amer 
experiments application iohmms model financial returns series tech 
rep informatique recherche op universit de montr eal 
abe warmuth computational complexity approximating distributions probabilistic automata machine learning vol 
july 
valiant theory learnable communications acm vol :10.1.1.116.3824
pp 

sipser inference minimization hidden marko chains proceedings th annual acm conference computational learning theory colt pp 
acm 
pp 

vapnik nature statistical learning theory 
new york springer 
markov example statistical investigation text eugene illustrating coupling tests chains proceedings academy science st petersburg vol :10.1.1.133.4884
pp 

pearl probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann 

press decoding instance pearl belief propagation algorithm ieee journal selected areas communications 
zweig russel speech recognition dynamic bayesian networks proceedings aaai conference madison wisconsin aaai press 
neural computing surveys www icsi berkeley edu jagota ncs zweig russel probabilistic modeling bayesian networks asr proceedings international conference statistical language processing sidney australia 
gray vector quantization ieee assp magazine pp :10.1.1.116.3824
april 

lee automatic speech recognition development sphinx system 
kluwer academic publ 
semi continuous hidden markov modeling international conference acoustics speech signal processing pp 

huang jack hidden markov models speech recognition 
edinburgh university press 
dempster laird rubin maximum likelihood incomplete data em algorithm journal royal statistical society vol :10.1.1.133.4884
pp 

lauritzen em algorithm graphical association models missing data computational statistics data analysis vol 
pp 

markov model switching regressions journal econometrics vol 
pp 

approach time series smoothing forecasting em algorithm journal time series analysis vol :10.1.1.16.2929
pp 


lee serial correlation discrete variable models journal econometrics vol 
cai markov model unconditional variance arch journal business economic statistics 
business cycle duration dependence parametric approach review economics statistics vol 
pp 

evidence business cycle duration dependence business cycles indicators forecasting stock watson eds chicago university chicago press :10.1.1.16.2929
time series model periodic stochastic regime switching tech 
rep discussion universite de montreal montreal quebec canada 
garcia effects monetary policy asymmetric tech 
rep montreal quebec canada 
theory practice recursive identification 
mit press 
ghahramani hinton parameter estimation linear dynamical systems tech 
rep technical report crg tr university toronto 
ghahramani jordan factorial hidden markov models advances neural information processing systems mozer touretzky perrone eds mit press cambridge ma :10.1.1.16.2929
neal connectionist learning belief networks artificial intelligence vol 
pp 

hinton dayan frey neal wake sleep algorithm unsupervised neural networks science vol 
