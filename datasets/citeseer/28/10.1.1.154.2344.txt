cognitive science symbolically speaking connectionist model sentence production franklin chang beckman institute university illinois urbana champaign north mathews avenue urbana il usa received september received revised form may accepted may ability combine words novel sentences argue humans symbolic language production abilities 
critiques connectionist models language center inability models generalize symbolically fodor pylyshyn marcus 
address issues connectionist model sentence production developed 
model variables role concept bindings inspired spatial representations landau jackendoff 
order take advantage variables novel dual pathway architecture event semantics proposed shown better symbolic generalization variants 
architecture pathway mapping message content words separate pathway enforces sequencing constraints 
analysis model hidden units demonstrated model learned different types information pathway model compositional behavior arose combination pathways 
model ability balance symbolic statistical behavior syntax acquisition model aphasic double provided independent support dual pathway architecture 
franklin chang 
published cognitive science society rights reserved 
keywords neural networks psychology language acquisition learning cognitive architecture computer simulation important language able talk novel events circumstances 
order need ability take words know combine novel ways 
applying knowledge new situation involves generalizing knowledge context originally learned 
example nouns sentence tel fax 
mail address osgood cogsci uiuc edu chang 
see front matter franklin chang 
published cognitive science society rights reserved 
pii chang cognitive science frames paired 
teach count noun produce sentence heard manner 
ability combine words sentence frames absence previous experience led researchers argue language requires symbolic capabilities knowledge language phrased terms variables operations variables fodor pylyshyn marcus pinker prince 
addition arguments symbolic processing research shows people recording detailed statistical properties sentences hearing producing 
source evidence role frequency language processing frequencies words syntactic structures influence processing language pearlmutter myers macdonald pearlmutter seidenberg 
statistical regularities sufficiently rich people encounter novel language sequences similarity novel sentences sentences experienced process novel sequences 
language system require symbolic statistical types knowledge theories developed separate mechanisms implement types processing theories called dual mechanism theories 
example type theory concerns processing english past tense 
english past tense regular form walk walked exceptional cases run ran 
pinker prince offer dual mechanism account regular form handled symbolic mechanism rule uses variables exceptional cases handled mechanism sensitive statistical regularities spreading activation lexical network 
theorists argued statistical learning powerful explain symbolic statistical processing single mechanism plunkett rumelhart mcclelland 
evidence certain classes connectionist models generalize way people 
example marcus simple recurrent network srn learn equivalence relations rose rose tulip tulip novel sentence fragment srn predict going word 
model activates words seen sentence position rose tulip 
humans experience equivalence sentence infer equivalence relation intended leads complete novel sentence fragment word 
srns complete novel fragment words seen similar sequences directly representing sequences experienced learning 
suggests develop variable frames variable bound word 
limitation important srns extensively modeling acquisition syntactic frames statistical regularities language processing christiansen chater elman plaut st john mcclelland 
sense problem generalization ability srns reflects basic problem statistical learning 
representations shaped learning difficult representations novel situations 
experience driven learning reduced incorporating specialized chang cognitive science mechanisms connectionist models yield models symbolic abilities hummel holyoak shastri ajjanagadde 
language requires symbolic representations bound lexical structural representations specific particular language representations incorporate statistical regularities clear specialized mechanisms integrate statistical representations way yield human language performance 
task connectionist models guide statistical learning develop representations operate symbolically extent humans operate symbolically 
symbol processing overt behavior definitions symbolic computation vary 
definitions require symbol processor ability bind instances variables variables rules operations fodor pylyshyn hadley marcus 
rules operations operate variables novel elements bound variables 
provide explicit account symbol processing instantiated statistical learning system compare generalization ability models sentence production 
describe messages grammars models section 
different model architectures compared section 
model architecture prod srn simple extension connectionist sequencing models production 
second model architecture dual path model novel model architecture features allow generalize symbolically 
feature model spatial representations people act symbolically objects world help model symbolic processing sentence production 
better understand architecture variants event semantics linked path models 
section describe results simulations model architectures section 
examined specific tests symbolic generalization understand architectures differ sections 
remainder article focuses dual path model successful generalization tasks 
see model represents tasks examine internal representations section 
show model computational properties explain human acquisition aphasia data 
acquisition syntactic structures model compared acquisition children see model constrains overgeneralization verbs syntactic structures way similar children section 
model lesioned see architecture consistent double aphasia section 

message structure sentence grammar speaking involves mapping set ideas called message toa sequence words bock levelt garrett 
learn mapping children exposed sentences situations infer message 
language researchers assume children implicitly learn internal representations help map messages sentences representations allow produce novel sentences pinker 
simulate language learning process training models created set training sentences sampled grammar 
model learns chang cognitive science table example sentences grammar sentence type subtype example identity locative motion intransitive dog dog 
cat near cafe 
dog go near church 
cat sleep 
transitive active dog chase cat 
passive cat chase dog 
transfer dative prepositional man give cup woman 
double object man give woman cup 
dative prepositional man bake cake woman 
double object man bake woman cake 
change state locative patient girl fill cup water 
cause motion patient locative girl pour water cup 
spray load locative patient girl spray wall water 
patient locative girl spray water wall 
rules grammar limited number training sentences exhibits knowledge producing sentences generated grammar 
grammar designed enable testing phenomena psychological literature sentence production 
table shows types sentences model grammar 
grammar include subject verb agreement verb inflections phenomena examination require morphemes eliminating model simpler 
creating data set training testing set messages generated 
messages defined propositional content target sentence encode actual surface structure sentence 
message created selecting action entities appropriate action 
example action eat paired entity living eater object living liquid object eating 
representation select lexical items matched constraints action 
action eat eater man object cake 
participants event classified event roles agent patient goal 
agent cause action goal final location object patient object motion affected object 
roles match exactly traditional definitions roles see dowty arguments traditional roles designed increase generalization capabilities model chang dell bock griffin 
example distinction themes patients collapsed role patient 
location arguments goals collapsed category model 
distinctions categories collapsed model expressed verb specific semantic information 
chang cognitive science model lexicon verbs nouns prepositions determiners adjectives sentence marker 
nouns animate inanimate 
verb types included dative give throw bake transitive hit build eat drink surprise scare change state fill spray load alternation spray load cause motion put pour intransitive sleep dance motion go walk existence 
training testing sets verbs equal probability selected 
existence intransitive verbs easy learn proportion reduced give verbs training see appendix details 
training message paired particular sentence structure 
natural languages allow particular meaning expressed alternative structures syntactic alternations shown table 
example active passive voice sentences transitive alternation similar meanings differ order noun phrases structural properties 
alternation model dative alternation prepositional dative double object dative express closely related meanings 
alternation occurred transfer give throw bake 
alternation spray load alternation varied order patient goal 
generation sentences arranged transitive sentences paired active voice rest passives 
spray load structures alternative occurred approximately time 
create extra variability structures produced percentages modified arguments sentences animate nouns tend go inanimate nouns time structures alternate 
distribution structures grammar vastly oversimplified real frequencies structures world maintained character alternations 
relationship meaning structure language arbitrary 
regularities way arguments message expressed syntactic distributions 
argued mapping meaning form represents unit language knowledge called construction constructions useful explaining people syntactic knowledge goldberg 
important feature construction meaning simply combination meaning component words speakers generalize words constructions paired intransitive say napkin table encode cause motion napkin 
meaning construction represented event semantics different semantics associated lexical concepts 
event semantics identify similarities constructions table helped models generalize construction related construction 
example intransitive motion construction girl goes cafe related cause motion construction woman put dog table girl dog undergoing motion 
represented having constructions share event feature motion 
cause motion construction related transfer construction man gives dog girl shared features cause motion 
evidence children adults sensitive event features language knowledge fisher gleitman gleitman pinker hollander goldberg wilson pinker hollander goldberg 
chang cognitive science table constructions sentence type event semantics verbs identity exist locative motion motion go walk intransitive sleep dance transitive cause affected hit chase eat drink cause create bake cause experience surprise scare transfer dative transfer give throw dative cause affected transfer hit chase eat drink cause create transfer bake cause experience transfer surprise scare change state cause change fill cause motion put pour spray load cause change motion spray load mentioned earlier example action eat arguments verb constrained appropriate 
implement knowledge construction associated argument constraints 
example goals cause motion events cake woman pushed car cake allowed inanimate goals transfer dative events required animate man woman gave car man 
constraint adjectives divided classes restricted animate arguments nice silly funny loud quiet restricted red blue pretty young old 
constraints sentences generated plausible allowed grammar generate implausible sentences 
difficult incorporate world knowledge needed constrain grammar way human language constrained 
doing model comparisons plausibility grammar model types differences models attributed constraints 
generating training testing sentences event semantics determine messages alternate 
order alternate messages related alternative structures means event features goldberg message structure mapping represented separate construction 
example messages event features cause motion transfer double object structure man give girl book designated default structure features 
combination features overlaps cause motion construction cause motion prepositional dative structure man give book girl 
spray load alternation arose messages event features cause motion change associated constructions 
cause motion construction licensed cause motion selected structure put patient goal man spray water wall chang cognitive science change state construction licensed cause change associated order put goal patient man spray wall water 
passive structure allowed alternate transitive constructions 
event semantics intended message influence sentence structure chosen speakers choose structure factors 
production studies people told repeat sentences hear part able 
verbatim memory structure guiding choice 
doing repetition people frequently change structure sentences potter 
suggests information message allows people control structure building information weak overcome factors bock 
represent weak control information model relative activation level event semantics 
consider active passive alternation 
passives affected feature active cause feature vice versa active sentences 
transfer feature active motion features double object produced prepositional dative produced 
spray load alternation feature change active motion locative patient sentence produced patient locative produced 
set differences prominence parameter set controlled difference activation levels features 
example activation feature motion activation transfer feature double object structure desired 
speakers select alternations production 
experimental sentence production shown speakers plan sentences incrementally adjusting structures fit words come bock ferreira ferreira dell 
create ability models models needed feedback previously produced words 
types feedback type corresponds feedback production corresponds feedback comprehension 
feedback type fixed sentence sentence experienced production mode comprehension mode 
production mode involved passing previously produced words input words sentence 
correspondingly comprehension mode involved passing previous heard target words input 
comprehension production modes attempt predict word sequence message differ terms external sequence help 
model learning production production outputs early training useful learning language larger percentage training sentences comprehension mode rest production mode 
testing model tested production mode interested production behavior 
sentence grammar generate training sentences 
order test model ability generalize training sentences extra restriction word dog goal sentence 
testing model ability produce dog goal sentence trained see model generalizes outside regularities training set 
test generalization test set created randomly generated sentences grammar 
grammar generate possible messages including surface form alternations chang cognitive science training set small testing set novel sentences provide picture accuracy model 

symbolic generalization different architectures show neural networks exhibit symbolic properties architecture influence ability architectures described compared 
architecture prod srn model model embodies hypothesis symbolic generalization simply due learning appropriate statistical representation 
non symbolic model compared model features called dual path model 
dual path architecture uses message variables sequences variables event semantics 
addition model places limits sequential information interact lexical semantics effectively creating pathways architecture 
show model behavior simply due addition variables compared version model lacks event semantics event semantics model 
show power variables depends dual pathways architecture dual path model compared fourth model linked path model links pathways dual path model keeps separate 
linking pathways expected diminish combinatorial abilities model 
addition training testing sets models shared features 
models way representing message message set external manipulation message production sentence 
models taught produce words output single unit represented word 
increase models tendency choose single word output units employed soft max activation function magnified activation differences see appendix details 
models trained back propagation error learning algorithm computes difference target representation model output passes information back network order guide weight changes rumelhart hinton williams 

statistical learning production prod srn model production simple recurrent network prod srn fig 
srn elman augmented message dell chang griffin jordan 
part network mapped representation previous word word sequence 
output word units received inputs set hidden units hidden units received inputs previous word indicates input feedback comprehension system set context units copy previous hidden unit states 
production involves planning sequence intended meaning opposed sequence prediction prod srn included static message 
message connected srn hidden units allowed model message guide sequence generation 
message representation binding space chang dell chang cognitive science fig 

prod srn model 
mcclelland kawamoto st john mcclelland 
different event roles represented different banks units 
bank set units represented slot message role slots agent patient goal slot action 
roles localist semantic representation unit meaning dog agent slot dog chased cat separate unit dog patient slot cat chased dog 
action represented unique action feature 
event semantics included message action slot giving event semantic feature unit 
table example message sentence man bake cake cafe 
message separate set semantic features slot message features slot labeled number agent patient goal show different feature slot cake cake cake 
action slot overlap slots features extra number index 
message event semantics features cause create transfer verb specific feature bake 
definite articles marked slot specific feature definite 
indefinite articles marked occur mass lexical items coffee leaving unmarked depend lexical semantic information information available 
output model localist representation words lexicon 
hidden layer context layer units context units initialized sentence 
mentioned section training testing sets representation set target previous word time comprehension table example message binding space role action agent patient goal features create transfer man cake cafe definite chang cognitive science mode set previously produced word time production mode 
model learns production comprehension units set sum previous word output target previous word 
production units solely dependent model production output comprehension units combination previous produced word previous target word 
analog human behavior people people say units filled predicted continuations word units 

symbolic connectionist model dual path model dual path model designed generalize symbolically differed substantially prod srn model 
language production symbolic generalization exhibited placing words novel sentence positions 
learn new word word variety frames 
get word generalization mapping lexical semantics word forms regardless word occurs sentence 
capturing lexical sentence level aspects words similar problem spatial processing visual input categorize object record position scene landau jackendoff 
process object categorization remove location specific information transform object take account point view viewer order get invariant representation categorization kosslyn 
process locating object hand need concern identity object order determine position space 
functions identified separate brain structures object location pathways mishkin ungerleider 
separate representations bound order know object occurs location 
resulting system recognize known objects new locations identify location unfamiliar objects 
generalizes 
separation binding object location information 
just spatial system generalize different ways separate representations model sentence production able generalize represented message separate representations linked 
idea basis dual path model 
architecture pathways representing mapping object semantics word forms representing mapping objects words describe appropriate sentence positions fig 

pathway model message lexical system see thick arrows fig 

subnetwork feed forward network message lexicon 
message model represented weight bindings layer units thematic layer units semantic 
type representation units represent meaning word regardless event role 
units represented agent patient goal event roles unit represented action information 
units represented semantics words localist representation prod srn 
messages model represented setting weight units units arbitrary value see appendix details 
chang cognitive science fig 

dual path model 
example dog agent agent unit connected dog feature units 
dog patient patient unit connected dog feature 
way represent different roles dog events maintaining common semantics dogs share 
part message lexical system connection units word units 
allowed model learn word label meaning 
set units prod srn model duplicated analogous units event role learning mapping semantic feature dog word dog allowed model generalize word event roles 
single semantic lexical network model line mainstream lexical production theories dell levelt meyer message lexical mapping prod srn model 
second pathway network sequencing system simple recurrent network inputs see thin arrows fig 

network mapped units hidden layer 
hidden layer received input context units prod srn model history previous states copy previous hidden unit states 
hidden layer mapped word units message lexical system 
elman model lexical layers word hidden units compression layers compress helped sequencing network create generalizations words word specific representations 
sequencing subnetwork received input reverse version message lexical network hidden 
subsystem model able vary sentence structures role previously produced word 
said cat sentence cat chased dog cat chased dog 
knowing role cat plays message know continue sentence active passive structure 
reverse chang cognitive science message lexical network tells sequencing network role word model produced allowed dynamically adjust rest sentence match earlier choices 
network mapped units units units variable binding units set analogous reverse binding initiation production sentence 
order model links learn mapping 
learn meaning word comprehension direction 
error signal word units produced word back propagated weights network error information weakened passes back network 
error signal words sufficient learn mapping way help learning production 
help units learn units provided previous units activation target activations 
units activation depend links units initially model learned control units targets give system 
happened model bootstrapped word learning incrementally learning comprehend previously produced semantics 
example suppose model learning sentence agent cat 
training network random weights 
get error signal units model learn unit cat linked unit cat model needs activate production unit cat activating agent unit layer 
activation layer depended hidden unit states states turn depend information 
slowly model learned activate units appropriately production unit activations distinctive error passed back units 
intuitively model learn pick role message associated word hears 
evidence children similar abilities actively guide attention elements scene learn right meanings words 
children go model respect addition able control attention word learning sophisticated joint attention abilities infer intended referents baldwin tomasello 
details dual path architecture mentioned 
hidden layer dual path model smaller prod srn model units units architecture hidden layer task mapping message elements words 
units soft max units forces units choose winner reduce activation competitors 
help model remember event roles produced model set context units called 
units summed activation previous states 
units strongly biased represent role input units helped model record history roles model gone 
hidden units received inputs set units held event semantics intended construction 
event semantics units helped sequencing system create appropriate sequences construction 
functionality event semantics examined comparing dual path model event semantics model identical model lacked features 
table example message message event semantics chang cognitive science role action agent patient goal event semantics features bake man cake cafe definite cause create transfer table shows dual path model represent example message earlier man bake cake cafe 
model prod srn model set semantic features features indexed number 
event semantics layer held construction specific features 
points message lexical sequencing systems interacted 
point connection hidden units sequencing system units message lexical system 
allowed model sequence units enabled produce message related words appropriate places 
sequencing network access message tended develop representations independent lexical semantic content intended message 
representations tended syntactic show analysis hidden units 
second point interaction systems word units 
message lexical system activated meaning related possibilities sequencing system activated syntactically appropriate possibilities 
intersecting activation sources enabled production message appropriate words message lexical system proper positions sentences sequencing system 
separate networks mapping consistent sentence production showed lexical semantic factors syntactic factors independent effects sentence structures bock bock 
complexity model useful example fully trained model produce sentence man baked cake cafe 
demonstrate operation trained model production mode comment differences occur comprehension mode 
feed forward connectionist models break processing timesteps 
timestep activation propagated forward network back propagation networks error back propagated 
timestep message set links see appendix 
case agent unit linked man semantics patient linked cake semantics goal linked cafe definite features corresponding reverse links set links 
message set external manipulation 
event semantics units set time target sentence dative construction feature cause activated feature create activated feature transfer 
model learned sentences event semantics activation values associated activations units appropriate chang cognitive science dative structure event semantics units helpful creating target order 
setting message event semantics units production timestep 
context units needed initialized default values timestep normally activation values come activation units previous timesteps 
units set context units set 
activation spread bottom units top word units identifying word output 
model production mode produce word inputs initialized 
model compares output target difference error back propagated adjust weights model better producing word sentences message 
second timestep word output copied back units 
activation values units previous timestep target values units point helps model associate activation agent lexical semantics man model experiences variety different nouns doesn learn strong connection particular noun 
activation units copied units summed previous activation units 
context units receive copy previous hidden units states 
activation spread word units model trained say man point 
model output compared comprehended target word man difference adjust weight back propagation 
process setting copied units input units spreading activation forward back propagation continues word sentence 
comprehension mode difference external comprehended word previously produced word summed set activation values 
mentioned earlier primary inspiration representation message weights units inspired distinction spatial processing object location processing 
visual representations pre segmented types representations representations bound temporary links representations instantiate type variable representation location variables index semantic content 
language location variables instantiated systems long new concepts temporarily bound variables language system new concepts constructing sentences 
arrangement allow connectionist model symbolic abilities statistical learning develop sequences activate variables variables allow novel elements incorporated sequences 
prediction approach spatial factors influencing language processing paragraphs provide evidence influence 
idea spatial factors influence language long tradition certain linguistic theories cognitive grammar lakoff langacker talmy 
theories syntactic operations represented movement spatial representation theories particularly explaining non motion events motion vocabulary change states mood went chang cognitive science bad 
cognitive argue non motion constructions spatial path trajector mood source state goal bad state allows speakers talk state changes movement spatial representation 
related claim organization spatial system influences organization syntactic categories 
landau jackendoff suggested distinction nouns prepositions direct result distinctions spatial system object representations location representations 
idea language perceptual representations important part general accounts cognition argue cognition inherently modality specific involves perceptually represented symbols barsalou 
developmental psychologists argued spatial representations important development conceptual representations 
mandler claimed children analyze multi modal perceptual information information internally image schemas path containment force 
image schemas represent spatial relationships 
particular concept fillers participate relationships simply treat variables 
identifying relationships components image schemas children derive important thematic distinctions difference agency 
children perceptual information derive distinctions necessary language reasonable perceptual systems interface manner language 
evidence spatial nature conceptual representations influence language acquisition spite language input children receive 
example comes non conventional uses year old english speakers 
clark carpenter children tended mark agents cases adults marked agents passive preposition really scared tommy caught 
argue children collapsing agents causes spatial source category normally marked preposition drove home 
mark agents suggests source default category agents modified mark agenthood 
addition links representations language spatial processing evidence links spatial system line language production 
griffin bock eye movements picture description coordinated order elements sentences speakers producing way suggested tight connection processes 
system language spatial representations propositional representations mediate show tight connection eye movements sentence structure modular theory mediating representations difficult map backwards syntactic decisions passive structure eye movements look left 
theory spatial organized messages production seen movement attention spatial variables easier understand syntactic processing eye movements closely coordinated 
language spatial representations related relationship implemented 
dual path model assumption link space chang cognitive science language limited organization message 
ways spatial properties influenced representation message 
separation object characteristics relational characteristics separate banks units bindings 
concrete effect reducing number units needed represent message prod srn units dual path units 
second event roles instantiated units thought corresponding components spatial path evidence linguistics jackendoff lakoff developmental literatures mandler suggests distinctions thematic roles agent patient experiencer arise elaborations spatial roles source start event theme object center attention goal event represent path event see regier connectionist model path representation model cross linguistic acquisition preposition 
idea implemented model collapsing thematic roles distinctions spatial roles 
description message representations labeled roles agent patient goal map appropriate label sentences grammar 
agent slot collapsed agents causes labeled source role 
likewise patient slot collapsed patients themes relabeled theme object role 
goal slot represents roles goals recipients locations 
distinction different thematic roles single spatial role represented event semantic features associated units feature experience distinguished patients 
distinction spatial path approach thematic roles effect reducing number units needed represent message 
linguistic developmental processing findings definitive growing consensus language space interrelated see bloom peterson nadel garrett summary issue 
consensus suggests prefer models provide means linking language space link indirectly 
representation dual path model reflects preference 

alternative architectures event semantics linked path models existence variables dual path model lead symbolic abilities model 
recognized advocates symbolic theories variables require lot information control 
having variable called agent tell agents occur subject position active english sentences phrase english passives marked particle ni japanese passives 
language users learn variables way appropriate particular language 
dual path model characteristics constrain variables 
event semantics provide sequencing system information type structure convey variables message 
architecture model allows model ignore content variables decides 
show factors influence symbolic generalization dual path model chang cognitive science compared models lacks event semantics units event semantics model violates architecture dual path model linked path model 
consider event semantics model 
designed identical dual path model event semantics units disabled 
disabling units prevents sequencing system getting information intended message representations syntactic 
sequencing system predictions lexical items produced comprehended previously 
example dual path model event semantics feature transfer activated model restrict sentential subjects animate nouns dative subjects grammar tended animate 
information model tend activate nouns constructions grammar subject sentence animate inanimate 
model event semantics learn syntactic structures link variables message lexical system 
event semantics model implements idea syntactic rules variables system needs symbolic generalization 
dual path network better event semantics model suggests variables useful information guides case event semantics 
comparison dual path linked path model attempt show symbolic capabilities necessarily associated complex models 
dual path model variables special architecture generalize better simpler model prod srn model 
generally true simpler symbolic models fewer capabilities complex models connectionist models learning develop internal representations tend opportunistic information complicated models inappropriate ways learn represent task 
demonstrate dual path model compared identical model link units hidden units 
removes separation pathways 
linked path model hidden weights message information sequencing system develop representations optimal particular sentences training set 
optimization expected reduce ability generalize symbolically 
dual path model generalizes better linked path model concluded separation pathways plays important role acquisition production skills 

model comparison experiments summarize different model architectures compared prod srn dual path event semantics linked path 
prod srn binding space message message 
event semantics model dual path model sequencing system event semantics units 
linked path model dual path model sequencing system access lexical semantic content message hidden links 
different training sets sentences created different random seeds 
sets prod srn dual path event semantics linked path models trained epochs 
amount training resulted accuracy chang cognitive science amount time 
analogy human subjects different people experience different sentences lifetime label model subject refer differences due particular training set 
model type model subjects yielding total models 
model weights initialized random values 
epochs training model tested training set set randomly generated test sentences model subjects 
sentence accurately produced activated output word activation higher threshold matched target output word position sentence 
dependent measure analyses percentage sentences accurately produced sets 
averaged model subjects model types achieved higher accuracy training set training fig 

differed somewhat time took reach achieve accuracy level prod srn reached epochs event semantics reached epochs linked path reached epochs dual path reached epochs final accuracy level epochs shows architectures ultimately differ ability represent knowledge needed produce sentences training set 
test generalization looked accuracy set test sentences generated randomly grammar fig 

test sentences differences architectures evident 
prod srn model generalized 
training accuracy reaches testing accuracy climbs 
event semantics model better reaching final accuracy 
dual path linked path models jumped epochs models reaching maximum accuracy training set 
diverge dual path model reached linked path model fell accuracy 
repeated measures analysis variance anova performed accuracy epoch model types training set random factor 
model type significant 
pairwise fig 

average training set accuracy 
chang cognitive science fig 

average testing set accuracy 
comparisons performed different model types differences significant ps difference dual path model linked path model 
large differences generalization abilities epoch training accuracy approximately suggest architecture plays crucial role model ability generalize 
point notice dual path model lose generalization ability reached accuracy training set 
model continued improve going epoch training epoch 
dual path model avoid overfitting training set 
overfitting problem generalization error learning systems especially model weights hertz krogh palmer 
normally better adapted model particular characteristics training data worse dealing new data 
linked path model may suffer overfitting training set epoch testing set accuracy reached asymptote began decline 
test linked path model overfitting difference sentence accuracy epoch computed model subjects model type epoch insure linked path models reached asymptote 
mean difference negative linked path model positive models prod srn event semantics dual path model type significant 
comparisons differences revealed linked path worse dual path model 
dual path linked path models maintained level accuracy training set period dual path continued improve linked path degenerated 
account linked path model took advantage message information units help model sequencing system memorize regularities training set 
wrong thing wants generalize new sentences 
dual path model avoided overfitting isolation lexical semantics sequencing kept message specific knowledge reducing generalization 
chang cognitive science 
dog goal test part symbolic generalization ability bind words novel event roles generate sentences convey novel meanings 
training model grammar constrained word dog allowed goal sentence 
testing model messages goal bound dog see model generalize experience goals produce novel sentences correctly 
test sentences randomly generated dog goal slot 
weights epoch model subjects model architectures tested dog goal test set 
dependent measure analysis percentage sentences words match target sentence exactly sentence accuracy 
prod srn model produced dog goal sentences correctly fig 

models generalized fairly event semantics model linked path model dual path model 
anova performed model type significant 
pairwise comparisons showed differences significant ps 
dog goal test helps explain dual path linked path models better models previous test generalization test sentences 
models achieved accuracy sentences trained novel test sentences words roles trained training set relatively small compared possible sentences grammar generate 
dual path linked path event semantics models derived benefit dual pathways architecture allowed semantics units different roles 
learned say dog models link semantic unit dog word unit dog allowed said different sentence positions 
prod srn binding space message representation different roles set semantic units 
training set include sentences dog goal semantics dog goal slot dog associated units able unit activate word dog keep producing appropriate position 
fig 

dog goal test result 
chang cognitive science event semantics information crucial dual path model better event semantics model 
event semantics information helped sequencing system know message goal goal tended occur certain sentence positions 
helped model sequence goal unit time goal produced 
difference dual path linked path models suggests linked path model including lexical semantics dative syntactic representations dog follow hurt ability produce novel dog goal sentences 
dog goal test showed slot independent lexical mapping event semantics information syntactic frames effect symbolic generalization 

identity construction test dog goal test test ability models generalize word novel sentence position 
test overestimate generalization abilities models goal occurred sentence constructions know model able continue generate structure producing word novel position 
accidental distributional properties dative construction influencing generalization 
consequently test carried 
inspired marcus claim inability srns produce novel sentences identity construction see models generalize 
identity test takes advantage accidental fact random generation training set subset words identity construction 
recall existence intransitive verbs frequent verbs training 
novel identity construction sentences randomly generated model subject actual number varied training set different identity sentences 
model subjects model architecture tested sentences epoch fig 

difference dual path model models quite dramatic 
dual path model accuracy models get 
anova performed model type significant 
pairwise comparisons dual path model superior prod srn fig 

identity construction test results 
chang cognitive science event semantics models ps marginally superior linked path 
fact dual path model better test surprising dog goal test 
surprising bad models producing novel words simple construction 
models identity construction patient slot instantiate single argument construction 
models experience mapping words patient slot pre post verbal sentence positions constructions 
experience able information help generalization 
developed sequencing representations specific particular words experienced construction 
prod srn linked path models particular probably learning lexical semantic specific mappings event semantics model probably learned verb followed set nouns experienced training 
separation lexical semantics sequencing dual path model allowed sequencing system avoid lexical semantic information representation identity construction event semantics helped dual path model strongly activate message lexical system appropriate time overwhelm lexical specific sequencing regularities sequencing system picked 
identity construction test similar marcus test differs models compared previous experience placing nouns surface positions 
marcus test novel word human experience placing novel word sentence position 
models previous experience placing nouns positions prod srn event semantics models experience increase generalization 
models compared dual path model gets right 

novel adjective noun pairing test previous generalization tests involved placing noun structure trained 
ability natural outcome incorporating variables bound semantics phrases phrase ordering knowledge generalize variables 
question symbolic generalization model requires element variable 
question addressed looking novel sequences noun phrases phrasal semantics bound single unit phrase internal elements variables 
test model exploited restrictions adjectives training grammar 
grammar restricted adjectives pair appropriate nouns 
kinds adjectives restricted animate entities nice silly funny loud quiet restricted red blue pretty young old 
dogs nice nice 
dogs old 
previous training test sets restrictions enforced 
people metaphorical extensions animate adjectives inanimate elements 
example car nice easy maintain 
wall silly painted crazy fashion 
see model generalize symbolically separate variables produce nice car silly wall message calls 
chang cognitive science fig 

novel adjective noun pairs test result 
randomly generated test sentences generated animate adjectives attached inanimate nouns 
model subjects model type tested novel adjective noun test set epoch fig 

dual path model best producing sentences correctly 
linked path model produced event semantics model produced correct prod srn worst correct pairwise comparisons differences significant ps 
ability dual path model generalize better models case primarily due system previous tests 
earlier tests model produced appropriate unit right time models chance generalizing symbolically 
adjective noun semantics units connected unit strategy case phrase internal sequences 
model develop way sequence words symbolic manner variables 
occur model get things right 
appropriate unit phrase activated 
event semantics model probably activate unit appropriately know message 
second part sequence words phrase occurrence frequency 
prod srn model record lexical specific occurrence frequency hidden units access semantics phrase prefer adjectives followed animate nouns 
dual path linked path models able meet requirements producing novel phrases 
event semantics helped activate right unit right time 
compress units sequencing system kept models recording lexical specific occurrence frequencies 
fact models metaphorical extension suggests model developed symbolic ability sequence words phrases addition ability symbolically sequence phrases sentence 

symbolic generalization different architectures dual path model better generalization models 
dimensions manipulated comparisons 
message type 
chang cognitive science prod srn slot message models representation 
message allowed models learn syntactic structures units activate variable information links 
comparisons models message better prod srn generalizing definite benefit type variable representation 
second dimension architecture network 
issue separation message syntactic representations needed achieve generalization 
comparison seen differences dual path network linked path network 
networks equivalent linked path network linked pathways allowed syntactic representations information message produced 
dual path model clearly better linked path model magnitude generalization measures significantly better comparisons marginally significant 
comparison shows architecture plays crucial role keeping model learning wrong representations symbolic generalization 
third dimension manipulated presence absence event semantics information 
true dual path model better event semantics model measures higher performance solely due existence event semantics 
recall prod srn model message representation event semantics information 
prod srn model event semantics help learning sequencing constraints learn individual combinatorial relationship event semantics features message semantic lexical mappings separately 
dual path architecture event semantics connected srn blind message able sequence variables units 
network value event semantics increased sequence variables frames 
addresses limitations connectionist models marcus points worthwhile frame model comparison terms notion training space 
marcus argues multi layer connectionist models back propagation generalize training space marcus 
training space set input feature values experienced training 
input feature values associated output outcomes model interpolate input values find interpolated output values 
outside training space models extrapolate find appropriate output values 
model types received training set model subject architecture models created different training spaces model 
prod srn model message role occupies different units 
means training space role dependent word semantics trained particular role generalize appropriately 
models set units represents lexical semantics event roles 
model learns produce word correctly word semantics training space 
linked path dual path models event semantic units ability produce sentence construction correctly event semantic inputs allows sentences construction training space 
problem linked path model sequence representations uses contaminated lexical semantic information chang cognitive science link message lexical system sequencing system 
dual path model overcomes limitation isolating systems forcing sequencing system limited number syntactic categories distinctions useful 
dual path model successful capturing character human sentence generalization training space dual path model divided constructions operate syntactic categories variables lexical semantic representations select words way dividing language appropriate characterizing human language 
surprising aspect model comparison relationship variables symbolic generalization 
literature symbolic generalization fodor pylyshyn expect straightforward relationship existence variables ability generalize novel elements 
novel adjective noun pairings show symbolic generalization arise separate variables element event semantics model showed variables meaning generalize 
comparisons suggest symbolic generalization language production really separate types generalization 
interaction variables syntactic categories yield type novel pairings event semantics variables yield different kind 

hidden units analysis order understand pathway dual path model works valuable examine activation units model process sentences 
useful look compress units understand sequencing system units directly influence production words effects sequencing system words propagated layer 
see message lexical system works useful look corresponding input word units units units depend message specific links 
looked activation units message dependent 
single dual path model tested novel sentences test set training epoch activation output compress units recorded 
units agent patient goal action compress units 
average activation units model subject tested novel sentences calculated results quantized distinct levels similarities units evident table 
averaging diagnostic information comes strongly activated units dark elements table activated units reflect averaging strong weak elements different sentences 
activation units averaged syntactic class verbs separated verb class intransitive transitive psych change state cause motion spray load dative 
consider compress units represented output sequencing system 
goal test claim represent syntax states model 
activation quite distributed clear patterns 
verbs mainly units activate appropriate verb similar verb classes chang cognitive science similar activation patterns 
specialized phrasal elements nouns adjectives prepositions determiners 
nouns adjectives shared units nouns activated 
determiners prepositions units activated 
intransitive verbs shared 
syntactic categories activated units layer depended sequential system categories closed class words depend pathway open class elements 
compress units distributed representations encoded important syntactic knowledge major syntactic category distinctions verb class information 
activations units function syntactic category tell different story table right side 
representations strongly differentiating categories 
action role ac active verbs prepositions determiners nouns 
agent role ag active determiners nouns adjectives intransitive verbs patient role pa similar pattern prepositions activated role 
goal role gl active prepositions 
syntactic information available distinctions categories strong 
verb class table averaged activations compress units units syntactic category levels activation black high white low 
chang cognitive science distinctions maintained units 
sequencing system system responsible syntactic behavior model 
understand units influence processing second analysis done looking activation units particular sequence syntactic categories 
english sequences syntactic categories encode role information units defined grouped basis preceding sequence 
unit activations came single dual path model tested novel sentences epoch 
table average activation units syntactic category particular sequence marked bold 
example sentence man gave cake cat state compress units recorded word sentence 
states averaged sentences similar sequences syntactic categories novel sentences test set get average det noun det noun prep det noun state representation placed table 
table average activation prepositional dative sentence word dog prepositional phrase appended comparison nouns 
lines appended show average activation prepositional phrases adjective 
table average activation compress units predicated syntactic sequences chang cognitive science model produced sentences activation units tracked phrases model producing sentence man give cake cat example 
produced subject det man agent ag unit activated strongly 
turned action unit ac activated dative verb give produced 
noun phrase started patient pa goal gl activated patient phrase won cake gl unit shut 
demonstrated incremental nature model decisions structure selection planned sentence structure earlier gl deactivated production patient phrase 
patient phrase preposition produced model produced goal phrase activating gl unit cat 
gl unit stayed activated phrase 
lexical semantic information embedded links sequential activation units exactly behavior expect order extract information appropriate moment 
independent support thinking sentence production process involves moving attention event roles sequential activation deactivation roles study griffin bock mentioned earlier 
speakers describe pictures events tend fixate picture elements right naming sentences 
fixation depended syntactic structure suggested syntactic structure eye movements linked manner 
production theories static message representations chang predict eye movements synchronized structural decisions 
dual path model message representations spatially represented links dynamically activated production 
event description activation appropriate unit related focusing attention elements scene 
case structural decisions eye movements related activation units provides reason syntax spatial processing synchronized 
incrementality seen noun phrases 
model produces noun phrases adjective det versus don det activation values word determiner det differ 
bottom table activation states compress units units identical production noun sequence 
prep det cat production adjective sequence 
prep det red 
shows model plan produce adjective noun specifically point determiner 
simply produces word activated point sentence 
adjective produced model activated deactivated produce noun timestep 
noun produced model done producing sentence 
model incremental various points processing consistent experimental language production demonstrating sentence construction sensitive lexical availability words different points processing bock ferreira dell 
hidden unit analysis help understand model generalized words novel positions dog goal test 
look activation pattern nouns trained goal role 
prep det different nouns chang cognitive science trained role 
prep det dog 
model learned treat novel message way identical messages construction 
architecture dual path network ability due event semantics units concert information activating goal unit appropriate moment sentence 
mapping event semantics units units novel shared dative sentences model sequence word attached goal unit 
equivalent mapping prod srn involved mapping role specific semantic units dog goal slot appropriate sentence position 
dog dog trained dog prod srn model failed generalize properly 
dog training space prod srn model explicitly trained 
dual path model uses event semantic information select appropriate sequence unit activation sentences produced sentences novel lexical items 
hidden unit analysis tells things dual path model 
syntactic categories represented primarily sequence system distributed representations activation system reflect target phrase produced 
second processing model incremental incrementality seen way lexical factors influence structure selection sequencing roles 
third point model treating novel sentences way identical way sentences construction treated 

constraining overgeneralization baker paradox previous sections concentrated computational properties model computational properties led symbolic behavior 
humans exhibit symbolic behaviors behaviors constrained statistical regularities 
dual path model implements symbolic processing statistical learning mechanism able see influence types regularities aspect model behavior way functionally similar aspect human behavior 
useful domain look role statistical processing way verbs paired structural frames 
nouns verbs selective structures paired relationship probabilistic graded 
nouns easily paired sentence frames verbs easily paired frames heard tomasello 
problem constraining verb generalization problem symbolic systems verbs nouns controlled variables 
mechanism gives nouns ability generalize different frames think give verbs abilities 
property symbolic systems led learnability problem described baker referred baker paradox 
paradox arises fact children able verb novel frame reluctant 
behavior explained children started tendency gradually learned constrain generalization negative evidence parents 
adults give detailed direct negative chang cognitive science evidence children avoid overgeneralization puzzle learn constrain 
dual path model implemented symbolic processing framework statistical learning algorithm questions verb generalization applied model 
dual path model simply symbolic system generalizes freely may subject baker paradox training set provide type information necessary restrict generalization 
specifically model received negative evidence verbs occur alternative constructions expect verbs generalize equally 
hand model simply statistical learning system expect verbs generalize novel frames novel pairings statistical frequency zero 
dual path model employs right mix symbolic statistical properties exhibit properly constrained generalization 
experimental evidence baker paradox seen experiments 
third experiment taught novel verb neutral frame demonstrating transfer action moves saying 
tested child ability produce novel verb double object frame ball 
asked child describe action known dative verb give ball 
elicited double object responses verbs child knew experiment give double object responses novel verbs 
associated double object frame child statistical evidence go frame able produce double object responses evidence sole basis 
incorrect 
way children generalize variable action frame variables agent action goal patient predict verbs structure give 
don fact generalizes intermediate level suggests accounts explain generalization 
examine baker paradox model messages produce double object dative structures verb throw boy throw girl cup generated 
lists created replacing action semantics throw sentences verbs dance hit chase surprise pour load 
training throw occurred double object frame 
verbs occurred frame 
dance occurred intransitive construction 
hit chase surprise occurred transitive frames 
verb pour occurred cause motion frame 
verb load occurred cause motion frame change state frame 
create double object dative test set goal argument prominent setting event semantics unit transfer active motion double object target structure 
model subjects model type tested test sets 
fig 
shows average sentence accuracy test sets dual path model epochs 
verb throw trained double object structure model achieved high level accuracy epochs 
verbs trained structure 
pour generalized structure epochs 
load generalized epochs fell epochs 
verbs surprise chase hit generalized epochs fell chang cognitive science fig 

accuracy generalization double object dative frames 
epochs 
verb dance generalized double object structure 
test verbs throw training double object structure different degrees overgeneralization 
verbs generalized free variable manner pour generalized way reflected occurrence properties construction verb dance 
compare model children look epoch approximates model childhood state 
epoch model produced double objects verbs experienced double object dative frame throw double objects verbs appeared structure average chase dance hit load pour surprise shows model capture intermediate nature generalization 
developmental pattern model resembled way generalization changes children 
model initially unable produce sentences learned language started epoch epoch 
overgeneralization slowly reduced model continued learn 
surprise chase hit load showed pattern 
pattern partially due differences speed pathways model learned corresponding representations 
mapping event semantics message lexical system shared sentences construction learned quickly 
allowed model 
mapping units sequencing system word units requires lexical specific learning takes longer constrain generalization 
knowledge eventually reduced overgeneralization model 
mappings resemble broad narrow constraints pinker argued 
mapping event semantics units units similar operation broad constraints semantics construction influences order arguments 
mapping chang cognitive science units word units sequencing system represents operation narrow constraints involves way lexically specific classes restrict generalization construction brooks tomasello 
reason variability model different verbs dance vs pour degree ability generalize double object dative structure partially due overlap event semantics 
dance throw shared event semantics difficult produce dance dative frame 
pour generalized double object frame shared features cause motion dative construction 
link event semantics verb probably main reasons children 
generalized context suggested event semantics transfer event test 
reason variability stems available syntactic frames verb seen 
example pour load shared features cause motion throw 
load occur change state construction boy loaded hay goal entity undergoes state change occurs verb 
initially load dative pour 
epoch model learned change state construction put goal verb ability double object dative reduced epoch pour load 
change state construction puts goal verb similar double object puts goal verb 
similarity ability change state construction interfere ability verb double object dative 
change state construction said preempt double object construction way goal 
preemption blocking important way children reduce overgeneralization clark pinker 
important reason variability model generalization due simplicity model verb representations 
lexical semantic similarity captured model eat drink shared semantic features unit representations event semantics transfer provided reliable information generalization 
people verbs cluster semantic classes smaller broad classes specified event semantics subclasses predictive syntactic frames appear fisher 
model subject baker paradox constrains generalization way similar character operations children 
making pathway message lexical system constrains generalization learning lexical specific information sequencing pathway 
results promising children exposed larger collection words structures need address types models scale language real children experience 

processing systems aphasia connectionist models link understanding normal language processing cases brain damage impaired critical processing systems dell schwartz chang cognitive science martin gagnon plaut mcclelland seidenberg patterson studies helped understand architecture language processing system influence type symptoms appear impaired patients 
describing architecture dual path model concentrated way architecture enables model exhibit certain functional behaviors 
desirable show model approximates architecture language brain 
done establishing damage physical architecture model lead symptoms similar patients injury real brain systems 
lesions applied separate pathways resulting behavioral effects analyzed 
behavioral effects compared aphasic symptoms see model processing abilities damaged ways similar patients brain injuries 
double production different lexical categories important type evidence separate processing systems 
researchers suggested function words prepositions determiners verbs content words nouns adjectives verbs represented separate systems kaplan 
patients difficulty function words relatively difficulty content words 
patients opposite pattern content words relatively spared function words relatively impaired 
researchers light heavy verbs schwartz 
light verbs go give get take learned frequent speech children languages learned clark 
aphasic patients trouble heavy verbs relatively spared respect light verbs berndt 
patients reverse pattern 
double important demonstrate behavioral pattern dependent different brain areas exists way lesion system automatically impairing 
gordon dell argue function content word dissociation light heavy verb dissociation reflect underlying distinction syntactic semantic representations lexical items dependent separate representations different degrees 
layer connectionist model learned produce simple sentences showed model learned representations light verbs relied syntactic system heavy verbs depended semantic system 
light heavy verbs syntactic semantic determinants model arose differences degree dependence system 
dual path model claims different types representations result different pathways independently influencing lexical representation possible examine model exhibit aphasic 
test importance separate pathways trained dual path model lesions applied pathways create types impaired models 
word lesioned model model created message lexical system specifically links units word units 
hidden word lesioned model model created damaging sequencing system links hidden units word units 
lesions created randomly removing weights sets units 
trained dual path model subjects received lesions chang cognitive science table sample output word lesioned model hidden word lesioned model target man eat man blue eat man target cup scare woman scare cup scare woman target man pour silly dog boy blue man put cup man pour silly silly silly silly boy target girl give woman grass give girl girl put target owl throw boy cafe blue woman give blue blue owl throw dashes mark positions output word units threshold 
links hidden word links 
models tested novel sentences test set results coded analysis 
word links damaging network amount hidden word links 
reduce differences due severity lesion hidden word lesion removed connections word lesion affected links 
lesions led average word accuracy correct word correct position model subjects models respectively significantly different suggests severity lesion equal 
word accuracy finer measure model accuracy capture partial productivity lost measures sentence accuracy 
table shows intended target output lesioned models try produce target 
sample illustrates differences way lesions influence production 
example model get local word ordering correct omit content words substitute non contextual words girl grass 
model tends convey appropriate content wrong order determiners repeating content words 
determiner model frequent appropriate positions model repeats multiword sequences says blue twice model repeats single words 
model replaces heavy verbs light counterparts pour put throw give 
broadly speaking model acting aphasic model acting aphasic humans models quite bit variability 
examine function content words percentage function words correctly produced corresponding percentage content words dependent chang cognitive science fig 

accuracy producing function content words depending lesion type 
measures 
function words model include prepositions determiners verbs 
content words constituted words 
shown fig 
model produced function words better model respectively 
reverse true content words producing fewer correct words model 
double dissociation natural outcome constraints dual path architecture 
content words definition content meaning depend message word pathway 
function words produced certain syntactic contexts needed syntactic information provided sequential system 
pathways selectively damaged component leaving relatively spared 
dissociation examined model light heavy verb dissociation 
theories verb semantics argued light verbs represent basic primitives sentence meaning goldberg 
model incorporated ideas treating verbs default verb construction verbs marked bold table 
means verbs features action event role 
example verb throw feature action event role verb give 
difference features links model depend message lexical system heavy verbs sequencing system light verbs 
shown fig 
model produced light verbs correctly time model produced time 
heavy verbs model impaired model fig 

accuracy producing verbs depending lesion type 
chang cognitive science 
model exhibited double dissociation verb complexity aphasic literature 
function content word light heavy verb model exhibited double argued reflect selective impairment processing modules aphasic brains 
model modules concrete instantiations shown produce sentences 
module corresponded message lexical system impaired model supported production semantically rich information content words heavy verbs 
module sequencing system impaired model supports categories identified syntactic frames function words light verbs 
original motivation modules computational demands getting connectionist model generalize symbolically 
solution problem nicely accounts aphasic 
addition ability model aphasia dual path model broadly consistent theories language knowledge distributed brain surprising theories brain localization language designed account aphasic symptoms 
describing theories important clarify relationship computational models data brain connectivity localization function 
brains immensely complex networks dynamic computational model neurons provide satisfactory model neural system 
comparing properties different computational models evidence brain connectivity localization function provide converging evidence supporting particular architecture 
evidence neuropsychological studies comparison prod srn dual path model 
difference architectures dual path model sequencing representations isolated message prod srn model sequencing representations linked message 
theory language localization supports dual path model ullman proposed 
argues variety behavioral neuropsychological data way language represented frontal temporal lobes differs 
left temporal lobe tends encode semantic episodic knowledge left frontal lobe encode language rules 
prod srn model support separation types information separate lobes semantic knowledge message influence sequencing representations directly 
dual path model naturally models separation message lexical system acts temporal lobe sequencing system acts frontal lobe 
furthermore original evidence distinction localization separate pathways object spatial processing milner goodale mishkin ungerleider evidence bound temporal lobe consistent approach dual path model binds object spatial roles message lexical system prod srn model separate objects roles 
architecture dual path model appropriately captures functional relationships brain architecture language issue research 
suggests incorporating distinctions brain helpful modeling effects brain injury explaining architecture brain contributes symbolic creativity language 
chang cognitive science 
summary incremental connectionist model able learn produce sentences message sentence pairs generalize knowledge novel sentence structures 
accomplished generalization isolating different types knowledge pathways creating novel combinations pathways 
involved putting word novel position dog goal identity novel adjective noun pairing tests involved limiting generalization sensitivity statistical regularities lexical items novel verb frame pairings 
furthermore architecture model exhibited double resemble aphasia 
key innovation weights represent temporary variable bindings messages sequencing network activate variables 
arguing variable behavior language emerges distributed representations statistical learning model instantiates idea variable behavior arises pre existing variables spatial system linked sequencing representations language system 
space language proposed related behavioral scientists linguists developmental psychologists proposed inhabit similar regions temporal lobe natural combining spatial variables connectionist learning algorithms yield system treats language manner similar human speakers 
symbolic abilities model powerful 
model parameters allow vary dependence symbolic statistical processing 
example size compress layer determines sequence system influence lexical selection compress units model statistical sequencing information influences production 
able process sentences conform experiences model important feature model 
chomsky known claim english speakers recognize grammaticality meaningless sentences green ideas sleep evidence simply construct sentences statistical semantic regularities 
model ability produce novel adjective noun pairings green ideas violations lexical experience verbs ideas sleep reflection ability finite means generate greater set possibilities 
part power variable representations came fact linked structural frames 
frames arose learning combine different types information predict sequences abstractness structural frames depended keeping message lexical content separate sequencing system dual path model 
abstractness frames help symbolic generalization model allowing member syntactic category operate certain position message set appropriately 
time frames little concrete information decide frames 
model event semantics job 
architecture increases abstractness syntactic representations event semantics give structures specific conditions 
combination variables syntactic structural frames event semantics powerful 
combination puts somewhat outside main schools connectionism 
school connectionist approach emphasizes power error learning chang cognitive science algorithms extract statistical regularities training environment elman plunkett plaut :10.1.1.124.2820
time school de emphasizes role input representations architecture model 
dual path model statistical regularities extracted learning algorithm particularly sequencing system large emphasis placed pre existing representations representations architecture network dual path architecture explaining model works 
school structural connectionist approach takes pragmatic approach building networks focusing particular computational operations variable binding shastri ajjanagadde task decomposition jacobs jordan barto 
dual path model shares pragmatic approach building networks emphasis modules build complex systems places large emphasis way different systems interact way learning important learning system internal representations sequence system weights learned systems interact combining outputs message lexical sequence systems crucial light heavy verb 
dual path model compromise approaches incorporating aspects single framework 
addition suggesting structural connectionism combined modeling illustrated usefulness building models attempt link different domains 
psycholinguistic research suggested needed connectionist learning algorithms architectures capture detailed lexical statistical regularities arise incremental processing macdonald macwhinney 
insights biological developmental linguistic theories spatial representations hinted messages structured led message 
message representation turn necessitated dual system architecture order place symbolic processing connectionist framework 
dual system architecture allowed model constrain verb generalization baker paradox account certain double occur aphasic patients 
sum combination ideas computational psycholinguistic biological developmental neuropsychological literatures 
cross domain approach taken solutions particular problems domain propagate interact domains 
instantiating ideas different domains model input representations network architecture connectionist learning algorithms glue resulting model human counterparts emergent abilities arise complex interactions different systems 
note 
language large finite set sentences 
recursive structures allow utterances infinite length subject memory constraints 
model constrained finite way approach creating language strings allow handle languages recursive properties 
model sentences produced incrementally making representations activated chang cognitive science moment 
approach production place finite constraints long sentences 
point clear messages controlled way allows recursive structures 
acknowledgments research formed part author ph dissertation university illinois urbana champaign 
preparation article supported national science foundation sbr language processing training mh national institutes health dc 
gary dell helpful comments detailed reviewing manuscript support project 
kathryn bock nick chater victor ferreira cindy fisher adele goldberg griffin harris dennis norris kris doug anonymous reviewers useful suggestions comments 
appendix models implemented lens neural network software 
learning algorithm back propagation modified momentum algorithm doug momentum similar standard momentum descent exception pre momentum weight step vector bounded length exceed 
momentum 
learning rate started reduced linearly reached epochs fixed rest training 
batch size set size training set 
word units soft max activation function 
soft max units caused output passed exponential function magnified small differences result normalized leaving activated unit squashing activation weaker competitors 
soft max units word output units error function units divergence function sum units target log target output 
units logistic activation function 
models event semantics units units provided information target sentence order 
dative sentence man bake cake cafe event semantics units cause create transfer 
prepositional dative structure cause feature activation create feature activation transfer feature activation 
double object dative man bake cafe cake create feature activation transfer feature activation 
links instantiated variables store message 
production sentence links units set initially individual links roles units setting weight value 
units corresponding link units level weight 
lens software allowed code run chang cognitive science sentence sequence initiated functions set value weights set message representation production sentence 
units unbiased input driven units context bias 
units received previous timestep activation units targets 
error function cross entropy function sum units target log target output target log target output 
represent temporal information copies previous network states copied special units calls elman units units input state model elman 
units elman units summed activation units previous activation 
context units elman units initialized sentence 
units elman units received values sum external input representing previous word sequence output word units 
production external input units copy previous word units activation 
comprehension previous word unit activation external input summed 
units initialized start sentence 
generate message verb randomly selected list possible verbs 
arguments randomly chosen possible nouns appropriate verb 
random selection adjectives done adjectives appropriate particular head noun terms head noun 
prepositions selected construction passive random selection appropriate construction verb 
messages single event role quickly learned training sets arranged verbs dance sleep frequent verbs 
reduce occurrence verbs chosen verb selection repeated 
second selection select verbs 
occurred second selection allowed basis message 
message ended sentence markers model changed sentence structure fully produced longer sentence structures shorter passives words 
dual path model units units units units units event semantics units hidden units context units units units compress units word units 
inadvertently semantics prepositions left models influence results prepositions crucial tests comparisons models 
prepositions lexical semantics associated event semantics syntactic frames 
verbs go give put fill considered light verbs verb semantics 
event semantics linked path models number units layer dual path model 
prod srn model designed training testing patterns models 
done placing static binding space message weights bias unit invisible unit connected biased units message units 
prod srn model units message units slots units action slot units hidden units context units word units 
chang cognitive science baker 

syntactic theory projection problem 
linguistic inquiry 
baldwin 

early referential understanding infants ability recognize referential acts 
developmental psychology 
barsalou 

perceptual symbol systems 
behavioral brain sciences 
berndt 

verb retrieval aphasia 
characterizing single word impairments 
brain language 
bloom peterson nadel garrett 

language space 
cambridge ma mit press 
bock 

cognitive psychology syntax information processing contributions sentence formulation 
psychological review 
bock 

meaning sound syntax lexical priming sentence production 
journal experimental psychology learning memory cognition 
bock 

ordinating words syntax speech plans 
ellis ed progress psychology language vol 
pp 

london erlbaum 
bock levelt 

language production grammatical encoding 
gernsbacher ed handbook psycholinguistics pp 

san diego ca academic press 
bock 

conceptual roles structural relations bridging syntactic cleft 
psychological review 
schwartz 

semantic factors verb retrieval effect complexity 
brain language 
brooks tomasello 

children constrain argument structure constructions 
language 
chang dell bock griffin 

structural priming implicit learning comparison models sentence production 
journal psycholinguistic research 
chomsky 

syntactic structures 
hague mouton 
christiansen chater 

connectionist model recursion human linguistic performance 
cognitive science 
clark 

discovering words 
farkas jacobsen eds 
papers lexicon pp 

chicago il chicago linguistics society 
clark 

principle contrast constraint language acquisition 
macwhinney ed mechanisms language acquisition pp 

hillsdale nj lawrence erlbaum associates 
clark carpenter 

notion source language acquisition 
language 
dell 

spreading activation theory retrieval sentence production 
psychological review 
dell schwartz martin gagnon 

lexical access aphasic speakers 
psychological review 
dell chang griffin 

connectionist models language production lexical access grammatical encoding 
cognitive science 
dowty 

thematic proto roles argument selection 
language 
elman 

finding structure time 
cognitive science 
elman 

learning development neural networks importance starting small 
cognition 
ferreira 

better give donate 
syntactic flexibility language production 
journal memory language 
ferreira dell 

effect ambiguity lexical availability syntactic lexical production 
cognitive psychology 
fisher gleitman gleitman 

semantic content subcategorization frames 
cognitive psychology 
fodor pylyshyn 

connectionism cognitive architecture critical analysis 
cognition 
chang cognitive science pearlmutter myers 

contribution verb bias plausibility comprehension temporarily ambiguous sentences 
journal memory language 
garrett 

processes language production 
ed linguistics cambridge survey vol 

language psychological biological aspects pp 

cambridge uk cambridge university press 
goldberg 

constructions construction grammar approach argument structure 
chicago university chicago press 
kaplan 

assessment aphasia related disorders 
philadelphia pa lea 
gordon dell 

learning divide labour syntax semantics connectionist account deficits light heavy verb production 
brain cognition 
griffin bock 

eyes say speaking 
psychological science 
pinker hollander goldberg wilson 

learnability acquisition dative alternation english 
language 
pinker hollander goldberg 

direct objects role lexical semantics acquisition verb argument structure 
cognition 
hadley 

cognition computational power connectionist networks 
connection science 
hertz krogh palmer 

theory neural computation vol 

redwood city ca addison wesley 
hummel holyoak 

distributed representations structure theory analogical access mapping 
psychological review 
jackendoff 

semantic structures 
cambridge ma mit press 
jacobs jordan barto 

task decomposition competition modular connectionist architecture vision tasks 
cognitive science 
jordan 

serial order parallel distributed processing approach 
ics technical report university california san diego la jolla ca 


new insights functions superior temporal cortex 
nature reviews neuroscience 
kosslyn 

image mind 
cambridge ma harvard university press 
lakoff 

women fire dangerous things 
chicago university chicago press 
landau jackendoff 

spatial language spatial cognition 
behavioral brain sciences 
langacker 

foundations cognitive grammar 
stanford ca stanford university press 
levelt meyer 

theory lexical access speech production 
behavioral brain sciences 
macdonald pearlmutter seidenberg 

lexical nature syntactic ambiguity resolution 
psychological review 
macwhinney 

competition model 
macwhinney ed mechanisms language acquisition pp 

hillsdale nj lawrence erlbaum associates 
mandler 

build baby ii 
conceptual primitives 
psychological review 
marcus 

rethinking connectionism 
cognitive psychology 
marcus 

algebraic mind 
cambridge ma mit press 
mcclelland kawamoto 

mechanisms sentence processing assigning roles constituents sentences 
mcclelland rumelhart eds parallel distributed processing explorations microstructure cognition pp 

cambridge ma mit press 
milner goodale 

visual brain action 
oxford oxford university press 
mishkin ungerleider 

contribution striate inputs functions cortex monkeys 
behavioral brain research 
pinker 

learnability cognition acquisition argument structure 
cambridge ma mit press 
pinker prince 

language connectionism analysis parallel distributed processing model language acquisition 
cognition 
chang cognitive science plaut mcclelland seidenberg patterson 

understanding normal impaired word reading computational principles quasi regular domains 
psychological review 
plunkett 

connectionist model english past tense plural morphology 
cognitive science 
potter 

regeneration short term recall sentences 
journal memory language 
regier 

model human capacity categorizing spatial relations 
cognitive linguistics 


lens light efficient network simulator 
carnegie mellon university department computer science pittsburgh pa plaut 

language acquisition absence explicit negative evidence important starting small 
cognition 
rumelhart mcclelland 

learning past tenses english verbs 
mcclelland rumelhart pdp research group eds parallel distributed processing explorations microstructure cognition vol 

psychological biological models pp 

cambridge ma mit press 
rumelhart hinton williams 

learning representations back propagating errors 
nature 
shastri ajjanagadde 

simple association systematic reasoning connectionist representation rules variables dynamic bindings temporal synchrony 
behavioral brain sciences 
st john mcclelland 

learning applying contextual constraints sentence comprehension 
artificial intelligence 
talmy 

motion language ception 
bloom peterson nadel garrett eds language space pp 

cambridge ma mit press 
tomasello 

cultural origins human cognition 
cambridge ma harvard university press 
tomasello 

differential productivity young children nouns verbs 
journal child language 
ullman 

perspective language declarative procedural model 
nature reviews neuroscience 
