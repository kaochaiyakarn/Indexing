wrappers performance enhancement oblivious decision graphs dissertation submitted department computer science committee graduate studies stanford university partial fulfillment requirements degree doctor philosophy ron kohavi september copyright ron kohavi rights reserved ii certify read dissertation opinion fully adequate scope quality tation degree doctor philosophy 
yoav shoham principal certify read dissertation opinion fully adequate scope quality tation degree doctor philosophy 
jerry friedman statistics certify read dissertation opinion fully adequate scope quality tation degree doctor philosophy 
nils nilsson approved university committee graduate studies iii doctoral dissertation study basic problems machine learning new hypothesis spaces corresponding learning algorithms 
problems accuracy estimation feature subset selection parameter tuning 
problems related studied wrapper approach 
motivated oodg results unrelated oodg cross validation stratified cv bootstrap automatic parameter tuning accuracy estimation wrapper approach search engine hill climbing best search bottom feature subset selection fss operators compound operators read graph construction continuous data binning holte entropy top decision tables chronological ow topics 
small circles top node indicate chapters dissertation appear 
john kohavi wrapper approach introduced method feature subset selection induction algorithm 
year ort concentrated problem feature subset selection kohavi kohavi eld 
results feature subset selection wrapper generalized general optimization parameters kohavi john specialized subset selection decision tables kohavi :10.1.1.133.9187
rough sets related decision tables oodgs kohavi kohavi 
wrapper approach led large experiment see accuracy estimation method kohavi :10.1.1.133.9187
oodgs resumed experimental comparison discretization methods allow experiments real datasets dougherty kohavi sahami top induction algorithm kohavi li 
chapter combines chapter 
john kohavi wrapper approach introduced method feature subset selection induction algorithm 
year ort concentrated problem feature subset selection kohavi kohavi eld 
results feature subset selection wrapper generalized general optimization parameters kohavi john specialized subset selection decision tables kohavi :10.1.1.133.9187
rough sets related decision tables oodgs kohavi kohavi 
wrapper approach led large experiment see accuracy estimation method kohavi :10.1.1.133.9187
oodgs resumed experimental comparison discretization methods allow experiments real datasets dougherty kohavi sahami top induction algorithm kohavi li 
chapter combines chapter 
done kohavi years entropy discretization discretize data top algorithm nds initial subset wrapper searches better node original hoodg algorithm inducing oodgs black box 
principal contributions jones law signi cant contribution eld endeavor stays eld long obstruction progress direct proportion importance original contribution 

compact 
related comprehensibility imply 
perceptron see compact classi er instance may hard understand labelling process 
alternatively decision table kohavi see chapter page may large labelling instance trivial simply look table :10.1.1.133.9187
michie reported id output chess domain shown domain expert master completely opaque 
accurate tree large obscure chess master total blackout 
phenomenon con rmed related chess material shapiro niblett similar claims domains 
researchers statistics community error rates minus accuracy accuracy 
years generally assumed higher folds cross validation leave yield better estimates usually expense longer computation time 
example weiss kulikowski write leaving preferred technique large sample sizes may computationally quite expensive 
mosteller tukey wrote suppose set aside individual case optimize left test set aside case 
repeating case data dry 
incremental induction algorithms allows cross validation time independent folds kohavi moore lee :10.1.1.133.9187
algorithms leave takes exactly time fold cross validation clear leave preferred 
leave unbiased efron high variance leading unreliable estimates 
results theoretical experimental shown chapter 
accuracy estimation case increasing number folds bene cial especially variance important bias case model selection 
bias severe implications applied blindly regard resulting induced concept 
example medical diagnosis task set features describing patient include patient social security number ssn 
assume features ssn su cient determine correct diagnosis 
focus searches minimum set features pick ssn feature needed uniquely determine label ssn induction algorithm expected generalize poorly 
relief algorithm relief algorithm kira rendell kira rendell kononenko assigns relevance weight feature meant denote relevance feature target concept :10.1.1.51.6297
relief randomized algorithm 
samples instances randomly training set updates relevance values di erence selected instance nearest instances opposite class near hit near 
relief algorithm nds weakly relevant features relief help redundant features 
features relevant concept select fraction necessary concept description kira rendell page 
arti cial datasets oc best performer followed hoodg idtm 
explanation surprising success naive bayes dissertation large number medical datasets 
chose datasets believe rational criteria turned related medical domains features usually understood probabilistically independent label value 
unexpected result dissertation power decision tables 
original decision tables kohavi power noted failed domains continuous features :10.1.1.133.9187
dissertation data discretized idtm appears truly powerful algorithm real world datasets 
interesting observation trevor hastie naive bayes rep extreme classi ers terms feature interaction 
naive bayes assumes features independent label represent full interaction models 
exception soybean large dataset di erences surprisingly small di erent models 
