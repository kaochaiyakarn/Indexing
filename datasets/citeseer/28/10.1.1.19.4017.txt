copyright chen optimal anytime search constrained nonlinear programming chen university science technology china china thesis submitted partial fulfillment requirements degree master science computer science graduate college university illinois urbana champaign urbana illinois thesis study optimal anytime stochastic search algorithms ssas solving general constrained nonlinear programming problems nlps discrete continuous mixed integer space 
algorithms general sense assume di convexity functions 
search algorithms develop theory ssas propose optimal ssas iterative deepening order minimize expected search time 
optimal ssas develop optimal anytime ssas generate improved solutions search time allowed 
ssas solving general constrained nlps theory discrete constrained optimization lagrange multipliers shows equivalence set constrained local minima clm dn set discrete neighborhood saddle points sp dn 
implement theory propose general procedural framework locating sp dn incorporating genetic algorithms framework evaluate new constrained search algorithms constrained genetic algorithm cga combined constrained simulated annealing genetic algorithm csaga 
contrast similar results continuous space optimization 
general framework look sp dn observations existing various approaches look discrete space saddle points lacks general framework unifies mechanisms 
framework impossible know di erent algorithms variations 
propose thesis framework solving constrained nlps unifies simulated annealing sa genetic algorithms ga greedy searches looking saddle points 
framework allows show leading algorithms dlm csa ga search penalty formulations similar algorithms di er components framework :10.1.1.13.9279
order necessary su cient conditions theorem depicts general stochastic optimization procedure look sp dn procedure maintains list candidate points searched 
consists loops loop updates variables order perform subspace loop start insert candidate list sorting criterion annealing deterministic search subspace generate new candidate probabilistic greedy initial candidate initial generate random probabilistic greedy subspace genetic generate new candidate stopping met 
condition update lagrangian values candidates list annealing loop loop general iterative stochastic procedural framework look sp dn updates variables unsatisfied constraints candidate list order perform subspace 
procedure better probes generated subspaces 
optimality subproblem di cult achieve practice finite amount time solve subproblem 
leads suboptimal solutions result subproblem optimal 
solutions intermediate subproblems may related final goal finding clm dn cgm dn penalties large 
approximations process sacrifice global optimality solutions developed 
various constraint handling techniques developed dynamic penalty formulations :10.1.1.13.9279
requiring domainspecific knowledge heuristics di culties finding feasible regions maintaining feasibility nonlinear constraints get stuck easily local minima :10.1.1.13.9279
typical constraint handling techniques explained 
note techniques heuristic nature 
general methods dynamic penalty formulations best guarantee find clm dn consider simple problem constraint function objective 
leads suboptimal solutions result subproblem optimal 
solutions intermediate subproblems may related final goal finding clm dn cgm dn penalties large 
approximations process sacrifice global optimality solutions developed 
various constraint handling techniques developed dynamic penalty formulations :10.1.1.13.9279
requiring domainspecific knowledge heuristics di culties finding feasible regions maintaining feasibility nonlinear constraints get stuck easily local minima :10.1.1.13.9279
typical constraint handling techniques explained 
note techniques heuristic nature 
general methods dynamic penalty formulations best guarantee find clm dn consider simple problem constraint function objective 
dynamic penalty algorithm starts min dn min dn regardless large penalty feasible solution 
advantage sa lies global convergence property 
applied solve constrained nlps penalty formulations global convergence sa ensures search converge optimal solution penalty function may infeasible point clm dn cgm dn original constrained problem 
success sa constrained optimization depends heavily proper choice penalties 
sa requires slow cooling schedule order converge optimal solution high probabilities 
genetic algorithm ga stochastic global optimization algorithm reachability :10.1.1.13.9279
generally maintains population individuals 
generation uses genetic operators crossovers mutations generate new points 
old new points ranked fitness function objective function case unconstrained optimization problems 
starts new generation top fit individuals 
iterating su cient number generations expected converge best individual 
similar sa ga requires choice penalties penalty formulation order search converge cgm dn original constrained problem 
search may finding clm dn infeasible solutions 
variants penalty formulations gas handle constraint 
include methods multi level static penalties generation dynamic penalties annealing penalties adaptive penalties :10.1.1.13.9279
di er ways modifying penalties adjust penalties generation unconstrained problem previous penalty levels minimized 
accordingly methods achieve asymptotic convergence 
di culties choosing suitable penalties di erent constrained nlps 
sa ga stochastic global optimization strategies popular applied solve discrete constrained nlps penalty formulations 
technical report cs oregon graduate institute 
michalewicz 
genetic algorithms data structure evolution programs 
springerverlag berlin 
michalewicz dasgupta schoenauer :10.1.1.13.9279
evolutionary algorithms constrained engineering problems 
computers industrial engineering journal 
michalewicz janikow 
handling constraints genetic algorithms 
