information extraction hmm structures learned stochastic optimization dayne freitag andrew mccallum just research henry street pittsburgh pa justresearch com research demonstrated strong performance hidden markov models applied information extraction task populating database slots corresponding phrases text documents 
remaining problem selection state transition structure model 
demonstrates extraction accuracy strongly depends selection structure presents algorithm automatically finding structures stochastic optimization 
algorithm begins simple model performs hill climbing space possible structures splitting states performance validation set 
experimental results show technique finds hmm models perform fixed model superior average performance tasks 
internet available tremendous amount text generated human consumption unfortunately information easily manipulated analyzed computers 
internet available tremendous amount text generated human consumption unfortunately information easily manipulated analyzed computers 
information extraction process filling fields database automatically extracting fragments human readable text 
examples include extracting location meeting email message name corporate target 
research demonstrated effectiveness hidden markov models hmms information extraction 
hmms applied successfully sub domains information extraction named entity extraction task bikel task recovering sequence set entities occurring close proximity dense extraction seymore sparse extraction task object extract relevant phrases documents containing irrelevant text leek freitag mccallum :10.1.1.51.4029
cases accuracy hmms applied tasks state art significantly better alternative learning approaches 
address sparse extraction task 
assume document corpus corresponding relational record template slot empty filled fragment text document 
copyright american association artificial intelligence www aaai org 
typically uses goodness statistical fit data structure selection process step optimizes performance task hand 
shows greater improvements due structure learning 
improvement possible context sparse extraction question research 
experimental results different data sets 
learned structures give higher accuracy previously attained hand built models outperform srv rapier state art information extraction systems employ ilp methods freitag califf :10.1.1.32.8501:10.1.1.29.2439
hmms information extraction tasks involving discrete sequences performance information extraction tasks relies powerful modeling context current observations 
finite state machines hidden markov models offer balance simplicity expressiveness context 
hidden markov models finite state automaton stochastic state transitions symbol emissions rabiner :10.1.1.131.2084
automaton models probabilistic generative processes sequence symbols produced starting state transitioning new state emitting symbol selected state transitioning emitting symbol designated final state reached 
experimental results different data sets 
learned structures give higher accuracy previously attained hand built models outperform srv rapier state art information extraction systems employ ilp methods freitag califf :10.1.1.32.8501:10.1.1.29.2439
hmms information extraction tasks involving discrete sequences performance information extraction tasks relies powerful modeling context current observations 
finite state machines hidden markov models offer balance simplicity expressiveness context 
hidden markov models finite state automaton stochastic state transitions symbol emissions rabiner :10.1.1.131.2084
automaton models probabilistic generative processes sequence symbols produced starting state transitioning new state emitting symbol selected state transitioning emitting symbol designated final state reached 
associated set states fs 
probability distribution symbols emission vocabulary fw 
wk probability state emit vocabulary item written 
probability moving state state written js 
prior state distribution 
training data consists sequences observed emissions written fo 
om information extraction hmms model parameters performed determining sequence states generated entire document extracting symbols associated certain designated target states 
determining sequence efficiently performed dynamic programming viterbi algorithm rabiner :10.1.1.131.2084
models characteristics hmm extracts just type field seminar speaker 
multiple fields extracted document seminar speaker seminar location separate hmm constructed field 
model entire document require pre processing segment document sentences pieces 
entire text training document train transition emission probabilities 
parameter estimation state transition structure determined remaining parameters model transition emission probabilities 
estimated labeled training data sequences words target words identified 
hmm structures labels determine unique path states 
unique path exist em form baum welch iteratively estimate parameters fill missing path 
step estimate expected path exactly rabiner obey target label constraints :10.1.1.131.2084
example iteration step forward procedure sjs js backward procedure modified analogously 
transition probabilities low degree multinomials estimate maximum likelihood ratios counts traditional 
emission probabilities hand high degree multinomials require smoothing prior training data extremely sparse relative number parameters 
smoothing simply uniform distribution results build shrinkage hmms information extraction freitag mccallum 
experiments adopt basic procedure document collection partitioned times training set testing set 
train learner training set measure performance testing set 
case approach described training set select model structure set parameters 
exception jobs partitions training testing sets roughly equal size 
jobs partitions exactly califf califf contain training testing :10.1.1.29.2439
results experiments ran represent average performance training testing splits 
rapier scores reported califf califf represent average performance training testing splits :10.1.1.29.2439
extraction problems consider single correct filler may occur times document slight variations slot answer template 
test document learner identify target fragment decline perform extraction 
case approach described training set select model structure set parameters 
exception jobs partitions training testing sets roughly equal size 
jobs partitions exactly califf califf contain training testing :10.1.1.29.2439
results experiments ran represent average performance training testing splits 
rapier scores reported califf califf represent average performance training testing splits :10.1.1.29.2439
extraction problems consider single correct filler may occur times document slight variations slot answer template 
test document learner identify target fragment decline perform extraction 
order extraction counted correct precise boundaries target fragment identified 
learner issues multiple predictions document take highest confidence 
order extraction counted correct precise boundaries target fragment identified 
learner issues multiple predictions document take highest confidence 
metrics characterize performance learner precision number correct extractions divided number documents learner issued prediction recall number correct extractions divided number documents containing target fragments 
report harmonic mean precision recall 
compare structure learning approaches rule learning approaches previously reported literature srv freitag rapier califf static hmm models :10.1.1.32.8501:10.1.1.29.2439
srv rapier relational rule learners shown perform variety tasks 
srv induces rules top general rule specializing 
rapier induces rules bottom successively generalizing cover target phrases 
rows labeled simple hmm complex hmm show performance static models shown 
seymore describe experiments structure learning hmms model fields simultaneously address problems ordering fields sought location single field large body background text 
bikel 
bikel applies hmms named entity recognition problem problem identifying text fragments signify particular types entities people organizations regard role document 
describe manually designed hmms state type entity gram statistics hmm structure exploit context 
hmms leek leek carefully designed state transition structure emission distributions model syntactic constraints particular extraction problem :10.1.1.51.4029
problem learning hmm structure tasks information extraction seen fair amount 
stolcke omohundro stolcke omohundro propose state merging approach begins large maximally specific topology iteratively merges pairs states 
merging criterion performance particular task bayesian combination prior expectations regarding suitable topologies goodness fit data 
seymore 
