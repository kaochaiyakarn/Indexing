actor critic algorithms vijay john tsitsiklis february 
propose analyze class actor critic algorithms 
time scale algorithms critic uses temporal di erence td learning linearly parameterized approximation architecture actor updated approximate gradient direction information provided critic 
show features critic ideally span subspace prescribed choice parameterization actor 
study algorithms markov decision processes general state action spaces 
state prove results regarding convergence 

problems nance communication networks operations research formulated dynamic programming dp problems 
formulations su er fact dimension state space large problem tractable 
underlying dynamics seldom known usually di cult identify 
reinforcement learning rl neuro dynamic programming methods try overcome di culties combining simulation learning compact representations policies value functions :10.1.1.32.7692
vast majority methods fall categories 
actor methods parameterized family policies 
gradient performance respect actor parameters directly estimated simulation parameters updated direction improvement :10.1.1.33.2793:10.1.1.129.8871:10.1.1.15.2736
possible drawback methods gradient estimators may large variance 
underlying dynamics seldom known usually di cult identify 
reinforcement learning rl neuro dynamic programming methods try overcome di culties combining simulation learning compact representations policies value functions :10.1.1.32.7692
vast majority methods fall categories 
actor methods parameterized family policies 
gradient performance respect actor parameters directly estimated simulation parameters updated direction improvement :10.1.1.33.2793:10.1.1.129.8871:10.1.1.15.2736
possible drawback methods gradient estimators may large variance 
furthermore policy changes new gradient estimated independently past estimates 
learning sense accumulation consolidation older information 

methods limited case lookup table representations policies value functions 
propose actor critic algorithms critic uses linearly parameterized approximations value function provide convergence proof 
algorithms important observation number parameters actor update relatively small compared number states critic need attempt compute approximate exact value function high dimensional object 
fact show critic ideally compute certain projection value function low dimensional subspace spanned set basis functions completely determined parameterization actor 
key insight derived simultaneous independent included discussion certain actor critic algorithms :10.1.1.146.4070
outline follows 
section state formula gradient average cost markov decision process nite state action space 
provide new interpretation formula section derive algorithms 
section consider markov decision processes gradient average cost greater generality describe algorithms general setting 
second algorithm propose td critic compute approximate exact projection 
additional features result reduction approximation error 
avoid rst possibility choose features critic assumption satis ed 
assumption 
exists jj parameter vector critic stores auxiliary parameters scalar estimate average cost vector represents sutton eligibility trace :10.1.1.32.7692
actor critic updates take place course simulation single sample path controlled markov chain 
parameters critic parameter vector actor time state action pair time 
new state obtained action applied 
new action generated rsp corresponding actor parameter vector critic carries update similar average cost temporal di erence method positive step size parameter :10.1.1.146.4070
exists jj parameter vector critic stores auxiliary parameters scalar estimate average cost vector represents sutton eligibility trace :10.1.1.32.7692
actor critic updates take place course simulation single sample path controlled markov chain 
parameters critic parameter vector actor time state action pair time 
new state obtained action applied 
new action generated rsp corresponding actor parameter vector critic carries update similar average cost temporal di erence method positive step size parameter :10.1.1.146.4070
variants critic di erent ways updating td critic 
state appearing assumption 
td critic 
actor 

map continuously di erentiable 

exists sup kq 
lim sup assumptions wish prove gradient formula similar valid ln immediate consequence assumption proofs outline proof theorem imitated show measurable jf map :10.1.1.146.4070
bounded bounded derivatives 
need properties average cost function 
assume stage cost function assumption 
exists jc kq 

bounded expectation respect du 
di erentiate sides equation respect obtain interchange di erentiation integration justi ed uniform 
inner product sides equation facts fi fi obtain hq second equality follows fact necessarily collinear easily veri ed fact 
complete proof need show existence family functions shown imitating proofs outline :10.1.1.146.4070
assumptions construct slightly enlarged probability space regeneration time sampled markov chain fx kn controlled policy splitting technique ney 
regeneration time obtain representation average cost function 
solutions poisson equations 
ju furthermore implies measures equivalent 
systems control letters 

method convergence stochastic approximation reinforcement learning 
siam journal control optimization 
cao chen :10.1.1.146.4070:10.1.1.15.2736
perturbation realization potentials sensitivity analysis markov processes 
ieee transactions automatic control 
:10.1.1.32.7692
stochastic approximation monte carlo optimization 
siam journal control optimization 
cao chen :10.1.1.146.4070:10.1.1.15.2736
perturbation realization potentials sensitivity analysis markov processes 
ieee transactions automatic control 
:10.1.1.32.7692
stochastic approximation monte carlo optimization 
proceedings winter simulation conference pages 
:10.1.1.146.4070
likelihood ratio gradient estimation stochastic recursions 
ieee transactions automatic control 
:10.1.1.32.7692
stochastic approximation monte carlo optimization 
proceedings winter simulation conference pages 
:10.1.1.146.4070
likelihood ratio gradient estimation stochastic recursions 
advances applied probability 
singh jordan :10.1.1.27.9242
reinforcement learning algorithms partially observable markov decision problems 
proceedings winter simulation conference pages 
:10.1.1.146.4070
likelihood ratio gradient estimation stochastic recursions 
advances applied probability 
singh jordan :10.1.1.27.9242
reinforcement learning algorithms partially observable markov decision problems 
advances neural information processing systems volume pages san francisco ca 
morgan kaufman 
jordan 

sutton barto 
reinforcement learning 
mit press cambridge ma 
sutton mcallester singh mansour :10.1.1.146.4070
policy gradient methods reinforcement learning function approximation 
advances neural information processing systems volume pages 
tsitsiklis van roy 
analysis temporal di erence learning function approximation 
advances neural information processing systems volume pages 
tsitsiklis van roy 
analysis temporal di erence learning function approximation 
ieee transactions automatic control 
tsitsiklis van roy :10.1.1.146.4070
average cost temporal di erence learning 
automatica 
williams 
simple statistical gradient algorithms connectionist reinforcement learning 
