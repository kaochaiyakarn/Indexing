independent component analysis extended infomax algorithm mixed sub gaussian super gaussian sources te won lee mark girolami sejnowski edu ci ac uk terry edu howard hughes medical institute computational neurobiology laboratory institute la jolla california usa department computing information systems university pa scotland department biology university california san diego la jolla california usa institut fur technische universitat berlin berlin germany neural computation vol number text pages number figures number tables extension infomax algorithm bell sejnowski able blindly separate mixed signals sub super gaussian source distributions 
achieved simple type learning rule derived girolami choosing projection pursuit index 
parameterized probability distributions super gaussian regimes derive general learning rule preserves simple architecture proposed bell sejnowski optimized natural gradient amari uses stability analysis cardoso switch sub super gaussian regimes 
demonstrate extended infomax algorithm able easily separate sources variety source distributions 
applied high dimensional data eeg recordings effective separating artifacts eye line noise weaker electrical signals arise sources brain 
blind source separation independent component analysis ica received attention potential signal processing applications speech enhancement systems telecommunications medical signal processing 
applied high dimensional data eeg recordings effective separating artifacts eye line noise weaker electrical signals arise sources brain 
blind source separation independent component analysis ica received attention potential signal processing applications speech enhancement systems telecommunications medical signal processing 
goal ica recover independent sources sensor observations unknown linear mixtures unobserved independent source signals 
contrast correlation transformations principal component analysis pca ica reduces statistical dependencies signals attempting signals independent possible 
blind source separation problem studied researchers neural networks statistical signal processing jutten herault comon cichocki bell sejnowski cardoso amari pearlmutter oja karhunen girolami :10.1.1.110.696
see nadal historical review ica karhunen review different neural blind source separation algorithms 
general ica reviews cardoso lee 

bell sejnowski developed unsupervised learning algorithm entropy maximization single layer feedforward neural network 
similar adaptive method source separation proposed cardoso 
simple general learning rule learning algorithm derived maximum likelihood formulation 
mle approach blind source separation proposed pursued pearlmutter cardoso 
probability density function observations expressed amari cardoso det jp hypothesized distribution 
log likelihood equation log det log maximizing log likelihood respect gives learning algorithm bell sejnowski deltaw theta gamma gamma gamma gamma delta delta delta gamma un un un efficient way maximize log likelihood follow natural gradient amari deltaw theta gamma proposed amari :10.1.1.110.696:10.1.1.110.696
relative gradient cardoso 
gradient simplifies learning rule equation speeds convergence considerably 
shown general learning algorithm equation derived theoretical viewpoints mle pearlmutter infomax bell sejnowski maximization girolami :10.1.1.110.696:10.1.1.110.696
lee 
probability density function observations expressed amari cardoso det jp hypothesized distribution 
log likelihood equation log det log maximizing log likelihood respect gives learning algorithm bell sejnowski deltaw theta gamma gamma gamma gamma delta delta delta gamma un un un efficient way maximize log likelihood follow natural gradient amari deltaw theta gamma proposed amari :10.1.1.110.696:10.1.1.110.696
relative gradient cardoso 
gradient simplifies learning rule equation speeds convergence considerably 
shown general learning algorithm equation derived theoretical viewpoints mle pearlmutter infomax bell sejnowski maximization girolami :10.1.1.110.696:10.1.1.110.696
lee 
review techniques show relation 
parametric density estimate plays essential role success learning rule equation 
local convergence assured derivative log densities sources 
particular remains open question extent learning rule robust parametric mismatch limited number data points 
despite limitations extended infomax ica algorithm applications sub gaussian super gaussian sources need separated additional prior knowledge statistical properties 
extended infomax ica algorithm proposed promising generalization satisfies general stability criterion mixed sub gaussian super gaussian sources cardoso 
learning algorithm derived girolami natural gradient extended infomax algorithm shown excellent performance large real data sets derived electrical blood flow measurements functional activity brain 
compared originally proposed infomax algorithm bell sejnowski extended infomax algorithm separates wider range source signals whilst maintaining simplicity :10.1.1.110.696:10.1.1.110.696
acknowledgments lee supported german academic exchange program 
girolami supported ncr financial systems knowledge laboratory advanced technology development division scotland 
sejnowski supported howard hughes medical institute 
indebted jean francois cardoso insights helpful comments stability criteria tony bell general comments discussions 
