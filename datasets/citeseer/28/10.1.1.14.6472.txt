tagging english text probabilistic model bernard merialdo institut experiments probabilistic model tag english text assign word correct tag part speech context sentence 
main novelty experiments untagged text training model 
simple model looking best way estimate parameters model depending kind amount training data provided 
approaches particular compared combined text tagged hand computing relative frequency counts text tags training model hidden markov process maximum likelihood principle 
experiments show best training obtained tagged text possible 
show maximum likelihood training procedure routinely estimate hidden markov models parameters training data necessarily improve tagging accuracy 
show maximum likelihood training procedure routinely estimate hidden markov models parameters training data necessarily improve tagging accuracy 
fact generally degrade accuracy limited amount hand tagged text available 

lot effort devoted past problem tagging text assigning word correct tag part speech context sentence 
main approaches generally considered rule klein simmons martin brill probabilistic bahl mercer marshall leech merialdo derose church beale merialdo cutting :10.1.1.79.5346
proposed neural networks mackie anderson nakamura 
multimedia communications department institut route des cedex france merialdo fr 
carried author visitor continuous speech recognition group ibm watson research center yorktown heights ny usa 
part material included ieee international conference acoustics speech signal processing toronto canada may 

probabilistic formulation probabilistic formulation tagging problem assume alignments generated probabilistic model probability distribution case depending criterion choose evaluation optimal tagging procedure follows evaluation sentence level choose probable sequence tags sentence argmax argmax call procedure viterbi tagging 
achieved dynamic programming scheme 
evaluation word level choose probable tag word sentence argmax argmax ti ti tag assigned word wi tagging procedure context sentence call procedure maximum likelihood ml tagging 
interesting note commonly method viterbi tagging see derose church optimal method evaluation word level :10.1.1.79.5346
reasons preference presumably viterbi tagging simpler implement ml tagging requires computation asymptotic complexity viterbi tagging provides best interpretation sentence linguistically appealing ml tagging may produce sequences tags linguistically impossible choice tag depends contexts taken 
experiments show viterbi ml tagging result similar performance 
computational linguistics volume number course real tags generated probabilistic model able determine model exactly cause practical limitations 
models construct approximations ideal model exist 
