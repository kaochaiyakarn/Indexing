exponentiated gradient versus gradient descent linear predictors kivinen manfred warmuth ucsc crl june revised december center computer engineering information sciences university california santa cruz santa cruz ca usa consider algorithm line prediction linear model 
algorithms known gradient descent gd algorithm new algorithm call sigma maintain weight vector simple updates 
gd algorithm update subtracting gradient squared error prediction 
sigma algorithm uses components gradient exponents factors updating weight vector 
worst case loss bounds sigma compare previously known bounds gd algorithm 
bounds suggest losses algorithms general incomparable sigma smaller loss components input relevant predictions 
variables needed prediction loss bound sigma grows number variables 
gd algorithm biased hypotheses small norm variables relevant uses dimensions futile search predictor small norm 
feel situation favors sigma algorithm natural arise practice 
linear predictors restricted natural extension expand instance including new input variables values suitably chosen basis functions linear prediction algorithm linear combination basis functions predictor 
example include products original input variables :10.1.1.103.1189
assuming input variables range gamma increase norms instances 
assume outcomes degree polynomial input variables terms constant coefficient 
loss bound sigma algorithm expansion instances log 
gd algorithm suffer fact expansion increases norms instances loss kn 
experiments approximate algorithm extensive know trials gd cumulative losses gd sigma upper bounds sparse target expanded instances 
circumstances problems weights going zero 
performed preliminary experiments algorithm 
algorithms case input variables negative 
expanding instances experiment illustrates linear function learning learn nonlinear target functions means expanding instances way target function linear expanded inputs see boser :10.1.1.103.1189
example vector components monomials variables degree 
polynomial degree variables written delta coefficient vector accordingly polynomials degree learned linear functions expanded instances original instances input algorithm 
original instances components expanded instances components 
target polynomial terms target vector nonzero components sigma algorithm perform 
exponentiated gradient algorithms similarly generalized obtain new exponentiated back propagation algorithm 
long term research goal suggest developing family derived relative entropy distance measure 
neural network algorithms belong gradient descent family algorithms framework derived squared euclidean distance 
family includes perceptron algorithm thresholded linear functions gd algorithm linear functions standard back propagation algorithm multilayer sigmoid networks linear squares algorithm fitting line data points 
new family includes respectively winnow algorithm lit sigma algorithm exponentiated back propagation algorithm algorithm fitting line data points relative entropy coefficient vector minimized :10.1.1.130.9013
new family uses new bias favors sparse weight vectors 
observed case linear regression leads improved performance high dimensional problems target weight vector sparse 
expect see similar behavior general settings 
helmbold able prove worst case loss bounds single linear neurons tanh function sigmoid function loss function relative entropy loss 
technical report university tokyo tokyo 
amari 
em algorithm information geometry neural network learning 
neural computation january 
boser guyon vapnik :10.1.1.103.1189
training algorithm optimal margin classifiers 
proc 
th annu 
workshop comput 
academic press 
kss michael kearns robert schapire linda 
efficient agnostic learning 
machine learning 
lit littlestone :10.1.1.130.9013
learning quickly irrelevant attributes abound new algorithm 
machine learning 
lit littlestone 
line batch learning 
