appears proceedings twelfth national conference artificial intelligence aaai 
incorporating advice agents learn reinforcements richard maclin jude shavlik computer sciences dept university wisconsin west dayton street madison wi email cs wisc edu learning reinforcements promising approach creating intelligent agents 
reinforcement learning usually requires large number training episodes 
approach addresses shortcoming allowing connectionist learner accept advice time natural manner external observer 
approach advice watches learner occasionally suggestions expressed instructions simple programming language 
techniques knowledge neural networks programs inserted directly agent utility function 
techniques knowledge neural networks programs inserted directly agent utility function 
subsequent reinforcement learning integrates refines advice 
empirical evidence shows approach leads statistically significant gains expected reward 
importantly advice improves expected reward regardless stage training 
successful increasingly popular method creating intelligent agents learn reinforcements barto sutton watkins lin mahadevan connell :10.1.1.75.7884
approaches suffer need large numbers training episodes 
approaches speeding reinforcement learning proposed largely unexplored approach design learner accept advice external observer 
evaluate approach creating learners 
illustrate general idea advice imagine watching agent learning play video game 
section framework advice reinforcement learners 
subsequent section presents experiments investigate value approach 
list possible extensions describe relation research 
general framework section describe approach creating reinforcement learner accept advice 
connectionist learning sutton watkins form reinforcement learning rl :10.1.1.132.7760
shows general structure reinforcement learner augmented bold extensions 
rl learner senses current world state chooses action execute occasionally receives rewards 
reinforcements environment task learner improve action choosing module increases amount rewards receives 
augmentation observer watches learner advice behavior reinforcement action state learner environment observer rl external advisor 
key step determine optimal weights new piece advice rl fine tune 
step 
judge value advice 
currently rely learning wash poor advice 
envision circumstances game learner play tesauro agent builds internal world model sutton straightforward empirically evaluate new advice :10.1.1.20.3760
possible allow observer retract bad advice 
process adding network fragments shown figures assuming advice came learner began exploring learning 
key agent enemy obstacle empty reward hidden units wall enemy obstacle empty sector wall enemy obstacle empty sector ring wall enemy obstacle empty sector ring actions sensor inputs reward reward reward action test environment sample configuration sample division environment sectors distances measured agent sensors neural network computes utility actions 
experimental study empirically judge value approach providing advice rl agent 
table mean number enemies captured rewards collected number actions taken experiments summarized table 
advice added enemies rewards survival time baseline surrounded related tasks intend address near term 
current experiments demonstrate value giving single piece advice 
plan empirically study effect providing multiple pieces advice different times training 
intend evaluate replay periodic retraining remembered pairs states reinforcements method shown greatly reduce number training examples needed learn policy function lin :10.1.1.75.7884:10.1.1.75.7884
number research efforts related 
utgoff lin whitehead developed methods advisor provides feedback learner advisor evaluates chosen action suggests appropriate action 
lin investigated teaching method input rl system includes previous input values 
thrun mitchell investigated rl agents prior knowledge form neural networks trained predict results actions 
