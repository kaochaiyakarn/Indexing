support vector machine learning interdependent structured output spaces ioannis tsochantaridis thomas hofmann department computer science brown university providence ri thorsten joachims department computer science cornell university ithaca ny altun department computer science brown university providence ri learning general functional dependencies main goals machine learning 
progress kernel methods focused designing flexible powerful input representations 
addresses complementary issue problems involving complex outputs suchas multiple dependent output variables structured output spaces 
propose generalize multiclass support vector machine learning formulation involves features extracted jointly inputs outputs 
resulting optimization problem solved efficiently cutting plane algorithm exploits sparseness structural decomposition problem 
demonstrate versatility effectiveness method problems ranging supervised grammar learning named entity recognition taxonomic text classification sequence alignment 

deals general problem learning mapping inputs discrete outputs training sample input output pairs xn yn unknown probability distribution 
case multiclass classification interchangeable arbitrarily numbered labels consider structured output spaces elements may instance sequences strings labeled trees appearing proceedings st international conference machine learning banff canada 
copyright authors 
cs brown edu th cs brown edu tj cs cornell edu altun cs brown edu lattices graphs 
problems arise variety applications ranging multilabel classification classification taxonomies label sequence learning sequence alignment learning supervised grammar learning name just 
problems generalizing large margin methods specifically multi class support vector machines svms weston watkins crammer singer broader problem learning structured responses 
naive approach treating separate class intractable leads multiclass problem witha large number classes 
overcome problem specifying discriminant functions exploit structure dependencies th respect collins perceptron learning witha similar class discriminant functions 
maximum margin algorithm propose advantages terms accuracy specific loss functions 
similar philosophy kernel methods learning general dependencies pursued kernel dependency estimation kde weston 
separate kernels inputs outputs kernel pca standard regression techniques significantly differs formulation straightforward natural generalization multiclass svms 

discriminants loss functions interested general problem learning functions training sample input output pairs 
illustrating example consider case natural language parsing function maps sentence parse tree 
illustration natural language parsing model 
depicted graphically 
pursue learn discriminant function input output pairs derive prediction maximizing response variable specific input general form hypotheses argmax denotes parameter vector 
useful think parameterized family cost functions try design way minimum desired output inputs interest 
assume linear combined feature representation inputs outputs 
specific form depends nature problem special cases discussed subsequently 
natural language parsing illustrative example chose get model isomorphic probabilistic context free grammar pcfg 
parse tree sentence corresponds grammar rule gj turn 
valid parse trees trees witha designated start symbol root words sentence leaves sentence scored sum wj nodes 
score written histogram vector counting grammar rule gj occurs tree efficiently computed finding structure maximizes cky algorithm see manning schuetze 
learning structured output spaces inevitably involves loss functions standard classification loss cf 
weston 

example natural language parsing parse tree differs correct parse nodes treated differently parse tree radically different 
typically correctness predicted parse tree measured score see johnson harmonic mean precision recall calculated overlap nodes trees 
assume availability bounded loss function quantifies loss associated prediction ifthe true output value ifp denotes data generating distribution goal find function hypothesis class risk dp 
minimized 
assume unknown finite training set pairs xi yi generated 
performance function training sample described empirical risk 
parameterized hypothesis classes write similarly empirical risk 

margins margin maximization consider separable case exists function parameterized empirical risk zero 
assume condition zero training error compactly written set non linear constraints max xi xi yi 
yi inequality equivalently replaced linear inequalities resulting total linear constraints yi defined shorthand xi yi xi 
set inequalities feasible typically solution specify unique solution propose select score correct label yi uniformly different closest yi yi xi generalizes maximum margin principle employed svms vapnik general case considered 
resulting hard margin optimization problem svm min yi 
allow errors training set introduce slack variables propose optimize soft margin criterion 
ways doing follow crammer singer introduce slack variable non linear constraint result upper bound empirical risk offers additional algorithmic advantages 
adding penalty term linear slack variables objective results quadratic program svm min yi 
alternatively penalize margin violations quadratic term leading analogue optimization problem refer svm constant controls tradeoff training error minimization margin maximization 
svm implicitly considers zero classification loss 
argued inappropriate problems natural language parsing large 
propose approaches generalize formulations case arbitrary loss functions 
re scale slack variables loss incurred linear constraints 
intuitively violating margin constraint involving yi yi penalized severely violation involving output value loss 
accomplished multiplying violation loss equivalently scaling slack variables withthe inverse loss yields problem svm min yi 
yi justification formulation subsequent proposition proof omitted 
proposition 
denote optimal solu upper bound tion svm empirical risk 
optimization problem svm derived analogously yi replaced yi order obtain upper bound empirical risk 
second way include loss functions re scale margin proposed taskar 
special case hamming loss 
margin constraints setting take form yi yi set constraints yield optimization problem svm results upper bound 
opinion potential disadvantage margin scaling approach may give significant weight output values close confusable withthe target values yi increase loss increases required margin 

support vector machine learning key challenge solving generalized svm learning large number margin constraints specifically total number constraints 
cases may extremely large particular product space sort grammar learning label sequence learning 
standard quadratic programming solvers unsuitable type problem 
propose algorithm exploits special structure maximum margin problem smaller subset constraints needs explicitly examined 
algorithm generalization svm algorithm label sequence learning hofmann altun algorithm inverse sequence alignment joachims 
show compute arbitrarily close approximations svm optimization problems polynomial time large range structures loss functions 
algorithm operates dual program derive wolfe dual various soft margin formulations 

dual programs denote iy lagrange multiplier enforcing margin constraint label yi example xi yi 
standard lagrangian duality techniques arrives dual qp hard margin case svm max iy iy yi yi iy 
kernel replace inner products inner products easily expressed inner products original vectors 
soft margin optimization re scaling linear penalties svm additional box constraints yi iy yi added dual 
quadratic slack penalties svm lead dual svm altering inner product ij ij ifi 
yi yj case margin re scaling loss function affects linear part objective function max iy yi quadratic part unchanged introduces standard box constraints yi iy 
algorithm algorithm propose aims finding small set active constraints ensures sufficiently accurate solution 
precisely creates nested sequence successively tighter relaxations original problem cutting plane method 
implemented variable selection dual formulation 
show valid strategy exists polynomially sized subset constraints corresponding solution fulfills constraints witha precision 
means remaining potentially exponentially constraints guaranteed violated need explicitly adding optimization problem 
base optimization dual program formulation important advantages primal qp 
depends inner products joint feature space defined allowing kernel functions 
second constraint matrix dual program svms supports natural problem decomposition block diagonal block corresponds specific training instance 
pseudocode algorithm depicted algorithm 
algorithm applies svm formulations discussed 
difference way cost function gets set step 
algorithm maintains working set si example xi yi keep track selected constraints define current relaxation 
iterating training examples xi yi algorithm proceeds algorithm algorithm solving svm loss re scaling formulations svm svm input xn yn si repeat set cost function svm yi svm yi svm yi svm yi sj jy 
compute yh compute max maxy si si si optimize dual isi 
si changed iteration finding potentially violated constraint involving output value line 
appropriately scaled margin violation constraint exceeds current value line dual variable corresponding added working set line 
variable selection process dual program corresponds successive strengthening primal problem cutting plane cuts current primal solution feasible set 
chosen cutting plane corresponds constraint determines lowest feasible value constraint added solution recomputed wrt 
line 
alternatively devised scheme optimization restricted si optimization full performed frequently 
beneficial due block diagonal structure optimization problems implies variables jy sj simply frozen current values 
notice variables included respective working set implicitly treated 
algorithm stops constraint violated 
algorithm implemented available part svm light note svm optimization problems iteration iteration differ single constraint 
restart svm optimizer current solution greatly reduces runtime 
convenient property algorithms general defined interface independent choice svmlight joachims org 
apply algorithm sufficient implement feature mapping explicit joint kernel function loss function yi maximization step 
particular constraint cut selection method treated black boxes 
modeling yi straightforward solving maximization problem constraint selection typically requires exploiting structure 

analysis straightforward show algorithm finds solution close optimal svm adding feasible point primal maximum 
immediately obvious fast algorithm converges 
show algorithm converges polynomial time large class problems despite possibly exponential infinite 
elementary lemma helpful proving subsequent results 
quantifies dual objective changes optimizes single variable 
lemma 
positive definite matrix define concave quadratic program assume 
maximizing respect keeping components fixed increase objective hr provided hr 
proof 
denote solution withthe changed hr difference maximized hr notice hr 
lemma lower bound improvement dual objective step algorithm 
brevity focus case svm similar results derived variants 
proposition 
define maxy yi ri maxy 
step algorithm improves dual objective svm ir proof 
notation algorithm apply lemma denoting newly added constraint hr yi yi iy yi yi note 
fact yi iy yi lemma shows increase objective function optimizing yi iy yi yi yi yi step follows fact yi condition step 
replacing quantities denominator upper limit proves claim jointly optimizing variables just increase dual objective 
leads polynomial bound maximum size theorem 
maxi ri maxi algorithm svm terminates incrementally adding constraints working set proof 
optimal value dual 
constraint added violated provided constraint exists 
solving relaxed qp step objective increase proposition 
constraints dual objective times amount 
result follows fact dual objective upper bounded minimum primal turn bounded 
note number constraints depend 
crucial exponential infinite interesting problems 
problems step computed polynomial time algorithm runtime polynomial constraint added cycling instances step polynomial 

applications experiments demonstrate effectiveness versatility approach report results number different tasks adapt algorithm new problem sufficient implement feature mapping loss function yi maximization step 

multiclass classification algorithm implement conventional wta multiclass classification crammer singer follows 
yk stack vectors vk weight vector associated yk 
crammer singer define yk vk denotes arbitrary input representation 
discriminant functions equivalently represented proposed framework defining joint feature map follows 
refers orthogonal binary encoding label tensor product forms products coefficients argument vectors 

classification taxonomies generalization propose interesting output features orthogonal representation exemplary application kind show take advantage known class taxonomies 
taxonomy treated lattice classes minimal elements 
node lattice corresponding super class class introduce binary attribute indicating predecessor notice count number common predecessors 
performed experiments document collection released world intellectual property organization uses international patent classification ipc scheme 
restricted sections section consisting documents collection 
experiments indexed title claim tags 
furthermore subsampled training data investigate effect training set size 
document parsing tokenization term normalization performed table 
results alpha corpus section groups fold fold cross validation respectively 
flt standard flat svm multiclass model tax hierarchical architecture 
denotes training classification loss refers training 
flt tax flt tax training instances class acc loss training instances class acc loss retrieval engine 
suitable loss function tree loss function defines loss classes height common ancestor taxonomy 
results summarized table show proposed hierarchical svm learning architecture improves performance standard multiclass svm terms classification accuracy terms tree loss 

label sequence learning label sequence learning deals withthe problem predicting sequence labels sequence inputs 
subsumes problems segmenting annotating observation sequences widespread applications optical character recognition natural language processing information extraction computational biology 
study algorithm named entity recognition ner problem 
specifically consider sub corpus consisting sentences spanish news wire article corpus provided special session conll devoted ner 
label set corpus consists non name continuation person names organizations locations miscellaneous names resulting total different labels 
setup followed altun 
joint feature map histogram state transition plus set features describing emissions 
adapted version viterbi algorithm solve argmax line 
svm second degree polynomial kernel 
results table zero loss compare generative hmm conditional random fields crf lafferty collins www com table 
results various algorithms named entity recognition task altun 
method hmm crf perceptron svm error table 
results various svm formulations named entity recognition task 
method train err test err const avg loss svm svm svm svm algorithm 
discriminative learning methods substantially outperform standard hmm 
addition svm performs slightly better perceptron crfs demonstrating benefit large margin approach 
table shows svm formulations perform comparably probably due fact vast majority support label sequences having hamming distance correct label sequence notice loss equal svm formulations equivalent 

sequence alignment show apply proposed algorithm problem learning align sequences 
pair sequences alignment methods smith waterman algorithm select sequence operations insertion substitution transforms maximizes linear objective function derived negative operation costs histogram alignment operations 
value measure similarity 
order learn cost vector training data type 
native sequence xi similar sequence zi believed close optimal alignment ai 
addition set decoy sequences alignments 
goal zt find cost vector sequences close native sequence decoy sequences away 
yi zi zk output space th example seek xi zi ai exceeds xi zt implies zero loss hypotheses form xi yi maxa smith waterman algorithm implement maxa 
table shows test error rates fraction times homolog selected synthetic dataset table 
error rates number constraints depending number training examples 
train error test error svm svm const xi zj xi zj described joachims 
results averaged train test samples 
model contains parameters substitution matrix cost insert delete 
train model svm compare generative sequence alignment model substitution matrix computed ij log laplace estimates 
generative model report results performs best test set 
despite unfair advantage svm performs better low training set sizes 
larger training sets methods perform similarly small preference generative model 
advantage svm model straightforward train gap penalties 
predicted theorem number constraints low 
appears grows sub linearly withthe number examples 

natural language parsing test feasibility approach learning weighted context free grammar see subset penn treebank wall street journal corpus 
consider sentences length sections training set sentences test set 
setup johnson start part speech tags learn weighted grammar consisting rules occur training data 
solve argmax line algorithm modified version cky parser mark johnson incorporated svm light results table 
show accuracy micro averaged training test set 
line shows performance generative pcfg model maximum likelihood estimate mle computed johnson implementation 
second line show svm loss lines give results loss yi yi svm www cog brown edu mj software htm table 
results learning weighted context free grammar penn treebank 
cpu time measured hours 
train test training efficiency method acc acc const cpu qp pcfg svm svm svm svm results 
values gave comparable results 
zero loss achieves better accuracy predicting complete tree correctly score marginally better 
loss gives substantially better scores outperforming mle substantially 
difference significant mcnemar test scores 
conjecture achieve gains incorporating complex features grammar impossible best awkward generative pcfg model 
note approach handle arbitrary models overlapping features argmax line computed 
terms training time table shows total number constraints added working set small 
roughly twice number training examples cases 
training faster zero loss time solving remains roughly comparable 
re scaling formulations lose time argmax line 
sped naive algorithm experiments 

formulated support vector method supervised learning interdependent outputs 
joint feature map input output pairs covers large class interesting models including weighted context free grammars hidden markov models sequence alignment 
furthermore approach flexible ability handle application specific loss functions 
solve resulting optimization problems proposed simple general algorithm prove convergence bounds 
empirical results verify algorithm tractable 
furthermore show generalization accuracy method comparable exceeds conventional approaches wide range problems 
promising property method train complex models difficult handle generative setting 
acknowledgments authors cai conducting experiments classification taxonomies 
supported kanellakis dissertation fellowship nsf itr iis nsf career award 
altun tsochantaridis hofmann 

hidden markov support vector machines 
icml 
collins 

discriminative training methods hidden markov models theory experiments algorithms 
emnlp 
collins 

parameter estimation statistical parsing models theory practice distribution free methods 
crammer singer 

algorithmic implementation multi class kernel vector machines 
machine learning research 
hofmann tsochantaridis altun 

learning structured output spaces joint kernel functions 
sixth kernel workshop 
joachims 

learning align sequences maximum margin approach technical report 
cornell university 
johnson 

pcfg models linguistic tree representations 
computational linguistics 
lafferty mccallum pereira 

conditional random fields probabilistic models segmenting labeling sequence data 
icml 
manning schuetze 

foundations statistical natural language processing 
mit press 
taskar guestrin koller 

markov networks 
nips 
vapnik 

statistical learning theory 
wiley sons weston chapelle elisseeff sch lkopf vapnik 

kernel dependency estimation 
nips 
weston watkins 

multi class support vector machines technical report csd tr 
department computer science royal holloway university london 
