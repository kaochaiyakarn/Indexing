mining features sequence classification neal lesh mohammed zaki ogihara lesh merl com zaki cs rpi edu ogihara cs rochester edu merl mitsubishi electric research laboratory broadway th floor cambridge ma computer science dept rensselaer polytechnic institute troy ny computer science dept rochester rochester ny classification algorithms difficult apply sequential examples vast number potentially useful features describing example 
past feature selection focused searching space subsets features intractable large feature sets 
adapt sequence mining techniques act preprocessor select features standard classification algorithms naive bayes winnow 
experiments different datasets show features produced algorithm improve classification accuracy classification algorithms thousands features describing example littlestone 
domains number potentially useful features exponential size examples 
data mining algorithms zaki search billions rules patterns select interesting ones 
adapt data mining techniques act preprocessor construct set features classification 
past rules produced data mining algorithms construct classifiers primarily ordering rules decision lists segal etzioni liu merging general rules occur training data lee 
convert patterns discovered mining algorithm set boolean features feed standard classification algorithms 
classification algorithms turn assign weights features allows evidence different rules combined order classify new example 
lot feature selection mainly concentrated non sequential domains 
contrast focus sequence data example represented sequence events event described set predicates 
examples sequence data include text dna sequences web usage data execution traces 
combine powerful mining paradigms sequence mining efficiently search patterns correlated target classes classification learns weigh evidence different features classify new examples 
scalable disk feature mining algorithm 
specify criteria electing features pruning rules allow efficient feature mining 
integrates pruning constraints algorithm post processing enabling efficiently search large pattern spaces 
data mining features formulate algorithm feature mining 
set distinct features finite set possible values 
set possible feature value pairs 
sequence ordered list subsets example example sequence ab bc 
sequence denoted sequence element subset length sequence width maximum size say subsequence denoted exists integers 
ij example ab subsequence ab bc 
set class labels 
example pair sequence label 
example unique identifier eid time stamp occurred 
example said contain sequence 
input database consists set examples 
means data look multiple sequences composed sets items 
frequency sequence denoted fr fraction examples contain 
sequence class label 
confidence rule denoted conf conditional probability label example contains sequence 
conf fr dc fr dc subset examples class label sequence said frequent frequency user specified min freq threshold 
rule said strong confidence user specified min conf threshold 
goal mine frequent strong patterns 
shows database examples 
examples belonging class belonging class 
general classes 
looking different min freq class 
example frequent class frequent class 
rule confidence rule confidence 
sequence classifier function sequences classifier evaluated standard metrics examples eid eid time items class new boolean features ab frequent sequences class min freq ab ab ab class min freq original database new database boolean features accuracy coverage 
describe frequent sequences features classification 
recall input standard classifiers example represented vector feature value pairs 
represent example sequence vector pairs treating sequence boolean feature true iff 
example suppose features bc cd 
sequence ab bd bc represented true true false note features skip steps feature bc holds ab bd bc 
selection criteria mining specify selection criteria selecting features classification 
objective find sequences representing examples sequences yield highly accurate sequence classifier 
want search space subsets features caruana freitag want evaluate feature isolation pair wise comparison candidate features 
certainly criteria selecting features depend domain classifier 
believe domain classifier independent heuristics useful selecting sequences serve features features frequent 
features distinctive class 
feature sets contain redundant features 
intuition heuristic simply rare features definition rarely useful classifying examples 
problem formulation heuristic translates requirement features minimum frequency training set 
note different min freq class patterns rare entire database frequent specific class 
ignore class patterns rare class 
intuition second heuristic features equally classes help determine class example belongs 
course conjunction multiple non distinctive features distinctive 
case algorithm prefers distinctive conjunction feature non distinctive conjuncts 
encode heuristic requiring selected feature significantly correlated class frequent 
motivation third heuristic features closely correlated useful classification 
show reduce number features time needed mine features pruning redundant rules 
addition wanting prune features provide information want prune feature feature available provides strictly information 
set examples contain feature say feature subsumes feature respect predicting class data set iff dc dc 
intuitively subsumes class superior predicting covers example training data covers covers subset non examples covers 
third heuristic leads pruning rules feature mining algorithm described 
pruning rule extend specialize feature accuracy 
feature contained examples class 
specializations may pass frequency confidence tests definition feature mining subsumed 
lemma captures pruning rule lemma fi fj conf fi fi subsumes fj respect class pruning rule concerns correlations individual items 
recall examples represented sequence sets 
say examples occurs set sequence occurs 
lemma states feature containing set subsumed generalizations prune lemma subsumed feature mining define feature mining task 
inputs algorithm set examples parameters min freq maxw maxl 
output non redundant set frequent distinctive features width maxw length maxl 
formally examples parameters min freq maxw maxl return feature set feature fi class cj length fi maxl width fi maxw fr min freq cj conf cj significantly greater chi squared test dc contains fi contains feature subsumes fi respect class cj data set efficient mining features algorithm leverages existing data mining techniques efficiently mine features set training examples 
proposed spade algorithm zaki fast discovery sequential patterns 
spade scalable disk algorithm handle millions example sequences items 
consequently shares properties 
construct adapted spade algorithm search databases labeled examples 
mines patterns predictive classes database simultaneously 
opposed previous approaches mine millions patterns apply pruning post processing step integrates pruning techniques mining algorithm 
enables search large space previous methods fail 
uses observation subsequence relation defines partial order sequences 
say general specific 
relation monotone specialization relation respect frequency fr frequent sequence subsequences frequent 
algorithm systematically searches sequence lattice spanned subsequence relation general specific sequences depth manner 
eid class ab ab frequent sequence lattice class index table eid time eid time original id list database frequency frequency intersect intersect eid suffix joins id lists eid time ab ab time eid time frequency table sequence lattice frequency computation frequency computation uses vertical database layout associate item sequence lattice idlist denoted list example ids eid event time time pairs containing item 
sequence determine support sequence simply intersecting length subsequences 
check cardinality resulting idlist tells new sequence frequent 
shows idlist obtained intersecting lists 
similarly ab 
maintain class index table indicating classes example 
table able determine frequency sequence classes time 
example occurs eids 
eids label label 
frequency 
class frequencies pattern shown frequency table 
limited amount main memory breaks sequence search space small independent manageable chunks processed memory 
accomplished partition 
say length sequences equivalence class partition share common length suffix 
partitions length suffixes called parent partitions 
parent partition independent sense complete information generating frequent sequences share suffix 
example class elements possible frequent sequences step item lead frequent sequence suffix qx 
min freq ci parent partitions pi parent partition pi pi elements ai elements aj ai aj ai aj maxw maxl false frequency ci min freq ci ci maxw maxl width maxw length maxl return true accuracy return true return false algorithm feature enumeration processes parent partition depth manner shown pseudo code 
input procedure partition idlist elements 
frequent sequences generated intersecting distinct pairs sequences partition checking cardinality resulting idlist min sup ci 
sequences frequent class ci current level form partitions level 
process repeated find frequent sequences 
integrated constraints integrates pruning constraints mining algorithm applying pruning post processing step 
shall show allows search large spaces efficiently infeasible 
rule prune procedure eliminates features pruning rules length width constraints 
pruning rule tested time extend sequence new item exists efficient time method applying rule 
idea compute frequency length sequences 
fr ab fr remove ab suffix partition 
guarantees ab appear set sequence 
empirical evaluation describe experiments test features produced system improve performance winnow littlestone naive bayes duda hart classification algorithms 
ran experiments datasets 
case experimented various settings min freq maxw maxl generate reasonable results 
report values 
random parity problems describe nonsequential problem standard classification algorithms perform poorly 
problem consists parity problems size distracting irrelevant features 
boolean feature fi additionally irrelevant boolean feature ik 
generate instance randomly assign boolean feature true false probability 
example instance true false true true false false true false 
features distinct instances 
choose weights wn assign instance class labels follows 
instance credited weight wi iff ith set features parity 
score instance sum weights wi number true features fi fi 
instance score greater half sum weights wi instance assigned class label assigned 
note feature indicative class label parity problems hard classifiers 
job essentially features grouped 
example features produced true true true false 
min freq maxl maxw forest fire plans algorithm originally motivated task plan monitoring stochastic domains 
example domain constructed simple forest fire domain loosely phoenix fire simulator hart cohen 
grid representation terrain 
grid cell contain vegetation water base 
simulation fire started random location 
iteration simulation fire spreads stochastically 
probability cell time calculated cell vegetation wind direction cell neighbors burning time 
additionally contain fire reach bases 
example terrain hand designed plan dig fire line fire 
speed varies simulation simulation 
example simulation looks time ignite time moveto bd time moveto bd time bd time ignite time ignite time ignite time ignite 
form database instances set simulations follows 
idea predict success failure plan finished instance list events happen time vary experiments 
label instance success locations bases burned final state failure 
job classifier predict prevent bases burning partial execution trace plan 
example features produced domain moveto bd time ignite time moveto sequence holds bd moves second column time 
second holds fire second column moves third row time 
correlations second pruning rule described experiment wfm parity parity parity fire time fire time fire time spelling vs spelling vs spelling vs spelling re vs table classification results winnow bayes wfm winnow bayes resp 
experiment evaluated selected features features random fire world time spelling vs table mining results section arise data sets 
example ignite arises test plans moves eighth column 
fire data boolean features describe event 
maxw maxl possible composite features describing sequence events 
experiments reported min freq maxw maxl 
context sensitive spelling correction tested algorithm task correcting spelling errors result valid words substituting golding roth 
test chose commonly confused words searched sentences word brown corpus kucera francis containing word 
removed target word represented word word part speech tag brown corpus position relative target word 
example sentence politics translated word tag cc pos word tag rb pos word tag pos word politics tag nn pos 
example features produced include pos word indicating word occurs words target word pos tag nn pos indicating noun occurs words target word 
features reasons obvious significantly correlated training set 
experiments reported min freq maxw maxl 
results test parity fire domains mined features examples pruned features pass chi squared significance test correlation class feature frequent examples trained classifier examples 
entire training process required examples 
tested resulting classifier fresh examples 
results tables averaged trials process retrained re tested classifier fresh examples trial 
spelling correction trained percent examples brown corpus tested remaining percent 
training mined features sentences trained classifier training examples 
table shows features produced improved classification performance 
compared feature set produced fea experiment cpu seconds cpu seconds cpu seconds features features examined features pruning examined examined pruning pruning pruning pruning pruning random fire world hours spelling table impact pruning rules results taken data set example 
primitive features features length 
winnow naive bayes performed better features produced 
parity experiments mined features dramatically improved performance classifiers experiments mined features improved accuracy classifiers significant amount 
table shows number features evaluated number returned problems 
largest parity problem evaluated features selected 
fact possible features booleans features giving rise feature value pairs searched depth 
rejected implicitly pruning rules 
table shows impact pruning rule mining time 
results data set domain slightly higher values maxl maxw experiments 
pruning rule improve mining time cases tremendous difference fire world problems event descriptors appear 
pruning fire world problems essentially unsolvable finds frequent sequences 
related great deal done feature subset selection motivated observation classifiers perform worse feature set caruana freitag 
algorithms explore exponentially large space subsets feature set 
contrast explore exponentially large sets potential features evaluate feature independently 
approach infeasible problems consider contain hundreds thousands millions potential features 
golding roth applied winnow algorithm context sensitive spelling correction 
sets features features prune classification accuracy individual features 
obtain higher accuracy 
approach involves ensemble combined majority weighting took care choosing parameters specific task 
goal demonstrate features produced improve classification performance 
data mining algorithms applied task classification 
liu build decision lists patterns association mining 
ali bayardo combine association rules form classifiers 
sequence mining generalization association mining 
pruning rules resemble ones segal etzioni employs data mining techniques construct decision lists 
previous data mining classification focused combining highly accurate rules 
contrast algorithm weigh evidence features low accuracy order classify new examples 
liu setiono describes feature subset selection 
apply probabilistic las vegas algorithm data sets features 
problems parity problem described contains features 
algorithms search space subsets available features 
comparison applied algorithms parity problems features results pairs 
algorithm searches set conjunctions maxw feature value pairs 
handle millions examples items extremely scalable 
close spirit kudenko hirsh constructs set sequential boolean features classification algorithms 
employ heuristic search algorithm called incrementally generalizes features cover training examples classification performance hold set training data perform exhaustive search depth accept features meet selection criteria 
additionally different feature language tested approaches different classifiers 
ali ali srikant 
partial classification association rules 
kdd 
bayardo jr bayardo 
brute force mining classification rules 
kdd 
caruana freitag caruana freitag 
greedy attribute selection 
icml 
duda hart duda hart 
pattern classification scene analysis 
wiley 
golding roth golding roth 
applying winnow context sensitive spelling correction 
icml 
hart cohen hart cohen 
predicting explaining success task duration phoenix planner 
st intl 
conf 
ai planning systems 
kucera francis kucera francis 
computational analysis day american english 
brown university press providence ri 
kudenko hirsh kudenko hirsh 
feature generation sequence categorization 
aaai 
lee lee stolfo mok 
mining audit data build intrusion detection models 
kdd 
littlestone littlestone 
learning quickly irrelevant attributes abound new linear threshold algorithm 
machine learning 
liu setiono liu setiono 
issues scalable feature selection 
th world congress expert systems application advanced info 
technologies 
liu liu hsu ma 
integrating classification association rule mining 
kdd 
segal etzioni richard segal oren etzioni 
learning decision lists homogeneous rules 
aaai 
zaki zaki 
efficient enumeration frequent sequences 
th intl 
conf 
info 
knowledge management 
