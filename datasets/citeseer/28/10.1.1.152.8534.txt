gene functional classification heterogeneous data paul pavlidis columbia genome center columbia university pp columbia edu cai department medical informatics columbia university dmi columbia edu jason weston technologies georgia jason com william noble grundy department computer science columbia university cs columbia edu proceedings fifth international conference computational molecular biology april 
appear 
attempts understand cellular function molecular level able synthesize information disparate types genomic data 
consider problem inferring gene functional classifications heterogeneous data set consisting dna microarray expression measurements phylogenetic profiles genome sequence comparisons 
demonstrate application support vector machine svm learning algorithm functional inference task 
results suggest importance exploiting prior information heterogeneity data 
particular propose svm kernel function explicitly heterogeneous 
show knowledge heterogeneity aid feature selection 
primary goal biology understand molecular machinery cell 
sequencing projects currently underway provide view machinery 
complementary view provided data dna microarray hybridization experiments 
describe computational techniques inferring gene function distinct types data 
techniques step longer term goal learning gene function simultaneously different types genomic data 
clearly availability complete genomic sequence human species provides tremendous opportunity understanding functions biological macromolecules 
infer gene function phylogenetic profiles derived comparison gene collection complete genomes 
profile characterizes evolutionary history gene 
genes similar phylogenetic profiles similar functions assumption similar pattern inheritance species result functional link 
gene function inferred dna microarray expression data 
offering snapshot messenger rna expression levels thousands genes microarrays allow biologists formulate models gene expression scale years ago 
initial analyses type corresponding author department computer science columbia university computer science building mail code amsterdam avenue new york ny tel fax data focused clustering algorithms hierarchical clustering self organizing maps 
unsupervised algorithms attempt automatically locate clusters genes share similar expression patterns may share similarity function 
brown applied collection supervised learning techniques set microarray expression data yeast 
showed algorithm known support vector machine svm provides excellent classification performance 
extend methodology brown learn gene functional classifications heterogeneous data set consisting microarray expression data phylogenetic profiles 
show combination successful svm operates feature space explicitly heterogeneous 
svms members larger class algorithms known kernel methods non linearly mapped higher order feature space replacing dot product operation input space kernel function mercer theorem shows positive semi definite kernel function corresponds dot product operation higher dimensional feature space 
construct explicitly heterogeneous kernel function computing separate kernels data type summing results 
resulting kernel incorporates prior knowledge heterogeneity data accounting higher order correlations features data type ignoring higher order correlations data types 
heterogeneous kernel leads improved performance respect svm trained directly concatenated data 
show prior knowledge heterogeneity exploited selecting subsets input features classification 
gene functional classifications investigated type genomic data provides significantly better training data type 
feature selection algorithms available automatically selecting useful features training classifier 
demonstrate data feature selection algorithms select data types learn phylogenetic profiles gene expression data perform better algorithms directly select features combined data set 
idea combining heterogeneous data sets infer gene function new 
marcotte describe algorithm functional annotation uses expression vectors phylogenetic profiles evolutionary evidence domain fusion 
algorithm consists predicting functional links pairs genes type data separately cataloging complete list links 
contrast svm method described considers various types data making single prediction gene respect functional category 
performance svms data types combined single hypothesis formed superior combining independent hypotheses believe true wide range techniques 
methods experiments carried types genomic data 
data set derives collection dna microarray hybridization experiments 
data point represents logarithm ratio expression levels particular gene different experimental conditions 
data consists set element gene expression vectors yeast genes 
genes selected eisen availability accurate functional annotations 
data generated spotted arrays samples collected various time points shift mitotic cell division cycle temperature reducing shocks available stanford web site www genome stanford edu 
addition microarray expression data yeast genes characterized phylogenetic profile 
simplest form phylogenetic profile bit string boolean value bit indicates gene interest close homolog corresponding genome 
profiles employed contain position negative logarithm lowest value reported blast version search complete genome negative values corresponding values greater truncated :10.1.1.17.9507
genes organism similar phylogenetic profiles reasons 
genes high level sequence similarity definition similar phylogenetic profiles 
second genes lack sequence similarity similarity phylogenetic profiles reflects similar pattern occurence homologs species 
coupled inheritance may indicate functional link genes hypothesis genes absent function independently 
profiles study constructed complete genomes collected institute genomic research website www org tdb sanger centre website www sanger au uk 
prior learning gene expression phylogenetic profile vectors adjusted mean variance 
classification experiments carried gene functional categories munich information center protein sequences yeast genome database www mips biochem mpg de proj yeast 
database contains functional classes definitions come biochemical genetic studies gene function 
experiments reported classes containing genes substantially encompassed class amounting classes 
complete data set corresponding classifications available www cs columbia edu 
class support vector machine trained discriminate class members nonmembers 
support vector machine supervised learning algorithm developed past decade vapnik 
form employed svms learn binary classifications svm learns answer question gene belong functional class category ribosomal genes sugar support vector machines classify points locating respect hyperplane separates class members non class members high dimensional feature space 
characteristics feature space determined kernel function selected apriori 
current experiments employ kernel function shown produce classification performance classes gene expression data set 
function dot product raised third power kernel function takes account pairwise tertiary correlations gene expression measurements 
normalization term denominator projects data unit sphere 
previous svm uses soft margin accounts disparity number positive negative examples class 
details adjustment see 
useful svms available software perform experiments www cs columbia edu 
types data gene expression phylogenetic profiles combined different fashions refer early intermediate late integration 
methods summarized 
early integration types vectors concatenated form single set length vectors serve input svm 
intermediate integration kernel values type data pre computed separately resulting values added 
summed kernel values training svm 
kernel function heterogeneous kernel subscripts denote gene expression phylogenetic profile data respectively 
late integration svm trained data type resulting discriminant values added produce final discriminant gene 
intermediate integration heterogeneous kernel propose attempt incorporate prior knowledge task hand 
method creates local features polynomial relationships inputs single type data 
local features combined linearly create global features 
global features hyperplane constructed 
constrast feature space produced early integration method polynomial relationships different types inputs ignored 
restriction reflects intuition correlations inputs type data stronger early integration data kernel matrix discriminants intermediate integration late integration methods learning heterogeneous data support vector machine 
early integration types data concatenated form single set input vectors 
intermediate integration kernel values computed separately data set summed 
late integration svm trained data type resulting discriminant values summed 
correlations data types 
theoretical terms removal correlations reduces overfitting unneeded capacity reduced 
approach similar spirit digit recognition problems order incorporate prior knowledge spatial location 
incorporation achieved constructing sparse polynomials sum sub kernels computed small patches image 
experiments pixel input spaces authors decreased number polynomial features reported reduction test error 
classification experiment performed cross validation 
class positively labeled negatively labeled genes split randomly groups fold cross validation 
svm trained groups tested remaining group 
procedure repeated times time different group genes test set 
experiments fold cross validation 
leave cross validation experiments simply fold cross validation equal total number training examples 
performance svm measured examining classifier identifies positive negative examples test sets 
judge performance define cost method number false positives method number false negatives method number members class 
false negatives weighted heavily false positives data number positive examples small compared number negatives 
see unequal cost weighting important consider classifiers trained recognize class contains genes 
say test set genes classifier correctly identifies includes non list positive genes 
hand suppose classifier classifies test set negative 
clearly classifier learned recognize classifier learned 
equal weighting false positive false negatives classifiers yield cost 
assign higher cost false negatives order implement intuition failing recognize positive examples worse inaccurately including negative examples test set 
cost method compared cost null learning procedure classifies test examples negative 
define normalized cost savings learning procedure total number positive examples class 
perfect classifier normalized cost savings null classifier normalized cost savings 
performed feature selection combined data fisher criterion score :10.1.1.21.2174
feature compute mean standard deviation feature positive examples respectively negative examples 
fisher criterion score gives higher values features means differ greatly classes relative variances 
nearest neighbors algorithm see means selecting type data learn 
algorithm labels test point positive closest euclidean distance neighbors training set labeled positive point labeled negative 
nearest neighbor amounts assigning point label associated closest neighbor 
set features measure quality set leave error algorithm 
results initial experiments aimed determining classes selected learnable data type 
cost savings measure data type selected top classes study 
classes appear lists yielding total classes 
results summarized columns table 
included table equivalents classes brown 
experiments show svm methodology generalizes phylogenetic profile data new type data allows characterization new functional classes 
second set experiments tests ability svms learn types data 
final columns table summarize results table provides details top classes 
integrating data heterogeneous kernel function provides normalized cost savings best performing comparable best performing method classes comparable means values differ sum standard deviations 
considerably classes methods 
furthermore average cost savings classes higher method paired student test 
similarly intermediate integration scheme fails learn classify classes fewer classes methods 
learning data types idea 
classes combined methods lead decreased classification performance relative svm trained single type data 
case decrease occurs data type provides information indicating inferior data type contributes noise disrupts learning 
observation suggests feature selection algorithm effectively eliminates noisy features allow svm learn classes accurately 
experiments demonstrate naive feature selection algorithm take account heterogeneity data typically yield improved classification performance 
shows results fisher criterion score select features combined data 
treating feature independently simple feature selection algorithm take account possible correlations features algorithm advantages simplicity efficiency 
part classification performance declines features removed 
classes combining types data leads substantial decline performance feature selection yields functional catalog revised publication brown changing composition classes substantially 
example class subsumed class 
svm performance classes differs performance reported earlier 
class exp early intermediate late amino acid ribosomal proteins sugar mitochondrial organization acid pathway metabolism organization cytoplasm transport amino acid metabolism metabolism degradation respiration organization chromosome structure phosphate utilization organization plasma membrane phosphate pathway cellular import protein folding stabilization pheromone response generation nuclear organization drug organization organization cell wall mean cost savings number best performing number non learnable table summary learning performance results 
row table contains cost savings classification 
cost savings computed fold cross validation standard deviation calculated repetitions 
columns svms trained single type data gene expression phylogenetic profiles 
remaining columns svms trained early intermediate late integration data described text 
values bold face best performing comparable best performing methods 
missing value indicates cost savings significantly greater zero 
rows summary statistics giving average values method total number bold face missing values column 
class size fp fn amino acid ribosomal proteins sugar metabolism mitochondrial organization table error rates selected classes 
table lists error rates learnable classes 
row contains name size class average numbers false positives false negatives class svm intermediate integration 
mitochondrial organization amino acid ribosomal proteins sugar 
dna metabolism normalized cost savings number features effect feature selection learning performance 
series shows performance svm trained combined data set varying numbers features selected fisher criterion score 
examples representative results obtained classes tested 
express early fisher sel nn sel cv sel wins trials wins trials table feature selection optimal data set choice 
table lists number classes various data set selection algorithms choose best standard deviation best performing data set 
results computed separate trials values case best choice trials trials 
algorithms columns left right choosing expression phylogenetic combined data selecting dataset fisher criterion score leave error nearest neighbor algorithm fold cross validation svms 
small improvement 
cases feature selection yield level performance comparable obtained single data type 
sophisticated feature selection method take account correlations nonlinearities yields similar results 
method uses svm solution measure quality features removes features appear contribute called filter method see :10.1.1.102.7476
give details method results marginally superior fisher criterion results problem arises svm feature selection method achieve performance equal best performing single data type cases combined data set performs poorly 
worse cases hand picking best performing single data set trying various ways adding features leads consistent deterioration performance 
apparently data set performs quite poorly compared amino acid sugar useful information gleaned 
problem combined data performs poorly general dataset kernel combination performs better suggests useful determine outcome attempting train 
words able optimal choice options data type combination training 
task special case feature selection selecting best set features distinct sets traditional quality measures feature selection 
example fisher criterion score select best data set choosing largest mean fisher criterion scores 
attempt estimate svm performance directly generalization bounds vc bounds span bound cross validation :10.1.1.33.6025
cross validation useful vc bound span bound loose case 
measure leave oneout error nearest neighbor algorithm considered compromise fisher criterion score cross validation terms computation speed versus accuracy 
performed test idea selecting data sets gene expression data phylogenetic profiles concatenated data set 
classes counted times trials method chose best performing data set 
results shown table indicate choosing correct data set ahead time give improved results 
cross validation nearest neighbors superior fisher criterion score methods choose outright best data set choose close best hinted wins trials results 
crossvalidation gains improvement expense high computational cost nearest neighbors provides cheap compromise 
bearing mind nearest neighbors feature space kernels preliminary results suggest estimate performance kernel including intermediate integration method fairly low computational cost 
discussion quantity variety genomic data increases molecular biology shifts hypothesis driven model data driven 
previously single laboratory collect data test hypotheses regarding single system pathway new paradigm requires combining genome wide experimental results typically gathered shared multiple laboratories 
example constructing single element phylogenetic profile requires availability complete genomic sequences clearly generated single laboratory 
data driven model requires sophisticated computational techniques handle large heterogeneous data sets 
support vector machine learning algorithm technique 
svms scale successfully large training sets domains text categorization image recognition 
furthermore demonstrate svms learn heterogeneous data sets 
appropriate kernel function svm learns combination different types feature vectors 
cases resulting trained svm provides better gene functional classification performance svm trained data set 
data classifications heterogeneous kernel introduce intermediate integration method performs better techniques investigated 
hypothesize improved performance results kernel ability exploit prior knowledge correlations type data stronger correlations different types 
may useful consider feature selection algorithms determine type data appropriate class single method provides consistently best performance classes 
data type selection addressed simple feature selection metric fisher criterion score reliably computational expense cross validation 
preliminary experiments expect obtain near best performance classes 
results show supervised learning methodology proposed brown extended straightforward fashion classes 
shown majority classifications learnable gene expression data phylogenetic profiles 
believe failure learn classes failure svm method 
mean fisher score combined data set intermediate mean fisher scores subsets 
choose combined data set score falls highest alternative 
functional classes data simply informative 
expression data informative genes class regulated level transcription conditions tested 
similarly phylogenetic profiles limited resolution part relatively genomes available 
particular genomes derived phylogenetic profiles bacterial 
difficult generate useful phylogenetic profiles genes specific 
availability expression sequence kinds data increasing steadily expect tools developing continue improve power 
experiments primary utility phylogenetic profiles appears lie ability summarize sequence similarity information inheritance patterns genes various speciation events 
analysis classes easily learnable phylogenetic profiles shows classes share considerable sequence similarity members example various transporter classes data shown 
previously published report phylogenetic profiles yeast effect eliminated merging groups similar genes making links pairs genes requiring phylogenetic profile similarity extend entire functional class 
experiments removing combining data genes sequence similarity undesirable effect forcing combining corresponding expression data reason think genes sequence similarity generally regulated expression level 
obviously benefits considering sequence similarity gene classification task consider techniques summarizing sequence similarities fixed length vector 
experiments reported fixed kernel function svm learning order allow direct comparisons different feature combination feature selection algorithms 
selected particular kernel straightforward interpretation accounting primary features tertiary correlations data previous showed kernel performs gene expression data set 
experiment radial basis kernel gave performance previously optimize kernel respect phylogenetic profile data 
performance various algorithms doubt improved empirical kernel optimization 
reason suppose particular method benefit optimization 
experiments reported suggest avenues research 
obvious research direction involves including additional types data 
having shown types data fruitfully combined plan extend techniques described feature vectors derived example upstream promoter regions genes 
plan experiment fisher kernel method type data compared probabilistic model domain 
converting heterogeneous features probability gradients hope various types data directly comparable 
support vector machines part larger class algorithms known kernel methods gaining popularity 
kernel method algorithm employs kernel function implicitly operate higher dimensional space 
addition svm classifiers kernel methods developed regression discriminant analysis principal components analysis 
members promising class algorithms applied problems computational biology 
acknowledgments authors wish ilya muchnik vladimir vapnik helpful discussions 
funded award bioinformatics foundation national science foundation dbi 
altschul madden schaffer zhang zhang miller lipman :10.1.1.17.9507
gapped blast psi blast new generation protein database search programs 
nucleic acids research 
bishop 
neural networks pattern recognition 
oxford oxford uk 
brown grundy lin cristianini jr ares haussler 
knowledge analysis microarray gene expression data support vector machines 
proceedings national academy sciences united states america 
burges 
tutorial support vector machines pattern recognition 
data mining knowledge discovery 
chapelle vapnik :10.1.1.33.6025
model selection support vector machines 
sara solla todd leen klaus robert ller editors advances neural information processing systems 
mit press 
chu derisi eisen botstein brown 
transcriptional program yeast 
science 
cristianini shawe taylor 
support vector machines 
cambridge 
derisi iyer brown 
exploring metabolic genetic control gene expression genomic scale 
science 
hart pattern classification scene analysis 
wiley new york 
eisen spellman brown botstein 
cluster analysis display genome wide expression patterns 
proceedings national academy sciences united states america 
cristianini duffy schummer haussler 
support vector machine classification validation cancer tissue samples microarray expression data 
bioinformatics 
appear 
jaakkola haussler 
fisher kernel method detect remote protein homologies 
proceedings seventh international conference intelligent systems molecular biology pages menlo park ca 
aaai press 
jaakkola haussler 
exploiting generative models discriminative classifiers 
advances neural information processing systems san mateo ca 
morgan kauffmann 
marcotte pellegrini thompson yeates eisenberg 
combined algorithm genome wide prediction protein function 
nature 
mika tsch weston sch lkopf 
ller 
fisher discriminant analysis kernels 
proceedings ieee neural networks signal processing workshop 
pavlidis haussler grundy 
promoter region classification genes 
proceedings pacific symposium biocomputing 
appear 
pellegrini marcotte thompson eisenberg yeates 
assigning protein functions comparative genome analysis protein phylogenetic profiles 
proceedings national academy sciences united states america 
sch lkopf burges smola editors 
advances kernel methods support vector learning 
mit press cambridge ma 
sch lkopf smola 
ller 
nonlinear component analysis kernel eigenvalue problem 
neural computation 
sch lkopf smola 
ller 
kernel principal component analysis 
proceedings icann springer lecture notes computer science page 
spellman sherlock zhang iyer anders eisen brown botstein 
comprehensive identification cell cycle regulated genes yeast saccharomyces cerevisiae microarray hybridization 
mol biol cell 
tamayo slonim mesirov zhu lander golub 
interpreting patterns gene expression self organizing maps 
proceedings national academy sciences united states america 
vapnik 
theory 
adaptive learning systems signal processing communications control 
wiley new york 
weston mukherjee chapelle pontil poggio vapnik :10.1.1.102.7476
feature selection svms 
sara solla todd leen klaus robert ller editors advances neural information processing systems 
mit press 

