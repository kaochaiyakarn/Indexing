probabilistic independence networks hidden markov probability models padhraic smyth department information computer science university california irvine ca 
smyth ics uci edu david heckerman microsoft research building redmond wa microsoft com michael jordan department brain cognitive sciences mit cambridge ma jordan psyche mit edu may graphical techniques modeling dependencies random variables explored variety di erent areas including statistics statistical physics arti cial intelligence speech recognition image processing genetics 
formalisms manipulating models developed relatively independently research communities 
explore hidden markov models hmms related structures general framework probabilistic independence networks pins 
contains self contained review basic principles pins 
shown known forward backward viterbi algorithms hmms special cases general inference algorithms arbitrary pins 
furthermore existence inference estimation algorithms general graphical models provides set analysis tools hmm practitioners wish explore richer class hmm structures 
examples relatively complex models handle sensor fusion coarticulation speech recognition introduced treated graphical model framework illustrate advantages general approach 
multivariate statistical modeling applications hidden markov modeling speech recognition identi cation manipulation relevant conditional independence assumptions useful tool model building analysis 
jet propulsion laboratory california institute technology oak grove drive pasadena ca 
considerable amount exploring relationships conditional independence probability models structural properties related graphs 
particular separation properties graph directly related conditional independence properties set associated probability models 
key point analysis manipulation generalized hmms complex hmms standard rst order model facilitated exploiting relationship probability models graphs 
major advantages gained model description graphical model provides natural intuitive medium displaying dependencies exist random variables 
particular structure graphical model clari es conditional independencies associated probability models allowing model assessment revision 
computational ciency graphical model powerful basis specifying cient algorithms computing quantities interest probability model calculation probability observed data model 
inference algorithms speci ed automatically initial structure graph determined 
refer probability models graphical models 
consists structure parameters 
structure model consists speci cation set conditional independence relations probability model set missing edges graph graphical model 
parameters probability graphical models consist speci cation joint probability distribution factored form probability model de ned locally nodes graph graphical model 
inference problem calculation posterior probabilities variables interest observable data speci cation probabilistic model 
related task map identi cation determination state set unobserved variables observed variables probabilistic model 
learning estimation problem determining parameters possibly structure probabilistic model data 
reviews applicability utility graphical modeling hmms 
section introduces basic notation probability models associated graph structures 
section summarizes relevant results literature probabilistic independence networks pins short particular relationships exist separation graph conditional independence probability model 
section interprets standard rst order hmm terms pins 
section standard algorithm inference directed pin discussed applied standard hmm section 
result interest viterbi algorithms shown special cases inference algorithm 
section shows inference algorithms undirected pins essentially discussed directed pins 
section introduces complex hmm structures speech modeling analyzes graphical model framework 
section reviews known estimation results graphical models discusses potential implications practical problems estimation hmm structures section contains summary remarks 
notation background fx xng represent set discrete valued random variables 
purposes restrict attention discrete valued random variables results stated generalize directly continuous mixed sets random variables lauritzen wermuth whittaker 
lower case xi denote values variable xi notation taken mean sum possible values 
xi shorthand particular probability xi xi xi represents probability function xi table values xi assumed discrete full joint distribution function xn xn denotes particular value assignment note full joint distribution xn provides possible information needs calculate marginal conditional probability interest subsets disjoint sets random variables conditional independence relation bjc de ned independent bjc ajc bjc 
conditional independence symmetric 
note marginal independence conditioning general imply conditional independence conditional independence general imply marginal independence whittaker 
set random variables associate graph de ned 
denotes set vertices nodes graph mapping nodes graph random variables fx xng 
denotes set edges fe shorthand nodes xi xj edges form interest allowed graphs discussed 
edge may directed undirected 
convention directed edge directed node node case say parent child ancestor node node child ancestor subset nodes ancestral set contains ancestors 
descendant child child descendant nodes adjacent contains undirected directed edge 
undirected path sequence distinct nodes mg exists undirected directed edge pair nodes fl path 
directed path sequence distinct nodes mg exists directed edge pair nodes fl path 
graph singly connected exists undirected path nodes graph 
un directed cycle path nodes un directed path 
contains undirected edges graph undirected graph ug 
contains directed edges graph directed graph dg 
important classes graphs modeling probability distributions consider acyclic directed graphs directed graphs having directed cycles 
note passing exists theory graphical independence models involving directed undirected edges chain graphs whittaker discussed 
ug subset nodes separates subsets nodes path joining pair nodes contains node analogous somewhat complicated separation properties exist 
example upin structure captures particular set conditional independence relationships set variables fx 
example fx 
graph complete edges pairs nodes 
cycle undirected graph successive pairs nodes cycle adjacent 
undirected graph triangulated cycles graph contain nodes 
nd cycle length triangulated 
clique undirected graph subgraph complete 
clique tree tree cliques correspondence cliques nodes tree 
probabilistic independence networks brie review relation probability model xn probabilistic independence network structure 
results section largely summarized versions material pearl whittaker probabilistic independence network structure pin structure graphical statement set conditional independence relations set random variables absence edge implies independence relation xi xj 
pin structure particular way specifying independence relationships probability model 
say implies set probability models denoted pg pg 
reverse direction particular model embodies particular set conditional independence assumptions may may representable consistent graphical form 
derive conditional independence properties inference algorithms interest graphical models 
emphasized statistical ai literature reiterated context hidden markov models distinct advantages gained graphical formalism 
undirected probabilistic independence networks upin composed upin structure upin parameters 
upin structure speci es set conditional independence relations probability model form undirected graph 
upin parameters consist numerical speci cations particular probability model consistent upin structure 
terms literature described form include markov random elds geman geman markov networks pearl boltzmann machines hinton sejnowski log linear models bishop fienberg holland 
conditional independence semantics upin structures disjoint subsets nodes undirected graph ug undirected probabilistic independence network structure upin structure separates conditional independence relation bjs holds 
set conditional independence relations implied separation constitute global markov properties shows simple example upin structure variables 
separation upin structure implies conditional independence probability model constrains belong set probability models pg obey markov properties graph 
note complete ug trivially upin structure sense constraints 
perfect undirected map upin structure conditional independence relations represented separation probability models perfect undirected maps 
weaker condition upin structure minimal probability model removal edge implies independence relation model structure edge longer upin structure 
minimality equivalent perfection upin structures example exist probability models independencies represented complete upin structure 
example consider marginally independent conditionally dependent independent causal variables common ect case complete graph minimal upin structure fx zg perfect presence edge probability functions upin structures upin structure joint probability distribution expressed simple factorization xn ac xc vc set cliques xc represents assignment values variables particular clique ac xc non negative clique functions 
domain ac xc set possible assignments values variables clique range ac xc semi nite interval 
set clique functions associated upin structure provides numerical parameterization upin 
upin equivalent markov random eld 
markov random eld literature clique functions generally referred potential functions 
vc triangulated version upin structure 
related terminology context boltzmann machine hinton sejnowski energy function 
exponential negative energy con guration boltzmann factor 
scaling boltzmann factor sum boltzmann factors partition function yields factorization joint density boltzmann distribution product clique functions 
advantage de ning clique functions directly terms exponential energy function range clique functions allowed contain zero 
eq 
represent con gurations variables having zero probability 
model said decomposable minimal upin structure triangulated 
upin structure decomposable triangulated 
special case decomposable models converted junction tree tree cliques arranged cliques satisfy running intersection property node appears di erent cliques appears cliques undirected path cliques 
associated edge junction tree separator contains variables intersection cliques links 
junction tree representation factorize product clique marginals separator marginals pearl vc xc vs xs xc xs marginal joint distributions variables clique separator respectively vc vs set cliques separators junction tree 
product representation central results rest 
basis fact globally consistent probability calculations carried purely local manner 
mechanics local calculations described 
point su cient note complexity local inference boltzmann machine special case upin clique functions decomposed products factors associated pairs variables 
boltzmann machine augmented include higher order energy terms clique graph general markov random eld upin restricted positive probability distributions due exponential form clique functions 
algorithms scales sum sizes clique state spaces clique statespace equal product variable clique number states variable 
local clique updating probability calculations tractable brute force inference model decomposes relatively small cliques 
probability models interest may decomposable 
de ne decomposable cover triangulated necessarily minimal upin structure upin triangulated simply addition appropriate edges identify decomposable cover decomposable cover may minimal contain edges obscure certain independencies model example complete graph decomposable cover possible probability models cient inference goal nd decomposable cover contains extra edges possible original upin structure discuss speci algorithm nding decomposable covers arbitrary pin structures 
singly connected upin structures imply probability models pg decomposable 
note particular probability model upin process adding extra edges create decomposable cover change underlying probability model added edges convenience manipulating graphical representation underlying numerical probability speci cations remain unchanged 
important point decomposable covers running intersection property factored equation local clique updating possible non decomposable models conversion 
complexity local inference scales sum size clique state spaces decomposable cover 
summary upin structure converted junction tree permitting inference calculations carried purely locally cliques 
directed probabilistic independence networks dpin composed dpin structure dpin parameters 
dpin structure speci es set conditional independence relations probability model form directed graph 
dpin parameters consist numerical speci cations particular probability model consistent dpin structure 
referred literature di erent names including bayes network belief network recursive graphical model causal belief network probabilistic causal network 
conditional independence semantics dpin structures dpin structure adg correspondence elements set random variables fx xng 
convenient de ne moral graph undirected graph obtained placing undirected edges non adjacent parents node dropping directions remaining directed edges see example 
term moral coined denote nonadjacent parents 
motivation procedure clear discuss di erences section 
shall see dpin structure captures set independence relationships set fx 
example jx 
moral graph parents linked 
conversion dpin upin convenient way solve dpin inference problems transforming problem undirected graphical setting advantage general theory available undirected graphical models 
de ne dpin follows 
disjoint subsets nodes dpin structure separates conditional independence relation bjs holds 
de nition upin structure separation complex interpretation directed context separates directed graph separates moral undirected graph smallest ancestral set containing lauritzen 
shown de nition dpin structure equivalent intuitive statement values parents variable xi independent nodes directed graph descendants 
upin structure dpin structure implies certain conditional independence relations turn imply set probability models contains simple example dpin structure 
probability functions basic property dpin structure implies direct factorization joint probability distribution ny xi pa xi denotes value assignment parents xi 
probability model written factored form trivial manner conditioning rule 
note directed graph containing directed cycles necessarily yield factorization 
dpin structure encode fact depends 
example consider independent coin ips bell rings ips 
perfect upin structure encode dependence relationships 
upin structure encodes jfx jfx 
perfect dpin structure encode dependencies 
possible dpin structures consistent particular probability model potentially containing extra edges hide true conditional independence relations 
de ne minimal dpin structures manner exactly equivalent upin structures deletion edge minimal dpin structure implies independence relation hold similarly perfect dpin structure dpin structure conditional independence relations represented separation upin structures minimal imply perfect dpin structures 
example consider independence relations jfx jfx minimal dpin structure contains edge see 
di erences directed undirected graphical representations important point directed undirected graphs possess di erent conditional independence semantics 
common conditional independence relations perfect dpin structures perfect upin structures vice versa see examples 
dpin structure markov properties upin structure obtained dropping directions edges dpin structure 
answer dpin structure contains subgraphs node nonadjacent parents whittaker pearl 
general shown upin structure decomposable triangulated markov properties dpin structure practical level dpin structures frequently encode causal information formally represent belief xi xj causal sense temporally 
application causal modelling applied statistics arti cial intelligence 
popularity elds stems fact joint probability model speci ed directly equation speci cation con ditional probability tables functions spiegelhalter 
contrast speci ed terms clique functions equation may easy cf 
geman geman zhang 
examples ad hoc design clique functions image analysis 
frequently problems image analysis statistical physics associations thought correlational causal 
decomposable moral upin structure gm obtained dpin structure gd imply new independence relations gd triangulation additional edges may obscure conditional independence relations implicit numeric speci cation original probability model associated dpin structure gd furthermore gm may triangulated decomposable 
addition appropriate edges moral graph converted non unique triangulated graph decomposable cover gm manner probability model gd dpin structure construct decomposable cover mapping dpin structures upin structures rst discussed context cient inference algorithms lauritzen spiegelhalter 
advantage mapping derives fact analysis manipulation resulting upin considerably direct dealing original dpin 
furthermore shown inference algorithms fact special cases inference algorithms considerably cient shachter 
modeling hmms pins pins hmms hidden markov modeling problems baum petrie rabiner huang jack elliott moore interested set random variables fh hn hn ong hi discrete valued hidden variable index oi corresponding discrete valued observed variable index results directly extended continuous valued observables :10.1.1.131.2084
index denotes sequence example discrete time steps 
note oi considered univariate convenience extension multivariate case observables straightforward omitted simplicity illuminate conditional independence relationships hmm 
known simple rst order hmm obeys conditional independence relations hi fh hi oi oi oi fh hi oi refer rst order hidden markov probability model hmm notation hmm de ned hidden state model represented conjoined con guration underlying random variables model hn hn hn pin structure hmm corresponding junction tree 
hn hn hn hn dpin structures hmm dpin structure hmm probability model dpin structure dpin structure hmm probability model 
state memory depth notation clearer sections discuss speci examples 
construction pin hmm particularly simple 
undirected case assumption requires state hi connected hi set fh hi oi oi 
assumption requires oi connected hi 
resulting upin structure hmm shown 
graph singly connected implies decomposable probability model hmm cliques form fhi fhi hig 
section see joint probability function expressed product function junction tree leading junction tree de nition familiar viterbi inference algorithms 
directed case connectivity dpin structure 
natural choose directions edges hi hi going reverse direction chosen changing markov properties graph 
directions edges hi oi chosen going hi oi reverse direction 
reverse arrows imply oi marginally independent hi true hmm probability model 
proper direction edges implies correct relation oi conditionally independent hi hi 
dpin structure hmm possess subgraph non adjacent parents 
stated earlier implies implied independence properties dpin structure corresponding upin structure obtained dropping directions edges dpin structure result junction tree structure 
hmm probability model minimal directed undirected graphs possess markov properties imply conditional independence relations 
furthermore pin structures perfect maps directed undirected cases respectively 
inference map problems hmms context hmms common inference problem calculation likelihood observed evidence model denote observed values 
section assume dealing particular model structure parameters determined explicitly indicate conditioning model 
brute force method obtaining probability sum unobserved state variables full joint probability distribution hn hn hi denotes possible values hidden variable hi 
general computations scale mn number states hidden variable 
practice algorithm rabiner perform inference calculations lower complexity nm likelihood observed evidence obtained forward step algorithm calculation state posterior probabilities requires forward backward steps :10.1.1.131.2084
algorithm relies factorization joint probability function obtain locally recursive methods 
key points graphical modeling approach provides automatic method determining local cient factorizations arbitrary probabilistic model cient factorizations exist ci relations speci ed model 
map identi cation problem context hmms involves identifying hidden state sequence observed evidence 
just inference problem viterbi algorithm provides cient locally recursive method solving problem complexity nm inference problem graphical modeling approach provides automatic technique determining cient solutions map problem arbitrary models cient solution possible structure model 
inference map algorithms inference map algorithms quite similar upin case involves subtleties encountered discussion upin inference map algorithms deferred section 
inference algorithm developed jensen lauritzen referred jlo algorithm descendant inference algorithm rst described lauritzen spiegelhalter 
jlo algorithm applies discrete valued variables extensions jlo algorithm gaussian gaussian mixture distributions discussed lauritzen wermuth 
closely related algorithm jlo algorithm developed dawid solves map identi cation problem time complexity jlo inference algorithm 
show jlo dawid algorithms strict generalizations known viterbi algorithms hmm applied arbitrarily complex graph structures large family probabilistic models hmm handle missing values partial inference forth straightforward manner 
variations basic jlo dawid algorithms 
example pearl describes related versions algorithms early shown shachter known exact algorithms inference equivalent level jlo dawid algorithms 
su cient consider jlo dawid algorithms discussion subsume graphical inference algorithms 
jlo dawid algorithms operate step process 
construction step involve series sub steps original directed graph moralized triangulated junction tree formed junction tree initialized 

propagation step junction tree local message passing manner propagate ects observed evidence solve inference map problems 
rst step carried graph 
second propagation step carried time new inference graph requested 
construction step jlo algorithm dpin structures junction trees illustrate construction step jlo algorithm simple dpin structure gd discrete variables fx shown 
jlo algorithm rst constructs moral graph 
moral graph obtain decomposable cover 
algorithm operates simple greedy manner fact graph triangulated nodes eliminated node eliminated neighbors pairwise linked 
node eliminated neighbors de ne clique junction tree eventually constructed 
triangulate graph generate cliques junction tree eliminating nodes order adding links necessary 
node eliminated adding links choose node eliminated adding links yield clique smallest state space 
triangulation jlo algorithm constructs junction tree clique tree satisfying running intersection property 
junction tree construction fact 
de ne weight link cliques number variables intersection 
tree cliques satisfy running intersection property spanning tree maximal weight 
jlo algorithm constructs junction tree choosing successively link maximal weight creates cycle 
simple dpin structure 
corresponding undirected moral graph 
corresponding triangulated graph 
corresponding junction tree 
junction tree constructed cliques de ned dpin structure triangulation shown 
worst case complexity triangulation heuristic log maximal spanning tree portion algorithm 
construction step carried initial step convert original graph junction tree representation 
initializing potential functions junction tree step take numeric probability speci cations de ned directed graph gd equation convert information general form junction tree representation equation 
achieved noting variable xi contained clique junction tree 
assign xi just clique clique de ne potential function ac product xi xi assigned clique variables assigned clique 
de ne separator potentials equation initially 
section follows describe general jlo algorithm propagating messages junction tree achieve globally consistent probability calculations 
point su cient know schedule local message passing de ned converges globally consistent marginal representation potential clique separator marginal clique separator joint probability function 
local message passing go initial potential representation de ned marginal representation vc xc vs xs point junction tree initialized 
operation useful interest ability propagate information graph observed data initialized junction tree calculate posterior distributions variables interest 
point onwards implicitly assume junction tree initialized described potential functions local marginals 
local message propagation junction trees jlo algorithm general expressed vc ac xc vs bs xs ac bs non negative potential functions potential functions initial marginals described example 
note representation generalization representations equations 
fac vcg fbs scg representation 
function admit di erent representations di erent sets clique separator functions satisfy equation particular 
mentioned jlo algorithm carries globally consistent probability calculations local message passing junction tree probability information passed neighboring cliques clique separator potentials updated local information 
key point cliques separators updated fashion ensures times representation equation holds times 
eventually propagation converges marginal representation initial model observed evidence 
message passing proceeds follows 
de ne ow clique ci cj manner ci cj cliques adjacent junction tree 
sk separator cliques 
de ne sk aci summation state space variables ci sk cj acj sk sk sk sk update factor 
passage ow corresponds updating neighboring clique probability information contained originating clique 
ow induces new representation fa vcg fb scg 
schedule ows de ned cliques eventually updated relevant information junction tree reaches equilibrium state 
direct scheduling scheme phase operation node denoted root junction tree 
collection phase involves passing ows edges node scheduled incoming ow ows absorbed sequentially 
collection complete distribution phase involves passing ows root reverse direction edges 
ows edge tree non redundant schedule 
note directionality ows junction tree need directed edges original dpin structure 
jlo algorithm inference observed evidence particular case calculating ect observed evidence inference handled manner 
consider observe evidence form fxi xj ue fxi xj denotes set variables observed 
uh ue denote set hidden unobserved variables uh value assignment uh consider calculation 
de ne evidence function ge xi xi xi 
xi je 
obtain operations junction tree proceeds follows 
assign observed variable xi particular clique contains termed entering evidence clique 
denote set cliques evidence entered manner 
gc xc fi xi entered cg xi gc xc propagate ects modi cations tree collect distribute schedule described 
xh denote value assignment hidden unobserved variables clique schedule ows complete gets new representation local potential clique xc xh joint probability local unobserved clique variables observed evidence jensen similarly separator potential functions 
clique unobserved local clique variables gets probability observed evidence directly 
similarly normalizes potential function clique sum obtains conditional probability local unobserved clique variables evidence je 
complexity propagation step jlo algorithm general time complexity propagation junction tree pnc ci nc number cliques junction tree ci number states clique state space ci 
inference cient need construct junction trees small clique sizes 
problems nding optimally small junction trees nding junction tree smallest maximal clique np hard 
heuristic algorithm triangulation described earlier practice jensen 
inference map calculations hmm algorithm hmm special case jlo algorithm shows junction tree hmm 
apply jlo algorithm hmm junction tree structure obtain particular inference algorithm hmm 
mentioned earlier hmm inference problem consists set values observable variables fo inferring likelihood model 
described previous section problem solved exactly local propagation junction tree jlo inference algorithm 
appendix shown forward backward steps procedure hmm exactly recreated general jlo algorithm hmm viewed pin 
equivalence surprising algorithms solving exactly problem local recursive updating 
equivalence useful provides link known hmm inference algorithms general pin inference algorithms 
furthermore clearly pin framework provide direct avenue analyzing complex hidden markov probability models discuss hmms section 
evidence entered observable states assuming discrete states hidden variable computational complexity solving inference problem jlo algorithm nm complexity standard procedure 
note obvious structural equivalence pin structures hmm noted buntine frasconi bengio demonstration equivalence speci inference algorithms new far aware 
equivalence dawid propagation algorithm identifying map assignments viterbi algorithm consider wishes calculate maxx xk xk wishes identify set values unobserved variables achieve maximum number unobserved hidden variables 
calculation achieved local propagation algorithm junction tree modi cations standard jlo inference algorithm described 
algorithm due dawid pointed earlier general algorithm set related methods 
firstly ow marginalization separator replaced bs xs max cns ac xc originating clique ow 
de nition xs changed obvious manner 
secondly marginalization clique replaced maximization fc max changes shown propagation operations carried described earlier resulting representation kf equilibrium potential function clique xc max uh xh denotes value assignment hidden unobserved variables clique kf representation obtained locally identify values xh maximize full joint probability arg xc probabilistic expert systems literature procedure known generating probable explanation mpe observed evidence pearl 
hmm map problem consists set values observable variables fo inferring max hn hn set arguments maximum 
dawid algorithm applicable junction tree directly applied hmm junction tree 
appendix shown dawid algorithm applied hmm exactly equivalent standard viterbi algorithm 
equivalence surprising dawid method viterbi algorithm direct applications dynamic programming map problem 
important point dawid algorithm speci ed general case arbitrary pin structures directly applied complex hmms hmm discussed section 
inference map algorithms section described jlo algorithm local inference dpin procedure similar changes algorithm 
rst trivial observation moralization step necessary 
second di erence initialization junction tree trivial 
section described go speci cation conditional probabilities directed graph initial potential function representation cliques junction tree 
utilize undirected links model speci cation process requires new machinery perform initialization step 
particular wish compile model standard form product potentials cliques triangulated graph cf 
equation ac xc vc initialization step achieved jlo propagation procedure proceeds 
consider cycle shown 
suppose parameterize probability distribution graph specifying pairwise marginals pairs neighboring nodes 
wish convert local speci cation globally consistent joint probability distribution marginal representation 
algorithm known iterative proportional fitting ipf available perform conversion 
classically ipf proceeds follows bishop fienberg holland 
suppose simplicity random variables discrete gaussian version ipf available whittaker joint distribution represented table 
table initialized equal values cells 
marginal turn table rescaled multiplying cell ratio desired marginal corresponding marginal current table 
algorithm visits marginal turn iterating set marginals 
set marginals consistent single joint distribution algorithm guaranteed converge joint distribution 
joint available potentials equation obtained principle marginalization 
ipf solves initialization problem principle ine cient 
developed cient version ipf avoids need storing joint distribution table avoids need explicit marginalization joint obtain clique potentials 
version ipf represents evolving joint distribution directly terms junction tree potentials 
algorithm proceeds follows 
set subsets xi denote desired marginal subset joint distribution represented product junction tree potentials equation ac initialized arbitrary constant 
visit turn updating corresponding clique potential ac potential ac follows xc ac xc xi xi marginal xi obtained jlo algorithm current set clique potentials 
intelligent choices order visit marginals minimize amount propagation needed compute xi 
algorithm simply cient way organizing ipf calculations inherits guarantees convergence 
note algorithm requires triangulation step order form junction tree calculation xi 
worst case triangulation yield highly connected graph case algorithm reduces classical ipf 
sparse graphs maximum clique smaller entire graph algorithm substantially cient classical ipf 
triangulation algorithm need run pre processing step case jlo algorithm 
complex hmms speech modeling hidden markov models provided exceedingly useful framework modeling speech signals true simple hmm model underlying standard framework strong limitations model speech 
real speech generated set coupled dynamical systems lips tongue lungs air columns obeys particular dynamical laws 
coupled physical process modeled unstructured state transition matrix hmm 
rst order markov properties hmm suited modeling ubiquitous coarticulation ects occur speech particularly ects extend phonemes cf 
kent mini 
variety techniques developed basic weaknesses hmm model including mixture modeling emission probabilities triphone modeling discriminative training 
methods leave intact basic probabilistic structure hmm expressed pin structure 
section describe extensions hmm assume additional probabilistic structure assumed hmm 
pins provide key tool study complex models 
role pins twofold rst provide concise description probabilistic dependencies assumed particular model second provide general algorithm computing likelihoods 
second property particularly important existence jlo algorithm frees having derive particular recursive algorithms case case basis 
upin structure hmm model upin structure 
triangulation rst model consider viewed coupling hmm chains saul jordan 
model useful general sensor fusion problems example fusion audio signal video signal lipreading 
di erent sensory signals generally di erent bandwidths may useful couple separate markov models developed speci cally individual signals 
alternative force problem hmm framework oversampling slower signal requires additional parameters leads high variance estimator downsampling faster signal generally data yields biased estimator 
consider hmm structure shown 
model involves hmm backbones coupled undirected links state variables 
denote ith state ith output fast chain re denote ith state ith output slow chain 
spectively suppose fast chain sampled times slow chain 
connected equal 
value markov model coupled chain implies conditional independencies state variables fh fh conditional independencies output variables fo fh additional conditional independencies read upin structure see 
readily seen hmm graph triangulated hmm probability model decomposable 
graph readily triangulated form decomposable cover hmm probability model see section 
jlo algorithm provides cient algorithm calculating likelihoods graph 
seen show triangulation hmm graph 
triangulation adds nh links graph nh number hidden nodes graph creates junction tree clique cluster state variables underlying upin structure 
assuming values state variable chain obtain algorithm time complexity 
compared naive approach transforming hmm model cartesian product hmm model disadvantage requiring subsampling oversampling time complexity 
directed graph semantics play important role constructing interesting variations hidden markov model theme 
consider shows hmm model single output stream coupled pair underlying state sequences 
speech modeling application structure capture fact acoustic pattern multiple underlying articulatory causes 
example equivalent shifts formant frequencies caused lip rounding tongue raising phenomena generically refered trading relations speech psychophysics literature 
particular acoustic pattern observed causes dependent example evidence lips rounded act discount inferences tongue raised 
inferences propagate forward backward time couple chains 
formally induced dependencies accounted links added state sequences moralization hn hn gn gn dpin structure hmm single observable sequence coupled pair underlying state sequences moralization dpin structure 
upin structure hmm 
graph see 
gure shows underlying calculations model closely related earlier hmm model speci cation di erent cases 
saul jordan proposed second extension hmm model motivated desire provide ective model coarticulation see stolorz 
model shown uences modeled additional links output variables states hmm backbone 
approach performing calculations model treat kth order markov chain transform hmm model de ning higher order state variables 
graphical modeling approach exible possible example introduce links states outputs time steps apart introducing links intervening time intervals 
generally graphical modeling approach hmm model allows speci cation di erent interaction matrices di erent time scales awkward kth order markov chain formalism 
hmm graph triangulated time complexity jlo algorithm 
general hmm graph creates cliques size jlo algorithm runs time 
examples suggest graphical modeling framework provides useful framework exploring extensions hidden markov models 
examples clear graphical algorithms panacea 
complexity hmm prohibitive large generalization hmm hmm couplings chains intractable 
research focused approximate algorithms inference structures see saul jordan hmm ghahramani jordan williams hinton hmm 
authors developed approximation methodology mean eld theory statistical physics 
discussion mean eld algorithms scope worth noting graphical modeling framework plays useful role development approximations 
essentially mean eld approach involves creating simpli ed graph tractable algorithms available minimizing probabilistic distance tractable graph intractable graph 
jlo algorithm called subroutine tractable graph minimization process 
learning pins assumed parameters structure pin known certainty 
section drop assumption discuss methods learning parameters structure pin 
basic idea techniques discuss true joint probability distribution described pin structure parameters uncertain structure parameters 
unable observe true joint distribution directly able observe set patterns um random sample true distribution 
patterns independent identically distributed true distribution note typical hmm learning problem ui consist sequence observed data 
data learn structure parameters encode true distribution 
parameter estimation pins consider situation know pin structure true distribution certainty uncertain parameters keeping rest assume variables discrete 
furthermore purposes illustration assume adg 
xk pa xi denote kth value variable xi jth con guration variables pa xi respectively qi ri 
just discussed assume conditional probability jpa xi possibly uncertain convenience represent probability parameter ijk 
ij denote vector parameters ij denote vector parameters note pri ijk method learning parameters bayesian approach 
treat parameters random variables assign parameters prior distribution sjs update prior distribution data um bayes rule sjd sjs dj normalization constant depends patterns random sample equation simpli es sjd sjs prediction interest depends say posterior distribution compute expected prediction jd sjd associated assumption data random sample structure uncertain parameters set conditional independence assertions 
surprisingly assumptions represented directed pin includes possible observations parameters variables 
shows assumptions case fx structure directed edge 
pattern pattern pattern pattern bayesian network structure binary variable domain fx showing conditional independencies associated random sample assumption added assumption parameter independence 
gures assumed network structure generating database 
certain additional assumptions described example spiegelhalter lauritzen evaluation equation straightforward 
particular pattern ul complete variable observed ny qi ri ijk equal xi pa xi pa xi pattern cl zero 
combining equations obtain sjd sjs ny qi ri nijk ijk nijk number patterns xi pa xi pa xi nijk su cient statistics random sample assume parameter vectors ij qi mutually independent assumption call parameter independence get additional simpli cation sjd ny qi ri nijk ijk assumption parameter independence variable example illustrated 
complete data parameter independence parameter vector ij updated independently 
update particularly simple parameter vector conjugate distribution 
discrete variable discrete parents natural conjugate distribution dirichlet ri ijk ijk case equation sjd ny qi nijk ijk ijk conjugate distributions include normal wishart distribution parameters gaussian codebooks dirichlet distribution mixing coe cients codebooks degroot buntine heckerman geiger 
heckerman geiger describe simple method assessing priors 
priors learning parameters standard hmms gauvain lee 
parameter independence usually assumed general hmm structures 
example hmm model standard assumption fortunately parameter equalities easily handled framework see thiesson detailed discussion 
addition assumption patterns complete clearly inappropriate hmm structures general variables hidden observation 
data missing exact evaluation posterior sjd typically intractable turn approximations 
accurate slow approximations monte carlo sampling neal 
approximation accurate cient observation certain conditions quantity sjs dj converges multivariate gaussian distribution sample size increases see kass mackay ab 
accurate cient approximations observation gaussian distribution converges delta function centered maximum posteriori map eventually maximum likelihood ml value standard hmm model discussed discrete gaussian gaussian mixture codebooks ml map estimate known cient approximation rabiner :10.1.1.131.2084
map ml estimates traditional techniques gradient descent expectation maximization em dempster 
em algorithm applied ciently likelihood function su cient statistics xed dimension data set 
em algorithm nds local maximum initializing parameters random clustering algorithm repeating steps 
step compute expected su cient statistic parameters current values particular variables discrete parameter independence assumed hold priors dirichlet obtain mx pa xi jul important feature em algorithm applied pins assumptions term sum computed jlo algorithm 
jlo algorithm may parameters equal likelihoods variables gaussian gaussian mixture distributions lauritzen wermuth 
step expected su cient statistics actual su cient statistics set new values map ml values statistics 
variables discrete parameter independence assumed hold priors dirichlet ml ijk ri map ijk ijk ri ijk model selection averaging pins assume uncertain parameters pin uncertain true structure pin 
example may know true structure hmm structure may uncertain values solution problem bayesian model averaging 
approach view possible pin structure parameters model 
assign prior probabilities di erent models compute posterior probabilities data sjd djs dj js indicated equation compute djs averaging likelihood data parameters addition computing posterior probabilities models estimate parameters model computing distribution jd gaussian map ml approximation distribution 
prediction interest model separately equation compute weighted average predictions posterior probabilities models weights 
complication approach data missing example variables hidden exact computation integral equation usually intractable 
discussed previous section monte carlo gaussian approximations may 
simple form gaussian approximation bayesian information criterion bic described schwarz log djs log dj log ml estimate number patterns dimension typically number parameters rst term score rewards data second term punishes model complexity 
note score depend parameter prior applied easily 
examples applications bic context pins statistical models see raftery 
bic score additive inverse rissanen minimum description length mdl 
scores viewed approximations marginal likelihood hypothesis testing raftery cross validation fung crawford 
buntine caveat bic score derived assumption parameter prior positive domain 
press provides comprehensive review scores model selection model averaging context pins 
complication bayesian model averaging may possible models averaging intractable 
case select handful structures high relative posterior probabilities predictions limited set models 
approach called model selection 
trick nding model models high posterior probabilities 
detailed discussions search methods model selection pins madigan raftery heckerman 
spirtes meek 
case true model hmm structure may additional prior knowledge strongly constrains possible values exhaustive model search practical 
summary probabilistic independence networks provide useful framework analysis application multivariate probability models considerable structure model form conditional independence 
graphical modelling approach clari es independence semantics model yields cient computational algorithms probabilistic inference 
shown useful cast hmm structures graphical model framework 
particular known viterbi algorithms shown special cases general algorithms graphical modelling literature 
furthermore complex hmm structures traditional rst order model analyzed pro directly generally applicable graphical modeling techniques 
mij gratefully acknowledges discussions ste en lauritzen application ipf algorithm 
research described carried part jet propulsion laboratory california institute technology contract national aeronautics space administration 
appendix forward backward algorithm hmm special case jlo algorithm consider junction tree hmm shown 
nal clique chain containing hn hn root clique 
non redundant schedule consists rst recursively passing ows oi hi hi hi hi hi appropriate sequence collect phase distributing ows reverse direction root clique 
interested calculating likelihood model distribute phase necessary simply marginalize local variables root clique obtain 
comment notation subscripts potential functions update factors indicate variables deriving potential update factor fo indicates potential updated information information variables 
hi hi local message passing hmm junction tree collect phase left right schedule 
ovals indicate cliques boxes indicate separators arrows indicate ows 
assume junction tree initialized potential function clique separator local marginal 
observed evidence individual piece evidence entered clique oi hi clique marginal oi hi oi hi entering evidence equation 
consider portion junction tree particular ow oi hi hi hi 
de nition potential separator hi updated oi hi oi hi oi hi update factor separator owing clique hi hi oi hi hi hi update factor absorbed hi hi follows jhi oi hi hi hi hi oi hi hi hi jhi consider ow clique hi hi clique hi hi 
foi denote set consecutive observable variables fo denote set observed values variables assume potential separator hi updated hi hi earlier ows schedule 
update factor separator hi hi hi hi gets absorbed clique hi hi produce hi hi oi hi hi hi hi hi jhi hi hi jhi hi calculate new potential separator hi hi hi hi hi hi ow clique hi hi jhi hi jhi proceeding recursively manner hi hi hi nally obtains root clique hn hn hn hn get likelihood evidence hn hn hn hn note equation directly corresponds recursive equation equation rabiner variables forward phase algorithm standard hmm inference algorithm 
particular left right schedule updated potential functions separators hidden cliques hi functions exactly variables 
applied hmm jlo algorithm produces exactly local recursive calculations forward phase algorithm 
show equivalence backward phase algorithm jlo inference algorithm 
left clique chain root clique de ne schedule ows go right left 
shows local portion clique tree associated ows 
consider potential clique hi hi updated earlier ows right 
de nition hi hi hi hi potential separator hi hi hi hi calculated hi hi hi hi hi hi hi jhi jhi virtue various conditional independence relations hmm hi hi jhi hi hi hi hi jhi hi jhi hi hi hi hi hi hi hi local message passing hmm junction tree collect phase right left schedule 
ovals indicate cliques boxes indicate separators arrows indicate ows 
de ning update factor separator yields hi hi hi hi jhi hi hi hi jhi hi set recursive equations corresponds exactly recursive equation equation rabiner variables backward phase algorithm 
fact update factors separators exactly variables 
shown jlo inference algorithm algorithm special case hmm probability model 
appendix viterbi algorithm hmm special case dawid algorithm inference problem nal clique chain containing hn hn root clique schedule rst left right collection phase root clique followed right left distribution phase root clique 
assumed junction tree initialized potential functions local marginals observable evidence entered cliques manner described inference algorithm 
refer sequence ow absorption operations identical inference algorithm exception marginalization operations replaced maximization 
potential separator oi hi hi hi initially updated update factor separator foi hi max hi oi hi oi oi hi hi hi absorption clique hi hi gets jhi foi hi hi hi hi jhi consider ow clique hi hi hi hi 
hi fhi denote set consecutive observable variables fh denote observed values variables assume potential separator hi updated earlier hi max hi ows schedule 
update factor separator hi hi maxh hi hi gets absorbed clique hi hi produce hi hi foi hi hi hi hi hi jhi maxh hi hi obtain new potential separator hi hi ow clique hi hi hi hi hi max hi jhi max hi hi jhi max fp hi max hi result expects updated potential clique 
express separator potential hi recursively equation hi jhi max hi fp hi recursive equation variables viterbi algorithm equation rabiner separator potentials dawid algorithm left right schedule exactly viterbi method solving map problem hmm 
proceeding recursively manner nally obtains root clique hn hn max hn hn hn get likelihood evidence state hidden variables max hn hn hn hn max identi cation values hidden variables maximize evidence likelihood carried standard manner viterbi method keeping pointer clique ow forward direction back previous clique backtracking list pointers root clique collection phase complete 
alternative approach distribute phase dawid algorithm ect distribution ows completed local clique calculate maximum value evidence likelihood hidden variables values hidden variables maximum local particular clique 
baum petrie 
statistical inference probabilistic functions state markov chains 
ann 
math 
stat 
nite bishop fienberg holland 
discrete multivariate analysis theory practice 
mit press cambridge ma 
buntine 
operations learning graphical models 
journal arti cial intelligence research 

buntine press 
guide literature learning probabilistic networks data 
ieee transactions knowledge data engineering 
dawid 
applications general propagation algorithm probabilistic expert systems 
statistics computing 

degroot 
optimal statistical decisions 
mcgraw hill new york 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society series 
elliott moore 
hidden markov models estimation control 
new york springer verlag 
frasconi bengio 
em approach grammatical inference input output hmms 
proceedings th iapr intl 
conf 
pattern recognition ieee computer society press 

fung crawford 
system induction probabilistic models 
eighth national conference arti cial intelligence boston ma aaai 
gauvain lee 
maximum posteriori estimation multivariate gaussian mixture observations markov chains 
ieee trans 
sig 
audio proc 

geman geman 
stochastic relaxation gibbs distributions bayesian restoration images 
ieee trans 
patt 
anal 
mach 
intell 

ghahramani jordan 
factorial hidden markov models 
touretzky mozer hasselmo eds advances neural information processing systems mit press cambridge ma 
huang jack 
hidden markov models speech recognition 
edinburgh edinburgh university press 
heckerman geiger 
likelihoods priors bayesian networks 
msr tr microsoft redmond wa 
heckerman geiger chickering 
learning bayesian networks combination knowledge statistical data 
machine learning 

hinton sejnowski 
learning relearning boltzmann machines 
parallel distributed processing explorations microstructure cognition rumelhart mcclelland pdp research group editors 
cambridge ma mit press ch 


spatial point processes markov random international statistical review 

elds 
jensen lauritzen olesen 
bayesian updating recursive graphical models local computations 
computational statistical quarterly 


ective implementation iterative proportional tting procedure 
computational statistics data analysis 

kass tierney kadane 
asymptotics bayesian computation 
bayesian statistics bernardo degroot lindley smith editors oxford uk oxford university press 
kent mini 
coarticulation speech production models 
journal phonetics 

lauritzen spiegelhalter 
local computations probabilities graphical structures application expert systems discussion 
roy 
statist 
soc 
ser 

lauritzen wermuth 
graphical models associations variables qualitative quantitative 
annals statistics 

lauritzen dawid larsen 
properties directed markov elds 
networks 

independence 
explaining phonetic variation sketch theory 
speech production speech modeling marchal eds 
kluwer dordrecht 

bayesian belief networks tool stochastic parsing 
speech communication 

mackay 
bayesian interpolation 
neural computation 
mackay 
practical bayesian framework backpropagation networks 
neural computation 
madigan raftery 
model selection accounting model uncertainty graphical models occam window 
am 
stat 
assoc 
neal 
probabilistic inference markov chain monte carlo methods 
crg tr department computer science university toronto 
pearl 
probabilistic reasoning intelligent systems networks plausible inference san mateo ca morgan kaufmann publishers 
pearl geiger verma 
logic uence diagrams 
uence diagrams belief nets decision analysis 
oliver smith 
eds 
chichester john wiley sons 

matthies jordan 
trading relations tongue body raising lip rounding production vowel pilot motor equivalence study 
journal acoustical society america 


hidden markov models guided tour 
proceedings ieee international conference acoustics speech signal processing 
new york ieee press 
vol 
rabiner 
tutorial hidden markov models selected applications speech recognition 
proceedings ieee 
raftery 
bayesian model selection social research 
marsden sociological methodology 
cambridge ma 
rissanen 
stochastic complexity discussion 
journal royal statistical society series 
saul jordan 
boltzmann chains hidden markov models 
tesauro touretzky leen eds advances neural information processing systems mit press cambridge ma 
saul jordan 
exploiting tractable substructures intractable networks 
touretzky mozer hasselmo eds advances neural information processing systems mit press cambridge ma 
shachter anderson szolovits 
global conditioning inference belief networks 
proceedings uncertainty ai conference san francisco ca morgan kaufmann 
schwarz 
estimating dimension model 
annals statistics 

spiegelhalter dawid hutchinson cowell 
probabilistic expert systems graphical modelling case study drug safety 
phil 
trans 
soc 
lond 

spiegelhalter lauritzen 
sequential updating conditional probabilities directed graphical structures 
networks 
spirtes meek 
learning bayesian networks discrete variables data 
proceedings international conference knowledge discovery data mining menlo park ca aaai press 
stolorz 
recursive approaches statistical physics lattice proteins 
hunter ed 
proc 
th hawaii intl 
conf 
system sciences 
tao 
generalization discrete hidden markov model viterbi algorithm 
pattern recognition 
thiesson 
score information recursive exponential models incomplete data 
institute electronic systems aalborg university aalborg denmark technical report october 
suetens marchal 
continuous voxel classi cation stochastic relaxation theory application imaging angiography 
image vision computing 

whittaker 
graphical models applied multivariate statistics chichester uk john wiley sons 
williams hinton 
mean eld networks learn discriminate temporally distorted strings 
proc 
connectionist models summer school san mateo ca morgan kaufmann 
zhang 
markov random eld model approach image segmentation 
ieee trans 
patt 
anal 
mach 
int 


