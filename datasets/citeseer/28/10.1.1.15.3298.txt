analysis temporal di erence learning approximation john tsitsiklis benjamin van roy laboratory information decision systems massachusetts institute cambridge ma mail mit edu mit edu research supported nsf dmi daal 
discuss temporal di erence learning algorithm applied approximating cost go function nite horizon discounted 
algorithm updates function approximator line endless trajectory irreducible aperiodic markov chain nite state space 
proof convergence probability limit convergence abound onthe resulting approximation error 
furthermore analysis new line provides new intuition dynamics temporal di erence learning 
addition proving new stronger positive results previously available identify signi cance line updating hazards associated nonlinear function approximators 
larger bound deteriorates decreases 
worst bound kd 
bound strongly suggests higher values produce accurate approximations consistent examples constructed bertsekas 
ofthe error bound raises question sense set values 
experimental results sutton singh sutton sutton suggest setting values lead signi cant gains rate :10.1.1.51.4764
acceleration may critical computation time data event trajectories generated system limited 
understanding uences rate 
furthermore tune algorithm progresses possibly initially starting opposite advocated 
interesting directions research 
