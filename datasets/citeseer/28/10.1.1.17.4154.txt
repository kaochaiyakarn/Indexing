ecient boosting algorithm combining preferences raj iyer jr submitted department electrical engineering computer science partial ful llment requirements degree master science massachusetts institute technology september massachusetts institute technology 
rights reserved 
author 
department electrical engineering computer science august certi ed 
david karger associate professor thesis supervisor accepted 
arthur smith chairman departmental committee graduate students ecient boosting algorithm combining preferences raj iyer jr submitted department electrical engineering computer science august partial ful llment requirements degree master science problem combining preferences arises applications combining results di erent search engines 
nearest neighbor methods 
regression methods 
general methods 
related 
boosting machine learning pattern recognition :10.1.1.30.9676
origins boosting 
pac model 
schapire algorithm 
boost majority algorithm 
tired risks movies heard alice seeks suggestions movies enjoy 
calls friends list top favorite movies 
alice combine recommendations determine movie rent 
problem may 
collaborative ltering problem internet websites movie critic devoted providing service alice friends :10.1.1.30.9676
asked recommendations system uses preferences previous users produce ranked list movie suggestions 
question system combine preferences single list recommendations 
example ranking problem set instances collection functions rank instances combine functions produce ordering instances approximates true order 
dissertation provide formal model ranking problem algorithm solve 
combine ranked lists 
raises question represent ranked lists 
search engines rank web pages query computing numeric similarity score web page ordering pages score 
hope combine rankings averaging scores pages 
commonly taken approach explore section :10.1.1.51.1783:10.1.1.51.1783:10.1.1.105.6964
approach encounter number diculties 
search engines altavista reveal computed similarity scores 
second di erent search engines may di erent numeric ranges scores rendering meaningless straightforward combination 
third signi cant scores numeric range di erent search engines may di erent numbers range express identical measures quality may numbers express di erent measures quality 
application making predictions ranked list probable outcomes 
example automatic speech recognizers receive input recorded clip spoken words return list textual transcriptions ranked likelihood 
combine lists various recognizers goal increasing collective accuracy 
despite wide range applications combine rankings problem received relatively little attention machine learning community 
methods devised combining rankings usually nearest neighbor methods regression methods optimization techniques gradient descent :10.1.1.51.1783:10.1.1.105.6964:10.1.1.136.4322
cases rankings viewed real valued scores problem combining di erent rankings reduces numerical search set parameters minimize disparity combined scores feedback user 
discussed approaches guarantee combined system match user preference view scores means express preferences 
dissertation introduce analyze ecient algorithm called rankboost combining multiple rankings 
main di erence majority previous absolute numeric scores combine ranking functions 
cases rankings viewed real valued scores problem combining di erent rankings reduces numerical search set parameters minimize disparity combined scores feedback user 
discussed approaches guarantee combined system match user preference view scores means express preferences 
dissertation introduce analyze ecient algorithm called rankboost combining multiple rankings 
main di erence majority previous absolute numeric scores combine ranking functions 
algorithm freund schapire adaboost algorithm successor developed schapire singer :10.1.1.32.8918:10.1.1.32.8918
similar boosting algorithms rankboost works combining rankings instances 
may weakly correlated target ranking attempting approximate 
show combine weak rankings single highly accurate ranking prove bound quality nal ranking terms quality weak rankings 
movie task simple rankings partition movies equivalence sets preferred preferred 
general methods de ne correlation measure predicted order true order search best prediction numerical methods gradient descent 
brie summarize examples methods 
discussed previous chapter approaches guarantee combined system match user preference view scores means express preferences 
main di erence majority previous absolute numeric scores combine ranking functions 
nearest neighbor methods researchers reported results nearest neighbor methods rank items :10.1.1.136.4322
shardanand maes built collaborative ltering system utilized database user preferences recommend music albums new user 
system received ranked list new user preferences various music albums set user averaged predictions 
authors experimented user similarity measures mean squared di erence pearson correlation coecient 
measure ignores scores ranking proportional measure disagreement 
measure ignores scores ranking proportional measure disagreement 
best measures nding supports choice disagreement measure 
approaches theirs similar spirit 
regression methods regression methods combine user preferences 
hill earliest collaborative ltering movie recommendation task described chapter alice preferences movies seen examine database users preferences recommend movies alice hasn seen :10.1.1.105.6964
task user treated ranking function 
create combination users hill rst subset users preferences correlated target user preferences 
combined scores predictions regression equation details see section 
approach di ers regression methods depend actual numeric scores ranking scores may idea explained chapter 
general methods regression seeks maximize certain utility function euclidean distance setting parameters linear algebra 
general approach choose di erent utility function maximize numerical search 
approach ranking problem views set scores assigned ranking function vector high dimensional vector space de nes utility function takes input target vector set vectors approximate target free parameters weights form linear combination set vectors 
problem combining di erent rankings reduces numerical search set parameters maximize utility function 
:10.1.1.51.1783
bartell cottrell belew developed method automatically combining multiple ranking functions goal ranking set documents user query 
ranking function example keyword search orders documents returns predicted relevance query 
combined classi er output system weighted combination individual ranking functions set documents ordered score assigned combined classi er 
task system estimate optimal weight function combined classi er produce ranking documents 
etzioni proposed formal model task 
assumed search engine cost associated querying goal answer query minimizing cost meta search 
assume query sole relevant document rendering ordering issues unimportant 
approach super cially related 
chapter boosting described chapter solution ranking problem machine learning :10.1.1.30.9676
chapter basic ideas machine learning pattern recognition section 
history development particular machine learning method called boosting 
rst examine theoretical origins boosting led discovery adaboost algorithm direct predecessor boosting algorithm section 
survey experiments demonstrated ability adaboost produce highly accurate prediction rules section 
boosting works running learning algorithm training set multiple times time focusing learner attention di erent training examples 
boosting process nished rules output learner combined single prediction rule provably accurate training set 
combined rule usually highly accurate test set veri ed theoretically experimentally 
section outline history development rst boosting algorithms popular adaboost algorithm 
pac model leslie valiant introduced computational model learning known probably approximately correct pac model learning :10.1.1.105.6964
pac model di ers slightly probabilistic model pattern recognition described section explicitly considers computational costs learning thorough presentation pac model see instance kearns vazirani 
pac learning problem speci ed instance space concept boolean function de ned instance space represents information learned 
email classi cation task described section instance space consists email messages concept service request 
goal pac learning algorithm output boolean prediction rule called hypothesis approximates concept 
boost majority algorithm schapire boosting algorithm certainly theoretical breakthrough algorithm analysis quite complicated 
algorithm runs polynomial time impractical repeated recursive calls 
addition output nal hypothesis complex due recursive construction 
simpler ecient algorithm constructed yoav freund year schapire original 
freund algorithm called boost majority algorithm works constructing di erent distributions instance space :10.1.1.37.5438
constructed distributions weak learner order focus learner attention dicult regions unknown distribution 
weak learner outputs weak hypothesis distribution receives intuitively hypotheses perform di erent portions instance space 
boosting algorithm combines hypotheses nal hypothesis single majority vote nal hypothesis provably low expected error instance space 
freund elegantly presents main idea boosting algorithm abstracting hypothesis boosting problem game calls majority vote game 
algorithm outputs classi ers space suciently small vc dimension zero error training set produce classi er arbitrarily small generalization error training suciently large number training examples 
useful proving theoretical results bound accurate practice 
typical learning scenarios involve xed set training data build classi er 
situation vapnik theorem agrees intuition output classi er suciently simple accurate training data generalization error small 
proved vc dimension majority vote classi er generated boost majority algorithm td number rounds boosting vc dimension space hypotheses generated weak learner :10.1.1.32.8918:10.1.1.32.8918
large training sample boost majority able produce arbitrarily accurate combined hypothesis 
summary summary freund boost majority algorithm uses weak learner create nal hypothesis highly accurate training set 
similar spirit schapire algorithm boost majority achieves presenting weak learner di erent distributions training set forces weak learner output hypotheses accurate di erent parts training set 
boost majority major desired generalization error number training examples required polynomial required pac model section 
voting algorithm chooses prediction weighted majority vote experts 
correct label instance revealed voting algorithm expert may su er loss 
view process voting algorithm rst receiving instance receiving vector losses expert 
examining loss expert instance voting algorithm may increase decrease weight expert expert predicted correct label 
hedge learning algorithm freund schapire working particular voting algorithm called hedge led discovery new boosting algorithm :10.1.1.32.8918:10.1.1.32.8918
hedge algorithm receives input set experts learning rate parameter :10.1.1.51.1783
initializes weight vector hp uniform probability distribution experts 
initial weight vector initialized prior distribution information available 
learning trial algorithm receives instance corresponding loss vector loss expert instance :10.1.1.51.1783
correct label instance revealed voting algorithm expert may su er loss 
view process voting algorithm rst receiving instance receiving vector losses expert 
examining loss expert instance voting algorithm may increase decrease weight expert expert predicted correct label 
hedge learning algorithm freund schapire working particular voting algorithm called hedge led discovery new boosting algorithm :10.1.1.32.8918:10.1.1.32.8918
hedge algorithm receives input set experts learning rate parameter :10.1.1.51.1783
initializes weight vector hp uniform probability distribution experts 
initial weight vector initialized prior distribution information available 
learning trial algorithm receives instance corresponding loss vector loss expert instance :10.1.1.51.1783
loss hedge su ers expected loss prediction current distribution experts 
hedge learning algorithm freund schapire working particular voting algorithm called hedge led discovery new boosting algorithm :10.1.1.32.8918:10.1.1.32.8918
hedge algorithm receives input set experts learning rate parameter :10.1.1.51.1783
initializes weight vector hp uniform probability distribution experts 
initial weight vector initialized prior distribution information available 
learning trial algorithm receives instance corresponding loss vector loss expert instance :10.1.1.51.1783
loss hedge su ers expected loss prediction current distribution experts 
hedge updates distribution rule ect decreasing weight expert prediction incorrect probability distribution 
freund schapire proved cumulative loss hedge algorithm trials best expert meaning expert loss min speci cally proved cumulative loss hedge bounded min ln constants turn best achievable line learning algorithm 
application boosting adaboost hedge algorithm bounds performance freund schapire derived new boosting algorithm 
hedge updates distribution rule ect decreasing weight expert prediction incorrect probability distribution 
freund schapire proved cumulative loss hedge algorithm trials best expert meaning expert loss min speci cally proved cumulative loss hedge bounded min ln constants turn best achievable line learning algorithm 
application boosting adaboost hedge algorithm bounds performance freund schapire derived new boosting algorithm 
natural application hedge boosting problem consider xed set weak hypotheses experts training examples trials 
incorrect prediction weight hypothesis decreased multiplication factor :10.1.1.51.1783
problem boosting algorithm order output highly accurate prediction rule reasonable amount time weight update factor depend worst case edge exactly dependence trying avoid 
freund schapire fact dual application experts correspond training examples trials correspond weak hypotheses 
weight update rule similarly reversed weight example increased current weak hypothesis hedge algorithm analysis direct generalizations weighted majority algorithm littlestone warmuth 
algorithm adaboost training examples initialize train weak learner distribution get weak hypothesis error 
denotes empirical probability training sample implies generalization error arbitrarily small training large number examples 
suggests xed training sample number rounds boosting increases 
non binary classi cation freund schapire generalized adaboost algorithm handle classi cation problems classes 
speci cally algorithms multiclass problems label space nite set 
algorithm regression problems :10.1.1.51.1783
schapire error correcting codes produce boosting algorithm multiclass problems see dietterich bakiri :10.1.1.32.8860:10.1.1.105.6964
generalization binary adaboost schapire singer proposed multiclass boosting algorithm algorithm problems instance may correct label 
summary adaboost algorithm breakthrough 
boosting practical experiments 
suggests xed training sample number rounds boosting increases 
non binary classi cation freund schapire generalized adaboost algorithm handle classi cation problems classes 
speci cally algorithms multiclass problems label space nite set 
algorithm regression problems :10.1.1.51.1783
schapire error correcting codes produce boosting algorithm multiclass problems see dietterich bakiri :10.1.1.32.8860:10.1.1.105.6964
generalization binary adaboost schapire singer proposed multiclass boosting algorithm algorithm problems instance may correct label 
summary adaboost algorithm breakthrough 
boosting practical experiments 
section discuss empirical evaluation adaboost 
conclude discussion questions raised experiments adaboost led theoretical study algorithm 
decision trees experiments adaboost algorithm usually apply classi cation problems 
recall classi cation problem speci ed space instances space labels instance assigned label unknown labelling function assume label space nite 
input base learning algorithm set training examples assumed correct label instance 
goal algorithm output classi er closely approximates unknown function rst experiments adaboost improve performance algorithms generate decision trees de ned follows :10.1.1.133.1040:10.1.1.49.2457
suppose instance represented vector attributes ha take discrete continuous values 
example attribute vector represents human physical characteristics weight hair color eye color skin 
values attributes particular person kg black dark brown tani 
decision tree hierarchical classi er classi es instances values attributes 
algorithms cart successors greedy strategy generate partition label assignment low error training set 
algorithms run risk tting meaning creating specialized decision tree highly accurate training set performs poorly test set 
resist growing tree algorithms prune tree nodes thought specialized 
boosting decision trees describe experiments adaboost improve performance decision tree classi ers 
rst experiment base learner simple algorithm generating decision nal hypothesis output adaboost weighted combination stumps :10.1.1.133.1040:10.1.1.105.6964
experiment adaboost compared bagging method generating combining multiple classi ers order separate ects combining classi ers particular merits boosting approach 
adaboost compared standard decision tree learning algorithm 
second experiment base learner boosting compared bagging :10.1.1.133.1040:10.1.1.49.2457
report results experiments brie describe bagging quinlan presentation :10.1.1.49.2457
boosting decision trees describe experiments adaboost improve performance decision tree classi ers 
rst experiment base learner simple algorithm generating decision nal hypothesis output adaboost weighted combination stumps :10.1.1.133.1040:10.1.1.105.6964
experiment adaboost compared bagging method generating combining multiple classi ers order separate ects combining classi ers particular merits boosting approach 
adaboost compared standard decision tree learning algorithm 
second experiment base learner boosting compared bagging :10.1.1.133.1040:10.1.1.49.2457
report results experiments brie describe bagging quinlan presentation :10.1.1.49.2457
bagging invented breiman bagging bootstrap aggregating method generating combining multiple classi ers repeatedly sampling training data 
base learner training set examples bagging runs rounds outputs combined classi er 
round training set size sampled replacement original examples 
rst experiment base learner simple algorithm generating decision nal hypothesis output adaboost weighted combination stumps :10.1.1.133.1040:10.1.1.105.6964
experiment adaboost compared bagging method generating combining multiple classi ers order separate ects combining classi ers particular merits boosting approach 
adaboost compared standard decision tree learning algorithm 
second experiment base learner boosting compared bagging :10.1.1.133.1040:10.1.1.49.2457
report results experiments brie describe bagging quinlan presentation :10.1.1.49.2457
bagging invented breiman bagging bootstrap aggregating method generating combining multiple classi ers repeatedly sampling training data 
base learner training set examples bagging runs rounds outputs combined classi er 
round training set size sampled replacement original examples 
training set size original data examples may appear may appear 
major di erences bagging boosting 
bagging training set round uniform distribution examples 
contrast boosting round di erent distribution modi ed performance classi er generated previous round 
second bagging uses simple majority vote classi ers boosting uses weighted majority vote weight classi er depends error relative distribution generated 
boosting decision stumps base learner freund schapire simple greedy algorithm nding decision lowest error relative distribution training examples :10.1.1.133.1040:10.1.1.105.6964
ran experiments benchmark datasets repository university california irvine 
set number boosting bagging rounds 
boosting signi cantly uniformly better bagging 
boosting test error rate worse bagging error rate dataset improvement bagging boosting 
algorithm beat benchmarks tied lost 
improvement performance single decision compared boosting 
boosting algorithm produces decision classi er thought weak learner 
experiment showed boosting able dramatically improve performance bagging greater degree 
freund schapire quinlan investigated abilities boosting bagging improve considerably stronger learning algorithm :10.1.1.133.1040:10.1.1.105.6964:10.1.1.49.2457
base learner boosting bagging evenly matched boosting slight advantage 
freund schapire experiments revealed average boosting improved error rate bagging 
bagging superior datasets tied boosting superior datasets degraded performance dataset 
boosting beat bagging benchmarks bagging beat boosting amount benchmark 
freund schapire experiments revealed average boosting improved error rate bagging 
bagging superior datasets tied boosting superior datasets degraded performance dataset 
boosting beat bagging benchmarks bagging beat boosting amount benchmark 
remaining benchmarks di erence performance 
quinlan results bagging boosting compelling :10.1.1.49.2457
ran boosting bagging rounds datasets uci repository half freund schapire 
bagging reduced classi cation error average superior datasets degraded performance worst increase 
boosting reduced error improved performance datasets degraded performance worst increase :10.1.1.30.9676
compared boosting superior bagging datasets 
remaining benchmarks di erence performance 
quinlan results bagging boosting compelling :10.1.1.49.2457
ran boosting bagging rounds datasets uci repository half freund schapire 
bagging reduced classi cation error average superior datasets degraded performance worst increase 
boosting reduced error improved performance datasets degraded performance worst increase :10.1.1.30.9676
compared boosting superior bagging datasets 
quinlan concluded boosting outperforms bagging signi cant amount bagging prone degrade base learner 
drucker cortes adaboost able improve performance 
adaboost build ensembles decision trees optical character recognition ocr tasks 
drucker cortes adaboost able improve performance 
adaboost build ensembles decision trees optical character recognition ocr tasks 
experiments boosted decision trees performed better single tree reducing error factor 
boosting past zero quinlan experimented try determine cause boosting occasional degradation performance 
original adaboost freund schapire attributed kind degradation tting :10.1.1.32.8918:10.1.1.32.8918
discussed earlier goal boosting construct combined classi er consisting weak classi ers 
order produce best classi er naturally expect run adaboost training error combined classi er reaches zero 
rounds situation increase complexity combined classi er improve performance training data 
test hypothesis degradation performance due tting quinlan repeated experiments stopped boosting training error reached zero 
dietterich boosting performs worse bagging noisy data 
jackson craven employed adaboost sparse perceptrons weak learning algorithm 
testing datasets boosted sparse perceptrons outperformed general multi layered perceptrons 
main feature results boosted classi ers simple easy humans interpret classi ers produced multi layered perceptrons complex incomprehensible 
maclin opitz compared boosting bagging neural networks decision trees :10.1.1.105.6964
performed experiments datasets uci repository boosting methods better able improve performance base learners 
observed boosting sensitive noise 
experiments surveyed include dietterich bakiri dietterich schapire bengio :10.1.1.32.8860:10.1.1.105.6964
summary seen experiments adaboost algorithm revealed able base learning algorithm produce highly accurate prediction rule 
main feature results boosted classi ers simple easy humans interpret classi ers produced multi layered perceptrons complex incomprehensible 
maclin opitz compared boosting bagging neural networks decision trees :10.1.1.105.6964
performed experiments datasets uci repository boosting methods better able improve performance base learners 
observed boosting sensitive noise 
experiments surveyed include dietterich bakiri dietterich schapire bengio :10.1.1.32.8860:10.1.1.105.6964
summary seen experiments adaboost algorithm revealed able base learning algorithm produce highly accurate prediction rule 
adaboost usually improves base learner quite dramatically minimal extra computation costs 
lines leslie valiant adaboost knuth prize lecture way get give extremely simple algorithm minutes magic 
referring quinlan results 
check tried simple update schemes distribution examples 
update form number times example misclassi ed previously constructed classi ers tested waveform data 
best arc 
higher values tested improvement possible 
breiman arc performed comparably adaboost producing classi ers small test error veri ed researchers :10.1.1.105.6964:10.1.1.105.6964
breiman observation boosting strength ability reduce variance weak learner replicated researchers fact opposite boosting tends reduce bias variance 
particular freund schapire discussed breiman journal issue 
schapire gave detailed discussion merits weaknesses application bias variance analysis boosting 
ered alternative explanation ectiveness boosting section 
principle states order achieve test error classi er simple possible 
simple mean classi er chosen restricted space classi ers 
space nite cardinality measure complexity nite vc dimension closely related number parameters de ne classi er 
typically theory practice di erence training error test error increases complexity classi er increases 
freund schapire predicted typical behavior adaboost rst developed algorithm :10.1.1.32.8918:10.1.1.32.8918
showed combined hypothesis generated training sample size weak learner hypothesis space high probability generalization error bounded prd pr td pr 
denotes empirical probability sample eq 
section 
bound suggests boosting run rounds large 
showed combined hypothesis generated training sample size weak learner hypothesis space high probability generalization error bounded prd pr td pr 
denotes empirical probability sample eq 
section 
bound suggests boosting run rounds large 
error cumulative distribution rounds margin error curves margin distribution graphs boosting letter dataset reported schapire :10.1.1.30.9676
left training test error curves lower upper curves respectively combined classi er function number rounds boosting 
horizontal lines indicate test error rate rst base classi er test error nal combined classi er 
right cumulative distribution margins training examples iterations included short dashed long dashed hidden solid curves respectively 
reprinted schapire 
give reasonable probability estimate 
friedman hastie tibshirani connections adaboost logistic regression additive models 
particular derive new boosting algorithms connections 
new algorithms exhibit comparable superior performance adaboost demonstrated experiments decision trees 
boosting presence noise quinlan rst notice adaboost increased error base learner decision trees case :10.1.1.49.2457
aaai talk mentioned believed poor performance algorithm due noise meaning training examples 
dietterich con rmed observation nding bagging outperformed adaboost noisy training data 
maclin opitz similar ndings neural networks introduced noise data adaboost produced classi ers test error worse arc worse bagging :10.1.1.105.6964
bauer kohavi reported detailed behavior boosting presence noise 
new algorithms exhibit comparable superior performance adaboost demonstrated experiments decision trees 
boosting presence noise quinlan rst notice adaboost increased error base learner decision trees case :10.1.1.49.2457
aaai talk mentioned believed poor performance algorithm due noise meaning training examples 
dietterich con rmed observation nding bagging outperformed adaboost noisy training data 
maclin opitz similar ndings neural networks introduced noise data adaboost produced classi ers test error worse arc worse bagging :10.1.1.105.6964
bauer kohavi reported detailed behavior boosting presence noise 
decision trees adaboost performed worse single best tree datasets de nitely contained training data 
listed problem rst list open problems main problem boosting lack robustness noise 
attempted drop instances high weight experiments show successful approach 
learning settings learning system performs unseen data depends factors number instances covered training representational complexity ranking produced learner 
various methods evaluate ranking 
discussed chapter 
boosting algorithm described section attempts minimize possible measure called ranking loss 
boosting algorithm ranking task section describe approach ranking problem machine learning method called boosting particular freund schapire adaboost algorithm successor developed schapire singer :10.1.1.32.8918:10.1.1.32.8918
boosting method producing highly accurate prediction rules combining weak rules may moderately accurate 
current setting seek learning algorithm produce function induced ordering approximate relative orderings encoded feedback function 
formalize goal 
maxf negative entries carry additional information set zero 
case minimize analytically follows abbreviate simple calculus veri ed minimized setting ln yields weak hypotheses range restricted attempt nd tends minimize eq 
set eq 

third method 
consider weak hypotheses intermediate generality range :10.1.1.51.1783
hypotheses third method setting approximation speci cally convexity function veri ed real 
approximate right hand side eq 
minimized ln plugging eq 
yields approximately minimize weak hypotheses range attempt maximize jrj de ned eq :10.1.1.51.1783
consider weak hypotheses intermediate generality range :10.1.1.51.1783
hypotheses third method setting approximation speci cally convexity function veri ed real 
approximate right hand side eq 
minimized ln plugging eq 
yields approximately minimize weak hypotheses range attempt maximize jrj de ned eq :10.1.1.51.1783
set eq 

consider case methods setting assign weak hypothesis weight 
example eq 
test sample hx hy test error pq pq pq pr pq similarly training sample hx xm hy training empirical error mn goal show high probability di erence small meaning performance combined hypothesis training sample representative performance random sample 
vc analysis bound di erence training error test error combined hypothesis output rankboost standard vc dimension analysis techniques 
show high probability taken choice training set di erence small happens matter combined hypothesis chosen algorithm training error combined hypothesis accurately estimate generalization error 
way saying probability choice training set small exists di er small amount 
words show exists small pr mn choice determined course proof :10.1.1.30.9676
approach separate probabilities choice choice bound classi cation generalization error theorems :10.1.1.30.9676
order theorems need convert binary function 
de ne function indicates pair meaning 
function care performance pairs say incurs penalty ordering instances quantity inside absolute value rewritten mn mn prove exist pr pr shown high probability implies sum likewise quantity inside expectation expectation prove standard classi cation results follows symmetric argument :10.1.1.30.9676
vc analysis bound di erence training error test error combined hypothesis output rankboost standard vc dimension analysis techniques 
show high probability taken choice training set di erence small happens matter combined hypothesis chosen algorithm training error combined hypothesis accurately estimate generalization error 
way saying probability choice training set small exists di er small amount 
words show exists small pr mn choice determined course proof :10.1.1.30.9676
approach separate probabilities choice choice bound classi cation generalization error theorems :10.1.1.30.9676
order theorems need convert binary function 
de ne function indicates pair meaning 
function care performance pairs say incurs penalty ordering instances quantity inside absolute value rewritten mn mn prove exist pr pr shown high probability implies sum likewise quantity inside expectation expectation prove standard classi cation results follows symmetric argument :10.1.1.30.9676
consider xed means single argument binary valued function 
words show exists small pr mn choice determined course proof :10.1.1.30.9676
approach separate probabilities choice choice bound classi cation generalization error theorems :10.1.1.30.9676
order theorems need convert binary function 
de ne function indicates pair meaning 
function care performance pairs say incurs penalty ordering instances quantity inside absolute value rewritten mn mn prove exist pr pr shown high probability implies sum likewise quantity inside expectation expectation prove standard classi cation results follows symmetric argument :10.1.1.30.9676
consider xed means single argument binary valued function 
set functions xed choice comes theorem vapnik applies gives choice depends size training set error probability complexity measured vc dimension details see vapnik devroye gy 
speci cally pr ln ln parameters remains calculate vc dimension 
note classi cation result bound probability correspond peculiar classi cation problem trying di erentiate picking natural interpretation 
speci cally pr ln ln parameters remains calculate vc dimension 
note classi cation result bound probability correspond peculiar classi cation problem trying di erentiate picking natural interpretation 
determine form functions xed sign constant xed 
functions consist possible thresholds linear combinations weak hypotheses 
freund schapire theorem bounds vc dimension class terms vc dimension weak hypothesis class applying result vc dimension log base natural logarithm :10.1.1.32.8918:10.1.1.32.8918
nal step repeating reasoning keeping xed putting probability choice training sample satisfy ln ln ln ln log vc dimension class weak hypotheses 
chapter experimental evaluation rankboost chapter report experiments rankboost ranking problems 
rst simpli ed web meta search task goal build search strategy nding homepages machine learning researchers universities 
second task collaborative ltering problem making movie recommendations new user preferences users 
details 
meta search rst experiments learning combine results web searches 
problem exhibits facets require general approach 
instance approaches combine similarity scores applicable similarity scores web search engines di erent semantics unavailable 
description task data set order test rankboost task data cohen schapire singer :10.1.1.30.9676:10.1.1.30.9676
goal simulate problem building domain speci search engine 
test cases picked fairly narrow classes queries retrieving homepages researchers ml retrieving homepages universities univ 
chose test cases partly feedback readily available web 
obtained list machine learning researchers identi ed name institution homepages similar list universities identi ed name geographical location yahoo 
evaluate pair returned url obtain predicted ranking url 
evaluation divided data training test sets fold crossvalidation 
created partitions data base queries training testing 
course learning algorithms access test data training 
see cohen schapire singer list search templates :10.1.1.30.9676:10.1.1.30.9676
train error rounds boosting weaklearn weaklearn cum weaklearn weaklearn cum test error rounds boosting weaklearn weaklearn cum weaklearn weaklearn cum performance weak learners weaklearn cum ml dataset :10.1.1.30.9676
left train error right test error experimental parameters evaluation search templates access set documents url returned top documents search template interpreted ranking url returned documents 
set parameter def default value weak hypotheses see section 
implementation rankboost de nition ranking loss modi ed original section eq 
evaluation divided data training test sets fold crossvalidation 
created partitions data base queries training testing 
course learning algorithms access test data training 
see cohen schapire singer list search templates :10.1.1.30.9676:10.1.1.30.9676
train error rounds boosting weaklearn weaklearn cum weaklearn weaklearn cum test error rounds boosting weaklearn weaklearn cum weaklearn weaklearn cum performance weak learners weaklearn cum ml dataset :10.1.1.30.9676
left train error right test error experimental parameters evaluation search templates access set documents url returned top documents search template interpreted ranking url returned documents 
set parameter def default value weak hypotheses see section 
implementation rankboost de nition ranking loss modi ed original section eq 
output hypothesis ranked equal pair instances feedback ranked unequal assigned hypothesis error 
second method sets minimum third method sets approximately minimize third method implemented easily runs faster 
implemented methods called weaklearn weaklearn determine extra time required second method times third method ml dataset reduction test error rate 
implemented weak learners restricted hypotheses positive cumulative weights order test hypotheses helpful harmful reducing test error discussed section 
called weaklearn cum weaklearn cum 
measure accuracy weak learner dataset round boosting top top top top top top avg ml domain rank rankboost best top best top best top university domain rankboost best single query table comparison combined hypothesis individual search templates :10.1.1.30.9676
plotted train test error combined hypothesis generated far 
ran weak learner rounds boosting partitions data averaged results 
displays plots train error left test error right rst rounds boosting ml dataset 
slopes curves change remaining rounds 
weaklearn cum preliminary experiments revealed weak learner achieved lower test error rate weaklearn tting 
experiments ran rankboost rounds 
algorithms comparison compared performance rankboost data set algorithms regression algorithm nearest neighbor algorithm 
regression 
regression algorithm similar ones hill :10.1.1.105.6964
algorithm employs assumption scores assigned movie target user alice described linear combination scores assigned movie movie viewers 
formally row vector components scores alice assigned movies discarding unranked movies 
matrix containing scores viewers subset movies alice ranked 
viewers ranked movies ranked alice need decide default rank movies 
results shown 
coordinate point average density features single bin example average density features density range 
relative performance algorithms 
rankboost winner best able improve performance additional information provided features 
feature density increased rankboost nn regression prot rankboost nn regression disagreements rankboost nn regression coverage rankboost nn regression performance algorithms di erent feature densities :10.1.1.30.9676
prot dashed line shows expected performance random permutation movies 
disagreement expected performance random ordering 
coverage expected performance 
nn performance ap disagreement coverage improved signi cantly simply number features increased 
partitioned users bins density way previous experiment 
ran algorithms target users density half movies ranked user training half testing 
xed randomly chosen set features 
repeated experiment random splits data averaged results appear 
noticeable ect increasing feedback density degrades perfor rankboost nn regression random prot rankboost nn regression random disagreements rankboost nn regression random coverage rankboost nn regression random performance algorithms including randomly ordering movies di erent feedback densities :10.1.1.30.9676
mance algorithms ap coverage performance nn regression prot rankboost able improve prot 
comparative performance algorithms previous experiments exceptions 
ap random ordering higher algorithms target user ranked movies 
advantage random ordering disappeared target user ranked movies suggests algorithms need train ranking movies order beat random guessing 
chapter summary problem combining preferences arises applications including combining results di erent search engines collaborative ltering tasks making movie recommendations 
important property tasks relevant information combined represents relative preferences absolute ratings 
formal framework ecient algorithm problem combining preferences experiments indicate works practice 
comparison cohen schapire singer 
model ranking problem similar proposed cohen schapire singer :10.1.1.30.9676:10.1.1.30.9676
consider ranking features form totally ordered set incomparable elements indicates ranking 
combine ranking features step approach 
rst step construct function indicates preference instance :10.1.1.51.1783
set fr input learning algorithm framework output nal hypothesis pref weighted combination :10.1.1.51.1783
comparison cohen schapire singer 
model ranking problem similar proposed cohen schapire singer :10.1.1.30.9676:10.1.1.30.9676
consider ranking features form totally ordered set incomparable elements indicates ranking 
combine ranking features step approach 
rst step construct function indicates preference instance :10.1.1.51.1783
set fr input learning algorithm framework output nal hypothesis pref weighted combination :10.1.1.51.1783
second step faced problem constructing total order minimum disagreement pref disagreement similar ranking loss de ned eq 

prove problem call min disagree np complete give approximation algorithm 
model ranking problem similar proposed cohen schapire singer :10.1.1.30.9676:10.1.1.30.9676
consider ranking features form totally ordered set incomparable elements indicates ranking 
combine ranking features step approach 
rst step construct function indicates preference instance :10.1.1.51.1783
set fr input learning algorithm framework output nal hypothesis pref weighted combination :10.1.1.51.1783
second step faced problem constructing total order minimum disagreement pref disagreement similar ranking loss de ned eq 

prove problem call min disagree np complete give approximation algorithm 
framework ranking features form allows combine output features change complexity min disagree 
pointed ranking problems bipartite feedback viewed binary classi cation problems 
problems interesting compare rankboost adaboost combined weak minimizing classi cation error 
adaboost outputs real valued score instance thresholded produce classi cation see section 
compare rankboost ordering adaboost ordering instances classi cation weight see minimizing ranking loss superior minimizing classi cation error 
anecdotal evidence case thorough empirical evaluation needed :10.1.1.105.6964
rankboost algorithm rst method setting general requires numerical search 
schapire singer suggest general iterative meth ods newton raphson 
methods proof convergence numerically unstable nd special purpose iterative method proof convergence 
course practical method need converge quickly 

www com 
metacrawler 
www metacrawler com 
movie critic :10.1.1.30.9676
www com 

www com 
brian bartell cottrell richard belew 
empirical comparison voting classi cation algorithms bagging boosting variants 
machine learning appear 
eric baum david haussler 
size net gives valid generalization 
neural computation :10.1.1.30.9676
bernhard boser isabelle guyon vladimir vapnik :10.1.1.105.6964
training algorithm optimal margin classi ers 
proceedings fifth annual acm workshop computational learning theory pages :10.1.1.30.9676
leo breiman 
machine learning appear 
eric baum david haussler 
size net gives valid generalization 
neural computation :10.1.1.30.9676
bernhard boser isabelle guyon vladimir vapnik :10.1.1.105.6964
training algorithm optimal margin classi ers 
proceedings fifth annual acm workshop computational learning theory pages :10.1.1.30.9676
leo breiman 
bagging predictors 
size net gives valid generalization 
neural computation :10.1.1.30.9676
bernhard boser isabelle guyon vladimir vapnik :10.1.1.105.6964
training algorithm optimal margin classi ers 
proceedings fifth annual acm workshop computational learning theory pages :10.1.1.30.9676
leo breiman 
bagging predictors 
machine learning 
leo breiman 
wadsworth brooks 
rich caruana baluja tom mitchell 
sort rankprop learning medical risk evaluation 
advances neural information processing systems pages 
william cohen robert schapire yoram singer :10.1.1.30.9676:10.1.1.30.9676
learning order things 
journal arti cial intelligence research 
cortes vladimir vapnik 
support vector networks 
springer 
thomas dietterich 
experimental comparison methods constructing ensembles decision trees bagging boosting randomization 
unpublished manuscript 
thomas dietterich bakiri :10.1.1.105.6964
solving multiclass learning problems error correcting output codes 
journal arti cial intelligence research january 
harris drucker cortes 
boosting decision trees 
proceedings twelfth annual conference computational learning theory pages 
yoav freund robert schapire 
discussion arcing classi ers leo breiman 
annals statistics 
yoav freund robert schapire :10.1.1.133.1040:10.1.1.105.6964
experiments new boosting algorithm 
machine learning proceedings thirteenth international conference pages :10.1.1.30.9676
yoav freund robert schapire 
game theory line prediction boosting 
discussion arcing classi ers leo breiman 
annals statistics 
yoav freund robert schapire :10.1.1.133.1040:10.1.1.105.6964
experiments new boosting algorithm 
machine learning proceedings thirteenth international conference pages :10.1.1.30.9676
yoav freund robert schapire 
game theory line prediction boosting 
proceedings ninth annual conference computational learning theory pages 
yoav freund robert schapire :10.1.1.32.8918:10.1.1.32.8918
machine learning proceedings thirteenth international conference pages :10.1.1.30.9676
yoav freund robert schapire 
game theory line prediction boosting 
proceedings ninth annual conference computational learning theory pages 
yoav freund robert schapire :10.1.1.32.8918:10.1.1.32.8918
decision theoretic generalization line learning application boosting 
journal computer system sciences august 
yoav freund robert schapire 
adaptive game playing multiplicative weights 
boosting limit maximizing margin learned ensembles 
proceedings fifteenth national conference arti cial intelligence 
amit 
multiclass learning boosting codes 
proceedings twelfth annual conference computational learning theory pages :10.1.1.30.9676
guttman 
statistics 
statistician 
david haussler 
statistics 
statistician 
david haussler 
decision theoretic generalizations pac model neural net learning applications 
information computation :10.1.1.30.9676
hill larry stead mark rosenstein george furnas :10.1.1.105.6964
recommending evaluating choices virtual community 
human factors computing systems chi conference proceedings pages 
iba langley 
statistician 
david haussler 
decision theoretic generalizations pac model neural net learning applications 
information computation :10.1.1.30.9676
hill larry stead mark rosenstein george furnas :10.1.1.105.6964
recommending evaluating choices virtual community 
human factors computing systems chi conference proceedings pages 
iba langley 
polynomial learnability probabilistic concepts respect kullback divergence 
machine learning 
nick littlestone manfred warmuth 
weighted majority algorithm 
th annual symposium foundations computer science pages october 
richard maclin david opitz :10.1.1.105.6964
empirical evaluation bagging boosting 
proceedings fourteenth national conference arti cial intelligence pages 
thomas dietterich 
pruning adaptive boosting 
technical report systems engineering australian national university 
merz murphy 
uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
quinlan :10.1.1.49.2457
bagging boosting 
proceedings thirteenth national conference arti cial intelligence pages 
ross quinlan 
programs machine learning 
addison wesley 
gerard salton michael mcgill 
modern information retrieval 
mcgraw hill 
robert schapire :10.1.1.105.6964
personal communication 
robert schapire 
strength weak learnability 
machine learning 
machine learning 
robert schapire 
design analysis ecient learning algorithms 
mit press 
robert schapire :10.1.1.32.8860
output codes boost multiclass learning problems 
machine learning proceedings fourteenth international conference pages 
robert schapire 
theoretical views boosting 
human factors computing systems chi conference proceedings 
robert tibshirani 
bias variance prediction error classi cation rules 
technical report university toronto november 
valiant :10.1.1.105.6964
theory learnable 
communications acm november 
valiant 
learning computational knuth prize lecture 
nature statistical learning theory 
springer 
vovk 
game prediction expert advice 
journal computer system sciences april :10.1.1.30.9676

