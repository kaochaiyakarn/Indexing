topic driven crawlers machine learning issues menczer srinivasan university iowa iowa city ia menczer srinivasan edu may topic driven crawlers increasingly seen way address scalability limitations universal search engines distributing crawling process users queries client computers 
context available crawlers guide navigation links goal efficiently locating highly relevant target pages 
developed framework fairly evaluate topic driven crawling algorithms number performance metrics 
framework employed evaluate different algorithms proven highly competitive proposed literature previous research 
particular focus tradeoff exploration exploitation cues available crawler adaptive crawlers machine learning techniques guide search 
find best performance achieved novel combination bias introduce evolutionary crawler equals performance best nonadaptive crawler 
early search engines ranked pages principally lexical similarity query 
key strategy devise best weighting algorithm represent web pages queries vector space closeness space correlated semantic relevance 
structure hypertext links recognized powerful new source evidence web semantics 
machine learning techniques employed link analysis page linkage pages content estimate relevance 
best known example link analysis pagerank algorithm successfully employed google search engine :10.1.1.109.4049
machine learning techniques extract meaning link topology identifying hub authority pages eigenvalue analysis graph theoretical approaches 
purely link methods effective cases link analysis combined lexical pre post filtering 
research addresses half problem 
matter sophisticated ranking algorithm build results pages indexed search engine page retrieved indexed 
rate change varies domains 
example takes days com domain versus months gov domain extent change 
despite valiant attempts search engines index web expected subspace indexing continue grow 
solution offered search engines capacity answer query user recognized limited 
comes surprise development topic driven crawler algorithms received significant attention years :10.1.1.22.3686:10.1.1.1.9569:10.1.1.12.8656:10.1.1.43.1111:10.1.1.43.7796
topic driven crawlers known focused crawlers respond particular information needs expressed topical queries interest profiles 
needs individual user query time online crawlers community shared interests topical search engines portals 
topic driven crawlers support crawling process scalable approach 
www google com additional benefit crawlers driven rich context topics queries user profiles interpret pages select links visited 
topic driven crawlers support crawling process scalable approach 
www google com additional benefit crawlers driven rich context topics queries user profiles interpret pages select links visited 
starting early breadth depth crawlers defining beginnings research crawlers see variety algorithms 
shark search aggressive variant de bra fish search 
crawlers decisions rely heavily link criteria :10.1.1.107.9226:10.1.1.22.3686
example backlinks context graphs estimate likelihood page leading relevant page source page relevant 
exploit lexical conceptual knowledge 
example chakrabarti hierarchical topic classifier select links crawling :10.1.1.43.1111
emphasize contextual knowledge topic including received relevance feedback :10.1.1.12.8656:10.1.1.43.7796
shark search aggressive variant de bra fish search 
crawlers decisions rely heavily link criteria :10.1.1.107.9226:10.1.1.22.3686
example backlinks context graphs estimate likelihood page leading relevant page source page relevant 
exploit lexical conceptual knowledge 
example chakrabarti hierarchical topic classifier select links crawling :10.1.1.43.1111
emphasize contextual knowledge topic including received relevance feedback :10.1.1.12.8656:10.1.1.43.7796
example aggarwal learn statistical model features appropriate topic crawling 
previous authors menczer belew show organized portions web effective crawling strategies learned evolved agents neural networks evolutionary algorithms 
highly creative phase regarding design topic driven crawlers accompanied research evaluation crawlers complex problem 
crawlers decisions rely heavily link criteria :10.1.1.107.9226:10.1.1.22.3686
example backlinks context graphs estimate likelihood page leading relevant page source page relevant 
exploit lexical conceptual knowledge 
example chakrabarti hierarchical topic classifier select links crawling :10.1.1.43.1111
emphasize contextual knowledge topic including received relevance feedback :10.1.1.12.8656:10.1.1.43.7796
example aggarwal learn statistical model features appropriate topic crawling 
previous authors menczer belew show organized portions web effective crawling strategies learned evolved agents neural networks evolutionary algorithms 
highly creative phase regarding design topic driven crawlers accompanied research evaluation crawlers complex problem 
example challenge specific web crawlers magnitude retrieval results limits availability user relevance judgments 
example aggarwal learn statistical model features appropriate topic crawling 
previous authors menczer belew show organized portions web effective crawling strategies learned evolved agents neural networks evolutionary algorithms 
highly creative phase regarding design topic driven crawlers accompanied research evaluation crawlers complex problem 
example challenge specific web crawlers magnitude retrieval results limits availability user relevance judgments 
previous research started explore alternative approaches assessing quality web pages summarizing crawler performance :10.1.1.1.9569
companion expand methodology describing detail framework developed fair evaluation topic driven crawlers 
performance analysis quality space resources 
formalize class crawling tasks increasing difficulty propose number evaluation metrics various available sources relevance evidence analyze time complexity scalability number crawling algorithms proposed literature study relationship performance various crawlers various topic characteristics 
goal examine algorithmic aspects topic driven crawlers 
crawl task involves seeking relevant pages starting crawl links links away known relevant pages 
exceptions crawl task rarely considered literature fact difficult problem task 
problem realistic quite commonly users unable specify known relevant urls 
effort somewhat related task authors start crawl general points amazon com 
cho start crawls general point stanford web site topics primitive roles research target set identified :10.1.1.22.3686
believe important understand crawlers task contexts corresponding single value varying degrees difficulty serve different valid search modes 
analyze performance number crawlers purposes limit analyses difficult task 
note large average fanout web pages problem bit looking needle haystack 
illustrate point observe breadth crawler chance visit target seed assuming average fanout links 
theoretically generate topics target sets frequent queries search engines user assessments 
approach expensive obtain large number topics target sets cumbersome keep dataset date time dynamic nature web 
fortunately data available web directories updated human editors 
past research yahoo 
due popularity current open directory dmoz commercial bias ii open resource iii maintained large diverse number volunteer editors :10.1.1.1.9569:10.1.1.1.9569
collected topics running randomized breadth crawls starting main categories open directory site 
crawlers identify dmoz leaves pages children category nodes 
leaves external links derive topics 
topic represented types information derived corresponding leaf page 
third concatenate text descriptions anchor text target urls written dmoz human editors form topic description 
difference topic keywords topic description give crawlers models short query topics detailed representations topics gauge relevance crawled pages post hoc analysis 
table shows sample topics 
experiments described topics 
performance metrics previous research explored alternative methodologies evaluating crawlers :10.1.1.1.9569
include methods assessing page relevance linear classifiers similarity computations 
explored different methods summarizing crawler performance plotting mean similarity retrieved pages topic descriptions time computing average fraction retrieved pages assessed relevant classifiers crawl 
previous experience companion discuss detail number measures selected minimal set needed provide rounded assessment crawler performance 
particular propose assess recall precision static dynamic perspective target lexical link criteria page relevance 
edu domain third may similarity source page guide link selection 
crawlers serving goal may adopt different crawl strategies 
crawler descriptions section describe evaluate different crawling algorithms implemented evaluation framework breadth best pagerank shark search infospiders 
selec tion intended broad representation crawler algorithms reported literature 
algorithms infospiders derives prior current research :10.1.1.43.7796
section generalize algorithms best shark search level 
breadth breadth crawler simplest strategy crawling 
algorithm explored early webcrawler research :10.1.1.22.3686
uses frontier fifo queue crawling links order encountered 
selec tion intended broad representation crawler algorithms reported literature 
algorithms infospiders derives prior current research :10.1.1.43.7796
section generalize algorithms best shark search level 
breadth breadth crawler simplest strategy crawling 
algorithm explored early webcrawler research :10.1.1.22.3686
uses frontier fifo queue crawling links order encountered 
note frontier full crawler add link crawled page 
breadth algorithm shown 
breadth baseline crawler knowledge topic expect performance provide lower bound sophisticated algorithms 
specifically done link selection process computing lexical similarity topic keywords source page link 
similarity page topic estimate relevance pages pointed url best estimate selected crawling 
cosine similarity crawler links minimum similarity score removed frontier necessary order exceed buffer size max buffer 
offers simplified pseudocode best search bfs algorithm 
sim function returns cosine similarity topic page sim kp kq topic fetched page frequency term preliminary experiments task shown bfs competitive crawler want assess compares state art algorithms challenging crawling task :10.1.1.1.9569
pagerank pagerank topic starting urls frequency foreach link starting urls enqueue frontier link visited max pages multiplies visited frequency recompute scores pr link dequeue top link frontier doc fetch link score sim sim topic doc enqueue buffered pages doc score sim buffered pages max buffer dequeue bottom links buffered pages merge frontier extract links doc score pr frontier max buffer dequeue bottom links frontier pseudocode pagerank crawler 
pagerank proposed brin page possible model user surfing behavior :10.1.1.109.4049
pagerank page represents probability random surfer follows links randomly page page page time 
page score depends recursively scores pages point 
cosine similarity crawler links minimum similarity score removed frontier necessary order exceed buffer size max buffer 
offers simplified pseudocode best search bfs algorithm 
sim function returns cosine similarity topic page sim kp kq topic fetched page frequency term preliminary experiments task shown bfs competitive crawler want assess compares state art algorithms challenging crawling task :10.1.1.1.9569
pagerank pagerank topic starting urls frequency foreach link starting urls enqueue frontier link visited max pages multiplies visited frequency recompute scores pr link dequeue top link frontier doc fetch link score sim sim topic doc enqueue buffered pages doc score sim buffered pages max buffer dequeue bottom links buffered pages merge frontier extract links doc score pr frontier max buffer dequeue bottom links frontier pseudocode pagerank crawler 
pagerank proposed brin page possible model user surfing behavior :10.1.1.109.4049
pagerank page represents probability random surfer follows links randomly page page page time 
page score depends recursively scores pages point 
source pages distribute pagerank outlinks 
formally page scored set pages pointing set links constant damping factor represents probability random surfer requests random page 
pagerank page represents probability random surfer follows links randomly page page page time 
page score depends recursively scores pages point 
source pages distribute pagerank outlinks 
formally page scored set pages pointing set links constant damping factor represents probability random surfer requests random page 
originally proposed pagerank intended combination content criteria rank retrieved sets documents :10.1.1.109.4049
fact pagerank google search engine 
pagerank guide crawlers assess page quality :10.1.1.22.3686
previous evaluated crawler pagerank :10.1.1.1.9569
efficient algorithm calculate pagerank 
source pages distribute pagerank outlinks 
formally page scored set pages pointing set links constant damping factor represents probability random surfer requests random page 
originally proposed pagerank intended combination content criteria rank retrieved sets documents :10.1.1.109.4049
fact pagerank google search engine 
pagerank guide crawlers assess page quality :10.1.1.22.3686
previous evaluated crawler pagerank :10.1.1.1.9569
efficient algorithm calculate pagerank 
see equation pagerank requires recursive calculation convergence computation resource intensive process 
ideal situation recalculate pageranks time url needs selected frontier 
formally page scored set pages pointing set links constant damping factor represents probability random surfer requests random page 
originally proposed pagerank intended combination content criteria rank retrieved sets documents :10.1.1.109.4049
fact pagerank google search engine 
pagerank guide crawlers assess page quality :10.1.1.22.3686
previous evaluated crawler pagerank :10.1.1.1.9569
efficient algorithm calculate pagerank 
see equation pagerank requires recursive calculation convergence computation resource intensive process 
ideal situation recalculate pageranks time url needs selected frontier 
improve efficiency recomputed pageranks regular intervals 
shows pseudocode shark search crawler 
parameters represent tively maximum depth relative importance inherited versus neighborhood scores 
implementation set 
closely follows details 
infospiders infospiders adaptive population agents search pages relevant topic evolving query vectors neural nets decide links follow :10.1.1.43.7796
evolutionary approach uses fitness measure similarity local selection criterion 
original algorithm see shark topic starting urls foreach link starting urls set depth link enqueue frontier link visited max pages link dequeue top link frontier doc fetch link doc score sim topic doc depth link foreach outlink extract links doc score neighborhood score outlink inherited score outlink doc score set depth outlink set depth outlink depth link enqueue frontier outlink score frontier max buffer dequeue bottom link frontier pseudocode shark search crawler 
previously simplified implemented crawler module :10.1.1.1.9569
crawler outperformed bfs due extreme local behavior absence memory link followed agent visit pages linked new page 
closely follows details 
infospiders infospiders adaptive population agents search pages relevant topic evolving query vectors neural nets decide links follow :10.1.1.43.7796
evolutionary approach uses fitness measure similarity local selection criterion 
original algorithm see shark topic starting urls foreach link starting urls set depth link enqueue frontier link visited max pages link dequeue top link frontier doc fetch link doc score sim topic doc depth link foreach outlink extract links doc score neighborhood score outlink inherited score outlink doc score set depth outlink set depth outlink depth link enqueue frontier outlink score frontier max buffer dequeue bottom link frontier pseudocode shark search crawler 
previously simplified implemented crawler module :10.1.1.1.9569
crawler outperformed bfs due extreme local behavior absence memory link followed agent visit pages linked new page 
number improvements original algorithm retaining capability learn link estimates neural nets focus search promising areas selective reproduction 
resulting novel algorithm schematically illustrated pseudocode 
infospiders agents independent crawl parallel 
discussed section task finding target pages difficult akin searching haystack 
stated results sections averages topics 
error bars plots correspond standard error standard deviation mean topics 
interpret non overlapping error bars statistically significant difference confidence level assuming normal noise 
dynamic performance prior experiments pagerank crawler competitive worse breadth due minimal exploitation topic context resource limitations :10.1.1.1.9569
pagerank metric designed global measure values little computed small set pages 
average similarity pages crawled max buffer pr bfs average similarity topic descriptions pagerank best crawlers task 
data topics :10.1.1.1.9569
illustrate plots average similarity topic descriptions crawl starting target pages 
interpret non overlapping error bars statistically significant difference confidence level assuming normal noise 
dynamic performance prior experiments pagerank crawler competitive worse breadth due minimal exploitation topic context resource limitations :10.1.1.1.9569
pagerank metric designed global measure values little computed small set pages 
average similarity pages crawled max buffer pr bfs average similarity topic descriptions pagerank best crawlers task 
data topics :10.1.1.1.9569
illustrate plots average similarity topic descriptions crawl starting target pages 
poor performance pagerank performance measures tasks report crawling algorithm remainder 
performance state art crawlers breadth best shark search infospiders compared 
ran variations crawling algorithm values max buffer 
crawler decide pages visit cues provided links nearby pages 
assumes relevant page higher probability near relevant pages random page quality estimate pages provide cues exploited bias search process 
short range relevance clues web graph relevant page links apparently irrelevant 
balancing exploitation quality estimate information exploration suboptimal pages may crucial performance topic driven crawlers want confirm empirically 
crawler descriptions section describe generalized versions best shark search crawlers designed tune level crawlers study tradeoff exploitation exploration appropriate crawling application :10.1.1.1.9512
generalized best best described section 
generalization bfs iteration batch top links crawl selected 
completing crawl pages crawler decides batch 
offers simplified pseudocode class 
increase get greater exploitation decrease greater exploration extreme case generating uniform probability distribution frontier 
shark topic starting urls foreach link starting urls set depth link enqueue frontier link visited max pages links crawl dequeue top links frontier foreach link links crawl doc fetch link doc score sim topic doc depth link foreach outlink extract links doc score neighborhood score outlink inherited score outlink doc score set depth outlink set depth outlink depth link enqueue frontier outlink score frontier max buffer dequeue bottom links frontier pseudocode crawlers 
shark corresponds algorithm 
average recall average recall max buffer bfs bfs pages crawled max buffer shark shark pages crawled average similarity average similarity max buffer bfs bfs pages crawled max buffer shark shark pages crawled average target recall left similarity topic descriptions right representatives top bottom crawling algorithms 
experimental results focus classes crawlers behavior straightforward interpret terms exploration parameter results section summarize extend preliminary experiments showing crawlers yields best performance number measures dynamic performance dynamic results summarized :10.1.1.1.9512
parameter max buffer set links 
readability plotting performance selected subset crawlers 
behavior remaining crawlers bfs bfs bfs extrapolated curves corresponding bfs bfs 
holds crawlers 
cybenko 
dynamic web 
proc 
th international world wide web conference 
brin page :10.1.1.109.4049
anatomy large scale hypertextual web search engine 
computer networks 
chakrabarti van den berg dom :10.1.1.43.1111
focused crawling new approach topic specific web resource discovery 
th international world wide web conference 
brin page :10.1.1.109.4049
anatomy large scale hypertextual web search engine 
computer networks 
chakrabarti van den berg dom :10.1.1.43.1111
focused crawling new approach topic specific web resource discovery 
computer networks 
cho garcia molina 
evolution web implications incremental crawler 
computer networks 
cho garcia molina 
evolution web implications incremental crawler 
proceedings th international conference large databases vldb 
cho garcia molina page :10.1.1.22.3686
efficient crawling url ordering 
computer networks 

sizing internet 
menczer ae monge 
scalable web search adaptive online agents infospiders case study 
klusch editor intelligent information agents agent information discovery management internet pages 
springer berlin 
menczer ruiz srinivasan :10.1.1.1.9569
evaluating topic driven web crawlers 
proc 
th annual intl 
acm sigir conf 
th international world wide web conference 
menczer 
evolve intelligent web crawlers 
autonomous agents multi agent systems 
srinivasan menczer :10.1.1.1.9512
exploration versus exploitation topic driven crawlers 
proc 
www workshop web dynamics 
pinkerton 
