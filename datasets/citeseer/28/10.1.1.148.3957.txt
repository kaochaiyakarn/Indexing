racing committees large datasets eibe frank geoffrey holmes richard mark hall department computer science university waikato hamilton new zealand eibe geoff cs waikato ac nz 
proposes method generating classifiers large datasets building committee simple base classifiers standard boosting algorithm 
permits processing large datasets underlying base learning algorithm efficiently 
basic idea split incoming data chunks build committee classifiers built individual chunks 
method extends earlier introducing method adaptively pruning committee 
essential applying algorithm practice dramatically reduces algorithm running time memory consumption 
possible efficiently race committees corresponding different chunk sizes 
important empirical results show accuracy resulting committee vary significantly chunk size 
show pruning crucial method practical large datasets terms running time memory requirements 
surprisingly results demonstrate pruning improve accuracy 
ability process large datasets important institutions automatically collect data purpose data mining 
addresses problem generating classification models large datasets task predict value nominal class set attributes 
popular learning algorithms classification models directly applicable large datasets slow require memory 
apart specialized algorithms particular classification models generic remedies problems proposed literature 
broadly classified subsampling strategies learning committee machines 
strategies appears particularly promising require data discarded building classifier allows incremental learning model updated new chunk data arrives 
basic idea committee learning large datasets build committee splitting data chunks learning model chunk combining predictions different models form prediction 
maximum chunk size kept small polynomial time algorithms applied induce individual models reasonable amount time 
working chunks process memory efficient chunk discarded processed learning scheme 
focus boosting algorithm building committee machines 
boosting advantage combine weak classifiers committee significantly powerful individual classifier :10.1.1.133.1040
particularly advantageous application individual classifiers built relatively small samples data necessarily weak idea boosting large datasets new appears proposed breiman 
main contribution method adaptively efficiently pruning incrementally built committee classifiers process computationally feasible large datasets 
possible choose appropriate chunk size candidates racing candidate solutions 
important correct chunk size determined priori 
apart making method practical pruning desirable side effect resulting predictions accurate 
structured follows 
section method constructing committees large datasets 
start naive method perform pruning move practical method incorporates pruning strategy 
discuss resulting committees raced 
section contains experimental results collection benchmark datasets demonstrating importance choosing appropriate chunk size pruning strategy 
section discusses related combining classifiers built chunks data 
section summarizes contributions 
algorithm describe basic algorithm called incremental boosting generates committee incoming chunks data 
explain incremental boosting modified incorporate pruning 
racing strategy pruned committees built different chunk sizes 
incremental boosting standard boosting algorithms implement basic strategy 
step prediction model built training data underlying weak learning algorithm added initially empty committee 
second step weight associated training instance modified 
step process repeated number iterations 
resulting committee prediction combining predictions individual models 
boosting works individual models complement 
model added committee instances weights changed instances committee finds difficult classify correctly get high weight easy classify get low weight 
model built focus difficult parts instance space easy ones difficulty measured committee built far 
strategy generates diverse committee models diversity appears main reason boosting works practice 
turns boosting viewed statistical estimation procedure called additive logistic regression logitboost boosting procedure direct implementation additive logistic regression method maximizing multinomial likelihood data committee :10.1.1.30.3515
contrast adaboost related algorithms advantage directly applicable multiclass problems 
jointly optimizes class probability estimates different classes appears accurate multi class boosting methods :10.1.1.30.3515
reason chose underlying boosting mechanism incremental boosting strategy 
logitboost assumes underlying weak learner regression algorithm attempts minimize mean squared error 
example regression tree learner 
experimental results reported learning algorithm regression stumps regression stumps level regression trees 
implementation stumps ternary splits branch handles missing attribute values 
difference standard boosting incremental boosting uses different dataset iteration boosting algorithm incoming training data split mutually exclusive chunks size model generated chunks 
new chunk data available existing committee predictions chunk weight data new model learned weighted chunk added committee 
fashion committee boosted models incrementally constructed training data processed 
depicts basic algorithm incremental boosting 
algorithm assumes new models added committee data exhausted 
may feasible memory constraints 
section discuss pruning method reducing committee size 
drawback algorithm time complexity quadratic number chunks quadratic number training instances 
iteration base models invoked order predictions chunk ki instances weighted 
consequently naive algorithm applied chunk size large relative size full dataset 
start empty committee repeat data chunk ci weight chunk ci predictions learn model mi chunk ci add committee chunks fig 

incremental boosting 
incremental boosting pruning algorithm practical necessary reduce number committee members generated 
preferably done adaptively accuracy data affected negatively 
design decision concerns pruning operations apply 
second problem decide pruning occur 
boosting sequential process new models built data weighted predictions previous models 
may detrimental prune models middle committee subsequent models generated predictions previous models account 
consequently model consider pruning model sequence 
pruning straightforward computationally efficient procedure existing committee compared new committee additional member latest chunk data 
judged accurate model discarded boosting process continues chunk data 
pruning process possible skip chunks data contribute positively committee accuracy 
experimental results section show especially useful small chunks build committee 
experimental results show advisable building committee bad chunk data encountered chunks data may prove useful lead models improve committee accuracy 
second aspect pruning choice evaluation criterion 
pruned model needs compared unpruned 
pruning occur negatively affect committee generalization performance 
fortunately target application domains share common property exhibit abundance data 
means generous reserve data pruning 
call data validation data data held completely separate data training models 
implementation instances encountered size validation dataset skipped boosting process 
consequently chunk data generates potential committee member starts instance 
start empty committee validation data repeat data chunk ci weight chunk ci predictions learn model mi chunk ci loglikelihood mi loglikelihood add mi chunks fig 

incremental boosting pruning 
accuracy validation data obvious performance measure 
empirically pruning criterion 
preliminary results showed results useful models skipped change accuracy immediately improve accuracy conjunction models built process 
logistic regression attempts maximize likelihood data model 
alternative candidate measuring performance loglikelihood validation data 
measures accuracy class probability estimates generated committee 
turns loglikelihood avoids sensitive potential committee member manages extract useful additional information 
resulting pruning algorithm loglikelihood depicted 
pruning reduces size committee properties data 
ideally models added committee information data exhausted 
case exists upper bound number models generated time complexity linear number training instances allowing large datasets processed effectively 
course apart affecting running time pruning reduces amount memory needed store committee 
racing committees experimental results show performance committee varies dramatically chunk size 
chunk size large individual committee member reliable predictor 
chunk size increases returns individual committee member diminish 
point productive increase diversity committee starting new chunk 
best chunk size depends properties particular dataset weak learner boosting process 
observations appears impossible determine appropriate chunk size priori 
consequently sensible strategy decide range chunk sizes run different committees corresponding different chunk sizes parallel race 
keep track committee performs best best performing committee prediction 
typically best performing chunk size changes data available 
validation data pruning compare performance committees 
contrast pruning loglikelihood employed measure performance appropriate percent correct want committee maximizes percent correct data 
question remains committees run parallel set chunk sizes 
ultimately depends computing resources available 
number committees constant time space complexity racing corresponding complexities worst case member 
consequently assuming pruning works certain number iterations models added committee time complexity linear number instances space complexity constant 
experiments chunk sizes 
kept maximum chunk size relatively small decision stumps particularly weak classifiers returns adding data diminish quickly 
doubling chunk size candidate advantage committee corresponding largest chunk size may changed true smaller ones comparison validation data point fair committees seen amount training data 
experimental results evaluate performance racing unpruned pruned committees performed experiments datasets ranging size approximately roughly instances 
properties datasets shown table 
obtained uci repositories 
kdd cup data reduced version full dataset reduced incremental boosting pruning applied data 
train column shows amount data training committee excluding validation data 
attempted set sufficient amount data aside validation testing obtain accurate performance estimates 
experiments validation set size set half size test set 
split data course application domain requires accurate probability estimates appropriate loglikelihood choosing best committee 
datasets small chose include comparison lack publicly available large datasets 
table 
datasets characteristics dataset train validation test numeric nominal classes anonymous adult shuttle census income kdd cup training validation test data randomized obtain independent identically distributed samples 
row shows results anonymous data 
leftmost graph shows percent incorrect test set unpruned committees increasing amount training data processed 
points mark graph corresponding committee performs best validation data committee prediction point racing scheme 
middle graph shows pruned committees rightmost graph shows committee sizes pruned committees 
worst performing chunk size insufficient data build large committee 
final ranking committees pruning resulting error rates comparable 
pruning appears smooth fluctuations error test data 
pruning substantially reduces committee size small chunk sizes 
training instances models built pruning chunk size pruning appears number models reached plateau 
pruning done chunk sizes 
appears pruning occurred chunk size 
loglikelihood validation data increase models added consequently pruning occurs 
second row shows results adult data 
substantial pruning occurs chunk sizes 
cases smoothes fluctuations performance results improved final error 
interesting see final size pruned committee chunk size larger size committee chunk size 
pruning appears behave correctly final error lower 
shuttle data third row different previous datasets high accuracy scores achieved 
results show pruning produces substantially smaller committees chunk sizes 
chunk sizes results fractionally lower accuracy 
choosing best performing committee validation data racing scheme results approximately final error rate pruning 
size unpruned committees shown increases linearly amount training data 
fig 

error test data pruning left loglikelihood pruning center 
committee sizes loglikelihood pruning right anonymous best validation anonymous best validation anonymous adult best validation adult best validation adult shuttle best validation shuttle best validation shuttle fig 

error test data pruning left loglikelihood pruning center 
committee sizes loglikelihood pruning right census income best validation census income best validation census income kddcup best validation kddcup best validation kddcup best validation best validation table 
percent incorrect standard logitboost compared racing scheme dataset logitboost iterations racing pruning racing pruning anonymous adult shuttle census income dataset consider census income 
row shows results 
striking aspect effect pruning small chunk sizes 
domain fluctuation error extreme pruning 
pruning erratic behavior disappears error rates decrease dramatically 
pruning results marginally lower final error rate largest chunk sizes 
results show size pruned committees starts level certain amount training data seen 
note chunk size results accurate pruned committee test data chunk size chosen prediction superior performance validation set 
kdd cup domain similar shuttle domain high accuracy achieved 
shuttle domain occurs small chunk sizes note degradation performance corresponds fractions percent 
difficult see graphs pruning marginally improves performance largest chunk size 
racing scheme final performance approximately pruning 
dataset large pruning results substantial savings memory runtime 
behavior largest dataset third row similar seen census income dataset 
difference pruned version chooses chunk size census income chosen 
pruning substantially increases accuracy chunk sizes eliminating erratic behavior unpruned committees 
best performing committee pruned unpruned chunk size 
pruning improve accuracy final predictor racing scheme 
lead substantial savings memory runtime 
final committee chunk size half size unpruned version 
table compares final error racing scheme standard logitboost weak learner applied full training set boosting iteration 
set number iterations standard logitboost number committee members largest unpruned committee built chunk size 
table include results largest datasets processing standard logitboost computing resources 
expected standard logitboost slightly accurate test sets 
results close 
related breiman appears apply boosting arcing problem processing large datasets different subsample iteration boosting algorithm 
shows produces accurate predictions bagging fashion 
shows incremental boosting conjunction appropriate subsample size produces classifiers accurate ones generated standard boosting applied full dataset 
address problem decide committee members discard 
fan propose incremental version adaboost works similar fashion 
method retains fixed size window weak classifiers contains built classifiers 
method applicable large datasets terms memory time requirements 
remains unclear appropriate value determined 
street kim propose variant bagging incremental learning data chunks maintains fixed size committee 
iteration attempts identify committee member replaced model built chunk data 
algorithm bagging data points receive equal weight simple majority vote performed prediction algorithm limited potential boost performance underlying weak classifiers 
russell propose incremental versions bagging boosting differ require underlying weak learner incremental 
method limited large datasets underlying incremental learning algorithm scale linearly number training instances 
unfortunately time complexity incremental learning algorithms worse linear 
prodromidis stolfo pruning methods ensemble classifiers built different unweighted subsets dataset 
methods require unpruned ensemble built pruning applied 
pruned ensemble updated incrementally new data arrives 
similarly dietterich investigate pruning methods ensembles built standard adaboost algorithm weak classifier built entire dataset boosting iteration 
method applied unpruned ensemble generated 
method efficiently processing large datasets standard learning techniques wrapping incremental boosting algorithm 
main contribution pruning method making procedure efficient terms memory runtime requirements 
accuracy resulting committee depends appropriately chosen chunk size 
experimental results obtained racing candidate solutions different chunk sizes demonstrate effectiveness method real world datasets 
technique online setting applied domains concept drift target concept changes time assumes incoming data independent identically distributed 
acknowledgments bernhard pfahringer valuable comments 

blake keogh merz 
uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 

leo breiman 
arcing classifiers 
annals statistics 

leo breiman 
pasting small votes classification large databases line 
machine learning pages 

wei fan salvatore stolfo zhang 
application adaboost distributed scalable line learning 
th acm sigkdd int 
conf 
knowledge discovery data mining pages 

yoav freund robert schapire 
experiments new boosting algorithm 
proc 
th int 
conf 
machine learning pages 
morgan kaufmann 

jerome friedman trevor hastie robert tibshirani 
additive logistic regression statistical view boosting 
annals statistic 

bay 
uci kdd archive 
kdd ics uci edu 

john langley 
static versus dynamic sampling data mining 
nd acm sigkdd int 
conf 
knowledge discovery databases data mining pages 

dietterich 
pruning adaptive boosting 
proc 
th int 
conf 
machine learning pages 

stuart russell 
experimental comparisons online batch versions bagging boosting 
th acm sigkdd int 
conf 
knowledge discovery databases data mining pages 

prodromidis stolfo chan 
pruning classifiers distributed meta learning system 
proc 
st national conference new information technologies pages 

andreas prodromidis salvatore stolfo 
cost complexity pruning ensemble classifiers 
knowledge information systems 

srinivasan 
study sampling methods analysing large datasets ilp 
data mining knowledge discovery 

nick street kim 
streaming ensemble algorithm sea large scale classification 
th acm sigkdd int 
conf 
knowledge discovery databases data mining pages 
