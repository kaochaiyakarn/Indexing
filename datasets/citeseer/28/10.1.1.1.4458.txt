probabilistic latent semantic indexing proceedings second annual international sigir conference research development information retrieval probabilistic latent semantic indexing novel approach automated document indexing statistical latent class model factor analysis count data 
fitted training corpus text documents generalization expectation maximization algorithm utilized model able deal domain speci synonymy polysemous words 
contrast standard latent semantic indexing lsi singular value decomposition probabilistic variant solid statistical foundation de nes proper generative data model 
retrieval experiments number test collections indicate substantial performance gains direct term matching lsi 
particular combination models di erent dimensionalities proven advantageous 
advent digital databases communication networks huge repositories textual data available large public 
order computers interact naturally humans deal potential vagueness user requests recognize di erence user say meant 
typical scenario human machine interaction information retrieval natural language queries user formulates request providing number keywords free form text expects system thomas hofmann international computer science institute berkeley ca eecs department cs division uc berkeley hofmann cs berkeley edu return relevant data amenable representation form ranked list relevant documents 
retrieval methods simple word matching strategies determine rank relevance document respect query 
known literal term matching severe drawbacks mainly due words unavoidable lack precision due personal style individual di erences word usage 
latent semantic analysis lsa approach automatic indexing information retrieval attempts overcome problems mapping documents terms representation called latent semantic space :10.1.1.108.8490
lsa usually takes high dimensional vector space representation documents term frequencies starting point applies dimension reducing linear projection 
speci form mapping determined document collection singular value decomposition svd corresponding term document matrix 
general claim similarities documents documents queries reliably estimated reduced latent space representation original representation 
rationale documents share frequently occurring terms similar representation latent space terms common 
general claim similarities documents documents queries reliably estimated reduced latent space representation original representation 
rationale documents share frequently occurring terms similar representation latent space terms common 
lsa performs sort noise reduction potential bene detect synonyms words refer topic 
applications proven result robust word processing 
lsa applied remarkable success di erent domains including automatic indexing latent semantic indexing lsi number de mainly due unsatisfactory statistical foundation :10.1.1.108.8490
primary goal approach lsa factor analysis called probabilistic latent semantic analysis plsa solid statistical foundation likelihood principle proper generative model data 
implies particular standard techniques statistics applied questions model tting model combination complexity control 
addition factor representation obtained plsa allows deal polysemous words explicitly distinguish di erent meanings di erent types word usage 
aspect model core plsa statistical model called aspect model :10.1.1.46.2857
lsa applied remarkable success di erent domains including automatic indexing latent semantic indexing lsi number de mainly due unsatisfactory statistical foundation :10.1.1.108.8490
primary goal approach lsa factor analysis called probabilistic latent semantic analysis plsa solid statistical foundation likelihood principle proper generative model data 
implies particular standard techniques statistics applied questions model tting model combination complexity control 
addition factor representation obtained plsa allows deal polysemous words explicitly distinguish di erent meanings di erent types word usage 
aspect model core plsa statistical model called aspect model :10.1.1.46.2857
latent variable model general occurrence data associates unobserved class variable fz observation occurrence word fw wmg document fd 
terms generative model de ned way select document probability pick latent class probability generate word probability 
result obtains observed pair latent class variable discarded 
translating process joint probability model results expression wjd wjd essentially derive sum possible choices generated observation 
aspect model statistical mixture model independence assumptions observation pairs assumed generated independently essentially corresponds bag words approach 
secondly conditional independence assumption conditioned latent class words generated independently speci document identity number states smaller number documents acts bottleneck variable predicting conditioned notice contrast document clustering models document speci word distributions wjd obtained convex combination aspects factors 
documents assigned clusters characterized speci mixture factors weights 
mixing weights er modeling power conceptually di erent posterior probabilities clustering models unsupervised naive bayes models cf 
:10.1.1.46.2857
likelihood principle determines maximization log likelihood function log denotes term frequency number times occurred noticing equivalent symmetric version model obtained inverting conditional probability help bayes rule results just re parameterized version generative model described 
model fitting tempered em standard procedure maximum likelihood estimation latent variable models expectation maximization em algorithm 
em alternates steps expectation step posterior probabilities computed latent variables current estimates parameters ii maximization step parameters updated posterior probabilities computed previous step 
aspect model symmetric parameterization bayes rule yields step probability word particular document explained factor corresponding standard calculations arrives step re estimation equations alternating de nes convergent procedure approaches local maximum log likelihood 
long performance held data improves continue tem iterations value iv 
decreasing yield improvements goto step ii 
perform nal iterations training held data 
experiments typical number iterations tem performed starting randomized initial conditions iteration requires pass data order arithmetical operations 
probabilistic latent semantic analysis latent semantic analysis mentioned key idea lsa map documents symmetry terms vector space reduced dimensionality latent semantic space :10.1.1.108.8490
mapping computed decomposing term document matrix svd orthogonal matrices diagonal matrix contains singular values lsa approximation computed thresholding largest singular values zero rank optimal sense matrix norm known linear algebra obtains approximation note norm approximation prohibit entries negative 
geometry aspect model consider class conditional multinomial distributions jz vocabulary aspect model represented points dimensional simplex possible multinomials 
convex hull set points de nes dimensional sub simplex 
modeling assumption expressed conditional distributions jd approximated representable convex combination class conditionals jz 
compared lsi plsi plsi variants plsi plsi results obtained combining plsi models plsi plsi respectively 
lsi indicates performance gain achieved baseline result dimensions combination baseline score reported case 
tdt collection real love context family life opposed staged love sense hollywood 
folding queries folding refers problem computing representation document query contained original training collection 
lsa approach simply done linear mapping ectively represents document query center constituent terms appropriate term weighting :10.1.1.108.8490
plsa mixing proportions computed em iteration factors xed mixing proportions adapted step 
table shows factors tdt collection clearly re ect vocabulary dealing certain events war iraq crisis earthquake kobe 
factors computed representation test query consisting terms aid food medical people un war 
visualizes evolution posterior probabilities mixing proportions course em procedure 
query designed factor matching query terms un involved kobe earthquake medical aid provided iraq gulf war 
seen factor highest weight rst iteration notice factors account half probability 
changes em iterations aspect model introduces feedback terms 
example term un best explained factor context query terms drastically increases probability particular occurrence un related events 
mechanism able detect true :10.1.1.33.1187
probabilistic latent semantic indexing vector space models lsi popular families information retrieval techniques vector space model vsm documents 
vsm variant ingredients transformation function called local term weight ii term weighting scheme called global term weight iii similarity measure 
experiments utilized representation term frequencies tf combined ii popular inverse document frequency idf term weights iii standard cosine matching function 
representation applies queries matching function baseline methods written pp pp idf weighted word frequencies 
experiments utilized representation term frequencies tf combined ii popular inverse document frequency idf term weights iii standard cosine matching function 
representation applies queries matching function baseline methods written pp pp idf weighted word frequencies 
latent semantic indexing original vector space representation documents replaced representation low dimensional latent space similarity computed representation 
queries documents part original collection folded simple matrix multiplication cf 
details :10.1.1.108.8490
precision precision med tf recall cos tf lsi plsi cos tfidf lsi plsi med tfidf recall cran tf recall cos tf lsi plsi cos tfidf lsi plsi cran tfidf recall cacm tf recall cos tf lsi plsi cos tfidf lsi plsi cacm tfidf recall cisi tf recall cos tf lsi plsi cos tfidf lsi plsi cisi tfidf recall precision recall curves test collections idf term weighting lower row upper row 
depicted curves direct term matching lsi best performing plsi variant 
experiments considered linear combinations original similarity score weight derived latent space representation weight suggested cf 
detailed empirical investigation linear combination schemes information retrieval systems 
plsi plsi report best result obtained models lsi report best result obtained optimal dimension exploring dimensions step size 
combination weight cosine baseline score coarsely optimized hand med cran cacm cisi general slightly smaller weights utilized combined models 
experiments consistently validate advantages plsi lsi 
substantial performance gains achieved data sets term weighting schemes 
particular plsi plsi particularly raw term frequencies lsi hand may fail completely accordance results reported :10.1.1.108.8490
explain fact large frequencies dominate squared error deviation svd idf weighting necessary get reasonable decomposition term document matrix 
plsi take advantage term weighting scheme plsi plsi performs slightly better case 
suspect better results achieved improved integration term weights plsi 
bene ts model combination substantial 
empirical evaluation clearly con rmed bene ts probabilistic latent semantic indexing achieves signi cant gains precision standard term matching lsi 
investigation needed take full advantage prior information provided term weighting schemes 
shown bene ts plsa extend document indexing similar approach utilized language modeling collaborative ltering 
acknowledgment supported postdoctoral fellowship 
deerwester dumais furnas landauer harshman indexing latent semantic analysis :10.1.1.108.8490
journal american society information science 
dempster laird rubin maximum likelihood incomplete data em algorithm 
royal statist 
soc 
hofmann latent class models collaborative ltering 
proceedings th international joint conference onarti cial intelligence ijcai 
hofmann probabilistic latent semantic analysis 
proceedings th conference uncertainty ai 
hofmann puzicha jordan unsupervised learning dyadic data :10.1.1.46.2857
advances neural information processing systems vol 

linguistic data consortium 
tdt pilot study corpus 
