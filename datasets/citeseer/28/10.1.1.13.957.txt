evolutionary artificial neural networks xin yao department computer science university college university new south wales australian defence force academy canberra act australia published international journal neural systems vol 
pp 
evolutionary artificial neural networks eanns considered combination artificial neural networks anns evolutionary search procedures genetic algorithms gas 
distinguishes levels evolution eanns evolution connection weights architectures learning rules 
reviews kind evolution detail analyses major issues related kind evolution 
shown lot evolution connection weights architectures research evolution learning rules early stages 
argued evolution learning rules interactions levels evolution play vital role eanns 
keywords evolutionary artificial neural networks learning evolution genetic algorithms 
yao evolutionary artificial neural networks interest combinations anns evolutionary search procedures grown rapidly years 
covers wide range topics weight training architecture design learning learning rule input feature selection genetic reinforcement learning initial weight selection ann analysis ann conferences papers combinations 
workshop solely devoted area :10.1.1.13.957
emphasis different review papers :10.1.1.13.957
concerned understanding development eanns regarded general framework adaptive systems systems change architectures learning rules different environments human intervention 
combinations ann analysis gas covered 
interested exploring possible benefits arising interactions anns evolutionary search procedures anns evolutionary search procedures shall concentrate popular models anns evolutionary search procedures study feedforward anns gas trying cover kinds models 
keywords evolutionary artificial neural networks learning evolution genetic algorithms 
yao evolutionary artificial neural networks interest combinations anns evolutionary search procedures grown rapidly years 
covers wide range topics weight training architecture design learning learning rule input feature selection genetic reinforcement learning initial weight selection ann analysis ann conferences papers combinations 
workshop solely devoted area :10.1.1.13.957
emphasis different review papers :10.1.1.13.957
concerned understanding development eanns regarded general framework adaptive systems systems change architectures learning rules different environments human intervention 
combinations ann analysis gas covered 
interested exploring possible benefits arising interactions anns evolutionary search procedures anns evolutionary search procedures shall concentrate popular models anns evolutionary search procedures study feedforward anns gas trying cover kinds models 
discussion applicable models 
section devoted evolution architectures evolution lead near optimal architecture tasks hand 
known architecture eann decides maximum functionality eann 
evolution architectures provides powerful adaptive system decide architecture tasks hand 
automatic way designing eann architectures human intervention 
motivation constructive destructive learning algorithms :10.1.1.125.6421:10.1.1.13.957
important issues encode architectures evolutionary search procedure addressed section 
problems overlooked current research indicated 
imagining eann connection weights architectures hardware easier understand importance evolution eann software learning rules 
section discusses evolution learning rules eanns examines relationship learning evolution learning guides evolution learning evolves 
section 
evolution connection weights learning anns roughly divided supervised unsupervised reinforcement learning 
supervised learning usually formulated minimisation error function total mean square error target outputs actual outputs 
supervised learning algorithms back propagation bp conjugate gradient algorithms gradient descent search 
successful applications bp algorithms various areas :10.1.1.19.9928
drawbacks bp algorithm exist due gradient descent nature 
gets trapped local minimum error function inefficient finding global minimum error function multimodal 
detailed review current state bp algorithm learning algorithms 
way overcome bp gradient descent search training algorithms shortcomings formulate training process evolution connection weights environment determined architecture learning task 
reproduce number children individual current generation fitness 

apply genetic operators crossover mutation child individual generated obtain generation 
typical cycle evolution connection weights 
fitness function lot done evolution connection weights :10.1.1.31.6731
evolution connection weights provides alternative approach training eanns 
evolutionary approach consists major stages 
stage decide genotype representation connection weights form binary strings 
second evolution driven gas evolutionary search procedures genetic operators crossover mutation decided conjunction representation scheme 
hidden nodes eanns essence feature extractors detectors 
separating inputs hidden node far apart binary representation increase difficulty constructing useful feature detectors may destroyed recombination operators easily 
probability obtaining better feature detector combining part feature detector part feature detector low 
open issue destructive effect crossover maintain feature detectors far evolution connection weights 
problem eanns general including evolution connection weights course permutation problem :10.1.1.31.6731
caused mapping representation actual eann eanns label hidden nodes differently different representations functionally equivalent 
problem recombination operators difficult produce highly fit children 
shows equivalent eanns different labels hidden nodes 
advantages binary representation lie simplicity generality 
possible improvement method cauchy random mutation 
worth pointing permutation problem destructive effect crossover feature detectors eanns exist representing connection weights real numbers problem concerning representation precision weights gone 
comparison evolutionary training gradient training indicated section evolutionary training approach attractive handle global search problem better vast complex multimodal space 
depend gradient information error fitness function particularly appealing gradient information unavailable costly get 
example evolutionary approach reinforcement learning recurrent network learning higher order network learning :10.1.1.13.957
noted evolutionary algorithm training different networks yao evolutionary artificial neural networks feedforward networks recurrent networks higher order networks generality save lot human efforts developing different training algorithms different type network 
evolutionary training approach easier generate eanns special characteristics 
method decrease eann complexity improve generalisation ability include penalty term fitness function 
gradient training worry term differentiable 
bp algorithms sensitive initial conditions 
hybrid training major problem gas inefficiency fine tuned local search global search 
efficiency evolutionary training improved significantly incorporating local search procedure evolution combining ga global search ability local search fine tuning ability 
gas locate region space local search procedure find near optimal solution region 
local search algorithm bp algorithm random search algorithms :10.1.1.31.6731
belew gas search near optimal set initial connection weights bp algorithm perform local search initial weights 
results showed hybrid ga bp approach efficient ga bp algorithm 
taken account fact bp algorithm run times practice order find connection weights bp sensitivity initial conditions hybrid training algorithm quite competitive comparison gradient training algorithms 
similar evolution initial weights done competitive learning neural networks 
example learning task eann connections hidden nodes linear node transfer function may able perform task due limited information processing ability eann large number connections hidden nodes nonlinear node transfer function may powerful task eann poor generalisation ability 
unfortunately architecture design human expert job 
depends heavily expert experience tedious trial error process 
systematic way design near optimal architecture automatically task 
research constructive destructive algorithms effort automatic design architectures :10.1.1.125.6421:10.1.1.13.957
roughly speaking constructive algorithm starts minimal network network minimal number hidden layers nodes connections adds new layers nodes connections necessary training destructive algorithm opposite starts maximal network deletes unnecessary layers nodes connections training 
design optimal architecture eann formulated search problem architecture space point represents architecture 
performance optimality criteria fastest learning lowest complexity architectures performance level architectures forms surface space 
optimal architecture design equivalent finding highest point surface 

reproduce number children individual current generation fitness 
apply genetic operators children generated obtain generation 
typical cycle evolution architectures 
yao evolutionary artificial neural networks advantages evolutionary design architectures lot research carried years :10.1.1.56.9962
research deals topological structure eanns little done evolution node transfer functions evolution topological structures node transfer functions 
concentrate evolution topological structures analyse genotype representation scheme topological structures section 
convenience sake term architecture interchangeably term topological structure sections 
section discusses evolution node transfer functions briefly 
training result pertaining architecture error training time fitness function 
complexity measurement number nodes connections fitness function 
matter fact lot criteria information theory statistics readily introduced fitness function difficulty 
improvement eann generalisation ability expected criteria adopted 
schaffer experiment shows eann designed evolutionary approach better generalisation ability training bp algorithm human designed architecture :10.1.1.13.957
major problem evolutionary design architectures permutation problem illustrated section 
functionally equivalent eanns label hidden nodes differently different genotype representations probability producing highly fit child crossover low 
researchers abandoned crossover adopted mutation evolution architectures shown crossover useful important increasing efficiency evolution 
hancock suggested permutation problem severe supposed population size selection mechanism yao evolutionary artificial neural networks increased number ways solving problem outweigh difficulties bring building blocks 
typical cycle evolution learning rules described 
similar reason indicated section fitness evaluation individual encoded learning rule noisy techniques alleviate problem weighted average training results eanns different initial connection weights fitness function 
architecture pre defined individual evolution evaluated different architectures 
additional noise introduced evaluation case 
yao evolutionary artificial neural networks evolution algorithmic parameters adaptive adjustment bp algorithm parameters learning rate momentum evolution considered attempt evolution learning rules :10.1.1.31.6731
harp encoded bp algorithm parameters genotypes eann architecture 
evolutionary approach different non evolutionary evolution algorithmic parameters architectures facilitates exploration interactions learning algorithm architectures near optimal combination bp algorithm architecture evolved 
cost benefit mentioned section larger search space longer computation time 
researchers evolutionary process find parameters bp algorithm eann architecture pre defined :10.1.1.31.6731
yao evolutionary artificial neural networks evolution algorithmic parameters adaptive adjustment bp algorithm parameters learning rate momentum evolution considered attempt evolution learning rules :10.1.1.31.6731
harp encoded bp algorithm parameters genotypes eann architecture 
evolutionary approach different non evolutionary evolution algorithmic parameters architectures facilitates exploration interactions learning algorithm architectures near optimal combination bp algorithm architecture evolved 
cost benefit mentioned section larger search space longer computation time 
researchers evolutionary process find parameters bp algorithm eann architecture pre defined :10.1.1.31.6731
parameters evolved case tend optimised architecture general applicable ones 
number bp algorithms adaptive learning rate momentum non evolutionary approach 
comparison approaches quite useful 
evolution learning rules evolution algorithmic parameters certainly interesting hardly touches fundamental part training algorithm learning rule weight adjusting rule 
experiments demonstrate advantages hybrid global local search different levels issue optimal combination different search procedures needs investigation 
increasing power parallel computers simulation large eanns feasible 
simulation discover possible new eann architectures learning rules offer way model creative process result eann adaptation dynamic environment 
author anonymous referees comments suggestions help improve greatly 
whitley schaffer ed :10.1.1.13.957
proc 
int workshop combinations genetic algorithms neural networks 
ieee computer society press los alamitos ca 
schaffer whitley eshelman 
ieee press new york ny 
hertz krogh palmer 
theory neural computation 
addison wesley reading ma 
:10.1.1.13.957
progress supervised neural networks 
ieee signal processing magazine january 
holland 
adaptation natural artificial systems st mit press edn 
proc 
eleventh int joint conf 
artificial intelligence pages 
morgan kaufmann san mateo ca 
yao evolutionary artificial neural networks caudell dolan :10.1.1.31.6731
parametric connectivity training constrained networks genetic algorithms 
schaffer editor proc 
third int conf 
genetic algorithms applications pages 
schaffer editor proc 
third int conf 
genetic algorithms applications pages 
morgan kaufmann san mateo ca 
schaffer caruana eshelman :10.1.1.13.957
genetic search exploit emergent behavior neural networks 
physica 
wilson 
perceptron emergence structure 
