adaptive combination behaviors gent olivier alain francois 
agents interest mainly confronted complex tasks 
propose methodology automated design agents framework markov decision processes case global task decomposed simpler possibly concurrent sub tasks 
accomplished automatically combining basic behaviors reinforcement learning methods 
main idea build global policy weighted combination basic policies weights learned agent simulated annealing case 
basic behaviors learned reused previous tasks need tuned new task 
problem find optimal mapping states actions maximize reward received time usually expressed utility function js 
mapping called policy mdp known optimal deterministic policy exists 
agent partial view environment learning task confronted belongs general class partially observed markov decision processes 
assumption agent faces markovian problem 
truly weak approximation policies learned way clearly sub optimal explained :10.1.1.48.9833
fact better case look stochastic policies gradient descent algorithms example :10.1.1.146.4116
markovian approximation problem combinatorial explosion remains 
number agent possible observations huge locality perceptions helps reducing 
algorithm takes advantage possibility decompose task subtasks address specific problem 
mapping called policy mdp known optimal deterministic policy exists 
agent partial view environment learning task confronted belongs general class partially observed markov decision processes 
assumption agent faces markovian problem 
truly weak approximation policies learned way clearly sub optimal explained :10.1.1.48.9833
fact better case look stochastic policies gradient descent algorithms example :10.1.1.146.4116
markovian approximation problem combinatorial explosion remains 
number agent possible observations huge locality perceptions helps reducing 
algorithm takes advantage possibility decompose task subtasks address specific problem 
previous similar works 
notations section defines basic behavior 
basic behaviors basic behavior defined type configuration stochastic decision policy learned reinforcement utility policy 
basic behavior note type configuration 
notion type configuration essential scalability approach allows generic basic behaviors instantiated times observation 
policy behavior mapping configurations probability distributions actions :10.1.1.146.4116
note different configurations belong type configuration agent deal behavior agent try avoid holes time 
knowledge policy basic behavior sufficient take efficient decisions agent deal concurrent motivations 
weight way giving higher priority danger avoidance important reward sight suggest evaluate situation values give expectation discounted reward utility configuration action pair 
values learned learning policy basic behavior sum behavior calculated tables upcoming probability choose action seeing configuration expected discounted reward choosing action configuration basic behaviors usefully collected library reusable situations 
basic behaviors preparation step simply learn basic behaviors 
evolution policies efficiencies shown sample runs axis gives number simulation steps multiplied axis gives agent reward simulation steps 
cases reward maximized maximum zero avoid behavior reward negative positive maximum push behavior 
number steps avoid hole push tile hole 
learning basic behaviors just notice simple avoid hole behavior practically immediately learned classical approach order compare approach classical reinforcement learning tried apply gradient ascent adapted learning complete tile world problem different numbers tiles holes environment :10.1.1.146.4116
tile hole case learning quite fast efficient really fast increase difficulty adding objects 
gradient ascent succeed simplest case bringing just results learning 
environment considered size cells 
sizes different observation spaces shown table explain phenomenon appears large problems gradient method prone fall local optima agent avoids holes push tiles holes 
main idea learn weight different basic behaviors needed agent weight type behavior 
possible generic basic behaviors associated type configuration instantiated times 
validity approach tested classical tile world problem wide range complex problems 
focus improving combination basic behaviors order build finer complex behaviors 
baxter bartlett :10.1.1.146.4116
infinite horizon policy gradient estimation 
journal artificial intelligence research 
brooks 
robust layered control system mobile robot 
kim kaelbling 
learning cooperate policy search 
proc 
uai 
singh jaakkola jordan :10.1.1.48.9833
learning state estimation partially observable markovian decision processes 
proc 
icml 
stone veloso 
