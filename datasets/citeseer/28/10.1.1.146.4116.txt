journal artificial intelligence research submitted published infinite horizon policy gradient estimation jonathan baxter whizbang 
labs 
henry street pittsburgh pa peter bartlett technologies 
addison street suite berkeley ca whizbang com bartlett com gradient approaches direct policy search reinforcement learning received attention means solve problems partial observability avoid problems associated policy degradation value function methods 
introduce simulation algorithm generating biased estimate gradient average reward partially observable markov decision processes controlled parameterized stochastic policies 
similar algorithm proposed kimura kobayashi 
algorithm chief advantages requires storage twice number policy parameters uses free parameter natural interpretation terms bias variance trade requires knowledge underlying state 
prove convergence show correct choice parameter related mixing time controlled briefly describe extensions controlled markov chains continuous state observation control spaces multiple agents higher order derivatives version training stochastic policies internal states 
companion baxter bartlett weaver show gradient estimates generated traditional stochastic gradient algorithm conjugate gradient procedure find local optima average reward 

dynamic programming method choice solving problems decision making uncertainty bertsekas 
application dynamic programming problematic large infinite state spaces situations system dynamics unknown state partially observed 
cases looks approximate techniques rely simulation explicit model parametric representations policy exact representations 
simulation methods rely parametric form value function tend go name reinforcement learning extensively studied machine learning literature bertsekas tsitsiklis sutton barto 
approach yielded remarkable empirical successes number different domains including learning play checkers samuel backgammon tesauro chess baxter tridgell weaver job shop scheduling zhang dietterich dynamic channel allocation singh bertsekas 
despite success algorithms training approximate value functions suffer theoretical flaw performance greedy policy derived approximate guaranteed improve iteration fact worse old policy ai access foundation morgan kaufmann publishers 
rights reserved 
baxter bartlett amount equal maximum approximation error states 
happen parametric class contains value function corresponding greedy policy optimal 
illustrate concrete simple example appendix alternative approach circumvents problem approach pursue consider class stochastic policies parameterized compute gradient respect average reward improve policy adjusting parameters gradient direction 
note policy directly parameterized generated indirectly value function 
case value function parameters parameters policy adjusted minimize error approximate true value function parameters adjusted directly improve performance policy generated value function 
policy gradient algorithms long history operations research statistics control theory discrete event systems machine learning 
describing contribution appropriate introduce background material explaining approach 
readers familiar material may want skip directly section contributions described 
brief history policy gradient algorithms large scale problems problems system dynamics unknown performance gradient computable closed form challenging aspect policy gradient approach find algorithm estimating gradient simulation 
naively gradient calculated numerically adjusting parameter turn estimating effect performance simulation called crude monte carlo technique prohibitively inefficient problems 
somewhat surprisingly mild regularity conditions turns full gradient estimated single simulation system 
technique called score function likelihood ratio method appears proposed sixties rubinstein computing performance gradients 
independently identically distributed processes 
specifically suppose performance function depends random variable probability parameterized mild regularity conditions gradient respect expected performance may written see rewrite sum differentiate source requirement mild regularity conditions obtain 
see equation closed form expression performance gradient 
policy gradient estimation rewrite observe formula equivalent 
simulator available generate samples distributed sequence generated gives unbiased estimate law large numbers probability 
quantity known likelihood ratio score function classical statistics 
performance function depends replaced 
unbiased estimates performance gradient regenerative processes extensions likelihood ratio method regenerative processes including markov decision processes glynn glynn weiss independently episodic partially observable markov decision processes williams introduced algorithm samples previous section sequences states random length encountered visits designated recurrent state sequences states start state goal state 
case written sum transition probability parameters 
equation admits recursive computation course regenerative cycle form state transition term estimate form addition recursively computed function estimate cycle computed storage parameters parameter update performance function 
entire estimate computed storage real parameters follows 

thresholded version algorithms neuron elements described earlier barto sutton anderson 

vector known reinforcement learning eligibility trace 
terminology barto 

baxter bartlett algorithm policy gradient algorithm regenerative processes 

set 

state transition episode finished set set 
return goto 
examples recursive performance functions include sum scalar reward cycle scalar reward associated state corresponds average reward multiplied expected recurrence time negative length cycle implemented assigning reward state task time taken get goal state case just discounted reward start state discount factor 
williams pointed simplification possible case sum scalar rewards depending state possibly time starting state 
case update single regenerative cycle may written changes influence rewards associated earlier states write able drop term parentheses right hand side proof entirely trivial intuition shown correct 
equation allows simpler recursive formula estimating performance gradient 
set introduce new variable set 
iteration set estimate updated iteration suggests away altogether simply update directly suitable step sizes proving convergence 
usual requirements convergence stochastic gradient algorithm policy gradient estimation algorithm straightforward normal stochastic gradient algorithms updates gradient direction expectation sum updates regenerative cycle 
tsitsiklis provide convergence proof know albeit slightly different update form moving estimate expected performance updated line update suggested context jaakkola 

tsitsiklis considered case dependent rewards recall discussion baird moore algorithm value policy search 
contains interesting insight suitable choices performance function combine policy gradient search approximate value function methods 
resulting algorithms viewed actor critic techniques spirit barto 
policy actor value function critic 
primary motivation reduce variance policy gradient estimates 
experimental evidence phenomenon number authors including barto 
kimura kobayashi baird moore 
subject includes sutton 
tsitsiklis 
discuss style updates section 
far addressed question parameterized state transition probabilities arise 
course simply generated parameterizing matrix transition probabilities directly 
alternatively case sor state transitions typically generated feeding observation depends stochastically state parameterized stochastic policy selects control random set available controls approximate value function approaches generate controls stochastically form lookahead fall category 
distribution successor states fixed function control 
denote probability control parameters observation discussion carries replaced case algorithm precisely williams algorithm 
algorithm variants extended cover multiple agents peshkin policies internal state meuleau importance sampling methods meuleau 
refer reader rubinstein shapiro rubinstein melamed depth analysis application likelihood ratio method discrete event systems particular networks queues 
worth mentioning large literature infinitesimal perturbation analysis ipa seeks similar goal estimating performance gradients operates restrictive assumptions approach see example ho cao 
biased estimates performance gradient algorithms described previous section rely identifiable recurrent state update gradient estimate case line algorithm zero eligibility trace reliance recurrent state problematic main reasons 
variance algorithms related recurrence time visits typically grow state space grows 
furthermore time visits depends baxter bartlett parameters policy states frequently visited initial value parameters may rare performance improves 

situations partial observability may difficult estimate underlying states determine gradient estimate updated eligibility trace zeroed 
system available simulation difficult impossible obtain unbiased estimates gradient direction access recurrent state 
solve look biased estimates 
principle techniques introducing bias proposed may viewed artificial eligibility trace method takes starting point formula eligibility trace time simply truncates fixed random number terms rubinstein cao wan eligibility trace updated transition looking backwards glynn case state rewards estimated gradient direction steps exceeds maximum recurrence time infinite ergodic markov chain biased estimate gradient direction bias approaches zero 
variance diverges limit large illustrates natural trade selection parameter large ensure bias acceptable expectation true gradient direction large variance prohibitive 
experimental results cao wan illustrate nicely bias variance trade 
potential difficulty method likelihood ratios remembered previous time steps requiring storage parameters 
obtain small bias memory may grow bound 
alternative approach requires fixed amount memory discount eligibility trace truncating 
ease exposition kept expression terms likelihood ratios rely availability underlying state available replaced policy gradient estimation discount factor 
case estimated gradient direction steps simply precisely estimate analyze 
similar estimate replaced reward baseline proposed kimura 
continuous control kimura kobayashi 
fact place affect expectation estimates algorithm judicious choice reward baseline reduce variance estimates 
algorithm kimura 
provides estimates expectation stationary distribution gradient discounted reward show fact biased estimates gradient expected discounted reward 
arises stationary distribution depends parameters 
similar estimate proposed tsitsiklis time replaced estimate average reward zeroed visits identifiable recurrent state 
final note observe eligibility traces defined simply filtered versions sequence order infinite impulse response filter case th order finite impulse response filter case raises question addressed interesting theory optimal filtering policy gradient estimators 
contribution describe general algorithm generating biased estimate performance gradient general controlled parameterized stochastic policies 
denotes average reward policy parameters rely access underlying recurrent state 
writing expectation estimate produced show quantitatively close true gradient provided exceeds mixing time markov chain induced truncated estimate trade preventing setting arbitrarily close variance algorithm estimates increase approaches prove convergence probability discrete continuous observation control spaces 
algorithms general parameterized markov chains controlled parameterized stochastic policies 
extensions investigated version written 
outline developments briefly section 
companion show gradient estimates produced perform gradient ascent average reward baxter 
describe traditional stochastic gradient algorithms conjugate gradient algorithm utilizes gradient estimates novel way perform line searches 
experimental results 
mixing time result applies markov chains distinct eigenvalues 
better estimates bias variance may bartlett baxter general markov chains treated refined notions mixing time 
roughly speaking variance grows bias decreases function baxter bartlett ing theoretical results toy problem practical aspects algorithms number realistic problems 

reinforcement learning problem model reinforcement learning markov decision process finite state space stochastic matrix giving probability transition state state 
state associated reward matrix belongs parameterized class stochastic matrices 
denote markov chain corresponding assume markov chains rewards satisfy assumptions assumption 
unique stationary distribution satisfying balance equations denotes transpose 
assumption 
magnitudes rewards uniformly bounded states 
assumption ensures markov chain forms single recurrent class parameters 
finite state markov chain ends recurrent class properties class determine long term average reward assumption mainly convenience include recurrence class quantifier theorems 
consider gradient ascent algorithms baxter 
assumption restrictive guarantees recurrence class change parameters adjusted 
ordinarily discussion complete mention actions available state space policies available learner 
particular parameters usually determine policy directly indirectly value function determine transition probabilities purposes care dependence arises just satisfies assumption differentiability assumptions shall meet section 
note easy extend setup case rewards depend parameters transitions 
equally straightforward extend algorithms results cases 
see section illustration 
goal find maximizing average reward denotes expectation sequences transitions generated assumption independent starting state equal bertsekas 

stochastic matrix 

results apply bounded stochastic rewards case expectation reward state 
policy gradient estimation 
computing gradient average reward general little known average reward finding optimum problematic 
section see general assumptions gradient exists local optimization possible 
ensure existence suitable gradients boundedness certain random variables require parameterized class stochastic matrices satisfies additional assumption 
assumption 
derivatives exist ratios uniformly bounded second part assumption allows zero probability transitions zero case set example forbidden transition example satisfying assumption parameters assuming moment exists justified shortly suppressing dependencies reward depend 
note convention takes precedence operations equations regarded shorthand notation equations form compute differentiate balance equations obtain baxter bartlett system equations defined constrained invertible balance equations show left eigenvector zero eigenvalue 
denote dimensional column vector consisting matrix stationary distribution row 
rewrite see inverse exists matrix satisfying write easy prove induction converges assumption 
exists equal write sufficiently small number states solved exactly yield precise gradient direction 
general state space small exact solution possible small derive optimal policy policy iteration table lookup point pursuing gradient approach place problems practical interest intractable need find way computing gradient 
approximate technique doing section 

argument leading coupled fact unique solution justify existence 
specifically run steps computing value small show expression unique matrix satisfying 
equation may useful case tractable dynamic programming algorithm 
policy gradient estimation 
approximating gradient parameterized markov chains section show gradient split components negligible discount factor approaches denote vector expected discounted rewards state dependence obvious just write proposition 
proof 
observe satisfies bellman equations bertsekas 
shall see section second term estimated single sample path markov chain 
fact theorem kimura shows gradient estimates algorithm converge bellman equations equal implies algorithm kimura 
estimates second term expression 
important note quantities disagree term 
arises stationary distribution depends parameters 
algorithm kimura 
estimate gradient expected discounted reward 
fact expected discounted reward simply times average reward singh fact gradient expected discounted reward proportional gradient average reward 
theorem shows term negligible approaches notice immediate proposition arbitrarily large limit theorem 
baxter bartlett proof 
recalling equation discussion preceeding stochastic matrix rewritten discount factor consider expression clearly complete proof just need show 
invoke observation write particular converges take back sum right hand side write 
theorem shows approximation gradient approaches turns values close lead large variance estimates describe section 
theorem shows need small provided transition probability matrix distinct eigenvalues markov chain short mixing time 
initial state distribution states markov chain converges stationary distribution provided assumption assumption existence uniqueness stationary distribution satisfied see example lancaster theorem 
spectral resolution theorem lancaster theorem implies distribution converges stationarity exponential rate time constant convergence rate mixing time depends eigenvalues transition probability matrix 
existence unique stationary distribution implies 
motivates different kind algorithm estimating differential rewards tsitsiklis 

back sum right hand side diverges 
reason converges orthogonal limit large view sum orthogonal components infinite direction finite direction finite component need estimate 
approximating way rendering component finite hopefully altering component 
substitutions lead better approximations context see final paragraph section 
policy gradient estimation largest magnitude eigenvalue multiplicity corresponding left eigenvector stationary distribution 
sort eigenvalues decreasing order magnitude turns determines mixing time chain 
theorem shows small compared gradient approximation described accurate 
estimate direction update parameters theorem compares directions gradient estimate 
theorem denotes spectral condition number nonsingular matrix defined product spectral norms matrices denotes euclidean norm vector theorem 
suppose transition probability matrix satisfies assumption stationary distribution distinct eigenvalues 
matrix right eigenvectors corresponding order eigenvalues 
normalized inner product satisfies notice expectation stationary distribution mixing time bound theorem depends parameter markov chain spectral condition number markov chain reversible implies eigenvectors orthogonal equal ratio maximum minimum probability states stationary distribution 
eigenvectors need nearly orthogonal 
fact condition transition probability matrix distinct eigenvalues necessary condition number replaced complicated expression involving spectral norms matrices form proof 
existence distinct eigenvalues implies expressed lancaster theorem 
follows polynomial write proposition shows baxter bartlett easy verify left eigenvector corresponding choose 
write follows proposition cauchy schwartz inequality 
schwartz inequality obtain apply cauchy spectral norms bound second factor numerator 
clear definition spectral norm product nonsingular matrices satisfies spectral norm diagonal matrix 
follows combining equation proves 
policy gradient estimation 
estimating gradient parameterized markov chains algorithm introduces markov chain gradient algorithm estimating approximate gradient single line sample path markov chain requires reals stored dimension parameter space parameters eligibility trace parameters gradient estimate note time steps average far algorithm markov chain gradient algorithm parameter parameterized class stochastic matrices satisfying assumptions 
arbitrary starting state state sequence generated markov chain transition probabilities 
reward sequence satisfying assumption 
set 
state visited theorem 
assumptions algorithm starting initial state generate sequence satisfying proof 
denote random process corresponding entire process stationary 
proof easily generalized arbitrary initial distributions fact assumption asymptotically stationary 
baxter bartlett stationary write probability respect stationary distribution process fact follows boundedness magnitudes rewards assumption lebesgue dominated convergence theorem 
rewrite equation denotes indicator function state expectation respect stationary distribution 
chosen stationary distribution process ergodic 
process defined obtained fixed function stationary ergodic breiman proposition 
bounded assumption ergodic theorem surely policy gradient estimation concentrating second term right hand side observe bounds magnitudes rewards assumptions 
unrolling equation algorithm shows equal required 

estimating gradient partially observable markov decision processes algorithm applies parameterized class stochastic matrices compute gradients section consider special case arise parameterized class randomized policies controlling partially observable markov decision process 
partially observable qualification means assume policies access observation process depends state general may see state 
specifically assume controls observations 
determines stochastic matrix depend parameters 
state observation generated independently probability distribution observations 
denote probability observation policy simply function mapping observations probability distributions controls observation distribution controls denote probability control observation randomized policy observation distribution corresponds markov chain state transitions generated selecting observation state baxter bartlett distribution selecting control distribution generating transition state probability parameterize chains parameterize policies function set parameters observation markov chain corresponding state transition matrix equation implies algorithm introduces algorithm observable markov decision process modified form algorithm updates note algorithm require knowledge transition probability matrix observation process requires knowledge randomized policy 
essentially algorithm proposed kimura 
reward baseline 
algorithm assumes policy function current observation 
immediate algorithm works finite history observations 
general optimal policy needs function entire observation history 
extended apply policies internal state aberdeen baxter 
algorithm algorithm 
parameterized class randomized policies satisfying assumption 
partially observable markov decision process controlled randomized policies corresponds parameterized class markov chains satisfying assumption 
arbitrary unknown starting state observation sequence generated controls generated randomly reward sequence satisfying assumption hidden sequence states markov decision process 
set 
observation control subsequent reward policy gradient estimation convergence algorithm need replace assumption similar bound gradient assumption 
derivatives exist ratios uniformly bounded theorem 
assumptions algorithm starting initial state generate sequence satisfying proof 
proof follows lines proof theorem 
case expectation respect stationary distribution process defined control process observation process 
result follows arguments proof theorem 
control dependent rewards circumstances rewards may depend controls example controls may consume energy may wish add penalty term reward function order conserve energy 
simplest way deal define state expected reward baxter bartlett redefine terms expectation trajectories 
performance gradient approximated due fact satisfies bellman equations replaced take account dependence controls fifth line replaced straightforward extend proofs theorems setting 
parameter dependent rewards possible modify rewards depend directly 
case fifth line replaced convergence approximation theorems carry provided uniformly bounded 
parameter dependent rewards considered glynn tsitsiklis baird moore 
particular baird moore showed suitable choices lead combination value policy search 
example approximate value function setting usual reward discount factor gives update seeks minimize expected bellman error effect minimizing bellman error driving system policy states small bellman error 
motivation approach understood considers zero bellman error states 
case greedy policy derived optimal regardless actual policy parameterized expectation zero gradient computed kind update known actor critic algorithm barto policy playing role actor value function playing role critic 

rewards depend current previous state substantially alter analysis 
policy gradient estimation extensions infinite state observation control spaces convergence proof algorithm relied finite state observation control spaces 
clear modification algorithm applied immediately countably uncountably infinite countable changes kernel density observations 
addition appropriate interpretation applied uncountable specifically subset probability density function density subsets euclidean space finite set theorem extended show estimates produced algorithm converge surely 
fact prove general result implies case densities subsets finite case theorem 
allow general spaces satisfying topological assumption 
definitions see example dudley 
assumption 
control space associated topology separable hausdorff countable 
corresponding borel algebra generated topology finite measure defined measurable space say measure similarly observation space topology borel algebra measure satisfying conditions 
case theorem finite associated measure counting measure 
measure lebesgue measure 
assume distributions absolutely continuous respect measures corresponding radon nikodym derivatives probability masses finite case densities euclidean case satisfy assumption 
assumption 
probability measure absolutely continuous respect measure probability measure absolutely continuous respect measure 
measure derivatives exist ratios bounded assumptions replace algorithm radon nikodym derivative respect measure case convergence result 
generalizes theorem applies densities euclidean space theorem 
suppose control space observation space satisfy assumption measure control space consider algorithm baxter bartlett replaced assumptions algorithm starting initial state sequence satisfying generate proof 
see appendix 
new results version extended new settings proved new properties algorithm 
section briefly outline results 
multiple agents single agent generating actions suppose multiple agents parameter set distinct observation environment generate actions policy agents receive reward signal may cooperating solve task example applied collective obtained concatenating observations controls parameters single vectors respectively 
easy calculation shows gradient estimate generated collective case precisely obtained applying agent independently concatenating results 
estimate produced applied agent 
leads line algorithm agents adjust parameters independently explicit communication collectively adjustments maximizing global average reward 
similar observations context see peshkin 

algorithm gives biologically plausible synaptic weight update rule applied networks spiking neurons neurons regarded independent agents bartlett baxter shown promise network routing application tao baxter weaver 
policies internal states far considered purely reactive memoryless policies chosen control function current observation 
easily extended cover case policies depend finite histories observations general optimal control policy function entire observation history 
fortunately observation history may summarized form belief state current distribution states updated current observation knowledge sufficient optimal behaviour smallwood sondik sondik 
extension policies parameterized internal belief states described aberdeen baxter similar spirit extension described meuleau 

policy gradient estimation higher order derivatives generalized compute estimates second higher order derivatives average reward assuming exist single sample path underlying see second order derivatives observe density performance measure denotes matrix second derivatives hessian 
verified second term right hand side outer product matrix entries 
sequence states visits recurrent state parameterized markov chain recall section combined yields squared terms expression outer products 
expression derive algorithm computing biased estimate hessian involves maintaining addition usual eligibility trace second matrix trace updated follows time steps algorithm returns average far second term outer product 
computation higher order derivatives second order gradient methods optimization policy parameters 
bias variance bounds theorem provides bound bias relative applies underlying markov chain distinct eigenvalues 
extended result arbitrary markov chains bartlett baxter 
extra generality comes price bound involves number states chain theorem 
supplies proof variance scales providing formal justification interpretation terms bias variance trade 

general algorithm computing arbitrarily accurate approximations gradient average reward parameterized markov chain 
chain transition matrix distinct eigenvalues accuracy approximation shown controlled baxter bartlett size subdominant eigenvalue 
showed algorithm modified apply partially observable markov decision processes controlled parameterized stochastic policies discrete continuous control observation state spaces 
finite state case proved convergence probability algorithms 
briefly described extensions multi agent problems policies internal state estimating higher order derivatives generalizations bias result chains non distinct eigenvalues new variance result 
avenues research 
continuous time results follow extensions results 
algorithms applied countably uncountably infinite state spaces convergence results needed cases 
companion baxter experimental results showing rapid convergence estimates generated true gradient 
give line variants algorithms variants gradient ascent estimates 
experimental results showing effectiveness algorithms variety problems including state mdp nonlinear physical control problem call admission problem 
supported australian research council benefited comments anonymous referees 
research performed authors research school information sciences engineering australian national university 
appendix simple example policy degradation value function learning approximate value function approaches reinforcement minimizing form error approximate value function true value function 
long known may necessarily lead improved policy performance new value function 
include appendix illustrates phenomenon occur simplest possible system state provides geometric intuition phenomenon arises 
consider state markov decision process mdp 
controls corresponding transition probability matrices takes system state probability regardless starting state state probability opposite 
state reward state optimal policy select action policy stationary distribution states infinite horizon discounted value state discount value expectation state sequences state transitions generated solving bellman equations yields policy gradient estimation state markov process suppose trying learn approximate value function state scalar feature dimensionality ensure really approximate 
parameter learnt 
greedy policy obtained optimal value state state purposes illustration choose negative 
temporal difference learning popular techniques training approximate value functions sutton barto 
shown linear functions converges parameter minimizing expected squared loss stationary distribution van roy substituting previous expressions optimal policy solving yields values wrong sign 
situation optimal policy implementable greedy policy approximate value function class just choose observing optimal policy converge value function corresponding greedy policy implements suboptimal policy 
geometrical illustration occurs shown 
points graph represent values states 
scales state state axes weighted respectively 
way squared euclidean distance graph points corresponds expectation stationary distribution squared difference values value function shaded region corresponding greedy policy optimal value functions rank state state 
bold line represents set realizable approximate value functions solution approximate value function projecting point corresponding true value function line 
illustrated 
projection suboptimal weighted mean squared distance value function space take account policy boundary 
appendix proof theorem proof needs topological lemma 
definitions see example dudley pp 

baxter bartlett legend optimal policy approximate value function plot value function space state system 
note scale axis weighted square root stationary probability corresponding state optimal policy 
solution td simply projection true value function set approximate value functions 
lemma 
topological space hausdorff separable countable 
borel algebra generated measurable space sequence sets satisfies conditions 
partition distinct elements empty intersection 

proof 
separable countable dense subset 
countable neighbourhood base 
construct partitions countable set follows 
define policy gradient estimation clearly measurable partition 
hausdorff pair distinct points pair disjoint open sets dense pair contains neighbourhoods disjoint 
sufficiently large fall distinct elements partition 
true pair follows reverse inclusion trivial 
measurability singletons follows measurability fact shall lemma result show approximate expectations certain random variables single sample path markov chain 
lemma 
measurable space satisfying conditions lemma suitable sequence partitions lemma 
probability measure defined space 
absolutely integrable function 
event define unique element containing proof 
clearly signed finite measure defined absolutely continuous respect equation defines radon nikodym derivative respect 
derivative defined see example gurevich section 
radon nikodym theorem dudley theorem expressions equal 

proof 
theorem 
definitions absolutely continuous respect measure write baxter bartlett depend integral obtain absolutely integrable differentiate avoid cluttering notation shall denote distribution denote distribution 
notation probability measure generated 
write notation lemma define measurable set notice sequence partitions lemma denote element containing lemma policy gradient estimation assumption lebesgue dominated convergence theorem interchange integral limit 
probabilities expectations respect stationary distribution andthe distributions random process inside expectation asymptotically stationary ergodic 
ergodic theorem surely easy see double limit exists order reversed argument proof theorem shows tails uniformly bounded 
follows required 
ignored aberdeen baxter 

policy gradient learning controllers internal state 
tech 
rep australian national university 


stochastic 
engineering cybernetics 
baird moore 

gradient descent general reinforcement learning 
advances neural information processing systems 
mit press 
baxter bartlett bartlett baxter 

hebbian synaptic modifications spiking neurons learn 
tech 
rep research school information sciences engineering australian national university 
csl anu edu au bartlett papers nov ps gz 
bartlett baxter 

estimation approximation bounds gradient reinforcement learning 
journal computer systems sciences 
invited special issue colt 
barto sutton anderson 

neuronlike adaptive elements solve difficult learning control problems 
ieee transactions systems man cybernetics smc 
baxter bartlett weaver 

experiments infinite horizon policy gradient estimation 
journal artificial intelligence research 
appear 
baxter tridgell weaver 

learning play chess temporal differences 
machine learning 
bertsekas tsitsiklis 

neuro dynamic programming 
athena scientific 
bertsekas 

dynamic programming optimal control vol ii 
athena scientific 
breiman 

probability 
addison wesley 
cao wan 

algorithms sensitivity analysis markov chains potentials perturbation realization 
ieee transactions control systems technology 
dudley 

real analysis probability 
wadsworth brooks cole belmont california 
glynn 

stochastic approximation monte carlo optimization 
proceedings winter simulation conference pp 

glynn 

likelihood ratio gradient estimation stochastic systems 
communications acm 
glynn 

likelihood ratio gradient estimation regenerative stochastic recursions 
advances applied probability 
ho cao 

perturbation analysis discrete event dynamic systems 
kluwer academic boston 
jaakkola singh jordan 

reinforcement learning algorithm partially observable markov decision problems 
tesauro touretzky leen 
eds advances neural information processing systems vol 

mit press cambridge ma 
kimura kobayashi 

analysis actor critic algorithms eligibility traces reinforcement learning imperfect value functions 
fifteenth international conference machine learning pp 

policy gradient estimation kimura kobayashi 

reinforcement learning continuous action stochastic gradient ascent 
intelligent autonomous systems ias pp 

kimura miyazaki kobayashi 

reinforcement learning pomdps function approximation 
fisher 
ed proceedings fourteenth international conference machine learning icml pp 

kimura kobayashi 

reinforcement learning stochastic hill climbing discounted reward 
proceedings twelfth international conference machine learning icml pp 

tsitsiklis 

actor critic algorithms 
neural information processing systems 
mit press 
lancaster 

theory matrices 
academic press san diego ca 
tsitsiklis 

simulation optimization markov reward processes 
tech 
rep mit 
meuleau peshkin kaelbling kim 

policy policy search 
tech 
rep mit intelligence laboratory 
meuleau peshkin kim kaelbling 

learning finite state controllers partially observable environments 
proceedings fifteenth international conference uncertainty artificial intelligence 
peshkin kim meuleau kaelbling 

learning cooperate policy search 
proceedings sixteenth international conference uncertainty artificial intelligence 
weiss 

sensitivity analysis likelihood ratios 
proceedings winter simulation conference 
weiss 

sensitivity analysis simulations likelihood ratios 
operations research 
rubinstein 

problems monte carlo optimization 
ph thesis 
rubinstein 

optimize complex stochastic systems single sample path score function method 
annals operations research 
rubinstein 

decomposable score function estimators sensitivity analysis optimization queueing networks 
annals operations research 
rubinstein melamed 

modern simulation modeling 
wiley new york 
rubinstein shapiro 

discrete event systems 
wiley new york 
samuel 

studies machine learning game checkers 
ibm journal research development 
baxter bartlett gurevich 

integral measure derivative unified approach 
prentice hall englewood cliffs singh jaakkola jordan 

learning state estimation partially observable markovian decision processes 
proceedings eleventh international conference machine learning 
singh bertsekas 

reinforcement learning dynamic channel allocation cellular telephone systems 
advances neural information processing systems proceedings conference pp 

mit press 
smallwood sondik 

optimal control partially observable markov decision processes finite horizon 
operations research 
sondik 

optimal control partially observable markov decision processes infinite horizon discounted costs 
operations research 
sutton barto 

reinforcement learning 
mit press cambridge ma 
isbn 
sutton mcallester singh mansour 

policy gradient methods reinforcement learning function approximation 
neural information processing systems 
mit press 
tao baxter weaver 

multi agent policy gradient approach network routing 
tech 
rep australian national university 
tesauro 

practical issues temporal difference learning 
machine learning 
tesauro 

td gammon self teaching backgammon program achieves master level play 
neural computation 
van roy 

analysis temporal difference learning function approximation 
ieee transactions automatic control 
williams 

simple statistical gradient algorithms connectionist reinforcement learning 
machine learning 
zhang dietterich 

reinforcement learning approach job shop scheduling 
proceedings fourteenth international joint conference artificial intelligence pp 

morgan kaufmann 

