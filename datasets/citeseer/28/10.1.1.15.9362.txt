support vector networks cortes vladimir vapnik labs research usa 
support vector network new learning machine group classification problems 
machine conceptually implements idea input vectors non linearly mapped high dimension feature space 
feature space linear decision surface constructed 
special properties decision surface ensures high generalization ability learning machine 
idea network previously implemented restricted case training data separated errors 
pattern input space compared support vectors 
resulting values nonlinearly transformed 
linear function transformed values determine output classifier 
optimal hyperplane generalizes technical problem treat high dimensional feature space remains 
shown order operations constructing decision function interchanged making non linear transformation input vectors followed dot products support vectors feature space compare vectors input space dot product distance measure non linear transformation value result :10.1.1.103.1189
see 
enables construction rich classes decision surfaces example polynomial decision surfaces arbitrarily degree 
call type learning machine support vectors network technique support vector networks developed restricted case separating training data errors 
article extend approach support vector networks cover separation error training vectors name emphasize crucial idea expanding solution support vectors learning machines 
means ff oe linearity dot product implies classification function unknown vector depends dot products oe delta ff oe delta oe idea constructing support vectors networks comes considering general forms dot product hilbert space oe delta oe hilbert schmidt theory symmetric function expanded form oe delta oe oe eigenvalues oe du oe integral operator defined kernel 
sufficient condition ensure defines dot product feature space eigenvalues expansion positive 
guarantee coefficients positive necessary sufficient theorem condition satisfied du functions satisfy theorem dot products 
consider convolution dot product feature space function form exp gamma ju gamma vj oe call potential functions 
convolution dot product feature space function satisfying condition particular construct polynomial classifier degree dimensional input space function delta different dot products construct different learning machines arbitrarily types decision surfaces :10.1.1.103.1189
decision surface machines form ff image support vector input space ff weight support vector feature space 
find vectors weights ff follow solution scheme original optimal margin classifier soft margin classifier 
difference matrix determined uses matrix ij general features support vector networks constructing decision rules support vector networks efficient construct support vector networks decision rule solve quadratic optimization problem gamma ffi simple constraints ffi matrix ij determined elements training set function determining convolution dot products 
solution optimization problem efficiently solving intermediate optimization problems determined training data currently constitute support vectors 
see text 
degree raw support dimensionality polynomial error vectors feature space theta theta theta theta theta table results obtained dot products polynomials various degree 
number support vectors mean value classifier 
problem hand 
effect smoothing database pre processing support vector networks investigated :10.1.1.103.1189
experiments chose smoothing kernel gaussian standard deviation oe agreement :10.1.1.103.1189
experiments database constructed polynomial indicator functions dot products form 
input dimensionality order polynomial ranged 
table describes results experiments 
degree raw support dimensionality polynomial error vectors feature space theta theta theta theta theta table results obtained dot products polynomials various degree 
number support vectors mean value classifier 
problem hand 
effect smoothing database pre processing support vector networks investigated :10.1.1.103.1189
experiments chose smoothing kernel gaussian standard deviation oe agreement :10.1.1.103.1189
experiments database constructed polynomial indicator functions dot products form 
input dimensionality order polynomial ranged 
table describes results experiments 
training data linearly separable 
classification multivariate normal distributions different covariance matrices 
ann 
math 
stat 
boser guyon vapnik :10.1.1.103.1189
training algorithm optimal margin classifiers 
proceedings fifth annual workshop computational learning theory volume pages pittsburg 
acm 
bottou cortes denker drucker guyon jackel lecun simard vapnik miller 
