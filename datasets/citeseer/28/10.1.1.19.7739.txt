implicit skills explicit knowledge bottom model skill learning ron sun edward merrill todd peterson department university missouri columbia columbia mo usa university alabama usa presents skill learning model clarion 
different existing models high level skill learning top approach turning declarative knowledge procedural knowledge practice adopt bottom approach low level skill learning procedural knowledge develops declarative knowledge develops 
model formed integrating connectionist reinforcement symbolic learning methods perform line reactive learning 
adopts level dual representation framework sun combination localist distributed representation :10.1.1.19.5799
compare model human data minefield navigation task demonstrating match model human data respects 
cognitive science society rights reserved 
consistent known modularity claim fodor smith similar idea multitude action systems competing 
learning procedural knowledge captured number different ways 
learning setting correct input output provided straight backpropagation supervised learning algorithm network 
supervised learning procedures require priori determination uniquely correct output input 
learning setting input output mapping externally provided reinforcement learning ai sense developed sutton watkins :10.1.1.48.6005
preferred skill learning uniquely correct action feedback usually available 
reinforcement learning measure goodness action payoff reinforcement signal ranging say extremely extremely bad possibilities 
adjustment weights increase chance selecting actions receive positive reinforcement reduce chance selecting actions receive negative reinforcement 
process local weight updating 
pair wise relations needed subjects explicit learning inducing explicit rule 
mathews 
essentially demonstrated point 
may possible explanations findings plausible differences reflect contrast separate learning processes 
differences representation learning declarative procedural knowledge lead naturally level architectures see sun sun consist main components top level encodes explicit declarative knowledge bottom level encodes implicit procedural knowledge neural networks prescribed :10.1.1.19.5799:10.1.1.19.5799
existing theories support level architecture see anderson bower reber logan posner snyder sun :10.1.1.19.5799:10.1.1.19.5799
relative contributions levels learning performance may manipulated experimentally degree 
performance affected agent influenced engage explicit processing 
instance individual forced explicit top level mechanisms may engaged performance may enhanced extent unaffected depending circumstances effect may similar deeper level processing see stanley willingham smith heath squire sun 
mathews 
essentially demonstrated point 
may possible explanations findings plausible differences reflect contrast separate learning processes 
differences representation learning declarative procedural knowledge lead naturally level architectures see sun sun consist main components top level encodes explicit declarative knowledge bottom level encodes implicit procedural knowledge neural networks prescribed :10.1.1.19.5799:10.1.1.19.5799
existing theories support level architecture see anderson bower reber logan posner snyder sun :10.1.1.19.5799:10.1.1.19.5799
relative contributions levels learning performance may manipulated experimentally degree 
performance affected agent influenced engage explicit processing 
instance individual forced explicit top level mechanisms may engaged performance may enhanced extent unaffected depending circumstances effect may similar deeper level processing see stanley willingham smith heath squire sun 
individual forced overly explicit top level mechanisms may fully engaged bottom level mechanisms may hampered observed reber see section details performance may sun cognitive science reber berry broadbent see section 
learning difference different learning methods levels learning online levels 
bottom learning sufficient priori knowledge available learning bottom 
separability relative contributions levels learning performance experimentally manipulated studied dual task verbalization manipulation 
separate instance episodic memory 
hypotheses form basic model sun sun :10.1.1.46.8335:10.1.1.37.7799
see fig 

implementation model 
high level algorithm describes model ion stands connectionist learning adaptive rule induction line follows 
set threshold threshold threshold 
effect dual task mainly top level activities see sun cognitive science stadler nissen macleod effect captured clarion increasing rule learning thresholds activities occur top level 
rule construction threshold threshold increased rule expansion threshold threshold 
effect regular verbalization stanley smith heath sun stem explicit rule learning activities lesser extent previous episodes instances 
model reduced rule learning thresholds encourage rule learning instance memory replay sutton :10.1.1.48.6005
rule construction threshold threshold reduced rule expansion threshold threshold 
instance memory size encountered step probability entered memory random replacement step randomly selected step instance memory training agents 
capture effect verbalization hypothesized earlier verbalization caused agent engage overly rule learning activities effect switched completely explicit mode learning result engagement top level reber berry broadbent 
hypothesis adopted clarion encourage rule learning top level reduced rule construction threshold threshold rule expansion threshold threshold threshold threshold 
obvious difference compared bottom approach skill learning models designed learning involves small number features dimensions deal sequential skills obvious way 
approach remote resemblance ai rule learning concept formation example id quinlan aq michalski 
batch algorithms directly usable line bottom learning require teacher input available 
addition perform temporal credit assignment handle sequential skill learning tasks 
incremental unsupervised learning models cobweb fisher differ approach sun cognitive science sparse feedback available may delayed usually complete description instance base decisions partial observability :10.1.1.138.7286
models combining multiple heterogeneous learning algorithms techniques supervised concept learning combining decision trees backpropagation networks algorithms 
approaches bear similarity aimed different tasks cognitively motivated 
see sun peterson comparisons 
cognitive architectures 
arguments distributed representation needed modeling explicit processes see mcclelland adopted distributed representation modeling implicit learning explicit learning 

hypothesis instance memory may enhance learning provides additional opportunities 

terms simplicity performance learning best similar reinforcement learning methods lin sun :10.1.1.75.7884
learning convergence proofs watkins 

dimension ordinal discrete continuous nominal 
discussion focus ordinal values nominal values handled similarly 
attempt model response time 
current interface simulation software allow model response time gordon 

model deal problem high dimensionality 
result lookup table implementation learning bottom level possible tesauro lin :10.1.1.75.7884
functional approximator backpropagation network 
setting comparable complexity important reinforcement learning applications ai tesauro mahadevan connell 

tried discrete analog input encoding 
