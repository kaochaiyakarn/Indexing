advances neural information processing systems pp 
mit press policy gradient methods reinforcement learning function approximation richard sutton david mcallester satinder singh mansour labs research park avenue florham park nj function approximation essential reinforcement learning standard approach approximating value function determining policy far proven theoretically intractable 
explore alternative approach policy explicitly represented function approximator independent value function updated gradient expected reward respect policy parameters 
williams reinforce method actor critic methods examples approach 
main new result show gradient written form suitable estimation experience aided approximate action value advantage function 
result prove time version policy iteration arbitrary differentiable function approximation convergent locally optimal policy 
large applications reinforcement learning rl require generalizing function approximators neural networks decision trees instance methods 
dominant approach decade value function approach function approximation effort goes estimating value function action selection policy represented implicitly greedy policy respect estimated values policy selects state action highest estimated value 
value function approach worked applications limitations 
oriented finding deterministic policies optimal policy stochastic selecting different actions specific probabilities see singh jaakkola jordan 
second arbitrarily small change estimated value action cause selected 
discontinuous changes identified key obstacle establishing convergence assurances algorithms value function approach bertsekas tsitsiklis 
example learning sarsa dynamic programming methods shown unable converge policy simple mdps simple function approximators gordon baird tsitsiklis van roy bertsekas tsitsiklis 
occur best approximation step changing policy notion best mean squared error sense slightly different senses residual gradient temporal difference dynamic programming methods 
explore alternative approach function approximation rl 
approximating value function compute deterministic policy approximate stochastic policy directly independent function approximator parameters 
example policy represented neural network input representation state output action selection probabilities weights policy parameters 
denote vector policy parameters performance corresponding policy average reward step 
policy gradient approach policy parameters updated approximately proportional gradient positive definite step size 
achieved usually assured converge locally optimal policy performance measure 
value function approach small changes cause small changes policy state visitation distribution 
prove unbiased estimate gradient obtained experience approximate value function satisfying certain properties 
williams reinforce algorithm finds unbiased estimate gradient assistance learned value function 
reinforce learns slowly rl methods value functions received relatively little attention 
learning value function reduce variance gradient estimate appears essential rapid learning 
jaakkola singh jordan proved result similar special case function approximation corresponding tabular pomdps 
result strengthens theirs generalizes arbitrary differentiable function approximators 
tsitsiklis prep 
independently developed result 
see baxter bartlett prep 
tsitsiklis 
result suggests way proving convergence wide variety algorithms actor critic policy iteration architectures barto sutton anderson sutton kimura kobayashi 
take step direction proving time version policy iteration general differentiable function approximation convergent locally optimal policy 
baird moore obtained weaker superficially similar result vaps family methods 
policy gradient methods vaps includes separately parameterized policy value functions updated gradient methods 
vaps methods climb gradient performance expected long term reward measure combining performance accuracy 
result vaps converge locally optimal policy case weight put value function accuracy case vaps degenerates reinforce 
similarly gordon fitted value iteration convergent value find locally optimal policy 
policy gradient theorem consider standard reinforcement learning framework see sutton barto learning agent interacts markov decision process mdp :10.1.1.32.7692
state action reward time 
denoted st rt respectively 
environment dynamics characterized state transition probabilities pa ss pr st st expected rewards rt st agent decision making procedure time characterized policy pr st parameter vector 
assume respect parameter exists 
usually write just 
function approximation ways formulating agent objective useful 
average reward formulation policies ranked long term expected reward step lim rn limt pr st stationary distribution states assume exists independent policies 
average reward formulation value state action pair policy defined rt second formulation cover designated start state care long term reward obtained 
give results apply formulation definitions rt rt st discount rate allowed episodic tasks 
formulation define discounted weighting states encountered starting tpr st 
result concerns gradient performance metric respect policy parameter theorem policy gradient 
mdp average reward start state formulations 
proof see appendix 
way expressing gradient discussed average reward formulation tsitsiklis related expression terms state value function due jaakkola singh jordan cao chen 
extend results start state formulation provide simpler direct proofs 
williams theory reinforce algorithms viewed implying 
event key aspect expressions gradient terms form effect policy changes distribution states appear 
convenient approximating gradient sampling 
example sampled distribution obtained unbiased estimate course normally known estimated 
approach actual returns rt rt rt rt start state formulation approximation st 
leads rt st st corrects oversampling actions preferred known follow expected value williams 
williams episodic reinforce algorithm st policy gradient approximation consider case approximated learned function approximator 
approximation sufficiently hope place point roughly direction gradient 
example jaakkola singh jordan proved special case function approximation arising tabular pomdp assure positive inner product gradient sufficient ensure improvement moving direction 
extend result general function approximation prove equality gradient 
fw approximation parameter natural learn fw updating rule wt st st fw st fw st st unbiased fw st estimator st rt 
process converged local optimum fw fw 
theorem policy gradient function approximation 
fw satisfies compatible policy parameterization sense fw fw 
proof combining gives fw tells error fw orthogonal gradient policy parameterization 
expression zero subtract policy gradient theorem yield fw 
fw fw application deriving algorithms advantages policy parameterization theorem derive appropriate form value function parameterization 
example consider policy gibbs distribution linear combination features sa sb tsitsiklis personal communication points fw linear features righthand side may way satisfy condition 
sa dimensional feature vector characterizing state action pair meeting compatibility condition requires fw sa sb natural parameterization fw fw sa sb words fw linear features policy normalized mean zero state 
algorithms easily derived variety nonlinear policy parameterizations multi layer backpropagation networks 
careful reader noticed form fw requires zero mean state fw sense better think fw approximation advantage function baird convergence requirement really fw get relative value actions correct state absolute value variation state state 
results viewed justification special status advantages target value function approximation rl 
fact generalized include arbitrary function state added value function approximation 
example generalized fw arbitrary function 
follows immediately choice affect theorems substantially affect variance gradient estimators 
issues entirely analogous reinforcement baselines earlier williams dayan sutton 
practice presumably set best available approximation results establish approximation process proceed affecting expected evolution fw 
convergence policy iteration function approximation theorem prove time form policy iteration function approximation convergent locally optimal policy 
theorem policy iteration function approximation 
fw differentiable function approximators policy value function respectively satisfy compatibility condition max 
sequence defined step size sequence 
mdp bounded rewards wk fw fw converges 
proof theorem assures update direction gradient 
bounds mdp rewards assure bounded 
step size requirements necessary conditions apply proposition page bertsekas tsitsiklis assures convergence local optimum 
authors wish martha steenstrup precup comments michael kearns insights notion optimal policy function approximation 
baird 

advantage updating 
wright lab 
technical report wl tr 
baird 

residual algorithms reinforcement learning function approximation 
proc 
twelfth int 
conf 
machine learning pp 

morgan kaufmann 
baird moore 

gradient descent general reinforcement learning 
nips 
mit press 
barto sutton anderson 

neuronlike elements solve difficult learning control problems 
ieee trans 
systems man cybernetics 
baxter bartlett 
prep 
direct gradient reinforcement learning gradient estimation algorithms 
bertsekas tsitsiklis 

neuro dynamic programming 
athena scientific 
cao chen 

perturbation realization potentials sensitivity analysis markov processes ieee trans 
automatic control 
dayan 

reinforcement comparison 
touretzky elman sejnowski hinton eds connectionist models proceedings summer school pp 

morgan kaufmann 
gordon 

stable function approximation dynamic programming 
proceedings twelfth int 
conf 
machine learning pp 

morgan kaufmann 
gordon 

chattering sarsa 
cmu learning lab technical report 
jaakkola singh jordan 
reinforcement learning algorithms partially observable markov decision problems nips pp 

morgan kaufman 
kimura kobayashi 

analysis actor critic algorithms eligibility traces reinforcement learning imperfect value functions 
proc 
icml pp 

tsitsiklis 
prep 
actor critic algorithms 
tsitsiklis 
simulation optimization markov reward processes technical report lids massachusetts institute technology 
singh jaakkola jordan 

learning state estimation partially observable markovian decision problems 
proc 
icml pp 

sutton 

temporal credit assignment reinforcement learning 
ph thesis university massachusetts amherst 
sutton barto 

reinforcement learning 
mit press 
tsitsiklis van roy 

feature methods large scale dynamic programming 
machine learning 
williams 

theory reinforcement learning connectionist systems 
technical report nu ccs northeastern university college computer science 
williams 

simple statistical gradient algorithms connectionist reinforcement learning 
machine learning 
appendix proof theorem prove theorem average reward formulation formulation 
def ss ss summing sides stationary distribution stationary ss 
start state formulation def ss pr ss ss steps unrolling pr probability going state state steps policy 
immediate rt pr 


