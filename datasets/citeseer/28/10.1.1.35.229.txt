naive bayes exemplar approaches word sense disambiguation revisited gerard german 
describes experimental comparison standard supervised learning methods naive bayes exemplar classification word sense disambiguation wsd problem 
aim twofold 
firstly attempts contribute clarify confusing information comparison methods appearing related literature 
doing directions explored including testing modifications basic learning algorithms varying feature space 
secondly improvement algorithms proposed order deal large attribute sets 
resolving ambiguity words central problem language understanding applications associated tasks including instance machine translation information retrieval hypertext navigation parsing speech synthesis spelling correction resolution automatic text summarization wsd important open problems natural language processing nlp field 
despite wide range approaches investigated large effort devoted tackle problem fact date large scale broad coverage highly accurate word sense disambiguation system built 
successful current lines research corpus approach statistical machine learning ml algorithms applied learn statistical models classifiers corpora order perform wsd 
generally supervised approaches learn previously semantically annotated corpus obtained better results unsupervised methods small sets selected highly ambiguous words artificial pseudo words 
standard ml algorithms supervised learning applied bayesian learning exemplar learning decision lists neural networks research center software department technical university barcelona email lsi upc es mooney provides comparative experiment restricted domain previously cited methods including decision trees rule induction algorithms :10.1.1.14.5763:10.1.1.75.2005:10.1.1.75.2005:10.1.1.11.5417:10.1.1.14.4304:10.1.1.14.9674:10.1.1.125.3683
despite results obtained limited domains supervised methods suffer lack widely available semantically tagged corpora construct really broad coverage systems 
known knowledge acquisition bottleneck 
ng estimates manual annotation effort necessary build broad coverage semantically annotated corpus :10.1.1.11.5417
extremely high overhead supervision additionally serious overhead learning testing commonly algorithms scaling real size wsd problems explain supervised methods seriously questioned 
generally supervised approaches learn previously semantically annotated corpus obtained better results unsupervised methods small sets selected highly ambiguous words artificial pseudo words 
standard ml algorithms supervised learning applied bayesian learning exemplar learning decision lists neural networks research center software department technical university barcelona email lsi upc es mooney provides comparative experiment restricted domain previously cited methods including decision trees rule induction algorithms :10.1.1.14.5763:10.1.1.75.2005:10.1.1.75.2005:10.1.1.11.5417:10.1.1.14.4304:10.1.1.14.9674:10.1.1.125.3683
despite results obtained limited domains supervised methods suffer lack widely available semantically tagged corpora construct really broad coverage systems 
known knowledge acquisition bottleneck 
ng estimates manual annotation effort necessary build broad coverage semantically annotated corpus :10.1.1.11.5417
extremely high overhead supervision additionally serious overhead learning testing commonly algorithms scaling real size wsd problems explain supervised methods seriously questioned 
due fact works focused reducing acquisition cost need supervision corpus methods wsd 
consequently lines research currently studied design efficient example sampling methods lexical resources wordnet www search engines automatically obtain internet accurate arbitrarily large word sense samples unsupervised em algorithms estimating statistical model parameters :10.1.1.14.13:10.1.1.14.1677:10.1.1.14.4304:10.1.1.44.6158:10.1.1.125.3683:10.1.1.125.3683
belief body particular second line provide evidence opening acquisition bottleneck near 
known knowledge acquisition bottleneck 
ng estimates manual annotation effort necessary build broad coverage semantically annotated corpus :10.1.1.11.5417
extremely high overhead supervision additionally serious overhead learning testing commonly algorithms scaling real size wsd problems explain supervised methods seriously questioned 
due fact works focused reducing acquisition cost need supervision corpus methods wsd 
consequently lines research currently studied design efficient example sampling methods lexical resources wordnet www search engines automatically obtain internet accurate arbitrarily large word sense samples unsupervised em algorithms estimating statistical model parameters :10.1.1.14.13:10.1.1.14.1677:10.1.1.14.4304:10.1.1.44.6158:10.1.1.125.3683:10.1.1.125.3683
belief body particular second line provide evidence opening acquisition bottleneck near 
reason worth investigating application supervised ml methods wsd thoroughly comparing existing alternatives 
comments related unfortunately direct comparisons alternative methods wsd 
commonly stated naive bayes neural networks exemplar learning represent state art accuracy supervised wsd :10.1.1.14.1677:10.1.1.11.5417:10.1.1.14.9674:10.1.1.125.3683
consequently lines research currently studied design efficient example sampling methods lexical resources wordnet www search engines automatically obtain internet accurate arbitrarily large word sense samples unsupervised em algorithms estimating statistical model parameters :10.1.1.14.13:10.1.1.14.1677:10.1.1.14.4304:10.1.1.44.6158:10.1.1.125.3683:10.1.1.125.3683
belief body particular second line provide evidence opening acquisition bottleneck near 
reason worth investigating application supervised ml methods wsd thoroughly comparing existing alternatives 
comments related unfortunately direct comparisons alternative methods wsd 
commonly stated naive bayes neural networks exemplar learning represent state art accuracy supervised wsd :10.1.1.14.1677:10.1.1.11.5417:10.1.1.14.9674:10.1.1.125.3683
regarding comparison naive bayes exemplar methods works mooney ng ones basically referred :10.1.1.11.5417:10.1.1.14.9674:10.1.1.14.9674
mooney shows bayesian approach clearly superior exemplar approach 
explicitly said accuracy naive bayes points higher example algorithm slightly accuracy frequent sense classifier obtain 
exemplar approach algorithm applied classifying new examples standard neighbour nn hamming distance measuring closeness 
belief body particular second line provide evidence opening acquisition bottleneck near 
reason worth investigating application supervised ml methods wsd thoroughly comparing existing alternatives 
comments related unfortunately direct comparisons alternative methods wsd 
commonly stated naive bayes neural networks exemplar learning represent state art accuracy supervised wsd :10.1.1.14.1677:10.1.1.11.5417:10.1.1.14.9674:10.1.1.125.3683
regarding comparison naive bayes exemplar methods works mooney ng ones basically referred :10.1.1.11.5417:10.1.1.14.9674:10.1.1.14.9674
mooney shows bayesian approach clearly superior exemplar approach 
explicitly said accuracy naive bayes points higher example algorithm slightly accuracy frequent sense classifier obtain 
exemplar approach algorithm applied classifying new examples standard neighbour nn hamming distance measuring closeness 
example weighting attribute weighting applied set number attributes said 
mooney shows bayesian approach clearly superior exemplar approach 
explicitly said accuracy naive bayes points higher example algorithm slightly accuracy frequent sense classifier obtain 
exemplar approach algorithm applied classifying new examples standard neighbour nn hamming distance measuring closeness 
example weighting attribute weighting applied set number attributes said 
second compares naive bayes approach pe bls sophisticated exemplar learner especially designed dealing examples symbolic features :10.1.1.35.8359:10.1.1.35.8359
shows large number nearest neighbours performance algorithms comparable cross validation parameter setting pebls slightly outperforms naive bayes 
noted comparison carried limited setting features attribute example weighting facilities provided pebls 
author suggests poor results obtained mooney due metric associated nn algorithm test mvdm metric pebls superior standard hamming distance 
surprising result appears ng accuracy results obtained higher reported author year running exactly algorithm data larger richer set attributes :10.1.1.14.4304
second compares naive bayes approach pe bls sophisticated exemplar learner especially designed dealing examples symbolic features :10.1.1.35.8359:10.1.1.35.8359
shows large number nearest neighbours performance algorithms comparable cross validation parameter setting pebls slightly outperforms naive bayes 
noted comparison carried limited setting features attribute example weighting facilities provided pebls 
author suggests poor results obtained mooney due metric associated nn algorithm test mvdm metric pebls superior standard hamming distance 
surprising result appears ng accuracy results obtained higher reported author year running exactly algorithm data larger richer set attributes :10.1.1.14.4304
apparently paradoxical difference attributed author feature pruning process performed older 
apart contradictory results obtained previous papers methodological drawbacks comparisons pointed 
hand ng applies algorithms broad coverage corpus reports accuracy results single testing experiment providing statistical tests significance 
hand mooney performs thorough rigorous experiments compares alternative methods limited domain consisting single word reduced set senses 
basic methods naive bayes naive bayes classifier classical setting 
cm different classes set feature values test example 
naive bayes method tries find class maximizes 
assuming independence features goal algorithm stated arg max arg max estimated training process relative frequencies 
avoid effects zero counts estimating conditional probabilities model simple smoothing technique proposed ng :10.1.1.11.5417:10.1.1.14.9674
consists replacing zero counts number training examples 
method referred nb 
exemplar approach basic implementation examples stored memory classification new example nn algorithm uses hamming distance measure closeness doing examples examined 
greater resulting sense majority sense nearest neighbours 
variant consists ranking attributes relevance making contribute distance calculation weight proportional importance 
attribute weighting done rlm distance measure 
measure belonging distance information families attribute selection functions showed better performance alternatives experiment decision tree induction pos tagging 
variant referred modifications put resulting algorithm referred investigated effect alternative metric 
ffl modified value difference metric mvdm proposed cost salzberg allows making graded guesses match different symbolic values :10.1.1.35.8359
values attribute mvdm distance defined jp jv gammap jv fi fi fi gamma fi fi fi number classes number training examples value vx attribute classified class training corpus nx number training examples value vx attribute class 
variant referred algorithm example weighting facility version incorporating attribute weighting discussed plan address 
setting corpus experiments approaches evaluated semantically annotated corpus containing occurrences nouns verbs corresponding frequent ambiguous english words 
corpus collected ng colleagues available linguistic data consortium ldc experiments group words nouns verbs frequently appear wsd literature selected :10.1.1.14.4304
ffl modified value difference metric mvdm proposed cost salzberg allows making graded guesses match different symbolic values :10.1.1.35.8359
values attribute mvdm distance defined jp jv gammap jv fi fi fi gamma fi fi fi number classes number training examples value vx attribute classified class training corpus nx number training examples value vx attribute class 
variant referred algorithm example weighting facility version incorporating attribute weighting discussed plan address 
setting corpus experiments approaches evaluated semantically annotated corpus containing occurrences nouns verbs corresponding frequent ambiguous english words 
corpus collected ng colleagues available linguistic data consortium ldc experiments group words nouns verbs frequently appear wsd literature selected :10.1.1.14.4304
words described left hand side table 
goal acquire classifier word row represents classification problem 
number classes senses ranges number training examples ranges 
mfs column table show percentage frequent sense word accuracy naive frequent sense classifier obtain 
nouns verbs attributes sets attributes referred seta respectively 
gamma gamma gamma context consecutive words word disambiguated 
attributes refer context way 
ffl seta contains attributes gamma gamma gamma gamma gamma correspond collocations consecutive words 
attributes exactly represent local context ambiguous word proven informative features wsd :10.1.1.11.5417:10.1.1.14.9674
note attribute refers position falls boundaries sentence certain example default value assigned attribute 
part speech tag word cm unordered set open class words appearing sentence 
examples consisting full sentence ambiguous word appears tagged set labels corresponding minor changes senses wordnet 
ldc address ldc cis upenn edu ffl enriches local context gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma part speech information gamma gamma gamma additionally incorporates broad context information cm intended represent realistic set attributes wsd note attributes binary valued denoting presence absence content word sentence context 
note seta constant number attributes number depends concrete word ranges 
experiments comparison algorithms performed series controlled experiments exactly training test sets method 
experimental methodology consisted fold cross validation 
accuracy error rate figures appearing averaged results folds 
statistical tests significance performed fold cross validation paired student test confidence value :10.1.1.37.3325
exemplar algorithms run times different number nearest neighbours results corresponding best choice reported seta table shows results methods variants tested words seta set attributes frequent sense mfs naive bayes nb exemplar hamming distance variants th th columns approach mvdm metric variants th th columns included 
best result word printed boldface 
figures drawn ffl methods significantly outperform mfs classifier 
ffl referring variants performs significantly better confirming results ng values greater needed order achieve performance nn approach :10.1.1.11.5417:10.1.1.14.9674
statistical tests significance performed fold cross validation paired student test confidence value :10.1.1.37.3325
exemplar algorithms run times different number nearest neighbours results corresponding best choice reported seta table shows results methods variants tested words seta set attributes frequent sense mfs naive bayes nb exemplar hamming distance variants th th columns approach mvdm metric variants th th columns included 
best result word printed boldface 
figures drawn ffl methods significantly outperform mfs classifier 
ffl referring variants performs significantly better confirming results ng values greater needed order achieve performance nn approach :10.1.1.11.5417:10.1.1.14.9674
additionally example weighting attribute weighting significantly improve combination achieves additional improvement 
ffl mvdm metric superior hamming distance 
accuracy eb cs significantly higher variant 
unfortunately weighted examples lead improvement case 
ffl mvdm metric superior hamming distance 
accuracy eb cs significantly higher variant 
unfortunately weighted examples lead improvement case 
drawback mvdm metric computational overhead introduced calculation 
table shows times faster seta fact incorporates attributes exception morphology target word verb object syntactic relation :10.1.1.14.4304
order construct real nn system wsd parameter estimated cross validation training set case cross validation inside cross validation involved testing process generate prohibitive overhead :10.1.1.11.5417:10.1.1.14.9674
current programs implemented perl run sun ultrasparc machine mb ram 
table 
results algorithms set words seta 
accuracy eb cs significantly higher variant 
unfortunately weighted examples lead improvement case 
drawback mvdm metric computational overhead introduced calculation 
table shows times faster seta fact incorporates attributes exception morphology target word verb object syntactic relation :10.1.1.14.4304
order construct real nn system wsd parameter estimated cross validation training set case cross validation inside cross validation involved testing process generate prohibitive overhead :10.1.1.11.5417:10.1.1.14.9674
current programs implemented perl run sun ultrasparc machine mb ram 
table 
results algorithms set words seta 
accuracy word pos mfs nb eb eb eb eb eb eb cs eb cs eb cs age art car child church cost fall head interest know line set speak take avg 
avoid drawback alternative representation attributes proposed successfully tested 
furthermore representation improves efficiency algorithms large set attributes 
test corpus allows estimate realistic scenario best tradeoff performance computational requirements achieved positive exemplar algorithm set attributes hamming distance example weighting 
research algorithms carried near includes study behaviour respect number training examples study robustness presence highly redundant attributes testing algorithms alternative sense tagged corpora automatically acquired web 
cost salzberg weighted nearest neighbor algorithm learning symbolic features machine learning :10.1.1.35.8359
dietterich approximate statistical tests comparing supervised classification learning algorithms neural computation :10.1.1.37.3325
duda hart pattern classification scene analysis wiley 
engelson dagan minimizing manual annotation cost supervised training corpora connectionist statistical symbolic approaches learning natural language processing eds riloff wermter scheler lecture notes artificial intelligence springer 
fujii tokunaga tanaka selective sampling example word sense disambiguation computational linguistics 
furthermore representation improves efficiency algorithms large set attributes 
test corpus allows estimate realistic scenario best tradeoff performance computational requirements achieved positive exemplar algorithm set attributes hamming distance example weighting 
research algorithms carried near includes study behaviour respect number training examples study robustness presence highly redundant attributes testing algorithms alternative sense tagged corpora automatically acquired web 
cost salzberg weighted nearest neighbor algorithm learning symbolic features machine learning :10.1.1.35.8359
dietterich approximate statistical tests comparing supervised classification learning algorithms neural computation :10.1.1.37.3325
duda hart pattern classification scene analysis wiley 
engelson dagan minimizing manual annotation cost supervised training corpora connectionist statistical symbolic approaches learning natural language processing eds riloff wermter scheler lecture notes artificial intelligence springer 
fujii tokunaga tanaka selective sampling example word sense disambiguation computational linguistics 
gale church yarowsky method disambiguating word senses large corpus computers humanities 
moldovan automatic method generating sense tagged corpora proceedings th national conference artificial intelligence 
aaai press 
miller fellbaum gross miller papers wordnet special issue international journal lexicography 
miller leacock semantic concordance proceedings arpa human language technology 
mooney comparative experiments disambiguating word senses illustration role bias machine learning proceedings st conference empirical methods natural language processing emnlp :10.1.1.14.9674
ng exemplar base word sense disambiguation improvements proceedings nd conference empirical methods natural language processing emnlp :10.1.1.11.5417:10.1.1.14.9674
ng getting serious word sense disambiguation proceedings workshop :10.1.1.11.5417
ng lee integrating multiple knowledge sources disambiguate word sense exemplar proceedings th annual meeting association computational linguistics :10.1.1.14.4304
acl 
aaai press 
miller fellbaum gross miller papers wordnet special issue international journal lexicography 
miller leacock semantic concordance proceedings arpa human language technology 
mooney comparative experiments disambiguating word senses illustration role bias machine learning proceedings st conference empirical methods natural language processing emnlp :10.1.1.14.9674
ng exemplar base word sense disambiguation improvements proceedings nd conference empirical methods natural language processing emnlp :10.1.1.11.5417:10.1.1.14.9674
ng getting serious word sense disambiguation proceedings workshop :10.1.1.11.5417
ng lee integrating multiple knowledge sources disambiguate word sense exemplar proceedings th annual meeting association computational linguistics :10.1.1.14.4304
acl 
pedersen bruce knowledge lean word sense disambiguation proceedings th national conference artificial intelligence :10.1.1.14.4304
miller fellbaum gross miller papers wordnet special issue international journal lexicography 
miller leacock semantic concordance proceedings arpa human language technology 
mooney comparative experiments disambiguating word senses illustration role bias machine learning proceedings st conference empirical methods natural language processing emnlp :10.1.1.14.9674
ng exemplar base word sense disambiguation improvements proceedings nd conference empirical methods natural language processing emnlp :10.1.1.11.5417:10.1.1.14.9674
ng getting serious word sense disambiguation proceedings workshop :10.1.1.11.5417
ng lee integrating multiple knowledge sources disambiguate word sense exemplar proceedings th annual meeting association computational linguistics :10.1.1.14.4304
acl 
pedersen bruce knowledge lean word sense disambiguation proceedings th national conference artificial intelligence :10.1.1.14.4304
aaai press 
miller leacock semantic concordance proceedings arpa human language technology 
mooney comparative experiments disambiguating word senses illustration role bias machine learning proceedings st conference empirical methods natural language processing emnlp :10.1.1.14.9674
ng exemplar base word sense disambiguation improvements proceedings nd conference empirical methods natural language processing emnlp :10.1.1.11.5417:10.1.1.14.9674
ng getting serious word sense disambiguation proceedings workshop :10.1.1.11.5417
ng lee integrating multiple knowledge sources disambiguate word sense exemplar proceedings th annual meeting association computational linguistics :10.1.1.14.4304
acl 
pedersen bruce knowledge lean word sense disambiguation proceedings th national conference artificial intelligence :10.1.1.14.4304
aaai press 
towell voorhees disambiguating highly ambiguous words computational linguistics :10.1.1.75.2005
ng exemplar base word sense disambiguation improvements proceedings nd conference empirical methods natural language processing emnlp :10.1.1.11.5417:10.1.1.14.9674
ng getting serious word sense disambiguation proceedings workshop :10.1.1.11.5417
ng lee integrating multiple knowledge sources disambiguate word sense exemplar proceedings th annual meeting association computational linguistics :10.1.1.14.4304
acl 
pedersen bruce knowledge lean word sense disambiguation proceedings th national conference artificial intelligence :10.1.1.14.4304
aaai press 
towell voorhees disambiguating highly ambiguous words computational linguistics :10.1.1.75.2005
yarowsky decision lists lexical ambiguity resolution application accent restoration spanish french proceedings nd annual meeting association computational linguistics pp :10.1.1.14.5763:10.1.1.75.2005
las cruces nm 
ng lee integrating multiple knowledge sources disambiguate word sense exemplar proceedings th annual meeting association computational linguistics :10.1.1.14.4304
acl 
pedersen bruce knowledge lean word sense disambiguation proceedings th national conference artificial intelligence :10.1.1.14.4304
aaai press 
towell voorhees disambiguating highly ambiguous words computational linguistics :10.1.1.75.2005
yarowsky decision lists lexical ambiguity resolution application accent restoration spanish french proceedings nd annual meeting association computational linguistics pp :10.1.1.14.5763:10.1.1.75.2005
las cruces nm 
acl 

acl 
pedersen bruce knowledge lean word sense disambiguation proceedings th national conference artificial intelligence :10.1.1.14.4304
aaai press 
towell voorhees disambiguating highly ambiguous words computational linguistics :10.1.1.75.2005
yarowsky decision lists lexical ambiguity resolution application accent restoration spanish french proceedings nd annual meeting association computational linguistics pp :10.1.1.14.5763:10.1.1.75.2005
las cruces nm 
acl 

