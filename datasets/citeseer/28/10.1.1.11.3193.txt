lexical chains text summarization regina mathematics computer dept ben university beer israel cs ac 
investigate produce summary original text full seman interpretation relying model topic text derived lex chains new com chains text merging robust knowledge sources wordnet thesaurus part speech tagger shallow parser cation nominal groups algo rithm hearst summarization proceeds steps text seg mented constructed strong chains sentences extracted text tins empirical results strong chains sentences summarization ts process condensing source text shorter version preserving reformation content serve goals survey analysis field notes general text producing summary arbitrary text challenge full understanding text lm decide text worth reading naturally produce tins gate method production summaries text jones summarization step process building source text source summary generation summary representation source representation step output summary text framework reformation included source representation order create summary michael elhadad computer dept ben beer israel cs ac xl elhadad types source text reformation tlc domain text aspects chosen source represen summaries deep semantic anal source text example investigate ways produce ent summary texts describing event semantic representation source texts available case muc style systems interpret source texts early systems luhn source formation frequent words represent concepts text approach source representation frequency table text words tins representation abstracts text words contrast extreme source representation full semantic representa tion text reducing simple frequency table deal tins issue pro summary arbitrary text re full understanding able knowledge sources goal find middle ground source representation rich quality indicative summaries easy extract source text text harm source representation trivial illustration con sider sequences dr sn controls rate wh ch ana ss blood dr doctor spent years research dr appears sequences ii ii sequence ts ch sequence doctor example zf source representation supply semantically re lated terms capture text summary capture point original text norton introduced hasan captures part different parts text cohesion related terms means frequent type discussed sion created semantically related words hasan cohesion category category synonyms hyponyms relation words tend occur lex contexts works teacher school collocation relations ld cat identifiable surface oi text occurs terms sequences related words called ez cal chains chains provide struc text chains retrieval correction st appear lem cal source representation summarization str source text captured re lated coherence coherence defines macro level semantic structure connected dls course cohesion creates connectedness non structural manner coherence represented terms coherence ous text segments cla cause researchers ono structure encoded rst mann thompson source clearly thin representation ms ex question com contrast coherence cult fy complete understand mg text complex ad criteria differ ent consider example hobbs john open safe mt show relation sentences interpreted ng text knowledge close course structure related words tend occur text surface struc ture identify struc ture connect paragraph markers tense shifts investigate chains model source text pur pose producing summary obviously source text need integrated text representation produce summaries want investigate far go exploiting mainly chains rest algorithm lex chain construct results strong chains dates produced algorithm describe chains identify sentences source text eventually produce algorithm chain computing advantages zt able chains computation computational model chains define terms categories index entries pointers roget thesaurus evaluated relatedness criterion covered created ing new text word related chain relatedness criteria introduce notion chain returns take account occurrences related words analyze fac tors contributing strength chain rep density length hn st algorithm machine readable roget thesaurus drawbacks approach word appear ruth sense ts occurrences tt belong chain semantically ambiguous words lead mixing senses furniture array note choosing appropriate chain word tins word context known fl cult problem text un algorithms calculation chains hirst st wordnet database de relatedness words miller senses wordnet database repre sented synonym sets synsets sets words sharing com mon sense example senses computer represented calculator es computer person computes computer data processor computer reformation processing system wordnet contains word forms words category semantic rela tions synonymy hyponymy polysemous words appear syn sets example occurs synsets words wordnet polysemous noted fig ure proportion wordnet nouns latin labels biological en titles nature experience news report texts processed ts half nouns encountered polysemous generally procedure constructing ch follows steps select set date words word find chain relying relatedness cute members chains insert word chain update example procedure represented st preprocessor step words appear noun entry word net chosen relatedness words xs terms distance occurrences shape path connecting wordnet thesaurus kinds relation de extra strong word tts rep strong words connected wordnet synsets words longer paths satisfying certain restrictions ac distance related words de kind extra strong rela stance strong rela window sentences strong relations sentences back find chain insert word extra strong preferred strong relations preferred strong relations chain word inserted appropriate sense senses words ing chain updated word connected new word chain relates selected senses new chain created date word ts inserted possible senses wordnet greedy strategy implemented algorithm example person invented machine uses micro computers control rate blood machines new hu device uses micro computers closer mg pump ug algorithm chain word created lex kr sense belongs synset word person related tins chain sense human ng relation chain contains entries lex sense lex person sense person nd dual man algorithm processes word relates hen wordnet sense person chosen sense words machine person related strong relation tins case machine ts wrong way tins occurrence machine strong dence supporting common sense macro computer point correct sense tins context performs zn performance tins example greedy decision order choose right sense word chain text propose develop chaining model alternatives word senses choose best tins method exam ii ii ii ii ii pie node word created lex sense kr date word person senses person pronouns verb forms person choice sense person chain world interpretations shown step interpretations define component list interpretations exclusive component words influence selection tive senses candidate word re lated word component ate new component single word senses mach machine sense ic ent person related senses person influences selection senses machine component picture component shown ff continue process insert wor ds micro pump nu greatly increases strongest interpretations figures assumption text define best interpretation tion connections edges graph tins case second interpretation step selected predicts right sense machine define score interpretation sum chain scores chain deter mined number weight relations tween chain members fixed weight synonym antonym gorithm develops possible interpretations main self contradiction number possible interpretations larger certain threshold prune weak tions tins criteria select component strongest interpretation pe step tmr marina machine step interpretation individual ne step step interpret non step interpretations :10.1.1.15.205
algorithm differs gorithm introduces addition re criterion members chain non greedy heuristic select senses chain members differ major criterion selection candidate words operative text unit choose candidate words simple nouns noun compounds mentioned nouns main contributors text noun synsets dominate wordnet rely nouns candidate words algorithm rely results part speech tagging algorithm nouns go step select tokens happen occur nouns wordnet addition extend set candidate words include compound eval noun compounds account noun compounds wordnet entries wordnet noun compounds sea level lms pc computer step interpretation step interpretation digital productive system noun comp domain new noun compounds tions wordnet play major role issue shallow parser developed ido dagan team bar ilan um identify noun compounds characterization noun sequences tins major benefits important concepts domain example text tum computing token noun com pound computing wordnet words occur ere dates chain member example quantum computing selected word selected beneficial tins example text quantum computers noun compound selected relatedness criterion wordnet mg head noun quantum computer related machine computer second algorithm operative notion text text segments ob tained hearst algorithm text segmentation hearst chains segment relatedness criteria second stage merge chains segments stronger criteria connectedness chains merged segment boundary contain common word sense segment relatedness criterion strict members synsets related node graph related graph related length path threshold relation text segmentation lex chain derived partially common source knowledge repetitions fact chains serve algorithm tation hearst algorithm behaves type texts checked effectively chains construction building summaries lexical chains investigate chains serve source representation original text summary question build sum mary representation tins source representation prevalent topic play important role summary lex cal chains central topic text ate measure strength show picking concepts represented strong chains better central text frequent words text forms zero hypothesis example show appendix sample text network technology concept network represented words network occurrences ct system ruth summary representa tion reflect words represent concept summary gen eration stage extract information separately term chain representation approach completely problem tl terms occur chain reflects represent concept ad argument chain representa tion opposed word frequency model case concept represented number words relatively low bayesian network sample text con represented words datum concept model tins text :10.1.1.15.205
important concept computer occurs times combines number occurrences members overcome weight single word computer scoring chains order chains outlined identify strongest chains produced algorithm frequent summarization formal way evaluate chain strength formal method evaluate summary quality fore rely methodology developed compute graph chains evaluate capture topics texts shows chains visualized help human testers evaluate importance visual representa collected data set texts extracted popular scientific american popular science genre text ally ranked chains terms relevance computed different formal measures including length text text span covered chain density graph topology diameter graph words number results data set indicate parameters predictors strength length number occurrences members chain homogeneity index number distract occurrences divided length score function chains score chain length ty ranking score evaluated strong strength criterion core cha results con experience texts analyzed ex erent methods score function results plan extending formal methods determine scoring function average number strong chains selected selection method texts words average words words mum originally generated average strongest sample text represented appendix extracting significant sentences strong chains selected step summarization algorithm extract full sen original text chain investigated alternatives step heuristic chain summary rep resentation choose sentence contains appearance chain member text heuristic produced summary text shown appendix steve heard planning huge internet offering local entertainment ma gates hu concerns advantage re bayesian networks pl body knowledge area mapping cause effect key encoding numbers repr ent ss ect programmed computers systems auto 
generate optimal pred decisions key pieces tn hired eric david erman jack development systems colleague field surprised problem tins approach words chain reflect concept extent example ai chain chain token ts related con aa words af eid suitable represent topic ai con text text chain members representatives topic contribute defined criterion evaluate member represent chain frequency occurrence experimentally words call words frequency chain average word frequency chain example third chain representative words field ai heuristic defined second notion representative words chain summary representation choose sentence contains appear ance representative chain member text special case heuristic gives result heuristic topic number places text chain dl text text unit global topic central topic focus segment try identify extract sentences related topic segment successive segments characterize text cluster suc segments high density chain mere heuristic approach chain find text chain highly concentrated extract sentence chain appearance tins central con computed number chain mem occurrences segment number nouns segment chain high ff chains cluster successive segments segment contains chain members note sentence extracted chain regardless strength texts tested second tech niques produce results output second bet tex generally second produces best summary checked methods texts data set surprisingly tic intuition predicts cated gives indicative results tins may due factors criteria cen clustering may insufficient problem related text structure third heuristics tends extract sentences middle text extract sentences places text single chain complete results experiments cs 
ac za limitations identified problems 
