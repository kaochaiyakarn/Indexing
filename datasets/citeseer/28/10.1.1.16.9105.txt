ieee transactions neural networks vol 
march constructive neural network learning algorithms pattern classification parekh member ieee yang member ieee vasant honavar member ieee constructive learning algorithms offer attractive approach incremental construction near minimal neural network architectures pattern classification 
help overcome need ad hoc inappropriate choices network topology algorithms search suitable weights priori fixed network architectures 
algorithms proposed literature shown converge zero classification errors certain assumptions tasks involve learning binary binary mapping classification problems involving binary valued input attributes output categories 
constructive learning algorithms real real extend pyramid tiling algorithms respectively learning real ary mappings classification problems involving real valued input attributes multiple output classes 
prove convergence algorithms empirically demonstrate applicability practical pattern classification problems 
estimation expected case complexity learning task practical learning problems known computationally hard solve 
little known expected case complexity problems encountered successfully solved living systems primarily difficult mathematically characterize properties problems 
constructive algorithms successful provide useful empirical estimates expected case complexity practical learning problems 
parekh constructive neural network learning algorithms pattern classification tradeoffs performance measures different constructive learning algorithms allow trading certain performance measures learning time network size generalization accuracy 
incorporation prior knowledge constructive algorithms provide natural framework incorporating problem specific knowledge initial network configurations modifying knowledge additional training examples :10.1.1.48.2048:10.1.1.51.3055
lifelong learning research lifelong learning proposed training networks learn solve multiple related problems applying knowledge acquired simpler problems learn difficult ones 
constructive learning algorithms lend lifelong learning framework 
network domain knowledge simpler task built architecture explicitly setting values connection weights training form building block system constructively learns difficult tasks 
network pruning network pruning offers approach dynamically determining appropriate network topology 
alternatively algorithms cascade correlation family construct multilayer networks structural interconnections hidden neurons allow net approximate complex functions relatively simple neuron transfer functions sigmoid 
pattern classification special case function approximation function output restricted discrete values classes involves real ary function mapping 
neural network solving classification problems typically input neurons output neurons 
th output neuron trained output output neurons trained output zero patterns belonging th class 
clearly class constructive algorithms implement general real real mapping adapted pattern classification see example :10.1.1.53.2895
special class constructive learning algorithms designed closely match unique demands pattern classification 
sufficient output neuron binary valued output zero individual neurons implement simple threshold hard limiting activation function outputs zero continuous activation function sigmoid 
threshold neurons offer advantages continuous counterparts potentially easier implement hardware 
second perceptron learning rule simple iterative procedure training threshold neurons 
threshold neurons offer advantages continuous counterparts potentially easier implement hardware 
second perceptron learning rule simple iterative procedure training threshold neurons 
learning rules sigmoid neurons complicated computationally expensive 
third threshold functions clearly described terms simple rules 
easier incorporate domain expertise usually available form rules network threshold neurons :10.1.1.51.3055
similar argument suggests task extracting learned knowledge network threshold neurons considerably simpler 
focus constructive learning networks threshold neurons pattern classification 
constructive learning iterative weight update number algorithms incrementally construct networks threshold neurons learning binary binary mapping proposed literature example tower pyramid tiling oil spot sequential algorithms 
algorithms differ terms choices regarding restrictions input representation binary bipolar valued inputs add neuron add neuron connectivity added neuron weight initialization added neuron train added neuron subnetwork affected addition 
constructive neural network learning algorithms capable handling multiple output categories real valued pattern attributes 
exploiting geometric properties constructive learning class constructive learning algorithms focus trains individual neurons iterative weight update strategy perceptron rule 
class constructive learning algorithms shot learning strategy deserves mention 
algorithms exploit geometric properties training patterns directly shot determine appropriate weights neurons added network 
grow learn gal algorithm distal algorithm construct single hidden layer network implements kind nearest neighbor classification scheme :10.1.1.30.3522
hidden neuron exemplar representing group patterns belong class close terms suitably chosen distance metric 
minimizing resources method method voronoi diagram approach idea partitioning input space constructing linear hyperplanes 
hidden layer neurons trained partition input space homogeneous regions region contains patterns belonging single output class 
output layer neurons combine regions represent output class 
remainder organized follows section ii gives overview elementary concepts describes notation 
sections iii iv describe real real constructive learning algorithms respectively prove convergence 
section illustrates practical applicability algorithms section vi concludes discussion directions research 
framework general potentially applied entire class constructive algorithms pattern classification 
interested reader referred application framework tower perceptron cascade sequential learning algorithms :10.1.1.16.9105
ii 
preliminaries threshold logic units input threshold logic unit tlu known perceptron elementary processing unit computes threshold hard limiting function weighted sum inputs 
output tlu weights weight referred threshold bias response pattern notational convenience prefix pattern denote tlu output hard limiting function perceptron learning rule input tlu implements dimensional hyperplane partitions dimensional euclidean pattern space defined coordinates regions classes 
tlu function category classifier 
convergence proof hidden layer contains master neurons ancillary neurons trained achieve faithful representation patterns 
subset training set pattern belonging outputs exactly 
designate output vector prototype patterns belong exactly class desired output prototype faithful representation patterns representation observed output patterns desired output patterns correctly classified 
said algorithm convergence proved parts show possible obtain faithful representation training set real valued attributes layer 
earlier version algorithm appeared :10.1.1.16.9105
fig 

real network example dataset 
table ii response layer neurons table iii response layer neurons prove additional layer number classification errors reduced 
seen table ix test accuracy real real algorithms slightly worse single layer network case segmentation dataset real performs better 
suggests case pima vehicle datasets constructive learning algorithms add value 
possible datasets contain irrelevant noisy attributes unduly complicate learning task 
experiments shown genetic algorithm feature selection scheme significantly improves generalization performance distal constructive learning algorithm 
experiments shown choice algorithm training individual tlu constructive learning significantly impact convergence generalization properties constructive learning algorithms :10.1.1.16.9105:10.1.1.16.9105
shown thermal perceptron algorithm replaced algorithms correction procedure pocket algorithm modification algorithm training individual tlu performance constructive learning algorithms certain datasets superior terms convergence properties generalization ability 
definitely interest explore impact feature subset selection choice different tlu weight training algorithms performance constructive algorithms 
unfortunately issues scope 
parekh constructive neural network learning algorithms pattern classification issue network training times critical large training sets 
focused family algorithms incrementally construct feedforward networks threshold neurons 
number algorithms proposed literature limited category pattern classification tasks binary bipolar valued input attributes 
constructive learning algorithms real real extend pyramid tiling algorithms respectively handle classification patterns attributes 
algorithms provided rigorous proofs convergence zero classification errors finite training sets 
proof technique sufficiently general see application technique constructive learning algorithms :10.1.1.16.9105
convergence algorithms established showing modification network topology guarantees existence weights reduce classification error assuming exists weight modification algorithm find weights 
rigorous proof graceful variants perceptron learning algorithms practice satisfy requirements imposed find optimal suitable constructive algorithms proposed incremental construction recurrent neural networks rnn learn finite state automata labeled examples 
interested reader referred discussion constructive learning rnn 
sense term yield minimal networks set weights 
hard determine priori constructive learning algorithms suitable particular problem recommend real algorithm preliminary analysis tends better convergence properties real algorithm practice 
directions research include 
evaluating performance constructive learning algorithms systematic experimental theoretical comparisons constructive algorithms neural network machine learning algorithms pattern classification interest 
characterization inductive representational biases different algorithms guide users selecting algorithms specific problems easily measurable properties datasets 
hybrid constructive learning algorithms related shown choice specific tlu weight training algorithm significant impact performance constructive learning algorithms :10.1.1.16.9105
study hybrid network training schemes dynamically select appropriate network construction strategy appropriate tlu weight training algorithm appropriate output computation strategy obtain locally optimal performance step classification task worth pursuing 
combining constructive learning feature selection generalization performance learning algorithms improved help suitable feature selection techniques 
feature subset selection algorithms proposed pattern recognition literature 
effectiveness genetic algorithms feature subset selection conjunction distal algorithm demonstrated 
boosting error correcting output codes improved generalization advances machine learning resulted development techniques boosting ieee transactions neural networks vol 
march error correcting output codes improving generalization capability learning algorithms 
application techniques constructive learning framework clearly interest 
knowledge extraction trained constructive neural networks constructive neural network learning algorithms successfully theory refinement 
available domain specific knowledge incorporated initial network topology refined additional labeled examples constructive learning :10.1.1.48.2048:10.1.1.51.3055
question existing strategies see example design suitable new methods extracting learned knowledge trained constructive network 
gal networks grow learn shrink forget int 
comput 
sci 
int 
joint conf 
neural networks ijcnn ak pp 

parekh yang honavar constructive neural network learning algorithms real valued pattern classification dept comput :10.1.1.16.9105
sci iowa state univ tech 
rep cs tr 
pruning strategies constructive neural network learning algorithms proc 
ieee int conf 
pruning strategies constructive neural network learning algorithms proc 
ieee int conf 
neural networks icnn pp 

empirical comparison performance single layer algorithms training threshold logic units neural parallel sci :10.1.1.16.9105
comput published 
correction procedure fast method learning threshold units proc 
vol 
washington july pp 
syst 
special issue feature transformation subset selection vol 
pp 

yang parekh honavar constructive learning algorithm multi category pattern classification proc :10.1.1.16.9105
world 
neural networks san diego ca pp 

distal inter pattern distance constructive learning algorithm intell :10.1.1.30.3522
yang parekh honavar constructive learning algorithm multi category pattern classification proc :10.1.1.16.9105
world 
neural networks san diego ca pp 

distal inter pattern distance constructive learning algorithm intell :10.1.1.30.3522
data anal vol 
pp 

yang parekh honavar data driven theory refinement kbdistal proc 
intell 
data anal 
ida amsterdam netherlands pp 

yeung constructive neural networks estimators bayesian discriminant functions pattern recognition vol :10.1.1.53.2895
pp 

parekh received degree computer technology bombay india ph degrees computer science specialization artificial intelligence iowa state university ames respectively 
currently data mining group research planning center menlo park ca 
