detecting emotions speech thomas polzin alex waibel school computer science carnegie mellon university pittsburgh pa usa fakultat fur informatik university karlsruhe germany 
human language carries various kinds information 
human computer interaction detection emotional state speaker re ected utterances crucial 
investigation explore acoustic prosodic information detect emotional state speaker 
show prosodic information combined integrated acoustic information hidden markov model architecture allows observations rate appropriate phenomena modeled 
architecture demonstrate prosodic information adds discriminative power system 
keywords multimodal communication user modelling emotion detection human language carries various kinds information 
merely considering strings words regard manner spoken important aspects utterance 
additional information carry cues underlying emotional state speaker encoded acoustic level scherer 
scherer 
human computer interaction system ability sense emotional state speaker dialogue crucial reasons 
depending emotional state speaker true sentence uttered meaning di erent meaning derived considering words uttered system allocate special resources process input system output behavior adapt 
research attempts detect emotions visual cues kaiser scherer appear kaiser takeuchi nagao usage acoustic cues detection emotions speech widely explored frick dellaert 

focus research date emphasized synthesis emotional speech murray arnott 
studying emotions speech needs distinctions emotional attitude speaker hearer emotional attitude speaker message emotional state speaker halliday 
investigation focus emotional state speaker 
widely studied psychology psycho linguistics attempt determine acoustic prosodic features speaking rate intonation intensity encode emotional state speaker scherer 

investigation show acoustic prosodic information detect underlying emotional state speaker 
section describe acoustic information combined prosodic information integrated hidden markov model hmm architecture additional suprasegmental states allow dynamic observation rates depending consider acoustic prosodic information 
suprasegmental hidden markov model architecture permits observations time scale appropriate phenomena modeled 
property introduce prosodic information processing 
sect 
describe experiments conducted system detect emotional state speaker 
experiments corpus asked drama students utter sentences pretending emotional states happy sad angry afraid neutral 
conclude investigation summary describe extensions system intend incorporate near discuss potential architecture integrate information di erent modalities 
suprasegmental hidden markov models suprasegmental hidden markov models permit summarization states hidden markov model call suprasegmental state 
suprasegmental states allow consideration observation sequence spanned constituent states suprasegmental states look observation sequence larger window 
suprasegmental states allow observations rates appropriate phenomena intended model 
example prosodic information observed rate acoustic modeling 
prosodic information applies example syllables words phrases observed time window ms time frame acoustic events usually looked 
application acoustic events modeled conventional hidden markov states prosodic events phone syllable word utterance level modeled suprasegmental states 
basic idea fig 

combine acoustic suprasegmental information formula log acoustic model suprasegmental model speech signal log acoustic model speech signal log model speech signal time leave suprasegmental state phone syllable add log probability suprasegmental state respective suprasegmental observations speech signal log probability current acoustic model respective acoustic observations speech signal 
weight factor determined empirically 
details theory see polzin appear 
suprasegmental observations suprasegmental states capture prosodic properties phones syllables words utterances allow observations time scale suitable prosodic phenomena 
suprasegmental observations comprise information duration respective segment information fundamental frequency pitch intensity 
possible determine intensity fundamental frequency time point speech signal information far meaningful observe intensity fundamental frequency duration say syllable word 
observation fig 

suprasegmental hidden markov model 
hidden markov states form suprasegmental state phone 
states form di erent suprasegmental state 
suprasegmental states constitute suprasegmental state syllable 
transition probabilities hidden markov states represented aij indicates state leaving state going 
transition probabilities suprasegmental states represented bij denotes suprasegmental state leaving denotes suprasegmental state going 
allows derivation additional observations mean variance correlation intensity fundamental frequency intensity fundamental frequency steady falling rising segment question 
note conventional hmm observations constant rate possible look dynamic behavior intensity fundamental frequency course syllable word time want observe acoustic events smaller time scale 
choice suprasegmental observations re ect issues 
principle observations computed possible segmentation viterbi 
order get acceptable run time behavior computing observations unreasonably computationally expensive 

observations robust respect noise idiosyncrasies speakers 
experiments corpus hand generated sentences corpus 
sentences comprised questions statements orders 
sentence length varied words mean sentence length words 
corpus comprised word tokens types 
asked drama students pronounce sentences emotional label square brackets sentence computer screen 
students asked portray sentences emotional mood happy sad angry afraid 
addition asked neutral pronunciation sentences 
maximum sentences student 
hmd hmd microphones recordings 
recording system gradient model sampling rate khz 
recordings transcribed hand 
human performance conducted small informal experiment determine human performance detecting underlying emotional state speaker 
subjects listen utterances speaker played back random order 
task subject choose emotion happy sad angry afraid 
human performance accuracy 
note baseline random guessing 
baseline experiments janus speech recognition system 
trained independently di erent corpus spontaneous speech english spontaneous scheduling task 
word accuracy wa corpus 
recognition system determine uence emotional speech accuracy 
resulting word accuracy table 
word accuracy dropped emotions angry compared neutral pronunciation 
table 
word accuracy depending emotional state speaker percent emotion happy afraid angry sad neutral wa training training similar training conventional hmms 
addition necessary train suprasegmental models top acoustic models 
investigation derived emotion dependent models emotion dependent acoustic suprasegmental models 
example word di erent suprasegmental word models happy sad angry afraid 
corpus training acoustic suprasegmental models 
rest corpus testing 
testing underlying emotional state determined way 
utterance recognized emotion independent recognition system described sect 

emotion dependent recognition system system emotion dependent acoustic suprasegmental models looked highest probability sentence recognized step produced emotion dependent models forced alignment speech signal sentence models af stand happy sad afraid angry respectively 
tested emotions obtained probabilities emotion 

probabilities returned step compared 
took highest probability indicative actual emotional state speaker maximized emotional state arg max fh af ang speech signal sentence models experiment acoustic models 
rst developed emotion dependent acoustic models obtain emotion dependent speech recognition systems 
models determined emotional state speaker procedure outlined section emotional state arg max fh af ang speech signal sentence acoustic models emotion detection accuracy table 
emotion detection accuracy table 
emotion detection accuracy percent emotion dependent acoustic models emotion happy afraid angry sad emotion accuracy 
acoustic models enabled system detect correct emotional state chance level 
high accuracy underlying emotion utterances spoken angry state recognized correlate high word accuracy utterances table 
experiment suprasegmental models 
starting emotion dependent acoustic models included emotion dependent suprasegmental models see prosodic information add discriminative power system 
uence suprasegmental information probability computation regulated factor mentioned sect 

determined empirically independent development set 
emotion dependent acoustic suprasegmental models detected emotional state procedure described sect 
emotional state arg max fh af ang speech signal sentence acoustic models suprasegmental models resulting emotion detection accuracy table 
emotion detection accuracy amounts absolute improvement 
particular prosodic information appears help detection happy sad 
table 
emotion detection accuracy percent emotion dependent suprasegmental models emotion happy afraid angry sad emotion accuracy investigation shows acoustic prosodic information combined integrated hmm speech recognition system suprasegmental states 
acoustic prosodic information system nearly achieves human performance levels trying detect emotional state speaker 
show prosodic information essential reliable detection underlying emotional state speaker absolute improvement 
test system capabilities detect emotional state speaker certainly test additional real world data 
intend apply techniques human telephone conversations 
choice words probably indication speaker emotional state 
straightforward extension system emotion dependent language models 
extension comes mind combine audio visual information 
combination proved quite useful speech recognition information lip movements combined acoustic information successfully reduced word error rate system bregler 
stiefelhagen 
consider suitable tool integrate observations di erent modalities allow observation rate appropriate modality 
demonstrated prosodic information processing architecture 
approach conceivable integrate additional modalities gestures facial expressions heartbeat body temperature 
architecture allows consideration modality appropriate rate synchronizes observations modalities 
example develop word models access information dynamic behavior fundamental frequency pitch intensity accompanying hand gestures facial expressions 
bregler hild waibel 
improving connected letter recognition lip reading 
icassp minneapolis 
ieee int 
conf 
acoustics speech signal processing 
dellaert polzin waibel 
recognizing emotions speech 
icslp 
frick 
communicating emotion 
role prosodic features 
psychological bulletin 
halliday 
spoken english 
oxford university press london england 
kaiser 
automated coding facial behavior human computer interactions facs 
journal nonverbal behaviour 
murray arnott 
synthesizing emotions speech time get excited 

polzin 
suprasegmental hidden markov models 
technical report school computer science carnegie mellon university forbes avenue pittsburgh pa usa appear 
stiefelhagen meier yang 
real time lip tracking lip reading 
eurospeech 
scherer ladd silverman 
vocal cues speaker ect testing models 
journal acoustic society america 
scherer 
vocal cues emotion encoding decoding 
motivation emotion 
scherer 
acoustic pro les vocal emotion expression 
journal personality social psychology 
kaiser scherer 
models normal emotions applied facial vocal expressions clinical disorders 
laird eds emotions new york oxford university press appear 
takeuchi nagao 
communicative facial display new conversational display 
technical report tr sony computer science laboratory tokyo 
finke ries waibel 
recognition conversational telephone speech janus speech engine 
ieee international conference speech signal processing munich germany 
