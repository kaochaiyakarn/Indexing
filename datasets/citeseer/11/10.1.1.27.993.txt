international journal neural systems 
url ftp ftp cs colorado edu pub time series experts ps www cs colorado edu andreas time series experts ps university colorado computer science technical report cu cs 
nonlinear gated experts time series discovering regimes avoiding overfitting andreas weigend department computer science institute cognitive science university colorado boulder andreas cs colorado edu morgan de france direction des etudes recherches av 
du general de france department computer science university colorado boulder morgan der edf fr ashok srivastava department electrical computer engineering center space construction university colorado boulder cs colorado edu analysis real key problems nonstationarity form switching regimes overfitting particularly serious noisy processes 
article addresses problems gated experts consisting nonlinear gating network nonlinear competing experts 
expert learns predict conditional mean expert adapts width match noise level regime 
gating network learns predict probability expert input 
article focuses case gating network bases decision information inputs 
contrasted hidden markov models decision previous state output gating network previous time step averaging predictors 
contrast gated experts soft partition input space 
article discusses underlying statistical assumptions derives weight update rules compares performance gated experts standard methods time series series obtained randomly switching nonlinear processes time series santa fe time series competition light intensity laser chaotic state daily electricity demand france real world multivariate problem structure time scales 
main results gating network correctly discovers different regimes process widths associated expert important segmentation task characterize sub processes overfitting compared single networks homogeneous multi layer perceptrons experts learn match variances local noise levels 
viewed matching local complexity model local complexity data 
www cs colorado edu andreas home html different regimes different noise levels need gated experts conventional time series models global models 
linear assuming value linear superposition preceding values yule nonlinear conveniently described quite general language neural networks hidden units rumelhart lapedes farber 
single global traditionally univariate models suited problems stationary dynamics 
assumption stationarity violated real world time series 
important sub class nonstationarity piece wise stationarity called stationarity parts multi stationarity series switches different regimes 
example regimes electricity demand depend seasons regimes financial forecasts depend economy expansion contraction called growth recession granger diebold 
single global model principle emulate function including regime switching hard extract unstructured global model data 
particular trying learn regimes different noise levels single network mismatch network extract features generalize regime local overfitting learned potentially regime local underfitting 
final motivation different experts different regions individually focus subset input variables relevant specific region 
turns particularly advantageous modeling multivariate problems different variables important different regimes 
addressing problems class models time series prediction call gated experts 
introduced connectionist community mixture experts jacobs called society experts rumelhart 
term gated experts nonlinearly gated nonlinear experts 
input space split nonlinearly hidden units gating network sub processes nonlinear hidden units expert networks 
basic idea gated experts simple single global model learn local models experts data 
simultaneously learn split input space 
problem splitting input space unknown information available value series 
requires blending supervised unsupervised learning supervised component learns predict observed value unsupervised component discovers hidden regimes 
observable combination gate experts different ways splitting input space fitting local models possible 
trade flexibility gates flexibility experts important degree freedom model class 
summarizing key elements gated experts ffl nonlinear gate experts ffl soft partitioning input space ffl adaptive noise levels variances experts 
contrast related hamilton jordan jacobs allow noise level parameter associated individual expert adapt separately data 
experience expert specific variances important reasons facilitate segmentation areas different predictability grouped prevent overfitting different regimes approximated different accuracy 
new approach problem overfitting 
related gated experts solid statistical basis 
compared prior connectionist addressing segmentation temporal sequences 
elman uses size errors zipser large changes activations hidden units indicate segmentation 
levin adds set auxiliary input units encode discrete state set fixed values training supervised estimated testing 
architecture single network difficult task learning potentially quite different mappings set units 
gated experts compared contrasted connectionist architectures local basis functions architecture radial basis functions broomhead lowe casdagli poggio girosi split input space local regions opposed global sigmoids incentive learning algorithm find regions defined similar structure noise level dynamics 
time series community idea splitting input space subspaces new 
examples threshold autoregressive tar model tong lim 
contrast gated experts splits simple ad hoc probabilistic interpretation 
tar models quite popular economics econometrics 
typically cut input variables introduced subspace separate hyperplane fit 
constraint continuity cut introduced hand emerges naturally gated experts 
tar models tend successful relatively data points available learn splits 
splits exogenous variable volatility engle bollerslev bollerslev 
multivariate adaptive regression splines mars friedman flexible model applied forecasting financial data lewis 
radial basis functions tar mars models define state proximity observed variables 
state directly observable 
latent hidden states popularized econometrics community years ago hamilton hamilton traced back 
expressed connectionist language models hidden units gate experts linear 
different sets coefficients associated regime functions linear markov transition probabilities constant 
knowledge double nonlinear gated experts flexible individual noise levels different regimes economics econometrics granger hamilton 
rigorous probabilistic interpretation linear regime switching models fully generalizes gated experts discussed 
important inspiration mixture models connectionist community jacobs jordan nowlan hinton 
subsequently jordan jacobs introduced architecture hierarchical mixture linear experts fixed widths 
jordan xu proved convergence waterhouse robinson applied hierarchical mixture linear experts time series prediction sunspots weigend nowlan hinton nonlinear regression example noise heterogeneity weigend nix nix weigend 
xu applied architecture linear ar processes suited linear experts 
furthermore gated experts compared approach developed independently pawelzik follows gated experts automatically adjust local noise levels experts regimes pawelzik externally adjust global granularity approximation anneal learning 
gate connected inputs gate inputs case partitioning input space happen indirectly 
segmentation driven local predictability form pattern pattern local noise levels segmentation assumption switching certain number steps errors added certain number steps examples acausal filter 
statistics literature mixture models goes back century 
pearson pearson modeled forehead size crabs mixture gaussians method moments estimate parameters 
arrival computers half century allowed estimation mixture models maximum likelihood framework 
titterington cite applications mixture models real problems 
redner walker review mixture density estimation known decade ago statistics community 
problem estimating local noise levels known statistics literature noise heterogeneity wild 
assume specific stringent model expected value noise variance function input 
steve nowlan suggested application gaussian mixture models time series analysis article summarizes done includes results invited talks ieee workshop neural networks signal processing neural networks capital markets 
important difference needs pointed discussed literature assumes parameters distribution component mixture gating independent inputs flexible case linear functions inputs 
contrast allow relationships nonlinear 
structure article section discusses assumptions gated experts data generating process gives mathematical probabilistic interpretation architecture cost function search 
section summarizes performance architecture learning algorithm time series problems 
specifically section discusses instructive example computer generated data regime switches 
section predicts analyzes laser data santa fe time series competition weigend gershenfeld section applies gated experts predicting electricity demand france 
section analyzes gated experts help avoid overfitting compares method developed predict local error bars weigend nix nix weigend 
section summarizes usefulness gated experts time series analysis 
theory gated experts section describes ingredients needed specify model wiring network architecture activation functions cost function reflecting error model priors parameters outputs search algorithm adjusts parameters magnitude cost function reduces 
architecture goal create architecture allows encourages carving input space experts 
article assume hard decisions allow pattern assigned softly regimes 
see eq 
price paid expert proportional entropy distributing pattern experts 
goal carving input space compared simple averaging different experts 
contrast additive model weights individual predictors fixed independent input outputs gating network mixture model vary input 
allows experts specialize learn areas responsible simple averaging requires sub models equally responsible entire space allowing characterize data terms regime 
time series problems linear ordering patterns visualized particularly nicely analyzing output gating network function time series 
note extracting regime information sacrifice prediction accuracy 
contrary obtain better predictions experts truly experts region opposed covering poorly 
part model carving input space assigns patterns experts called gate 
information gate base decision 
choices external information form inputs gating network internal information extracted network estimate state 
focuses case gate bases decision solely external inputs 
daily energy prediction problem example gate information external variables temperature cloud coverage information day week proximity holiday 
applying gated experts financial problems inputs gating network include variables useful determining regime empirical volatilities interest rate differentials quantities capture market sentiment trader behavior 
second case connections input gate gate bases decision estimate applying gated experts financial markets useful allow differentiation inputs experts experts primarily technical indicators primarily fundamental quantities experts information important market specialize mean reversion 
previous state 
mapping linear described transition matrix hidden states architecture identical hidden markov model hmm rabiner baldi chauvin fraser nadas mercer 
conceptual generalization hmms nonlinear transition functions straightforward transition matrix replaced network hidden units 
note hmms split input space indirectly gate directly connected inputs 
direct connection facilitate splitting 
information external inputs previous internal state gating network combined cacciatore nowlan discussing control problems feed output gating network back input gating network time step providing gating network information state 
bengio frasconi jordan framework input output hidden markov model allows gate access external information inputs past outputs 
shows architecture gated experts consisting experts gating network 
experts gating network access inputs share inputs different sets inputs 
task expert approximate function region input space 
task gating network assign expert input vector 
ultimately goal expert pattern 
soft constraint paying price entropy distribution gate outputs pattern expert gated 
time series prediction teacher signal directly available value series splitting input space known 
solve problem need blend supervised unsupervised learning supervised component learns predict observed value unsupervised component discovers hidden regimes 
variance variance expert expert gating network architecture gated experts 
denotes inputs drawn bottom 
inputs connected experts gate 
box nonlinear neural network tanh hidden units 
gating outputs weight expert outputs expected value 
product network outputs represents different function class different representational bias single network 
probability density function output variable mixture gaussians plotted input outputs experts determine centers gaussians vary location input space 
widths oe independent input 
gaussian densities sketched weighted corresponding gating outputs snapshot early training clean segmentation emerged 
detail expert learns function implemented standard neural network linear output unit tanh hidden units 
denoting ith input activation hth hidden unit tanh hi weights hi offsets called biases model parameters estimated 
specific choice tanh nonlinearities internal building blocks important choice architecture particular choice cost function reflects underlying assumptions switching error distributions priors activations weight values 
output interpreted parameter conditional target distribution 
example gaussians building blocks distribution corresponds mean gaussian 
parameter gaussian width oe property expert depend specific input vector adapts learning noise level regime expert see eq 

fig 
sketches mixture gaussians centers depend input different location input space centers gaussians different locations weighted differently widths 
gating network output expert denoted 
goal estimate probability input generated expert hidden units gating network tanh activation functions building blocks nonlinearities eq 

outputs gating network normalized exponentials called softmax units mccullagh nelder bridle 
choice incorporates architecture constraints outputs positive sum unity 
normalized exponentials combine hidden unit activations weight vector output delta delta delta intermediate activation jh sum extends hidden units 
exponentiated normalized sum giving final output exp exp gating network generates mutually completing probabilities function input built constraint gating outputs sum unity implements competition experts 
competition soft outputs take value points close boundary regimes entropy having experts active input traded reduction error see eq 

cost function recall know regimes start hidden variables 
implies simple supervised learning need employ general framework derive cost function incorporates beliefs 
statistical framework maximum likelihood allows obtain cost function 
defining variables ffl input vector ffl target desired output value ffl output expert corresponding mean gaussian ffl oe width gaussian represented expert ffl probability associated jth expert stochastic variable takes value single expert model constant noise level variance equivalent minimizing squared error output target value negative logarithm gaussian rumelhart 
allow width gaussian function inputs adding second output unit network predict local error bar obtain model estimating local noise level nix weigend 
article assume output scalar 
generalization multivariate outputs identity covariance matrix times oe noise level expert trivial terms update rules terms interpretation expert having certain degree predictability associated 
diagonal covariance matrix different diagonal elements done framework pre scaling outputs 
full covariance matrix hard interpret single predictability measure expert 
ffl output gating network denoting probability input pattern generated jth expert ffl posterior probability jth expert output pattern ffl denotes event pattern generated jth expert ffl pattern index ffl iteration search index ffl denote set parameters expert gate respectively 
recall goal carving input space expressed section 
explicitly assume expert responsible pattern assume experts mutually exclusive probability different experts generating data point input zero 
view experts ways observing gamma expert expert delta delta delta expert delta expert denotes event expert chosen notation shortened replacing expert comma 
assuming mutually exclusive probabilities bayes rule write probability sight eq 
contain serious identifiability problem infinite ways writing function product functions 
constraints imposed class functions gating outputs take values 
constraints enter indirectly search specific architectures different complexities case recurrent networks time constants gate experts 
significantly room modeling need assumptions case single feed forward network 
product model eq 
represents broader functional class single networks 
far dealt probability distributions 
goal obtain single number prediction take expected value probability density 
linear combination expected values individual experts theta weighted weighted mean experts expected value useful statistic distribution unimodal 
assumption pattern generated single expert correct binary learning 
case expert remains active pattern goal predicting single point justified 
check observe distribution values 
remain intermediate levels corresponding close model may mis specified indicating violation underlying assumptions 
case interesting read entire conditional density compare simple density estimation gaussians bishop nonparametric density estimation fractional binning weigend srivastava 
order evaluate likelihood data model model predicts observed data need assume specific distribution measurement errors 
focuses regression continuous unbounded output gaussian error model reasonable choice 
gaussian defined parameters mean events kg mutually exclusive delta delta delta ak 
case assumes gamma expert expert delta kg 
variance higher moments expressed furthermore distribution highest entropy moments 
expert represent gaussian density mean output expert varies nonlinearly input 
expert variance independent location input space usually different experts see variances play crucial role organizing experts areas similar predictability 
probability generating value expert proportional oe exp gamma gamma gamma delta oe parameters variance oe characterize expert depends input oe 
probability density associated observation assumption statistical independence measurement errors pattern superscript enumerates patterns total number allows obtain full likelihood product likelihoods individual patterns oe exp gamma gamma gamma delta oe cost function negative logarithm likelihood function gamma ln gamma ln oe exp gamma gamma gamma delta oe described global probability model need estimate parameters delta delta delta oe oe delta delta delta oe minimizing cost function respect parameters 
principle possible standard gradient descent 
turns quite hard learn time individual maps experts splits input space gating network 
furthermore sum inside logarithm cost function significantly complicated case single network 
search cost function derived previous section eq 
difficult minimize gradient descent 
reformulate problem allows apply expectation maximization algorithm em dempster nowlan tanner hamilton jordan xu 
algorithm assumption variables missing hidden 
map problem em need identify missing variables choose probabilities pattern generated expert trick introducing indicator variable ae pattern generated jth expert set random indicator variables fi ng constitute missing data 
indicator variables binary filter true term 
note assumption expert responsible pattern identical assumption needed eq 

classification problems usual choice binomial distribution mccullagh nelder rumelhart 
binomial distribution described single parameter mean variance related 
lack independent order parameter variance gaussian independent mean difficult model binomial error distribution carve input space degree predictability 
appropriate choice general classification model beta distribution mean width chosen independently 
summarize reasons indicator variables embed assumption mutually exclusive experts allow replace awkward sum experts inside product eq 
product 
logarithm simple double sum yielding tractable log likelihood function 
problem know values step em algorithm comes 
step step compute expected values assuming network parameters known 
second step step update parameters model expected values previous step 
step assume likelihood complete data denote input target training data constituting observed data unobserved missing data ensemble parameters delta delta delta oe oe delta delta delta oe 
key feature em algorithm enters allows replace missing variables expected values 
computation expected values called step superscript denotes iteration number iterate back forth step step 
include iteration number explicitly suppress possible 
note binary expectation take value 
furthermore probabilities gating network output solely knowledge knowledge contrast posterior probability includes knowledge close expert output observed value far step assumed specific distribution 
order compute need specific measure closeness specify error model 
consistent eq 
assume experts output gaussian densities 
allows express parameters oe oe exp gamma gamma gamma delta oe oe exp gamma gamma gamma delta oe result step step 
step expectation negative logarithm eq 
replacing expected value yields cost function includes assumption missing values subscript gamma ln gamma ln oe exp gamma gamma gamma delta oe gamma ln gamma gamma gamma delta oe gamma ln oe gammah ln gamma gamma delta oe ln oe ln term eq 
describes cross entropy interpreted angles 
classification framework standard cost function rumelhart 
assume learns approximate viewed gamma ln entropy distributing pattern experts 
cost zero unity zero 
experts share pattern pay entropy price 
happen jointly manage reduce terms cost function pay entropy 
second term eq 
weighted actual squared error gamma expressed units average squared error expert oe plus cost associated average error 
larger log average squared error log width gaussian expensive 
weighting posterior probability implies expert doing better expert pattern expert large error matter suppressed small significantly different zero performance expert enter cost 
fact drives specialization experts carving input space 
note different simple averaging different predictors hope reduction statistical error degree individual predictors uncorrelated perrone jacobs 
cost function eq 
central gated experts model 
step minimizes cost function adjusting parameters 
parameters consideration variances experts weights experts gating network 
updates variances computed directly setting oe zero 
corresponding value oe iteration minimizes oe prior gamma sum extends patterns variance jth expert weighted average squared errors weight posterior probability expert generated pattern denominator normalizes weightings expert 
consider case certain expert happens win patterns fits small error expert flexible overfits training patterns consequently underestimates noise level 
nix weigend cross validation scheme subset data estimation oe different subset estimation network parameters 
gated experts potentially patterns assigned expert scheme statistically unreliable strongly depends specific splits data different subsets weigend lebaron 
maximum likelihood update eq 
incorporate prior include belief size variance update rule oe oe gamma oe oe prior variance value depends problem 
high noise financial data example set value corresponds random walk hypothesis 
low noise laser data santa fe competition set quantization error analog digital converter 
reflects belief prior value oe value reproduces maximum likelihood case eq 

large neglect term numerator denominator variance oe independent data 
determine value validation set 
gated experts maximum likelihood framework 
need introduce terms oe shows limitations framework 
gated experts expressed map maximum posterior framework 
eq 
approximation general case gamma prior waterhouse 
nonlinear hidden units weights networks computed directly require iterative techniques 
weight changes expert networks proportional difference desired value expert output gammah oe gamma learning rule adjusts parameters expert output moves desired value note factors front usual difference desired predicted value ffl factor modulates weight change proportional importance expert pattern ffl second factor oe modulates learning general noise level regime expert average squared error regime large influence error weight update scaled 
regime believed little noise small differences gamma exaggerated small number 
interpreted form weighted regression increasing effective learning rate low noise regions reducing high noise regions 
result network emphasizes small errors low noise regions low oe discounts learning patterns expected error going large anyway large oe 
ffl third factor usual difference desired value target things equal weight change proportional difference 
weight changes gating network proportional gradient cost function respect intermediate variable prior exponentiation normalization softmax part see eq 

usual activation function chosen update rules simple mccullagh nelder gamma gamma alternative worked laser data introduce lower bound oe set experimental resolution analog digital converter bit bits 
hard limit corresponds assumption prior distribution variance flat cut zero cut 
choosing appropriate prior important part modeling particularly short noisy data sets 
case nonlinear experts subtract denominator effective number determined parameters expert waterhouse treatment linear experts 
steve waterhouse pointing 
parameters gating network adjusted gets pulled recall discussion eq 

posterior probability jth expert computation uses input output information 
function input tries approximate knowing target value 
learning move scatter plot vs diagnostic 
close section remarks priors weights activations modification improves segmentation noisy data search algorithm employed 
improving generalization priors variances gating outputs weights context eq 
discussed explicitly incorporate prior assumptions variances experts 
need including priors direct maximum likelihood estimate eq 
arose problem sample patterns expert fitted expert maximum likelihood estimate misleading 
need prior heart general problem induction want build model data model perform new data 
simplest case restricts oe fixed range appropriate hard limit quantization noise laser example 
eq 
soft version penalizing deviation oe formally related assuming ln oe drawn gamma distribution waterhouse 
beliefs outputs gate incorporated similar way 
simplest case uses hard threshold eq 
js jmax weights changed prevents growing large 
large values assumed indicate overfitting gate sure expert responsible pattern 
soft version assumes drawn dirichlet distribution johnson kotz 
expressed prior implies exp gamma distributed 
experts dirichlet distribution reduces beta distribution see footnote 
beliefs hopes gating output enter additional terms cost function update rule eq 

discourage segmentation making cheap close initial value encourage segmentation making large values cheap effectively pushing away zero 
third way expressing prior beliefs weight values 
treating weights independently enters additional term update rules 
simplest case known ridge regression statistics community introduced weight decay connectionist community hinton le cun term proportional size weight subtracted standard change 
integrating gives cost proportional reflecting belief weights drawn gaussian centered zero 
usually better penalty problem weight decay nonlinear networks hard network develop significant nonlinearities achieved large weights 
trying avoid strong bias linear models method weight elimination 
counts significantly nonzero weights weights assumed drawn distribution sum uniform distribution weights gaussian weights absent weigend 
improving segmentation annealing modification learning improves segmentation implied outputs gating network 
eq 
eq 
contributions individual patterns oe weighted corresponding 
noisy data remain noisy training 
consider case data point lies center regime happens large observational error 
assigned regime noise pushes correspondingly due competition raises 
experts blame bad fit try learn point due random output error 
idea replace maximum likelihood update rule oe convex combination gamma general eq 
assuming gaussian noise eq 

value new parameter take 
want depend approximates expressing proximity squared error cross entropy set gammafi fi viewed knob sets scale usual interpret fi inversely proportional temperature annealed learning process 
reflects level granularity segmentation similarly level granularity clustering rose 
annealing fi discussed srivastava weigend 
fi positive lies 
corresponds probability observed generated outputs gating network 
learning starts small value large learned sense consult regime estimation 
training progresses increases unity decreases 
larger weight put smooth regime estimation noisy leads stable segmentation particularly high noise problems experts required 
improving learning time second order methods nonlinear optimization step weights gating network experts principle done order gradient descent 
experience learning gated experts significantly slower case single network due combined tasks learning unsupervised segmentation supervised individual mappings 
simulations reported second order method goldfarb shanno algorithm bfgs described press 
batch method entire learning set computes descent direction function second derivatives chooses best step direction 
question remains inner loop iterations step done going back outer loop re estimating 
go far step step estimate longer valid learn wrong things step cheap computer time compared iteration tend bfgs iterations step 
comparison cost functions close section interpreting cost function gated experts comparing related cost functions 
clarity suppress implicit dependencies parameters drop sum patterns called pattern cost function 
assuming mutually completing experts started mixture gaussians incomplete data gamma ln oe exp gamma gamma gamma delta oe introduced missing variables replaced distributions expected values 
showed combines information input target output expert sense viewed posterior probability pattern generated expert assumptions obtain expected value complete data likelihood cost function step gammah ln gamma gamma delta oe ln oe ln stands step 
term viewed cross entropy network output target supervised classification estimated target value hidden variables 
term viewed entropy experts measure disorder experts cheapest order expert fully responsible pattern 
cost increases expert gated reaches maximum experts evenly gated average experts taken 
expression square brackets weighting expert relevance pattern identical cost function derived discussed nix weigend case predicting local error bars single network output units conditional mean conditional variance gamma gamma delta oe ln oe ln stands local error bars 
architecture complicated variance oe explicit function input simple gating network 
eq 
square bracket eq 
share trade terms containing oe squared error term small large value oe cost increases logarithmically oe simplifying cost function assuming oe constant reduces standard mean squares lms gamma gamma delta oe ln oe ln interested finding minimum dropping constants eq 
equivalent minimizing squared error gamma gamma delta cost functions derived maximum 
cases incorporate prior assumptions outputs variances weights adding appropriate terms cost functions discussed specifically section see weigend buntine weigend waterhouse 
overview experiments remainder article compares performance gated experts standard methods time series computer generated series obtained randomly switching nonlinear processes section time series santa fe competition light intensity laser chaotic state section daily electricity demand france real world multivariate problem structure time scales section 
data sets try cover time series problems axes ffl artificial computer generated laboratory laser real world electricity ffl time scale computer generated time scales laser individual oscillations collapses multi scale days weeks years ffl deterministic chaos laser mix chaos stochastic behavior computer generated non chaotic noisy behavior ffl low noise laser mixed dynamics computer generated high noise electricity ffl clearly defined number regimes computer generated prior knowledge regimes laser electricity 
quite different problems consistently obtain results ffl gated experts yield significantly better results single networks ffl gated experts discover regimes correctly true segmentation compare come plausible segmentation true segmentation ffl gated experts allow characterize sub processes variances ffl gated experts show overfitting single networks homogeneous multi layer perceptrons due correct matching noise level 
segment computer generated time series 
left half noisy tanh map eq 

right half quadratic map eq 

dimensional return plot 
denotes data noisy tanh process quadratic map 
dimensional return plot computer generated time series 
computer generated data data mixture processes example computer generated toy problem data generation process matches assumption architecture 
processes gamma gamma switch tanh gamma mean var switch deterministic chaotic process quadratic map interval 
second noisy non chaotic process composition autoregressive process order gaussian noise variance relatively high noise level standard deviation squashed hyperbolic tangent confine interval quadratic map 
squashing effective noise level empirical variance 
error model assumes modeled gaussian 
chose parameters give similar appearance time domain fig 
behavior lag space different evidenced dimensional return plots fig 
fig 
respectively 
individual processes 
dynamics processes governed order markov process 
transition matrix probabilities process values diagonal probability staying state set diagonal probability switching process set 
corresponds average time switches time steps 
expected value known exact time switching occurs random 
experiments points training testing 
www cs colorado edu andreas time series experts demo html allows reader develop intuitions learning behavior gated experts stability segmentation running experiments 
architecture learning architecture consists experts gating network 
goal forecast point single step prediction 
expert access past values series inputs fx gamma gate access values 
example thinning input space experts single networks comparison access past values 
expert tanh hidden units gating network hidden units 
output units linear 
runs started different initial weights 
converged experts surviving expert patterns see fig 

learned splittings fig 
predictions switch similar real switches exact switching points randomly generated unpredictable 
variance associated expert quadratic process fig 
small limited prior introduced eq 
variance associated expert emulates noisy process close noise level process 
compared performance gated experts single network varying numbers hidden units 
cases gated experts give significantly better performance single networks 
express performance terms normalized mean squared error observation gamma prediction observation gamma mean compares performance model set simply predicting mean set 
test set obtain results expressed gated experts single network hidden units ffl ratio ffl excluding errors steps switch ratio 
significant improvement performance 
tried single network layers hidden units number weights gated experts 
fig 
shows layer results better layer results hidden layers better suited switching task single layer gated experts model 
guide eye minimum gated experts entered thin horizontal line value figures 
compared horizontal line minima test set single networks minimum gated experts model 
gated experts show weak overfitting reasons carving input space different values variances different regimes 
evolution individual variances shown fig 

comparison weak overfitting gated experts exhibit fig 
fig 
shows single network layer hidden units fig 
shows single network layers hidden units trained minimizing squared errors eq 
allowing different noise levels 
yields stronger overfitting trying minimize errors low noise regime degree errors high noise regime constitutes mismatch 
single network overfits high noise regime low noise regime 
effects hold single networks numbers hidden units small individual expert networks numbers hidden units large entire gated expert architecture 
experts suffice series start larger number experts want inject knowledge number experts 
furthermore adaptive variances final model interpretable 
running gated experts variances clamped equal fixed values updating gave comparably performance regimes longer discovered reliably 
gate gate gate switch data errors gating outputs test set 
compare true switch outputs gating network 
time outputs binary 
note gate discovers hidden switch gate zero 
iterations variances evolution variances training 
expert learns quadratic map variance reaches minimum iteration 
expert learns noisy tanh process estimates variance just 
iteration learning curves gated experts computer generated data 
training theta test 
minimum test error 
iteration learning curves single network layer hidden units 
minimum test error 
iteration learning curves single network layers hidden units 
minimum test error 
laboratory data data laser deterministic chaos second example applies gated experts laser time series santa fe time series prediction analysis competition weigend gershenfeld 
laser stationary system time scale observations time series 
behavior approximated reasonably set coupled nonlinear differential equations lorenz equations 
equations invariant time shift 
noise level low main source noise observed data quantization error analog digital converter 
dynamic range bits corresponding signal noise ratio 
collapses interrupting areas steadily growing envelopes shown top panel fig 

caused outside shocks part volume weigend gershenfeld describes data set attempts predict characterize 
data available anonymous ftp ftp cs colorado edu pub time series santafe file full 
papers data set internet date accessed www cs colorado edu andreas html 
internal dynamics nonlinear system 
long time behavior predicted due divergence nearby trajectories hallmark chaotic dynamics 
patterns training times santa fe competition size patterns sample data sets test test ii 
architecture learning goal forecast point single step prediction 
inputs expert values time series fx gamma gamma gamma 
dimension laser data fewer inputs suffice principle 
experience easier obtain predictions minimal number inputs 
expert single layer tanh hidden units linear output unit 
gating network sees inputs experts equipped layer tanh hidden units followed softmax outputs 
know optimal number experts 
size data set experience typically experts 
half runs experts half experts gating outputs remaining experts zero pattern 
fig 
shows run experts survive 
single step equal test set 
comparison architecture gated linear experts gave 
tried mixture linear experts 
errors gate gate gate gate gate gate data outputs gating network part test set 
note binary behavior gates 
just collapses expert remains gated half time amplitude exceeds certain level 
expert expert expert expert expert expert un gated outputs individual experts 
note different behavior experts experts collapses comparison experts deal rest series 
interpretation shows gating network allocates experts predicting mapping collapse 
fit rest series 
experts takes care small oscillations growth period 
oscillations get larger expert deals valleys gets help expert peaks 
expert comes right collapse expert post collapse expert care dynamics right re injection 
expert absent basically takes blame big error right collapse 
types segmentation shape peaks valleys regimes pre collapse collapse post collapse 
fig 
shows expert generates forecasts collapse expert forecasts post collapse 
training test test ii iterations squared error function iterations 
overfitting 
local increases error indicate phase separations see text 
iterations evolution variances 
numbers correspond numbering experts fig 
fig 

shows normalized mean squared error function epochs 
note areas increase viewed phase separations borrowing term processes different regimes emerge cooling 
increases possible step descends entire cost function eq 
gammah ln gamma gamma delta oe ln oe ln squared error analyzed corresponds middle term 
experience gate drives variances increase squared error implies decrease entropy corresponding creation new phase allows experts carve space better binary 
continuing analogy phase separations system evolves ordered mixed phases ordered separated phases 
phase separations corresponding new roles experts imply changes variances clearly observed fig 
iterations 
note variances training span orders magnitude 
large values correspond experts try fit collapse 
second example showed adaptive variances crucial 
real world data data electricity demand france third example real world task predict daily electricity demand france 
time series perspective series exhibits interesting features ffl multi variate inputs encompassing endogenous variable past electricity demand exogenous variables temperature cloud coverage weather ffl multi scale structure time scales daily patterns weekly patterns yearly patterns ffl multi stationary different regimes holidays vs summer vs winter 
data task performance described detail cottrell gated experts approach 
illustrate gated experts approach section focuses stage model developed remove non stationarity series section analyzes splitting input space generated gated experts shows information process obtained task directed clustering performed gated experts 
stage model split task parts 
model predicts electricity demand exogenous variables 
gated experts build conditional gaussian mixture model parameters vary nonlinearly exogenous inputs 
second model trained predict residual error 
point stage architecture residuals model effectively removing trends drifts stationary raw data 
gated experts stage 
turns single network suffices second stage 
electricity demand day delta delta delta denotes day delta delta delta exogenous variables available inputs stage 
number lags residual error model inputs second model 
delta delta delta gamma gamma delta delta delta gammap residual errors second model respectively 
prediction global model delta delta delta gamma gamma gammap expected value energy demand conditioned inputs 
obtained setting expected value zero 
stage corresponds regression exogenous variable 
experts layer tanh hidden units gating network hidden layer tanh units 
experts gating network connected exogenous inputs proximity holidays 
current day days holiday corresponding input binary inputs set 
input corresponds day days holiday input day days holiday 
task independent unsupervised clustering kohonen 
inputs self organizing map included standard inputs model currently 
day week 
binary inputs denote day week 
proximity special tariff days 
edf certain days special apply 
current day days day corresponding input binary inputs set 
input corresponds monday 
annual cycle 
continuous inputs determine position day year sine cosine period 
weather 
remaining inputs give temperature degree cloud coverage day day day day differences 
choice models currently france 
attempt improve set inputs 
role inputs different regimes considered context fig 

split available data sets ffl training set january december days ffl test set january march days 
performed runs different initial sets weights starting experts 
final number surviving experts varied runs runs runs 
variation may due relatively small range final variances different regimes factor low noise high noise experts 
data gate gate gate gate training set test set data second half training period entire test period 
annual cycle peaking cold winter months evident summer vacation 
gate picks holidays gate days holidays gate warmer weather gate seasons 
iterations variances log scale evolution variances experts 
training expert summers lowest variance expert holidays largest variance 
experts converge show trade final phase separation 
stage 
second network input residuals stage task predict daily energy demand 
target stage 
stage lagged energy demand variables inputs 
autoregression residual errors gated experts stage single network hidden layer tanh units 
turned second stage complicated architectures 
input consisted lags residual errors stage 
final performance standard pruning cottrell remove irrelevant weights 
basic idea compare size weight size fluctuations standard deviation weight changes response input patterns 
size weight small compared fluctuations removed 
electricity prediction task typically removed weights 
resulting network squared error smaller training set test set indicating essentially overfitting pruning 
performance analysis compare performance combined stage stage model benchmarks linear autoregressive model exogenous variables arx single neural network hidden layers cottrell 
terms squared error stage model gives improvement arx model presently 
performance single neural network falls gives improvement arx model 
give examples interpretation gated experts 
fig 
shows part time series corresponding activations gates 
fig 
shows evolution variances 
final ordering consistent knowledge engineers easier predict energy demand standard summer days standard winter days particularly difficult predict energy demand holidays expert days linear correlations inputs outputs gating network 
numbers axis denote input number described text section 
filled circles denote positive correlation open circles negative correlation 
size circles indicates absolute value correlation 
give scale correlation input output correlation input output 
holidays expert fig 
visualizes linear correlation coefficients outputs gating network inputs 
largest correlation gate input indicating holiday input 
inputs proximity special tariff days positively correlated gate negatively gates 
consistent fact special exist winter vicinity holiday 
general feature gates tend correlation coefficients inputs opposite signs 
values particularly large set inputs describes weather 
summary addition better performance single network gated experts stage allow gain insight dynamics system 
adaptive variances overfitting better segmentation problem conventional approaches serious problems applying flexible machine learning techniques noisy real world data problem overfitting 
section discussed approach combat overfitting modified cost function update rules incorporating prior beliefs variances output weight values 
furthermore section discussed briefly technique pruning non significant weights energy prediction example 
study effect gated experts architecture cost function overfitting problem techniques section monitor training test errors 
manifestation overfitting performance sample data plotted function training time starts deteriorating having reached optimal point 
indicates network learning features stage training generalize new data features 
technique overfitting simply consists stopping training early network minimum validation set weigend 
training proceeds network tends shift resources high noise regions noisy data point bigger error bigger effect error backpropagation 
assume regions input space noisy noise heterogeneity 
data point weighted equally noisy regions tend attract resources network noise signal trying model overfitting 
time resources moved away noisy regions resulting underfitting 
large networks early stopping usually better training convergence deal problem differences local structure enter global error model 
article considers alternatives standard network trained global mean squared errors 
section reviews technique introduced weigend nix nix weigend originally developed obtain local error bars regression prediction problems 
section compares dynamics overfitting architectures gated experts network local error bars performing weighted regression standard single network 
estimating local error bars weighted regression goal technique nix weigend estimate explicitly local noise level addition value prediction 
local means just prediction noise level function inputs 
illustrates architecture 
oe unit hidden layer receives connections unit hidden layer input pattern 
allows great flexibility learning oe 
contrast oe unit hidden layer network constrained approximate oe linear combination exponentiation features useful 
network architecture estimating local error bars auxiliary output unit 
architecture allows unit conditional variance oe access information input pattern hidden unit representation formed learning conditional mean 
reprinted weigend nix 
cost function discussed section eq 
gamma gamma delta oe ln oe ln derivation maximum likelihood framework assumes gaussian distributed errors outputs 
order able write weight update equations explicitly need specify activations functions 
choose linear output tanh hidden units exponential function oe unit incorporating constraint variance positive 
derivatives cost respect network weights obtain yj gamma oe gamma oe gamma oe ae gamma gamma oe oe denotes hidden unit activation 
weights connected output weight update equations derived chain rule way standard backpropagation 
note eq 
equivalent training separate function approximation network oe targets squared errors gamma step procedure search learning weights unit freezing learning weights oe unit stage update weights gradient descent different architecture method estimating local error bars shares gated experts method ability model local noise levels 
done explicitly local error bar model implicitly gated experts partitioning input space sub regions characterized similar noise levels 
interpretation terms weighted regression eq 
context gated experts holds 
section compares contrasts different approach example predicting daily energy demand france 
evolution learning learning curves investigate learning dynamics compare architectures prediction problem introduced section 
architectures gated experts fig 
learning local variances fig 
single neural network fig 

measure comparison normalized mean squared error defined eq 

iterations gated experts vs iterations gated experts 
solid line sample error training set broken lines sample errors test sets 
iterations local error bars learning curve local error bar model learns predict value series local variance 
test minima low gated experts 
iterations single network learning curves single network trained backpropagating squared errors 
overfits strongly exact stopping point important 
figures reveal significantly different degrees overfitting 
learning gated expert stable overfit local error bar network somewhat worse single network lot worse 
note sample error solid line significantly lower single network architectures single network trained minimize precisely performance measure cases complex costs minimized 
important distinguish full cost include penalty terms robust errors performance term ultimately sample interested take squared error 
order study dynamics overfitting different architectures switched penalty terms priors pruning performed descent maximum likelihood cost functions 
weighted regression implied local variance terms local error bars model helps additional segmentation gated experts helps 
sense viewed additions anti overfitting arsenal 
summary article describes nonlinearly gated nonlinear experts adaptive variances applies prediction analysis time series 
gated experts viewed new method data analysis particularly suited discovering processes changes conditions ranging importance vacations france market sentiment trading 
blend supervised unsupervised learning embedded architecture cost function gives results areas analyzed learning curves costs opposed examples costs overfit significantly 
ffl prediction 
performance gated experts significantly better single networks piece wise stationary processes switch regimes different dynamics noise levels 
ffl analysis 
gated experts discover hidden regimes 
analyzing individual experts terms average predictability regime sensitivity analysis importance inputs analyzing gate correlations outputs auxiliary variables yields deeper understanding underlying process 
ffl overfitting 
matching complexity model data central goal machine learning 
mismatches manifest overfitting 
article focuses aspect matching local noise level compares gated experts local matching done locality expert architecture predicting local error bars 
architectures implement weighted regression show overfitting standard backpropagation assumes global noise scale 
gated experts described serve starting point directions 
idea annealing achieve stable segmentation briefly mentioned section discussed detail srivastava weigend 
article assumes switching occurs random 
capture dynamics switching process introduce recurrence gating network 
experiments financial data find hidden markov style recurrence presenting output gating network additional input time step big difference segmentation performance 
equipping hidden units exponential memory fixed decay constant yields cleaner interpretable regimes weigend shi 
recurrence enables develop special architecture early detection regime switches allows extract temporal processes time scale 
acknowledgments applying mixture models time series analysis suggested steve nowlan 
andreas weigend steve waterhouse discussions priors klaus pawelzik klaus muller discussions similarities differences jong oh discussions phase segmentation shi implementation matlab tom mccabe ralph helping eliminate typos article 
morgan mike jordan discussions summer school de france edf acknowledges support edf visiting computer science department university colorado boulder 
material supported national science foundation 
ria ecs 
baldi chauvin baldi chauvin 

smooth online learning algorithms hidden markov models 
neural computation 
bengio frasconi bengio frasconi 

input output hmm architecture 
tesauro touretzky leen editors advances neural information processing systems nips pages 
mit press cambridge ma 
bishop bishop 

mixture density networks 
technical report aston university 
bollerslev bollerslev 

generalized autoregressive conditional 
journal econometrics 
bollerslev bollerslev chou jayaraman 

arch modeling finance review theory empirical evidence 
journal econometrics 
segmentation sole goal variation architecture uses sided acausal filter presenting data point input 
bridle bridle 

probabilistic interpretation feedforward classification network outputs relationships statistical pattern recognition 
fogelman soulie herault editors neuro computing algorithms architectures pages 
springer verlag 
broomhead lowe broomhead lowe 

multivariable functional interpolation adaptive networks 
complex systems 
buntine weigend buntine weigend 

bayesian back propagation 
complex systems 
cacciatore nowlan cacciatore nowlan 

mixtures controllers jump linear nonlinear plants 
tesauro alspector editors advances neural information processing systems nips pages san francisco ca 
morgan kaufmann 
casdagli casdagli 

nonlinear prediction chaotic time series 
physica 
cottrell cottrell girard girard muller 

neural modeling time series statistical stepwise method weight elimination 
ieee transaction neural networks 
dempster dempster laird rubin 

maximum likelihood incomplete data em algorithm 
roy 
stat 
soc 

diebold diebold 

measuring business cycles modern perspective 
review economics statistics 
zipser zipser 

unsupervised discovery speech segments recurrent networks 
touretzky elman sejnowski hinton editors proceedings connectionist models summer school pages san fransisco ca 
morgan kaufmann 
elman elman 

finding structure time 
cognitive science 
engle engle 

autoregressive conditional estimates variance united kingdom inflation 
econometrica 
zimmermann 

improving generalization performance model selection methods 
neural networks 
fraser fraser 

forecasting probability densities hidden markov models 
weigend gershenfeld editors time series prediction forecasting understanding past pages reading ma 
addison wesley 
friedman friedman 

multivariate adaptive regression splines 
annals statistics 
granger granger 

forecasting economics 
weigend gershenfeld editors time series prediction forecasting understanding past pages reading ma 
addison wesley 
granger granger 

modelling nonlinear economic relationships 
oxford university press oxford uk 
hamilton hamilton 

new approach economic analysis nonstationary time series business cycle 
econometrica 
hamilton hamilton 

analysis time series subject changes regime 
journal econometrics 
hamilton hamilton 

time series analysis 
princeton university press princeton 
muchnik 

estimation parameters hidden markov models noise signals abruptly changing probabilistic properties 
automation remote control 
jacobs jacobs 

methods combining experts probability assessments 
neural computation 
jacobs jacobs jordan nowlan hinton 

adaptive mixtures local experts 
neural computation 
johnson kotz johnson kotz 

distributions statistics 
continuous multivariate distributions 
wiley new york 
jordan jacobs jordan jacobs 

hierarchical mixtures experts em algorithm 
neural computation 
jordan xu jordan xu 

convergence results em approach mixtures experts architectures 
neural networks 
lapedes farber lapedes farber 

nonlinear signal processing neural networks 
technical report la ur los alamos national laboratory los alamos nm 
levin levin 

modeling time varying systems hidden control neural architecture 
lippmann moody touretzky editors advances neural information processing systems nips pages 
morgan kaufmann 
lewis lewis ray stevens 

modeling time series multivariate adaptive regression splines mars 
weigend gershenfeld editors time series prediction forecasting understanding past pages reading ma 
addison wesley 
muller weigend 

forecasting electricity demand mixture nonlinear experts 
world congress neural networks pages ii 
mccullagh nelder mccullagh nelder 

generalized linear models 
chapman hall london 
nadas mercer nadas mercer 

hidden markov models connections artificial neural networks 
smolensky mozer rumelhart editors mathematical perspectives neural networks 
lawrence erlbaum associates hillsdale nj 
nix weigend nix weigend 

learning local error bars nonlinear regression 
tesauro touretzky leen editors advances neural information processing systems nips pages 
mit press cambridge ma 
nowlan nowlan 

soft competitive adaptation neural network learning algorithms fitting statistical mixtures 
phd thesis school computer science carnegie mellon university 
technical report cmu cs 
nowlan hinton nowlan hinton 

simplifying neural networks soft 
neural computation 
pawelzik pawelzik kohlmorgen muller 

annealed competition experts segmentation classification switching dynamics 
neural computation 
pearson pearson 

contributions mathematical theory evolution 
phil 
trans 
royal soc 
see 
perrone perrone 

general averaging results complex optimization 
mozer smolensky touretzky elman weigend editors proceedings connectionist models summer school pages hillsdale nj 
lawrence erlbaum associates 
poggio girosi poggio girosi 

networks approximation learning 
proceedings ieee 


hidden markov models guided tour 
ieee international conference acoustics speech signal processing pages 
jordan jordan 

learning parameters hmms input 
technical report mit 
press press flannery teukolsky vetterling 

numerical recipes art scientific computing 
cambridge university press cambridge 


estimation parameters linear regression system obeying separate regimes 
journal american statistical association 
rabiner rabiner 

tutorial hidden markov models selected applications speech recognition 
proceedings ieee 
redner walker redner walker 

mixture densities maximum likelihood em algorithm 
siam review 
rose rose gurewitz fox 

statistical mechanics phase transitions clustering 
physical review letters 
rumelhart rumelhart durbin golden chauvin 

backpropagation basic theory 
chauvin rumelhart editors backpropagation theory architectures applications pages hillsdale nj 
lawrence erlbaum associates 
rumelhart rumelhart mcclelland group 

parallel distributed processing exploration microstructure cognition 
volume cognition 
mit press cambridge ma 
wild wild 

nonlinear regression 
wiley new york 
srivastava weigend srivastava weigend 

improving time series segmentation gated experts annealing 
technical report cu cs university colorado boulder computer science department 
tanner tanner 

tools statistical inference 
springer verlag nd edition 
titterington titterington smith makov 

statistical analysis finite mixture distributions 
john wiley new york 
tong lim tong lim 

threshold autoregression limit cycles cyclical data 
roy 
stat 
soc 

waterhouse waterhouse mackay robinson 

bayesian mixtures 
advances neural information processing systems nips 
mit press cambridge ma 
waterhouse robinson waterhouse robinson 

non linear prediction acoustic vectors hierarchical mixture 
tesauro touretzky leen editors advances neural information processing systems nips pages 
mit press cambridge ma 
weigend gershenfeld weigend gershenfeld editors 
time series prediction forecasting understanding past 
addison wesley reading ma 
weigend weigend huberman rumelhart 

predicting connectionist approach 
international journal neural systems 
weigend lebaron weigend lebaron 

evaluating neural network predictors bootstrapping 
proceedings international conference neural information processing iconip pages 
technical report cu cs computer science department university colorado boulder ftp ftp cs colorado edu pub time series bootstrap ps 
weigend nix weigend nix 

predictions confidence intervals local error bars 
proceedings international conference neural information processing iconip pages seoul korea 
weigend srivastava weigend srivastava 

predicting conditional probability distributions connectionist approach 
international journal neural systems 
xu xu 

signal segmentation finite mixture model em algorithm 
proceedings international symposium artificial neural networks pages taiwan 
yule yule 

method investigating periodicity disturbed series special sunspot numbers 
phil 
trans 
roy 
soc 
london 

