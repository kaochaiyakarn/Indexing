experiments reinforcement learning problems continuous state action spaces coins technical report december juan carlos ia richard sutton ashwin ram carlos cc gatech edu rich cs umass edu ashwin cc gatech edu college computing georgia institute technology atlanta ga graduate research center university massachusetts amherst ma key element solution reinforcement learning problems value function 
purpose function measure long term utility value state important agent decide 
common problem reinforcement learning applied systems having continuous states action spaces value function operate domain consisting real valued variables means able represent value infinitely state action pairs 
reason function approximators represent value function close form solution optimal policy available 
extend previously proposed reinforcement learning algorithm function approximators generalize value individual experiences state action spaces 
particular discuss benefits sparse coarse coded function approximators represent value functions describe detail implementations cmac instance case 
additionally discuss function approximators having different degrees resolution different regions state action spaces may influence performance learning efficiency agent 
propose simple modular technique implement function approximators non uniform degrees resolution represent value function higher accuracy important regions state action spaces 
performed extensive experiments double integrator pendulum swing systems demonstrate proposed ideas 
supported nsf ecs andrew barto richard sutton 
contents reinforcement learning definition state value function value function action value function function temporal difference methods function approximators step search learning sparse coarse coded function approximators lookup tables cerebellar model articulation controller memory function approximators non uniform preallocation resources implementation non uniform functions approximators results double integrator optimal solution uniform cmac non uniform cmac uniform instance non uniform instance pendulum swing uniform cmac non uniform cmac uniform case non uniform case discussion double integrator pendulum swing summary wide class sequential decision making problems states actions dynamic system described real valued variables 
common examples class problems robotics agents required navigate obstacles move manipulators grasp tools objects accomplish task 
states system examples corresponds set variables representing positions velocities joints body parts actions set forces torques applied actuators 
class problems agent select action infinitely alternatives fixed time interval basing decision currently perceived state possible states 
reinforcement learning solve sequential decision making problems 
main idea consists experiences progressively learn optimal value function function predicts best long term outcome agent receive state applies specific action follows optimal policy 
agent incrementally learn optimal value function continually exercising current non optimal estimate value function improving estimate experience 
specifically agent decision current non optimal estimate value function selecting action leading best value current state 
observing result executing action agent reinforcement learning algorithm sutton td algorithm watkins learning algorithm improve long term estimate value function associated state action :10.1.1.132.7760
systems having continuous state action spaces value function operate real valued variables representing states actions means able represent value infinitely state action pairs 
learning problem difficult agent experience exactly situations experienced 
agent able generalize value specific state action combinations experienced situation currently facing decision 
closed form solution optimal value function known advance choice value function representation real challenge 
able handle real valued variables inputs precisely map state action pairs assigned values memory resources efficiently support learning computational burden generalize immediate outcome specific state action combinations regions state action spaces 
value functions typically represented function approximators finite resources represent value continuous state action pairs 
additionally parameters agent adjust value estimates experience consequently improve performance 
issues related design function approximators 
research explore issues related questions context reinforcement learning applied systems continuous state action spaces 
agent function approximators generalize outcome experiences involving specific state action pairs regions state action spaces 
common approach represent value function quantize state action spaces finite number cells aggregate states actions cell :10.1.1.51.4764
simplest forms generalization states actions cell value 
value function approximated table cell specific value 
compromise efficiency accuracy class tables difficult resolve design time 
order achieve accuracy cell size small provide resolution approximate value function 
cell size gets smaller number cells required cover entire state action spaces grows exponentially causes efficiency learning algorithm deteriorate data required estimate value cells 
approach avoid problems associated state space altogether types function approximators neural networks rely quantization generalize value function states :10.1.1.75.7884
approach consists associating function approximator represent value states specific action 
reason useful systems continuous states discrete actions 
way agent generalize value specific state action states neighborhood action 
approach extended systems continuous states actions quantizing action space finite number cells associating function approximator action cell 
way actions action cell aggregated assigned value keeping state continuous 
quantized table approach number action levels increases function approximators required efficiency learning algorithm deteriorates due inability function approximator generalize action levels 
possible specific state action experiences improve estimate function approximator action value near boundary different action cells 
technical report propose approach extend function approximators systems continuous states action spaces 
approach consists function approximator combined state action spaces 
require quantization state action spaces generalize state action experiences regions states actions spaces 
evaluate feasibility proposed approach widely known systems continuous state action spaces double integrator pendulum swing 

effect designing function approximators non uniform resource preallocation 
important decision consider designing function approximators deals resource preallocation state action spaces 
resource preallocation refers distribution adjustable memory elements input domain 
important decision distribution affects way function approximator represents value state action pairs 
general adjustable memory elements available resolution function approximator 
design strategy prior knowledge system available resources uniformly entire state action spaces 
way function approximator able represent value function resolution desirable reason favor regions state action space 
possible design function approximators non uniform distribution resources order achieve different degrees resolution different regions state action space 
reasons designing non uniform function approximators may beneficial designing uniform ones 
absolutely prior knowledge system available designers know certain degree regions state action space 
cases premise lead design function approximator uniform resources preallocation longer applies 
presumably may advantage design function approximator uses fewer resources regions state action space known visited rarely resources heavily regions 
second fixed amount resources non uniform function approximator may lead better performance learning efficiency achieved uniform function approximator just able exploit resources efficiently 
may possible design function approximators dynamically allocate resources certain regions state action space increase resolution regions required perform line 
technical report describe modular method implement nonuniform function approximator implementation uniform 
method takes advantage conceptually simple computationally efficient implementation uniform function approximators produces results equivalent non uniform ones 
explore feasibility idea implementing uniform non uniform versions types sparse coarse coded function approximators cmac instance compare results learning efficiency final performance double integrator pendulum swing 
section describes problem formulation introduces concepts 
section presents role function approximators reinforcement learning describes detail classes sparse coarse coded function approximators experiments 
section introduces topic resource preallocation function approximators describes simple technique obtain non uniform resource preallocation keeping simplicity computational efficiency function approximators uniform preallocation 
results experiments performed uniform non uniform sparse coarse coded function approximators different reinforcement learning problems section section analyzes results 
section concludes 
reinforcement learning definition reinforcement learning problems decision maker agent attempts control dynamic system choosing actions sequential fashion 
agent receives scalar value reward action executes 
ultimate goal agent learn strategy selecting actions policy expected sum discounted rewards maximized 
dynamic system referred environment characterized state dynamics function describes evolution state agent actions 
state system captures information required predict evolution system agent actions 
assumed agent perceive state environment error base current decision information 
state value function value function key element solution reinforcement learning problem state value function simply value function associated policy defined fl state system time reward received performing action time fl discount factor fl 
state value function measures expected discounted sum rewards expected return agent receive starts state follows policy name state value function 
particular optimal value function measures maximum possible expected return agent receive starts state agent controlling deterministic system knows step reward function dynamics function optimal value function state action decide optimal action perform decision point performing step lookahead search arg max fr policy defined equation optimal selects actions maximizes expected return starting state 
additionally easy design efficient implementations step search cases 
action value function function step search value function useful agent knows step reward dynamics functions advance 
alternatively action value function simply function watkins defines measures expected return executing action state policy delta selecting actions subsequent states name action value function 
function corresponding policy defined flq advantage function agent able perform step lookahead search knowing step reward dynamics functions 
disadvantage domain function increases domain states domain state action pairs theta agent knows optimal function select optimal action step search arg max fq problems rewards negative convenient express positive costs 
keep consistency agent seek minimize discounted sum costs maximize discounted sum rewards 
keep mind formulations mathematically equivalent 
optimal function computed dynamic programming step reward dynamics functions known advance 
interested methods agent learn optimal policy experience 
specifically methods agent incrementally improve estimates function outcome previous actions 
temporal difference methods temporal difference methods exploit equation create update formula agent asymptotically learn function function observations :10.1.1.132.7760
specifically time agent selects action observes state reward verify equation holds computing error predictions flq 
error different zero update formula adjust estimate error maximally reduced 
sutton defines family update formulas temporal difference learning called td weight measure relevance previous predictions current error 
convergence td proved different conditions assumptions 
watkins shows function estimates asymptotically converge optimal values systems having discrete finite state action spaces td perform updates 
condition convergence states visited actions executed infinitely 
tsitsiklis van roy shows value function associated policy converges linear function approximator td updates 
proof showing convergence td complex function approximators stopped researchers trying methods different classes non linear function approximators 
successful results obtained multi layer neural networks sparse coarse coding methods cerebellar model articulation controllers :10.1.1.51.4764:10.1.1.75.7884
section describes role function approximators reinforcement learning problems continuous state action spaces 
function approximators optimal function sufficient agent optimally perform task agent step lookahead search equation select optimal action state 
agent optimal function readily available learn function experience 
idea provide agent initial estimate function decide best action current estimate 
agent outcome action asymptotically improve estimate optimal action value 
way accomplish function approximators represent function 
function approximator provide agent estimates expected returns state action pair state action spaces continuous 
specifically function approximator function policy form qw state action set adjustable parameters weights 
current estimate agent step search select best action state estimate 
additionally agent may adjust current estimate modifying weights function approximator td updates 
function approximators useful generalize expected return state action pairs agent experiences regions state action space 
way agent estimate expected return state action pairs experienced 
function approximators able represent function state action spaces continuous 
note function approximator may able accurately represent function entire state action space due finite resources 
classes function approximators advantages disadvantages 
choice function approximator depends mainly accurate generalizing values unexplored state action pairs expensive store memory supports computation step search learning td updates 
step search function approximator function indicate best action agent take state 
action determined performing step search arg max qw actual implementation step search depends function approximator 
common technique problems continuous state spaces function approximator action 
step search performed simply evaluating function approximator state 
best action associated function producing maximum value see example :10.1.1.51.4764:10.1.1.75.7884
technique quickly impractical number actions increases due storage implications computational efficiency number function approximators proportional number actions 
additionally function approximators operates isolation able generalize estimated return states 
evaluated action values finite predetermined set 
possible overcome difficulties technique expense elaborate computation performing step search 
function may represented function approximator optimal action state may numerical optimization method may example function approximators provide direct measure best action gradient descent method 
details implementation step search depends class function approximator 
algorithm shown applicable classes function approximators 
computational complexity algorithm depends step size delta cost evaluating qw dimensionality action space 
additionally resolution search action space delta resulting action value truly continuous action space 
advantage function approximator represent function delta 
clearly efficient algorithm conceptually simple widely applicable 
input state weights output best action algorithm 
action min umax step delta evaluate qw 
keep action produces maximum value 

return step search algorithm 
learning agent policy improve current estimate qw adjusting weights state transition td updates 
possible agent chooses action suggested policy 
agent perform exploratory actions able determine effect 
purpose time agent selects action state observes state reward results executing action uses td formula update weights function approximator deltaw ff fl gamma fl gammak rw qw qw rw gradient respect evaluated state action pair ff learning rate 
interpretation equation follows represent long term outcome discounted sum rewards associated current policy states respectively 
term fl gamma represents error incurred predicting words value state equal immediate reward plus value state properly discounted 
case error weights proportionally modified direction gradient rw order maximally reduce error 
discounted sum previous gradients rw credited current error influence decays exponentially rightmost sum recursive property fl gammak rw rw fl fl gammak rw rw fl line implementation update rule equation possible advantage recursive property 
idea maintain value rightmost sum equation variable easily update step rw flw input initial set weights algorithm 
initialization perceive current state curr select action curr ffl greedy policy curr 
accumulate gradient rw curr curr flw gamma 
perform action execute curr observe resultant reward state 
select action ffl greedy policy 
learn ff fl gamma curr curr 
loop curr curr curr goal state terminate goto 
gradient descent version sarsa algorithm 
approach perform computation store component multidimensional variable separately eligibility trace variable represents proportion blame eligibility associated component current error account past information details see :10.1.1.32.9278
order improve values policy chosen way improve agent collects information 
way accomplish choose action leading best estimate time ties resolved randomly 
small fraction ffl time action chosen randomly uniformly operating range 
policy called ffl greedy policy see :10.1.1.51.4764
performing learning put sarsa algorithm 
algorithm uses ffl greedy policy td updates improve estimates function experience consequently performance agent see 
sparse coarse coded function approximators step search sarsa algorithms outlined previous section apply kind function approximator 
function approximators equally efficient 
main characteristics function approximator exhibit useful efficient ffl generalization refers ability function approximator accurately generalize values unexplored state action pairs 
systems continuous state action spaces agent experiences exactly situation experienced 
important function approximator able extrapolate values experienced state action pairs unseen state action pairs 
ffl resolution refers granularity function approximator capacity represent different values small areas input space 
ability function approximator able accurately represent function depends resolution 
ffl storage refers memory resources implement function approximator 
storage function approximator need usable due cost associated maintenance 
cases storage compromise resolution function approximator finer resolution larger storage 
ffl computational efficiency refers complexity efficiency step search sarsa algorithms 
efficient function approximator provide support simple efficient evaluation state action pairs gradient computation agent performs operations extensively action selection learning 
concentrate research sparse coarse coded function approximators properties nicely support basic characteristics 
sparse coarse coded function approximators basic model associative memory 
represent content associated input physical memory locations generalize content sharing memory locations 
different inputs share memory locations similar contents 
resolution storage computational efficiency vary specific type function approximator implementation 
detailed description sparse distributed memory related models 
subsections describe types sparse coarse coded function approximators outline advantages disadvantages 
lookup tables lookup table common function approximators represent function states actions spaces discrete 
represent extreme class sparse coarse coded memory function approximators input associated memory location 
specifically state action pair element table store current estimate value associated pair 
size lookup table nu number states actions respectively 
lookup tables represent extreme sparse coarse coded function approximators memory location represent value associated state action pair 
system characterized continuous state action spaces element lookup table mapped cell state action spaces 
states actions region cell aggregated table element assigned value 
basic form generalization lookup tables implement 
lookup tables quickly impractical reasons 
state actions spaces quantize finite number cells 
difficult determine appropriate quantization scheme provide resolution accuracy low quantization error 
second number cells grows exponentially number variables geometrically number quantization levels 
creates storage maintenance problems 
third rate convergence learning algorithm extremely slow number states actions increases 
additionally quantized state action spaces create convergence problems form non markovian representation dynamics system see 
lookup tables efficiently support step searches td updates 
best action state indexing table holding state constant performing sweep possible actions values equation 
parameter associated stateaction pair lookup table td updates easy implement 
equation shows td update 
deltaq ff fl max gamma watkins showed certain conditions update rule asymptotically learn optimal values systems having discrete state action spaces 
peng williams described way td learning algorithm call results learning algorithm faster convergence rate :10.1.1.56.7356
conditions convergence algorithms rarely hold quantized versions continuous state action spaces 
cerebellar model articulation controller cerebellar model articulation controller cmac class sparse coarse coded memory models cerebellar functionality 
input state action pair activates specific set memory locations features arithmetic sum contents value stored value 
cmac takes advantage continuous nature input able store necessary data physical memory practical size 
cmac consists overlapping tilings state action space produce feature representation 
query performed activating features contain state action input summing values activated features 
shows bidimensional example cmac organization 
widely conjunction reinforcement learning :10.1.1.51.4764
size cmac depends number tilings size tiling 
tilings usually large provide resolution grow exponentially number variables 
common trick avoid large tilings consistent random hashing function collapse large number tiles tiling smaller set 
uniform tilings efficiently support step searches evaluation values computationally expensive 
cmac implementations td updates proportional number tiles cmac tile holds eligibility trace 
efficient implementations keep list non zero traces perform updates tiles 
description structure basic operations cmac follows ffl tile structure cmac consists set tilings delta delta delta tiling consists set tiles features cover entire input space tiling contiguously overlapping ij delta delta delta tile ij weight ij eligibility ij ffl tile selection tiling tile ij containing query point activated 
active tiles aggregated set 
ff ij ij dimension dimension tiling tiling query point active tiles cmac organization 
state action pair represented dot activates tile tiling 
sum weights activated tiles represents associated state action pair 
ffl function evaluation weight associated tiles summed 
ij ij ffl td update learning update performed tile cmac 
deltaw ij ff fl gamma ij ij cmac values state action pairs respectively 
ffl eligibility update tiles cmac updated contributions 
ij jf xq uq fl ij jf constant represents number tiles set 
note tile cmac represents weight function approximator active tile contribution jf estimating value 
cmac function approximator computationally efficient methods selection tiles size tile tiling constant 
td updates proportional number tiles cmac 
may deteriorate performance cmac situations tiles 
generalization resolution equation corresponds replace see 
naive cmac implementations active tile tiling 
number active tiles query point equal total number tilings cmac jf 
dimension query point active cases density parameter smoothing parameter memory function approximator 
state action pair represented dot activates cases close similarity metric euclidean distance 
weighted average values activated cases represents associated state action pair 
cases closer contribute final value cases farther 
new cases created nearest neighbors far away 
cmac depends number tilings number tiles tiling 
number tiles better resolution number tilings better generalization 
storage proportional total number tiles 
memory function approximators class sparse coarse coded memory memory 
memory function approximators widely conjunction reinforcement learning common tasks classification robot control see 
memory function approximator memory element represents state action pairs case agent experienced 
query performed retrieving nearest neighbors query point similarity metric performing weighted average values 
additionally new cases created added memory nearest neighbors far away query point 
way memory expands dynamically demand new regions state action space explored 
size memory function approximator dynamic 
memory starts empty grows density threshold determine new case added memory 
distance nearest neighbor query point greater new case added memory placed position query point 
small thresholds tend produce function approximators high resolution large amounts memory due high density cases 
choice density threshold depends problem 
shows bidimensional example memory function approximator 
choice similarity metric depends application 
common measure similarity metric continuous domains euclidean distance 
metric useful function expected continuous smooth state action space 
compute value associate input weighted average nearest neighbors performed kernel function gaussian smoothing parameter controls blending values nearest neighbors average 
generalization memory function approximators results combination weighted average nearest neighbors determined density cases memory determined design smoothing parameter larger density parameter way cases contribute value query point time large portions domain covered fewer cases 
describe implementation memory function approximators instance case 
differ structure memory elements selection nearest neighbors procedures compute weighted averages 
instance version uses storage useful value function smooth continuous 
casebased version uses storage able perform better generalizations achieve greater resolution instance version 
description structure basic operations function approximator follows ffl instance function approximator cases instance function approximator simple structure 
case consists state action combination agent past stores approximated value combination 
distance function required generalize values cases combined state action space 
may produce overgeneralization cases similar query state query action determine value state action pair 
case structure case consists state action pair agent experienced past associated value eligibility 
nearest neighbors selection similarity function compute distance query point point associated case memory 
set nearest neighbors query point nn determined rule nn fc memory function evaluation kernel function delta gaussian exp gammad compute relative contribution case value query point 
final value average nearest neighbor cases weighted relative contributions 
nn td update learning update performed case memory 
deltaq ff fl gamma memory values state action pairs respectively 
eligibility update cases memory updated relative contributions 
nn fl case addition criterion new case added memory distance closest neighbor min minfd larger threshold parameter ffl case function approximator case function approximator cases contain information stateaction pairs 
case represents region state space agent visited past set actions agent may may executed region state space set approximated values associated action 
additionally uses distance functions kernel functions generalize blend values state action spaces independently 
reasons case function approximator better resolution generalization capabilities instance function approximator may prove useful function changes abruptly actions similar states 
case version requires computation perform function evaluation td updates instance version 
case structure case consists state point agent experienced past associated value generalizes value actions state eligibility additionally case stores set set different actions ij associated values ij ij delta delta delta 
fu ij fq ij fe ij delta delta delta 
nearest neighbors selection similarity function associated state space compute distance query state state case memory 
set nearest neighbors query state nn determined rule nn fc memory case version similarity state space considered find nearest neighbors 
consequence case may selected near neighbor query action different stored case 
case contribute different values determine value query action 
produces increase generalization resolution function approximator value state action pair may estimated approximately agent tried number actions similar states time elements case represent value higher accuracy different actions similar states 
function evaluation evaluation function approximator query point requires kernel functions delta delta 
determine relative contribution case value query state equation corresponds replace 
determine relative contribution action ij case value query action evaluation performed phases phase similarity function associated action space compute distance ij query action action ij action case nearest neighbor set 
qvalue query action determined case weighted average gamma ae ae ij ij ij ij nn ae parameter controls blending value associated state values associated action ij represents value query point case blend value associated state weighted average values associated actions 
second phase distances associated state space kernel function delta determined relative contribution case merge weighted average 
equation shows computation 
nn td update learning update performed case memory action case 
deltaq ff fl gamma memory deltaq ij ff fl gamma ij ij values state action pairs respectively 
eligibility update cases memory actions case updated relative contributions 
gamma ae nn fl ij ae nn fl ij case addition criterion new case added memory distance closest neighbor min minfd larger threshold parameter memory function approximators memory efficient require computation calculate value state action pair perform td updates 
main advantage memory function approximators ability dynamically allocate resources regions state action space require 
equations corresponds replace 
original space skewed space important region enlarged region reduced region skewing function uniform function approximator unimportant region resources non uniformly skewing function 
function expands important regions compresses unimportant ones 
skewed space input uniform function approximator 
non uniform preallocation resources function approximators may resolution represent function entire state action space 
class function approximator provide different ways distributing resources non uniformly state action space regions resolution 
example may tilings non uniform tile sizes memory function approximators may different density thresholds different regions state action space 
designing non uniform function approximators complex designing uniform counterparts 
non uniform designs hinder computational efficiency memory storage function approximator 
idea distributing resources non uniformly inspired quantization techniques widely digital communication theory 
objective approximate analog signal quantized version error minimized 
purpose quantization levels allocated regions amplitude signal frequently reduces average error performed quantization process see example 
proposed technique takes advantages simplicity efficiency uniform versions function approximators equivalent elaborate accurate non uniform counterparts 
technique consists distorting state action space applying skewing function skewed state action space input uniform function approximator 
way skewing function designed expand compress different regions state action space 
expanded regions cover area skewed state action space resources function approximator consequently better resolution map value function 
conversely compressed regions cover area fewer resources function approximator resolution map value function 
restrictions skewing function able map point state action space point skewed state action space 
demonstrates graphically process resources non uniformly 
design principle follow research specify skewing function inspired optimization techniques digital communication theory 
field skewing function determined optimize design criterion mean square quantization error probability distribution input signal 
main factors influence mean square input state weights skewing function delta delta output best action algorithm 
action min umax step delta compute skewed state action pair 
evaluate qw 
keep action produces maximum value 

return step search algorithm skewing function 
quantization error difference quantized real value input point number times input point evaluated 
mean quantization error proportional product actual error input point number times input point 
idea optimization mean squared quantization error reduce error heavily input points frequently 
principle empirically design skewing function function approximator resolution regions state action space frequently 
increased resolution regions allow function approximator closer real value experience incur error desired 
priori knowledge system guide design skewing function 
knowledge required expected frequency regions state action space 
skewing function designed way expands frequently regions compresses infrequently 
alternatively function approximator design dynamically allocate resources regions state action space frequently 
way resolution function approximator increases automatically need prior knowledge 
implementation non uniform functions approximators step search sarsa algorithms require little modification accommodate non uniform function approximators 
cases standard implementation uniform function approximator skewed state action pair accomplish effect nonuniform resource preallocation 
figures show new versions step search sarsa algorithms additions shown bold 
results section describes results uniform non uniform versions cmac instancebased case function approximators problems continuous state actions spaces 
problems studied double integrator pendulum swing 
linear input initial set weights skewing function delta delta algorithm 
initialization perceive current state curr select action curr ffl greedy policy curr 
compute skewed state action pair curr curr curr curr 

accumulate gradient rw curr curr flw gamma 
perform action execute curr observe resultant reward state 
select action ffl greedy policy 
compute skewed state action pair 

learn ff fl gamma curr curr 
loop curr curr curr goal state terminate goto 
gradient descent version sarsa algorithm skewing function 
position velocity acceleration target double integrator 
car moving flat terrain subject application single force 
dynamics system second non linear quadratic costs negative quadratic rewards depend state action values 
function approximators double integrator cmac instance ones pendulum swing cmac case 
source code perform experiments compressed tar file available anonymous ftp url ftp ftp cc gatech edu pub ai students carlos rli experiments tar gz source code follows standard software interface reinforcement learning problems developed richard sutton juan ia double integrator double integrator system linear dynamics bidimensional state 
represents car unit mass moving flat terrain subject application single force see 
state system consists current position velocity car vectorial representation 
action acceleration applied system vectorial representation 
objective move car starting state origin sum rewards maximized 
step reward function negative quadratic function difference current desired position acceleration applied gamma vectorial representation gamma gamma gamma ru positive definite theta theta matrices respectively defined 
step reward function penalizes agent heavily distance current desired states large action applied large 
type reward function widely robotic applications specifies policies drive system desired state quickly keeping size driving control small 
tradeoffs specified appropriate selection matrices 
formulation standard optimal control theory objective minimize costs maximize rewards 
formulations mathematically equivalent 
documentation standard software interface url envy cs umass edu people sutton html 
dynamics double integrator simple described equations dp dt dv dt acceleration bounded range minimum maximum acceleration values min max min gamma max 
double integrator instance general class linear dynamic systems expressed convenient linear matrix equation deltat deltat ax deltat bu deltat simulation time step deltat seconds new control actions selected time steps 
trial consists starting system position velocity running system decision steps elapsed simulated seconds state gets bounds jpj jvj whichever comes 
case agent receives negative reward punishment units discourage going bounds 
experiment consists replications trials measuring number time steps cumulative cost negative sum rewards replication trial 
average number time steps cumulative cost replications measures performance 
optimal solution systems linear dynamics quadratic cost functions simple closed form solution optimal policy known linear quadratic regulator lqr 
derivation follows solution hamilton bellman jacobi partial differential equation see example details 
dynamical system dimensional state vector dimensional action vector linear dynamic function gamma gamma theta theta matrices respectively step quadratic cost negative reward function form gamma gamma gamma gamma gamma positive definite theta theta matrices respectively 
optimal policy gammak gamma associated optimal value function gamma gamma theta theta matrices equations respectively 
equation known continuous time equation find unknown matrix equation find called gain matrix 
gamma gammaq gamma gamma pa pbr gamma constant corresponds desired goal state constant corresponds required force maintain system desired state system applying results change 
continuous time equation simplified time invariant policies 
case 
equation simplifies ap pbr gamma resulting optimal controller referred linear quadratic regulator system 
optimal value function double integrator 
value function vertical axis plotted position velocity horizontal plane 
states near origin values closer zero states far origin takes time cost drive system origin 
double integrator values described previously 
solution time invariant case produces equation optimal actions gammak gamma instantaneous velocity position respectively 
figures show optimal value function optimal trajectory generated optimal policy 
simulator optimal controller achieves performance gamma initial state discrepancy actual sum rewards predicted value function due discretization time considered significant 
shows optimal trajectory cumulative reward controller different points trajectory 
uniform cmac configuration agent cmac approximate function 
dimensions interest position velocity acceleration cmac tilings 
dimensions divided intervals 
tilings depended variables 
tilings depended position velocity pairs remaining tilings depended dimension position velocity 
tilings sharing dimensions uniformly dimension 
cmac configurations tried double integrator optimal trajectory 
starting point center right goal point center 
optimal trajectory discounted sum rewards 
discounted sum rewards vertical axis decreases faster system far origin closer origin 
table summary design agent uniform cmac function approximator double integrator 
factor description variables position gamma intervals velocity gamma intervals acceleration gamma intervals tilings uniformly 
uniformly 
uniformly 
uniformly 
step search ffl greedy policy ffl delta amax gammaa min learning sarsa algorithm fl ff compared empirically configuration described produced best results 
sarsa algorithm updating weights cmac traces indicated equation 
values free constants fl ff ffl 
step search performed equally spaced values acceleration minimum value min gamma maximum value max delta max gamma min 
table summarizes agent design 
figures show average number time steps accumulated cost trial respectively 
error bars represent sigma standard deviations mean replications 
non uniform cmac configuration agent cmac configuration previous experiment 
cmac skewed versions variables interest position velocity acceleration 
skewing functions follows shows effect skewing functions class 
function expands region near origin cmac tiles represent value function accurately region 
larger value larger expansion 
dimension corresponding position skewed heavily velocity acceleration 
chose skewing function design knew system spend time near origin orbiting goal state order reduce punishments 
inferred region near origin state action space require resolution function approximator 
additionally step reward function depends position results function sensitive position velocity refer trial double integrator uniform cmac average steps trial uniform cmac 
trial double integrator uniform cmac average cumulative cost trial uniform cmac 
matrix 
learning algorithm free constants uniform cmac 
table summarizes agent design 
figures show average number time steps accumulated cost trial respectively 
error bars represent sigma standard deviations mean replications 
uniform instance experiment agent instance representation function 
case represents point state action space holds associated value 
density threshold smoothing parameters set respectively 
similarity metric euclidean distance kernel function gaussian 
produces cases spherical receptive fields blending function small number cases 
sarsa algorithm update values case eligibility traces indicated equation 
values free constants fl ff ffl 
step search performed equally spaced values acceleration minimum value min gamma maximum value max delta max gamma min 
table summarizes agent design 
figures show average number time steps accumulated cost trial respectively 
error bars represent sigma standard deviations mean replications 
non uniform instance agent instance configuration previous experiment skewed version variables interest 
skewing functions non uniform cmac 
effect instance function approximator skewed state action space similar having nonconstant density smoothing parameters 
density cases increases near origin region gets expanded skewing function 
similarly smoothing parameter table summary design agent non uniform cmac function approximator double integrator 
factor description variables position gamma intervals velocity gamma intervals acceleration gamma intervals tilings uniformly 
uniformly 
uniformly 
uniformly 
skewing functions step search ffl greedy policy ffl delta amax gammaa min learning sarsa algorithm fl ff variable variable skewing functions 
left skewing function right skewing function uniform intervals skewed variable vertical axis correspond non uniform intervals variable horizontal axis 
expansion near origin stronger value increases 
trial double integrator non uniform cmac average steps trial nonuniform cmac 
trial double integrator non uniform cmac average cumulative cost trial non uniform cmac 
table summary design agent uniform instance function approximator double integrator 
factor description variables position gamma velocity gamma acceleration gamma distance function euclidean gamma gamma gamma density threshold kernel function gaussian exp smoothing parameter step search ffl greedy policy ffl delta amax gammaa min learning sarsa algorithm fl ff trial double integrator uniform instance average steps trial uniform instance 
trial double integrator instance uniform average cumulative cost trial uniform instance 
trial double integrator non uniform instance average steps trial nonuniform instance 
trial double integrator non uniform instance average cumulative cost trial non uniform instance 
adjusts accordingly increased density cases near origin produce blending values 
table summarizes agent design 
figures show average number time steps accumulated cost trial respectively 
error bars represent sigma standard deviations mean replications 
pendulum swing pendulum non linear dynamics system bidimensional state 
represents single bar held extremum swing vertical plane 
bar actuated motor applies torque hanging point see 
state system consists current angle angular velocity pendulum vectorial representation 
action angular acceleration ff applied system ff vectorial representation 
objective move pendulum rest position hanging velocity table summary design agent non uniform instance function approximator double integrator 
factor description variables position gamma velocity gamma acceleration gamma distance function euclidean gamma gamma gamma density threshold kernel function gaussian exp smoothing parameter skewing functions step search ffl greedy policy ffl delta amax gammaa min learning sarsa algorithm fl ff bar mg pendulum 
bar hanging extremum subject gravity torque applied motor 
origin state space bar facing velocity sum rewards maximized 
step reward function class double integrator 
negative quadratic function difference current desired angular position angular velocity angular acceleration applied gamma delta ff vectorial representation gamma deltax deltax ru positive definite theta theta matrices respectively defined 
dynamics pendulum equations 
dt ml ff sin dt 
mass length bar respectively gravity 
angular acceleration bounded range minimum maximum angular acceleration values ff ff min ff max ff min gamma ff max 
system difficult ones linear dynamics 
double integrator closed form analytical solution exist optimal solution complex numerical methods required compute 
maximum minimum angular acceleration values strong move pendulum straight starting state creating angular momentum 
optimal solution requires apply action moves system direction goal state order build momentum able swing bar top 
additionally equilibrium point goal state unstable means agent needs manage swing bar actively balance goal position 
difference actual desired angle positions performed modulo denoted delta 
table summary design agent uniform cmac function approximator pendulum swing 
factor description variables position gamma intervals velocity gamma intervals acceleration ff gamma intervals tilings ff uniformly 
uniformly 
uniformly 
uniformly 
step search ffl greedy policy ffl delta ff max gammaff min learning sarsa algorithm fl ff double integrator simulation time step delta seconds new control actions selected time steps 
trial consists starting system position gamma angular velocity running system decision steps elapsed simulated seconds angular velocity pendulum gets bounds 

double integrator angle link wrap causing trial terminate 
experiment consists replications trials measuring number time steps cumulative cost negative sum rewards replication trial 
average number time steps cumulative cost replications measure performance 
uniform cmac double integrator configuration agent uniform cmac approximate function 
cmac consisted tilings state action variables arranged configuration double integrator intervals dimension tiling depending variables tilings depended position velocity pairs remaining tilings depended dimension position velocity 
tilings sharing dimensions uniformly dimension 
sarsa algorithm updating weights 
values free constants fl ff ffl 
step search performed equally spaced values angular acceleration minimum value ff min gamma maximum value ff max delta ff max gamma ff min 
table summarizes agent design 
figures show average number time steps accumulated cost trial respec trial single pendulum uniform cmac average steps trial uniform cmac 
trial single pendulum uniform cmac average cumulative cost trial uniform cmac 
tively 
error bars represent sigma standard deviations mean replications 
non uniform cmac experiment agent non uniform cmac configuration previous experiment 
skewing functions variables follows ff ff 
double integrator skewing functions expands regions near origin cmac tiles represent value function resolution 
chose design know system spend time balancing bar corresponds region near origin 
double integrator skewing function position velocity step reward function depends equally variables refer matrix 
learning algorithm free constants uniform cmac 
table summarizes agent design 
show average number time steps accumulated cost trial replications 
error bars represent sigma standard deviations mean replications 
uniform case experiment agent case representation function 
performed experiments different configurations instance function approximator agent able successfully learn maintain bar balanced reached goal 
appears instance function approximator resolution effectively represent value function due complexity non linear dynamics pendulum 
case function approximator uses resources better resolution instancebased 
case case function approximator represents point state space value equally spaced actions interval ff min ff max 
blending factor set ae means value computations case contributes value associated state values associated actions 
density threshold set 
smoothing parameters state table summary design agent non uniform cmac function approximator pendulum swing 
factor description variables position gamma intervals velocity gamma intervals acceleration ff gamma intervals tilings ff uniformly 
uniformly 
uniformly 
uniformly 
skewing functions ff ff step search ffl greedy policy ffl delta ff max gammaff min learning sarsa algorithm fl ff trial single pendulum non uniform cmac average steps trial nonuniform cmac 
trial single pendulum non uniform cmac average cumulative cost trial non uniform cmac 
trial single pendulum uniform case average steps trial uniform case 
trial single pendulum uniform case average cumulative cost trial uniform case 
space action space set respectively 
similarity metric input space weighted euclidean distance kernel function gaussian 
produces cases elliptical receptive fields major axis oriented diagonally state space bias function approximator generalize regions angle velocity pendulum sign 
kind bias proved useful bar reached goal position active balancing began 
similarity metric output space euclidean distance 
sarsa algorithm update values case eligibility traces indicated equations 
values free constants fl ff ffl 
step search performed equally spaced values acceleration minimum value ff min gamma maximum value ff max delta ff max gamma ff min 
table summarizes agent design 
figures show average number time steps accumulated cost trial respectively 
error bars represent sigma standard deviations mean replications 
non uniform case agent case configuration previous experiment skewed version variables interest 
skewing functions non uniform cmac 
effect case function approximator skewed state action space similar having non constant density smoothing parameters 
density cases increases near origin region gets expanded skewing function 
similarly smoothing parameter adjusts accordingly increased density cases near origin produce blending values 
table summarizes agent design 
figures show average number time steps accumulated cost trial respectively 
error bars represent sigma standard deviations mean replications 
table summary design agent uniform case function approximator pendulum swing 
factor description variables inputs position gamma velocity gamma output acceleration ff gamma case structure number actions equally spaced gamma blending factor ae distance functions input space weighted euclidean dm gamma gamma gamma density threshold output space euclidean dm jff gamma ff kernel functions input space gaussian exp smoothing parameter output space exp smoothing parameter step search ffl greedy policy ffl delta ff max gammaff min learning sarsa algorithm fl ff table summary design agent uniform case function approximator pendulum swing 
factor description variables inputs position gamma velocity gamma output acceleration ff gamma case structure number actions equally spaced gamma blending factor ae distance functions input space weighted euclidean dm gamma gamma gamma density threshold output space euclidean dm jff gamma ff kernel functions input space gaussian exp smoothing parameter output space exp smoothing parameter skewing functions ff ff step search ffl greedy policy ffl delta ff max gammaff min learning sarsa algorithm fl ff trial single pendulum non uniform case average steps trial nonuniform case 
trial single pendulum non uniform case average cumulative cost trial non uniform case 
discussion section discusses detail results obtained double integrator swing pendulum problems 
double integrator table shows average standard deviation cumulative cost steps cmac instance function approximators trial 
effect resources skewing functions clear cmac instance function approximators 
cmac statistical evidence confidence level trial cumulative cost achieved agent nonuniform version smaller achieved uniform version value 
additionally learning performance agent non uniform cmac stabilized faster approximately trial uniform cmac approximately trial see figures 
similarly time agent able stay bounds trial occurred trial non uniform cmac 
event occured trial agent uniform cmac see figures 
trials agent test hypotheses assume unknown different variances statistic gamma testing case distributed approximately degrees freedom gamma null hypothesis true 
non uniform cmac consistent agent uniform cmac smaller variance performance non uniform oe uniform oe 
summarizing main effects cmac tiles heavily near origin state action space improvement learning performance faster learning better consistency 
expected system spends time origin trying maximize rewards 
requires small adjustments small changes acceleration ability represent function resolution region state space 
skewing functions represent non uniform cmac designed purpose 
effect resource preallocation significant agent instance function approximator 
trial statistical evidence confidence level reject hypothesis versions uniform non uniform perform similarly value 
additionally improvement learning performance achieved agent non uniform version agent uniform able stabilize performance trial took trial see figures 
agent non uniform version learned stay bounds trial trail 
event occurs trial uniform version see figures 
cmac version significant difference consistency performance variance non uniform version smaller variance uniform version non uniform oe uniform oe 
summary effect resource preallocation instance function approximator similar observed cmac function approximator 
instance version allocates resources dynamically creating new cases exploring new regions state action space effect skewing functions appears show heavily advantage dynamically allocating resources task 
appears statistical significant difference performance achieved uniform versions cmac instance function approximators gamma value 
statistical significant difference performance achieved non uniform versions instance version appears improvement cmac value 
difference may dependent design decisions involved types function approximators sensibility decisions learning behavior agent research needed issue 
additionally contrasting performance curves agents agents instance function approximator appears agents consistent ones 
increased consistency achieved cmac presumably due capacity smoothly generalize values new regions state action space 
occur instancebased version abrupt change peak value function time new case added library 
pendulum swing table shows average standard deviation cumulative cost steps cmac case function approximators trial 
agent instance function approximator able successfully learn maintain bar balanced reached goal 
effect resources skewing functions shows significantly cmac case function approximators 
cmac statistical table summary results double integrator trial 
function approximator skewing cumulative cost steps mean std 
dev 
mean std 
dev optimal cmac uniform non uniform instance uniform non uniform cumulative cost sum immediate cost agent received step trial 
lower values represent better solutions 
steps number steps agent took trial 
mean value standard deviation means agent completed replications successfully 
evidence trial cumulative cost achieved non uniform version better achieved uniform version value 
additionally learning performance agent non uniform cmac stabilized faster approximately trial uniform cmac approximately trial see figures 
similarly time agent able stay bounds trial replications occured trial agent non uniform cmac 
event occured trial agent non uniform cmac see figures 
trial agents uniform non uniform cmac achieved similar level consistency replication uniform oe non uniform oe 
summarizing main effects cmac tiles heavily near origin state action space improvement learning performance faster learning 
results similar ones achieved double integrator problem pendulum swing problem difficult 
improvement performance cumulative cost agent uniform cmac agent non uniform cmac reveals importance function approximator higher resolution region near goal state 
pendulum highly unstable region agent required perform small adjustments small changes acceleration order keep bar balanced 
mistake causes agent loose control balance bar produces lot cost bar fall oscillate 
effect resource preallocation case function approximators show clearly cmac function approximators 
trial statistical evidence reject hypothesis versions uniform non uniform perform similarly gamma value 
noticeable difference learning performance versions take trials stabilize see figures trials learn stay bounds see figures 
significant difference consistency performance trial variance non uniform version smaller variance uniform version non uniform oe uniform oe 
agent uniform version able stay bounds replications trial agent non uniform version replications achieve steps trial see figures 
presumably skewing functions focus resources excess region near goal state produces insufficient generalization table summary results pendulum swing trial 
function approximator skewing cumulative cost steps mean std 
dev 
mean std 
dev cmac uniform non uniform instance uniform non uniform case uniform non uniform cumulative cost sum immediate cost agent received step trial 
lower values represent better solutions 
steps number steps agent took trial 
mean value standard deviation means agent completed replications successfully 
agent regain control bar looses balance force system go 
summary effect resource preallocation case function approximator notorious cmac function approximator 
expected case version allocates resources dynamically creating new case exploring new regions state action space 
type function approximator effects skewed variables state action space 
appears strong differences performance trial agents non uniform cmac uniform case non uniform case function approximators 
cumulative cost achieved agents types function approximators units achieved agent uniform cmac units see table 
context learning rate consistency performance replications agents case versions appear take trials stabilize performance agent cmac version appears incur greater costs consistency earlier trials learning curve 
differences drastic ones obtained contrasting performance agents performance agent uniform cmac 
summary learning performance achieved agents case versions learning performance achieved non uniform cmac 
effect resource preallocation shows clearly agents type function approximator allocate resources dynamically regions state action space require 
summary different types function approximators cmac memory tested double integrator pendulum swing problems 
problems characterized dynamical systems having continuous state action spaces results obtained experiments support feasibility function approximator represent function stateaction space generalizing value individual experiences states actions 
additionally results obtained demonstrate feasibility modular implementation non uniform function approximators skewing functions 
proposed approach appears benefit efficiency function approximators static resource allocation cmac 
hand proposed approach appears affect efficiency function approximators dynamic resource allocation case 
particular effect non uniform resource preallocation resulted faster learning better performance improved consistency 
results showed significantly pendulum swing double integrator presumably difficult actively balance pendulum car goal state 
effect function approximator high resolution region containing goal state shows significantly pendulum swing double integrator 
memory function approximator evaluated instance case 
instance function approximator uses state action pairs agent experienced represent function new state action pairs incorporated distance nearest neighbor measured metric function larger density threshold 
case function approximator uses memory elements complex simply state action pairs 
case represents state agent visited past set actions associated values 
actions case actions agent may may experienced 
enhanced representation better support credit assignment generalization agent able update associated values actions experienced state action pairs similar distance function 
case function approximator able solve problems instance function approximator may solve case pendulum swing 
research explored important issues related reinforcement learning applied systems described continuous state action spaces 
question asked possible ways generalize outcome experiences involving states actions continuous spaces 
described techniques researchers represent value function proposed alternative method represent generalize value states actions pairs outcome individual experiences 
idea consists function approximator represent function combined state action space familiar sarsa learning algorithm adapt parameters function approximator td backups 
disadvantage approach compared simpler ones step search computationally expensive 
efficient implementations step search may possible exploiting class function approximator 
additionally theoretical guarantee approach converge correct function function approximator may able represent exact value function entire state action space results obtained different kinds sparse coarse coded function approximators different classes continuous systems encouraging 
results support idea function approximators solve reinforcement learning problems systems continuous state actions 
function approximators able generalize value unseen regions state action space estimates regions state action space 
provide compact representation function trained incrementally agent collects data 
second question asked effect designing function approximators resource preallocated state action space 
important design decision directly affects resolution capabilities function approximator different regions state action space 
usually function approximators uniformly distribute resources efficient easier implement non uniform counterparts 
hypothesized need nonuniform function approximators due better resources obtained increasing resolution function approximator important regions state action space 
results obtained tend support hypothesis 
addition technique combines simplicity efficiency uniform function approximators benefits nonuniform ones 
technique skewing function transform original state action space deformed version domain uniform function approximator 
skewing function designed advance expand important regions state action space function approximator resources estimate corresponding values resolution 
skewing function adds little computational complexity procedure 
explored feasibility memory function approximators reinforcement learning problems continuous state action spaces 
memory function approximators belong class sparse coarse coded function approximators dynamically allocate resources experiences 
results obtained research showed memory function approximator sensible preallocation resources cmac function approximator 
conjecture effect may due capability allocating resources regions state action space allows dynamically adjust resolution regions 
may serve advantage designers know advance regions state action space function approximator represent higher resolution 
experiments skewing functions remained constant experiment 
possible adaptive skewing functions agent dynamically increase resolution function approximator collects data 
research address issue 
albus 
new approach manipulator control cerebellar model articulation controller cmac 
journal dynamic systems measurement control september 
atkeson 
memory learning control 
proceedings american control conference volume pages boston ma 
barto sutton anderson 
neuronlike elements solve difficult learning control problems 
ieee transactions systems man cybernetics 
bellman 
dynamic programming 
princeton university press princeton nj 
bertsekas 
dynamic programming optimal control volume 
athena scientific belmont ma 

truncating temporal differences efficient implementation td reinforcement learning 
journal artificial intelligence research 
kanerva 
sparse distributed memory related models 
editor associative neural memories theory implementation chapter 
oxford university press new york ny 
kibler aha 
instance prediction real valued attributes 
computational intelligence 
lin :10.1.1.75.7884
self improving reactive agents reinforcement learning 
machine learning 
mahadevan connell 
scaling reinforcement learning robotics exploiting subsumption architecture 
proceedings international workshop machine learning volume pages 
morgan kaufmann 
mccallum tesauro touretzky leen 
instance state identification reinforcement learning 
advances neural information processing systems 
moore atkeson 
parti game algorithm variable resolution reinforcement learning multidimensional state spaces 
machine learning 
narendra 
stable adaptive systems 
prentice hall englewood cliffs nj 
peng 
efficient dynamic programming learning control 
phd thesis department computer science northeastern university 
peng williams :10.1.1.56.7356
incremental multi step learning 
machine learning proceedings eleventh international conference pages aberdeen scotland 
ram ia continuous case reasoning 
artificial intelligence 
richards 
dynamics control 
longman new york ny 
rummery niranjan 
line learning connectionist systems 
technical report cued tr cambridge university department 

digital analog communication systems 
longman new york ny 
singh sutton 
reinforcement learning replacing eligibility traces 
machine learning 

optimal control estimation 
dover publications ny 
sutton :10.1.1.132.7760
learning predict methods temporal differences 
machine learning 
sutton :10.1.1.51.4764
generalization reinforcement learning successful examples sparse coarse coding 
advances neural information processing systems 
tham 
reinforcement learning multiple tasks hierarchical cmac architecture 
robotics autonomous systems 
tsitsiklis van roy 
analysis temporal difference learning function approximation 
advances neural information processing systems 
watkins 
learning delayed rewards 
phd thesis univeristy cambridge england 

