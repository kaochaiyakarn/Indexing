influence learning evolution busy francisco pereira costa instituto superior de de coimbra da nora coimbra portugal centro de sistemas da universidade de coimbra ii coimbra portugal dei uc pt 
goal research study individual learning interacts evolutionary algorithm search candidates busy beaver problem 
learning models designed act local search procedures proposed 
experimental results show local search methods able perform modifications structure individual learning step provide important advantage 
insight role evolution learning play search 
evolution learning major forces promote adaptation individuals environment 
evolution operating population level includes mechanisms genetic changes occur organisms generations 
learning operates different time scale 
gives individual ability modify phenotype life order increase adaptation environment chance selected reproduction 
standard evolutionary computation ec optimisation learning usually implemented local search algorithms 
methods iteratively test alternatives neighbourhood learning individual trying discover better solutions 
learning process quality individual measure initial fitness ability improve leads better understanding fitness landscape 
combination local search heuristics ec techniques contexts situations known memetic algorithms 
designation inspired richard concept meme represents unit cultural evolution exhibit plastic adaptation 
research interested studying learning evolution may combined computer simulations 
busy beaver bb problem testbed study mentioned interactions 
proposed problem context existence non computable functions 
defined follows suppose turing machine tm way infinite tape tape alphabet blank 
question asked maximum number written state halting tm started blank tape 
number function number states denoted 
tm produces non blanks cells called busy beaver 
bb considered interesting theoretical problems proposal attracted attention researchers 
values corresponding tms known today small values number states increases problem harder candidates set lower bounds value 
prove particular candidate state bb perform exhaustive search space state tms verify machine produces higher number ones 
extremely complex due halting problem 
search space bb problem possesses characteristics dimension complexity extremely appealing ec field 
attempts apply ec techniques reported past years different levels success 
previous proposed local search algorithms designed act learning procedures seeking solutions problem 
experimental results showed unable improve performance evolutionary algorithm 
partial explanation proposed mentioned suggested combination factors local search methods completely ineffective structure landscape behaviour learning procedures 
topology search landscape highly irregular complex structure 
performed empirical analysis verified different areas search space small groups neighbour valid solutions bb problem 
size groups quality tms compose varies tend surrounded large low fitness areas composed invalid solutions 
combination factors space prone premature convergence 
simulation ec methods quickly identify areas 
high evolutionary pressure area impossible escape premature convergence 
hand learning algorithms study act typical hill 
learning step perform modification current solution accepting lead decrease fitness 
described situation efforts learning methods exploration close neighbourhood ineffective escape basin attraction group valid tms 
introduce new local search method able perform modifications structure tm learning step 
goal determine influence new situation evolutionary process 
literature reports learning methods carry modification step overcome limitations searching difficult landscapes 
structure section formal definition bb problem 
section describe evolutionary model including learning procedures 
section comprises experimental details simulation 
section results experiments performed analyse 
section suggest directions 
busy beaver problem deterministic tm specified finite set states alphabet input symbols alphabet tape symbols transition function start state final state 
transition function assume forms 
usual qg qg denotes move head left move right 
machines transition function format called tuple tms 
common variation consists considering transition function form qg machines type known tuple tms 
performing transition tuple tm writes symbol tape moves head enters new state 
tuple tm writes new symbol tape moves head entering new state 
original definition bb considers tuple tms states states anonymous halt state 
tape alphabet symbols blank input alphabet 
productivity tm defined number initially blank tape machine halts 
machines halt productivity zero 
defined maximum productivity achieved state tm 
tm called busy beaver 
tuple variant productivity defined length sequence produced tm started blank tape halting scanning leftmost sequence rest tape blank 
machines halt halt configuration productivity zero 
machine halt reading leftmost string exception string tape blank 
research focus attention tuple variant 
experimental model representation experiments reported searching candidates tuple bb 
loss generality consider set initial state final state 
blank essential information needed represent potential solution reduced state transition table 
shows tuple tm states plus halting state state transition table 
codify information contained table integer string genes genes state format 
new state action blank new state action state new state action blank new state action state simulation evaluation evaluate individual simply decode information chromosome simulate resulting tm 
due halting problem establish limit maximum number transitions maxt 
machines don halt limit considered non halting tms 
assign fitness consider factors decreasing order importance halting reaching predefined limit number transitions accordance tuple rules productivity number transitions number steps halting 
consider factors assign fitness intend explore differences bad individuals 
fitness function tm leaves state considered worse goes states non halting machines productivity 
preliminary experiments approach proved effective productivity fitness 
state tuple tm corresponding transition table 
blank symbol represented 
learning models evaluation individual selected learning 
research perform experiments different local search procedures 
random local search rls 
current tm machine built information encoded chromosome individual selected learning perform actions 
select transition simulation current tm 

randomly modify action performed transition 
evaluate resulting tm 
possible actions transition write blank write move left move right 
blank new state action new state action 
fitness resulting tm equal higher fitness current tm resulting tm current 

maximum number learning steps learning 
go 
rls identical proposals previous 
learning cycle rls performs modification structure tm accepting lead decrease fitness 
changes structure tm limited actions performed transitions 
ensure restriction biasing results performed additional tests enabling rls change actions new states verified significant difference outcomes 
results achieved experiments rls useful just comparison measure results obtained new algorithm multi step learning msl 
important difference rls msl new method learning cycle individual performs changes structure 
algorithmic description msl follows multi step learning msl 
current tm machine built information encoded chromosome individual selected learning perform actions 
select transition simulation current tm lead final state 

randomly modify action performed transition 

probability randomly modify state transition leads 
final state considered possibility selecting new destiny 

state leads 
select equal probability transition state 
randomly modify action performed transition 

evaluate resulting tm 

fitness resulting tm equal higher fitness current tm resulting tm current 

maximum number learning steps learning 
go 
modifications structure tm done components directly connected starting action transition destiny state transition changes probability action transitions state 
msl individual possibility jump point space close current position 
hope give learning individual higher chance escape local optima allow evolutionary process perform better 
lamarckian strategy learning 
lamarckian theory evolution claims phenotypic characteristics acquired individuals lifetime re encoded genes directly inherited descendants 
experiments learning period changes induced current tm coded back genotype learning individual 
lamarckian theory proved wrong biological systems idea usefully applied experiments ec field provided special considerations taken account 
prevent tendency lamarckian learning increase convergence rate evolutionary algorithm global parameter learning rate lr defined probability individual subject learning 
way able restrict number individuals learn generation hope control tendency premature convergence 
experimental settings experiments concern search tuple bb 
settings evolutionary algorithm number evaluations population size elitist strategy tournament selection size single point mutation mutation rate graph crossover maximum graph crossover size crossover rate maxt maximum number transitions lr 
graph crossover 
designed individuals graph structure 
main idea operator exchange sub graphs individuals 
maximum graph crossover size defines number states belonging sub graph 
results confirmed domain clearly outperforms classical crossover operators 
learning number steps number learning cycles performed individual set remains fixed experiments 
step counts evaluation 
initial population randomly generated set parameters performed runs initial conditions different random seeds 
section results distinct experiments individuals learn experiments learning procedures rls msl 
difference experiments learning method lr value 
values different settings set heuristically performed tests values verified moderate range significant difference outcomes 
results graph different experiments productivity best individual final generation runs 
brief perusal results suggests settings learning able cause considerable improvement search process 
remainder section try identify conditions required obtain improvements 
application ec techniques tuple bb productivity best known candidate 
adopt value threshold minimum quality focus attention runs able find tms higher productivity 
assuming see experiments msl able outperform experiment learning 
standard evolutionary approach able find tms productivity runs runs able find machines 
msl lr percentage raises runs 
rls able help search process 
results lrs inferior quality achieved experiments msl experiment 
best experiment rls tms productivity 
result confirms previous 
rls just performs small organisation tms 
structure landscape unable help search escape local optima areas 
graph productivity best individual final generation runs 
focus analysis results achieved msl 
clear results obtained experiments msl clearly better obtained experiments rls 
confirms msl effective method help evolution escape traps exist landscape 
searching solutions highly irregular landscapes important advantage learning individual allowed enlarge neighbourhood region region includes points individual allowed jump just learning step 
significant differences results achieved experiments msl suggesting ability perform larger jumps sufficient condition improve performance 
results graph show infrequent learning lr beneficial situation 
lr msl helps evolution find tms productivity 
number decreases lr lr 
surprising performance experiments msl decreases lr increases 
re encodes changes back genotype lamarckian learning strong mechanism pushes search fast local optimum 
landscape dealing effect magnified search process high probability converge local optimum 
looking left columns graph runs best tms productivity possible verify situation happens higher probability experiments lr 
convergence occurs nearly runs individuals subsequent generations able escape basin attraction 
productivity nr 
runs rls rls rls msl msl msl graph period simulation tms productivity 
addition increasing likelihood finding promising solutions msl helps evolution discover earlier 
graph period simulation tms productivity discovered 
discard tms millions steps probably due lucky move see experiments msl consistently candidates evaluations 
experiment learning started find candidates period 
gives credit conviction learning really helping evolution find solutions 
results collected experiments help clarify happening search process 
table contribution evolution learning discovery new best individuals 
contribution evolution includes new best individuals generated crossover mutation contribution learning includes new best individuals result application local search procedures 
values obtained experiments divided temporal periods 
look results line labelled msl experiment msl lr important features distinguish experiments learning 
number improvements obtained evolution 
period ranging evaluations number improvements due evolution similar values achieved experiment learning 
evaluations value superior vs remarkable result especially consider experiments lr approximately half evaluations spent learning process 
result suggests learning preventing evolution sampling space 
contrary helping evolution task 
second feature concerns relative weight learning process finding new solutions 
values parenthesis columns labelled learn represent percentage improvements period due learning 
lowest values line labelled msl 
evolution clearly effective learning process finding new best solutions 
final period search stable weight learning increases values similar ones evolution 
suggests evolutionary evaluations millions nr 
tms rls rls rls msl msl msl algorithm starting explore space moderate low lr lamarckian learning give clues best paths follow 
evolution guiding process 
role learning hints allow early discovery promising areas explore 
just shown small effect important obtain results 
different scenario occurs pressure lr high 
situation learning acts primary guiding force search process evolution plays secondary role 
learning context definition local procedure search nearest local optimum 
table contributions evolution columns labelled ev 
learning columns labelled learn improvement best solution simulation 
experiment results sum runs 
results divided temporal periods simulation 
values parenthesis columns labelled learn represent percentage improvements period due learning 
periods simulation evals 
millions ev 
learn ev 
learn ev 
learn ev 
learn rls rls rls msl msl msl studied interactions occur evolution learning searching solutions bb problem 
results experiments performed lamarckian framework showed procedure able perform modifications structure learning individual learning step beneficial 
identified conditions met maximise search performance evolutionary algorithm evolution primary force responsible sampling landscape 
moderate contribution learning procedure considerable degree freedom concerns definition local neighbourhood important help evolution task 
study kind interactions learning strategy known baldwin effect 
intend analyse conditions identified current extensible learning framework 
acknowledgments partially funded portuguese ministry science technology program praxis xxi 

belew mitchell 

adaptive individuals evolving populations models algorithms santa fe institute sciences complexity vol 
reading ma addison wesley 

pereira machado costa cardoso rodriguez soto 

busy learn 
proceedings congress evolutionary computation cec pp 


sasaki tokoro 

adaptation changing environments various rates inheritance acquired characters comparison darwinian lamarckian evolution 
mckay yao newton kim 
eds proceedings nd asia pacific conference simulated evolution learning seal 

whitley gordon mathias 

lamarckian evolution baldwin effect function optimization 
davidor schwefel manner 
eds 
parallel problem solving nature ppsn iii pp 


corne glover dorigo 

new ideas optimization 
mcgraw hill 


non computable functions bell system technical journal vol 
pp 

jones rawlins 

reverse hillclimbing genetic algorithms busy beaver problem 
forrest 
ed 
proceedings th international conference genetic algorithms icga pp san mateo ca morgan kaufmann 

machado pereira cardoso costa 

busy beaver influence representation poli nordin langdon fogarty 
eds 
proceedings second european workshop genetic programming 

bull 

baldwin effect 
artificial life vol 
pp 


pereira costa 

influence learning optimization royal road functions proceedings rd international mendel conference genetic algorithms optimization problems fuzzy logic neural networks rough sets mendel pp 

jeffrey 

computability logic cambridge university press 

ackley littman 

case lamarckian evolution 
langton 
ed artificial life iii pp 
addison wesley 

araki 

lamarckian evolution associative memory 
proceedings third international conference evolutionary computation icec pp 

pereira machado costa cardoso 

graph crossover case study busy beaver problem 
banzhaf daida eiben garzon honavar smith 
eds 
gecco proceedings genetic evolutionary computation conference pp 
morgan kaufmann 



representation busy beaver candidate turing machines technical report van gogh group rensselaer polytechnic institute 
