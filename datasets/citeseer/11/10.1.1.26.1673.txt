evolving optimal neural networks genetic algorithms occam razor tak zhang heinz muhlenbein artificial intelligence research division german national research center computer science gmd schloss birlinghoven sankt augustin germany mail zhang gmd de gmd de genetic algorithms neural networks main ways optimize network architecture train weights fixed architecture 
previous focuses options investigates alternative evolutionary approach called breeder genetic programming bgp architecture weights optimized simultaneously 
genotype network represented tree depth width dynamically adapted particular application specifically defined genetic operators 
weights trained ascent hillclimbing search 
new fitness function proposed quantifies principle occam razor 
optimal trade error fitting ability parsimony network 
simulation results benchmark problems differing complexity suggest method finds minimal size networks clean data 
experiments noisy data show occam razor improves generalization performance accelerates convergence speed evolution 
published complex systems evolving optimal neural networks constructing multilayer neural networks involves difficult optimization problems finding network architecture appropriate application hand finding optimal set weight values network solve problem 
genetic algorithms solving optimization problems 
weight optimization set weights represented chromosome genetic search applied encoded representation find set weights best fits training data 
encouraging results reported comparable conventional learning algorithms 
architecture optimization topology networks encoded chromosome genetic operators applied find architecture fits best specified task explicit design criteria 
optimization neural network architectures finding minimal network particular applications important speed accuracy learning performance dependent network complexity type number units connections connectivity units 
example network having large number adjustable connections tends converge fast usually leads overfitting training data 
hand small network achieve generalization converges needs generally large amount training time 
size network small possible sufficiently large ensure accurate fitting training set 
general way evolving genetic neural networks suggested muhlenbein kindermann 
works focused genetic algorithms separately optimization problem mainly optimizing network topology 
harp miller described representation schemes anatomical properties network structure encoded bit strings 
similar representation whitley prune unnecessary connections 
kitano gruau suggested encoding schemes network configuration indirectly specified graph generation grammar evolved genetic algorithms 
methods backpropagation algorithm gradient descent method train weights network 
koza provides alternative approach representing neural networks framework called genetic programming en evolving optimal neural networks ables modification weights architecture neural network 
method provides general method representing arbitrary feedforward network mechanism finding network minimal complexity 
describe new genetic programming method called breeder genetic programming bgp employs occam razor fitness function 
method optimal trade error fitting ability parsimony network preferring simple network architecture complex choice networks having fitting errors 
weights trained backpropagation ascent hillclimbing search 
organization follows 
section grammar representing multilayer feedforward neural networks 
section describes genetic operators control algorithm adapting architectures weights 
section derives fitness function genetic search minimal complexity solutions 
experimental results section followed analysis fitness landscapes section discussions section 
representing neural networks trees multilayer feedforward neural networks multilayer perceptrons networks simple processing elements called neurons units organized layers 
external inputs input layer fed forward layers hidden units output layer 
connection units layer 
commonly adopted architecture involves full connectivity neighboring layers 
allow partial connectivity direct connections non neighboring layers important finding parsimonious architecture 
specifically architecture allows input units connected directly output units 
compares usual multilayer perceptron general architecture adopted 
options type neural units 
confine mcculloch pitts neurons method described easily extended employ types neurons 
evolving optimal neural networks architectures multilayer perceptrons 
commonly architecture adopts full connectivity neighboring layers left architecture allows local receptive fields direct connections non neighboring layers right 
mcculloch pitts neuron binary device possible states 
neuron threshold 
neuron receive inputs excitatory inhibitory synapses 
input vector net input ith unit computed ij ij connection weight unit unit denotes receptive field unit neuron active sum weighted inputs exceeds threshold 
neuron inactive 
formally units activated threshold activation function denotes threshold value unit usually considered weight connected extra unit activation value 
despite simplicity mcculloch pitts neurons powerful 
fact shown finite logical expression realized 
case layer hidden layer architecture ith output evolving optimal neural networks network expressed function inputs weights ij jk label output hidden input units respectively 
note include input units hidden units direct connections input output units possible case identity function 
genetic optimization represent feedforward network set trees corresponding output unit 
shows grammar generating feedforward network input output units 
nonterminal symbol represent neural unit output units having threshold weights 
integer indicates receptive field width unit 
connection weight represented nonterminal node consisting symbol weight value followed nonterminal symbol indicating recursively neural unit external input unit external input described symbol followed integer denoting index input unit 
simulations binary thresholds 
mcculloch pitts neurons allow integer thresholds 
networks binary thresholds realize networks integer thresholds additional neurons 
similarly integer weights realized neurons binary weights 
number weights units usually reduced genotype transformed network integer values 
illustrated denote number units adjustable weights respectively 
binary weights useful trained simple hillclimbing search expensive gradient method 
possible disadvantage binary weight representation requires larger chromosome representation integer weights directly 
mean convergence accelerated automatically search space reduced integers binary weights 
advantage binary integer weights automatically functions regularizing factor avoiding arbitrary growing chromosome size 
evolving optimal neural networks nn gamma 
ym gamma 
gamma 
fy xg gamma 
gamma 
theta bin theta int theta bin gamma 
theta int gamma 
gamma omega bin omega int omega bin gamma 
omega int gamma 
sigma sigma sigma gamma 
gamma 
grammar generating genotype feedforward network mcculloch pitts neurons 
network represented set trees having arbitrary number subtrees 
leaf trees indexes external input units 
conversion tree networks 
tree representation allows fine tuning network structure 
integer weights network represented tree means multiple binary weights 
evolving optimal neural networks genetic breeding neural networks breeder genetic programming bgp evolution optimal neural networks concepts breeder genetic algorithm bga muhlenbein 
usual genetic algorithms model natural evolution bga models rational selection performed human breeders 
bga considered recombination evolution strategies es genetic algorithms ga 
bga uses truncation selection performed breeders 
selection scheme similar strategy es 
search process bga mainly driven recombination making bga genetic algorithm 
approach differs bga variable size chromosomes characteristic genetic programming gp 
call method breeder genetic programming bgp 
bgp differs usual gp 
gp uses proportional selection combined crossover main operator bgp uses truncation selection combined crossover plus local hillclimbing 
shown ranking selection easier balance accuracy parsimony solutions 
bgp evolutionary learning algorithm summarized 
algorithm maintains population consisting individuals variable size 
individual represents neural network 
networks initial population generated random number layers 
receptive field neural unit width chosen randomly 
st population created steps selection hillclimbing mating 
selection step fit individuals accepted mating pool 
parameter determines selection intensity value interval 
fitness function derived section balances error fitting ability parsimony networks 
selection individual undergoes hillclimbing search weights network adapted mutation 
results revised mate set 
mating phase repeatedly selects random parent individuals mate generate offspring new population applying crossover operators population size amounts notice size individuals population may different ja ja mg evolving optimal neural networks 
generate initial population networks random 
set current generation 
evaluate fitness values networks training set examples 

termination condition satisfied evolution 
continue step 
select upper networks population mating pool 

network undergoes local hillclimbing resulting revised mating pool 

create st population size applying genetic operators randomly chosen parent networks 

replace worst fit network best 

set return step 
summary bgp algorithm size individual subsequent population may different ja ja mg 
new population generated repeatedly acceptable solution variance fitness falls specified limit value min procedure terminates gamma min average fitness individuals 
algorithm stops specified number generations max carried 
genetic operators weights network trained applying hillclimbing search individuals accepted truncation selection 
evolving optimal neural networks parent offspring offspring parent crossover operation 
individual parent second parent mate crossing produce new individuals offspring offspring 
example network second grew 
guided appropriate selection mechanism network architecture adapted way specific application 
network ascent hillclimbing procedure finds better chromosome new repeatedly applying mutation operator weight configuration having better fitness sweep individual 
sequence mutation defined depth search order 
mutation operation performed replacing value node tree finding class replacing member set class value node mutated arbitrary values 
example weight value drawn set gamma 
biases mutated way weights 
index input units mutated input index 
mutation crossover operator adapts size shape network architecture 
crossover operation starts choosing random parent individuals mating pool 
actual crossover individuals done representations nodes tree numbered depth search evolving optimal neural networks order crossover sites chosen random conditions size size length individual size defined total number units weights 
crossover points subtrees parent individuals exchanged form offspring 
label nodes belong class type type nodes 
number arguments operator plays role syntactically correct subtree node completely replaced syntactically correct subtree 
fitness function occam razor occam razor states unnecessarily complex models preferred simpler ones :10.1.1.31.4284
section gives quantitative occam razor constructing minimal complexity neural networks genetic algorithms 
defining minimality important network able approximate training set specified performance level 
small network preferred large network achieve comparable performance 
algorithm reduce approximation error preferring smaller networks powerful solve task 
term fitness function individual network error function 
error function commonly data set ng examples sum squared errors desired actual outputs djw jx jx ij gamma ij denotes jth component ith desired output vector denotes jth actual output network architecture set weights ith training input vector evolving optimal neural networks complexity neural network architecture dependent task learned defined various ways depending application 
general number free parameters adjustable weights network minimal important factors determining speed accuracy learning 
additionally large weights general penalized hope achieving smoother simpler mapping 
technique called regularization 
define complexity network ja number free parameters 
notice arbitrarily large fit architectures 
case binary weights reduces number synaptic connections 
complexity measure extended additional cost terms number layers application requires fast execution trained network 
combined fitness function try minimize defined djw ffc ja fie djw ff fi constants trade error fitting complexity reduction 
fitness function elegant probabilistic interpretation learning process bayesian framework minimizing identical finding probable network architecture weights see define 
training data set function fl fl model function fl assignment possible pair number yjx representing hypothetical probability network specified architecture weights viewed model fa wg predicting outputs function input accordance probability distribution yjx exp gammafie yjx fi evolving optimal neural networks fi positive constant determines sensitivity probability error value fi exp gammafie yjx dy normalizing constant 
assumption gaussian error model true output expected include additive gaussian noise standard deviation oe yjx oe exp gamma yjx oe fi oe fi oe 
prior probability assigned alternative network model written form ja exp ja ff ff exp ja measure characteristic network complexity 
posterior probability network model jd exp ja gamma fie djw ff fi ff fi exp ja gamma fie djw gammai log prior probability model gammalog ja gammai djm log probability djm gamma log yjx evolving optimal neural networks probability true occurs exp gammai djm known results posterior probability model maximizes best fit 
real applications computed exactly involved probabilities known 
easily seen minimization fitness function approximates maximization assumption 
simulation results convergence generalization properties bgp method studied classes problems different complexity majority parity 
majority function inputs odd returns half input units returns 
parity function outputs number input pattern size odd outputs 
tasks chosen test neural net learning algorithms results compared standard solutions 
important observe genetic search performed variable dimensional space minimal usually larger input size depending task 
experiments fitness function djw djw ja normalized version equation djw djw delta number output units size training set 
notice error term satisfies djw 
revised measure network complexity defined ja ja max evolving optimal neural networks denote number layers units respectively 
max normalization factor complexity term satisfy ja 
experiments set cmax assuming problems solved ja 
term penalizes deep architecture requires large execution time training 
term penalizes large number units realization expensive weights 
normalization functions hinder probabilistic interpretation network learning ranking selection strategy proportionate selection survival ranking importance 
notice eqn 
complexity term ja divided number training examples error term play major role determining total fitness value network 
ensures small network preferred large network achieve comparable performance 
performed kinds experiments separately 
interested bgp method able find minimal solutions method scales problems increasing complexity 
experiments entire set examples evaluate fitness individual networks 
examples 
second series experiments tested performance bgp noisy data 
generalization performance learning speed different strategies compared study effect occam razor 
results experiments summarized table 
shows complexity discovered minimal solutions required time generations 
number weights table terms number connections thresholds binary values 
experiments top population selected mating 
fit individual retained new generation truncation selection strategy 
solutions network counterpart minimal comparison known standard solutions 
illustrated depicts solution input parity problem method 
comparison minimal solution problem depicted 
fitness value minimal solution min layers units delta solution 
note standard minimal solution shown illustration evolving optimal neural networks popsize layers units weights generations majority parity table network complexities discovered minimal solutions number layers units weights 
shown number generations obtain solution 
selection strategy top truncation 
solutions input parity problem 
compared known minimal solution left typical solution genetic method right contains unit additional connection weights terms binary valued connections discovered solution connections minimal solution 
evolving optimal neural networks purposes 
general learning methods known find solution architecture plus weight values 
existing search methods including iterated hillclimbing methods simulated annealing backpropagation genetic algorithms search space fixed size search space variable size 
difference ability combined different parameters algorithm comparison learning speed difficult 
fitness function worked balancing ability solve problem parsimony solution 
typical evolution network complexity shown 
globally complexity network grows evolution locally growth pruning repeated fit errors hand minimize complexity network hand 
corresponding evolution fitness values best individuals generation depicted 
interesting notice global behavior optimization method comparable group method data handling additional terms incrementally added existing polynomial approximator achieve minimal description length model complex system 
performance bgp method noisy data tested majority problem inputs 
previous experiments possible examples noise insertion run training set examples noise 
means average examples false output value 
population size upper best individuals selected mate 
shows typical evolution fitness value best individuals th generation 
comparison depict generalization performance complete test set consisting noise free examples 
notice test set selection training error generalization error correspond 
performance bgp method fitness function compared method uses just error term fitness measure djw djw 
noisy data majority problem 
method runs executed th generation observe training generalization performance solutions 
table shows average network size th generation 
corresponding performance learning time shown table 
learning time measured millions evaluations evolving optimal neural networks generation network complexity num weights num layers num units evolution network complexity terms number weights layers units best individual generation 
growth pruning repeated find optimal complexity parsimonious large solve problem 
generation fitness combined fitness error complexity evolution network fitness decomposed normalized error extended complexity spite fixed occam factor relative importance complexity term increases evolution proceeds 
evolving optimal neural networks generation misclassification rate learning error generalization error evolution network performance noisy data input majority function 
shown generalization performance complete test set noise free examples 
method layers units weights sigma sigma sigma sigma sigma sigma table network complexity occam razor method learning generalization learning time sigma sigma sigma sigma sigma sigma table comparison performance occam razor evolving optimal neural networks arithmetic operations associated calculating activation values neural units 
results show occam razor leads decreased performance training set eventually results improved generalization performance 
supposed effect occam razor avoiding overfitting noisy data 
advantage occam razor accelerated convergence 
experiments proposed fitness function decreased network size order magnitude speed factor learning approximately 
general method evolved architecture cases optimal solution terms parameters chosen balancing error fitting ability complexity solution 
classes large problems convergence slow 
simple optimization method exist performs better optimization method reasonable large class binary functions size effective sophisticated optimization method tuned application 
order assess complexity optimization problem speed genetic search investigation fitness landscapes necessary 
analysis fitness landscapes fitness landscapes analyzed boolean networks kauffman random traveling salesman tsp problems kirkpatrick euclidean tsp problems muhlenbein 
general characterization fitness landscape difficult 
number local optima distribution basins attraction important parameters describe fitness landscape 
evaluation search strategies specific questions answered ffl distribution local optima error term fitness function 
ffl distribution local optima change search space enlarged 
questions steps general problem evolving optimal neural networks ffl fitness function fitness landscape simpler complex compared error fitness function fixed minimal network architecture 
questions studied context problems xor function inputs 
problem analyzed search spaces different dimension 
feedforward network architecture free parameters binary weights plus binary thresholds 
search space architecture having free parameters binary weights plus binary thresholds 
describing landscapes focus statistical characteristics spaces large list details 
analysis fitness function consisted error term coefficient ff set zero fi 
fitness distributions shown 
notice xor networks binary inputs resulting input output pairs 
specific network fitness values case examples classified correctly example classified incorrectly 
analysis shows xor network isolated global optima net fifteen optima 
growth dimension increases proportion optima xor reduced 
shows fitness uniformly distributed xor suggesting search step network space get information step xor space 
see local optima vary computed probability individual finding better worse fit neighbor single mutation respectively 
better fit neighbor means smaller attempt minimize fitness function 
shows instance xor probability finding better neighbor fitness individual 
corresponding probability 
important result concluded fitness value figures 
xor minimal network architecture global minima isolated neighbors global optimum 
enlarged search space chance global optimum reached bit mutation 
behavior observed problem 
analysis suggests increase dimensionality evolving optimal neural networks fitness number misclassified examples probability xor fitness number misclassified examples probability fitness distribution fitness number misclassified examples probability fn fn fi fn fi fitness number misclassified examples probability fn fn fi fn fi fitness distribution neighbors fitness value xor fitness number misclassified examples probability fn fn fi fn fi fitness number misclassified examples probability fn fn fi fn fi fitness distribution neighbors fitness value evolving optimal neural networks search space leads change fitness distributions landscapes turn easier train weights 
computed probability configuration finding better fit neighbor steepest descent hillclimbing looking neighbors hamming distance 
surprisingly kind landscape xor chance finding better configuration 
probability 
means steepest descent hillclimbing effective xor 
explains part experiments showed scaling property majority function kind comparison parity problem smallest size xor 
discussion evolutionary method called breeder genetic programming bgp learning network architecture weights time 
method uses trees represent feedforward network size topology dynamically adapted genetic operators 
new fitness function occam razor proposed proved class problems studied 
simulation results indicate resources method finds minimal complexity networks 
experiments noisy data show occam razor improves generalization performance accelerates convergence genetic programming 
extensions refinements expected areas 
information fitness landscape speed convergence 
shown fitness landscapes characterized large plateaus 
basin attraction global optimum fairly small 
seen fitness landscapes changed modifying architectures 
expected fitness landscapes generally large plateaus network complexity approaches minimum difficult hillclimber reach minimum 
possible method accelerating convergence speed start larger networks supposed minimal network pruned occam factor 
supported results landscape analysis increase dimensionality search space leads larger chance finding better solutions near global optima 
evolving optimal neural networks concerns study factors instance effect training set convergence speed generalization performance algorithm 
genetic programming involves time consuming process evaluating training examples 
fitness evaluation time saved enormously efficient method selecting examples critical specific tasks 
integration active data selection genetic programming improve efficiency scaling property method described 
simple ascent hillclimbing adjustment discrete weights traditional search methods purpose 
examples include iterated hillclimbing procedures developed symbolic artificial intelligence 
discrete valued weights may extended general real valued weights 
extension necessary modify replace discrete hillclimbing search continuous parameter optimization method may genetic algorithms conventional gradient search methods 
notice adaptation change top level structure breeder genetic programming method described 
opposed conventional learning algorithms neural networks genetic programming method relatively assumptions network types 
method breed network architectures networks radial basis functions sigma pi units mixture threshold sigmoid units 
potential evolving neural architectures customized specific applications interesting properties genetic algorithms 
hand neural net optimization provides interesting problem worthy theoretical study genetic algorithm point view 
example problem discussed handle variable length chromosomes fitness landscape modified evolution 
kind optimization problem contrasted usual applications genetic algorithms search space fixed 
ultimate usefulness bgp method tested implementing systems solve real world problems pattern recognition time series prediction 
may need extensions current implementation 
believe general framework fitness function provided value problem balancing accuracy complexity solution evolving optimal neural networks fundamental neural networks genetic programming 
research supported part real world computing program project statistical inference foundation genetic algorithms 
authors jurgen frank dirk schlierkamp voosen members learning systems research group gmd institute applied information technology valuable discussions suggestions 
wish anonymous reviewers comments helped improve clarity 
abu mostafa vapnik chervonenkis dimension information versus complexity learning neural computation 
back 
schwefel overview evolutionary algorithms parameter optimization evolutionary computation 
kauffman levin general theory adaptive walks rugged landscapes journal theoretical biology 
gent walsh understanding hill climbing procedures sat proceedings th national conference artificial intelligence aaai mit press 
goldberg genetic algorithms search optimization machine learning addison wesley 
gruau genetic synthesis boolean neural networks cell rewriting developmental process tech 
rep laboratoire de informatique du parall 
evolving optimal neural networks harp samad guha genetic synthesis neural networks proceedings third international conference genetic algorithms icga morgan kaufmann 
holland adaptation natural artificial systems university michigan press ann arbor 
polynomial theory complex systems ieee transactions systems man cybernetics smc 
kirkpatrick vecchi optimization simulated annealing science 
kitano designing neural networks genetic algorithms graph generation system complex systems 
koza genetic programming programming computers means natural selection mit press 
mackay bayesian methods adaptive models ph thesis caltech pasadena ca 

mcculloch pitts logical calculus ideas nervous activity bull 
math 
biophysics 
miller todd hegde designing neural networks genetic algorithms proceedings third international conference genetic algorithms icga morgan kaufmann 
minsky papert perceptrons computational geometry mit press 
montana davis training feedforward neural networks genetic algorithms proceedings international joint conference artificial intelligence 
morris breakout method escaping local minima proceedings th national conference artificial intelligence aaai mit press 
evolving optimal neural networks muhlenbein darwin continental cycle simulation prisoner dilemma complex systems 
muhlenbein evolution time space parallel genetic algorithm foundations genetic algorithms edited rawlins morgan kaufmann 
muhlenbein parallel genetic algorithms combinatorial optimization computer science operations research edited zenios pergamon oxford 
muhlenbein evolutionary algorithms theory applications local search combinatorial optimization edited aarts lenstra wiley 
muhlenbein gorges schleuter kramer new solutions mapping problem parallel systems evolution approach parallel computing 
muhlenbein kindermann dynamics evolution learning genetic neural networks connectionism perspective edited pfeifer elsevier 
muhlenbein schlierkamp voosen predictive models breeder genetic algorithm continuous parameter optimization evolutionary computation 
poggio girosi networks approximation learning proceedings ieee 
rechenberg evolutionsstrategie optimierung technischer systeme nach prinzipien der biologischen evolution stuttgart 
rosenblatt principles neurodynamics spartan books washington 
evolving optimal neural networks rumelhart hinton williams learning internal representations error propagation parallel distributed processing vol 
edited rumelhart mcclelland mit press 

schwefel numerical optimization computer models chichester wiley 
selman kautz empirical study greedy local search satisfiability testing proceedings th national conference artificial intelligence aaai mit press 
neural network constructive algorithms trading generalization learning efficiency circuits systems signal processing 
sorkin quantitative occam razor international journal theoretical physics 

lee self organizing network optimum supervised learning ieee transactions neural networks 
tishby levin solla consistent inference probabilities layered networks predictions generalization proceedings international joint conference neural networks ijcnn vol 
ii ieee 
whitley starkweather genetic algorithms neural networks optimizing connections connectivity parallel computing 
zhang learning genetic neural evolution german isbn infix verlag sankt augustin 
available informatik berichte institut fur informatik universitat bonn july 
zhang accelerated learning active example selection appear international journal neural systems 
evolving optimal neural networks zhang muhlenbein genetic programming minimal neural nets occam razor proceedings fifth international conference genetic algorithms icga edited forrest morgan kaufmann 
zhang focused incremental learning improved generalization reduced training sets artificial neural networks proceedings international conference artificial neural networks icann vol 
edited kohonen 
elsevier 
zhang neural networks teach genetic discovery novel examples proceedings international joint conference neural networks ijcnn vol 
ieee 
