influence caches performance sorting anthony lamarca xerox parc coyote hill road palo alto ca richard ladner department computer science engineering university washington seattle wa lamarca parc xerox com ladner cs washington edu investigate effect caches performance sorting algorithms experimentally analytically 
address performance problems high cache penalties introduce restructure mergesort quicksort heapsort order improve cache locality 
algorithms improvement cache performance leads reduction total execution time 
investigate performance radix sort 
despite extremely low instruction count incurred linear time sorting algorithm relatively poor cache performance results worse performance efficient comparison sorting algorithms 
algorithm provide analysis closely predicts number cache misses incurred algorithm 
caches main memory continued grow slower relative processor cycle times 
time service cache memory grown cycles vax alphaserver 
cache penalties grown point performance achieved cache performance 
consequence change computer architectures algorithms designed minimize instruction count may achieve performance algorithms take account instruction count cache performance 
common tasks computers perform sorting set unordered keys 
sorting fundamental task hundreds sorting algorithms developed 
explore potential performance gains cache conscious design offers understanding improving performance popular sorting algorithms mergesort quicksort heapsort preliminary version appears proceedings eighth annual acm siam symposium discrete algorithms 
knuth traces mergesort back card sorting machines 
radix sort mergesort quicksort heapsort comparison sorting algorithms radix sort 
sorting algorithms choose implementation variant potential performance heavily optimize variant traditional techniques minimize number instructions executed 
heavily optimized algorithms form baseline comparison 
comparison sort baseline algorithms develop apply memory optimizations order improve cache performance hopefully performance 
radix sort optimize cache performance varying radix 
process develop simple analytic techniques enable predict memory performance algorithms terms cache misses 
cache misses analyzed precisely due number factors variations process scheduling operating system virtual physical page mapping policy 
addition memory behavior algorithm may complex analyze completely 
reasons analyses approximate validated empirically 
comparison purposes focus sorting array keys bit integers chosen uniformly random 
main study uses trace driven simulations actual executions measure impact memory optimizations performance 
concentrate performance measures instruction count cache misses performance time machines modern memory systems 
results summarized follows 
comparison sorting algorithms memory optimizations improve cache performance 
improvements performance heapsort mergesort significant improvement quicksort modest 
interestingly memory optimizations heapsort reduce instruction count 
radix sort radix minimizes cache misses minimizes instruction count 

large arrays radix sort lowest instruction count relatively poor cache performance performance worse memory optimized versions mergesort quicksort 

study done machine demonstrate robustness results showing comparable speedups due improved cache performance achieved machines 

effective analytic approaches predicting number cache misses sorting algorithms incur 
cases analysis difficult highly predictive actual performance 
main general lesson learned study cache penalties large growing larger new generation processor selecting fastest algorithm solve problem entails understanding cache performance 
improving algorithm performance may require increasing number instructions executed time reducing number cache misses 
consequently cache conscious design algorithms required achieve best performance 
knuth traces radix sorting method sorting machine assist united states census 
caches order speed memory accesses small high speed memories called caches placed processor main memory 
accessing cache typically faster accessing main memory 
unfortunately caches smaller main memory hold subset contents 
memory accesses consult cache see contains desired data 
data cache main memory need consulted access considered cache hit 
data cache considered data loaded main memory 
block containing accessed data loaded cache hope data block accessed 
hit ratio measure cache performance total number hits divided total number accesses 
major design parameters caches ffl capacity total number bytes cache hold 
ffl block size number bytes loaded written memory time 
ffl associativity indicates number different locations cache particular block loaded 
way set associative cache particular block loaded different cache locations 
direct mapped caches associativity load particular block single location 
fully associative caches extreme load blocks cache 
ffl replacement policy indicates policy block remove cache new block loaded 
direct mapped cache replacement policy simply remove block currently residing cache 
modern machines cache placed processor main memory 
hierarchies caches configured smallest fastest cache processor largest slowest cache main memory 
largest penalty typically incurred cache closest main memory cache usually direct mapped 
consequently design analysis techniques focus improving performance direct mapped caches 
assume cache parameters block size capacity known programmer 
high cache hit ratios depend program stream memory exhibiting locality 
program exhibits temporal locality chance accessed data item accessed near 
program exhibits spatial locality chance subsequently accessed data items located near memory 
programs tend exhibit kinds locality typical hit ratios greater 
hit ratio cutting number cache misses half effect raising hit ratio 
may big improvement penalties order cycles normal programs exhibit speed ups approaching execution time 
accordingly design techniques attempt improve temporal spatial locality sorting algorithms 
cache misses categorized compulsory capacity conflict misses 
compulsory misses occur block accessed brought cache 
capacity misses caused fact blocks accessed fit time cache 
conflict misses occur blocks map location cache accessed 
address techniques reduce number capacity conflict misses sorting algorithms 
design evaluation methodology cache locality thing 
spatial temporal locality improved cost done 
develop techniques improving locality results increase total number executed instructions 
represents significant departure traditional design optimization methodology 
take approach order show large impact cache performance performance 
interestingly design techniques particularly new 
optimizing compilers algorithms external storage devices parallel algorithms 
similar techniques successfully development cache efficient algorithm 
mentioned earlier focus measures performance instruction count cache misses performance terms execution time 
dynamic instruction counts cache simulation results measured atom 
atom toolkit developed dec instrumenting program executables alpha workstations 
dynamic instruction counts obtained inserting increment counter instruction executed algorithm 
cache performance determined inserting calls load store maintain state simulated cache keep track hit statistics 
cases configure simulated cache block size capacity second level cache architecture measure execution time 
execution times measured dec alphastation execution times study represent median trials 
machine study byte cache blocks direct mapped second level cache bytes 
study set sorted varied size keys bytes 
provide analytic methods predict cache performance terms cache misses 
cases analysis quite simple 
example traditional mergesort fairly oblivious pattern access memory making analysis quite straightforward 
memory access patterns algorithms heapsort oblivious requiring sophisticated techniques approximations accomplish analysis 
noted due unpredictable factors operating system scheduling analyses extremely accurate approximations 
analyses consistently indicate number keys sorted indicate number blocks cache indicate number keys fit cache block 
evaluate analyses dec alphastation sorting byte keys 
mergesort sorted lists merged single sorted list traversing lists time sorted order repeatedly adding smaller key single sorted list 
treating set unordered keys set sorted lists length keys repeatedly merged single sorted set keys remains 
algorithms sort manner known mergesort algorithms recursive iterative variants 
base algorithm base algorithm chose iterative mergesort easy implement amenable traditional optimization techniques 
standard iterative mergesort dlog ne passes array th pass merges sorted subarrays length gamma sorted subarrays length optimizations applied base mergesort include alternating merging process array avoid unnecessary copying making sure subarrays merged opposite order avoid unnecessary checking conditions sorting subarrays size fast line sorting method loop unrolling 
number merge passes dlog dlog odd additional copy pass needed move sorted array input array 
base mergesort algorithm low instruction count executing fewest instruction comparison sorting algorithms 
memory optimizations base mergesort executes instructions potential terrible cache performance 
base mergesort uses data item pass pass large wrap cache keys ejected 
set keys size bc smaller entire sort performed cache compulsory misses incurred 
set size larger bc temporal reuse drops sharply set size larger cache temporal reuse occurs 
improve inefficiency apply memory optimizations base mergesort 
applying optimizations yields tiled mergesort applying yields multi mergesort 
tiled mergesort employs idea called tiling optimizing compilers 
tiled mergesort phases 
improve temporal locality phase subarrays length bc sorted mergesort 
second phase returns base mergesort complete sorting entire array 
order avoid final copy dlog odd subarrays size sorted line size 
tiling base mergesort drastically reduces misses incurs added loop overhead increases instruction counts little 
tiling improves cache performance phase second phase suffers problem base mergesort 
pass source array second phase needs fault blocks reuse achieved passes set size larger cache 
fix inefficiency second phase employ multi way merge similar external sorting knuth devotes section book techniques multi merging sec 

multi mergesort replace final dlog bc merge passes tiled mergesort single pass merges pieces 
single pass memory optimized heap hold heads lists multi merged 
multi merge introduces complications algorithm significantly increases dynamic instruction count 
resulting algorithm excellent cache performance incurring roughly constant number cache misses key executions 
performance mentioned earlier performance mergesort algorithms measured sorting sets uniformly distributed bit integers 
shows number instructions executed key cache missed incurred execution times mergesort variants 
instruction count graph shows expected base mergesort tiled mergesort execute number instructions 
instruction count curve base mergesort due final copy may need take place depending final merge wrote source array auxiliary array 
set size smaller cache multi mergesort executes number instructions tiled mergesort 
size performed graph shows increase causes instruction count 
keys executes instructions 
striking feature cache performance graph sudden increase cache misses base mergesort set size grows larger cache 
graph shows large impact tiling base mergesort cache misses keys tiled mergesort incurs fewer cache misses base mergesort 
graph shows clear success cache perspective incurring slightly key regardless input size 
graph execution times shows size second level cache algorithms perform 
size base mergesort performs worst due large number cache misses incurs 
tiled mergesort executes faster base mergesort showing significant impact cache misses phase execution time 
multi way merge performed multi mergesort performs worse tiled mergesort due increase instruction count 
due lower cache misses multi mergesort scales better performs tiled mergesort largest set sizes 
analysis fairly easy approximate number cache misses incurred mergesort variants memory patterns algorithms fairly oblivious 
base algorithm 
bc number misses key simply compulsory misses 
algorithms employ base algorithm bc number misses key range 
bc number misses key iterative mergesort algorithm approximately dlog dlog mod term expression comes capacity misses incurred dlog merge passes 
pass key moved source array destination array 
th key visited source array results cache th key written destination array results cache cache misses key pass 
second term compulsory misses key incurred initial pass sorting groups 
final term number misses key caused final copy 
bc number misses key tiled mergesort approximately dlog bc term expression number misses key final dlog bc merge passes 
second term number misses key sorting bc size pieces 
number passes forced additional cache misses caused copy 
shows analysis base mergesort tiled mergesort predict actual performance 
bc number misses key multi mergesort approximately closely matches approximately key shown cache misses graph 
phase multi mergesort tiled mergesort incurs misses key 
algorithm sure number passes phase odd second phase multi merges auxiliary array input array 
second phase multi merges pieces stored auxiliary array back input array causing misses key 
multi merge employs heap array containing bce keys 
practical purposes safe assume smaller bc 
bc accesses input array removes member heap array cache 
additional misses bc accesses input array 
give additional misses key incurred multi merge phase 
number negligible practical values quicksort quicksort place divide conquer sorting algorithm considered fastest comparison sorting algorithm set keys fit memory 
quicksort key set chosen pivot keys set partitioned pivot 
usually accomplished walking array keys outside swapping keys left greater pivot keys right pivot 
pass set keys partitioned pivot pivot guaranteed final position 
quicksort algorithm recurses region left pivot region right 
simple recursive quicksort simple elegant algorithm expressed lines code 
base algorithm excellent study fast implementations quicksort conducted sedgewick optimized quicksort develops base algorithm 
optimizations sedgewick suggests sorts small subsets faster sorting method 
suggests sorting natural course quicksort recursion small unsorted subarrays left unsorted time sorted insertion sort single final pass entire array 
employ optimizations recommended sedgewick base quicksort 
memory optimizations practice quicksort generally exhibits excellent cache performance 
algorithm sequential passes source array keys block spatial locality excellent 
quicksort divide conquer structure gives excellent temporal locality 
subset sorted small fit cache quicksort incur cache block subset fully sorted 
despite improvements develop memory optimized versions quicksort memory tuned quicksort multi quicksort 
memory tuned quicksort simply removes sedgewick elegant insertion sort sorts small subarrays encountered unoptimized insertion sort 
small subarray encountered just part partitioning ideal time sort keys cache 
saving small subarrays sense instruction count perspective exactly wrong thing cache performance perspective 
multi quicksort employs second memory optimization similar spirit 
quicksort incurs cache block set cache sized smaller larger sets incur substantial number misses 
fix inefficiency single pass divide full set number subsets cache sized smaller 
multi partitioning parallel sorting algorithms divide set subsets multiple processors order quickly balance load 
choose number pivots number subsets larger cache small sufficiently high probability 
feller shows points placed randomly range length chance resulting subrange size greater exactly gamma vol 
pg 

total number keys number keys cache block capacity cache blocks 
multi quicksort partition input array bc pieces requiring bc gamma pivots 
feller formula indicates multi partition chance subset larger bc gamma bc bc gamma limit grows large percentage subsets larger cache gamma 
number complications design multi partition phase multi quicksort algorithm done efficiently place executes instructions base quicksort 
resulting multi quicksort efficient cache perspective 
performance shows performance quicksort algorithms sorting bit uniformly distributed integers 
instruction count graph shows base quicksort executes fewest instructions memory tuned quicksort executing constant number additional instructions key 
difference due inefficiency sorting small subsets individually suggested sedgewick 
large set sizes multi quicksort performs result significant increase number instructions executed 
cache performance graph shows quicksort algorithms incur cache misses 
base quicksort incurs fewer misses key keys lower algorithms point exception multi mergesort 
cache curve memory tuned quicksort shows removing instruction count optimization base quicksort improves cache performance approximately cache misses key 
cache graph shows multi way partition produces flat cache curve curve multi mergesort 
maximum number misses incurred key multi quicksort slightly larger key validating conjecture uncommon produce subsets larger size cache 
graph execution times shows execution times quicksort algorithms 
algorithms perform similarly dec alphastation 
graph shows sorting small subsets early benefit reduction cache misses outweighs increase instruction cost 
initially hurts performance multi quicksort due increase instruction cost low number cache misses competitive set size increased 
graph suggests memory available larger sets sorted multi quicksort outperform base quicksort memory tuned quicksort 
analysis analyzing memory tuned quicksort slightly behavior base quicksort 
bc memory tuned quicksort incurs misses key compulsory misses 
base quicksort extra pass perform insertion sort number cache misses key incurred base quicksort plus number cache misses key incurred memory tuned quicksort 
cache graph clearly show additional misses key base quicksort simulation 
bc expected number misses key memory tuned quicksort approximately ln bc analyze algorithm parts 
part assume partitioning array size costs misses bc misses 
second part correct estimating cache misses 
expected number misses incurred quicksort assumption part analysis 
recurrence gamma gamma gamma bc bc 
standard techniques recurrence solves ln bc bc 
correction misses incurred subproblem reaches size bc 
analysis count zero misses fact subproblem may part cache 
account add misses approximately keys subproblems reach size bc 
partitioning quicksort cache misses necessarily subsequent partitionings 
partitioning array left subproblem cache 
hits counting misses analysis 
note right subproblem hits time algorithm reaches right subproblem data removed cache 
analyze expected number subproblems size bc 
recurrence gamma gamma gamma bc bc 
recurrence solves exactly bc gamma bc 
subproblems left subproblem 
average bc keys left subproblem cache 
accesses keys hits 
right pointer partitioning enjoys benefit access keys left pointer eventually accesses blocks map blocks cache replacing 
loss generality assume right pointer partitioning begins right cache 
scenarios 
scenario right pointer progresses left left pointer starts blocks left middle cache progresses right 
average hits 
second scenario left pointer starts blocks right middle cache progresses right 
average gammai hits 
assume left pointer start block equal probability estimate expected number hits expected number hits accounted computation approximately adding pieces bc expected number misses key approximately gamma approximated closely expression 
shows approximation cache misses predicts actual performance memory tuned quicksort 
addition shows approximation misses key base quicksort predicted 
multi quicksort approximate analysis quite simple 
bc multi quicksort number misses key simply compulsory misses 
bc algorithm partitions input bc pieces vast majority smaller cache 
purposes analysis assume smaller cache 
multi partition phase move partitioned keys linked lists partition 
node linked list room keys 
way minimize storage waste time minimizing cache misses 
approximate number misses key phase block input array block linked list 
second phase partition returned input array sorted place 
costs approximately misses key 
cache overhead maintaining additional pivots negligible practical values total approximately misses key closely matches misses reported 
heapsort heaps variety purposes proposed williams part heapsort algorithm 
heapsort algorithm sorts building heap containing keys removing time sorted order 
array implementation heap results straightforward place sorting algorithm 
set keys williams algorithm takes log steps build heap log steps remove keys sorted order 
floyd proposed improved technique building heap better average case performance worst case steps 
williams base algorithm floyd improvement prevalent heapsort variant 
base algorithm base heapsort algorithm follow recommendations algorithm textbooks array implementation binary heap constructed floyd method 
addition employ standard optimization sentinel heap eliminate comparison level reduces instruction count 
literature contains number optimizations reduce number comparisons performed adds removes practice increase total number instructions executed improve performance 
reason include base heapsort 
memory optimizations base heapsort algorithm apply memory optimizations order improve performance 
previous results show william repeated adds algorithm building binary heap incurs fewer cache misses floyd method 
addition shown optimizations reduce number cache misses incurred remove min operation 
optimization replace traditional binary heap heap non leaf node children 
fanout chosen exactly keys fit cache block 
relatively small say added advantage number instructions executed add remove min reduced 
second optimization align heap array memory children lie cache block 
optimization reduces lebeck wood refer alignment misses 
memory optimized heapsort dynamically chooses williams repeated adds method floyd method building heap 
heap larger cache williams method offer reduction cache misses chosen floyd method 
call base algorithm memory optimizations applied memory tuned heapsort 
performance dec alphastation byte block size sorting byte keys keys fit block 
consequence choose memory tuned heapsort 
shows performance base heapsort memory tuned heapsort 
instruction count curves show memory tuned heapsort executes fewer instructions base heapsort 
graph cache performance shows set sorted fit cache minimum bytes bytes compulsory misses incurred key algorithms 
larger sets number cache misses incurred memory tuned heapsort half misses incurred base heapsort 
execution time graph shows memory tuned heapsort outperforms base heapsort set sizes 
memory tuned heapsort initially outperforms base heapsort due lower instruction counts 
set size reaches cache size gap widens due differences number cache misses incurred 
keys memory tuned heapsort sorts faster base heapsort 
analysis bc heapsort place sorting algorithm takes misses key 
bc adopt analysis technique collective analysis previous 
collective analysis analytical framework predicting cache performance algorithms algorithm memory access behavior approximated independent stochastic processes 
part analysis cache divided regions assumed accessed uniformly 
stating way process accesses region simple formula predict cache performance 
collective analysis number simplifications limit class algorithms accurately analyzed serves algorithms behavior understood exact pattern varies 
part collective analysis analyzed cache performance heaps hold model 
hold model number elements heap held constant elements alternately removed added heap 
adding additional removes adds hold model approximation discrete event simulation static number events 
number elements heap remain constant heapsort collective analysis heaps approximate cache performance 
heapsort algorithm goes phases build heap phase remove phase 
build heap phase recall william method building heap simply puts key bottom heap proper place 
pessimistically assume adds percolate root leaf root path cache 
case probability leaf cache chance parent leaf cache db chance grandparent leaf cache 
gives simple approximation number misses incurred key build phase gamma 
estimate expected number misses build heap phase dn gamma 
interesting note small constant factor compulsory misses incurred build heap algorithm 
divide remove phase heapsort bc subphase removing bc keys 
bc gamma model removal keys bci bc bc steps array size gamma bci hold model 
essence saying removal bc keys heap approximated bc repeated remove mins adds approximately size heap 
admittedly approximation convenience complete approximate analysis heap hold model 
combining remove phase estimate build heap predictions yields 
graph shows cache predictions heapsort traditional binary heap base heapsort heap heapsort 
predictions match simulation results surprisingly considering simplifying assumptions analysis 
radix sort radix sort important non comparison sorting algorithm today 
knuth traces radix sort suitable sorting main memory computer master thesis seward 
seward pointed radix sort keys accomplished key arrays count array size hold integers size radix 
seward method standard method radix sorting programs 
radix sort called linear time sort keys fixed length fixed radix constant number passes data sufficient accomplish sort independent number keys 
example sorting bit integers radix seward method db re iterations passes source array 
pass accumulates counts number keys radix 
counts determine offsets keys radix destination array 
second pass moves source array destination array offsets 
friend suggested improvement reduce number passes source array accumulating counts st iteration time moving keys th iteration 
requires second count array size improvement positive effect instruction count cache misses 
radix sort highly optimized version seward algorithm friend improvement 
final task pick radix minimizes instruction count 
done empirically universally best minimizes instruction count 
obvious memory optimization radix sort similar comparison sorts 
simple memory optimization choose radix minimizes cache misses 
happens implementation radix bits minimizes cache misses instruction count alphastation 
analysis subsection take closer look cache misses incurred radix sort 
performance study keys bit integers counts restricted bit integers 
bit radix count arrays th size megabyte cache 
shows resulting performance 
instruction count graph shows radix sort linear time behavior 
cache graph shows size input array reaches cache capacity number cache misses rapidly grows constant slightly misses key 
execution time graph clearly shows effect cache misses performance 
execution time curve looks instruction count curve input array exceeds cache size time cycles key increase cache curve 
analysis approximate cache analysis radix sort complicated previous analyses number reasons 
multitude parameters consider analysis number keys number bits key radix number keys block number counts block capacity cache blocks 
second cache effects consider 
focus feel significant cache effects capacity misses traversals source destination arrays conflict misses traversal source array accesses count arrays conflict misses traversal source array accesses destination array 
focus analyzing number cache misses fixed large number keys varying radix 
case radix analysis attempts predict flat part cache curve 
assume size count arrays cache capacity ac 
addition assume bc expected number cache misses key radix sort approximated sum terms count dest number misses key incurred traversing source destination arrays count expected number misses key incurred accesses count arrays dest expected number misses key destination array caused conflicts traversal source array 
approximate count dest mod count abc gamma gamma min bc abc gamma gamma min bc dest gamma gamma gamma bc gamma gamma gamma bc start derivation equation 
implement friend improvement initial pass input array counts accumulated count array 
db re iterations source array moved destination array offsets established count arrays previous iteration 
db re odd final sorted array copied back input array 
total db re db re mod passes source destination array 
ignoring conflicts source destination array number misses key times number passes 
quantity count accounts misses random accesses count arrays caused traversal source array 
total db re traversals source array conflict random accesses count array 
divides traversal conflict random accesses count array size traversals conflict random accesses count arrays size divide traversal conflicts random accesses count array size traversal conflicts random accesses count array size second traversal conflicts random access count array size count array size remaining traversals conflict random accesses count arrays size generally consider pass source array conflict count array size consider specific block count arrays 
bc steps algorithm traversal source array block cache 
block accessed count array bc steps algorithm incurred access 
probability block accessed bc steps gamma gamma min bc total expected number misses approximated quantity times number blocks array times number times visited traversal bc 
expected number misses key incurred traversal source array count array size approximately abc gamma gamma min bc divides effect passes conflict single count array size expression yields term equation second term negligible 
divide effect bb rc passes conflict count array size passes conflict count array size yields equation 
quantity dest accounts conflict cache traversal source array accesses destination array 
consider specific block destination array 
iteration algorithm block visited exactly times 
visit cache accounted 
remaining gamma visits cache hit traversal source array visits cache block visit destination array 
assume number pointers destination array approximation assume gamma remaining accesses block occur independently probability assumption probability traversal visit cache block visit destination array bc gamma gamma bc see suppose exactly steps traversal visit cache block probability accessed destination array traversal captures cache block approximately gamma probability bc traversal steps capturing cache block probability traversal capture cache block access block bc bc gamma gamma equal expression 
complete derivation equation cases consider depending divides 
divides traversals source array conflict traversal destination array 
pointers destination array 
expected number misses key number passes times bc gamma gamma bc times gamma totals term dest case second term dest 
divide bb rc traversals pointers traversal pointers 
analysis similar yields equation dest note accurate formula dest treat gamma accesses block independent derived 
accurate formula complex recurrence yield simple closed form solution tractable numerical solution 
independence assumption yield fairly accurate results 
compares approximation actual number cache misses incurred key function radix sort goes radix radix count arrays larger cache 
theoretical prediction simulation agree parameter set optimal radix 
discussion section compare performance fastest variant sorting algorithms examined study 
addition examine performance algorithms additional architectures order explore robustness memory optimizations 
performance comparison compare performance sorting algorithms shows instruction count cache misses cycles executed key fastest variants heapsort mergesort quicksort radix sort 
instruction count graph shows memory tuned heapsort executes instructions radix sort executes 
cache graph shows radix sort cache misses incurring twice misses memory tuned quicksort 
execution time graph strikingly shows effect cache performance performance 
largest data set memory tuned quicksort ran faster radix sort memory tuned quicksort performed times instructions 
graph shows small differences instruction count cache misses memory tuned mergesort memory tuned quicksort offset yield algorithms comparable execution times 
robustness order determine experimental results generalize dec alphastation ran programs platforms ibm power pc sparc dec alpha pentium base pc 
shows speedup memory tuned heapsort achieves base heapsort alphastation additional machines 
despite differences architecture platforms show similar speedups 
shows speedup tiled mergesort traditional mergesort machines 
heapsort case speedups mergesort differ substantially 
partly due differences page mapping policies different operating systems 
study assumed block contiguous pages virtual address space map block contiguous pages cache 
guaranteed true caches virtually indexed physically indexed 
unfortunately caches test machines physically indexed 
fortunately operating systems digital unix virtual physical page mapping polices attempt map pages blocks memory nearby virtual address space conflict cache 
heapsort algorithms tiled mergesort relies heavily assumption cache sized block pages conflict cache 
result speedup tiled mergesort relies heavily quality operating system page mapping decisions 
operating systems sparc alphas solaris digital unix cache conscious decisions page placement operating systems power pc pentium aix linux appear careful 
explored potential performance gains cache conscious design analysis offers classic sorting algorithms 
main effects caching extremely important need considered performance goal 
despite low instruction count radix sort outperformed mergesort quicksort due relatively poor locality 
despite fact multi mergesort executed instructions base mergesort sorts faster 
multi mergesort multi quicksort place stable 
algorithms offer 
incur cache misses renders performance far sensitive cache penalties 
result algorithms expected outperform relative cache penalties continue increase 
number memory optimizations applied algorithms order improve performance 
follows summary design principles applied ffl improving cache performance cost increased instruction count improve performance 
ffl knowing architectural constants cache size block size improve algorithm memory system performance generic algorithm 
ffl spatial locality improved adjusting algorithm structure fully utilize cache blocks 
ffl temporal locality improved padding adjusting data layout structures aligned cache blocks 
ffl capacity misses reduced processing large data sets cache sized pieces 
ffl conflict misses reduced processing data cache block sized pieces 
shows despite complexities caching cache performance algorithms reasonably approximated modest amount 
figures show approximate analysis gives information 
needs done improve analysis techniques accurate 
blelloch plaxton leiserson smith maggs zagha 
comparison sorting algorithms connection machine 
proceedings rd acm symposium parallel algorithms architecture pages july 
carlsson 
optimal algorithm deleting root heap 
information processing letters 
clark 
cache performance vax 
acm transactions computer systems 
de 
expected heights heaps 
bit 
feller 
probability theory applications 
wiley new york ny 
foley gist 
alphaserver series high server platform development 
digital technical journal 
robert floyd 

communications acm 
friend 
journal acm 
gonnet munro 
heaps heaps 
siam journal computing 
patterson 
computer architecture quantitative approach second edition 
morgan kaufman publishers san mateo ca 
hoare 
quicksort 
computer journal 

automatic programming pages 
li hui sevcik 
parallel sorting 
proceedings th acm symposium parallel algorithms architecture pages june 
johnson 
priority queues update finding minimum spanning trees 
information processing letters 
knuth 
art computer programming vol iii sorting searching 
addison wesley reading ma 
lamarca 
caches algorithms 
ph dissertation university washington may 
lamarca ladner 
influence caches performance heaps 
journal experimental algorithmics vol article 
lebeck wood 
cache profiling spec benchmarks case study 
computer oct 
nyberg barclay gray lomet 
risc machine sort 
acm sigmod international conference management data pages may 
sedgewick 
implementing quicksort programs 
communications acm october 
seward 
masters thesis digital computer laboratory report 
amitabh srivastava alan eustace 
atom system building customized program analysis tools 
proceedings acm symposium programming languages design implementation pages 
acm 
taylor davies 
tbl slice low cost high speed address translation mechanism 
proceedings th annual international symposium computer architecture pages 
williams 
heapsort 
communications acm 
wolfe 
iteration space tiling 
proceedings supercomputing pages 
set size keys base tiled multi set size keys base tiled multi set size keys base tiled multi performance mergesort sets keys 
top bottom graphs show instruction counts key cache misses key execution times key 
executions run dec alphastation simulated cache capacity megabytes byte block size 
set size keys base memory tuned multi set size keys base memory tuned multi set size keys base memory tuned multi performance quicksort sets keys 
top bottom graphs show instruction counts key cache misses key execution times key 
executions run dec alphastation simulated cache capacity megabytes byte block size 
set size keys base memory tuned set size keys base memory tuned set size keys base memory tuned performance heapsort sets keys 
top bottom graphs show instruction counts key cache misses key execution times key 
executions run dec alphastation simulated cache capacity megabytes byte block size 
set size keys base set size keys base set size keys base performance sets keys 
top bottom graphs show instruction counts key cache misses key execution times key 
executions run dec alphastation simulated cache capacity megabytes byte block size 
instructions key set size keys memory tuned heapsort tiled mergesort memory tuned quicksort radix sort cache misses key set size keys memory tuned heapsort tiled mergesort memory tuned quicksort radix sort time cycles key set size keys memory tuned heapsort tiled mergesort memory tuned quicksort radix sort instruction count cache misses execution time key best heapsort mergesort quicksort radix sort dec alphastation 
simulated cache capacity megabytes block size bytes 
cache misses key set size keys base mergesort measured base mergesort predicted tiled mergesort measured tiled mergesort predicted cache misses incurred mergesort measured versus predicted 
cache misses key set size keys base quicksort measured base quicksort predicted memory tuned quicksort measured memory tuned quicksort predicted cache misses incurred quicksort measured versus predicted 
cache misses key set size keys base heapsort measured base heapsort predicted memory tuned heapsort measured memory tuned heapsort predicted cache misses incurred heapsort measured versus predicted 
cache misses key radix bits measured predicted cache misses incurred radix sort measured versus predicted 
speedup set size keys sparc power pc pentium pc dec alphastation dec alpha speedup memory tuned heapsort base heapsort architectures 
speedup set size keys sparc power pc pentium pc dec alphastation dec alpha speedup tiled mergesort base mergesort architectures 
