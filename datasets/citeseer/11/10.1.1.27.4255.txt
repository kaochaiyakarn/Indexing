submitted neural networks oct pnl pnl sa optimal linear combinations neural networks hashem pacific northwest laboratory box ms wa usa 
internet hashem pnl gov www url www pnl gov people hashem html neural network nn modeling involves trying multiple networks different architectures training parameters order achieve acceptable model accuracy 
typically trained nns chosen best rest discarded 
hashem schmeiser proposed optimal linear combinations number trained neural networks single best network 
combining trained networks may help integrate knowledge acquired component networks improve model accuracy 
discuss extend idea optimal linear combinations olcs neural networks derive closedform expressions cases olcs 
algorithms selecting component networks combination improve generalization ability olcs 
experimental results demonstrate significant improvements model accuracy result olcs compared best network 
neural network nn modeling complexity design phase may significant obstacle limits nns potential users 
degrees freedom selecting network topology training algorithm 
parameters include number layers number hidden units type activation functions degree type connectivity 
training parameters include initial connection weights learning rates momentum term weight decay term 
choice topological training parameters significantly affect accuracy resultant model training time 
parameter selection result rules thumb combined trial error 
training process number trained networks produced typically chosen best optimality criterion rest discarded 
advocate mse optimal linear combinations mse olcs trained networks supported part prf research purdue university national science foundation ddm associated western universities northwest division er department energy environmental molecular sciences laboratory construction project pacific northwest laboratory 
pnl pnl sa 
acceptance article publisher recipient acknowledges government right retain non exclusive royalty free license copyright covering 
pacific northwest laboratory national laboratory operated department energy memorial institute contract de ac rlo 
september author address department engineering mathematics physics faculty engineering cairo university egypt 
just apparent best network 
introduce algorithms selecting component networks order enhance generalization ability robustness mse olcs 
hashem schmeiser proposed forming linear combination corresponding outputs trained neural networks nns 
combining trained networks may help integrate knowledge acquired component networks produces superior model accuracy compared single best trained network :10.1.1.27.4255
optimal linear combinations olcs neural networks nns constructed forming weighted sums corresponding outputs networks 
combination weights selected minimize mean squared error mse respect distribution model inputs criterion commonly statistics neural network communities 
resultant optimal linear combinations referred mse olcs 
constructing mse olcs straightforward 
expressions mse optimal obtained closed form require modest computational effort mainly simple matrix manipulations 
practice optimal combination weights need estimated observed data 
component nns trained approximate physical quantity quantities collinearity linear dependency corresponding outputs approximation errors component networks undermine robustness generalization ability estimated mse olcs 
improve robustness algorithms selecting component networks 
conduct experimental study examine effectiveness mse olcs guided selection algorithms improving model accuracy 
important remarks regarding discussions ffl class neural networks investigated class multilayer feedforward networks pages 
assumptions regarding network architecture learning method needed 
ffl focuses mainly function approximation regression problems 
applicable classification settings predefined classes decision join class represented probability belonging class 
classification problems formulated function approximation problems 
ffl evaluate effectiveness mse olc performance compared popular alternatives simple average equal combination weights apparent best nn 
method simple average require data estimation method best nn 
nn best fits training data necessarily true best number trained nns 
true function known common practice test trained nns data set separate training data set order select best performer 
examples treated true function known 
measure accuracy terms mse respect known function integrating squared error range inputs monte carlo integration 
resultant mse referred true mse 
divided main parts part briefly discuss related section formulate optimal linear combination problem study cases mse olcs section 
closed form expressions optimal combination weights section 
estimation optimal combination weights discussed section 
second part study robustness estimated mse olcs investigate ill effects collinearity sections 
propose algorithms improving robustness mse olc proper selection component networks section 
section experimental results explore merits mse olcs effectiveness proposed algorithms improving robustness 
summarized section 
related literature combining estimators rich diverse 
clemen cites studies review literature related combining forecasts including contributions forecasting psychology statistics management science literature 
traces idea combining estimators back laplace 
linear combinations estimators studied statistics community long time 
simple average number estimators frequently compared individual estimators cases improves resultant model accuracy 
idea combining multiple neural networks investigated number researchers 
application areas included regression classification 
context regression hashem schmeiser investigated combining number trained neural networks forming weighted sums corresponding outputs component networks 
hashem schmeiser illustrated optimal linear combinations olcs may significantly improve model accuracy 
studied cases olcs depending combination weights constrained sum 
perrone perrone cooper independently developed general ensemble method gem constructing improved regression estimates equivalent constrained mse optimal linear combination 
jacobs jordan developed hierarchical mixtures experts hme architecture uses divide conquer approach dividing complex problem simpler problems solved seperate expert networks 
final solution obtained combining outputs expert networks gating networks 
wolpert introduced stacked generalization scheme minimizing generalization error number generalizers combining 
breiman extended wolpert stacking regressions method forming linear combinations different predictors improve prediction accuracy 
combination weights computed cross validation approach involves constructing auxiliary set predictors structure original predictors 
auxiliary predictors trained leave cross validation data 
stacking may improve prediction accuracy obtaining combination weights expensive computationally especially predictors neural networks need re trained cross validation data 
context classification problems idea ensemble trained nns simply best nn investigated number researchers 
hansen salamon suggested training group nns architecture initialized different connection weights 
screened subset trained nns making final classification decision voting scheme 
similar approaches introduced alpaydin battiti 
cooper suggested constructing multi neural network system number nns independently compete accomplish required classification task 
multi network system learns experience networks effective separators determine final classification 
mani suggested training portfolio nns possibly different topologies variety learning techniques 
sketched approach lowering variance decision nns portfolio theory 
pearlmutter rosenfeld replicated networks number identical networks independently trained data results averaged 
concluded replication re sults decrease expected complexity network increases expected generalization 
mse olcs compared simple average corresponding outputs trained networks 
proposed architecture called parallel consensual neural network statistical consensus theory 
architecture consists stage networks outputs combined decision 
field medical diagnosis trained networks separately data sampled populations different likelihoods order simplify training improve model accuracy 
ohno machado musen multiple neural networks models improve classification performance 
rationale multiple networks classification absence true model may improve apparent best nn employing networks solve classification task independently constructing final vote making individual votes 
mse olc approach similar approaches assumes component networks trained approximate physical quantity quantities 
combined output mse olc weighted sum corresponding outputs component networks combination weights estimated observed data 
linear combinations neural networks nn perspective combining corresponding outputs number trained nns similar creating large nn trained nns subnetworks operating parallel combination weights connection weights output layer 
input output combined model weighted sum corresponding outputs component nns ff associated combination weights 
main difference situations combining nns connection weights trained nns fixed combination weights computed performing simple fast matrix manipulations discussed sections 
training large nn larger number parameters weights need simultaneously estimated trained 
training time large nn may longer risk fitting data may increase number parameters model large compared cardinality data set estimating parameters 
consider approximating multi input single output mappings 
approach multi output case compute optimal combination weight vector output separately 
independent treatment straightforward minimizes total mse multi input multi output mappings 
consider multi input single output mapping approximated trained nn 
trained nn accepts vector valued input returns scalar output response 
approximation error ffi gamma response real system true response linear combination outputs nns returns scalar output ff ff corresponding approximation error ffi ff gamma ff output jth network ff associated combination weight extend definition ff include constant term ff 
term allows correction bias ff ff ff linear combination outputs trained neural networks 
ff ff theta vectors 
problem find values combination weights ff ff ff approach select networks best say nn set ff set combination weights zero 
single network advantage simplicity disadvantage ignoring possibly useful information gamma networks 
approach widely forecasting community neural network community equal combination weights simple average simple average straightforward assumes component networks equally 
mse olc neural networks think input observation random variable usually unknown multivariate distribution function real response random variable output jth network random variable associated approximation error random variable ffi linear combination output random variable ff ff linear combination error random variable ffi ff gamma ff 
optimal linear combination olc defined optimal combination weight vector ff ff ff ff minimizes expected loss ffi ff df support loss function 
various loss functions pursued focus squared error loss ffi ffi objective minimize mean squared error mse mse ff ffi ff denotes expected value respect resultant linear combination referred mse optimal linear combination mse olc 
mse olc defined optimal combination weight vector ff ff ff ff minimizes mse ff 
mse olc combination weights mse olc problem equation mse olcs problems derived considered 
variations mse olc problems inclusion exclusion constant term ff defined section constraining combination weights ff sum 
inclusion constant term helps correcting possible biases component nns 
cases mse olcs unconstrained mse olc constant term discussed section theoretically yields smallest mse 
section obtain closed form expressions optimal combination weights expressions increase mse result constraining combination weights sum allowing constant term 
unconstrained mse olc constant term consider problem min ff mse ff differentiating mse respect ff leads optimal combination weight vector ff psi gamma psi ij theta matrix ehr theta vector 
corresponding minimal mse mse gamma psi gamma equations derived appendix 
constrained mse olc constant term consider case constraining combination weights ff ff sum min ff mse ff ff vector proper dimension component equal zero remaining components equal 
solving lagrangian equivalent leads optimal combination weight vector ff ff gamma fi psi gamma fi gamma psi gamma psi gamma corresponding minimal mse mse mse fi psi gamma second term expression mse easily shown non negative 
reflects cost increase mse result constraining sum combination weights unity 
equations derived appendix 
unconstrained mse olc constant term consider problem min ff mse ff ff vector proper dimension component equal remaining components equal zero 
solving lagrangian equivalent leads optimal combination weight vector ff ff gamma fi psi gamma fi psi gamma psi gamma corresponding minimal mse mse mse fi psi gamma second term expression mse easily shown non negative 
reflects cost increase mse result constant term ff combination 
equations derived appendix 
constrained mse olc constant term case ff set zero sum remaining combination weights constrained unity 
restricting sum combination weights unity mse olc weighted average outputs component networks 
unbiased statistical sense unbiased 
mse olc equivalent general ensemble method developed independently perrone cooper 
min ff mse ff ff ff solving lagrangian equivalent leads optimal combination weight vector ff ff gamma fi psi gamma gamma fi psi gamma fi psi gamma gamma fi psi gamma psi gamma fi psi gamma gamma psi gamma gamma psi gamma psi gamma psi gamma psi gamma gamma psi gamma corresponding minimal mse mse mse fi psi gamma fi psi gamma fi fi psi gamma terms expression mse easily shown non negative 
sum reflects cost increase mse constant term ff time restricting remaining combination weights sum 
equations derived appendix 
mse olc ordinary squares regression mse olc problems section equivalent ordinary squares ols regression 
equivalence provides alternate expressions optimal combination weights mse olc problems described sections 
facilitates studying properties estimators combination weights discussed section 
mse olc problem section equivalent regressing intercept term 
optimal combination weights equation equal ordinary squares ols regression coefficients 
problem equivalent ols problem intercept term 
optimal combination weights may alternatively obtained ff psi gamma psi ij thetap matrix ehr theta vector 
similarly constrained mse olc problem section equivalent regressing gamma gamma pg intercept term 
associated optimal weight vector may alternatively obtained ff psi gamma psi ij eh gamma gamma theta matrix indicator variable equal unity zero eh gamma gamma theta vector 
problem equivalent ols problem intercept term 
optimal combination weights may alternatively obtained ff psi gamma psi ij eh gamma gamma gamma theta gamma matrix eh gamma gamma gamma theta vector 
alternate expressions constrained mse olc combination weights sections constrained optimal combination weights expressed functions outputs nns alternatively problems optimal combination weights may expressed terms neural networks approximation errors ffi constrained mse olc constant term equation constrained mse olc equivalent min ff ffi ff ff solving lagrangian equivalent leads optimal combination weight vector ff omega gamma omega gamma omega 
ij ffi theta matrix 
corresponding minimal mse mse omega gamma equations derived appendix 
constrained mse olc constant term equation constrained mse olc problem equivalent min ff ffi ff ff ff solving lagrangian equivalent leads optimal combination weight vector ff ff ff omega gamma omega gamma vector proper dimension components equal omega ij ffi theta matrix 
corresponding minimal mse mse omega gamma equations derived appendix 
mse olc combination weights estimation problem section closed form expressions mse olc combination weights 
expressions expected values taken respect multivariate distribution func tion model inputs 
practice seldom knows psi psi omega omega equations need estimated 
section study problem estimating mse olc combination weights example illustrate significant improvement approximation accuracy result mse olcs 
problem definition set observed data estimate mse olc combination weights fk independently sampled ordinary squares estimators section equivalence relation mse olc combination weights ols regression coefficients allows ols estimators estimating mse olc combination weights 
ols estimators granger ramanathan estimating optimal combination weights combining forecasts 
analysis ols estimators straightforward may provide measures quality estimated mse olc discussed section 
unconstrained mse olc constant term mse olc problem section equivalent regression model ff ff random error zero mean variance oe ols estimator ff ff psi gamma psi ij assuming observed errors uncorrelated ff unbiased minimum variance unbiased estimators ff page 
assuming normally distributed ff multivariate normal distribution mean ff covariance matrix oe psi gamma estimate covariance matrix ff may obtained substituting psi psi substituting unbiased estimator oe mse 
obtain estimates standard deviations estimates mse olc combination weights estimates pairwise correlations 
joint confidence regions tests statistical significance ff easily constructed pages 
similar results apply ols estimators ff ff ff section 
descriptive measures association coefficient multiple determination adjusted coefficient multiple determination may computed combined model page 
measures proportionate reduction total variation associated set variables adjustment penalizes excessive regression parameters model 
constrained mse olc constant term mse olc problem section equivalent regression model ff ff gamma gamma pg ols estimator ff ff psi gamma psi gamma ij unconstrained mse olc constant term mse olc problem section equivalent regression model ff ols estimator ff ff psi gamma psi ij constrained mse olc constant term mse olc problem section equivalent regression model ff gamma gamma pg ols estimator ff ff psi gamma psi gamma ij alternate estimators constrained mse olc combination weights reasons discussed section consider alternate estimators optimal combination weights constrained mse olcs 
estimators expressions section 
constrained mse olc constant term expression mse olc combination weights section may estimate mse olc combination weights constrained mse olc problem ff omega gamma omega gamma omega ij ffi ffi constrained mse olc constant term constrained mse olc problem section expression combination weighs section may estimate mse olc ff ff ff omega gamma omega gamma omega ij ffi ffi example consider problem approximating single input single output function gamma cos sin interval reported 
range 
hidden layer nns hidden units hidden layer nn nn nn hidden layer nns hidden units nn nn nn 
network input unit output unit 
activation function hidden units output units logistic sigmoid function gammas gamma linear data scaling model input output 
networks initialized independent random connection weights uniformly distributed trained error backpropagation algorithm 
set independent uniformly distributed points training networks estimating optimal combination weights 
structural differences different initial connection weights networks trained manner 
equation yields estimated unconstrained optimal combination weight vector gamma gamma estimated standard deviation vector constant term appear statistically significant level significance associated sided value page 
combination weights statistically significant associated sided values 
equation omits constant yields estimated unconstrained optimal combination weight vector gamma gamma estimated standard deviation vector indicates combined model reduces total variation zero 
estimated unconstrained mse olc results true mse produced nn true best nn approximate simple average corresponding outputs nns 
results demonstrate mse olcs dramatically improve accuracy neural network model 
illustrates unconstrained mse olc yields superior fit compared best nn nn simple average nns 
interesting observation sum estimated optimal combination weights approximately constant term statistically significant 
experience quite common cases component nns trained accurate :10.1.1.27.4255
example root mean squared rms error associated best nn associated worst nn show component nns trained see 
constant term statistically significant illustrates nns may insignificant biases may need constant term mse olc 
robustness mse olcs construction unconstrained mse olc constant term theoretically yields minimal mse compared best network simple average mse olcs discussed section 
construction estimated unconstrained mse olc constant term superior accuracy mse sense combination data set defined section 
important performance measure accuracy measured separate data set sampled multivariate distribution performance measure referred sample performance generalization ability 
measure determines robustness mse olc 
terms sample performance generalization ability robustness interchangeably 
example true function true means computed relative true known response function 
mse olc nn average function approximations obtained unconstrained mse olc nn simple average 
known practice robustness olc model may evaluated testing performance model separate data set referred testing data set 
accuracy mse olc may compared apparent best nn simple average determine robustness mse olc 
potential problem affects estimation optimal combination weights robustness mse olc collinearity predictor variables regression model described section 
outputs nns trained approximate response variable expect highly positively correlated 
matrix psi equation may ill conditioned making inversion highly sensitive round errors highly sensitive small variations data case noisy data 
forecasting literature computational statistical ill effects collinearity blamed robustness olcs 
likewise literature combining neural networks perrone cooper point potential problems ill conditioned correlation matrices 
section discuss effect collinearity estimation optimal combination weights 
collinearity correlation collinearity correlation related concepts 
thing 
page gives precise definition collinearity literally variates collinear data vectors representing lie line subspace dimension 
generally variates collinear linearly dependent vectors represents exact linear combination vectors lie subspace dimension hand correlation variables variates defined expected value normalized product variables centered corresponding means normalized stands normalization respect standard deviations variables respectively 
pages indicates high correlation coefficient explanatory regressor variables point possible collinearity problems absence high correlations viewed evidence absence collinearity problems high correlation implies collinearity converse true 
pages shows variables perfectly collinear absolute pairwise correlation exceeds gamma 
point discussion draw attention fundamental points ffl collinearity correlation thing pages 
special diagnosis needs applied detect presence collinearity possibly addition estimating pairwise correlations variables studied 
ffl harm estimates combination weights pages robustness mse olc 
looking diagnostic tool detect presence collinearity needs look appropriate measure collinearity 
section mse olc problems section shown equivalent ols regression regressor variables outputs trained nns function outputs 
nns individually trained approximate response expect correlation fairly positively high 
inherent high positive correlations estimating mse olc prone collinearity problems section 
result robustness mse olc may affected 
alternate expressions section may appear vulnerable ill effects collinearity section expressions rely approximation errors component nns ffi 
correlations ffi may high positive 
example pairwise correlations outputs trained nns range fairly high 
pairwise correlations ffi trained nns range gamma 
unconstrained mse olc reduces true mse compared best nn simple average respectively section 
high positive correlations associated positive correlations ffi necessarily eliminate benefit combining result harmful collinearity 
discussion evident successful deployment mse olc needs answer questions 
detect presence collinearity identify regressor variables associated 

determine existing collinearity harmful 
write simplicity 
applies ffi subsequent discussions 
true refers mse respect true function true function known example 

deal harmful order improve robustness estimated mse olc 
regression literature procedures developed collinearity detection 
pages discusses main classes procedures points strengths weaknesses 
develop diagnostics explicit measurement severity collinearity 
diagnostics capable determining existence multiple identifying variables involved collinearity 
adopt diagnostics refer bkw diagnostics 
proceed investigate remaining questions 
detecting harmful collinearity example discussed sections demonstrates benefit mse olc significantly reducing mse approximating function 
mentioned section pairwise correlation example fairly high ranging 
correlations ffi positive larger 
high positive correlations considered conclusive evidence existing collinearity merely warning collinearity exists may harm robustness estimated mse olc 
discussing methods detecting harmful effects existing collinearity example demonstrate harmful effects really exist collinearity severely undermine robustness estimated mse olc 
example consider approximating function sin gamma range gamma 
nns nn nn nns nn nn nns nn nn initialized independent random connection weights uniformly distributed 
activation function hidden units output units logistic sigmoid function gammas gamma linear data scaling model input output 
nns trained error backpropagation algorithm 
training data set consists uniformly distributed independent points 
nn true best nn yields mse training data true mse 
simple average outputs nns yields mse training data true mse 
training data estimate optimal combination weights unconstrained mse olc constant term reduces mse training data set zero decimal places 
yields true mse larger true mses produced nn simple average orders magnitude indicates robustness estimated mse olc seriously undermined existing collinearity 
mse training data listed completeness true measure performance robustness true mse obtained relative true known function 
interesting observation mse olc sided values regression coefficients including constant term 
fact sided values regression coefficients 
individual regression coefficients statistically significant level significance 
statistical significance optimal combination weights may adequate measure robustness mse olc 
comes agreement argument usual statistic ols estimator estimator standard deviation testing statistical significance argues low may indicate data weaknesses high need indicate absence 
may argue reason lack robustness mse olc example small number data points combining words small number degrees freedom regression model 
increasing number data points combining nns points uniformly distributed independent results estimated yields true mse larger true mse produced nn equal true mse produced simple average 
extra points dramatic improvement robustness mse olc achieved 
estimated mse olc inferior best nn simple average 
acquiring data effective means breaking collinearity data page discussed section 
section introduce algorithms improving robustness mse olc reducing collinearity fixed set data 
cross validation approach detecting harmful collinearity harmful collinearity defined collinearity results lack robustness 
straightforward approach determine existing collinearity testing robustness mse olc 
simple test robustness mse olc compute resultant mse separate data set disjoint sampled distribution mse may measure performance estimated mse olc observations indicates estimated mse olc robust 
testing strategy straightforward similar strategy adopted testing generalization ability single nn 
likewise literature combining forecasts advocate sample testing combination 
construction estimated mse olc results smallest mse compared best nn component nns simple average corresponding outputs nns combination 
robustness resultant mse olc may tested comparing performance best nn simple average different data set referred validation data set 
mse olc best performer validation test may conclude robust 
may look corrective measures improve robustness mse olc 
asymptotically size validation set increases test measures true robustness mse olc 
important issues concerning application validation approach ffl definition robustness section data validation set need randomly sampled practice unknown set observed data available constructing mse olc 
cases assuming data independent equally observed data set may split estimation data set validation data set splitting expense reducing data estimating optimal combination weights 
needs sufficiently large order accurately test robustness mse olc 
ffl notion best nn needs precise definition 
practice know trained nns true best 
reason believe best nn training data set true best nn 
fact nn overfits training data lowest mse number trained nns 
consistent degrees freedom number data points number parameters regression model 
estimator true best best performer trained nns validation set refer network apparent best nn 
asymptotically number data points increases estimator yields true best nn 
improving robustness mse olc variety methods deal harmful collinearity :10.1.1.27.4255
methods ffl introducing new data breakup collinearity pattern page page 
example illustrates robustness mse olc may significantly improved introducing new data 
unfortunately method may limited ability cost acquiring extra data practice page 
ffl biased estimation techniques improve efficiency estimating regression coefficients optimal combination weights 
presence collinearity biased estimation procedure ridge regression pages latent root regression essentially trading small bias estimates reduction variances pages 
literature combining forecasts clemen indicated latent root regression produces efficient estimates combination weights compared ols estimates sample forecasting performances comparable 
critique biased regression methods practice refer 
ffl regressor variables may dropped model order lessen collinearity page especially regressor variables contribute redundant information page 
component nns approximations physical variable 
dropping collinear regressor variables justifiable 
section introduce algorithms selecting nns mse olcs 
presenting selection algorithms discuss common approach improving robustness olc 
clemen suggests may appropriate restrict combination weights require constant term restricted combination results efficient robust forecast 
holden peel agree clemen need constrain combination weights sum emphasize role constant term minimizing sample squared prediction errors 
argues unconstrained model give better fit past data constraints improve robustness combination forecasting 
examine effectiveness restricting combination weights sum removing constant term combination re examine mse olc example 
example continued original unconstrained mse olc constant term constructed section yields true mse 
consider mse olcs section order ffl constrained mse olc constant term training data estimating optimal combination weights 
estimated mse olc yields true mse unconstrained mse olc larger order magnitude true mses associated best nn simple average 
ffl unconstrained mse olc constant term training data estimating optimal combination weights 
estimated mse olc yields true mse constant term included combination 
resultant true mse larger order magnitude true mses associated best nn simple average 
ffl constrained mse olc constant term training data estimating optimal combination weights 
estimated mse olc yields true mse unconstrained mse olc constant term constrained mse olc constant term 
resultant true mse larger order magnitude true mses associated best nn simple average 
example constraining combination weights sum excluding constant term combination yields robust mse olc far 
performance current best mse olc far acceptable 
analysis collinearity structure bkw diagnostics reveals constant term mse olc involved strongest collinearity regressor variables 
may explain dropping constant term combination improves robustness 
section approach selecting component networks mse olcs bkw diagnostics 
algorithms selecting component neural networks section introduce approach improving robustness mse olc proper selection nns guided diagnostics collinearity outputs nns ffi approximation errors nns 
algorithms algorithm algorithm approach 
inputs algorithms trained nns estimation data set validation data set algorithms bkw collinearity diagnostics analyze collinearity structure networks determine relative strength existing identify nns involved collinearity 
algorithm relies diagnosing collinearity algorithm relies diagnosing collinearity ffi 
algorithms cross validation approach outlined section test robustness sample performance 
algorithms greedy sense target strongest collinearity 
nns involved strongest collinearity identified algorithms attempt breakup collinearity dropping worst performer combination 
worst performer defined nn yields largest mse case algorithms drop best nn combination 
algorithms performance simple average outputs component nns best nn measured validation set taken yard stick measure robustness resultant combination 
termination best combination algorithm produces selection procedure yields inferior performance compared best nn simple average algorithm selects final outcome best performer 
algorithms conservative 
drop nns combination current mse olc deemed inferior best nn simple average determined relative performance aggressive approach may allow dropping networks long performance keeps improving 
employing algorithms needs keep mind component nns may carry different information knowledge 
nns included final combination better 
reason excluding nns likewise constant term mse olc presence harmful collinearity 
algorithm algorithm employs unconstrained mse olcs olcs relies solely information provided bkw diagnostics collinearity 
algorithm proceeds follows 
determine mse best nn simple average nns 
consider nns combination 

form olc considered nns including constant term decision exclude constant taken earlier estimate optimal 

determine mse olc step 
olc yields lowest mse compared best nn simple average return current olc 

construct set nns involved strongest collinearity current combination bkw diagnostics 

elements ffl nns current combination constant term involved strongest collinearity drop worst performer combination 
go step 
ffl constant involved strongest collinearity drop combination 
ffl return best performer best nn simple average 
element ffl nn best nn constant term dropped combination indicates significant collinearity largest condition index associated mainly best nn return best performer best nn simple average 
drop constant term combination 
ffl drop nn combination 

nn left go step 
return best performer best nn simple average 
algorithm algorithm employs constrained mse olcs olcs relies solely information provided bkw diagnostic collinearity ffi 
discussed section olcs may robust olcs especially small samples 
algorithm uses olcs improve robustness cases robustness olc constant term deemed unsatisfactory 
algorithm identical algorithm features ffl robustness olc nns deemed unsatisfactory algorithm adopts olc olc subsequent steps 
ffl relying collinearity diagnosis algorithm relies diagnosing collinearity ffi 
set set nns involved strongest collinearity ffi 
algorithm proceeds follows 
determine mse best nn simple average nns 
consider nns combination 

execution step ffl form olc nns including constant term estimate optimal combination weights 
ffl determine mse olc ffl olc yields lowest mse compared best nn simple average return current olc 
ffl form olc nns including constant term decision exclude constant taken earlier estimate optimal 
ffl determine mse olc ffl olc yields lowest mse compared best nn simple average return current olc 

construct set nns involved strongest collinearity ffi current combination bkw diagnostics 

elements ffl nns current combination constant term involved strongest collinearity drop worst performer combination 
go step 
ffl constant involved strongest collinearity drop combination 
ffl return best performer best nn simple average 
element ffl nn best nn constant term dropped combination indicates significant collinearity largest condition index associated mainly best nn return best performer best nn simple average 
drop constant term combination 
ffl drop nn combination 

nn left go step 
return best performer best nn simple average 
modification algorithms section discussed need splitting data set order detect harmful collinearity 
illustrated section including extra data estimation optimal combination weights helps breakup collinearity networks 
compromise may achieved available data final estimation step algorithms decide networks included final combination 
illustrate effectiveness algorithms handling harmful collinearity problems re examine example 
example continued sections examined approaches constructing mse olcs trained nns generated example 
approaches yields performance comparable nn best nn simple average 
algorithms algorithm recommends nn algorithm recommends combining nn nn nn addition constant term 
estimated olc constructed algorithm yields true mse true mse nn true mse simple average networks 
algorithms capable handling harmful collinearity improving robustness estimated mse olcs 
section experimental results examine effectiveness algorithms 
experimental results consider approximating sin gamma 
range gamma 
conduct experiment independent replications 
replication copy nns nn nn nn nn nn nn initialized independent connection weights uniformly distributed 
nn nn nns hidden layer hidden units nn nn nns nn nn nns 
activation function hidden units output units logistic sigmoid function gammas gamma linear data scaling model input output 
independent training sets set replication 
uniformly distributed independent points 
replication say nns trained common training set nns 
separate replication different sets nns produced training process total trained nns 
nns example result replications examined 
independent validation sets set replication 
uniformly distributed independent points 
replication data set estimating mse olc combination weights equation 
sets algorithms explained section 
replication simple average trained nns constructed apparent best nn identified validation data set resultant true computed relative true function mses mse olc best nn simple average computed compared replication 
averaging replications mse olc yields mean percentage reduction true mse compared apparent best nn mean percentage reduction compared simple average 
corresponding standard errors mean percentage reductions respectively 
results demonstrate mse olcs substantially improve model accuracy 
algorithms yield better results corresponding mean percentage reductions true mse respectively compared apparent best nn mean percentage reductions respectively compared simple average 
corresponding standard errors mean percentage reductions respectively 
examine impact measurement noise observed data robustness mse olcs corrupt data sets gaussian noise added observed response 
retrained nns procedure constructing mse olcs algorithms averaging replications estimated mse olc selection algorithms yields mean percentage increases true mse compared apparent best nn compared simple average 
replication estimated mse olc resulted reductions true mse compared apparent best nn simple average 
remaining replication harmful collinearity undermined robustness estimated mse olc case studied example 
algorithms yielded corresponding mean percentage reductions true mse respectively compared apparent best nn mean percentage reductions compared simple average 
corresponding standard errors mean percentage reductions respectively 
results illustrate effectiveness algorithms handling harmful collinearity improving robustness estimated mse olcs 
recommendations mse olcs number trained neural networks substantially improve model accuracy compared single best network simple average corresponding outputs component networks 
multiple trained networks available byproduct modeling process additional computational effort required create mse olc essentially estimating optimal combination weights mainly matrix inverse 
unconstrained mse olc constant term yields theoretical minimal mse combination methods considered practice robustness estimated mse olc may seriously undermined due presence harmful collinearity 
proper selection component networks utilizing collinearity diagnostics cross validation methods significantly improve robustness estimated mse olc 
practical situation desired construct neural network model data generating process set observed data procedure combining number trained networks ffl construct unconstrained mse olc constant term training data 
construct simple average trained networks identify best network validation data set 
ffl compare performance mse olc validation data best network simple average 
ffl mse olc significantly outperforms best network simple average may need trying selection algorithms 
selection algorithms employed 
alpaydin 
multiple networks function learning 
proceedings ieee international conference neural networks volume pages 
ieee press apr 
battiti 
democracy neural nets voting schemes classification 
neural networks 

improving accuracy artificial neural network multiple differently trained networks 
neural computation 

assessing presence harmful collinearity forms weak data test signal noise 
journal econometrics 

conditioning diagnostics collinearity weak data regression 
john wiley sons new york 

regression diagnostics identifying influential data sources collinearity 
john wiley sons new york 
swain 
parallel consensual neural networks 
proceedings ieee international conference neural networks volume pages 
ieee press apr 
breiman 
stacked regressions 
technical report department statistics university california berkeley california usa aug 
revised june 

forecasting model 
journal forecasting 
clemen 
linear constraints efficiency combined forecasts 
journal forecasting 
clemen 
combining forecasts review annotated bibliography 
international journal forecasting 
clemen winkler 
combining economic forecasts 
journal business economic statistics jan 
assume training observed data set split training data set validation data set typically premature termination training avoid overfitting training data testing data set 
cooper 
hybrid neural network architectures equilibrium systems pay attention 
editors neural networks theory applications pages 
academic press 

statistically controlled activation weight initialization 
ieee transactions neural networks 
drucker le cun 
double backpropagation increasing generalization performance 
proceedings international joint conference neural networks seattle volume ii pages 
ieee press 
fahlman 
faster learning variations back propagation empirical study 
proceedings connectionist models summer school 
morgan kaufman 
davis jr yuan 
strategies issues applications neural networks 
proceedings international joint conference neural networks baltimore volume iv pages 
ieee press 
gardner jr 
forecasting 
international journal forecasting 
granger 
combining forecasts years 
journal forecasting 
granger ramanathan 
improved methods combining forecasts 
journal forecasting 
jr clemen 
collinearity latent root regression combining gnp forecasts 
journal forecasting 
hansen salamon 
neural network ensembles 
ieee transactions pattern analysis machine intelligence 
hashem :10.1.1.27.4255
optimal linear combinations neural networks 
phd thesis school industrial engineering purdue university dec 
hashem schmeiser 
approximating function derivatives mse optimal linear combinations trained feedforward neural networks 
proceedings world congress neural networks volume pages new jersey 
lawrence erlbaum associates 
hashem schmeiser 
improving model accuracy optimal linear combinations trained neural networks 
ieee transactions neural networks 
hashem schmeiser yih 
optimal linear combinations neural networks overview 
proceedings ieee international conference neural networks volume iii pages 
ieee press 
hashem yih schmeiser 
efficient model product allocation optimal combinations neural networks 
dagli burke fern andez ghosh editors intelligent engineering systems artificial neural networks volume pages 
asme press 
haykin 
neural networks comprehensive foundation 
ieee press new jersey 
hertz krogh palmer 
theory neural computation 
addison wesley 
holden peel 
efficiency combination economic forecasts 
journal forecasting 
jacobs jordan 
competitive modular connectionist architecture 
lippmann moody touretzky editors advances neural information processing systems pages 
morgan kaufman 
jacobs jordan 
hierarchical mixtures experts em algorithm 
neural computation 
kolen pollack 
back propagation sensitive initial conditions 
lippmann moody touretzky editors advances neural information processing systems pages 
morgan kaufman 
laplace 
eme suppl ement la th eorie des probabilit es 
paris 
reprinted compl de laplace vol 
paris gauthier 
lari samad 
effect initial weights back propagation variations 
proceedings ieee international conference systems man cybernetics pages 
ieee press 
levin tishby solla 
statistical approach learning generalization layered neural networks 
proceedings ieee oct 
mani 
lowering variance decisions artificial neural networks portfolios 
neural computation 
markowitz 
portfolio selection 
journal finance 
menezes 
specification predictive distribution combination forecasts 
methods operations research 
morgan bourlard 
generalization parameter estimation feedforward nets experiments 
touretzky editor advances neural information processing systems pages 
morgan kaufman 

improving generalising capabilities back propagation network 
international journal neural networks research applications 
wasserman 
applied linear statistical models 
irwin il 
ohno machado musen 
hierarchical neural networks partial diagnosis medicine 
proceedings world congress neural networks volume pages 
lawrence erlbaum associates 
munro doyle marino mitchel fung 
neural network classifier detection 
proceedings world congress neural networks volume pages new jersey 
lawrence erlbaum associates 
pearlmutter rosenfeld 
chaitin kolmogorov complexity generalization neural networks 
advances neural information processing systems pages 
perrone 
improving regression estimation averaging methods variance reduction extensions general convex measure optimization 
phd thesis department physics brown university may 
perrone cooper 
networks disagree ensemble methods hybrid neural networks 
editor neural networks speech image processing 
chapman hall 
forthcoming 

combining results neural network classifiers 
neural networks 

probability statistics engineers 
pws kent publishing boston 
smith campbell 
critique ridge regression methods 
journal american statistical association mar 

monte carlo method 
university chicago press 
translated adapted nd russian edition stone 
walpole myers 
probability statistics engineers scientists 
macmillan college publishing new york 
webster mason 
latent root regression analysis 
technometrics nov 
weigend rumelhart huberman 
generalization weight elimination application forecasting 
lippmann moody touretzky editors advances neural information processing systems pages 
morgan kaufman 
winkler clemen 
sensitivity weights combining forecasts 
operations research may june 
wolpert 
stacked generalization 
neural networks 

artificial neural systems 
west publishing st paul mn 
appendix mse olc combination weights unconstrained mse olc constant term consider problem min ff mse ff solution mse ff ffi ff ehr gamma ff ehr gamma ff ff gamma ff ff psi ff ehr theta vector psi ij theta matrix 
derivative mse respect ff equating derivative zero obtain optimal weight vector ff psi gamma corresponding minimum mse mse ehr gamma psi gamma gamma psi gamma constrained mse olc constant term consider problem min ff mse ff ff vector proper dimension component equal zero remaining components equal 
solution forming lagrangian function ehr gamma ff fi ff gamma gamma ff ff psi ff fi ff gamma differentiating respect ff ff gamma psi ff fi ff psi gamma gamma fi premultiplying condition ff ff yields fi gamma psi gamma psi gamma ff ff gamma fi psi gamma mse ehr gamma ff ehr gamma ff gamma fi psi gamma eh gamma ff fi psi gamma mse fi psi gamma alternate solution equivalent min ff ffi ff ff ffi ff gamma ff ff gamma ff ff gamma ff ffi ffi theta vector ffi gammay gamma ffi gamma 
ffi ff eh ff ffi ff omega ff omega 
ij ffi theta matrix 
forming lagrangian function ff omega ff ff gamma differentiating respect ff ff omega ff yields ff gamma omega gamma premultiplying condition ff ff yields gamma omega gamma ff omega gamma omega gamma mse omega gamma unconstrained mse olc constant term consider problem min ff mse ff ff vector proper dimension component equal remaining components equal zero 
solution forming lagrangian function ehr gamma ff fi ff gamma ff ff psi ff fi ff differentiating respect ff ff gamma psi ff fi ff psi gamma gamma fi premultiplying condition ff ff yields fi psi gamma psi gamma ff ff gamma fi psi gamma mse ehr gamma ff ehr gamma ff gamma fi psi gamma eh gamma ff fi psi gamma mse fi psi gamma constrained mse olc constant term consider problem min ff mse ff ff ff solution forming lagrangian function ehr gamma ff fi ff fi ff gamma gamma ff ff psi ff fi ff fi ff gamma differentiating respect ff ff gamma psi ff fi fi ff psi gamma gamma fi gamma fi ff gamma fi psi gamma gamma fi psi gamma premultiplying ff condition ff ff yields fi psi gamma gamma fi psi gamma psi gamma premultiplying ff condition ff ff yields fi psi gamma gamma psi gamma gamma psi gamma psi gamma psi gamma psi gamma gamma psi gamma mse ehr gamma ff ehr gamma ff gamma fi psi gamma gamma fi psi gamma eh gamma ff fi psi gamma fi psi gamma mse fi psi gamma fi psi gamma fi fi psi gamma note expressions easily shown fi fi expressed function fi fi follows fi fi gamma fi psi gamma psi gamma fi fi gamma fi psi gamma psi gamma psi gamma psi gamma psi gamma psi gamma gamma psi gamma alternate solution equivalent min ff ffi ff ff ff ffi ff gamma ff ff gamma ff ff gamma ff ffi ffi theta vector ffi gammay gamma ffi gamma 
condition ff ff reduces effective ffi 
keeping mind ff equivalent min ff eh ff ffi ff vector proper dimension components equal ff ffi theta vectors ffi ffi gamma 
eh ff ffi ff omega ff omega ij ffi theta matrix 
forming lagrangian function ff omega ff ff gamma differentiating respect ff ff omega ff yields ff gamma omega gamma premultiplying condition ff ff yields gamma omega gamma ff omega gamma omega gamma ff ff mse omega gamma 
