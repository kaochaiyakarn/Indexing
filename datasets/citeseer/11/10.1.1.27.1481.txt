transformation invariant clustering dimensionality reduction em brendan frey jojic submitted ieee transactions pattern analysis machine intelligence nov 
clustering dimensionality reduction simple effective ways derive useful representations data images 
procedures preprocessing steps sophisticated pattern analysis techniques 
fact procedures perform better sophisticated pattern analysis techniques 
situations input randomly transformed translation rotation shearing images methods tend extract cluster centers submanifolds account variations input due transformations interesting potentially useful structure 
example images human face clustered useful different clusters represent different poses expressions different translations rotations 
describe way add transformation invariance mixture models factor analyzers mixtures factor analyzers approximating nonlinear transformation manifold discrete set points 
contrast linear approximations transformation manifold assume amount transformation small method works large levels transformation 
show expectation maximization algorithm jointly learn set clusters subspace model mixture subspace models time infer transformation associated case 
illustrating technique difficult contrived problems compare technique methods filtering noisy images obtained scanning electron microscope clustering images faces different categories identification pose subspace modeling facial expressions subspace modeling images handwritten digits handwriting classification unsupervised classification images handwritten digits 
interested developing algorithms learn models different types object unlabeled images include background clutter spatial transformations translation rotation shearing 
example fig 
shows pixel greyscale images obtained scanning electron microscope 
electron detectors highspeed electrical circuits randomly translate images add noise 
standard filtering techniques appropriate images aligned 
due high level noise difficult properly align hand requires human effort 
fig 
shows pixel greyscale head images person walking outdoors 
camera track persons head perfectly head appears different locations 
images include variation pose head background clutter appears multiple images 
aligning images model frey faculty computer science university waterloo adjunct faculty electrical computer engineering university illinois urbana champaign urbana il usa 
jojic doctoral candidate image formation processing group beckman institute university illinois urbana champaign urbana il usa 
fig 

images size pixels taken scanning electron microscope 
electron detectors high speed electrical circuits introduce random translations 
person appearance temporal information difficult 
temporal information video sequence standard blob tracking methods due presence coherent background clutter 
fig 

images size pixels person walking outdoors 
head different poses appears different positions field view 
addition background highly cluttered variation lighting conditions 
fig 
shows preprocessed greyscale images handwritten digits postal envelopes 
microscope images described case boundaries digits envelopes easily identifiable digits normalized horizontal vertical scale translation sampling pixel images shown 
digits written different writing angles vertical stroke different versions thought fig 

images handwritten digits normalized horizontal vertical scale translation sampled pixel grid 
different writing angles introduce different levels shearing image 
randomly selected levels horizontal shearing 
appropriate level shearing needed normalize writing angle depends identity digit compare normalizing shearing straightforward preprocessing step 
propose general purpose statistical method jointly normalize transformations occur training data learning density model normalized data 
assume data ordered 
clearly temporal coherence provides useful cues modeling time series data video sequences 
show techniques introduced extended dynamic models hidden markov models 
approach data modeling machine learning labeled data train recognition model accurately predict class membership input 
supervised learning approach includes nonlinear regression techniques classification regression trees neural networks gaussian process regression support vector classifiers nearest neighborhood type methods including eigen space methods compute distances subspaces :10.1.1.12.7580
contrast approach take unlabeled data train probability density model data generative model unsupervised fashion 
common data processing techniques viewed way clustering linear dimensionality reduction principal components analysis 
procedures correspond estimation density models mixture gaussians factor analysis 
restricting density models various ways maximum likelihood estimation corresponds standard non probabilistic algorithms 
example setting covariance matrices gaussians mixture gaussians means clustering obtained 
restricting factor loading matrix sensor variances factor analyzer principal components analysis obtained 
probabilistic versions techniques distinct advantages 
unsupervised learning useful summarizing data finding common head poses data fig 
filtering data denoising images fig 
estimating density models data compression preprocessing step supervised methods removing shearing handwritten digits fig 
training classifier supervised fashion 
thinking unsupervised learning maximum likelihood estimation density model data incorporate extra knowledge problem 
way include extra latent variables unobserved variables model 
model extends mixture gaussians factor analyzer mixture factor analyzers include transformation latent variable 
model trained expectation maximization em algorithm 
section describe computationally efficient approaches modeling transformations 
describe approaches incorporated generative models mixture gaussians clustering factor analyzer linear dimensionality reduction mixture factor analyzers clustering dimensionality reduction 
refer models transformation invariant models 
describe transformation invariant models fit training set expectation maximization em algorithm 
illustrating models difficult contrived problems compare methods filtering noisy images obtained scanning electron microscope clustering images faces different categories identification pose subspace modeling facial expressions subspace modeling images handwritten digits handwriting classification unsupervised classification images handwritten digits 
focus vision problems methods applied type data 
ii 
discrete linear approximations transformation manifold data models invariant known type transformation input transformed versions particular input equivalent 
suppose element input undergoes transformation degree freedom example pixel greyscale image undergoes translation direction wrap 
imagine happens point dimensional pixel intensity space object translated 
due pixel mixing small amount subpixel translation move point slightly translation trace continuous dimensional curve space pixel intensities 
illustrated fig 
extensive levels translation produce highly nonlinear curve consider translating thin vertical line curve approximated straight line locally 
types continuous transformation applied manifold dimensional 
linear approximations transformation manifold significantly improve performance supervised classifiers nearest neighbors multilayer perceptrons 
linear generative models factor analysis mixtures factor analysis modified linear approximations transformation manifold build degree transformation invariance 
general linear approximation accurate transformations couple neighboring pixels inaccurate transformations couple pixels 
applications handwritten digit recognition input blurred linear approximation valid severe transformations 
multiresolution version linear approximation proposed 
general significant levels transformation nonlinear manifold better modeled discrete approximation 
example curve fig 
represented set points filled discs 
approach discrete set possible transformations specified parameters learned model invariant set transformations 
approach supervised framework design convolutional neural networks trained labeled data 
describe invariance discrete set transformations translation images built generative density model show em algorithm original density model extended new model computing expectations set transformations 
iii 
transformation discrete latent variable section show incorporate discrete linear approximations described various generative models 
conditioning discrete variables models jointly gaussian inference computa fig 

element input vector represented point unfilled disc dimensional space 
input undergoes continuous transformation degree freedom dimensional manifold traced 
transformation invariant data modeling want inputs manifold equivalent sense 
locally curve linear high levels transformation may produce highly nonlinear curve 
approximate manifold discrete points filled discs indexed 
efficient 
expressions may look complicated straightforward linear algebra 
posted matlab routines algorithms webpage www cs toronto edu frey 
sake clarity focus image data transformations translation 
represent th transformation sparse transformation matrix operates vector image pixel intensities 
example integer pixel translations image represented permutation matrices 
types transformation matrix may accurately represented permutation matrices useful types transformation represented sparse transformation matrices 
example rotation blurring represented matrices small number nonzero elements row rotations 
alternatively transformations approximated permutation matrices 
observed image linked latent image transformation index lg follows xj diagonal matrix sensor noise variances 
advantageous set described 
probability transformation may depend latent image joint distribution latent image transformation index observed image jz corresponding graphical model shown fig 

fig 

graphical model showing discrete transformation variable added density model latent image model observed image gaussian pdf xj captures th transformation plus small amount pixel noise 
box represent variables gaussian conditional pdfs 
explored transformed mixtures gaussians discrete cluster index transformed component analysis tca vector gaussian factors may model locally linear transformation perturbations mixtures transformed component analyzers transformed mixtures factor analyzers 
transformed gaussians model noisy transformed images just shape choose gaussian distribution mean gaussian covariance matrix 
usually take diagonal reduce number parameters need estimated 
simplicity assume absence observations independent jz 
joint distribution probability transformation 
parameters represent different types noise 
noise modeled added transformation applied noise modeled added transformation applied 
images large values diagonal indicate regions latent image accurately predicted 
regions may correspond background clutter parts object appear noisy blinking eyes 
fig 
shows hand crafted parameters transformed gaussian models face appearing different positions frame 
shown raster scan format 
diagonal shows diagonal elements raster scan format large variances painted bright small variances parameters transformed gaussian gl various translations generating transformed gaussian shift left fig 

hand crafted model illustrates discrete transformation index incorporated gaussian model 
models additive gaussian noise gets transformed models additive gaussian noise transformed 
painted dark 
variance map indicates head region modeled accurately surrounding region 
fig 
shows configuration variables model drawn joint distribution 
drawn adding independent gaussian noise variances 
transformation index drawn 
transformation applied independent gaussian noise variances case added pixels produce compute density image particular transformation integrate xj xjg indicates matrix transpose 
transformation corresponding mean image covariance matrix conditional density xj looks likelihood mixture factor analyzers 
likelihood computation latent pixels takes order time mixture factor analyzers takes linear time order models considered sparse 
probability density model notice full rank set wish 
may example reduce number parameters need estimated 
typically full rank full rank rank number pixels observed image input image compute probability transformation jx xj transformation normalization stabilization performed computing expected value latent image observed image jointly gaussian compute zj 
linear algebra obtain zj covariance cov zj normalized image computed zjx jx zj set cov zj zj transformed mixtures gaussians tmg fig 
shows graphical model transformed mixture gaussians tmg different clusters may different transformation probabilities 
cluster mixing proportion mean covariance matrix usually take diagonal reduce number parameters need estimated 
joint distribution probability transformation cluster marginalizing latent image gives cluster transformation conditional likelihood xj compute xj cluster transformation responsibility cjx cjx xj transformed component analyzer tca fig 
shows graphical model factor analysis transformed component analysis tca 
latent image modeled linearly combined isotropic gaussian variables joint distribution mean latent image matrix latent image components factor loading matrix diagonal noise covariance matrix latent image 
marginalizing factor variables latent image gives transformation conditional likelihood xj compute transformation responsibility jx 
general jg computed time linear determinant computed linear time set assume case jg jg jg jj jg jj ijj determinants computed time linear experiments reported set 
setting columns equal derivatives respect continuous transformation parameters tca accommodate local linear approximation discrete approximation transformation manifold described scn 
iii 
mixtures transformed component analyzers 
combination tmg tca jointly model clusters linear components transformations 
alternatively mixture gaussians invariant discrete set transformations locally linear transformations obtained combining tmg tca components set equal transformation derivatives described scn 
iii 
joint distribution combined model fig 
cluster transformation likelihood xj computed linear time set assume tca 
incorporating linear approximation transformation manifold turns simple way incorporate linear approximation global nonlinear approximation transformation manifold 
suppose model data mixture gaussians latent variables local global translations 
mixture transformed component analyzers number factor variables variables corresponds translation 
vector pixel intensities resample higher resolution apply small amount translation subsample original resolution form difference image 
approximation tangent vector transformation manifold 
direction set model 
joint distribution standard normal variable account small translations standard normal variable account small translations 
continuous transformations accounted locally similar way 
selecting number transformations 
number scalar operations likelihood computation linear kept mind attempting exhaustive set transformations cause grow polynomially horizontal translations vertical translations rotations scalings components large approximate inference algorithm 
iv 
estimating transformation invariant models expectation maximization algorithm em algorithm general model described 
em algorithm tmg emerges setting number factor variables 
em algorithm tca emerges setting number clusters 
conditioned discrete latent variables transformation index mixture component index mixture models remaining variables models jointly gaussian 
partial marginals gaussians gaussian distribution observed variables discrete latent variables just gaussian particular parameterization mean covariance matrix 
aside model incorporates linear approximation transformation manifold models linear parameters 
em algorithm models essentially just constrained reparameterized version em algorithm standard mixture gaussians 
section describe estimate model incorporates linear approximation transformation manifold 
step 

denote sufficient statistic computed averaging training set 
sufficient statistics computed shown scn 
iv 
diag denote vector containing diagonal elements matrix diag denote diagonal matrix diagonal contains elements vector denote element wise product vectors denote updated parameters step updates parameters follows hp cjx hp jx hp cjx yjx hp cjx diag hp cjx jx hp cjx diag jx hp cjx jx cjx yy jx reduce number parameters may assume depend held constant uniform distribution 
order avoid overfitting noise variances useful set diagonal elements equal 
step sufficient statistics step computed sparse linear algebra single pass training set 
follows important keep mind matrix sparse usually permutation matrix matrices sparse 
making pass training set compute sufficient statistics matrices computed cov zjx cov yjx case training set processed 
case jx computed combination described scn 
iii 
expectations computed yjx zjx jx zjx zjx diag diag jx zjx yjx expectations needed accumulate sufficient statistics computed cjx yjx jx zjx yjx cjx jx jx jx diag diag jx yjx yjx jx jx zjx zjx diag diag cjx jx jx jx cjx yy jx jx jx yjx yjx learning models incorporate linear approximation transformation manifold simple approach treat constants updating described 
updated new values compute sufficient statistics 
fig 

pixel sem images 
mean variance image pixels 
mean variance tmg reveal structure uncertainty 
experiments filtering images scanning electron microscope sem 
sem images fig 
low signal noise ratio due high variance electron emission rate modulation variance imaged material 
reduce noise multiple images usually averaged pixel variances estimate certainty rendered structures 
fig 
shows estimated means variances pixels sem images ones fig 

fact averaging images take account spatial uncertainties filtering imaging process introduced electron detectors high speed electrical circuits 
trained single cluster tmg horizontal shifts vertical shifts sem images iterations em 
keep number parameters equal number parameters estimated simple averaging transformation probabilities learned pixel variances observed image set equal step 
tmg parameter 
fig 
shows mean variance learned tmg 
compared simple averaging tmg finds sharper detailed structure 
variances significantly lower indicating tmg produces confident estimate image 
fig 

extracting transformation invariant structure synthetic data tmg 
training examples include background clutter fixed distraction 
means variances cluster tmg 
means variances cluster gaussian mixture model 
principal components eigenimages 
extracting clusters synthetic data 
fig 
shows examples training set cases images 
image contains shapes large square large circle small filled square small pacman 
background produced randomly selecting pixel intensities independently uniform distribution 
addition background includes fixed distraction form pixels set maximum intensity 
trained tmg containing clusters translation transformations horizontal shifts vertical shifts iterations em algorithm 
weights initialized small random values mixing proportions initialized equal 
fig 
shows mixing proportions cluster means diagonal elements cluster covariance matrices tmg extra clusters necessary clusters model pac man 
remaining clusters model remaining shapes 
notice cluster variances indicate pixels background pixels light high variance versus foreground pixels dark low variance 
fig 
shows parameters learned iterations em algorithm traditional mixture model cluster 
model viewed special type tmg uses just identity transformation 
shapes severely blurred model distraction 
number clusters increased model capture different transformations different clusters 
shapes transformations distinct clusters training set patterns 
training mixture model clusters patterns result severely overfitting noise 
fig 
shows principal components eigenimages training data :10.1.1.12.7580
difficult imagine components reconstruct data accurately 
clustering faces facial poses 
fig 
shows examples training set jerky images people walking cluttered background 
trained tmg clusters horizontal shifts vertical shifts iterations em initializing weights small random values 
loop rich matlab script executed minutes mhz pentium processor 
fig 
shows cluster means include sharp representations person face background clutter suppressed 
fig 
shows means mixture gaussians trained iterations em 
fig 
shows examples training set jerky images person different poses 
trained tmg clusters horizontal shifts vertical shifts iterations em 
fig 
shows cluster means capture poses suppress background clutter 
mean cluster includes part background cluster low mixing proportion 
traditional mixture gaussians trained iterations em finds means shown fig 

principal components try account lighting translation shown fig 

learning shape lighting representations noisy unaligned images object 
fig 
shows training set noisy images uniformly colored pyramid gray randomly selected positions illuminated parallel light rays randomly selected angle intensity 
cluttered background simu fig 

frontal face images people 
cluster means learned tmg mixture gaussians 
lated randomly selecting pixel values uniform distribution 
principal components training data scaled standard deviation projected data shown fig 

appears components implement multiresolution approximation model shifts object 
trained tca components transformations implementing horizontal vertical shifts iterations em algorithm 
initialize parameters mean variance pixel computed training data 
parameters initialized random values mean variance st order guide 
transformation probabilities set equal 
fig 
shows mean latent image columns shown images latent image noise shown image pixel intensity equal times standard deviation observed image noise mean clearly shows outline object determined uniform coloring determined point fig 

images person different poses 
cluster means learned tmg 
detailed cluster means learned mixture gaussians 
mean principal components data model lighting translation 
pyramid 
linear combinations components produce different lighting conditions see paragraph implies element rows proportional object surface normals rotation dimensional space 
variance map latent image shows model predicts low variance pixels belonging object high variance pixels background clutter 
variance map observed image accounts small amount noise images 
tca simulated noise free fashion drawing subspace representation computing computing fig 
shows examples simulated way 
show tca simulate different lighting conditions 
learning subspace representation facial expressions imperfectly aligned images 
fig 
shows training set images automatically aligned faces different expressions 
accuracy face detection algorithm align images pixels direction 
fig 

noisy images pyramid different locations different lighting conditions 
scaled principal components 
pixel colored gray halfway black white corresponds component value 
mean components noise deviation tca components iterations em 
pixels mean noise deviations colored scale training images 
examples simulated tca noise transformations 
fig 
shows mean training data principal components scaled standard deviation projected data 
components obviously account vertical horizontal diagonal shifts data re fig 

imperfectly aligned images faces different expressions 
fig 

mean scaled principal components face data 
mean components noise deviations fa tca identity transformation 
mean components noise deviations tca 
components blurred 
fig 
shows parameters fa model tca identity transformation trained iterations em 
parameters initialized mean variance pixel training data 
sum images far right fig 
gives variance map fa 
contrast pca different components represent similar amounts energy variance 
fa find preferred set basis vectors factors subspace 
pca fa finds blurred components 
trained tca components transformations implementing horizontal vertical shifts fig 

examples face data simulated pca subspace factor analyzer 
iterations em algorithm 
parameters initialized fig 

examples face data simulated tca model 
mean variance pixel training data 
transformation probabilities set equal 
fig 
shows mean components variance maps 
pca fa tca extracts clear components 
component appears expose teeth second component appears raise eyebrows raise upper lip expose tongue 
components tca unique unitary transformation component includes feature 
processing step applied find unitary transformation produces components spatially localized energy 
see pca subspace represents data draw subspace point axis aligned gaussian variances determined projected training data principal components map point image space 
examples simulated manner shown fig 

faces appear shifted field vision severely blurred 
fig 
shows examples simulated fa model adding sensor noise 
appear similar examples simulated pca model 
fig 
shows examples images simulated tca latent image observed image noise randomly selected transformations 
images clearer simulated pca subspace factor analyzer 
expressions training set fig 

means left column images sheared translated versions remaining columns training tca models sets handwritten digits containing examples 
top row pictures illustrates test pattern deformed translation shearing transformations 
translation shearing transformations low probability learning 
duced model generates novel realistic expressions training set th column st row st column rd row 
modeling handwritten digits 
performed supervised unsupervised learning experiments greyscale versions digits cedar cdrom hull 
preprocessed images fit window wide variation writing angle vertical stroke different angles 
produced set shearing translation transformations see top row fig 
transformed density models 
supervised learning experiments trained component tca class digit iterations em 
fig 
shows mean components models 
lower rows images fig 
show sheared translated means 
cases transformation probability image 
trained component factor analyzer class digit iterations em 
means components shown fig 

means tca sharper components factor analysis account writing angle see components components tca tend account line thickness arc size 
fig 
shows images randomly generated tca models fig 
shows images randomly generated factor analyzer models 
different components factor analyzers account different stroke angles simulated digits extra stroke digits simulated tcas contain fewer spurious strokes 
test recognition performance trained component factor analyzers tcas examples digit fig 

means left column images components remaining columns model fig 

ing iterations em 
noise variances allowed drop prevent overfitting pixel happens training data 
set models bayes rule classify test patterns 
results summarized table compared standard feedforward method nearest neighbors chosen leave cross validation 
tca lower error rate methods 
probability transformation tca learned believe overfitting 
example sheared image means fig 
faded average responsibilities generalizations 
regularized quite easily blurring fig 

means left column images components remaining columns training factor analysis models sets data train tca models 
factor analysis produces means tca 
fig 

digits randomly generated tca models 
step appropriate topology 
example transformations translation blurring applied transformations translations blurring applied unsupervised learning experiments fit cluster mixture models entire set digits see models identify digits 
tried mixture gaussians mixture factor analyzers cluster tmg 
case models trained iterations em model highest likelihood selected shown fig 

compared tmg methods blurred repeated classes 
identifying clus fig 

digits randomly generated factor analysis models 
table handwritten digit recognition rates training set images test set images 
method error rate nearest neighbors factor analysis transformed component analysis ter prevalent class digit methods error rates tmg lower error rate 
vi 
summary learning applications know data includes transformations easily specified nature shearing images handwritten digits 
density model learned directly data model account transformations data interesting potentially useful structure 
introduce way standard density models clustering linear dimensionality reduction invariant local global transformations input 
result latent variable model containing continuous discrete variables 
discrete variables distribution variables jointly gaussian inference estimation expectation maximization algorithm efficient 
algorithms able jointly normalize input data transformations translation shearing rotation images learn models normalized data 
illustrate algorithms variety difficult tasks 
example transformation invariant mixture gaussians able learn different facial poses set fig 

clustering handwritten digits 
different methods cluster training set cases containing digits classes 
means mixture gaussians 
means mixture factor analyzers 
means transformed mixture gaussians 
case models learned model highest likelihood training set selected 
door images showing person walking cluttered background varying lighting conditions 
matlab scripts transformation invariant clustering dimensionality reduction available webpage www cs toronto edu frey 
focus translational transformations types transformation rotation scale plane rotation warping images 
domains may quite different types transformation 
case time series data transformations neighboring time steps influence transformations current time step 
show techniques extended time series 
number computations needed exact inference scales exponentially dimensionality transformation manifold 
transformations type transformations second type exact inference learning takes time scales currently exploring performance faster variational inference learning method takes time scales exponential speedup achieved inferring different types transformation index separately 
inference particular index coupled inference indices variational parameters represent average influences indices 
believe approach prove useful applications require transformation invariant clustering dimensionality reduction 
golem cohen scanning electron microscope image enhancement technical report computer electrical engineering project report ben gurion university 
hull database handwritten text recognition research ieee transactions pattern analysis machine intelligence vol 
pp 

frey jojic estimating mixture models images inferring spatial transformations em algorithm proceedings ieee conference computer vision pattern recognition june pp 

frey jojic transformed component analysis joint estimation spatial transformations image components proceedings ieee international conference computer vision september 
jepson black mixture models optical flow computation proceedings ieee conference computer vision pattern recognition june pp 

wang adelson representing moving images layers ieee transactions image processing special issue image sequence compression vol 
pp 
september 
weiss smoothness layers motion segmentation nonparametric mixture estimation proceedings ieee conference computer vision pattern recognition june pp 

jojic petrovic frey huang transformed hidden markov models estimating mixture models images inferring spatial transformations video sequences proceedings ieee conference computer vision pattern recognition june 
frey jojic transformation invariant filtering expectation maximization proceedings ieee symposium adaptive systems signal processing communication control 
jojic frey video summarization filtering transformation invariant hidden markov models submitted international journal computer vision 
breiman friedman olshen stone classification regression trees wadsworth ca 
rumelhart hinton williams learning representations back propagating errors nature vol 
pp 


sung poggio example learning view human face detection mit ai memo cbcl 
rowley baluja kanade rotation invariant neural network face detection proceedings ieee conference computer vision pattern recognition june pp 

neal regression classification gaussian process priors bayesian statistics bernardo ed pp 

oxford university press 
vapnik statistical learning theory john wiley new york ny 
turk pentland eigenfaces recognition journal cognitive neuroscience vol 

moghaddam pentland probabilistic visual learning object recognition ieee transactions pattern analysis machine intelligence vol 
pp 
july 
bishop neural networks pattern recognition oxford university press new york ny 
rubin thayer em algorithms ml factor analysis psychometrika vol 
pp 

simard le cun denker efficient pattern recognition new transformation distance advances neural information processing systems hanson cowan giles eds 
morgan kaufmann san mateo ca 
simard le cun denker tangent prop formalism specifying selected invariances adaptive network advances neural information processing systems 
morgan kaufmann san mateo ca 
hinton dayan revow modeling manifolds images handwritten digits ieee transactions pattern analysis machine intelligence vol 
pp 

vasconcelos lippman multiresolution tangent distance affine invariant classification advances neural information processing systems jordan kearns solla eds 
mit press cambridge ma 
le cun bottou bengio haffner gradient learning applied document recognition proceedings ieee vol 
pp 
november 
ghahramani hinton em algorithm mixtures factor analyzers university toronto technical report crg tr 
frey jojic estimating mixture models images inferring spatial transformations em algorithm proceedings ieee conference computer vision pattern recognition june pp 

jojic frey topographic transformation discrete latent variable advances neural information processing systems solla leen 
muller eds 
mit press cambridge ma 
frey graphical models machine learning digital communication mit press cambridge ma see www cs utoronto ca frey 
