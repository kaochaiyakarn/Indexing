named entity extraction speech francis kubala richard schwartz rebecca stone ralph weischedel bbn technologies fawcett street cambridge ma report results hidden markov model extract information broadcast news 
identifinder tm trained broadcast news corpus tested hub development test data hub evaluation test data respect named entity ne task extracting names locations persons organizations dates times monetary amounts percentages 
evaluation automatic word alignment speech recognition output nist algorithm followed muc muc scorer ne text muc scoring assumes identical text system output answer key 
additionally experimental mitre scoring metric burger 
encouraging result language independent trainable information extraction algorithm degraded speech input word error rate recognizer 

motivating factors reasons effort go speech transcription dictation problem address shallow understanding speech 
result effort believe evaluating named entity ne extraction speech offers measure complementary word error rate wer represents measure understanding 
scores ne speech track quality speech recognition proportionally ne performance degrades worst linearly word error rate 
second motivation fact ne information extraction task text showing success error rates newswire 
named entity problem generated interest evidenced inclusion understanding task evaluated sixth seventh message understanding conferences muc muc second multilingual entity task evaluations met met planned track broadcast news evaluation 
furthermore commercial product emerged tm 
ne defined set annotation guidelines evaluation metric example data chinchor 

named entity problem speech named entity task identify named locations named persons named organizations dates times monetary amounts percentages 
sounds clear special cases arise require lengthy guidelines wall street journal artifact organization 
white house organization location 
branch offices bank organization 
street name location 
yesterday tuesday labeled dates 
mid morning time 
human annotator consistency guidelines numerous special cases defined seventh message understanding conference muc chinchor 
training data boundaries expression type marked sgml 
various guis support manual preparation training data answers 
problem relatively easy mixed case english prose solvable solely recognizing capitalization english 
capitalization indicate proper nouns english type entity person organization location identified 
proper noun categories marked product names book titles 
named entity recognition challenge case signal proper nouns chinese japanese german non text modalities speech 
task generalized languages multi lingual entity task met task definition longer dependent mixed case english 
broadcast news presents significant challenges illustrated table 
having mixed case removes information useful recognizing names english 
automatically transcribed speech recognition errors harder due lack punctuation spelling numbers words upper case snor speech normalized orthographic representation format 

overview hmm identifinder tm full description hmm named entity extraction appears bikel 
definition task single label assigned word context 
word hmm assign desired classes person organization label aname represent desired classes 
organize states regions region desired class plus name 
see 
hmm model desired class text 
implementation confined classes ne fact determines set classes sgml labels training data 
additionally special states start sentence sentence states 
regions statistical bigram language model emit exactly word entering state 
number states regions equal vocabulary size generation words name classes proceeds steps 
select name class nc conditioning previous name class previous word 

generate word inside name class conditioning current previous 

generate subsequent words inside current name class subsequent word conditioned immediate predecessor 

sentence go 
viterbi algorithm search entire space possible name class assignments maximizing pr nc 
model allows type name language separate bigram probabilities generating words 
reflects intuition generally predictive internal evidence regarding class desired entity 
consider evidence organization names tend stereotypical airlines utilities law firms insurance companies corporations government organizations 
organizations tend select names suggest purpose type organization 
person names person names stereotypical cultures chinese family names stereotypical 
chinese japanese special characters foreign names 
monetary amounts typically include unit term taiwan dollars yen german marks local evidence suggests boundaries class desired expressions 
titles signal beginnings person names 
closed class words determiners pronouns prepositions signal boundary 
corporate designators name 
number word states name class equal interior bigram language model ergodic mixed case crash second months 
dec american airlines jet crashed mountains near cali colombia killing th people board 
cause crash investigation 
upper case crash second months 
dec american airlines jet crashed mountains near cali colombia killing th people board 
cause crash investigation 
snor crash second months december american airlines jet crashed mountains near cali colombia killing people board cause crash investigation table illustration difficulties speech recognition output snor 
person organization name name classes start sentence sentence pictorial representation conceptual model 
probability associated transitions 
parameterized trained model transition observed model backs powerful model 

evaluation measures information extraction text measured terms precision recall terms borrowed information retrieval community number correct responses number hypothesized responses number correct responses number tags measure uniformly weighted harmonic mean precision recall rp muc met correct response label boundaries correct 
response half correct type correct response string overlaps string 
alternatively response half correct class type type boundaries correct 
type classes defined follows entity enamex person organization location time expression timex date time numeric expression numex money percent 
scoring ne speech merely matter applying muc scoring algorithm assumes source text answer key system output identical 
needs allow insertion deletion substitution errors speech recognizer compare answer 
developed procedure scoring ne speech 
shown aligns speech recognizer output hyp text ref merges ne annotation system aligned hyp merges ne answer key aligned ref 
word alignment software nist 
aligned ne annotated hyp ref scored muc scorer 
alternative burger palmer hirschman mitre development 
virtues 
flexible allowing alternative alignment strategies just word alignment 
second forgiving speech errors 
computes separate measure content variance answer ref versus smith hypothesis hyp 
allows specify weights different kinds errors replicate current muc metrics appropriate weights 
aligned ref annotation merger annotator align identifinder merger muc scorer transcript speech output aligned ref score form aligned hyp aligned hyp annotation ref hyp evaluation nist alignment software official muc scoring software 

results analysis results reported muc scoring metrics software chinchor ne performance speech compared results published text 
annotation annotation involved individuals independently marking ldc data snor format 
scored typically yielding agreement annotators 
differences third person 
look inconsistencies trained identifinder new data deliberately tested training 
scoring identifinder keys typically resulted scores 
discrepancies reviewed resulting gold standard 
annotated words training 
including double annotation test training management took roughly person days 
annotators college students happened graduate student 
ran experiment see having independent annotators important 
demonstrated highly practiced annotator scores affected 
highly practiced annotator followed test training followed just process required person days estimates 
effect training set size previously observed performance text input remarkably little words training data bikel performance continued improving additional training 
assess effect training set size broadcast news trained identifinder successively larger training sets testing time held snor transcripts linguistic data consortium ldc 
results runs appear table 
performance little words tags ne data surprisingly 
performance increases training increases gain doubling training decrease exponentially 
worth noting density ne exemplars broadcast news half newswire 
training text marked adequate number examples named entities half see 
look impact training way counted errors frequency exact ne string appearing training training consisted words containing tags ne strings 
see table 
expected error rate ne strings seen training highest 
furthermore error rate previously unseen ne strings errors 
surprising seeing ne string training dramatically reduces error error rate fairly constant strings observed times training 
suggests points opportunity improve improving performance sequences seen training 
reduction error rate additional training largely coming reducing fraction sequences seen training 
performance broadcast news versus newswire section compare performance broadcast news domain versus newswire 
comparison new york times news service nyt seventh message understanding conference muc wall street journal wsj muc hub data distributed ldc 
new york times text baseline upper case snor input conditions score performance variation nyt mixed case text versus upper case versus snor formatted input 
figures measure effect having case snor input new york times newswire nyt wall street journal wsj respectively 
baseline mixed case prose 
test materials 
words 
tags measure table effect increased training performance 
examples errors total error rate table error rate function frequency occurrence training 
uppercase see performance degrades somewhat upper case version 
degradation wsj nyt part believe due style conventions wsj 
example mention person usually full name john doe subsequent mentions individual tend include title doe 
test materials snor format training snor text performance upper case snor points score nyt wsj 
wall street journal text baseline upper case snor input conditions score performance variation wsj mixed case text versus upper case versus snor formatted input 
differences noted broadcast news ne exemplars half dense newswire 
snor format case punctuation numbers written digits 
obvious differences 
nyt consistently formatted written 
broadcast news hand contains carefully composed sections read news anchor spontaneous speech interviews dialogs 
result formal sections include disfluencies 
table contrasts performance nyt test data performance broadcast news snor format 
case identifinder trained appropriate source nyt training nyt test broadcast news training broadcast news test 
factors cited regarding differences nyt broadcast news suggest performance broadcast news worse newswire 
pleasantly surprising little performance degrades 
snor documents sure degradation due content versus due snor format 
roughly points lost due snor see 
speculate points due broad domain broadcast news nyt test muc specific domain air disasters news due segments spontaneous speech carefully edited prose 
effect recognition errors table show effect speech recognition errors 
preliminary experiment spring hub development test noticed drop performance word error rate speech recognizer 
degradation expect 
instance average length names data words 
degradation score word error rate predict single words 
performance terms score sensitive speech recognition performance linear respect anticipated improvements word error rate 
analysis errors speech recognition input showed dominant error missing names second prominent error spurious names 
source text type comments new york times normal consistently formatted new york times snor case punctuation broadcast news snor spoken domain formal snor manual transcription table effect broadcast news domain snor contrasted newswire 
source wer snor ref hyp snor ref hyp table degradation performance due speech recognition errors 
reran experiments hub data 
identifinder improved year baseline snor ref data contrasted lower score year ago 
degradation speech recognition errors word error rate predict 
average length name strings 
new errors occurred speech input error free transcripts due missing names recognition 
cases ne performance degraded roughly proportional speech recognition errors 

related variant automatically learning brill rules applied ne problem outlined aberdeen 
performance far reported mid muc wall street journal data contrasted identifinder performance mid 
burger report results applying brill rules speech 
bennett reports binary decision trees la ne task 
decision tree decides insert category mark category mark point sequence input words 
scores far contrasted identifinder test material 
technique far applied newswire text 
alternative learning approaches statistics decisions 
identifinder complete probabilistic model governs decisions models categories interest residual input interest 

drawn 
broadcast news represents difficult domain newswire 

degradation performance mixed case snor relatively small 

degradation speech recognition errors directly track word error rate 

evaluations speech named entity extraction appear interesting complementary word error rate 
acknowledgments reported supported part defense advanced research projects agency 
technical agents part fort contract number dabt 
views contained document authors interpreted necessarily representing official policies expressed implied defense advanced research projects agency united states government 
appreciate assistance hirschman david palmer mitre scoring software nancy chinchor aaron saic muc scoring software ann albrecht dan bikel michael crystal garcia scott miller bbn 

aberdeen burger day hirschman robinson vilain 
mitre description alembic system muc 
proceedings sixth message understanding conference muc morgan kaufmann publishers columbia maryland pp 


bennett aone lovell 
learning tag multilingual texts observation 
proceedings second conference empirical methods natural language processing providence rhode island pp 


bikel miller schwartz weischedel 
nymble high performance learning name finder 
proceedings fifth conference applied natural language processing association computational linguistics pp 


burger palmer hirschman 
named entity scoring speech input appear 

chinchor nancy muc named entity task definition dry run version version september available ftp telnet online muc saic com file ne training guidelines ne task def ps 
