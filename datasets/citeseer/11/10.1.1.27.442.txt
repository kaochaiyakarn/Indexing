hybrid svm hmm architectures speech recognition institute signal information processing department electrical computer engineering mississippi state university mississippi state ms edu describe powerful machine learning scheme support vector machines svm framework hidden markov model hmm speech recognition 
hybrid svm hmm system developed public domain toolkit 
hybrid system evaluated ogi corpus performs wer compared triphone mixture gaussian hmm system fifth training data triphone system 
important issues arise nature svm classifiers addressed 
process migrating technology large vocabulary recognition tasks switchboard 

speech recogn pa ern recognition problem desire unique sound ngu om sounds traditionally statistical models gaussian mixture models represent various modalities speech sound 
parameters gaussians estimated maximum likelihood ml criterion 
ml formulation representation acoustic space necessarily translate better recognition performance optimization effort spent learning intricacies training distributions 
extensions hmm learning paradigm involving discriminative training techniques maximum mu ua ma mm mum classification error mce attempt estimate pa ame ng bo pos hey con en improvements recognition performance techniques computationally expensive limited small vocabulary tasks 

support vector machines classifiers typically optimized form risk minimization 
empirical risk minimization commonly technique goal find parameter setting minimizes risk set adjustable parameters expected output input respectively 
minimizing necessarily imply best classifier possible 
example shows class problem corresponding decision regions form hyperplanes 
hyperplanes achieve perfect classification zero empirical risk 
optimal hyperplane maximizes distance margins offering better generalization 
form learning example structural risk minimization srm aim learn classifier minimizes bound expected risk empirical risk 
svm learning srm principle 
emp remp origin class class class hyperplane classifier example power svms lies ability transform data high dimensional space data separated linear hyperplane 
optimization process svm learning begins definition functional needs optimized terms parameters hyperplane 
functional defined guarantees classification perfect classification training data maximizes margin distance 
points lie hyperplane satisfy normal hyperplane bias hyperplane origin 
training examples represented tuples class labels 
satisfy constraints distance margins shown 
goal optimization process maximize margin 
posing quadratic optimization problem advantages functional compactly written lagrange multipliers 
observed previously training examples impact functional optimal decision surface 
translates fact optimization process small percent training examples non zero multipliers 
examples called support vectors 
note assumed data perfectly separable 
case real data 
problem handled introducing slack variables equation 
note number training errors characterized address need learning classifiers define non linear decision regions 

notice linearity svm design manifested dot products 
suppose transform data higher dimension space data linearly separable 
theory developed far holds case 
envision replacing formulation high dimensional space 
theory kernel functions avoid dealing directly high dimensional space excessive computations result transformations 
commonly kernels include polynomial rbf 
final classifier takes form number support vectors 
class sample belongs decided sign 
hybrid asr system significant drawback svms inherently static classifiers implicitly model temporal evolution data 
hmms advantage able handle dynamic data certain assumptions stationarity independence 
advantage relative strengths classification paradigms developed hybrid svm hmm system public domain speech recognition toolkit 
toolkit includes cepstral front viterbi decoder capable generating rescoring word graphs baum welch training module 
system provided components hmm portion hybrid system architecture 
estimating svms publicly available toolkit svmlight 
important issue addressed hybrid system fact svms output distance measure viterbi decoding algorithm typically uses likelihoods posterior probabilities 
estimate warping function maps svm distances posterior probabilities 
ways 
way exp estimate class conditional densities histogram svm distances positive negative examples 
posterior estimated bayes rule 
simpler approach estimating posterior assume posterior takes form sigmoid directly estimate sigmoid 
order avoid severe bias distances training data free parameters estimated cross validation set 
posteriors replace gaussians hmm system svm classifiers 

experimental setup shows hybrid architecture recognition experiments 
svm classifiers hmm system attempt train classifiers frame level data classifiers state hmm 
classifier trained vs classifier amount training data significant 
avoid burdening quadratic optimizer chose segment level data initial experiments 
segment level data means hmms simple state hmms train classifiers multi state hmms 
hmm system generate alignments phone level phone instance treated segment 
segment span variable duration need form sampling arrive fixed length vector classification 
methods attempted regard fixed variable sampling af exp techniques 
approach divide segment regions set ratio construct composite vector mean vectors regions 
experiments chose follow empirical evidence divide frames segment regions proportion 
shows example constructing composite vector phone segment 
svm classifiers hybrid system operate composite vectors 
decode time get segmentation information baseline hmm system cross word triphone system gaussian mixtures state 
composite vectors generated segments posterior probabilities hypothesized find best word sequence viterbi decoder 
better methodology follow generate segmentations hypothesis best list reorder list likelihoods generated svms 

results hybrid architecture benchmarked ogi corpus vocabulary words 
phones represent pronunciations words trained svm classifiers 
baseline hmm system trained dimensional feature vectors comprised cepstral coefficients energy delta acceleration coefficients 
training set sentences averaging words sentence 
svm classifiers trained composite feature vectors generated training sentences 
hybrid system architecture hmm convert segmental data segment information mel cepstral data hybrid decoder hypothesis recognition example composite vector construction proportion hh aw aa uw frames region region region mean region mean region mean region frames frames frames test set open loop speaker independent set sentences 
composite vectors normalized range avoid convergence problems quadratic optimizer 
table shows performance hybrid system various configurations 
system performs better baseline cross word triphone hmm system gaussian mixture components state gives wer dataset 
best performance achieved ratio segments composite feature vector agreement notion information state hmm provided central state 
results note rbf kernel typically better classification polynomial kernels owing ability model decision regions class encloses 
terms resource usage svm systems unique support vectors 
order magnitude number free parameters cross word triphone hmm system 

summary developed paradigm integrating svms hmm framework 
goal augment hmms powerful classifiers svms trained discriminatively 
results ogi data show hybrid system gives significant improvement relative baseline hmm system fifth training data 
expect extending approach process best lists give gains especially large vocabulary tasks switchboard 
process developing method convert variable length feature vectors fixed length vector sufficient statistics generated baum welch algorithm 

dempster maximum likelihood estimation incomplete data royal statistical society vol 
pp 

valtchev discriminative methods speech recognition ph 
thesis university cambridge uk 
continuous speech recognition hidden markov models ieee assp magazine vol 
pp 
july 
vapnik nature statistical learning theory springer verlag new york ny usa 
advances kernel methods support vector machines mit press cambridge ma usa december 
joachims svmlight support vector machine www ai informatik uni dortmund 
de forschung verfahren svm light svm light eng html university dortmund november 
support vector machines speech recognition proc 
icslp pp 
sydney australia november 
cole corpus www cse ogi edu corpora oregon graduate institute 
deshmukh hierarchical search large vocabulary conversational speech recognition ieee signal processing magazine vol 
pp 
september 
platt probabilistic outputs support vector machines comparisons regularized likelihood methods advances large margin classifiers mit press cambridge ma usa 
austin speech recognition segmental neural nets proc 
icassp pp 

ostendorf roukos stochastic model phoneme continuous speech recognition ieee trans 
assp vol 
pp 
december 
segment ratio polynomial kernel rbf kernel order order table performance hybrid system ogi numbers show percent word error rate 
