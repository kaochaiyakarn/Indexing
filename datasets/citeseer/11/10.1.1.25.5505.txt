perceptual interfaces information interaction joint processing audio visual information human computer interaction neti iyengar potamianos senior ibm thomas watson research center yorktown heights ny exploiting human perceptual principle sensory integration joint audio visual information improve recognition human activity speech recognition speech event detection speaker change intent intent speak human identity speaker recognition particularly presence acoustic degradation due noise channel 
experimental results variety contexts demonstrate benefit joint audio visual processing 

humans variety modes information audio visual touch smell recognize people understand activity speech emotion 
lab begun investigating methods combine audio visual information improve robustness naturalness human computer interfaces 
particular short term focus exploit visual information improve audio technologies speech recognition speaker recognition speech event detection speaker change detection presence acoustic mismatch due noise channel 
long term objective build perceptual interfaces information interaction multiple sensory information sources initially acoustic visual active acquisition recognition interpretation human activity intent perceptual computing 
applications include accurate audio transcription efficient search retrieval multimedia content improved human computer interfaces multiple modes robust recognition human activity speech gesture realistic environments automobiles public information kiosks background noise serious problem recognition technologies acoustics 
illustrate schematically steps involved audio visual fusion 
single percept identity person speech activity captured audio visual sensors 
streams sampled different rates 
sampled signals represented feature spaces followed feature fusion visual audio features concatenated form single feature set interpolating slower stream equalize rates single classifier combining independent decisions streams decision fusion multiple classifiers 
note case feature representation region interest percept considered extracted 
speech speaker recognition entails detecting face scene followed extracting region regions interest 
section describe methodology face detection followed experimental results demonstrate benefit joint audio visual processing speech recognition speaker recognition speaker change detection speech event detection 
section describe extraction appropriate regions interest representations 

face detection facial feature extraction face detection facial feature localization method described 
video frame face detection performed employing combination methods subsequent face feature finding 
face template size chosen pixels image pyramid permissible scales frame size face template search image space possible face candidates 
video signal color skin tone segmentation narrow search candidates contain significantly high proportion skin tone pixels 
remaining face candidate score fisher linear discriminant distance face space dffs 
candidate regions exceeding threshold score considered faces 
face ensemble facial feature detectors extract verify locations facial features including lip corners centers 
search features occurs hierarchically 
high level features located subsequently low level features located relative high level feature locations 
feature locations stages determined score combination prior statistics linear discriminant dffs 

speech recognition known humans fuse information audio visual stimuli recognize speech visual modality contains complementary information audio modality 
focus interest demonstrating meaningful improvements realistic tasks broadcast news transcription audio video indexing large vocabulary dictation hearing speech impaired 
source audio sensor video sensor audio noise sampling video noise feature transform feature transform confidence direct sum confidence collecting multi subject continuous large vocabulary audio visual database tm training utterance scripts data 
currently consists subjects close hours speech utterances 
database contains full frontal face color video subjects minor face camera distance lighting variations 
video captured resolution pixels interlaced frame rate hz mpeg encoded mbytes sec 
audio captured sampling rate khz video stream 
experiments selected database utterances subjects randomly split training test utterances subject creating multi subject utterance training set hours utterance test set hour 
process video data extract mouth interest roi 
statistical face detection feature localization templates trained video frames database subjects marked facial feature locations 
face detection performance tested database video sequences containing approximately frames correct assuming face video frame 
face detection mouth feature localization results roi estimated visual features 
summarize results word error rate wer large vocabulary continuous speech recognition lvcsr preliminary experiments database 
see details 
consider hmm lvcsr system vocabulary words tri gram language model 
visual speech represented cascade transform representation mouth region comprises image transform followed projection discriminant feature space 
standard cepstral features represent audio stream 
obtain audio wer visual wer audio visual wer 
feature fusion known fail improve asr performance small vocabulary tasks results lvcsr experiment surprising 
significant asr improvement achieved audio corrupted noise 
example audio corrupted babble noise signal noise ratio db matched noisy audio wer improves fv classifier classifier classifier similarity metric similarity metric audio visual information fusion decision engine incorporating visual information 
currently implementing decision fusion feature fusion audio visual lvcsr fusion strategy means multi stream hmm 
addition investigating various stream confidence estimation techniques 
believe significant lvcsr wer reduction achieved techniques clean audio case 

speaker recognition combine image video visual signatures face identification audio feature speaker identification improved person authentication see details 

image speaker identification facial features located 
include large scale features small scale sub features 
prior statistics restrict search area feature sub feature 
estimated sub feature locations gabor jet representation generated 
gabor jet set dimensional gabor filters sine wave modulated gaussian 
filter scale orientation 
scales orientations giving complex coefficients feature location 
simple distance metric compute distance feature vectors trained faces test candidates 
distance th trained candidate test candidate feature defined sik average similarities pj ak fvi kx sik gives measure similarity test face face template database 

audio speaker identification frame approach audio speaker identification described follows 
mi model corresponding th enrolled speaker represented mixture gaussian model defined parameter set pi pi consisting mean vectors covariance matrices mixture weight vectors pi 
goal speaker identification find model mi explains test data represented sequence frames total distance fai model mi test data taken sum distances di log pi pi test frames measured likelihood criterion 
fusion fai nx di audio speaker recognition face recognition scores audio visual speaker identification carried follows top scores generated audio video identification schemes 
lists combined weighted sum 
subsequently best scoring candidate chosen 
define combined score ca cv function single parameter ca cos cv sin angle selected relative reliability audio face identification 
may optimize gain maximum accuracy training data 
elaborate denote fai fvi respective scores ith enrolled speaker computed nth training clip 
define variable ti zero nth clip belongs ith speaker equal unity 
minimize number empirical errors nx arg max joint score fai fai fvi fvi 
experiments carried cnn video data collected part arpa hub broadcast news transcription task linguistic data consortium ldc 
digitized second clips anchors reporters frontal shots faces video tapes mpeg format 
training data contained clips speakers test data consisted additional clips speakers results combining audio visual information speaker recognition linear fusion methods shown table 
see details 

speaker change detection detection valuable piece information speaker identification metadata search retrieval acoustic condition clean noisy telephone audio id video id linear fusion table audio visual speaker id multimedia content 
currently exploring visual speaker scene change information remove audio detection see details 
hypothesis performance audio video techniques improved exploiting joint statistics audio stream associated video 
significant correlation audio video speaker changes newscast scenario example 
frequently video scene change follows shortly audio change 

audio speaker change problem detecting transition point time choose models data data set modeled single gaussian process xa distinct gaussian processes xa 
obvious notation mean vector covariance matrix 
bic model selection procedure considers difference bic values associated models classifier maximum likelihood ratio statistics log penalty dimension vectors 
consider transition point 

video speaker change video scene change detection statistical model criterion bic criterion describe alternate procedure 
consider dimensional color histogram generated video feature vectors xvi experiments consider liebler type divergence criterion gv nx vi log xk xk vi adjoining vectors vi superscript denotes kth component vectors 
compute average gv gv fixed number samples past consider transition point threshold gv 
fusion fusion problem intelligently combine probabilities 
probability fv pr video feature vectors past positive 
probability fa pr computed audio data positive 
fusion strategy devise adequate fusion map fc cv particular case consideration fusion strategy solve optimization problem fca cv arg max parameter accounts fact speaker change audio signal precedes speaker change video signal 
minutes television panel discussion analyzed audio speaker changes immediately followed seconds corresponding video change 
initial results video content suggest fusion helps improve precision recall rate 
example recall rate audio precision audiovisual precision 

speech event detection speech recognition systems opened way intuitive natural human computer interaction hci 
current hci systems speech recognition require human explicitly indicate intent speak turning microphone keyboard mouse 
key aspects naturalness speech communication involves ability humans detect intent speak 
experiments refer 
humans detect intent speak combination visual auditory cues 
visual cues include physical proximity pose lip movement automatic detection speech onset carried silence speech detection audio energy 
method combining methods may compute probability densities fa pr fv pr say mixtures gaussian pdfs 
simple fusion strategy linear combination 
experiments fc cv subset database described speech recognition section containing hrs training data comprising speakers able improve speech event detection speech silence classification audio db snr speech noise combining visual information 
addition built practical system uses visual cues proximity head pose visual mouth activity intuitively manipulate microphone state speech interaction 

fusion multiple sources information mechanism robustly recognize human activity intent context human computer interaction 
results suggest meaningful improvements obtained fusion audio visual information speech recognition speaker recognition speaker change detection speech event detection 


macdonald hearing lips seeing voices nature vol 
pp 


massaro stork speech recognition sensory integration american scientist vol 
pp 


basu neti senior subramaniam verma audio visual large vocabulary continuous speech recognition broadcast domain 
ieee workshop 

verma senior neti basu late integration audio visual continuous speech recognition 
automatic speech 
understanding workshop 

neti senior audio visual speaker recognition video broadcast news fusion techniques 
ieee workshop 

iyengar neti speaker change detection joint audio visual statistics 
riao paris france april 

neti senior audio visual intent detection human computer interaction 
ieee int 
conf 
acoustics speech signal processing 

potamianos graff discriminative training hmm stream exponents audio visual speech recognition 
proc 
icassp pp 

potamianos verma neti iyengar basu cascade image transform speaker independent automatic appear proc 
int 
conf 
multimedia expo new york 

potamianos neti stream confidence estimation audio visual speech recognition 
proc 
icslp beijing 
appear 

senior recognizing faces broadcast video 
ieee international workshop recognition analysis tracking faces gestures real time systems 
