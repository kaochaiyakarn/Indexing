learning making decisions costs probabilities unknown zadrozny charles elkan cs ucsd edu department computer science engineering university california san diego la jolla california technical report 
cs january machine learning domains misclassi cation costs di erent di erent examples way class membership probabilities 
domains costs probabilities unknown test examples cost estimators probability estimators learned 
rst discusses optimal decisions cost probability estimates presents decision tree learning methods obtaining calibrated probability estimates 
explains obtain unbiased estimators example dependent costs account diculty general probabilities costs independent random variables training examples costs known representative examples 
problem called sample selection bias econometrics 
solution nobel prize winning due economist james heckman 
show methods propose successful comprehensive comparison metacost uses known dicult dataset kdd data mining contest 
design supervised learning algorithms assumption errors incorrect predictions equally costly 
assumption true application areas 
example marketing cost making er person respond small compared cost contacting person respond 
medicine cost prescribing drug patient higher cost prescribing drug patient alternative treatments available 
information retrieval cost displaying relevant document may lower higher cost displaying irrelevant document 
animals failing recognize predator far costly non predator 
domains cost sensitive learning decision making needed including cases example falls alternative classes 
class rare example class patients cost recognizing example belongs class high 
domains learning methods fail take costs account perform 
extreme cases learning method cost sensitive may produce model useless classi es example belonging frequent class 
years realization cost sensitive learning methods required real world applications led substantial amount research 
turney provides bibliography research 
general method cost sensitive learning published far method named metacost due domingos alternative method call direct cost sensitive decision making 
analysis shows new method general metacost originally published experimental results show new method preferable metacost 
organized follows 
section explain metacost direct cost sensitive decision making 
section show apply methods dicult real world dataset kdd data mining contest 
metacost direct cost sensitive decision making require accurate estimates class membership probabilities 
section techniques allow accurate probability estimates obtained decision tree binning smoothing early stopping 
previous research assumption misclassi cation costs examples known advance general costs example dependent unknown way class membership probabilities example speci known advance 
section discuss issue issue sample selection bias ects cost estimation 
section describe heuristic method compensate possible biases estimating probabilities costs 
experimental results kdd dataset section section summarize main contributions 
related discussed necessary 
metacost versus direct cost sensitive decisionmaking domain cost sensitive learning method applied training example test example associated cost predicting class true class costs known straightforward compute optimal policy decision making 
optimal prediction optimal decision concerning label assign class leads lowest expected cost jjx alternative expected cost weighted average weight conditional probability class central idea metacost method change label training example optimal label equation learn classi er predicts new labels 
basic metacost idea implemented ways experimental results investigate di erent implementations total 
implementations di er described domingos important ways 
estimate probabilities bagging breiman pointed bagging gives voting estimates measure uncertainty base classi cation method concerning example actual class conditional probability example 
bagging simpler methods single decision trees 
second original description metacost assumption costs known advance examples dependence provost fawcett pointed assumption true problems di erent errors type di erent costs 
generalize metacost relaxing assumption compare variants metacost di erent methods estimating costs 
metacost uses equation requires knowledge conditional probability jjx training example possible true class probabilities part training data 
training data learn classi er estimates jjx training example classi er provide conditional probability estimates training examples provide conditional probability estimates test examples 
probability estimates directly compute optimal label test example equation 
process method call direct cost sensitive decision making 
testbed kdd charitable donations dataset dataset experimental described studied dicult dataset rst data mining contest associated kdd conference 
dataset associated documentation available uci kdd repository bay 
dataset contains information persons donations past certain charity 
decision making task choose donors request new donation 
task completely analogous typical marketing tasks organizations non pro pro mathematically task structure class cost sensitive learning decision making problems mentioned 
kdd dataset divided xed standard way training set test set 
training set consists records known person donation response person donated donation 
test set consists records similar donation information published kdd competition 
order experimental results directly comparable previous standard training set test set division 
mailing solicitation individual costs charity 
percentage donors potential recipients 
donation amount persons respond varies 
low response rate variation value gifts easy achieve pro higher obtained soliciting potential donors 
pro obtained soliciting individual test set pro attained winner kdd competition 
participants kdd competition submitted entries worse useless achieved pro ts substantially lower 
fact indicates individuals kdd dataset ltered reasonable set targets 
task cost sensitive learning decisionmaking method improve performance unknown method applied create kdd dataset 
research cost sensitive learning traditionally couched terms costs opposed bene ts pro ts 
domains including charitable donations domain easier talk consistently bene ts costs 
reason bene ts straightforward cash ows relative baseline wealth costs counterfactual opportunity costs 
accordingly formulation problem terms bene ts costs 
optimal predicted label example optimal decision solicit class maximizes jjx bene predicting class true class label mean person donate mean person donate 
person donation variable amount say 
cost mailing solicitation bene matrix actual non donor actual donor predict non donor predict donor mail notice example dependent unknown test examples 
shall argue xed matrix costs bene ts lead decisionmaking constant reasonable replace approaches task tasks structure xed cost bene matrix poor performance 
course approaches take account fact example dependent estimating explicitly 
expected bene soliciting person deciding jx jx expected bene soliciting jx jx jx jx jx optimal policy solicit exactly people expected bene mailing greater expected bene mailing individuals jx words optimal policy mail people expected return jx greater cost mailing solicitation jx order apply policy need estimate conditional probability making donation jx donation amount example training set case metacost 
need estimate values training test examples case direct cost sensitive decision making 
kdd dataset concreteness methods described apply cost sensitive learning general 
cost sensitive learning application order equation obtain optimal labeling need estimate conditional class membership probabilities accurately 
costs estimated unknown examples 
general test example unknown training example known pairs unknown pairs 
course costs example dependent examples costs need estimated training test examples 
special case case considered previous general research cost sensitive learning 
remainder discuss methods estimating costs probabilities applied wide variety domains 
estimating class membership probabilities estimate conditional probability membership class required training example metacost test example direct cost sensitive decision making 
decision tree learning method due quinlan pruning disabled obtain scores usefully correlated true class membership probabilities 
kdd domain examples training set training data elds attributes income household income code range date rst gift date gift number gifts number promotions received rfa frequency code range rfa amount gift code range rfa recency frequency amount star status blank 
derived attribute de ned zero records dataset concern people donated past 
research concerned feature selection choice attributes xed informally kdd winning submission georges classifying test examples default assigns raw training frequency score example assigned decision tree leaf contains positive training examples total training examples 
training frequencies accurate conditional probability estimates reasons 
high bias algorithm tries leaves homogeneous observed frequencies systematically shifted zero 
problem noted walker 

high variance number training examples associated leaf small observed frequencies statistically reliable 
pruning methods surveyed esposito principle alleviate problem removing leaves contain examples 
current pruning method suitable unbalanced datasets error rate minimization cost minimization 
kdd dataset method generates pruned tree single leaf 
base rate positive examples error rate single leaf tree tree useless estimating example speci conditional probabilities jx 
general trees pruned useful decision making cost misclassifying rare true positive example higher cost misclassifying common true negative example 
standard pruning method inappropriate tasks 
quinlan latest decision tree learning method cart breiman pruning error minimization 
rule set generators commonly alternative pruning quinlan decision tree methods produce set rules typically simpler accurate original tree 
pruning methods error minimization suitable highly cost sensitive applications 
standard methods pruning restructuring decision trees choose attempt improve accuracy decision tree probability estimates directly 
choice pruning supported results bradford nd performing pruning variants pruning adapted loss minimization lead similar performance 
pruning suggested bauer kohavi section 
improving probability estimates binning binning histogram method simple non parametric approach probability density estimation bishop set examples attribute measured obtain histogram dividing axis number bins 
conditional probability membership class approximated fraction examples bin containing belong raw attribute separate examples bins leaf frequency score example 
test example compute raw score place bin score 
binned conditional probability estimate test example fraction true positive training examples bin 
order obtain binned estimates training data partition training set subsets 
subset learning decision tree subset validation binning process 
subsets strati ed meaning proportion positive examples subset xed identical 
subset training called train contains training examples 
subset val contains remaining 
training examples assigned train learning tree involves making choices setting binned probabilities 
concretely train train apply resulting decision tree example val 
sort examples raw decision tree scores divide equal sized bins 
bin compute unbiased estimate conditional probability example positive decision tree score places bin averaging true labels examples val bin 
val examples sorted score binning 
solid line decision tree score examples val 
vertical dashed lines show separation bins 
stars average probability membership positive class bin 
shows results executing procedure 
solid line output decision tree learned examples val 
vertical dashed lines show scores separated bins 
stars average probability donation bin binned score examples bin 
note considerably overestimates scores examples rightmost bin 
similarly underestimates scores examples leftmost bin 
phenomenon expected tries leaves decision tree homogeneous 
separate validation set necessary averaging labels examples train bin eliminate overestimation underestimation bias 
order obtain conditional probability estimates examples entire dataset map raw score example bin 
mapping obtained determining bin raw score falls upper lower bounds bin 
binned score example average probability donation bin example mapped 
number di erent probability estimates binning yield limited number alternative bins 
number experiments small order reduce variance binned probability estimates increasing number values validation set averaged bin 
binning reduces resolution degree detail conditional probability estimates improving accuracy estimates reducing variance bias 
improving probability estimates smoothing discussed domingos provost way improving probability estimates decision trees estimates smoother adjust extreme 
provost domingos suggest laplace correction method 
class problem method replaces conditional probability estimate number positive training examples associated leaf total number training examples associated leaf 
laplace correction method adjusts probability estimates closer reasonable classes far equiprobable case real world applications 
general consider average probability positive class base rate smoothing probability estimates 
bayesian perspective conditional probability estimate smoothed corresponding unconditional probability 
replace probability estimate 
base rate parameter controls scores shifted base rate 
smoothing method called estimation cussens example leaf contains training examples positive raw decision tree score example assigned leaf 
smoothed score 
smoothed score 
increases observed training set frequencies shifted base rate 
previous papers suggested choosing cross validation 
base rate suggest bm approximately 
heuristic similar rule thumb chi squared goodness test reliable number examples cell contingency table 
shows smoothed scores kdd test set examples sorted raw scores 
expected smoothing shifts scores test examples sorted raw scores raw scores smoothed scores smoothed scores raw scores test examples sorted raw score 
base rate approximately desirable scores tend overestimates underestimates 
raw scores range smoothed scores range 
scores sets examples essentially una ected smoothing 
sets appear horizontal stretches line scores 
correspond leaves decision tree contain number training examples larger improving probability estimates early stopping discussed pruning tends training data create leaves number examples small induce conditional probability estimates statistically reliable 
smoothing attempts correct estimates shifting average probability base rate parent small leaf leaf training examples contains examples induce statistically reliable probability estimate assigning estimate test example associated small leaf may accurate assigning combination base rate observed leaf frequency done smoothing 
parent small leaf contains examples score grandparent leaf root tree reached 
root course observed frequency training set base rate 
method improving conditional probability estimates called early stopping classifying example searching decision tree soon reach node examples parameter method 
score parent node assigned example part decision tree generated classify test example income rfa rfa nodes grey path followed root leaf example classi ed 
question 
smoothing chosen cross validation heuristic making bv 
choose experiments example remainder section uses order smaller decision tree 
shows part decision tree generated entire kdd training set 
pruning tree leaves 
grey nodes path followed root leaf test example classi ed income rfa rfa note leaf contains examples 
early stopping score parent leaf contains examples providing reliable probability estimate 
estimated probability test example changed 
eliminating nodes training examples early stopping ectively creates decision tree shown 
distinction internal nodes leaves blurred tree node may serve internal node examples leaf depending attribute values examples 
early stopping equivalent type pruning pruning eliminates children node simultaneously early stopping may eliminate children keep depending number training examples associated child 
intuitively early stopping preferable pruning probability estimation nodes removed decision tree give unreliable probability estimates 
decision tree obtained early stopping tree 
grafting variant pruning eliminate internal nodes leaves esposito early stopping reminiscent grafting methods fundamental di erence 
grafting replaces subtree branches 
operation completely eliminates switching attribute tested root subtree 
early stopping hand eliminates switching values attribute values training examples available keeps switching values attribute 
words pruning grafting split di erent values attribute 
early stopping allows splitting values attribute 
shows early stopping scores test set examples sorted raw scores 
jagged lines chart show scores changed signi cantly early stopping 
smoothing scores subsets examples appear horizontal lines chart ected early stopping 
subsets correspond leaves decision tree contain number training examples larger range scores reduced smoothing 
minimum early stopping score maximum 
combining smoothing early stopping early stopping ectively eliminates decision tree nodes yield probability estimates statistically reliable small sample training examples 
tries decision tree nodes homogeneous test examples sorted raw scores raw scores early stopping scores early stopping scores raw scores test examples 
examples sorted raw score 
probability estimates training examples tend high low 
explained section smoothing compensate bias shifting estimates average probability 
investigate combination smoothing early stopping 
shows smoothed early stopping scores kdd test set examples sorted raw scores 
comparing chart shows smoothed early stopping scores extreme expected 
range 
estimating donation amounts general cost sensitive learning need estimate example speci misclassi cation costs example speci class conditional probabilities 
need estimate misclassi cation costs training examples metacost test examples direct cost sensitive decision making 
costs probabilities unknown estimating costs important making decisions estimating probabilities 
cost estimates important relative variation costs di erent examples greater relative variation probabilities 
dynamic range costs may greater dynamic range probabilities dynamic range true costs greater estimating costs accurately easier estimating probabilities accurately 
kdd domain example estimating donation probabilities dicult 
best method task early stopping test examples sorted raw scores raw scores smoothed early stopping scores smoothed early stopping scores raw scores test examples 
examples sorted raw score 
smoothing gives conditional probabilities narrow range 
estimating donation amounts easier past amounts excellent predictors amounts 
may appear non donors training set impute donation amount zero actual donation amount zero 
imputation analogous donation probability zero non donors fact donated clearly wrong 
responding solicitation person decisions 
rst donate second donate 
conceptually decisions governed di erent random processes necessarily sequential independent course 
donors training set outcome random process sets donation amount known non donors outcome unknown 
individuals test set outcome random processes unknown 
outcome processes unknown learning task estimate outcome 
non donors training set task estimate amounts donated donations 
compare di erent methods obtaining donation amount estimates 
rst method uses average donation amount known donors individuals actual donation amount unknown 
donors training set actual donation amount known method uses actual amount 
note donation estimate test examples means decision solicit person exclusively probability donate 
method equivalent xed cost matrix test examples 
general misclassi cation costs assumed xed di erent decisions di erent examples di erent conditional probability estimates examples 
second method estimating donation amounts uses squares multiple linear regression mlr 
donors training set donated input regression original attribute derived attributes dollar amount gift number gifts number promotions received average gift amount responses promotions 
mentioned topic variable selection somewhat arbitrarily choose attributes previous 
mentioned de ned 
linear regression equation estimate donation amounts examples training test sets 
donations rare domain donations recorded training set 
eliminate examples regression training set heuristic attempt reduce impact outliers regression 
included examples uence regression equation highest values regression equation chosen minimize sum squared errors 
important estimate values accurately individuals optimal decision solicit predicted donation probabilities 
accurate predicted donation probabilities close zero intrinsic diculty predicting person donate 
problem sample selection bias estimating donation amounts fundamental problem estimator example regression equation learned examples people donate 
estimator applied di erent population donors non donors 
problem known general sample selection bias 
occurs training examples learn model drawn di erent probability distribution examples model applied 
donations domain donation amount probability donation negatively correlated 
people respond solicitation tend smaller donations people larger donations respond 
relationship illustrated 
examples people donate training examples regression donation amounts estimated regression equation tend low test examples low probability donation 
explained previously elkan standard method compensating sample selection bias econometrics step procedure due james heckman university chicago heckman october heckman awarded nobel prize economics developing applying procedure 
expressed notation heckman procedure applicable example belongs classes dependent variable estimated observed training example estimated probability donation actual donation amount training set actual donation amount versus estimated probability donation donors training set 
negative correlation donation amount probability donation visible 

rst step procedure learn probit linear model estimate conditional probabilities jx 
probit model variant logistic regression cumulative gaussian probability density function sigmoid function 
second step heckman procedure estimate linear regression training examples including transformation estimated value jx 
heckman proved procedure yields estimates unbiased regardless certain conditions heckman third method estimating donation amounts nonlinear variant heckman procedure 
linear estimator jx decision tree obtain probability estimates described section 
include probability estimates directly additional attribute applying learning method obtain estimator 
learning method nonlinear method example neural network method order investigate carefully usefulness heckman idea hold constant just provide estimated jx values fourth attribute linear regression second method 
choosing threshold decisions seen section charitable donations domain optimal policy assign predicted label example jx 
estimated amount donation contribute jx probability cost mailing solicitation 
threshold making decisions optimal estimates jx unbiased 
attempt compensate errors estimated donation probabilities amounts replace threshold threshold heuristically equation optimal decision making strategy changed jx threshold determined empirically rst ranking examples training set product jx nding threshold yields maximum pro threshold test examples labeled 
relationship attained pro threshold value illustrated 
possible value threshold chart shows pro obtained sending examples training set jx threshold dependence attained pro training set threshold value 
optimal threshold value marked dashed line 
low values revenue donations high cost mailing high individual solicited 
hand high values people solicited 
case mailing costs lower total pro low donors receive solicitation 
case shown chart threshold optimal training set standard threshold yields total pro changing single number decision making threshold mathematically su cient compensate biases estimating jx estimated product jx monotonically increasing function real product 
perfectly monotonic relationship exactly true 
general adjusting threshold compensate completely errors estimated donation probabilities amounts may useful practice 
chart similar lift curve called gains chart 
major di erence lift curves probabilities jx expected revenue jx 
conventional approach learning decision making learn estimator jx select threshold individual solicited choice threshold heuristically takes account cost matrix compensates fact typically calibrated estimate jx 
major point policy type usually suboptimal 
marketing domain rational solicit person probability responding low expected value response respond high 
conversely irrational solicit response probability high expected value response low 
note costs bene ts di erent di erent individuals rational decisions need unbiased estimates true example speci class probabilities 
numerical score correlated true probabilities calibrated inadequate 
hand costs bene ts individuals possible classes monotonic transformation estimator jx just useful calibrated version estimator changing decision threshold compensate calibration error 
experimental results previous sections discussed alternative methods subproblems estimating example speci class probabilities estimating example speci costs bene ts setting threshold making decisions 
alternative general methods cost sensitive learning metacost direct cost sensitive decision making 
label choice experimental results possible combination alternatives 
combination experimental trial 
trial report number people solicited number donors reached total pro achieved 
give numbers training set test set 
important number course total pro achieved test set 
tables show results trials raw scores binned scores smoothed scores early stopping scores smoothed early stopping scores respectively 
metacost experiments alternative ways measuring performance training set relabeling training examples directly ii applying classi er learned relabeling examples training set obtain new labels 
direct cost sensitive decision making metacost report ii metacost experiments 
results ii better predictors test set results re ect behavior classi er applied test examples 
signi cant di erence results training set test set amount thresh 
method mailed hit pro mailed hit pro average xed metacost average xed direct average adjusted metacost average adjusted direct mlr xed metacost mlr xed direct mlr adjusted metacost mlr adjusted direct heckman xed metacost heckman xed direct heckman adjusted metacost heckman adjusted direct table experimental results raw decision tree scores probability estimates 
method achieving highest pro test set highlighted 
ii indicates able nd decision tree captures relationship attributes result relabeling 
statistical signi cance comparing results di erent trials important evaluate di erences attained pro statistically signi cant 
quantify signi cance roughly simple argument 
donors xed test set 
individuals average donation 
di erent test set drawn randomly probability distribution expect standard deviation uctuation number donors 
uctuation cause change 
total pro pro di erence methods statistically signi cant 
pro di erences methods observe 
avenues follow obtain statistically signi cant di erences methods 
avenue cross validation single training set single test set 
training set test set split standard 
results comparable previous dataset 
avenue multiple datasets comparing di erent methods done example domingos despite importance di erential costs learning tasks kdd dataset dataset uci repositories real world misclassi cation cost information available 
previous experimental research cost sensitive learning arbitrary cost matrices 
prefer real cost data especially interested situation costs di erent di erent examples 
main purpose experiments reported identify single best method cost sensitive learning decision making compare usefulness alternative proposed previous sections 
training set test set amount thresh 
method mailed hit pro mailed hit pro average xed metacost average xed direct average adjusted metacost average adjusted direct mlr xed metacost mlr xed direct mlr adjusted metacost mlr adjusted direct heckman xed metacost heckman xed direct heckman adjusted metacost heckman adjusted direct table experimental results binned decision tree scores probability estimates 
method achieving highest pro test set highlighted 
experiments designed alternative choices tried holding choices xed 
experimental design allows investigate particular alternative example systematically yields higher pro alternatives regardless choices 
respectively choices total choices 
consider example choice 
results pairs trials trial uses metacost trial uses choices uses direct cost sensitive decision making metacost 
null hypothesis methods equally successful expect metacost appear superior 
pairs standard deviation 

approximately pairs independent 
fact pairs test set pro achieved metacost lower 
result highly signi cant statistically magnitude di erence individual trials 
choose quantify level statistical signi cance doing require making assumptions certainly false 
particular trials training test sets pairs trials statistically independent 
comparing methods estimating donation amounts trials average donation xed donation amount estimate results test set bad 
trials yield pro test set signi cantly lower achievable trivially classifying test examples positive 
trials show huge di erence pro training test sets surprising number people solicited number donors reached approximately sets 
consider second line table 
training set average donation people solicited donate test set analogous average training set test set amount thresh 
method mailed hit pro mailed hit pro average xed metacost average xed direct average adjusted metacost average adjusted direct mlr xed metacost mlr xed direct mlr adjusted metacost mlr adjusted direct heckman xed metacost heckman xed direct heckman adjusted metacost heckman adjusted direct table experimental results smoothed decision tree scores probability estimates 
method achieving highest pro test set highlighted 

experiments average training set donation amount xed estimate unknown donation amounts exhibit similar discrepancies 
discrepancies occur true donation values positive examples training set average donation amount examples test set 
training set process leads selection people donate larger amounts 
test set donation estimate examples probability donation jx tends lower large donors product jx tends lower people 
individuals donate smaller amounts selected 
metacost direct cost sensitive decision making subject problem 
results con rm claim wrong impute xed quantity donation estimate test examples 
linear regression mlr applied estimate donation amounts results signi cantly improved direct cost sensitive decision making metacost 
heckman procedure uses probability estimates additional attribute linear regression improves results trial raw heckman adjusted metacost pro reduced 
improvement due heckman procedure systematic average 
average improvement small attributes original linear regression highly correlated probability making donation 
historical probability person responding unsurprisingly highly correlated probability response 
heckman procedure ectively implemented attributes 
fact improvement systematic indicates heckman procedure succeeds correcting sample selection bias 
expect bene cial impact heckman procedure greater ect sample selection bias accounted heuristic way 
training set test set amount thresh 
method mailed hit pro mailed hit pro average xed metacost average xed direct average adjusted metacost average adjusted direct mlr xed metacost mlr xed direct mlr adjusted metacost mlr adjusted direct heckman xed metacost heckman xed direct heckman adjusted metacost heckman adjusted direct table experimental results early stopping scores probability estimates 
method achieving highest pro test set highlighted 
adjusting threshold making decisions trials adjusting threshold making decisions away bene cial 
threshold changed training data serious risk tting data 
results test set signi cantly worse 
conditional probabilities donation amounts estimated unbiased way adjusting threshold unnecessary 
example trials binned probability estimates described table heckman procedure adjusted threshold 
number close optimal threshold probability cost estimates unbiased 
reason results xed adjusted thresholds similar 
conclude adjusting threshold useful probability cost estimates biased particular heckman procedure 
comparing methods estimating probabilities results trials binned scores better trials unmodi ed scores average 
attribute improvement fact binning corrects scores extreme underestimates overestimates 
smoothing early stopping systematically slightly better binning 
compared binning smoothing early stopping improve accuracy probability estimates reducing resolution reducing number distinct probability estimates just number bins 
combining smoothing early stopping yields slightly better results method separately average early stopping smoothing 
training set test set amount thresh 
method mailed hit pro mailed hit pro average xed metacost average xed direct average adjusted metacost average adjusted direct mlr xed metacost mlr xed direct mlr adjusted metacost mlr adjusted direct heckman xed metacost heckman xed direct heckman adjusted metacost heckman adjusted direct table experimental results smoothed early stopping scores probability estimates 
best result test set highlighted 
metacost versus direct cost sensitive decision making metacost performs consistently direct cost sensitive decision making 
best result obtained metacost best result obtained direct method statistically indistinguishable result obtained winner kdd contest 
conclude direct cost sensitive decision making preferable metacost 
attribute worse performance metacost diculty single model estimating costs probabilities accurately separate models 
learning single classi er relabeled training data causes errors approximating ideal decision boundary learning estimators 
main contributions explain general method cost sensitive learning performs systematically better metacost experiments 
provide solution fundamental problem costs di erent di erent examples unknown general 
solution includes solution problem sample selection bias fact training set available learning estimate costs representative test examples training examples 
methods propose evaluated carefully experiments large dicult highly cost sensitive real world dataset small datasets arbitrary cost data previous research 
simple methods probability estimation cost estimation order illustrate general cost sensitive learning approach provide baseline research 
sophisticated regression method estimating donation amounts preliminary results better winners kdd kdd contests 
experiments designed metacost alternative propose methods estimating costs probabilities 
expect direct cost sensitive decision making preferable remain valid estimation methods 
particular metacost direct cost sensitive decision making improved improvement techniques probability estimation 
example shows bagging useful probability estimation metacost method bene bauer kohavi eric bauer ron kohavi 
empirical comparison voting classi cation algorithms bagging boosting variants 
machine learning 
bay bay 
uci kdd archive 
bishop bishop 
neural networks pattern recognition chapter 
clarendon press oxford uk 
bradford bradford kunz kohavi brunk brodley 
pruning decision trees misclassi cation costs 
proceedings european conference machine learning pages 
breiman breiman friedman olsen stone 
classi cation regression trees 
wadsworth international group 
breiman breiman 
bagging predictors 
machine learning 
cussens james cussens 
bayes pseudo bayes estimates conditional probabilities reliability 
proceedings european conference machine learning pages 
springer verlag 
domingos provost domingos provost 
trained pets improving probability estimation trees 
working stern school business new york university ny ny 
domingos pedro domingos 
metacost general method making classi ers cost sensitive 
proceedings fifth international conference knowledge discovery data mining pages 
acm press 
elkan charles elkan 
cost sensitive learning decision making costs unknown 
workshop notes workshop cost sensitive learning seventeenth international conference machine learning 
esposito esposito malerba semeraro 
comparative analysis methods pruning decision trees 
ieee transactions pattern analysis machine intelligence may 
georges jim georges anne 
kdd competition knowledge discovery contest report 
available www cse ucsd edu users elkan html 
heckman heckman 
sample selection bias speci cation error 
econometrica 

class probability estimates evaluation classi ers 
workshop notes workshop cost sensitive learning international conference machine learning june 
provost fawcett foster provost tom fawcett 
robust classi cation imprecise environments 
machine learning 
appear 
quinlan quinlan 
programs machine learning 
san mateo ca morgan kaufmann 
turney peter turney 
cost sensitive learning bibliography 
institute information technology national research council ottawa canada 
ai iit nrc ca bibliographies cost sensitive html 
walker michael walker 
probability estimation classi cation trees 
technical report ksl knowledge systems laboratory stanford university 

