partially labeled classification markov random walks martin szummer mit ai lab cbcl cambridge ma szummer ai mit edu tommi jaakkola mit ai lab cambridge ma tommi ai mit edu classify large number unlabeled examples combine limited number labeled examples markov random walk representation unlabeled examples 
random walk representation exploits low dimensional structure data robust probabilistic manner 
develop compare estimation criteria algorithms suited representation 
includes particular multi way classification average margin criterion permits closed form solution 
time scale random walk representation set margin criterion favoring unambiguous classification 
extend basic regularization adapting time scales individual examples 
demonstrate approach synthetic examples text classification problems 
classification partially labeled examples involves limited dataset labeled examples large unlabeled dataset 
unlabeled examples classified provide information structure domain labeled examples identify classification task expressed structure 
common albeit tacit assumption context associates continuous high density clusters data pure classes 
assumption appropriate require labeled point cluster properly classify dataset 
data points typically relative global coordinate system associated metric 
metric may provide reasonable local similarity measure frequently inadequate measure global similarity 
example data may lie submanifold space revealed density global comparisons preferably manifold structure 
wish assign higher similarity values examples contained high density regions clusters implying comparisons ought incorporate density addition manifold structure 
representation examples satisfies desiderata constructed markov random walk similarly 
resulting global comparisons examples integrate volume paths connecting examples opposed shortest paths susceptible noise 
time scale markov process number transitions permit incorporate cluster structure data different levels granularity 
start defining representation subsequently develop classification methods naturally operating representations 
representation markov random walks define markov random walk locally appropriate metric 
metric basis neighborhood graph associated weights edges consequently transition probabilities random walk 
new representation examples obtained naturally random walk 
formally consider set points fx xn metric 
construct symmetrized nearest neighbor graph points assign weight ij exp undirected edge graph 
weights symmetric ii include self loops ij non neighbors 
note product weights path graph relates total length path way edge weights relate distances corresponding points 
step transition probabilities ik obtained directly weights ik ik ij ik non neighbor 
weights ik symmetric transition probabilities ik generally normalization varies node node 
tj kji denote step transition probabilities interpreted parameter random variable 
organize step transition probabilities matrix th entry ik simply matrix power calculate tj kji ik matrix row stochastic rows sum 
assume starting point markov random walk chosen uniformly random evaluate probability markov process started ended steps 
conditional probabilities jt ijk define new representation examples 
words point associated vector conditional probabilities jt ijk points representation close nearly distribution starting states 
representation crucially affected time scale parameter points indistinguishable provided original neighborhood graph connected 
small values hand merge points small clusters 
representation controls resolution look data points cf 
representation influenced local distance metric define step transition probabilities see section 
parameter estimation classification partially labeled data set xl yl xl xn wish classify unlabeled points 
labels may come classes typically number labeled points small fraction total points classification model assumes data point label distribution yji class labels 
distributions unknown represent parameters estimated 
point may labeled unlabeled interpret point sample step markov random walk 
labels associated original starting points posterior probability label point post yjk yji jt ijk classify th point choose class maximizes posterior argmax post cjk 
discuss techniques estimating unknown parameters yji maximum likelihood em maximum margin subject constraints 
em estimation estimation criterion conditional log likelihood labeled points log jk log ji jt ijk jt ijk fixed specific objective function jointly concave free parameters unique maximum value 
concavity guarantees optimization easily performed em algorithm 
ijk soft assignment component ijk ji jt ijk 
em algorithm iterates step ijk recomputed current estimates yji step update yji ijk ijk see 
margin estimation alternative discriminative formulation possible sensitive individual classification decisions product likelihoods 
define margin classifier labeled point class kd post jk post djk 
correct classification margin nonnegative classes kd zero correct class yk 
training find parameters yji maximize average margin labeled points forcing correctly classified 
unbalanced classes handled class margin obtain linear program max yji kd kd subject post jk post djk kd yji denotes number classes gives number labeled points class solution achieved extremal points parameter set surprising optimal parameters yji reduce hard values 
solution linear program closed form ji argmax nc jt ijk 
unlabeled labeled labeled top left local connectivity neighbors 
classifications markov random walks top bottom left right estimated average margin 
labeled points large cross circle unlabeled points classified small crosses circles unclassified small dots 
resulting posterior probabilities written compactly post cjk jt ijk 
closed form solution label distributions facilitates easy cross validated setting various parameters involved example representations 
large margin restricts dimension classifier section encourages generalization correct classification unlabeled points 
note margins bounded magnitude reducing risk single point dominate average margin 
criterion maximizes sum probabilities likelihood maximizes product probabilities easily dominated low probability outliers 
margin formulations possible 
separable problems maximize minimum margin average margin 
case classes global margin parameter labeled points 
algorithm focuses attention site minimum margin unfortunately outlier 
tackled noisy non separable problems adding linear slack variable constraint arrive average margin criterion linearity 
average min margin training yields hard parameters 
risk overfitting controlled smooth representation regularized increasing time parameter regularization desired applied maximum entropy discrimination framework bias solution uniform values 
additional regularization resulted similar classification performance adds computational cost 
examples consider example classification markov random walks 
labeled unlabeled points intertwining moons pattern 
pattern manifold structure distances locally globally euclidean due curved arms 
pattern difficult classify traditional algorithms global metrics svm 
euclidean local metric box extent show different timescales 
random walk connected unlabeled points labeled point 
parameters unconnected points affect likelihood margin assign uniformly classes 
points path classes fully assigned class 
points paths labeled points markov process mixed 
paths follow curved high density structure cross clusters 
markov process mixed points appropriately labeled 
parameter assignments hard class posteriors weighted averages remain soft 
sample size requirements quantify sample size needed accurate estimation labels unlabeled examples 
considering transduction problem finding labels observed examples sample size requirements assessed directly terms representation matrix 
probabilities jt ijj jt ijk denote conditional probabilities having started random walk process ends respectively 
simplicity consider binary problem classes ji ji 
classification decisions directly sign jt ijk 
lemma consider absolute distance representations points jk jp jt ijj jt ijk dimension binary transductive classifier upper bounded number connected components graph nodes adjacency matrix jk jk zero 
proof evaluate measure capacity classifier count number complete labelings consistent margin constraints labeled unlabeled points 
establish examples jk label 
follows directly jf jp jt ijj jt ijk jp post jk post jk jp jt ijj jt ijk jk difference larger discriminant functions different signs 
pair examples jk share label different labels assigned examples connected jk relation examples distinct connected components theorem applies generally transductive classifier weighted representation examples long weights bounded 
determine sample size needed dataset desired classification margin dimension 
high probability correctly classify unlabeled points log labeled examples 
helpful determine timescale reflected example full range 
average margin class class mac class win labeled error markov avg margin markov min margin markov max ent svm labeled windows vs mac text data 
left average class margins different labeled documents 
right classification accuracy labeled documents markov random walks best svm 
choices classifier robust rough heuristic choices 
follows 
local similarity measure 
typically euclidean distance 
local neighborhood size order manifold dimensionality sufficiently small avoid introducing edges neighborhood graph span outside manifold 
large preserve local topology ideally large create singly connected graph yielding ergodic markov process 
local scale parameter trades emphasis shortest paths low effectively ignores distant points versus volume paths high 
smoothness random walk representation depends number transitions 
regularization parameter akin kernel width density estimator 
limiting case employ local neighborhood graph 
special case obtain kernel expansion representation squared euclidean distance 
points labeled obtain nearest neighbors classifier 
limiting case representation node flat distribution points connected component 
choose unsupervised heuristics mixing time reach stationary distribution dissipation mutual information 
appropriate depends classification task 
example classes change quickly small distances want sharper representation smaller crossvalidation provide supervised choice requires labeled points accuracy 
propose choose maximizes average margin class labeled unlabeled data 
plot nc class kd separately labeled unlabeled points avoid issues relative weights 
labeled points class unlabeled points class class assigned classifier 
shows average margin function large text dataset section 
want large margins classes simultaneously choice gave best cross validation accuracy 
adaptive time scales far employed single global value desired smoothness may different different locations akin adaptive kernel widths kernel density estimation 
simplest graph multiple connected components set individual component 
ideally point time scale choice time scale optimized jointly classifier parameters 
propose restricted version criterion find individual time scales unlabeled point estimate single timescale labeled points 
principle select time scales unlabeled points encourages node identities common correlates labels 
precisely define yjk unlabeled point yjk jt ijk jt ijk summations labeled points 
probability labels unlabeled points yjk uniform unlabeled points corresponding start distribution 
note remains function individual time scales unlabeled points 
definitions principle setting time scales reduces maximizing mutual information label node identity ft arg max arg max fh yjk yjk marginal conditional entropies labels computed basis yjk respectively 
note ideal setting time scales determines labels unlabeled points uniquely basis labeled examples time preserving variability labels nodes 
happen example labeled examples fall distinct connected components 
optimize criterion axis parallel search trying discrete values large labeled point reached unlabeled point 
initialize smallest number transitions needed reach labeled point 
empirically initialization close refined solution objective 
objective concave separate random initializations generally yield answer convergence rapid requiring iterations 
experimental results applied markov random walk approach partially labeled text classification labeled documents unlabeled ones 
text documents represented highdimensional vectors occupy low dimensional manifolds expect markov random walk beneficial 
mac windows subsets newsgroups dataset examples classes dimensions 
estimated manifold dimensionality exceed histogram distances nearest neighbor peaked 
chose euclidean local metric leads single connected component reasonable falloff 
average margin criterion indicated cross validated plotted decay mutual information trained em margin formulations labeled points treating remaining points unlabeled 
trained random splits balanced class labels tested fixed separate set points 
results show markov random walk algorithms processed news www ai mit edu newsgroups removing rare words duplicate documents performing tf idf mapping 
clear advantage best svm labeled data linear kernel linear gaussian kernels different kernel widths values advantage especially noticeable labeled points decreases 
average margin classifier performs best 
handle outliers mislabeled points maximum min margin classifier stops improving labeled points supplied 
adaptive timescale criterion favors relatively small timescales dataset 
unlabeled points picks smallest timescale reaches labeled point point 
number labeled points increases shorter times chosen 
points criterion picks maximally smooth representation highest timescale considered possibly increase criterion 
preliminary experiments suggest adaptive time scales special classification advantage dataset 
discussion markov random walk representation examples provides robust variable resolution approach classifying data sets significant manifold structure labels 
average margin estimation criterion proposed context leads closed form solution strong empirical performance 
manifold structure absent unrelated classification task method expected derive particular advantage 
number possible extensions 
example choosing single resolution time scale may combine multiple choices 
done maintaining choices explicitly including time scales parametric form ta 
unclear exponential decay desirable 
facilitate continuum limit analysis establish better correspondence underlying density construct neighborhood graph basis balls nearest neighbors 
authors gratefully acknowledge support telegraph telephone ntt nsf itr iis 
szummer jaakkola 
kernel expansions unlabeled examples 
nips 
jaakkola meila jebara 
maximum entropy discrimination 
nips 
tishby slonim 
data clustering markovian relaxation information bottleneck method 
nips 
blum chawla 
learning labeled unlabeled data graph 
icml 
alon scale sensitive dimensions uniform convergence learnability 
acm tenenbaum de silva langford 
global geometric framework nonlinear dimensionality reduction 
science 
kondor lafferty diffusion kernels continuous spaces 
tech report cmu appear 
