ieee transactions neural networks vol 
xx 
month ieee 
personal material permitted 
permission reprint republish material advertising promotional purposes creating new collective works resale redistribution servers lists reuse copyrighted component works obtained ieee 
march draft ieee transactions neural networks vol 
xx 
month bankruptcy analysis self organizing maps learning metrics samuel kaski member ieee sinkkonen jaakko authors neural networks research centre helsinki university technology box fin hut finland :10.1.1.10.3633
mail samuel kaski hut sinkkonen hut jaakko hut 
march draft ieee transactions neural networks vol 
xx 
month introduce method deriving metric locally fisher information matrix data space 
self organizing map computed new metric explore nancial statements enterprises 
metric measures local distances terms changes distribution auxiliary random variable re ects important data 
variable indicates bankruptcy years 
conditional density auxiliary variable rst estimated change estimate resulting local displacements primary data space measured fisher information matrix 
self organizing map computed new metric visualizes data space topology preserving fashion represents local directions probability bankruptcy changes 
keywords bankruptcy analysis fisher information matrix information metric learning metric self organizing map great importance models business life general analysis eld science 
widely studied economics data analysis methods suggested problem 
traditional benchmark methods bankruptcy prediction problem argue important practical point view develop methods analyzing understanding di erent corporate behavior types relation bankruptcy 
task self organizing map som valuable tool mainly visualization capabilities 
introduces development som data analysis 
results show yields maps enhanced visualization bankruptcy risk statistically better separation healthy companies 
methodology directly utilized application areas 
success unsupervised algorithms self organizing map clustering methods depends crucially metric measure distance objects interest 
metric hand depends kinds variables chosen represent objects variable selection feature extraction 
processing steps ect supervised methods supervised methods principle unlimited resources universal approximators 
old problem feature extraction variable selection march draft ieee transactions neural networks vol 
xx 
month choosing represent input data persists crucial unsolved research topic pattern recognition neural computation data analysis 
simplest feature extraction reduces choosing scaling input variables generally nonlinear mapping input space space suitable processing 
successful feature extraction stages usually tailored task hand expert knowledge heuristic rules thumb 
implicit auxiliary information available relevance features input 
instance classi cation task relevant features separate classes 
implicit information relevance features may available unsupervised descriptive data analysis tasks 
relevant classi cation samples may known goal may nd natural grouping grouping re ects classi cation may example discover subclasses 
example process monitoring indicator performance process may associated data vector 
quality product suitable indicator 
goal nd factors ecting performance process 
ultimate aim develop algorithms take auxiliary information account order explicitly transform original metric input space 
space locally scaled new local distances measure change auxiliary information preliminary account see 
proximity relations loosely speaking topology input space retained 
note contrast change metric preserve proximity relations map close points input space di erent feature values generalization power originating smoothness model lost 
computational reasons new metric best suited algorithms rely local distances input space 
self organizing map example 
unsupervised algorithm learns new metric learning process useful combination supervised unsupervised learning 
proximity relationships input space preserved typical unsupervised methods metric local scaling space induced supervised manner 
apply new metric analyze bankruptcy risk enterprises basis nancial statements 
setting similar :10.1.1.10.3633
mapping continuous topology preserving may projective 
march draft ieee transactions neural networks vol 
xx 
month self organizing maps extend bankruptcy analysis traditional straightforward prediction bankruptcy visual exploratory analyses relationship nancial statements bankruptcy risk di erent kinds enterprises 
complement studies new metric som exploratory analyses 
enterprises organized som manner analysis concentrate local factors ect probability bankruptcy 
explore results nd important dimensions various kinds enterprises 
ii 
metric wish transform distance measure data space concentrate important di erences data samples disregard irrelevant dimensions 
may clear metric impossible construct auxiliary prior knowledge importance di erences 
assume auxiliary data available details auxiliary data implicitly de nes important relevant 
new metric learned data set distance measure subsequent analysis visualization set 
metric constructed re ects local importance di erent directions data space 
example measure changes nancial state ect bankruptcy risk kind enterprise 
due locality distance measure capable revealing di erent factors di erent kinds enterprises 
distance measure de ned original data space straightforward interpret results terms original variables indicators nancial states 
example distances type large axis corresponding pro pro contributes bankruptcy risk companies 
exploratory data analysis applications similarity relationships data samples enterprises visualized methods self organizing map precisely way previously 
di erence relative distances enterprises change 
di erent important dimension non linear route minimal length distance large di erent irrelevant dimension close practical computations local approximations nonlinear routes sensible algorithms som depend local distances 
march draft ieee transactions neural networks vol 
xx 
month learning metric principle seek describe similarity relationships items data space utilizing information joint distribution data auxiliary data denote joint probability density function pdf :10.1.1.10.3633
call primary data denote associated random variable denote random variable produces auxiliary data assumed speci cally conditional distributions cjx implicitly convey information kinds similarity relationships important data 
application bankruptcy analysis binary indicate enterprise goes bankrupt years feature vectors derived nancial statements 
important changes nancial state change probability bankruptcy distribution cjx 
change distributions measured kullback leibler divergence old result gives formula local kullback leibler divergence cjx kp cjx dx dx dx cjx log cjx log cjx fisher information matrix cjx denotes expectation possible values conditioned fisher information matrix representation tensor new metric original euclidean coordinates 
matrix positive semide nite de nes local scaling directions input space point de ne new local metric data space dx cjx kp cjx dx dx dx new metric conditional density cjx changes evenly directions points input space 
note fisher information matrix originally derived measuring ect change model parameters produces probability distributions models generate 
resulting distance called fisher information distance fisher information metric information geometry literature see 
measure ect march draft ieee transactions neural networks vol 
xx 
month change location primary data space obtain metric 
call resulting metric fisher metric call approach semisupervised auxiliary distribution way supervises construction metric 
note new metric de ned locally close points dx global distances de ned path integrals 
principle exists straightforward alternative simply measure distance non local pair cjx kp cjx 
measure useful applications disadvantage completely override original structure data space 
points identical density estimates cjx cjx zero distance points originally far away 
kind generalizability exists destroyed change topology 
fact original representations needed data points simply represented distributions space 
information contained primary data lost 
additional disadvantage new representations interpreted terms original data variables analyses 
bankruptcy application prime importance know aspects nancial state related changes bankruptcy risk method focuses problem 
learning metric computation conditional probability distribution cjx usually estimated data set fx method produces di erentiable estimates potentially useful 
choice estimator discussed section ii moment assume tentatively estimate cjx conditional density available 
estimate cjx principle place cjx approximate new metric 
numerical computations necessary form fisher information matrix explicitly get squared local distances directly dx cjx dx log cjx new metric supervised unsupervised method section iii describe compute self organizing maps visualization exploratory data analysis purposes 
march draft ieee transactions neural networks vol 
xx 
month fig 

metric generated pdf estimate dimensional class data set 
rst class sampled symmetrical gaussian topmost cluster gure second sum gaussians bottom clusters 
gaussians mutual distances centers equal unity 
gray scale background illustrates marginal density small line segments dots depict dominant direction relative distances local metric 
distances nonzero directions conditional density changes 
pdf estimated gaussian parzen estimator 
partially labelled data available best pdf estimator ability utilize data 
gaussian mixture model mda described section ii easily extended partially labelled data usually optimized em algorithm 
pdf estimator xed partially labelled data new metric straightforward metric de ned primary data space computed knowledge primary sample 
demonstration new metric arti cial easily dimensional data set 
note non local distances de ned minimal path integrals local distances minimum taken possible paths 
generates riemannian metric general treatments related information geometry see 
practice computation integrals extremely tedious resort local approximations sensible methods rely local distances see section iii 
march draft ieee transactions neural networks vol 
xx 
month note estimate cjx uneven fisher metric spans lowdimensional space metric regularized mixing original euclidean metric resulting metric tensor represented small positive constant identity matrix :10.1.1.10.3633
metrics kinds pdf estimates goal estimate probability density cjx auxiliary random variable conditioned plenty alternative methods available 
developed classi cation purposes reviews see 
methods typically suboptimal purpose classi er optimizes implicit probability density function pdf estimate near class borders generally near area decision criterion reaches critical values 
principle estimator produces di erentiable estimates conditional densities 
skip discussion merits di erent estimators rely classical methods 
rst computationally intensive performing nonparametric estimate parzen kernel estimator second gaussian mixture model 
estimators expressed general mixture density form 
consider additive mixture model generating component densities identi ed discrete random variable value jth component generator generated current data sample 
assume conditionally independent value joint density generated jth component ju xju model ju coe cient ji xju function parameterized notation model joint density data ji denote set parameters model 
applying bayes rule estimate conditional density obtained jx ji march draft ieee transactions neural networks vol 
xx 
month kernel estimator mixture density model di er parameterizations 
estimation parameters special cases discussed detail 
moment assume values parameters conditional density estimate known 
shown appendix component densities gaussians equal diagonal covariance matrices means distance dx cjx dx jx jx parameter governs width gaussians smoothness resulting pdf estimates 
method choosing value new metric learning self organizing maps described section iv suitable likelihood measure proposed value sigma selected maximize measure learning validation set 
kernel estimation kernel density estimation component densities called kernels number kernels equal number data points parameters set data samples prior probabilities set parameter ji jth data pair value ji 
free parameter left estimated variance kernels 
gaussian mixture component densities model chosen gaussians parameterized means model equivalent mixture discriminant analysis cf 
relation mixture discriminant analysis discussed detail section ii :10.1.1.10.3633
ji estimated data 
formulas estimating model em algorithm appendix related works knowledge introduced principle new 
works aspects resemble approach exist 
amari wu augmented support vector draft ieee transactions neural networks vol 
xx 
month chines making isotropic change metric near class border 
contrast metric non isotropic changes metric 
jaakkola haussler induced distance measure discrete input space generative probability model :10.1.1.44.7709
crucial di erences external information constrain metric preserve topology 
earlier works auxiliary information incorporated directly representations data see note goal works di erent 
auxiliary information encoded example manner concatenated data vectors main problem approach purposes arbitrary relative scale primary auxiliary data 
relative scale auxiliary data small primary data dominate distance measure goal measure changes auxiliary data represent changes distance measure primary data space 
relative scale discrete auxiliary data large hand data vectors ectively divided separate clusters corresponding possible value auxiliary variable 
proximity relations topology original data space destroyed 
mappings original space new lower equal dimensional space general de nition feature extraction relation method 
automatic methods optimizing mappings example maximizing mutual information proposed 
standard separate feature extraction stage change metric method de nes manifold general projected euclidean space lower dimensionality 
dimensionality preserving dimensionality reducing mapping local properties exists means change metric general operation feature selection dimensionality preserving nonlinear mapping 
change metric additionally interpreted kind nonlinear version linear discriminant analysis lda applications lda nance see 
lda nds linear transformation de ned globally data space aims maximizing class separability 
proposed variant called mixture discriminant analysis set gaussian kernels tted data optionally constraining dimensionality subspace kernels allowed reside :10.1.1.10.3633
contrast lda newer march draft ieee transactions neural networks vol 
xx 
month variants transform input space locally class distribution change rate direction 
allows inspection class distributions closely 
note discriminant analysis commonly tasks acquiring classi cations understanding relationships classes visualizations means 
model closely related task lda usually emphasizes 
classical canonical correlation analysis generalized replacing linear combinations nonlinear functions 
framework adapted task nding statistical dependencies data sets replacing discrete auxiliary random variable parametrized set features computed auxiliary continuous random variable explored 
iii 
self organizing maps fisher metric principle model utilizes local distances adapted fisher metric 
derive line self organizing map algorithm new metric data analysis 
self organizing map self organizing map som regular grid units model vector associated unit learning process model vectors gradually modi ed follow distribution input data ordered fashion model vectors close map lattice attain close locations input space 
dimensional map grids visualize various properties input data data analysis applications 
som algorithm iterates steps 
index winning unit closest current input sample time rst sought arg min distance function commonly euclidean 
model vectors adapted hwi euclidean distance adaptation rule familiar hwi march draft ieee transactions neural networks vol 
xx 
month hwi called neighborhood function decreasing function distance units map lattice 
height width hwi decrease gradually time 
details see 
som new metric organize soms fisher metric determined di erences estimated posterior distributions cjx rst construct pdf estimator distributions 
nd winning unit data sample calculate distances set model vectors nd closest general distances non local 
compute non local distances search minimal path integral required di erential distances paths de ned 
approximate non local distances local distance measure computed data sample 
winning unit 
dx small approximation fair far points approximation 
assumption approximation locally accurate preserve order distances model vector closest riemannian metric de ned shortest path equal mw computed local approximation 
sensible model vectors close measured true non local distances small dx local approximation usually ect unit winner 
occasionally may happen true test goodness approximation experimental results 
results case study section iv favourable 
original som model vector winning unit units neighborhood updated direction distance decreases rapidly proportionally magnitude change 
euclidean metric update gradient kx fisher metric riemannian metric steepest descent riemannian metric called natural gradient :10.1.1.10.3633
generally natural gradient equal conventional gradient multiplied representation metric tensor matrix 
original coordinate system metric tensor represented fisher information matrix natural gradient coordinates march draft ieee transactions neural networks vol 
xx 
month assuming close coincides direction shortest path update rule fisher metric euclidean som 
di erence lies de nition winner distance measure general de ned case gaussian kernel pdf estimators 
demonstration ect change metric som demonstrated dimensional class toy data set 
computed self organizing map original euclidean metric fisher metric visualized posterior class distribution soms 
seen gure classes distinctly orderly separated som computed fisher metric 
notably distribution class clearly visible 
data analysis applications som grid visualizing aspects data 
displays section iv 
computational complexity iteration som algorithm consists selection winning som unit current input update model vectors 
update rule fisher metric unchanged euclidean case computational complexity update ndim som neighborhood function covers map grid 
ndim dimensionality input som number som units 
select winner distances calculated input som unit 
local distance approximation done computing fisher information matrix rst directly calculating distances 
rst alternative requires ndim dim nc som operations nc number classes nu number mixture components pdf estimate 
second alternative requires ndim nc nu som operations 
dimensionality small compared number classes size som computing fisher information matrix explicitly may faster preferable calculate distances directly 
better method mda case study kernels number classes dimensionality winner search requires twice march draft ieee transactions neural networks vol 
xx 
month fig 

demonstration di erence soms computed fisher metric euclidean metric 
primary data dimensional distributed 
auxiliary data divided smoothly changing gaussian classes cjx probability density distribution 
class centers placed evenly origin km variance 
size data set points 
pdf estimate generated parzen model 
soms trained data stochastic algorithm 
posterior probabilities classes parzen estimate evaluated model vectors som shown soms size units organized represent data set fisher class class class standard euclidean metric 
probability shown lightest shade probability pure black 
amount computation required simple euclidean metric 
note exist speedup methods som see 
investigated detail fisher metric applicable 
iv 
application bankruptcy analysis method previous chapters applied bankruptcy analysis task 
traditionally quantitative studies bankruptcy directed prediction 
dominating approaches bankruptcy prediction problem classi cation probability estimation 
classi cation task possibly past data companies divided groups go bankrupt certain time interval 
probability estimation aim march draft ieee transactions neural networks vol 
xx 
month get estimates probability bankruptcy certain time interval simpli ed version rate companies bankruptcy risk requiring ratings true probabilities :10.1.1.10.3633
naturally probability estimation risk rating models er basis classi cation 
seminal bankruptcy prediction performed altman workers summarized applied linear discriminant analysis problem 
statistical method including neural network approaches proposed see :10.1.1.10.3633
generally observed methods especially advanced ones neural network models slightly lda 
cases improvement quite small excluding studies training set performance reported data set small 
view complementary bankruptcy prediction problem referred bankruptcy analysis trying understand di erent corporate behaviors relation risk bankruptcy 
uential qualitative area carried 
observations di erent bankruptcy types failure trajectories di er causes symptoms length 
lines thought research project helsinki university technology attempted quantify visualize di erent behavior patterns :10.1.1.10.3633
study closely related project ndings challenges brie summarized 
som increase accuracy bankruptcy prediction useful visualizing state possible directions development analyst gets accurate idea state visualization som single scalar estimating bankruptcy risk 
second di erent types corporate behavior trajectories identi ed som 
third problem visualization som data intrinsic dimensionality higher som grid discontinuities mapping result 
instance single cluster high bankruptcy risk may appear multimodal som 
fisher metric approach described previous chapters help problem manifold spanned fisher metric lower dimensionality original data space 
primary goal section new methods better understand nonlinear dependencies nancial indicators 
dependencies march draft ieee transactions neural networks vol 
xx 
month converted metric input space som visualize dependencies concise form 
metric chosen describe changes bankruptcy sensitivity som emphasize features input space locally contributing 
section brevity call som computed euclidean metric som som computed fisher metric som 
data nancial statements finnish small medium sized enterprises 
line business age size completeness available data selection criteria data rejected basis 
data set nancial statements companies :10.1.1.10.3633
statements concerned companies gone bankrupt :10.1.1.10.3633
take account development companies time 
multiple statements enterprise di erent years treated independent samples 
set common nancial indicators including measures growth pro solidity liquidity operational ciency samples primary data space dimensional real vectors 
indicators preprocessed separately histogram equalization 
auxiliary random variable binary indicating statement followed bankruptcy years 
methods data randomly divided estimation set test set roughly equal sizes 
pdf estimates rst parzen estimation gaussian kernels second gaussian mixture mixture components tted estimation set 
hexagonal soms size units computed euclidean fisher metric derived pdf estimates 
veri cation measures section measure goodness verifying som re ects aspects input data relevant risk bankruptcy 
components ecting goodness quality pdf estimator accuracy soms represent march draft ieee transactions neural networks vol 
xx 
month probability bankruptcy quality visualizations smoothness quality organization soms 
measure rst component assumed standard pdf estimators adequate 
accuracy representation measured log likelihood test data estimates locations winner units log jm regarding quality visualizations resort visual comparisons visualizations obtained som som 
likelihoods obtained pdf estimates som som computed wide range values parameter order average distance closest data points order maximal distance 
likelihood obtained directly pdf estimator computed nd approximation best possible performance model predicting prior probabilities classes served lower limit useful results 
note likelihood measuring accuracy representation connection quantization error commonly measuring quality soms 
quantization error de ned average distance original data winning som units kg 
som corresponding measure kullback leibler divergence posterior distributions cjx 
assuming close squared di erence computed fisher information matrix 
measure solely estimates pdfs data minimizing quantization error estimated fisher metric guarantee map represents real data 
exists simple remedy estimate cjx replaced real distribution cjx kullback leibler divergence measures deviance representation true pdf 
easily shown average divergence cjx approximated linear function likelihood 
note accuracy measured prediction accuracy goal simply maximize prediction accuracy quantify accuracy visualizations 
march draft ieee transactions neural networks vol 
xx 
month visualization results addition usual visualization methods available soms som visualize correlations bankruptcy sensitivity directions data space 
amount scaling direction dx revealed quadratic form dx measures ect direction bankruptcy sensitivity 
visualize magnitudes scalings easily interpretable directions data space original variables data 
relative amount scaling direction coordinate axis unit vector parallel axis 
large value indicates strong ect variable locally results likelihoods som som test set shown function parameter governs smoothness pdf estimates 
som expected performs clearly better euclidean som 
som roughly equal pdf estimate small pdf estimate resulting fisher metric probably uneven 
location nancial statement som accurate predictor bankruptcy location statement som 
test statistical signi cance performance di erence data divided separate sets 
test round set test data sets training data 
likelihood curves som som calculated test sets mda estimate peaks curves compared sign test 
som outperformed som 
variation nancial indicators som displays fig 
remarkably smooth comparable smoothness som displays :10.1.1.10.3633
bankrupt companies visually clearly separated soms sense distribution unimodal posterior class densities change smoothly map fig 

summary organization visualization capabilities som maintained improved fisher metric increased prediction accuracy 
relative scaling coordinate axes new metric visualized easily draft ieee transactions neural networks vol 
xx 
month log likelihood pdf som som priori log likelihood pdf som som priori fig :10.1.1.10.3633

accuracy soms computed euclidean metric som fisher metric som representing probability bankruptcy measured likelihood data locations best matching som units 
pdf estimated gaussian kernel parzen estimate 
pdf estimated gaussian mixture having mixture components 
curve marked pdf provides approximate upper limit likelihood data points best matching units 
curve marked priori provides lower limit sensible results obtained best constant estimates 
parameter governs smoothness pdf estimates 
fig 

separation bankruptcy prone healthy companies soms 
estimate probability bankruptcy map unit som som 
estimate posterior density gaussian mixture model 
darkest shade denotes probability lightest denotes probability 
note prior probability bankruptcy small just 
actual relative frequency test set map unit shown som som 
frequency graphs noisy number bankrupt companies small 
white black thirds companies gone bankrupt 
march draft ieee transactions neural networks vol 
xx 
month fig 
:10.1.1.10.3633
distribution values nancial indicators som som 
index pro liquidity capital structure 
fisher metric som computed gaussian mixture estimate 
overviews relative importance input variables 
examples 
non constant values suggest nonlinear ects exist justify nonlinear models data set 
discussion introduced new method deriving metrics estimated posterior distribution auxiliary relevance inducing variable computing self organizing maps 
metric fisher information matrix results local approximation kullback leibler divergence posterior densities close points primary data space 
new metric estimated posterior probabilities change evenly directions 
words metric represents local contribution directions data space changes relevance indicating random variable 
computed som fisher metric applied visualization bankruptcy sensitivity function quantitative nancial indicators 
som accurate march draft ieee transactions neural networks vol 
xx 
month fig 

relative contributions change bankruptcy sensitivity plotted gray levels map display indicators :10.1.1.10.3633
relative contribution pro indicator scale decreases contribution capital structure indicator scale increases bankruptcy zone stripe top left corner contribution liquidity indicator scale low :10.1.1.10.3633
representing estimated probability bankruptcy euclidean som visual quality maps comparable improved 
fisher metric discovering visualizing locally relevant dimensions kind automatic feature extraction stage 
way visualizing contributions input variables fisher metric 
general visualization metric tensor function primary data space subject experimentation research 
kind feature extraction stage method nice property changes metric preserving proximity relations original data space 
pdf estimator approaches real pdf number data grows results independent original coordinate system metric data 
preservation proximity relations course natural requirement sensible operation processing stages som topology original space worth preserving best suitable discontinuous preprocessing stage 
may worth noting change metric ects density fisher metric density changes jj 
change reduces density data points space posterior probabilities cjx change rapidly 
undesirable modi ed fisher metric constant magni cation factor 
nding relevant local features input space extraction fisher metric march draft ieee transactions neural networks vol 
xx 
month similar introduced kullback leibler clustering algorithm 
connection detailed papers 
summary extended som exploratory analyses factors ecting bankruptcy risk di erent kinds companies new learning metric 
fisher metric derived pdfs improved accuracy visual maps represent bankruptcy quality visualizations 
bankruptcy analysis nancial statements common task relatively known features meaningful ective dimensionality meaningful data spaces small 
hard improve methods eld 
fisher metric useful structure data known little justi cation manual feature selection 
appendix derivation distance mixture model gradient jx ji jx log jx ji jx gaussian having diagonal covariance matrix log jx ji jx march draft ieee transactions neural networks vol :10.1.1.10.3633
xx 
month expression brackets simpli ed ki ji ji ki jx jx plugging yields log jx jx jx jx xg jx xg jx jx plugging yields :10.1.1.10.3633
appendix em estimation gaussian mixture model joint densities em algorithm maximize likelihood model 
value random variable indicates generator produced data item considered missing data 
value data sample denoted data assumed independent identically distributed 
initialization set nu ji nu nc nu denotes number component generators nc denotes number possible values initialized means algorithm 
step consists sub steps 
joint distribution missing data inferred expected log likelihood model respect distribution computed conditioned old parameters data 
old set parameters joint distribution missing data fu jf jx march draft ieee transactions neural networks vol 
xx 
month generally superscript refer old parameters 
probability jx mixture component generated data sample jx ji jk shown expected log likelihood model respect distribution efl jk log log ji log step expected log likelihood maximized 
shown respect ji maximum ji jk jk maximum jk number data samples jk jk acknowledgments authors particularly data set kimmo valuable information bankruptcy analysis help regarding interpretation data academy finland nancial support 
march draft ieee transactions neural networks vol 
xx 
month kohonen self organized formation topologically correct feature maps biological cybernetics vol :10.1.1.10.3633
pp 
:10.1.1.10.3633
kohonen self organizing maps springer berlin third extended edition :10.1.1.10.3633
kaski sinkkonen metrics learn relevance proc 
ijcnn international joint conference neural networks vol 
pp 
:10.1.1.10.3633
ieee 
analyzing nancial statements self organizing map proceedings workshop self organizing maps pp 

helsinki university technology neural networks research centre espoo finland 
predicting self organizing map neurocomputing vol :10.1.1.10.3633
pp 

exploring corporate bankruptcy level self organizing maps 
decision technologies computational management science proceedings fifth international conference computational finance pp 

kluwer academic publishers boston 
kullback information theory statistics wiley new york :10.1.1.10.3633
rao information accuracy attainable estimation statistical parameters bull 
calcutta math 
soc vol 
pp 
:10.1.1.10.3633
murray rice di erential geometry statistics chapman hall london 

amari di erential geometrical methods statistics springer new york 
kass vos geometrical foundations asymptotic inference wiley new york 
laaksonen oja neural statistical classi ers taxonomy case studies ieee transactions neural networks vol 
pp 
:10.1.1.10.3633
ripley pattern recognition neural networks cambridge university press cambridge great britain 
hastie tibshirani buja flexible discriminant mixture models proc 
conf 
neural networks statistics kay titterington eds 
oxford university press :10.1.1.10.3633
trevor hastie robert tibshirani discriminant analysis gaussian mixtures journal royal statistical society series vol :10.1.1.10.3633
pp :10.1.1.10.3633
:10.1.1.10.3633
dempster laird rubin maximum likelihood incomplete data em algorithm journal royal statistical society series vol 
pp 

amari wu improving support vector machine classi ers modifying kernel functions neural networks vol 
pp 

jaakkola haussler exploiting generative models discriminative classi ers advances neural information processing systems kearns solla cohn eds pp :10.1.1.44.7709

morgan kau mann publishers san mateo ca 
ritter kohonen self organizing semantic maps biological cybernetics vol 
pp 
:10.1.1.10.3633
march draft ieee transactions neural networks vol 
xx 
month fisher iii principe methodology information theoretic feature extraction proc 
ijcnn international joint conference neural networks vol 
pp 

ieee 
torkkola campbell mutual information learning feature transformations proc 
icml seventeenth international conference machine learning morgan kaufmann pp 
:10.1.1.10.3633
altman complete guide predicting avoiding dealing bankruptcy john wiley sons 
becker mutual information maximization models cortical self organization network computation neural systems vol 
pp 

lai fyfe neural implementation canonical correlation analysis neural networks vol 
pp 

:10.1.1.10.3633
amari natural gradient works ciently learning neural computation vol 
pp 
:10.1.1.10.3633
kohonen kaski lagus honkela saarela self organization massive document collection ieee transactions neural networks vol 
pp 
:10.1.1.10.3633
financial ratios probabilistic prediction bankruptcy journal accounting research vol 
pp 

gordon karels arun prakash multivariate normality forecasting business bankruptcy journal business finance accounting pp 
winter :10.1.1.10.3633
tam managerial applications neural networks case bank failure predictions management science vol 
pp 
july 
wilson bankruptcy prediction neural networks decision support systems vol 
pp :10.1.1.10.3633
june :10.1.1.10.3633
yoon jr comparison discriminant analysis versus arti cial neural networks journal operational research society vol 
pp 
january :10.1.1.10.3633
fletcher goss forecasting neural networks application bankruptcy data information management vol 
pp 
march :10.1.1.10.3633

baba predicting japanese corporate bankruptcy terms nancial data neural networks computers industrial engineering vol 
pp 
september :10.1.1.10.3633
udo neural network performance bankruptcy classi cation problem computers industrial engineering vol 
pp :10.1.1.10.3633

martn del bro carlos serrano self organizing neural networks analysis representation data nancial cases neural computing applications vol :10.1.1.10.3633
pp 

neural network analysis russian banks proceedings workshop self organizing maps espoo finland june neural networks research centre helsinki university technology 
john corporate collapse causes symptoms mcgraw hill 
kimmo level self organizing maps analysis nancial statements proceedings ieee international joint conference neural networks ijcnn piscataway march draft ieee transactions neural networks vol 
xx 
month new jersey usa may ieee neural networks council vol 
pp 

kimmo comparing self organizing maps nancial data visualization methodologies conception design application soft computing proceedings th international conference soft computing information intelligent systems :10.1.1.10.3633
fukuoka japan gen matsumoto eds singapore oct vol 
pp 
world scienti sinkkonen kaski clustering similarity auxiliary space proceedings ideal second international conference intelligent data engineering automated learning kwong sak leung lai wan chan helen meng eds pp 

springer berlin 
sinkkonen kaski semisupervised clustering conditional distributions auxiliary space tech 
rep helsinki university technology laboratory computer information science espoo finland 
march draft ieee transactions neural networks vol 
xx 
month legends metric generated pdf estimate dimensional class data set 
rst class sampled symmetrical gaussian topmost cluster gure second sum gaussians bottom clusters 
gaussians mutual distances centers equal unity 
gray scale background illustrates marginal density small line segments dots depict dominant direction relative distances local metric 
distances nonzero directions conditional density changes 
pdf estimated gaussian parzen estimator 
demonstration di erence soms computed fisher metric euclidean metric 
primary data dimensional distributed 
auxiliary data divided smoothly changing gaussian classes cjx probability density distribution 
class centers placed evenly origin km variance 
size data set points 
pdf estimate generated parzen model 
soms trained data stochastic algorithm 
posterior probabilities classes parzen estimate evaluated model vectors som shown soms size units organized represent data set fisher class class class standard euclidean metric 
probability shown lightest shade probability pure black 
march draft ieee transactions neural networks vol 
xx 
month legends accuracy soms computed euclidean metric som fisher metric som representing probability bankruptcy measured likelihood data locations best matching som units 
pdf estimated gaussian kernel parzen estimate 
pdf estimated gaussian mixture having mixture components 
curve marked pdf provides approximate upper limit likelihood data points best matching units 
curve marked priori provides lower limit sensible results obtained best constant estimates 
parameter governs smoothness pdf estimates 
separation bankruptcy prone healthy companies soms 
estimate probability bankruptcy map unit som som 
estimate posterior density gaussian mixture model 
darkest shade denotes probability lightest denotes probability 
note prior probability bankruptcy small just 
actual relative frequency test set map unit shown som som 
frequency graphs noisy number bankrupt companies small 
white black thirds companies gone bankrupt 
march draft ieee transactions neural networks vol 
xx 
month legends distribution values nancial indicators som som :10.1.1.10.3633
index pro liquidity capital structure 
fisher metric som computed gaussian mixture estimate 
relative contributions change bankruptcy sensitivity plotted gray levels map display indicators :10.1.1.10.3633
relative contribution pro indicator scale decreases contribution capital structure indicator scale increases bankruptcy zone stripe top left corner contribution liquidity indicator scale low :10.1.1.10.3633
march draft 
