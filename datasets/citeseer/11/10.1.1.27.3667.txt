bayesian analysis mixtures unknown number components sylvia richardson france 
peter green university bristol uk 
revised version october summary new methodology fully bayesian mixture analysis developed making reversible jump markov chain monte carlo methods capable jumping parameter subspaces corresponding different numbers components mixture 
sample full joint distribution unknown variables generated basis thorough presentation aspects posterior distribution 
methodology applied analysis univariate normal mixtures hierarchical prior model offers approach dealing weak prior information avoiding mathematical pitfalls improper priors mixture context 
key words birth death process classification galaxy data heterogeneity lake data markov chain monte carlo normal mixtures predictive distribution reversible jump algorithms sensitivity analysis 
article contribution methodology fully bayesian mixture modelling 
stress word fully senses 
model number components mixture component parameters jointly base inference quantities posterior probabilities 
contrast previous bayesian treatments mixture estimation consider models different numbers components separately significance tests non bayesian criteria infer number components 
secondly aim posterior distributions objects inference model parameters predictive densities just best estimates 
key ideas treatment 
demonstrate novel mcmc methods reversible jump samplers introduced green sample mixture representations unknown varying number components 
believe methods preferable grounds convenience version workshop model interpretation model robustness highly structured stochastic systems june 
written version february 
avenue paul france 
email richardson fr department mathematics university bristol bristol bs tw uk 
email green bristol ac uk 
accuracy flexibility analytic approximations proposed mcmc techniques 
secondly show sample approach computation mixture models allows subtle extraction information posterior distributions 
give examples presentation posteriors tease alternative explanations data difficult discover approaches 
base experiments discussion hierarchical model mixtures aims provide simple generalisable way weakly informative parameters mixture models 
propose specific model univariate normal mixtures illustrate implementation performance emphasise rest methodology way restricted particular model 
issues considered including key ideas relevance mixture problems 
particular issues concerning presentation posterior distributions arise problems inference functions interesting questions raised labelling parameters occur statistical model partial invariance permutations variables 
century strong sustained interest finite mixture distributions attributable complementary aspects mixture models provide natural framework modelling heterogeneity establishing links cluster analysis secondly appealing semi parametric structure model unknown distributional shapes objective density estimation flexible construction bayesian priors 
field comprehensively discussed titterington smith makov mclachlan basford 
statistical analysis mixtures straightforward non standard problems posed geometry parameter space computational difficulties 
headway dealing challenging distributional problems testing hypotheses number mixture components lindsay computational side implementation variants em algorithm celeux diebolt 
strongly believe bayesian paradigm particularly suited mixture analysis especially unknown number components 
previous finite mixture estimation bayesian separated issues testing number components estimation fixed 
fixed case comprehensive bayesian treatment mcmc methods diebolt robert 
early approaches general case unknown typically adopted different style modelling treating problem example bayesian basing prior distributions dirichlet process see escobar west example 
authors example mengersen robert raftery roeder wasserman proposed respectively kullback leibler distance laplace metropolis estimator schwarz criterion choose number components 
direct line adopt modelling unknown case mixing fixed case making fully bayesian inference followed authors including phillips smith 
structured follows 
section bayesian hierarchical model mixtures 
markov chain monte carlo methods variable dimension problems discussed section adapted particular case mixture analysis 
section performance methodology assessed application real data sets sections sensitivity mcmc performance issues considered 
section covers classification mixture modelling conclude discussion extensions outline 
bayesian models mixtures basic formulation write basic mixture model independent scalar vector observations deltaj independently deltaj parametric family densities indexed scalar vector parameter 
objective analysis inference unknowns number components component parameters component weights summing 
model arises distinct contexts 
postulate heterogeneous population consisting groups sizes proportional random sample drawn 
identity label group observation drawn unknown 
situation natural regard group label th observation latent allocation variable 
supposed independently drawn distributions values observations drawn respective individual subpopulations jz deltaj independently second context prime focus mixture model thought convenient parsimonious representation non standard density objective inference kind semi parametric density estimation 
case formulation convenient calculation interpretation continued 
integrating brings back 
note specified population model components necessarily represented finite sample may empty components 
hierarchical model priors bayesian framework unknowns regarded drawn appropriate prior distributions 
joint distribution variables written general jz yj deltaj delta denote generic conditional distributions consistent joint specification notation natural impose conditional independences jz jk yj yj 
joint distribution simplifies give bayesian hierarchical model jk yj consider models yj 
full flexibility add extra layer hierarchy allow priors depend hyperparameters ffi respectively 
drawn independent 
joint distribution variables expressed factorisation ffi ffi kj ffi jk yj normal mixtures detailed exposition limited case univariate normal mixtures 
methodology generic applies widely 
specific generalisations implemented described section 
univariate normal case generic parameter vector expectation variance pairs oe yj yj oe oe exp gamma gamma oe prior distributions oe gamma drawn independently normal gamma priors gamma oe gamma gamma ff fi choosing parameterisation mean variance ff fi ff fi respectively generic ff fi 
fairly natural choices prior distributions giving advantages conjugacy advantages needed mcmc computation 
natural conjugate prior oe parameters pair priori dependent 
come important issue labelling components 
note model invariant permutation labels identifiability important adopt unique labelling 
stated increasing numerical order joint prior distribution parameters 
times product individual normal gamma densities restricted set delta delta delta prior taken symmetric dirichlet ffi ffi ffi 
necessary adopt proper prior distribution common choice poisson hyperparameter convenience presentation interpretation uniform distribution pre specified integer max choice discussed come experiments 
weak prior information component parameters consider bayesian mixture estimation set want strong prior information mixture parameters 
cases subjective priors preferable prior setting modified accordingly 
fully non informative obtaining proper posterior distributions possible mixture context 
possibility observations allocated components data uninformative standard choices independent improper non informative prior distributions component parameters diebolt robert roeder wasserman 
previous attempts circumvent problem involve dependent priors mentioned section 
purposes mixture modelling case keeping simple independence prior structure oe gamma outlined section defining weakly informative priors may may data dependent line taken raftery 
example interested identifying subpopulations priori information translated say range spread 
introduce hyperprior structure default hyperparameter choices correspond making minimal assumptions data 
natural take gamma prior flat interval variation data postulated priori corresponding observed range 
achieved dag specific normal mixture model implemented corresponding conditional independence graph simple way letting equal mid point interval setting equal small multiple length interval 
contrast case means restrictive suppose knowledge range data implies size oe recall oe gamma gamma ff fi independently 
introduce additional hierarchical level allowing fi follow gamma distribution parameters generally take ff express belief oe similar informative absolute size 
scale parameter small multiple ffi held fixed 
complete hierarchical model call random fi model displayed directed acyclic graph dag usual convention graphical models square boxes represent fixed observed quantities circles unknowns 
reversible jump mcmc algorithm mixtures mcmc algorithms variable dimension parameters markov chain monte carlo mcmc methods play central role modern bayesian computation see example tierney besag green higdon mengersen 
methods initially available problems posterior distribution density respect fixed standard underlying measure cases mixture estimation number things don know things don know 
mcmc methods varying dimension problems discussed grenander miller green phillips smith 
approach termed reversible jump mcmc elaborated green including applications change point analysis dimensions partition problems arising bayesian analysis factorial experiments 
brief reversible jump mcmc random sweep metropolis hastings method metropolis hastings adapted general state spaces 
letting denote state variable application complete set unknowns fi oe dx target probability measure posterior distribution consider countable family move types indexed 
current state move type destination proposed joint distribution essentially arbitrary measure dx 
probability dx move attempted 
move accepted probability ff min ae dx dx dx dx oe ratio measures rigorous definition ratio radon nikodym derivatives respect suitably chosen common dominating measure 
existence measure ensured dimension balancing condition dx effectively matches degrees freedom joint variation state proposal dimension changes move type change dimension parameter expression reduces familiar metropolis hastings acceptance probability ordinary ratio densities hastings dimension changing moves little care needed 
typical case concrete form 
suppose move type proposed point higher dimensional space 
implemented drawing vector continuous random variables independent setting invertible deterministic function 
reverse move accomplished inverse transformation proposal deterministic 
acceptance probability reduces min ae jy xjy fi fi fi fi fi fi fi fi oe probability choosing move type state density function note final term ratio jacobian arising change variable reversible jump moves normal mixtures reversible jump mcmc simply derived mathematically random scan form usual metropolis hastings methods idea equally valid available moves scanned systematically approach chosen take 
hierarchical normal mixture model move types updating weights updating parameters oe updating allocation updating hyperparameter fi splitting mixture component combining birth death empty component 
moves involve changing making necessary corresponding changes oe 
randomness scanning random choice splitting combining move birth death move 
complete pass moves called sweep basic time step algorithm 
mcmc algorithms full conditional distributions variables deriving helpful consult conditional independence graph system derived dag dropping arrows lauritzen spiegelhalter 
model graph displayed note variables oe conditionally independent similarly fi 
moves performed parallel 
move types conventional largely diebolt robert alter dimension complete parameter vector fi oe give detail 
conjugacy full conditional distribution weights remains dirichlet form wj delta delta delta ffi ffi fi jg delta delta delta denote conditioning variables 
updated gibbs move sampling full conditional drawing independent gamma random variables scaling sum 
full conditionals delta delta delta oe gamma oe gamma oe gamma gamma order preserve ordering constraint full conditional generate proposal accepted providing ordering unchanged 
full conditionals foe oe gamma delta delta delta gamma ff fi gamma allocation variables jj delta delta delta oe exp gamma gamma oe hyperparameter treating fixed fi gamma distribution fij delta delta delta gamma kff oe gamma variables gibbs kernel 
split combine move reversible jump mechanism needed 
recall need design moves tandem form reversible pair 
strategy choose proposal distributions informal considerations suggesting reasonable probability acceptance strictly subject requirement dimension matching 
having done conformation detailed balance condition determined acceptance probability 
point statistical model quantitatively 
move random choice attempting split combine probabilities gamma respectively depending course kmax max maximum value allowed choose max gamma 
combine proposal begins choosing pair components random adjacent terms current value means interval components merged reducing 
doing forming new component labelled reallocate observations create values oe 
reallocation simply done setting parameters assigned expedient matching th st nd moments new component combination replaces oe oe oe combine proposal deterministic discrete choices expression acceptance probability relevant 
reverse split proposal largely determined 
component chosen random split labelled weights parameters conforming 
degrees freedom achieving need generate dimensional random vector specify new parameters 
beta distributions set gamma gamma oe oe oe gamma oe oe gamma gamma oe provide required weights parameters satisfying 
readily shown valid weights variances positive 
point check adjacency condition satisfied 
move rejected split combine pair reversible 
test passed remains propose reallocation done analogously standard gibbs allocation move see equation 
acceptance probabilities split combine moves calculated quite convoluted form 
split move probability min likelihood ratio theta theta theta ffi gamma ffi gamma ffi gamma ffi theta exp gamma gamma gamma gamma gamma theta fi ff gamma ff oe oe oe gammaff gamma exp gammafi oe gamma oe gamma gamma oe gamma theta alloc theta fg gamma theta gamma joe oe gamma gamma oe number components split numbers observations proposed assigned delta delta beta function alloc probability particular allocation denotes beta density likelihood ratio ratio product terms new parameter set old 
corresponding combine move acceptance probability min gamma expression obvious differences substitutions 
correspondence fairly straightforward lines form ratio jy xjy factor line ratio order statistics densities parameters oe 
fourth line proposal ratio final line jacobian transformation oe oe oe 
birth death move somewhat simpler 
random choice birth death probabilities 
birth weight parameters proposed new component drawn gamma oe gamma gamma ff fi space new component existing weights re scaled weights sum gamma 
death random choice existing empty components chosen component deleted remaining weights re scaled sum 
changes proposed variables particular allocations unaltered 
detailed balance holds move provided accept births deaths oe play role prior distributions proposing values oe leads simplification resulting ratio 
acceptance probabilities birth death min min gamma respectively ffi ffi gamma gamma gammak theta gamma number components number empty components birth 
line prior ratio second line contains proposal ratio jacobian likelihood ratio 
completes specification moves claim optimal scheme tried 
validity algorithm compromised choice proposals detailed balance confirmed 
metropolis hastings methods rarely worth fine tuning proposal distribution especially doing prevents simple explicit random variate generation 
detailed balance satisfied remains check markov chain defined irreducible aperiodic 
clear arbitrarily small neighbourhood current state fi oe positive probability sweep moves chain lies neighbourhood 
irreducibility easily established chain move value value steps time move allocations positive probability parameters hyperparameters updated drawing continuous distributions supports natural parameter spaces 
statistical performance proposed methodology inter linked aspects proposed methodology demonstrated illustrating performance 
course display examples results obtain real data sets 
presentation substantive results inevitably associated questions sensitivity model specification especially regarding prior questions performance mcmc sampling method mixes 
aspects results sensitivity mixing interact closely 
avoid circularity presentation postpone discussion sensitivity mixing sections respectively description performance model default settings hyperparameters 
course sensitivity issues informed choices 
results data sets real data sets basis comparisons concerns distribution activity blood enzyme involved metabolism carcinogenic substances group unrelated individuals 
interest identifying subgroups slow fast marker genetic polymorphism general population 
data set analysed 
identified mixture skewed distributions maximum likelihood techniques implemented program maclean 

shall refer data set enzyme data 
second data set data concerns index measured sample lakes northeastern united states previously analysed mixture gaussian distributions log scale crawford log scale 
data set galaxy data described roeder subsequently analysed different mixture models authors including escobar west phillips smith 
consists velocities distant galaxies diverging galaxy 
histograms data sets shown 
data sets analysed hierarchical normal random fi mixture model defined sections settings previously unspecified constants ff ffi 
prior taken uniform integers max particularly easy convert results corresponding priors values identity jy jy denotes posterior alternative prior data sets report results corresponding sweeps burn period sweeps 
believe numbers exceed needed reliable results 
runs number components exceeded chosen value max inconsequential 
estimated posterior probabilities table 
data sets immediately apparent number competing explanations data tenable 
enzyme data posterior favours components 
example proviso prior evidence enzyme level normally distributed interpret existence components mixture terms simple underlying genetic model 
data fairly equal support components galaxy data posterior widely spread indicates higher number components 
case high number components related part skewness data normals needed fit skewed component mixture model imposes little structure priori contrast considered roeder wasserman 
data sets obtained world wide web www stats bris ac uk peter table posterior distribution data sets mixture model random fi default parameter values 
proportion data set split birth combine death moves accepted enzyme galaxy range default parameter values enzyme data ff ffi data ff ffi galaxy data ff ffi data density 
posterior distributions enzyme data sample posterior distribution galaxy data 
product implementation investigate changes posterior distribution gamma log yjk increasing enzyme data marked shift onwards substantial overlap deviance distributions see 
similar pattern emerges data sets substantial overlap data galaxy data 
predictive densities sweep algorithm values weights parameters produced densities deltaj computed 
posterior variation realised displayed enzyme data 
averaging mcmc run conditional fixed values gives estimate jk bayesian predictive density estimate mixture components 
averaging values gives estimate jy bayesian predictive density estimate distribution note density estimates shape finite mixture distributions 
density estimates particular mixtures considered summary posterior estimates weights parameters components mixture 
case posterior distribution fairly spread multimodal plug estimates give poor approximation predictive density 
density enzyme data density data density galaxy data predictive densities data sets unconditionally full line conditional various values dotted lines curves displayed galaxy data 
case note smallest shown gives appreciably different estimate 
predictive densities conditional unconditional shown data sets 
note difference successive predictive densities decreases increasing values unconditional plot gives convincing density estimate data distribution 
predictive fit posterior distribution give complementary evidence draw assessing number components 
enzyme data predictive plots components similar 
aim analysis data set identification interpretable subpopulations favour mixture components 
parameter estimates labelling postprocessing mcmc output ways natural consider parameters set labelling sweep convenient necessary density estimates summaries posterior distribution parameters component required 
appropriate labelling depend example analysed substantial bonus sample computation method investigated run algorithm 
weights density weights density weights density weights means density means density means density means sd density sd density sd density sd labelling ordering means labelling ordering variances posterior density parameter estimates choices labelling simulated data set 
understand issue labelling straightforward consider case population really normal components unambiguously labelled 
finite sample posterior distribution means overlap similarly weights variances extent overlaps depends separation sample size 
means separated labelling realisations posterior ordering means generally coincide population labelling separation reduces called label switching occur see mengersen robert 
depending relative separations label switching minimised choosing order variances weights combination parameters 
illustrate points simulated data set points drawn mixture gives skewed unimodal distribution oe oe 
displays posterior densities oe runs left hand side labelling corresponding ordering means right hand side labelling corresponding ordering variances 
labelling variances right hand side leads bimodal densities corresponds label switching half runs 
labelling ordering means gives clearer unimodal plots simultaneously parameters evidence switching 
real data set obvious choice labelling 
advisable postprocess run different choices labels order get clearest picture component parameters 
multimodality parameter densities quite apart labelling problem cases genuine multimodality posterior distribution parameter estimates corresponding different mixture models competing potential explanations data set 
weights weights means means enzyme data set posterior densities weights means second third component default prior model conditioning full line conditioning dotted line broken line 
areas proportional posterior probability 
example display full line posterior densities weights means second third component enzyme data labels ordering means 
evidence distributions weights second third components 
different labelling help clarify picture 
attempt exhibit possible competing explanations separate mcmc output runs corresponding plot parameter densities dotted lines 
produces clearly peaked posterior densities parameters show low values associated elevated values fact separation show small group corresponds indicating small fraction individuals high activity expected 
sensitivity results prior assumptions hierarchical approach mixture modelling involves hyperparameter specifications 
carried depth study influence led standard default recommendations previous section 
section highlight important aspects sensitivity analysis 
emphasise recommend study performed standard implementation approach 
sensitivity posterior distribution comparison prior models variance fixed versus random fi fixed fi random fi posterior distributions comparison sensitivity hyperparameters fixed random fi models ff fi ff varying ff hff varying 
main concern show model number components related prior information variances oe standard non hierarchical model fixed fi ff mean gamma distribution ff fi specifies typical value precision oe gamma natural relate oe range data increasing values fi ff lead models fewer components 
show data substantial change posterior distribution fi ff varied 
little overlap posterior distributions extreme cases 
standard model choices ff fi crucially influence posterior distribution difficult weakly informative 
contrast hierarchical model fixed ff random fi implemented allows weak information oe put higher level exhibit behaviour 
posterior distribution quite insensitive wide range values ratio related range lead similar posterior mean standard deviation fi 
illustrated shows strikingly similar posterior distributions sets values ff chosen prior order magnitude oe higher level hff ranges 
hierarchical formulation table influence prior distribution gamma posterior distribution data mixture model poisson prior random fi default parameter values 
gamma range highest oe gamma gamma fi fi gamma hff variance distribution mixture model allows high degree non informativeness 
view results choose hierarchical random fi model ff hff default option 
sensitivity prior distribution means important component mixture model prior model means defined drawn independently normal distribution gamma 
extremes data consider setting equal midrange precision gamma equal sensible weakly informative prior places effectively constraint location encourage fitting mixtures close subtle interplay prior information location means number components 
reducing gamma tend favour higher number components 
interpreted result defining prior means increasingly permissive components close means 
hand gamma reduced number components start decrease shrinkage effect active prohibition components means located extremes range 
illustrate points data 
hierarchical default option variances poisson prior hyperparameter settings encourage large values gamma decrease number components highest posterior probability increases reach peak value gamma decreases table 
far discussed sensitivity prior setting hierarchical random fi model component variances behaviour observed fixed fi model 
sensitivity discussed crawford computes data posterior distribution cases ff fi ff fi ff fi 
note restriction imposed means quite severe cases corresponding respectively gamma equal data set 
simultaneously increasing ff fi means components restricted standard deviations tightened fairly large value creating competing influences easy disentangle 
fitted model parameter settings crawford 
find support components previous analysis default priors cf 
table posterior distribution concentrated moderate variation cases jy equal respectively 
laplace approximation estimates jy crawford table vary orders magnitude 
sensitivity posterior distributions parameters complementary way mcmc runs investigate sensitivity posterior distributions component parameters various values shall briefly summarise features 
results concerning influence unsurprising 
expected reduction range means observed gamma decreased phenomenon noticeable large interesting compare influence prior specifications variances oe gamma gamma ff fi fixed fi random fi gamma model 
similar sensitivity pattern described posterior distribution emerges 
fixed fi case posterior means oe sensitive variations fi ff larger 
example data posterior means oe nearly halved fi ff varied 
random fi case posterior means oe remarkably similar 
evidence value including upper hierarchical level distribution oe mixture model 
performance mcmc sampler mixing performance jump moves sweep galaxy sweep galaxy sweep enzyme sweep example trace galaxy data set sweeps burn cumulative occupancy fractions data sets complete run including burn 
essential element performance mcmc sampler ability move different values plot changes number sweeps galaxy data 
shows mcmc algorithm mixes excursions high values short lived 
similar plots obtained data sets 
proportions accepted split combine moves vary table 
dimension changing moves proportions satisfactory show proposal adjacency sensible 
useful check stationarity plot cumulative occupancy fractions different values number sweeps 
represented data sets seen burn adequate achieve stability occupancy fractions 
model preclude existence empty components included count cause concern high number persisted long times 
including algorithm birth death moves specifically deal empty components improves convergence comparison algorithm relying split combine moves especially posteriors diffuse 
acceptance rate birth death moves highest small multimodal galaxy data set 
mean number empty components equal enzyme galaxy data sets respectively 
detected influence starting values distribution example enzyme data starting typically leads acceptance split sweeps accepted starting observations ranked equally allocated components sweeps needed reach 
specified runs start 
prior distribution appears acceptance ratio dimension changing moves influence mixing behaviour precisely ratio 
priors highly concentrated range values effectively algorithm accepting moves take outside range 
bayes factors fp jy jy xi fp theoretically independent mcmc estimates materially affected range visited reasonably 
example data estimated prior poisson distribution mean equal respectively uniform prior 
mixing mixing parameters range weak priors observed satisfactorily mixing patterns runs encountered trapping states reported robert 
checked runs different initial allocations gave identical posterior densities parameters 
enzyme data components displays typical time plots sweeps oe 
run sweeps included visits plotted sake clarity plots 
different pattern traces components seen 
component lowest mean standard deviation highest weight estimated precisely 
occasionally fewer observations allocated creates problem 
note occurs mean second component dips switching arises components 
weights second third components fluctuating 
third component competing explanations discussed earlier section clearly visible algorithm trouble covering wider range values higher means corresponding lower weights standard deviations 
sweep weights weights means enzyme data sweep means sweep sd traces parameter estimates visits enzyme data 
comparisons fixed sampler previous mcmc mixture estimation fixed encountered slow mixing especially weak priors 
usually caused presence modes posterior distribution separated far available mcmc moves concerned regions low probability 
statistical terms supported explanations data example data may fall separated clusters fixed may substantial posterior probability components fitted cluster second vice versa 
plausible presence multimodality mixing improved possibility varying particular situation described sampler stage combine components cluster subsequently split component second cluster complete transition mode visiting regions low posterior probability 
example physicists call tunnelling regions low energy energy negative logarithm probability 
example improved mixing obtained varying example somewhat contrived believe qualitatively similar real problems clustered data absence strong prior information 
take observations assemble synthetic data set size data points reflections origin 
default prior poisson prior 
situation joint posterior distribution possesses exact symmetry reflection 
compare results simulating joint posterior variable conditioning running fixed sampler moves section 
run lengths arranged numbers visits case 
results displayed 
symmetry true posterior 
sweep 
sweep sweep variable sampler fixed sampler comparison mixing variable fixed samplers 
left panels traces sweep number 
right panels upper posterior density estimates runs lower sequences estimates jy obtained runs proceed solid lines refer variable sampler 
jy symmetric particular jy 
panels left show traces sweep number 
variable sampler evidently mixes far better fixed 
improvement extends estimates density probability positive illustrated right hand panels 
sweeps results fixed sampler show severe asymmetry 
bayesian classification apart role facilitating computation allocation variables interest right form coherent basis classification observations 
care required interpreting sensibly 
rarely appropriate discuss classification conditional labels meaningful context particular declared unambiguous labelling mixture components example ordering classification done sample predictive basis 
posterior probabilities fp jjy kg appropriate directly estimated empirical averages mcmc run 
predictive classification addresses question classifying observation say 
allocation variable corresponding principle interested fp jjy unfortunately inclusion additional datum changes posterior distributions apparently requiring mcmc sampler re run new obviously impractical employ obvious approximation jjy jjy dw jjy dw jjy dw estimate integral expectation respect mcmc empirical average case oe oe oe oe oe delta oe normal density 
estimated sample predictive classification probabilities illustrated enzyme data set 
lower section panel shows cumulative classification probabilities jjy jjy gamma differences adjacent curves indicate class probabilities 
sample probabilities predictive ones coincide plotting accuracy 
effect law large numbers computed separately 
usual percent correctly classified loss function bayes classification existing observation respectively argmax jjy argmax jjy plotted upper part panel 
note monotonicity classification respect increasing values enzyme level 
data values classified third component lie side assigned second component 
due large variance third component 
correspondingly predictive curve delimiting second third component pronounced dip 
phenomenon persists larger indicating classifications simply interpreted terms shifts mean enzyme level take consideration combination mean spread 
data probability data probability data probability classifications enzyme data set sample crosses predictive solid curves classification probabilities optimal classifications 
discussion mcmc issues key idea constructing effective mcmc sampler design sensible moves current knowledge mixture 
basing moves notion adjacency generic character independent particular distributional assumptions move types adapted variety distributions 
believe suitable parameter density deltaj suitable transformation term mean variance parameter range mean unconstrained 
extended birth death moves include nonempty components similarly phillips smith dispensed split combine moves defined combine moves respect underlying partition data parameters line followed 
felt moves efficient developments aim perform comparisons 
moves contemplated example moves combine components take components simultaneously update relevant parameters weights allocations 
evidence results complications necessary 
adjacency preserving moments quite natural conditions onedimensional setting moves created restricted mixing applications bivariate multivariate mixtures 
feel algebra moves need extended 
calculating acceptance ratios dimension changing moves term cumbersome jacobian transformations 
higher dimensional parameter space symbolic computation tools useful point 
calculations simplified preserving symmetry possible definition moves 
mcmc sampler may convey impression computationally demanding 
fact burden excessive 
largest real data sets enzyme data default prior setting program sweeps second sun sparc workstation 
validation mcmc code important concern 
compared results analytic calculations small data sets correspondence 
checked data estimate joint posterior distribution tallies chosen prior 
presentation posterior distributions extracting information complex multidimensional posterior distribution challenge 
whilst mcmc methods circumvent restrictions conventional numerical methods provide raw information insightful disciplined summaries needed adapted particular context 
exploited opportunities involving relabelling predictive densities classification new summaries especially regarding joint parameter distributions developed 
postprocessing especially useful view inherent identifiability problems connected mixture estimation contrast variability parameter estimates plots representing competing explanations stability predictive density plots 
approach revealed clear evidence multimodality skewness posterior distributions features presence unsurprising view small numbers observations allocated components 
believe situation analytic approximations laplace misleading 
prior structures interaction model number components terms structural functional characteristics prior discussed illustrated extensively 
believe hierarchical prior structure introduced useful examples 
certainly black box procedure 
default choice hyperparameters aimed mixtures analysing heterogeneity semiparametric density estimation 
relationship prior distribution means posterior expected sensitivity values considering independent natural model notion separation means explicitly dependent priors 
example feature built joint prior distribution oe computational approach available 
dependent priors component parameters considered authors attempting noninformative mixture context 
essentially entails linking components global parameters flat priors assigned data points contribute estimation 
related distinct approaches line taken robert mengersen robert 
scaling respect component largest variance roeder wasserman placing markov priors means 
generalisations model related models extended mcmc sampling strategy escobar west mixture model dirichlet process prior 
hierarchical structure somewhat different equation range moves needed broadly elegant algebraic structure dirichlet process model facilitates evaluations needed implement split combine move 
approach far implemented normal mixtures interpretation number components conditional appropriate distribution subpopulations 
take phenotypic data enzyme data assumption normality supported biological considerations 
maximum likelihood procedure analysing mixtures maclean allow different degrees skewness box cox transformation original analysis enzyme data concluded data fitted highly skewed components 
extension considering development framework variable number components variable skewness mixture distribution simultaneously considered 
straightforward extension consider mixtures discrete distributions model commonly non parametric estimation usually estimated em algorithm separately different numbers mass points 
emphasise flexibility modelling incorporating extensions arise mixture estimation different application contexts 
particular aim consider problems involving constraints weights genetic analysis modelling component means terms covariates mixtures robust prior modelling bayesian analysis modelling unknown exposure distributions measurement error problems 
wish jim berger ed george agostino christian robert kathryn roeder duncan thomas larry wasserman stimulating discussions catherine introducing genetic applications mixture estimation pierre providing enzyme data set christine assistance computations referees suggestions improved presentation 
acknowledge financial support epsrc complex stochastic systems initiative sr esf network highly structured stochastic systems 
poisson 
population family study caffeine clinical pharmacology 
besag green higdon mengersen 
bayesian computation stochastic systems discussion statistical science 
celeux diebolt 
stochastic versions em algorithm experimental study mixture case journal statistical computation simulation appear 
crawford degroot kadane small 
modeling lake chemistry distributions approximate bayesian methods estimating finite mixture model 
technometrics 
crawford 
application laplace method finite mixture distributions journal american statistical association 

estimation order mixture 
pr de universit paris sud math ematiques 
diebolt robert 
estimation finite mixture distributions bayesian sampling 
journal royal statistical society 
escobar west 
bayesian density estimation inference mixture journal american statistical association 
green 
contribution discussion grenander miller 
journal royal statistical society 
green 
reversible jump markov chain monte carlo computation bayesian model determination 
biometrika 
grenander miller 
representations knowledge complex systems discussion 
journal royal statistical society 
robert wolpert 
estimating number components normal mixture 
technical report universit de 
hastings 
monte carlo sampling methods markov chains applications 
biometrika 
lauritzen spiegelhalter 
local computations probabilities graphical structures application experts systems discussion 
journal royal statistical society 
lindsay 
mixture models theory geometry applications 
nsf cbms regional conference series probability statistics vol 

institute mathematical statistics hayward california 
mclachlan basford 
mixture models inference applications clustering marcel dekker new york 
maclean morton yee 
skewness distributions 
biometrics 
mengersen robert 
testing mixtures bayesian entropy approach 
bayesian statistics berger bernardo dawid lindley smith eds 
oxford oxford university press press 
metropolis rosenbluth rosenbluth teller teller 
equations state calculations fast computing machines 
journal chemical physics 

bayesian analysis finite mixture distributions ph thesis carnegie mellon university 

optimum monte carlo sampling markov chains 
biometrika 
phillips smith 
bayesian model comparison jump diffusions chapter pp 
practical markov chain monte carlo gilks richardson spiegelhalter eds 
chapman hall london 
raftery 
hypothesis testing model selection chapter pp 
practical markov chain monte carlo gilks richardson spiegelhalter eds 
chapman hall london 
robert 
mixtures distributions inference estimation chapter pp 
practical markov chain monte carlo gilks richardson spiegelhalter eds 
chapman hall london 
roeder 
density estimation confidence sets exemplified voids galaxies journal american statistical association 
roeder wasserman 
practical bayesian density estimation mixtures normals 
technical report department statistics carnegie mellon university 
tierney 
markov chains exploring posterior distributions annals statistics 

titterington smith makov 

statistical analysis finite mixture distributions 
wiley chichester 

