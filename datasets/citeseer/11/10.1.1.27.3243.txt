bayesian methods hidden markov models recursive computing st century submitted jasa reviews steven scott november markov chain monte carlo mcmc sampling strategies simulate hidden markov model hmm parameters posterior distribution observed data 
mcmc methods computing hmm likelihoods conditional probabilities hidden states sequence states practice improved incorporating established recursive algorithms 
important set forward backward recursions calculating conditional distributions hidden states observed data model parameters 
show recursive algorithms mcmc context demonstrate mathematical empirical results showing gibbs sampler forward backward recursions mixes rapidly sampler hmm 
introduce augmented variables technique obtaining unique state labels hmm finite mixture models 
show recursive computing allows statistically efficient mcmc output estimating hidden states 
directly calculate posterior distribution hidden chain state space size mcmc circumventing asymptotic arguments underlying bayesian information criterion shown inappropriate frequently analyzed data set hmm literature 
log likelihood assessing mcmc convergence illustrated posterior predictive checks investigate application specific questions model adequacy 
key words forward backward kalman filter mcmc gibbs sampler recursion local computation assistant professor statistics marshall school business university southern california 
email sls usc edu hidden markov model hmm mixture model mixing distribution finite state markov chain 
hmm successfully applied problems variety fields including signal processing juang rabiner andrieu doucet biology fredkin rice puterman genetics churchill liu ecology guttorp image analysis romberg economics albert chib hamilton network security scott 
book macdonald illustrates applications hmm provides extended bibliography 
history hmm implemented recursive algorithms developed parameter estimation baum restoring hidden markov chain viterbi 
today markov chain monte carlo mcmc techniques allow researchers implement hmm traditional recursive algorithms viewed black boxes statisticians 
contrary perception recursions intuitive probabilistic interpretations improve mcmc methods 
recursive computing hmm users take advantage rapidly mixing mcmc algorithms mcmc output efficiently estimate hidden chain employ model selection techniques convergence diagnostics unavailable 
recursive algorithms appeal aesthetic says relying simulation produce directly computable answers time consuming inelegant needlessly inaccurate generally idea 
article reviews common recursive algorithms hmm explains role modern mcmc environment 
article structured important tasks facing bayesian modeler obtaining posterior distribution model parameters estimating hidden markov chain determining size hidden chain state space diagnostics assess model validity mcmc convergence 
section provides background hmm including closely linked recursive procedures evaluating hmm likelihood likelihood recursion posterior distribution hidden state observed data model parameters forward backward recursions 
section discusses methods sampling hmm parameters posterior distribution observed data particular emphasis gibbs samplers 
forward backward gibbs sampler fb capitalizes recursive techniques section 
direct gibbs sampler dg samples state hidden markov chain draws neighbors 
fb requires computer time iteration mixes rapidly dg produces useful quantities 
section discusses label switching issue hmm share finite mixture models 
section describes bayes empirical bayes methods estimating hidden markov chain 
section considers methods determining size hidden chain state space 
section uses likelihood recursion diagnose mcmc convergence illustrates posterior predictive checks determine model capacity produce specific features seen data 
section summarizes 
background hidden markov models assume distribution observed data point depends unobserved hidden state gamma 
elements follow markov chain stationary transition matrix initial distribution taken stationary distribution formally jh gamma gamma gamma gamma 
hmm require exist physical sense model compelling hidden states physical interpretation 
sections illustrate scientific insight simplify difficult problems choosing preventing label switching selecting model diagnostics 
full conditional distribution jd gammat gammat fd tg parameter vector probability distributions gamma function usually treated separate parameter 
words says conditionally independent missing observed data model parameters concept illustrated 
hmm closely related known classes models 
finite mixture models titterington everitt hand hmm rows equal 
state space models west harrison assume follows gaussian process 
markov modulated graphical depiction hmm dependencies 
conditional distribution value node values nodes depends nodes connected edge 
poisson process scott extends hmm continuous time 
clarity exposition restrict attention model defined note generalizations appropriate 
examples illustrate notation indicate ways usefully expanded 
example fetal lamb movements popular data set hmm literature record fetal lamb movements consecutive second intervals puterman replicated 
hidden state describes fetal activity level interval activity levels usually assumed somewhat active 
fetal activity category evolves stationary markov chain transition matrix number movements interval poisson mean example business cycle hamilton models gnp arima process evolving means determined hidden markov chain 
hidden state indicates economy expanding contracting time contains arima parameters mean gnp growth rates economic expansions contractions 
memory underlying business cycle captured example links added top part reflect dependence observed data example network intrusion scott uses hmm detect criminal intrusion telephone account 
observed data placement time th telephone call account augmented observed call characteristics 
customer responsible account generates traffic possibly non homogeneous poisson process 
criminals break leave account state continuoustime markov process 
criminal generates additional traffic interval number movements time fraud score number fetal lamb movements consecutive second intervals 
output fraud detection system telephone account 
vertical axis score indicating closely telephone call matches customer historical calling pattern 
higher scores indicate unusual calls 
time days jan 
second poisson process independent customer 
processes homogeneous data account may represented discrete time hmm 
hidden state indicates fraud status th call criminal presence absence th interval calls 
parameters functions calling rates customer criminal parameters describing distribution call characteristics customer criminal rates criminals break leave system 
returning general case extend include arbitrary vectors write 
likelihood function hmm observed data gamma sum elements quickly infeasible compute small values grows moderate size 
method evaluating needed statistical procedures depend likelihood including likelihood ratio testing bayesian model selection certain metropolis hastings samplers 
sections describe recursive procedures developed sidestep computational difficulties posed 
section explains technique quickly evaluating hmm likelihood 
section discusses related set forwardbackward recursions originally developed implement em algorithm maximizing 
slightly modified forward backward recursions lead gibbs sampler central rest article 
likelihood recursion likelihood recursion procedure calculating steps steps suggested direct evaluation 
define forward variable 
words joint probability density event fh sg 
likelihood contribution gamma yielding likelihood recursion computes gamma sg follows 
gamma gamma gamma rj gamma gamma computing requires sum quantities 
single step recursion evaluating indicated 
chib observes unstable calculates likelihood log likelihood stable modifications exist 
define sj sjd max log fp gamma rj easy show log obeys recursive relationship log log gamma log gamma exp log log gamma gamma rj gamma equation scales component log sj exponentiation prevent computer overflow increments log scaling factor log normalizing constant sj 
forward backward recursions forward backward recursions developed baum 
implement em algorithm maximizing 
recursions produce backward variable sj pr sjd needed step em algorithm 
readers familiar state space models note correspondence forward backward recursions prediction smoothing steps kalman filter kalman 
forward recursion accumulates information distribution moves hidden markov chain 
backward recursion updates distribution calculated forward step information collected observed data 
forward backward recursions traditionally derived forward recursion 
alternative representation generates matrices trs trs gamma sjd 
words joint distribution gamma model parameters observed data time computes gamma trs gamma jd gamma gamma rj proportionality reconciled trs 
notice sj trs computed known setting step recursion 
calculating margin equivalent calculation log likelihood stably computed byproduct forward recursion 
need perform separate likelihood calculation 
backward recursion replaces trs conditional distribution gamma model parameters note conditions observed data conditions data observed time clearly obtains bayes rule 
trs gamma sjd gamma sj trs sj sj sj computed appropriate margin focusing forward backward matrices forward backward variables helps strip away mystery algorithm 
hmm understood general theory graphical models cowell graph understood sense 
marginal distributions variables graphical model efficiently calculated local computation algorithm junction tree dawid describing relationships cliques graph 
clique set variables members neighbors graph cliques transitions gamma completed data 
local computation algorithm operates calculating marginal distributions cliques graph 
marginal distributions individual variables cliques may derived low dimensional sums integrals 
junction tree hmm chain delta delta delta represent cliques marginalized 
general forward recursion graphical models calculates marginal distribution clique recursively averaging marginal distribution parents junction tree 
backward recursion updates distribution clique information accumulated forward step ensuring marginal distributions joint distribution variables model 
see cowell 
details general algorithm 
forward backward recursions extensions hmm easily derived general local computation method graphical models 
computational gain recursions depends size cliques 
simple example suppose obeys second order markov chain jh gamma jh gamma gamma 
set forward backward recursions immediately available expanding array order 
see scott romberg 
examples modified forward backward recursions applied complicated graphs 
posterior sampling mcmc section discusses mcmc methods sampling posterior distribution hmm missing data structure naturally admits posterior samplers alternate simulating simulating complete data 
obvious way sample direct gibbs dg albert chib robert titterington robert 
dg draws full conditional distribution gammat gamma appropriate adjustments effects 
paired gibbs metropolis hastings steps sampling jd produces sequence markov chain limiting distribution hjd 
forward backward gibbs fb chib scott preferred alternative dg 
fb modifies stochastic version forward backward recursions sample directly hj 
modification leads rapidly mixing algorithm fewer components introduced gibbs markov chain 
forward recursion fb produces exactly section 
stochastic backward recursion begins drawing deltaj recursively draws distribution proportional column understand stochastic backward recursion factor hj hjd jd gamma gammat jh gammat notice may help gammat gammat gammat gammat gammat gammat stochastic backward recursion replaces section explains combining backward recursions leads improved estimates easy implement stochastic non stochastic backward recursions simultaneously forward recursion run 
recursive computing opens door mcmc procedures impossible 
example consider metropolis hastings sampler metropolis hastings proposing candidate values multivariate normal distribution centered close current draw promoted depending hastings probability determined relative likelihood jd candidate distribution 
sampler avoids drawing altogether likelihood recursion integrate calculating hastings probabilities 
improved candidate distributions may obtained modifying likelihood recursion produce derivatives log likelihood suitable langevin diffusion gilks roberts 
metropolis hastings procedures average useful celeux reasons prefer sampling integrating 
metropolis hastings algorithms tend perform poorly dimension large 
candidate draws high dimensional problems rejected variance candidate distribution small case algorithm moves slowly parameter space 
usual remedy partitioning applying smaller metropolis hastings algorithms elements partition unattractive hmm partitioning requires separate run likelihood recursion hastings probability computed 
highly correlated elements jd nearly independent jd including sampling algorithm provides expanded parameter space may accelerate mixing respect efficiently drawn 
idea augmented variables samplers spatial statistics damian higdon besag green 
function object scientific interest sampled may inference diagnostics model adequacy mcmc convergence robert 
subsections focus fb dg exclusion sampling schemes 
particular omit discussion approximate sampling methods involving mcmc liu chen 
section explains fb mixes rapidly dg drawing hjd 
section discusses label switching issue dealt drawing jd 
label switching problem regardless sampling scheme 
section uses model example illustrate method preventing label switching compare fb dg 
autocovariance fb dg samplers fb reduces dependence hidden states drawn previous gibbs iterations sampling directly hjd 
faster mixing translates faster mixing analogy duality principle introduced diebolt robert finite mixture models 
fb sampler dg sampler conditional independence graphs fb dg samplers 
conditional distribution variable quantities depends variables connected edge 
dg depends gamma gamma gamma section shows autocovariance complete data sufficient statistics drawn dg fb autocovariance plus penalty term 
penalty term tends positive increases posterior covariance hidden states 
theory special case liu 
terminology fb grouped dg sampler 
assume jh depends vector complete data sufficient statistics dependence henceforth suppressed section 
interpretation purposes imagine vector indicator variables describing gamma notation allows greater generality 
fb dg produce sequences markov chains stationary distribution 
fb conditionally independent gamma dg satisfies weaker assumption conditionally independent gamma ft gamma jg ft jg see 
fb dg achieved stationarity easily established gamma marginal distribution 
follows cov gamma phi cov gamma psi cov phi gamma psi phi cov gamma psi ar phi psi cov ar defined sampler 
term zero fb 
second sampler fb dg stationary distribution 
consequently gamma cov fb gamma edg phi gamma psi words says autocovariance complete data sufficient statistics direct gibbs fb autocovariance plus penalty term 
understand penalty term notice gamma decomposes gamma gamma gamma gamma ut lt notation ut lt indicates sums upper lower triangle gamma 
shown ut nonnegative definite autocovariance dg tends greater fb 
furthermore gamma cov dg performs worst elements highly related posterior distribution 
observation mathematically expresses known feature dg spatial statistics higdon besag green 
hard dg move configuration strong dependence neighbors 
label switching state collapsing prior modeling sampling complete data posterior trivial drawn draw complicated identifiability issue known label switching 
term label switching describes fact hmm likelihood invariant arbitrary permutations state labels 
literature label switching works context finite mixture models hmm face issues 
consider case 
swap values relabel points currently category category vice versa 
complete data likelihood achieves exactly value new labels old 
exchangeable prior distribution marginal posterior densities identical 
label switching obstacle time diff log gnp parameter parameter quarterly differences log gnp 
posterior distributions mean parameters state gaussian hmm applied gnp series 
panels assume prior distribution gamma df ss estimated em algorithm df ss chosen prior mean comparable sample variance 
panels truncate maximum minimum data values 
prior distribution right panel constrains unconstrained parameters left panel exhibit label switching 
hmm regardless sampling strategy 
effect seen shows posterior density estimates mean parameters state gaussian hmm deltaj oe applied economic time series 
model simplification model discussed example 
data contain information order state labels labels may identified posterior distribution assumptions prior 
example common assume obeys set constraints violated permuting state labels 
typical constraints involve ordering means variances mixture components 
parameter constraints enforced explicitly choosing prior zero mass regions ordering violated 
priors lead metropolis hastings algorithms reject draws violating constraint richardson green 
constraints enforced model robert titterington robert 
care needed choosing parameter constraints priors truncate support informative enforcing constraints produce different model 
celeux 
provide examples ordering different sets model parameters produces noticeably different posterior means 
note weakly informative prior little prevent label switching 
scientific insight may provide useful prior information suggesting order 
scientific insight prior information discussed terms mean concrete 
example model gnp intended associate economic expansions 
nonsensical removed consideration 
similar logic applied circumstances long physical interpretation 
selecting appropriate prior distributions help curb state collapsing special case label switching observations allocated particular state gibbs iteration 
parameters collapsed state drawn prior distribution weak priors produce draws far values justified data 
hierarchical priors allowing elements borrow strength help sampler recover collapsed state 
priors constraining mean parameters lie range data helpful 
frequent state collapsing evidence model parameterized 
variable dimension monte carlo methods discussed section help deal collapsed states allowing state die sensible location 
scientific insight available cases 
example hmm fit obtain predictive distribution cluster observations time series intuition exists source clustering 
stephens proposes method identifying mixture components prior information unavailable 
stephens method runs mcmc identifiability constraints sample posterior distribution containing 
symmetric modes 
mcmc run method searches permutation state labels leading marginal distributions minimize specified loss function 
loss functions developed promote unimodal marginal distributions efficient clustering example constrained markov poisson hmm section uses constrained markov poisson hmm poisson delta delta delta gamma illustrate general method ordering variance parameters hidden markov finite mixture models 
method works gamma distributional form invariant addition multivariate gaussian poisson gamma common scale parameter binomial common key view sum contributions various regimes 
regimes active regimes gamma passive contribute sum 
result ordered variance model adding non degenerate random variables increases variance 
gamma contains contribution regime st poisson case define gamma gamma gamma hj gamma gamma nrs gamma dst exp gamman number times regime active rs number transitions assume prior density gamma gamma ja gamma gamma dirichlet densities th row rows elements independent posterior distribution jd gamma st jd rs vector transitions state apply fb note hj hj djh 
fb simulates hj stochastic forward backward recursions 
detailed regime contributions conditionally independent regime higher contribute st full conditional distribution rt multinomial total probability vector proportional 
dg leads trouble full conditional depends sj gammat gamma drt exp gamma gamma rt st places probability events fh rg making difficult sampler move larger smaller values compromise defines missing data time samples nested conditioning 
full conditional factors theta density theta marginal distributions computed fb dashed dg solid samplers state markov poisson hmm applied fetal lamb data 
density estimates gibbs runs iterations dropped 
little difference fb dg density estimates fb dg sample target density 
gammat gammat gammat gammat gammat 
draw sj gammat gammat gamma draw fb 
displays marginal posterior density estimates calculated fb dg state markov poisson hmm applied fetal lamb data 
little difference densities samplers stationary distribution 
difference highlighted shows empirical autocorrelation functions gibbs draws fb dg sampler drawing dg sampler drawing nested conditioning 
illustrates cost introducing unnecessary gibbs components sampling algorithm 
rapidly mixing sampler components 
mixing suffers increases number gibbs components 
includes additional gibbs components highly correlated missing data drawn sampler causing terrible waste computer resources 
computationally dg sn algorithm fb 
dg speed advantage increases 
table shows cpu time needed fb dg generate draws posterior distribution data simulated markov poisson hmm different sample sizes state spaces 
directly comparing cpu time fb dg lag acf lag acf lag autocorrelation functions gibbs draws fb recursions direct gibbs sampling drawn conditional direct gibbs 
draws constrained state markov poisson hmm applied fetal lamb data 
direct gibbs mixes slower algorithms additional missing data 
completely fair fb calculates likelihood associated drawn quantity needed sections 
simulation times shown dg algorithm likelihood recursion 
table illustrates increasing toll forward backward recursions increases 
fb takes longer direct gibbs algorithm computing likelihood 
fb takes twice long 
pairing dg likelihood recursion eliminates dg speed advantage algorithm comfortably fb raw dg 
number states exceptionally large prefer fb superior mixing properties calculates likelihood free part forward recursion allows efficient estimation discussed section 
estimating hidden states estimating central question applied problems fraud detection problem example 
bayes estimates derive posterior distribution hjd high dimensional distribution summarized understood 
applications summarizing hjd marginal distributions pr sjd sufficient 
configuration interesting value individual state 
example dg fb dg fb dg fb table cpu time seconds required generate gibbs draws fb dg samplers applied data simulated markov poisson mixtures length dg algorithm running likelihood recursion 
true parameter values simulation gamma gamma gamma gamma gamma gamma gamma gamma 
gene sequencing liu represent dna base element represents gene speech recognition juang rabiner may phoneme word maximum posteriori map estimation selects maximize hjd ensuring coherent reconstruction hidden chain may differ corresponding reconstruction marginals 
recursive computing improves marginal map summaries hjd discussed sections respectively 
marginal distribution gibbs draws gibbs draws 
obvious estimate forward backward recursions improve rao blackwellization gelfand smith casella robert 
rao blackwellized estimate sj sheds layer monte carlo variability averaging probabilities events simulated probabilities 
calculating requires non stochastic backward recursion produce sj simultaneously running stochastic non stochastic backward recursions probability recession rao blackwellized estimates 
probability recession brute force estimates 
difference sd gamma sd 
probability recession empirical bayes estimates 
marginal probabilities recession gibbs iterations state gaussian hmm applied gnp data 
estimates agree closely official business cycle data www nber org cycles html 
requires little effort forward recursion implemented 
compares empirical bayes probability economic recession state gaussian model gnp data 
shows difference posterior standard deviations gibbs draws 
mcmc output independently sampled standard deviations represented divided produce standard errors estimates figures 
extra variability clearly overcome running sampler longer produces information content iteration virtually costless addition fb sampler 
bayes estimates similar empirical bayes estimate calculated forward backward recursions 
empirical bayes probabilities ignore uncertainty tend closer zero 
empirical bayes probabilities inferior full bayes useful cost time days jan probability fraud time probability fraud telephone calls account 
expected loss due fraud hmm dotted line mixture model considering time solid line 
mcmc calculations prohibitive 
example fraud detection model described example designed monitor real time data accounts 
mcmc may sparingly month update estimates account mcmc far slow monitor entire network accounts real time 
presents empirical bayes probability fraud telephone call account may quickly computed single run forward backward recursions past estimate 
forward backward recursions empirical bayes calculations hmm intractable 
ignore effect time screen fraud finite mixture model 
compares cumulative expected financial loss due fraud computed hmm finite mixture model 
hmm clearly superior finite mixture model allows information fraud shared time 
hmm understands occasional calls large fraud scores represent weaker evidence fraud cluster calls generate fewer false alarms finite mixture model 
maximum posteriori estimation viterbi algorithm viterbi fredkin rice method finding empirical bayes trajectory maximizing hjd 
algorithm uses forward backward strategy similar section replacing averages 
observing optimizing hjd optimizes problem reduces maximizing complete data likelihood 
define max gamma words largest contribution complete data likelihood obtained quantities delta computed recursively max gamma find run storing delta algorithm progresses 
choose maximize gamma choose arg max maximize complete data likelihood conditional note determined arg max frequent event small 
occurs say estimation converged 
convergence select maximize compute previously undetermined elements gamma 
earlier values delta longer necessary may discarded 
updating convergence limits memory demands viterbi algorithm allows algorithm run nearly real time 
viterbi algorithm suffers stability issues forward backward recursions remedies 
renormalizing iteration transforming algorithm log scale may accomplished analogy 
algorithm may expanded accommodate general hmm modifying general local computation method marginalization discussed section 
see cowell 
details 
viterbi style algorithm maximizing hjd averaging destroys state probability interval number estimating fetal lamb data 
trajectories times mcmc frequency 
marginal probabilities 
model markov structure 
simply evaluating objective function requires monte carlo 
hjd hjd jd hjd equation difficult optimize optimal need trajectory particular 
primitive solution select commonly occurring gibbs run 
random mcmc fluctuations require extremely long mcmc runs ample memory hard disk storage approach effective 
approximate preferable solution obtained viewing function selecting mcmc run 
set distinct produced sampler smaller set distinct 
shows trajectories fetal lamb data multiplied frequency mcmc output 
marginal probabilities included 
note states high marginal probability fetal activity intervals may represented estimated map 
selecting state space size assumed known context application deciding question scientific interest 
absent firm priori theories choosing model selection problem views random variable posterior distribution recursive computing important model selection standard bayesian model selection model averaging techniques require ability compute likelihoods models considered 
section explains calculate sjd mcmc 
section discusses developed variable dimension monte carlo methods may standard model selection theory 
calculating sjd mcmc practical reasons assume max bounding mild restriction choosing max number distinct values allows data value mixture component 
smaller values max chosen larger state space identifiable 
redefine smax parameter hmm state space gamma 
formally 
dimension smax number free parameters assume smax follows smax conditionally independent posterior distribution may independently sampled max parallel gibbs samplers 
represent draws samplers smax 
monte carlo calculation sjd sjd sjd jd sjd sjd 
normalizing constant sjd easily com size state space posterior probability mcmc run iterations 
maximized log posterior bic maximized log posterior number free parameters schwarz criterion bic comparing sjd bic markov poisson mixtures applied fetal lamb data 
methods give different 
puted takes values finite set 
proportionality averages max likelihoods corresponding life gibbs sampler 
calculating sjd mcmc improvement schwarz criterion schwarz kass raftery asymptotic approximation log sjd 
minus twice schwarz criterion bayesian information criterion bic select hidden markov models puterman 
schwarz criterion log gamma log likelihood maximized sample size number free parameters schwarz criterion equivalently bic assumes uniform approximates jd multivariate normal integrated laplace approximation tierney kadane 
compares mcmc estimates sjd bic markov poisson hmm applied fetal lamb data 
methods lead different 
bic suggests state model state model sjd 
bic penalizes larger models suggesting sample size large justify bic asymptotics 
shows marginal posterior densities state markov poisson models applied fetal lamb data 
bic normality assumptions violated asymmetry heavy tails proximity boundaries 
posterior parameter parameter density parameter parameter density state model parameter density parameter parameter density parameter density parameter parameter density parameter density parameter parameter density state model marginal posterior densities elements state markov poisson hmm applied fetal lamb data 
figures arranged order elements row second column shows posterior density 
results mcmc runs iterations deleted 
densities far normal especially 
density shown central plot specific example 
highly probable lies narrow interval centered small appreciable probability falls 
roughly speaking bic approximates posterior density normal distribution centered variance chosen cover narrow interval 
bic integrates approximating normal density ignoring probability mass 
variable dimension monte carlo variable dimension monte carlo methods sampling schemes allow dimension vary sampling algorithm monte carlo distribution sjd 
label switching attention paid finite mixture models hmm literature 
growing list algorithms applied finite mixture models includes jump diffusions grenander miller phillips smith reversible jump mcmc green richardson green birth death sampling stephens 
reversible jump mcmc applied hmm robert 
methods similarly extended 
reversible jump mcmc distinct methods accommodates sampling 
reversible jump mcmc forward backward recursions improve mixing periods algorithm remains constant 
jump diffusions birth death sampling integrate metropolis hastings algorithm discussed section 
recursive computing requirement methods able evaluate likelihood proposed jumps births deaths 
key difference variable dimension monte carlo definition parameter space 
theta max simulates theta theta theta delta delta delta theta theta smax variable dimension monte carlo samples theta theta delta delta delta theta smax distinction theta theta important scientific interpretation changes dimension way regression coefficients interpreted differently new variables added regression model 
max grows need run max gibbs samplers burden 
variable dimension monte carlo methods require careful tuning clever choices potential moves parameter spaces 
return escape computational burden large max rarely sampling regions small posterior probability 
advantages easily implemented max small computes sjd directly iteration simulation spirit section 
diagnostics section briefly discusses diagnostics mcmc convergence model adequacy 
carlin mengersen 
review large body mcmc convergence diagnostics 
call special attention robert 
deal specifically hmm 
mcmc convergence diagnostics seek determine distribution function stabilized long run frequency properties mcmc chain 
function typically left unspecified 
user may apply diagnostic component select global summary model parameters 
ways natural global summary multidimensional parameter value posterior distribution jd 

iteration log likelihood 
iteration log likelihood log posteriors gibbs runs state markov poisson hmm fetal lamb data 
values shown log posterior values scaled maximum achieved full run iterations 
chains models converge quickly 
final benefit likelihood forward backward recursions provide global summary model may monitored deduce mcmc convergence 
shows values fb sampler applied state markov poisson hmm fetal lamb data 
samplers rapidly achieve regions high posterior density 
second mode apparent responsible non normal posterior distribution noted 
frequent switching modes suggests mixing weights modes estimated 
posterior distribution excellent diagnostic model adequacy hidden states interpretable 
hidden states behaving expected model probably needs structure 
shows empirical bayes probabilities fraud detection model example applied account known contain fraud 
hidden states supposed represent criminal presence absence 
suggests describe office closed open business 
expanded model considers hourly daily calling behavior produces believable estimates shown 
appropriate diagnostics model adequacy depend application hand time probability contamination time empirical bayes probability criminal activity function time account known contain fraud 
hmm discussed example 
elaborate model considering accounts hourly daily calling patterns 
closely matches intended behavior model 
time measured days january 
shown restricted time scale prevent 
generally useful diagnostics autocorrelation function advocated macdonald predictive distribution data advocated robert 
acf determine structure added 
example model fit gnp data sections ignores possible dependence residuals gamma functions 
shows posterior distribution autocorrelation function residuals 
marginal distributions acf easily cover zero suggesting dependence residuals weak safely ignored 
predictive distribution data diagnostic finite mixture models 
hmm complicated predictive distribution added time dimension 
finite mixture models marginal distribution may obtained weighting gamma stationary distribution complicated summaries may evaluated posterior predictive checks rubin meng gelman 
drawn mcmc algorithm simulate data set compute summary example fetal lamb data ends run zeros longer runs data set 
calculates posterior lag acf longest run zeros posterior distribution autocorrelation function residuals parameter gaussian model applied gnp data 
posterior predictive distribution longest run zeros solid dotted state poisson models applied fetal lamb data 
vertical line indicates longest run zeros data 
predictive distribution longest run zeros state poisson models 
state model capable producing long runs zeros state model 
mcmc allows hidden markov models implemented recursive computing likelihood forward backward viterbi recursions bring richness models exist 
forward backward recursions lead gibbs sampler mixes faster natural competitor likelihood recursion opens door general samplers impossible tractable method computing hmm likelihoods 
backward recursion allows bayes estimates rao blackwellized empirical bayes estimates mcmc option 
viterbi algorithm allows approximate map reconstructions constructed efficiently naive mcmc sampling 
likelihood recursion indispensable classical bayesian model selection mcmc approximate methods allows variable dimension monte carlo techniques depend likelihood 
likelihood provides automatically computed summary model parameters monitored deduce mcmc convergence 
section illustrates ability draw directly hjd simplify task drawing missing data hidden markov chain 
second theme article recursive computing advantages understanding physical meaning selecting appropriate diagnostics preventing label switching simplified scientific understanding informative prior assumptions needed defeat intrinsic identifiability problems cause label switching 
assumptions may applied directly prior distribution indirectly model choosing loss functions estimation unidentified mcmc run 
intuition behave allows posterior distribution model diagnostic 
conclude note historical significance baum 

contains innovations developed implement hmm inequality recursion 
general techniques 
dempster 
showed inequality applied broader set problems introduced general em algorithm 
recursion special case local computation algorithm helped drive success graphical models promising area current research 
computing landscape changed considerably baum 
ideas remain relevant today mcmc world 
albert chib 

bayes inference gibbs sampling autoregressive time series subject markov mean variance shifts 
journal business economic statistics 
andrieu doucet 

joint bayesian model selection estimation noisy sinusoids reversible jump mcmc 
mcmc preprint service baum petrie soules weiss 

maximization technique occurring statistical analysis probabilistic functions markov chains 
annals mathematical statistics 
besag green 

spatial statistics bayesian computation 
journal royal statistical society series methodological 
casella robert 

rao blackwellisation sampling schemes 
biometrika 
celeux robert 

computational inferential difficulties mixture prior distributions 
journal american statistical association 
chib 

calculating posterior distributions modal estimates markov mixture models 
journal econometrics 
churchill 

stochastic models heterogeneous dna sequences 
bulletin mathematical biology 
cowell dawid spiegelhalter 

probabilistic networks expert systems 
springer 
carlin 

markov chain monte carlo convergence diagnostics comparative review 
journal american statistical association 
damian wakefield walker 

gibbs sampling bayesian non conjugate hierarchical models auxiliary variables 
journal royal statistical society series methodological 
dawid 

applications general propagation algorithm probabilistic expert systems 
statistics computing 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society series methodological 
diebolt robert 

estimation finite mixture distributions bayesian sampling 
journal royal statistical society series methodological 
everitt hand 

finite mixture distributions 
chapman hall 
fredkin rice 

bayesian restoration single channel patch clamp recordings 
biometrics 
gelfand smith 

sampling approaches calculating marginal densities 
journal american statistical association 
gelman carlin stern rubin 

bayesian data analysis 
chapman hall 
gilks roberts 

strategies improving mcmc 
gilks richardson spiegelhalter eds markov chain monte carlo practice chap 

chapman hall 
green 

reversible jump markov chain monte carlo computation bayesian model determination 
biometrika 
grenander miller 

representations knowledge complex systems disc 
journal royal statistical society series methodological 
guttorp 

stochastic modeling scientific data 
chapman hall 
hamilton 

new approach economic analysis nonstationary time series business cycle 
econometrica 
hamilton 

analysis time series subject changes regime 
journal econometrics 
hastings 

monte carlo sampling methods markov chains applications 
biometrika 
higdon 

auxiliary variable methods markov chain monte carlo applications 
journal american statistical association 
juang rabiner 

hidden markov models speech recognition 
technometrics 
kalman 

new approach linear filtering prediction problems 
trans 
asme 
basic eng 

kass raftery 

bayes factors 
journal american statistical association 
puterman 

maximum penalized likelihood estimation independent markov dependent mixture models 
biometrics 
liu chen 

sequential monte carlo methods dynamic systems 
journal american statistical association 
liu neuwald lawrence 

markovian structures biological sequence alignments 
journal american statistical association 
liu wong kong 

covariance structure gibbs sampler applications comparisons estimators augmentation schemes 
biometrika 
liu wong kong 

covariance structure convergence rate gibbs sampler various scans 
journal royal statistical society series methodological 
macdonald 

hidden markov models discrete valued time series 
chapman hall 
meng 

posterior predictive values 
annals statistics 
mengersen robert 

mcmc convergence diagnostics 
berger bernardo dawid lindley smith eds bayesian statistics 
oxford university press 
metropolis rosenbluth rosenbluth teller teller 

equation state calculations fast computing machines 
journal chemical physics 
phillips smith 

bayesian model comparison jump diffusions 
gilks richardson eds markov chain monte carlo practice 
chapman hall 
richardson green 

bayesian analysis mixtures unknown number components 
journal royal statistical society series methodological 
discussion 
robert 

mcmc specificities latent variable models 
bristol england 
robert celeux diebolt 

bayesian estimation hidden markov chains stochastic implementation 
statistics probability letters 
robert en titterington 

convergence controls mcmc algorithms applications hidden markov chains 
journal statistical computation simulation 
robert en titterington 

bayesian inference hidden markov models reversible jump markov chain monte carlo method 
journal royal statistical society series methodological 
robert titterington 

strategies hidden markov models bayesian approaches maximum likelihood estimation 
statistical computing romberg choi baraniuk 

bayesian tree structured image modeling wavelet domain hidden markov models 
ieee transactions image processing submitted rubin 

justifiable relevant frequency calculations applied statistician 
annals statistics 
schwarz 

estimating dimension model 
annals statistics 
scott 

bayesian analysis state markov modulated poisson process 
journal computational graphical statistics 
scott 

detecting network intrusion markov modulated nonhomogeneous poisson process 
journal american statistical association submitted 
stephens 

discussion richardson green 
journal royal statistical society series methodological 
stephens 

bayesian analysis mixtures number components alternative reversible jump methods 
annals statistics appear 
stephens 

dealing label switching mixture models 
journal royal statistical society series methodological 
tierney kadane 

accurate approximations posterior moments marginal densities 
journal american statistical association 
titterington smith makov 

statistical analysis finite mixture distributions 
wiley 
viterbi 

error bounds convolutional codes asymptotically optimum decoding algorithm 
ieee transactions information theory 
west harrison 

bayesian forecasting dynamic models 
springer 

