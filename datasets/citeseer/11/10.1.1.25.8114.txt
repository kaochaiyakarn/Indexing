text classi cation segmentation minimum cross entropy teahan school computing mathematical sciences robert gordon university aberdeen scotland email wjt scms ac uk phone methods classifying segmenting text described 
ranking text sequences cross entropy calculated xed order character markov model adapted ppm text compression algorithm 
experimental results show methods signi cant improvement previously methods number areas 
example text classi ed high degree accuracy authorship language dialect genre 
highly accurate text segmentation possible accuracy ppm chinese word segmenter close chinese news text similarly ppm method segmenting text language achieves accuracy 
having computer model able predict model natural language human critical cryptology language identi cation authorship attribution text correction text segmentation 
shown character algorithms text compression scheme ppm prediction partial match construct language models applied successfully problems 
applications require little knowledge text analysed successfully 
markov modelling approach ppm models previously applied problems natural language processing language identi cation dunning optical character recognition chen automatic spelling correction kukich kernighan church gale continuous speech recognition jelinek bahl jelinek mercer 
successful types language models called gram models base probability word preceding words pos models base probability preceding words parts speech 
models characters tried feature prominently literature 
vigorously applied domain text compression teahan thomas bell cleary witten application domains cryptography irvine language identi cation sherman various applications automatically correcting words texts ocr spell checking kukich 
ppm model applied problems text classi cation segmentation 
methods described new novelty adoption ppm model language model 
language modelling approaches include hidden markov modelling charniak jelinek context free grammars chen decision trees bahl local rules brill collocational matrices garside leech simpson neural networks schmid 
performance ppm model applications superior previously published methods 
example results described sections show variant ppm text compression algorithm applied problem segmenting words chinese english text achieve accuracy 
character model required substantially training text methods example mbyte brown corpus francis ku perform better previously published word model ponte croft trained gbyte text 
ppm model successfully novel applications 
section describes ppm identify language dialects american british english period historical texts old middle english 
section applies ppm famous problem authorship attribution determining authorship federalist papers arrives studies subject mosteller wallace 
section shows ppm classify text genre politics religion sport 
section demonstrates ppm locate di erent languages text high degree accuracy 
purpose threefold rstly show ppm model applied successfully wide range applications natural language processing secondly show certain cases performance ppm model superior methods thirdly show ppm model applied novel applications 
organized follows 
method classifying text minimum cross entropy multiple language models discussed section 
particular method calculating cross entropy described text compression scheme ppm 
applications ppm text classi cation explored language dialect identi cation authorship ascription classi cation genre 
class text correction algorithms described 
algorithms applied speci applications word segmentation segmenting text language 
minimum cross entropy text classi er information theory shannon fundamental coding theorem states lower bound average number bits symbol needed encode message sequence text entropy log probabilities independent sum possible symbols probability distribution 
entropy measure uncertainty involved selection symbol greater entropy greater uncertainty 
considered measure information content message probable messages convey information probable ones 
equation extended general case language probability distribution lim xm log xm called entropy language considered limit entropy length message gets large 
usually true probability distribution known 
upper bound obtained model approximation xm log xm correct model xm probabilities estimated model xm 
called cross entropy greater equal entropy best possible language model source text compression directly estimate upper bound entropy brown 
xm number bits required encode string xm model lim xm number bits symbol required encode long sequence text drawn cross entropy relevant provides measure approximate model doing test text closer inaccurate model gives useful yardstick comparing accuracy competing models 
done computing cross entropy model model smallest cross entropy inferred best 
information extracted text associating model semantic label trained english text written william shakespeare sport religion choosing label associated best model classify text 
section discusses particular method calculating cross entropy ppm text compression scheme 
description text compression schemes may bell cleary witten 
ppm prediction partial match ppm set performance standard lossless compression text past decade 
original algorithm rst published cleary witten series improvements described mo culminating careful implementation called benchmark version mo 
achieves results superior virtually compression methods despite attempts better 
methods ziv lempel coding ziv lempel ziv lempel commonly practice attractiveness lies relative speed superiority compression compression performance generally falls ppm benchmark tests bell cleary witten 
prediction partial matching ppm nite context statistical modelling technique viewed blending xed order context models predict character input sequence 
prediction probabilities context model calculated frequency counts updated adaptively symbol occurs encoded relative predicted distribution arithmetic coding mo neal witten witten neal cleary 
maximum context length xed constant increasing generally improve compression cleary teahan witten mo cleary witten 
ppm character model uses characters input stream predict upcoming 
models condition predictions immediately preceding symbols called nite context models order number preceding symbols 
ppm employs suite xed order context models di erent values pre determined maximum predict upcoming characters 
order order order order predictions predictions predictions predictions ab jaj esc ac esc esc esc ad esc esc br esc esc ca esc esc da esc esc ra esc table model processing string maximum order model note kept characters followed length subsequence observed far input number times occurred 
prediction probabilities calculated counts 
probabilities associated character followed characters past predict upcoming character 
model separate predicted probability distribution obtained 
distributions ectively combined single arithmetic coding encode character occurs relative distribution 
combination achieved escape probabilities 
recall model di erent value model largest default coding 
novel character encountered context means context encoding escape symbol transmitted signal decoder switch model smaller value process continues model reached character novel point encoded respect distribution predicted model 
ensure process terminates model assumed lowest level containing characters coding alphabet 
mechanism ectively blends di erent order models proportion depends values escape probabilities 
formally jc gives probabilities returned order ppm character model 
example illustration operation ppm table shows state models input string processed 
model previously occurring contexts shown associated predictions occurrence counts probabilities calculated 
convention character probabilities encoded probabilities encoded occupied exclusions exclusions log bit log 
bits jaj jaj log 


bits table encodings sample characters model table designates bottom level model predicts characters equally gives initial probability jaj alphabet 
policy adopted choosing probabilities associated escape events 
sound theoretical basis particular choice absence priori assumption nature symbol source alternatives evaluated witten bell 
method example commonly called method gives count escape event equal number di erent symbols seen context far mo example order column table escape symbol receives count di erent symbols seen context 
methods estimating escape probability described section 
sample encodings models shown table 
noted prediction proceeds highest order model 
context successfully predicts character input sequence associated probability encode 
example followed string prediction ra encode probability bit 
suppose character predicted current context ra 
consequently escape event occurs context ra coded probability context 
predict desired symbol prediction probability fact accurate estimate prediction probability context obtained noting character possibly occur encoded level 
mechanism called exclusion corrects probability shown third column table 
total number bits needed encode calculated 
character encountered say escaping take place repeatedly right base level 
level reached symbols equiprobable exclusion device need reserve probability space symbols appear higher levels 
assuming character alphabet coded probability base level leading total requirement bits including needed specify escapes 
example highlights probabilities allocated di erent symbols arbitrarily small large 
comparison order static model assigns probability character alphabet requires bits encode 
improvement called update exclusion mo counts context updated 
exclusions applied context performing predictions 
suggests modi cation counts updated counts updated predicted higher order context 
consequently counts accurately re ect symbols excluded higher order contexts 
bell cleary witten state update exclusion generally improves compression reducing time spent updating counts 
estimating probabilities various methods proposed computing escape symbol probabilities context models 
probabilities frequency counts symbols followed context time occurred text 
commonly methods method proposed mo method howard 
method uses number times novel symbol occurred number types basis probability 
number times particular context followed symbol number tokens followed sum counts symbols number types number unique symbols 
escape probability probability symbol particular context model method minor modi cation method novel event occurs adding symbol escape counts added methods described performance compared witten bell 
di erent implementations ppm usually identi ed escape method uses 
example stands prediction partial match escape method 
likewise escape methods maximum order models may included example xed order implementation ppm uses escape method ppm language models ppm character language models applied successfully applications natural language processing teahan language identi cation cryptography irvine various applications automatically correcting words texts ocr word segmentation 
experiments english text show ppm models upper bound characters context perform competitively ppm models shorter longer length contexts teahan 
performance models substantially improved training large amounts related text text language authorship genre unrelated text 
ppm compare performance human computer models 
teahan cleary shown ppm predicts english text humans 
performed experiments text claude shannon famous experiment estimate entropy english shannon 
shannon estimates humans guessing upcoming text letter letter 
teahan cleary ppm scheme build computer model performance close cases superior human results 
katz backo method method widespread language modelling literature uses similar blending mechanism ppm smooth probabilities methods higher order models de ned recursively terms lower order models 
essential di erence methods weight estimates lower order models 
ppm weights di erent order models escape mechanism 
katz method counts greater assigned weight counts ectively discounted room escape count backing lower order model zero count encountered 
major di erences ppm katz method fold rstly ppm excludes estimates lower order models predicted higher order models full exclusion mechanism secondly ppm counts incrementally updated line update exclusion mechanism 
ppm text classi cation text classi cation problem assigning text set pre speci ed categories 
useful indexing documents retrieval stage natural language processing systems content analysis roles lewis hayes 
ppm classi cation applications test text encoded ppm models pre trained text hopefully representative particular style text modelled 
example style particular language english french german particular author jane austen thomas je erson william shakespeare particular genre politics sport religion 
results sections show cross entropy calculated manner accurately distinguish language dialect british american english authorship 
section describes ppm identify language dialects american british english period historical texts old middle english 
section applies ppm famous problem authorship attribution determining authorship federalist papers arrives studies subject mosteller wallace 
section investigates dicult problem classifying genre 
novel method classi cation signi cantly speed classi cation loss accuracy large number classes 
language dialect identi cation language identi cation concerns problem identifying examples written spoken language language 
dunning describes statistical methods identifying language small pieces text se sont 
ganesan sherman adopt various statistical modelling methods perform various language recognition problems recognizing known language distinguishing known language uniform noise detecting non uniform unknown language 
calculated ppm models may equally applied problems provides means rank performance statistical models 
text analysed compressed ppm character language models trained representative texts written various languages 
language train model best compression performance 
table shows works sample text taken bible book genesis 
public domain versions bible di erent languages obtained english french german italian latin spanish library research guides 
sample texts rst characters book genesis compressed order ppm character models trained remaining genesis text di erent languages 
compression ratios shown gure bits character permutations training model original sample text best performed model sample text highlighted bold font 
shown compression ratios sample texts training 
cases best performed model trained language original text 
interestingly untrained model consistently outperforms models trained languages margin case 
process identify language period historical texts old middle early modern english teahan 
interesting possibility problem identifying dialect sample text 
training text compression ratio bpc original text untrained english french german italian latin spanish english french german italian latin spanish size training text chars 

original text taken rst characters book genesis 

training text remaining characters book genesis di erent languages 
table identifying text di erent languages 
brown lob johansson corpora represent diverse samples di erent dialects american british english provide means rigorously testing possibility 
experiment equal size partitions characters successively extracted corpora compressed order models trained remaining text text corpora just partition deleted 
improvements compression performance brown trained lob trained models partition vary brown corpus lob corpus 
cases decrease performance cases di erence performance 
results show method chooses correct classi cation model performing better british text brown trained model performing better american text case 
authorship ascription similar process possible determining authorship 
widely known style statistical properties di er markedly di erent authors davis 
exploited training models text written disputed authors compressing text question models 
hope style author suciently di erent clearly distinguish 
experiments english texts show true cases teahan 
famous problem authorship attribution concerns federalist papers written alexander hamilton john jay james madison 
years short essays addressed citizens new york constitution published newspapers pseudonym publius 
papers generally agreed jay wrote hamilton madison authorship remaining dispute hamilton madison 
mosteller wallace determine odds authorship papers applying bayes theorem evidence common words 
supports claim historians madison madison wrote disputed papers 
experiments ppm character computer models supports claim 
disputed papers compressed computer models trained text taken non disputed papers 
models test papers known authorship 
minimize uctuations due di ering periods papers tested consecutive order number excluding number written john jay 
similar results reported di erent method calculating 
disputed papers 
madison hamilton 
madison hamilton bpc bpc bpc bpc papers known papers known written madison written hamilton 
madison hamilton 
madison hamilton bpc bpc bpc bpc ascribing authorship federalist papers lists results obtained 
compression ratios madison lower disputed papers excluding supporting claim madison primary author papers 
non disputed papers result contradictory 
mosteller wallace state historians feel settled papers re ected results 
draw attention diculty task confronting researchers hamilton madison remarkably similar prose styles unique phenomenon educated americans late eighteenth century employed stylistic devices standard phrases remarkably similar sentence structure 
results surprising relatively small amount training text kilobytes required train models 
classi cation genre preliminary investigation ectiveness ppm classify genre performed newsgroups data set mccallum nigam 
data set contains articles evenly divided usenet discussion groups 
data set particularly interesting newsgroup categories fall confusable clusters example discuss religion soc religion christian talk religion misc alt atheism discuss politics talk politics hierarchy comp hierarchy 
initial experiment separate order models trained text contained rst articles remaining articles compressed see model performed best 
usenet headers including subject followup newsgroup lines removed articles 
newsgroup category correctly deduced articles compares favourably results reported mccallum nigam achieved accuracy multi variate bernoulli event model multinomial model training testing split data 
confusion matrix errors ppm classi er newsgroups shown table 
typical ppm classi er leading diagonal correctly aa sc se sm ss src tpg trm aa sc se sm ss src tpg trm table cross entropy confusion matrix newsgroups data set 
columns show results model trained text taken respective newsgroup rows show test results newsgroup articles included training data 
newsgroups shown 
alt atheism aa sci sci electronics sci med sci space soc religion christian talk politics guns talk politics mideast talk politics misc talk religion misc 
accounts classi cations 
newsgroups outliers major exception confusion alt atheism talk religion misc newsgroups reasonable confusion accounts mis classi cations 
mis classi cations correct newsgroup category ranked second third best small di erence cross entropy best 
choosing best improves accuracy best 
test article multiply classi ed penalty retrieval applications may willing pay increased accuracy 
penalty signi cantly reduced choosing discard second third best categories di erence cross entropy best threshold 
example examining best categories choosing di erence threshold little bits character cross entropy discard nearly multiple classi cations ensuring high accuracy level 
number enhancements improve ppm classi er 
replacing sequences non alphanumeric characters data single space accuracy achieved ppm models signi cantly smaller take time train 
hierarchical classi cation possible 
advantages classi cation faster categories branches tree searched memory required classi cation 
mccallum 
shown text classi cation signi cantly improved advantage hierarchy classes 
hierarchical ppm classi er decision tree models 
higher nodes tree ppm models built concatenation training text categories occur leafs sub tree 
node models decide branch take encoding sample text nding model compresses best 
branching continued root leaf tree reached 
category train ppm model leaf compresses sample text best 
example level hierarchy newsgroups data set rst pre classifying super categories comp rec sci talk rest classifying individually marginal ect original accuracy 
biber uses factor analysis identify dimensions variation basis linguistic occurrence patterns spoken written genres 
interesting possibility area compare ectiveness ppm classi er distinguishing variations 
ppm text correction segmentation essential component applications natural language processing language modeler able correct errors text processed 
optical character recognition ocr poor scanning quality extraneous pixels image may cause characters mis recognized spelling correction characters may transposed character may inadvertently inserted missed 
section describes method correcting english natural language texts noisy channel model ppm language modelling component means rank alternative re arrangements text sequences 
common framework statistical modelling natural language theory developed shannon bell laboratories model noisy communication channel telephone line 
adoption noisy channel model referred source channel model pioneered ibm research group yorktown heights new york applied problem continuous speech recognition bahl jelinek mercer jelinek bahl mercer 
model applied problems machine translation brown automatic spelling correction kernighan church gale applications part speech tagging optical character recognition ocr handwriting recognition chen page 
theory idea models model text model types errors occur original text referred confusion model 
word segmentation problem insertion spaces back text elided 
complex problems extraction text data ocr data speech 
principle models 
model confusion original glyphs phonemes text model text 
formally sequence text sent communications channel noisy text comes 
example spelling correction noisy text corresponds output typist spelling errors machine translation corresponds text language 
problem recover original input text output text 
done hypothesizing possible input texts selecting probable input output arg max applying bayes theorem rewrite arg max ojs arg max ojs probable sequence depends prior probability sequence occur observation channel probability ojs output observed sequence occurred 
depends application example speech recognition output observed input spelling correction output input perceive 
prior probability usually available model 
applying model problem ppm text correction wish nd sequence text maximizes prior observation probabilities arg max ojs extended part section rst published teahan 

survey techniques devised automatically correcting words text see kukich 
prior probability estimated training ppm model english text 
jc gives probabilities returned order ppm character model 
observation probability ojs estimated training data requires training texts rst consists typical output application containing representative sampling errors second corrected text 
estimate error probability example ocr application probability letter mistaken letter letters lc mistaken letter formally ocr text denotes concatenation transformed character sequence ojs jc probability computed confusion table ocr sequence reported place true character order nd probable correct text observed incorrect input text variation viterbi dynamic programming method viterbi 
viterbi algorithm commonly hidden markov models example part speech tagging programs assign sequence tags words sentence charniak 
key idea viterbi algorithm point search need retain possible active contexts sequence text minimum entropy poorer performing alternatives discarded 
markov model maximum length contexts xed ppm models number alternatives searched kept manageable limits 
section explores methods evaluating models text correction segmentation 
section explores correction algorithm applied speci problem word segmentation 
results segmenting english text described compared results chinese text viterbi algorithm discussed detail alternative algorithms heuristic search methods exhaustive search 
evaluating experimental results models evaluated literature measures recall precision 
word segmentation example comparing strings di er number spaces 
prediction spaces determined classes correct space prediction spurious space prediction missed space recall de ned recall precision de ned precision 
perfect model recall precision 
general measure available evaluating text correction models processed text compared directly original correct text 
di erence texts determined edit distance cormen leiserson rivest 
edit distance strings de ned minimum transformation sequence converts transformation operations delete character insert new character change character 
example assuming equal costs transformation edit distance ppm chinese word segmentation explored fully teahan 

original text unit new york kent cigarettes stopped cigarette lters 
spaces removed 
ponte croft method unit new york kent cigarettes stopped roc id lite inits micron cigarette lters 
ppm method unit new york kent cigarettes stopped croc cigarette lters 
table segmenting words english text 
sequence operations performs transformation delete change delete change word segmentation edit distance equals sum spurious space prediction plus missed space edit distance accuracy de ned edit distance corrected original text number characters original text 
ppm word segmentation english alphabetic orthography word spacing languages chinese japanese relatively easy adopt word basic unit blank space various punctuation marks delimiters 
word segmentation important task required applications start orthographic representation speech recognition automated transcription morse code 
ponte croft introduced method predicting space positions examined performance kbyte extract wall street journal 
word model trained gigabyte text produced recall precision 
teahan 
describe xed order ppm model correct sequences english text high degree accuracy 
applied algorithm problem segmenting words english text showed method signi cant improvement previously methods 
applied similar technique postprocessing stage pages recognized state art commercial ocr system 
shown accuracy ocr system increased decrease errors page 
results obtained training order language model text word mbyte brown corpus francis ku 
ppm character method corpus ponte croft produced recall precision rates edit distance accuracy 
improvement ponte croft results small fraction training text mbyte brown corpus compared gbyte 
table shows example ponte croft addition predictions ppm 
improvements ppm model provides evident small example 
word occur brown corpus word correctly segmented ppm 
likewise inits correctly segmented 
example ppm mistakes 
space predicted required bits encode text original required bits 
similarly extra null sw ot au bt au ob au ot sw bt sw sw bt bt bt word segmentation phrase tobe space added space reduced number bits encode 
ppm method consisted nding probable correct segmentation text rst generating alternative segmentations text inserting spaces letter searching best probable segmentation 
teahan cleary stated variation viterbi dynamic programming method viterbi nd probable segmentation 
viterbi algorithm guarantees best possible segmentation trellis search possible segmentation search paths extended time poorer performing alternatives lead conditioning context discarded 
drawbacks algorithm exhaustive search selective sequential decoding algorithms commonly convolutional coding rely search heuristics prune search space anderson mohan 
provides illustration algorithm works shows trellis constructed phrase tobe segmented 
example length conditioning context order ppm model arbitrarily xed length 
state diagram boxes labelled characters represent context initial states context length 
transitions states labelled possibilities character current position input phrase current character space attached 
order model xed maximum number states input position exceed possible segmentations characters space followed character character followed space 
possible segmentations represented diagram binary split state example preferred segmentation path transitions investigation revealed method teahan viterbi algorithm fact variation sequential decoding algorithm called stack algorithm ordered list best paths segmentation tree maintained best path terms metric extended worst paths deleted 
metric case compression codelength segmentation sequence terms order model 
stack algorithm ordered list paths vary length 
deleting worst paths traditional method list set maximum length worst paths longer length deleted 
ppm word segmenter variation 
algorithm average average time average edit distance accuracy secs nodes viterbi teahan stack size stack size table segmenting words chinese text paths ordered list rst deleted length path plus length rst path maximum order ppm model 
reasoning pruning heuristic extremely natural language sequences bad path perform better current best path needs encode characters despite worse codelength 
pruning method signi cantly improve execution speed 
viterbi algorithm poorer performing search paths lead conditioning context discarded 
viterbi algorithm search paths vary length poorer performing search paths length conditioning context discarded 
word segmentation chinese text word segmentation asian languages chinese japanese contrast english interesting problem written word 
poses problem number applications information retrieval compression better performance achieved application apply automatic method nd words text occur 
experiments segmenting chinese text performed guo jin mandarin chinese ph corpus pre segmented text containing half words newspaper stories news agency pr china written january march brew 
experiments order model trained second half corpus 
segment part rst half split fty sub sections containing words starting corpus 
word delimiters removed fty sub sections prior performing segmentation experiments 
results obtained shown table 
experiments repeated second half corpus start corpus training similar results 
shown table leftmost column di erent methods segment test subsections viterbi algorithm method teahan variants stack decoding algorithm maximum stack depth 
column table shows average edit distance observed segmented test sub sections corpus corresponding correct form obtained pre segmented corpus 
edit distance measure accuracy segmentation process teahan 
third column converts gures average edit distance accuracy average size text contained test sub sections 
fourth column lists average cpu time seconds took segment test sub sections pentium ii mhz processor mb ram running red hat linux 
nal column lists average number nodes stored paths tree constructed algorithms executed 
gure gauge memory consumption algorithms 
results seen best method terms application accuracy viterbi algorithm 
surprising method guarantees probable segmentation methods rely search heuristics prune search space 
di erence accuracy methods small ranging accuracy average number errors covered wider range edit distance 
method teahan nearest viterbi algorithm terms number errors slightly faster speed consuming slightly memory resources 
comparison performance stack algorithm stack depths worse twice times long execution speed requiring increased memory resources 
language segmentation language segmentation concerns problem identifying boundaries occur languages text 
particular importance applications especially information retrieval 
problem slightly di erent language identi cation problem tackled section interested assigning single categorization text document interested nding multiple languages exist document 
similar approach taken word segmentation solve problem single ppm model multiple models trained representative text language 
word segmentation alternative segmentations text generated best possible segmentation application viterbi algorithm 
segmentation achieved switching current ppm language model alternative language models character test sequence 
preceded special terminating character signal prior segmentation reset context null new model 
experimental results show method extremely high levels accuracy 
data section bible texts split testing data containing words training data containing prior text 
training data train separate models di erent languages 
testing data split samples words sample containing interleaved sub samples words extracted consecutively language 
segmentation method tried samples total characters contained mis classi ed edit distance accuracy 
interestingly achieved notion word boundary segmentation performed word boundaries errors corrected added bene reducing search space factor determined average word length 
interesting speculate similar approach authorship genre segmentation perform nding boundaries authored text text multiple genres 
experiments suitable data performed area hoped similar accuracy levels reported sections achieved 
summary performed text compression scheme ppm described 
ppm viewed blending xed order context models predict character input sequence 
number ppm techniques text classi cation segmentation 
applied successfully wide range problems natural language processing including cryptology language identi cation authorship attribution classifying genre ocr spelling correction word segmentation 
witten 
apply approach text mining show broad range patterns names dates text split sentence verse boundary sample slightly words 
email addresses urls located text 
cases applications built techniques achieve state art performance close 
models simplify aspects statistical language modelling require training 
potential performance comparable better traditional word methods statistical language modelling schemes 
models severely handicapped fact understanding way humans understand text analysed 
process simply translation composition results achieved simply predictions prior sequences characters text 
despite capable performing certain tasks decrypting ascribing authorship identifying language dialects humans nd dicult 
open question technology perform applications natural language processing text mining 
promising areas part speech tagging word sense disambiguation syntactic parsing 
knight notes able extract somewhat accurate syntactic parses raw text databases 
acknowledgments am grateful john cleary ian witten stuart fellow researchers waikato university new zealand provided valuable advice assistance aspects 
anderson mohan 
sequential coding algorithms survey cost analysis 
ieee transactions communications 
bahl brown desouza mercer 
tree statistical language model natural language speech recognition 
ieee transactions acoustics speech signal processing 
bahl jelinek mercer 
maximum likelihood approach continuous speech recognition 
ieee transactions pattern analysis machine intelligence 
brown della pietra della pietra lai mercer 
estimate upper bound entropy english computational linguistics 
bell cleary witten 
text compression 
prentice hall new jersey 
agarwal davis 
disambiguation prepositional phrases automatically labelled technical text 
aaai pages 
brill 
corpus approach language learning 
ph thesis university pennsylvania 
charniak 
language learning 
mit press cambridge massachusetts 
chen 
building probabilistic models natural language 
phil 
thesis harvard university 
cleary witten 
data compression adaptive coding partial string matching 
ieee transactions communications 
cleary teahan 
unbounded length contexts ppm 
computer journal 
cormen leiserson rivest 
algorithms 
mit press cambridge mass dunning 
statistical identi cation language 
technical report computing research laboratory new mexico state university 
francis ku 
frequency analysis english usage lexicon grammar 
houghton mi boston 
sherman 
statistical techniques language recognition guide 

garside leech sampson 
editors computational analysis english 
london 
longman 
brew 
error driven learning chinese word segmentation th paci conference language information edited guo lua xu singapore chinese oriental languages processing society 
howard design analysis ecient lossless data compression systems 
ph thesis brown university providence island 
irvine 
compression cryptology 
phil 
thesis university waikato jelinek 
self organized language modeling speech recognition speech recognition edited lee pages 
morgan kaufman publishers johansson atwell garside leech 
tagged lob corpus 
norwegian computing centre humanities bergen 

small corpora 
document categorization cross entropy proceedings workshop similarity categorization 
katz 
estimation probabilities sparse data language model component speech recognizer 
ieee trans 
acoustics speech signal processing 
kernighan church gale 
spelling correction program noisy channel model proceedings thirteenth international conference computational linguistics pages 
knight 
mining line text communications acm 
kukich 
techniques automatically correcting words text 
acm computing surveys 
lewis hayes 
guest editorial acm transactions information systems 
mccallum nigam 

comparison event models naive bayes text classi cation aaai workshop learning text categorization 
mccallum rosenfeld mitchell ng 

improving text classi cation shrinkage hierarchy classes icml proc 
th international conference 
library research guides 
mo 
implementing ppm data compression scheme 
ieee transactions communications 
mosteller wallace 
applied bayesian classical inference case federalist papers 
springer verlag new york 
ponte croft 
retargetable word segmentation procedure information retrieval 
firth annual symposium document analysis information retrieval 
las vegas nevada 
thomas 
new techniques context modeling 
proceedings rd annual meeting acl cambridge massachusetts june 
schmid 
part speech tagging neural networks proceedings international conference computational linguistics 
shannon 
mathematical theory communication 
bell system technical journal 
shannon 
prediction entropy printed english 
bell system technical journal pages 
teahan 
modelling english text 
phil 
thesis univ waikato teahan cleary 
entropy english ppm models proceedings dcc edited storer cohn ieee computer society press 
teahan cleary holmes 
correcting english text ppm models proceedings dcc edited storer cohn ieee computer society press 
teahan wen witten 
compression algorithm chinese word segmentation computational linguistics appear 
viterbi 
error bounds convolutional codes asymptotically optimal decoding algorithm ieee transactions information theory 
witten bray teahan 
text mining new frontier lossless compression proceedings dcc edited storer cohn ieee computer society press 
witten bray teahan 
language models generic entity extraction icml machine learning text data analysis 
