practical bayesian framework backprop networks david mackay computation neural systems california institute technology pasadena ca mackay hope caltech edu may submitted neural computation long quantitative practical bayesian framework described learning mappings feedforward networks 
framework possible objective comparisons solutions alternative network architectures objective stopping rules deletion weights objective choice magnitude type weight decay terms additive regularisers penalising large weights measure effective number determined parameters model quantified estimates error bars network parameters network output objective comparisons alternative learning interpolation models splines radial basis functions 
bayesian evidence automatically embodies occam razor penalising flexible complex architectures 
bayesian approach helps detect poor underlying assumptions learning models 
learning models matched problem correlation generalisation ability bayesian evidence obtained 
prerequisite bayesian framework regularisation model comparison described companion bayesian interpolation 
framework due gull skilling 
gaps backprop knobs black box backprop learning back propagation errors 
generally knobs set rules thumb trial error reserved data test generalisation ability sophisticated cross validation 
knobs fall classes parameters change effective learning model example number hidden units weight decay terms parameters concerned function optimisation technique example momentum terms 
concerned making objective choice parameters class ranking alternative solutions learning problem 
bayesian techniques described theoretically founded practically implementable 
review basic framework learning networks emphasising points objective techniques needed 
training set mapping learned set input target pairs fx label running pairs 
neural network architecture invented consisting specification number layers number units layer type activation function performed unit available connections units 
set values assigned connections network network defines mapping input activities output activities distance mapping training set measured error function example error entire data set commonly taken ed jw jw task learning find set connections gives mapping fits training set small error ed hoped learned connections generalise new examples 
plain backpropagation learns performing gradient descent ed space 
modifications include addition momentum term inclusion noise descent process 
efficient optimisation techniques may conjugate gradients variable metric methods 
discuss computational modifications concerned speeding optimisation 
address modifications plain backprop algorithm implicitly explicitly modify objective function decay terms regularisers 
moderately common extra regularising terms ew added example terms penalise large weights may introduced hope achieving smoother mapping 
hints fall category additive weight dependent energies 
sample weight energy term ew weight energy may implicit example weight decay subtraction multiple weight change rule corresponds energy 
gradient optimisation minimise combined function ffe fie jw ff fi black box parameters 
constant ff confused momentum parameter introduced backprop context ff decay rate regularising constant 
lacking procedures include host free parameters choice neural network architecture regularising constant ff 
established ways objectively setting parameters rules thumb see examples 
popular way comparing networks trained different parameter values assess performance measuring error unseen test set similar cross validation techniques 
utility techniques determining values parameters ff fi comparing alternative network solutions limited large test set may needed reduce signal noise ratio test energy 
furthermore parameters ff fi question repeat learning possible values parameters find best values test set 
parameters optimised line 
interesting study objective criteria setting free parameters comparing alternative solutions depend data set training 
criteria prove especially important applications total amount data limited doesn want sacrifice data test set 
describe practical bayesian methods filling holes neural network framework just described 
objective criteria comparing alternative neural network solutions particular different architectures single architecture may minimum objective function large disparity minima plausible choose solution smallest difference great desirable able assign objective preference alternatives 
desirable able assign preferences neural network solutions different numbers hidden units different activation functions 
occam razor problem free parameters model smaller data error ed achieve 
simply choose architecture smallest data error 

objective criteria setting decay rate ff 
choice occam razor problem small value ff equation allows weights large overfit noise data 
leads small value data error ed small value base choice ff ed bayesian solution implemented line necessary multiple learning runs different values ff order find best 

objective choice regularising function ew 
objective criteria choosing neural network solution solution different learning interpolation model example splines radial basis functions 
probability connection connections probabilistic inference neural networks discussed 
tishby introduced probabilistic view learning important step solving problems listed 
idea force probabilistic interpretation neural network technique able objective statements 
interpretation involve addition new arbitrary functions parameters involves assigning meaning functions parameters 
probabilistic framework extends concepts techniques adapted gull skilling bayesian image reconstruction methods 
adopts shift emphasis tishby concentrated predicting average generalisation ability network trained task drawn known prior ensemble tasks 
emphasis quantifying plausibility alternative solutions interpolation classification task task defined single data set produced real world know prior ensemble task comes 
review probabilistic interpretation network learning 
likelihood 
network specified architecture connections viewed making predictions target outputs function input accordance probability distribution jx fi exp fie jx zm fi zm fi dt exp fie 
error single datum fi measure presumed noise included quadratic error function corresponds assumption includes additive gaussian noise variance oe fi 
prior 
prior probability assigned alternative network connection strengths written form exp zw ff zw exp ff measure characteristic expected connection magnitude 
ew quadratic specified equation weights expected come gaussian zero mean variance oe ff 
alternative regularisers different energy function ew implicitly correspond alternative hypotheses statistics environment 
posterior probability network connections wjd ff fi exp fie zm ff fi zm ff fi exp fie 
notice exponent expression objective function defined 
framework minimisation ffe fie identical finding locally maximum posteriori parameters wmp minimisation ed backpropagation identical finding maximum likelihood parameters wml interpretation backpropagation energy functions ed ew parameters ff fi 
framework offers partial enhancements backprop methods levin possible predict average generalisation ability neural networks trained defined class problems 
clear lead practical technique choosing alternative network architectures real data sets 
le cun demonstrated estimate saliency weight change weight deleted 
measure successfully simplify large neural networks 
stopping rule weight deletion offered measuring performance test set 
denker le cun demonstrated hessian assign error bars parameters network outputs 
error bars quantified fi quantified prior knowledge extra data demonstrated 
fact fi estimated data reviewed 
review bayesian regularisation model comparison companion demonstrated control parameters ff fi assigned bayes alternative interpolation models compared 
noted satisfactory optimise ff fi finding joint maximum likelihood value ff fi likelihood skew peak maximum located probable values control parameters 
companion reviewed bayesian choice ff fi neatly expressed terms measure number determined parameters model fl 
assumed significant minimum approximated quadratic 
interpolation models discussed interpreted layer networks fixed non linear layer 
section briefly review bayesian framework retaining assumption 
section discuss framework modified handle neural networks landscape certainly quadratic 
determination ff fi bayes rule posterior probability parameters ff fi jd djff fi ff fi dja assign uniform prior ff fi quantity interest assigning preferences ff fi term right hand side evidence ff fi written djff fi zm ff fi zw ff fi zm zw defined earlier zd fied simple quadratic energy functions defined equations 
analysis easier complex cases principle handled approach 
number degrees freedom data set number output units times number data pairs number free parameters dimension immediately evaluate gaussian integrals zd zw zd fi zw ff want find zm ff fi exp ff fi 
supposing single minimum function wmp assuming locally approximate quadratic integral zm approximated zm wmp det notation abuses thereof 
rrm hessian evaluated wmp maximum djff fi useful properties ffe fl fie fl fl effective number parameters determined data fl ff eigenvalues quadratic form fie natural basis ew comparison different models rank alternative architectures penalty functions ew light data simply evaluate evidence ja appeared normalising constant 
integrating evidence ff fi dja djff fi ff fi dff dfi evidence bayesian transportable criterion model comparison 
adapting framework neural networks quadratic 
known typically local minima 
network symmetry permutation parameters know share symmetry single minimum belongs family symmetric minima example hidden units non degenerate minimum family size 
may case significant minima locally quadratic able evaluate zm evaluating significant minimum adding zm number minima unknown approach evaluating zm dubious 
luckily want evaluate zm referring previous section need evaluate zm order assign posterior probability ff fi evaluate evidence alternative architectures regularisers 
quite wish neural network perform mapping typically implement neural network time network parameters set particular solution learning problem 
alternatives wish rank different solutions learning problem different minima want posterior probability number hidden units able simultaneously implement entire posterior ensemble networks number hidden units 
similarly want posterior ff fi entire posterior ensemble wish allow solution minimum choose optimal value parameters 
having adopted slight shift objective turns set ff fi compare alternative solutions learning problem integral need evaluate local version zm assume posterior probability consists islands parameter space centred minima wish evaluate posterior probability mass islands 
consider minimum located define solution sw ensemble networks neighbourhood symmetric permutations ensemble 
evaluate posterior probability alternative solutions sw parameters ff fi ff fi ff fi zw ff fi ff fi appropriate permutation factor ff fi exp ff fi integral performed neighbourhood minimum refer quantity ff fi zw ff zd fi evidence ff fi sw ff fi chosen maximise evidence 
quantity want evaluate compare alternative solutions evidence sw sw ja ff fi zw ff fi ff fi dff dfi propose gaussian approximation may meet needs det rrm hessian evaluated ff fi approximation probably unacceptable need accurate small range ff fi close probable value 
regime approximation definitely break number constraints small relative number free parameters large central limit theorem encourages gaussian approximation 
matter research establish large approximation reliable 
obstacles remain prevent evaluating local need evaluate approximate inverse hessian need evaluate approximate determinant trace 
denker discussed approximate hessian ed purpose evaluating weight bayesian model comparison performed evaluating comparing evidence alternative models 
gull skilling defined evidence model djh 
existence multiple minima neural network parameter space complicates model comparison 
quantity djs includes prior ja called evidence quantity evaluate compare alternative solutions models 
saliency assigning error bars weights network outputs 
evaluated way backpropagation evaluates red alternatively evaluated numerical methods example second differences 
third option variable metric methods minimise gradient descent inverse hessian automatically generated search minimum 
important success bayesian method diagonal terms hessian evaluated 
denker method additional complexity 
diagonal approximation strong posterior correlations parameters 
demonstration demonstration examines evidence various neural net solutions small interpolation problem mapping joint robot arm 
cos cos sin sin training set regular samples restricted range gaussian noise magnitude added outputs 
neural nets hidden layer sigmoid units linear output units 
optimisation regulariser initially alternative regulariser introduced fi fixed true value enable demonstration properties quantity fl ff allowed adapt locally probable value 
illustrates performance typical neural network trained way 
output accompanied error bars evaluated denker method including diagonal hessian terms 
fi known advance inferred data equation 
solution displayed model estimate fi fact differed negligibly true value displayed error bars fi fixed 
shows data misfit versus number hidden units 
notice expected data error tends decrease monotonically increasing number parameters 
data misfit serve criterion choosing solutions 
shows evidence different solutions different numbers hidden units 
notice evidence maximum characteristic shape occam hill steep side parameters shallow side parameters 
quadratic approximations break number parameters big compared number data points 
figures introduce quantity fl discussed number measured parameters 
cases evaluation evidence proves difficult may fl serve useful tool 
example frequentist theory predicts addition redundant parameters model reduce unit measured parameter stopping criterion detect point parameters deleted started increase faster gradient decreasing fl 
fl requires prior knowledge noise level fi fi fixed known value demonstrations 
question predictor network quality evidence fact evidence maximum reasonable number hidden units promising 
furthermore shows performance solutions unseen test set similar large scale structure evidence 
shows evidence performance test set seen significant number solutions poor evidence perform test set 
time discussion relationship evidence generalisation ability 
return failure see rectified development new probable regularisers 
relation theory generalisation relationship evidence generalisation error close relative cross validation 
correlation certainly expected 
evidence necessarily predictor generalisation error see discussion 
illustrated error test set noisy quantity lot data devoted test set get acceptable signal noise ratio 
furthermore imagine models generated solutions interpolation problem probable interpolants completely identical 
case generalisation error solutions evidence general typically model priori complex suffer larger occam factor smaller evidence 
evidence measure plausibility ensemble networks optimum just optimal network 
evidence generalisation error 
bayesian method fails 
want dismiss utility generalisation error important detecting failures model 
example obtain poor correlation evidence generalisation error bayes fails assign strong preference solutions perform test data able detect attempt correct failures 
failure indicates things case able learn improve numerical inaccuracies evaluation probabilities caused typical neural network output 
inset training set output space ya yb network 
target outputs displayed small output network oe error bars shown dot surrounded ellipse 
network trained samples regions lower upper half planes inset 
outputs illustrated inputs extending short distance outside training regions bridging gap 
notice error bars get larger perimeter 
increase slightly gap training regions 
pleasing properties obtained diagonal hessian approximation 
solution created layer network hidden units 
data error number hidden units small start large start data error versus number hidden units 
point represents converged neural network trained pair training set 
neural net initialised different random weights different values initial value oe ff 
point styles correspond small large initial values oe error shown dimensionless units expectation error relative truth 
solid line number free parameters 
log evidence number hidden units small start large start log evidence solutions regulariser 
solution evidence evaluated 
notice evidence maximum achieved neural network solutions hidden units 
hidden units quadratic approximations evaluate evidence believed break 
number data points pairs number parameters net hidden units 
gamma total number parameters small start large start number determined parameters 
displays fl function network solutions 
data error gamma small start large start data misfit versus fl 
shows fl line gradient 
right data misfit reduced measured parameter 
model parameters left misfit gets worse greater rate 
test error number hidden units small start large start test error versus number hidden units training set test set data points 
test error solutions regulariser shown dimensionless units expectation error relative truth 
log evidence test error small start large start log evidence versus test error regulariser desired correlation evidence test error negative slope 
significant number points lower left violate desired trend failure bayesian prediction 
points violate trend networks significant difference typical weight magnitude layers 
networks learning initialised large value oe regulariser ill matched networks low evidence reflection poor prior hypothesis 
test error test error small start large start comparison test errors 
illustrates noisy performance measure test error point compares error trained network different test sets 
test sets consist data points distribution training set 
failure alternative hypotheses offered bayes poor selection ill matched real world example poor regulariser 
failure detected prompts examine models try discover implicit assumptions model data didn agree alternative models tried hypothesis data probable 
just met exactly failure 
establish assumption model caused failure learn 
note mechanism human learning available just test error performance criterion 
going test error indication serious mismatch model data 
back demonstration comparing different regularisers demonstrations far regulariser 
equivalent prior expects weights characteristic size 
inconsistent prior input output variables hidden unit activities arbitrarily rescaled mapping performed simple consistency requirement transformations variables imply independent rescaling weights hidden layer output layer 
scales layers weights unrelated inconsistent force characteristic decay rates different classes weights 
inconsistency major cause failure illustrated 
networks deviating substantially desired trend weights output layer far larger weights input layer poor match bias bias output layer hidden layer input layer classes weights second prior hidden unit weights 
hidden unit biases 
output unit weights biases 
weights class share decay constant ff model implicit regulariser causes evidence solutions small 
failure enables progress insight new regularisers 
alternative prior inconsistent way explained theoretical reasons expect better 
allow data choose evaluating evidence solutions new prior find new prior probable 
second prior independent regularising constants corresponding characteristic magnitudes weights different classes hidden unit weights hidden unit biases output weights biases see 
term ffe replaced ff 
hinton nowlan similar prior modelling weights coming gaussian mixture bayesian re estimation techniques update mixture parameters model discovering elegant solutions problems translation invariances 
second prior regularising constant independently adapted probable value evaluating number measured parameters fl associated regularising function finding optimum ff fl increased complexity prior model penalised occam factor new parameter ff see 
preempt questions lines didn weight classes 
way assigning weight decays just model try evaluating evidence find preference data alternative decay schemes 
new solutions second prior evidence evaluated 
evidence new solutions new prior shown 
notice evidence increased compared evidence prior 
solutions new prior probable factor log evidence number hidden units small start large start derived symmetries detected log evidence versus number hidden units second prior different point styles correspond networks learning initialised small large values oe networks previously trained regulariser subsequently trained second regulariser networks weight symmetry detected cases evidence evaluation possibly reliable 
crunch probable model predictions 
evidence second prior shown test error 
correlation greatly improved 
notice furthermore second prior probable best test error achieved solutions second prior slightly better achieved prior number solutions increased substantially 
bayesian evidence predictor generalisation ability bayesian choice regularisers enabled best solutions 
discussion bayesian method founded theoretically works practically remains seen approach scale larger problems 
particular data set evaluation evidence led objectively inconsistent regulariser probable 
evidence maximised sensible number hidden units showing occam razor successfully embodied ad hoc terms 
furthermore solutions greatest evidence perform better test set solutions 
believe currently technique reliably find identify better solutions training set 
essential success simultaneous bayesian optimisation regularising constants decay terms ff optimisation parameters orthodox search technique cross validation laborious log evidence test error small start large start derived symmetries detected log evidence second prior versus test error 
correlation evidence test error second prior 
note largest value evidence increased relative smallest test error decreased 
regularising constants easily case larger problems hard imagine search possible 
brings question bayesian calculations scale problem size 
terms number parameters calculation determinant inverse hessian scales note computation needs carried small number times compared immense number derivative calculations involved typical learning session 
large problems may demanding evaluate determinant hessian 
case numerical methods available approximate determinant trace matrix time 
application classification problems far discussed evaluation evidence backprop networks trained interpolation problems 
neural networks trained perform classification tasks 
publication demonstrate bayesian framework model comparison applied problems 
relation dimension papers advocate dimension criterion penalising complex models 
dimension applied classification problems evidence evaluated equally easily interpolation classification problems 
dimension worst case measure yields different results bayesian analysis 
example dimension indifferent regularisers value ff regulariser rule absolutely particular network parameters 
dimension assigns complexity model regularised 
set regularising constants ff compare alternative regularisers 
contrast preceding demonstrations show careful objective choice regulariser ff essential best solutions obtained 
worst case analysis complementary role alongside bayesian methods 
substitute 
tasks gaussian approximation evaluate evidence breaks number data points small compared number parameters 
model problems studied far gaussian approximation break significantly 
matter research characterise failure investigate techniques improving evaluation example random walks neighbourhood solution 
interesting see results evaluating evidence larger real world network problems 
appendix numerical methods quick dirty version numerical tasks automatic optimisation ff fi calculation error bars evaluation evidence 
describe cheap approximation solving tasks evaluating hessian 
neglect distinction determined poorly determined parameters obtain update rules ff fi ff fi ed want easy program taste bayesian framework offer try procedure update decay terms 
hessian evaluation hessian needed evaluate fl relates trace evaluate evidence relates det assign error bars network outputs 
methods evaluating approximate analytic method second differences 
approximate levine personal communication mentioned measure effective vc dimension regularised model developed 
speculated measure may turn identical fl equation 
analytic method denker backprop obtain second derivatives neglecting terms activation function neuron 
hessian built sum outer products gradient vectors 
denker ignore diagonal terms diagonal approximation 
evaluation fl methods gave similar results approach satisfactory 
evaluation evidence approximate analytic method failed give satisfactory results 
occam factors weak scaling log approximation apparently introduces systematic greater 
reason evidence evaluation sensitive errors fl evaluation fl related sum eigenvalues evidence related product errors small eigenvalues product sum 
expect exact analytic evaluation second derivatives resolve 
save programming effort second differences computationally demanding kn analytic approach 
problems errors small eigenvalues possible correct errors detecting eigenvalues smaller theoretically permitted 
demonstrations demonstrations performed follows initial weight configuration random weights drawn gaussian oe 
optimisation algorithm variable metric methods code times sequence values fractional tolerance decreasing loop regularising constants ff allowed adapt accordance re estimation ff fl precaution evaluating evidence care taken verify permutation term appropriately set 
may case probably mainly toy problems regulariser hidden units network adopt identical connection values alternatively hidden units may switch weights set zero cases permutation term smaller 
cases quadratic approximation perform badly quartic quadratic minima preferable automate deletion redundant units 
abu mostafa 
vapnik chervonenkis dimension information versus complexity learning neural computation 
abu mostafa 
learning hints neural networks complexity 
bridle 
probabilistic interpretation feedforward classification network outputs relationships statistical pattern recognition neuro computing algorithms architectures applications soulie editors springer verlag 
denker le cun 
transforming neural net output levels probability distributions preprint 
gull 
developments maximum entropy data analysis 
haussler kearns schapire 
bounds sample complexity bayesian learning information theory vc dimension preprint 
hinton nowlan personal communication 
hinton sejnowski 
optimal perceptual inference proc 
ieee conference computer vision pattern recognition 
ji 
generalizing smoothness constraints discrete samples neural computation 
le cun denker solla 
optimal brain damage advances neural information processing systems ed 
david touretzky morgan kaufmann 
lee 
optimal adaptive classifier design criterion hidden units necessary optimal neural network classifier purdue university tr ee 
levin tishby solla 
statistical approach learning generalization layered neural networks colt nd workshop computational learning theory 
luttrell 
bayesian entropic methods neural network theory 
mackay 
bayesian interpolation submitted neural computation 
mackay 
evidence framework applied classification networks preparation 
press flannery teukolsky vetterling 
numerical recipes cambridge 
rumelhart hinton williams 
learning representations back propagating errors nature 
rumelhart 
cited 
skilling 
eigenvalues mega dimensional matrices 
skilling editor 
maximum entropy bayesian methods cambridge kluwer 
tishby levin solla 
consistent inference probabilities layered networks predictions generalization proc 
ijcnn washington 
mike lewicki nick weir haim sompolinsky helpful conversations andreas herz comments manuscript 
supported caltech fellowship studentship serc uk 

