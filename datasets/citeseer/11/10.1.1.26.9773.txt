iglue lattice constructive induction system 
patrick iut de lens universit rue de universit sp lens cedex 
france 
univ fr machine learning ml system combines lattice instancebased learning ibl techniques motivated developed 
describe ibl system lattice theory called iglue signi cantly improved complexity accuracy lattice learning systems 
purpose iglue uses entropy function select relevant lattice nodes extracts set new numerical features original set boolean features nally applies nearest neighbor technique distance similarity measure instances 
iglue treats domains described symbolic features 
results experiments carried assess iglue performs real problems similarity measures selection functions 
combine selection functions similarity measures run experiments 
compare performance combined strategies collection ml benchmarks 
empirical results indicate iglue able achieve classi cation accuracy variety domains selection function similarity measure mentioned 
new functions measures highlight importance instance learning lattice theory 
keywords machine learning constructive induction concept lattice feature transformation instance learning concept learning galois lattice concept learning studied areas machine learning ml 
lot concept learning deals decision trees 
known decision tree systems id qui 
plays increasingly important role ml 
decade witnessed variants system qui probably widely distributed 
deals di erent structure lattice theory 
talking lattice theory refer principally birkho bir dilworth dil bm 
authors studied domain mathematics deals hierarchical systems general 
lattice theory intensively studied provides rich mathematical framework natural understand represents hierarchy inside speci ed problem 
galois lattice theory gives rise new research domain termed formal concept analysis wil concept lattice galois lattice bm understood basic answer fundamental questions concerning initial context set objects described set attributes question appropriate classi cation objects question dependencies attributes 
ganascia gan introduced formal model describe top induction system galois connection 
clearly shown concept lattice general framework learning strategies expressed 
years witnessed development lattice ml systems concepts representation including legal mn grand oos galois cr sah ls 
charade gan ml system notion hilbert cube similar concept lattice 
di erent lattice learning systems demonstrated eciency lattice approach compared treebased decision list methods 
systems successfully applied real problems coming di erent domains biology 
time space requirements ml systems grow considerably number features 
lattice systems legal lowest theoretical complexity builds join lattice 
major drawback related principle majority vote ignores relevant nodes contain positive examples target concept 
proposes new lattice approach avoids limitation classi cation technique combines feature selection join lattice instance learning 
feature transformation galois lattice important done order illustrate eciency preprocessing data area statistics pattern recognition machine learning lm 
ml systems generally focus selection transformation original features 
kohavi john kj overview feature subset selection propose optimal feature subset approach induction 
encouraging results classi cation feature projections ag classi cation overlapping feature intervals obtained 
koller sahami ks proposes approximate algorithm optimal feature selection intractable practice 
feature transformation ft relies extraction new features mapping function 
zheng describes ects di erent types new attributes conjunctive disjunctive represen tations 
new attributes binary nominal valued continuous valued attributes rst discretized 
describe constructive method feature extraction uses ml techniques generate new relevant features 
approach allows translate initial binary attributes continuous valued ones 
translation takes account original relevant attributes 
feature extraction achieved di erent purposes necessary reduce ect irrelevant attributes focusing attention way objects de ned initial context 
de nition object analyzed separately considering example descriptions 
literature di erent kinds discretization methods allow transform continuous valued features binary nominal ones 
conversely propose method achieve opposite process 
binary features appropriated instance learning choice similarity measure dicult boolean features numerical features 
instance learning galois lattice research instance learning ibl matured years researchers eld tackle real world problems hand crafted toy domains see aka dom cs ct lm 
strength ibl systems lies fact distance measure employed nearest neighbor nn technique classify instances 
distance measure allows similarities instances expressed analysis features descriptions 
nearest neighbor nn classi cation algorithm assumption examples closer instance space class 
unseen examples belong class nearest neighbor training set 
main diculty choice distance function de ne similar instance classi cation function de ne similar classi cation 
studies devoted distance metrics 
salzberg sal proposed distance metrics common euclidian distance metric 
metrics suitable numerical valued features symbolic ones 
nn systems perform signi cantly better numeric features symbolic ones compared inductive ml techniques 
avoid limitations order improve learning eciency di erent approaches investigated 
rst solution consists sophisticated distance measures symbolic feature domains simple overlap distance metric sw cs 
pebls system cs nn algorithm modi cation value di erence metric sw compute distance examples 
researchers preprocess data feature transformation ag combine nn technique ml technique 
example rise system dom combines rule induction ibl techniques 
develop di erent novel approach consists preprocessing data lattice learning method applying ibl technique data 
constructive approach presents feature extraction algorithm integrated ml technique design ibl system called iglue 
feature extraction algorithm consists steps 
builds join lattice initial context objects described binary features 
di erent functions select relevant nodes top lattice construction 
reduce lattice complexity relevant nodes top level taken account 
second initial examples way concerned relevant node join lattice attributes appearing lattice nodes veri ed example 
examples allows suppress irrelevant features introduced mistake initial description examples 
assume irrelevant features appear inside join lattice nodes 
transforms relevant initial binary features new numerical attributes depending examples appear relevant nodes 
precisely generate new continuous valued feature original features appears relevant node 
new feature value example number relevant nodes containing example corresponding binary feature 
iglue properties practical easier implement predecessor legal mn 
iglue uses entropy function qui cn ct select nodes lattice feature transformation process 
uses distance qs similarity measure classi cation process 
parameters chosen ad hoc manner 
parameters signi cant learning process describes experiments selection functions improved entropy hut laplace formula cb distance metrics euclidian manhattan bat system addition previous ones 
combine selection functions distance metrics conduct experiments combination collection ml benchmarks taken uci repository mm 
comparison allows point ect selection function distance metric strategy iglue 
main experiments laplace formula function performs signi cantly uniformly better entropy improved entropy 
system learning eciency identical distance metrics selection function xed 
results demonstrate eciency multi strategy approach learns concept galois lattice uses concept knowledge select transform relevant features applying nn algorithm decision process 
overview rest organized follows section presents galois lattice structure 
feature extraction algorithm decision process build iglue system described section 
section presents discusses results experimentation 
explain approach di ers existing methods section 
galois lattice section starts overview galois lattice theory 
lattice construction detailed 
terminology galois lattice mathematical framework allows build embedded classes set objects 
reader familiar notions basic de nitions key terms galois lattice framework 
context triple respectively set examples set attributes binary relation consider galois connection power sets de ned follows 
fa ig set attributes veri ed fo ig set objects share attribute extended formulas concept resp 
called extent resp 
intent concept 
order relation denoted de ned set concepts 
sub concept sub node 
order relation allows establish general speci ordering concepts 
system concepts context mathematical structure lattice pair concepts unique meet unique join called galois lattice notation avoid ambiguity uses term concept especially machine learning lattice concept termed node follows 
de nition regularity conjunction attributes intent node 
general lattice node concept consists pair sets objects attributes closely related mapping functions 
table matrix table context 
ona table shows context fo fa described matrix table object attribute ij value 
fg fo fa fo fa fo fa fo fa fo fa fo fa fo fa fo fa fo fa fo fa fg complete galois lattice previous context 
example illustrate di erent steps presentation 
fo fa node associated regularity complete lattice gure examined bottomup top approach 
bottom resp 
top approach fg resp 
fg lattice supernode fg resp 
fg lattice subnode 
nodes lattice order relation unique node nearest common resp 
subnode 
called join resp 
meet join lattice resp 
meet lattice nodes unique join resp 
meet 
lattice join meet lattice 
galois lattice construction methods galois lattice construction reported including incremental approaches gma cr oos non incremental ones gu bor 
focussed approach bordat non incremental algorithm bor uses top specialization process build links lattice nodes 
bordat algorithm begins general node supernode consisting objects common attributes fg case running example 
search space considered breadth rst order 
pseudocode see fifo list containing lattice nodes denotes lattice construction 
procedure insert insertion node new node sub nodes constructed context order relation 
sub nodes subroutine see pseudo code set attributes program nds attribute restricted objects node considered maximal subset 
lattice const fg fg fg remove sub nodes node insert add edge endfor endwhile return bordat lattice construction algorithm 
space complexity lattice depends matrix context 
attributes conjunctions obtained galois lattice verify concept de nition maximal rectangles value corresponding context 
bm shown galois lattice obtained unique data set 
interesting result knowledge lattice ml system depend training examples ordering 
lattice nodes form partition set objects case decision trees 
object appear di erent nodes 
lattice level increases top approach number objects node decreases number attributes node increases lattice concepts speci nodes top level lattice suitable generalization sense machine learning paradigm 
sub nodes fg attribute column max true equal fag column max false equal equal fa endfor max true equal equal endif endfor return sub nodes construction subroutine 
galois lattice contains maximal nodes called maximal rectangles objects attributes set node objects contains object veri es node attributes set node contains attribute veri ed nodes objects 
galois lattice structure concisely largest search space constructing regularities objects 
iglue system section presents parts iglue system learning decision processes 
iglue rst part uses galois lattice induction learn concept examples 
relevant lattice nodes context performed transforming relevant binary features numeric valued ones 
decision process consists applying nn technique context 
ona class fo fa fo fa fo fa fo fa fo fa xxxxx context join lattice obtained iglue 
learning process part system main steps construction join lattice initial context 
join lattice construction 
examples initial context split set positive examples set negative ones 
system uses modi ed bordat algorithm bor construct join galois lattice positive examples 
shows initial context objects class membership added positive negative examples example join lattice constructed iglue 
assume heuristic selection function decides lattice nodes relevant sense 
relevant nodes denoted solid rectangles 
dashed rectangles computed iglue inserted join lattice don satisfy selection function 
algorithm begins general node consisting positive examples common attributes fo fa case running example 
search space considered breadth rst order 
join algo fg fg fg fg remove sub nodes node level select insert add edge endif endif endfor endwhile return join lattice construction algorithm iglue 
pseudo code see de nitions remain indicated previous section 
set relevant regularities 
level function returns node level 
sub node generation uses algorithm described 
selection function procedure select discard nodes relevant target concept 
select veri es node relevant value selection function lower greater threshold 
selection done lattice construction 
consider lattice node jo resp 
jo number positive resp 
negative examples verify regularity comparative purpose iglue implementation includes di erent functions select relevant nodes chosen user 
entropy function id system qui 
log 
log 
improved entropy function hut ea 

value exponential function 
laplace formula cn system cb la rst functions minimized maximized 
functions advantage empirical process legal mn allow select relevant nodes contain training examples 
focus nodes quality ability best discriminate positive examples negative ones number positive examples contain 
system needs parameter selection function threshold 
parameter value determined system batch process value obtains lowest error rate training set 
user speci ed learning parameter lattice height 
lattice construction nodes level computed node consider 
ik examples features example find set relevant regularities holds regularity attribute regularity add ik endfor endfor endfor algorithm 
training set 
describe process generate new continuous valued features 
set relevant nodes join lattice purpose 
subset containing attributes appear relevant node 
attribute appears relevant lattice nodes relevant studied context 
attribute system generate new features 
set relevant regularities hold object appears nodes element intent 
veri es regularity attribute associate new descriptor de ned relation set new features fd 
dm number attributes new feature de ne number appearances attribute regularities jp table previous context 
ona ond class relation extended example 
feature quantitative variable correspondence number new features inferior equal number original binary features ja jaj jdj jaj 
built scheme explained 
notation ik refer value feature example value ik represents number times feature relevant description considered weight description consequently importance feature description object grows accordance weight value object 
table illustrates previous toy context see 
initial feature doesn appear relevant node built lattice irrelevant attribute 
new generated features respectively correspond original features decision process basic nn algorithm applied predict class membership object considering new description relevant features 
literature reports extensive studies nn algorithms learn examples wet 
similarity objects value distance metric applied descriptions objects 
interested classifying unseen example 
comparative purpose di erent distance measures considered implemented iglue distance ml systems 
distance qs mah jd ik jk ik jk 
manhattan distance sum feature value di erence man jd ik jk 
euclidian distance squares feature value di erence performing nal summation euc ik jk previous equations number relevant features 
manhattan euclidian distance functions equivalent distance function bat respectively 
de nition examples belonging training set test set similarity threshold de ne set training examples similar dist similarity measure resp 
subset positive resp 
negative training examples similar fo dist fo dist dist threshold computed system standard deviation distance example nearest neighbors class 
decision algorithm unseen example positive instance similar training examples number positive ones greater number negative ones js negative instance 
supervised learning scheme error occurs system didn predict correct class example 
experimentations experiments conducted datasets see table details coming uc irvine repository mm 
combine selection function similarity measure bring iglue systems 
report results practical comparison terms prediction accuracy di erent systems ml systems legal ib pebls 
reasons explain choice ml systems legal mn lattice system release qui decision tree frequently comparison ml domain ib aka standard ibl system nally pebls cs ibl system uses sophisticated distance measure symbolic feature domains 
datasets experiments selected arti cial datasets monks problems chess game problem natural ones breast cancer small large soybean congress voting flag credit approval 
table data set code number initial symbolic features indicated 
size test set monks problems 
datasets cross validation cv method applied 
monks ta monks problems corresponding logical concept description 
monks noise added 
breast cancer university wisconsin hospitals madison mw information patient determine su ers benign malignant cancer 
contains unknown attribute values 
soybean small large databases mic characterize soybean diseases 
large dataset rst attributes 
contains unknown attribute values 
votes congressional voting record database sch predict representative member house previous votes 
contains unknown attribute values 
flag flag database collins gem guide flags collins publishers predict religion nation information country land mass ag missing value 
credit credit approval submitted quinlan qui predict application credit card approved 
remove original dataset cases missing attribute value 
chess chess game king rook versus king pawn shapiro sha predict white player win game board description 
table brief description data sets 
data code nb 
class attrib 
examples orig 
bin 
train test monks monks monks breast bc cv soybean ss cv soybean ls cv votes vo cv flag fl cv credit cr cv chess ch cv methodology dataset monks problems training test sets 
apply fold cross validation cv method datasets measure accuracy systems iglue ib pebls 
di erent codes executed pentium machine ram 
entire dataset divided equally sized parts 
part turn test set remaining learning set chess dataset entire set test set remaining learn due memory problems encountered running di erent systems regular cv 
iglue decision process involves nn technique object class nearest instance training set predicted system 
method select iglue selection function threshold follows batch process choose threshold value range gives best result training set 
value test process 
done cross validation fold 
systems default value 
di erent codes executed pentium machine ram 
nominal continuous valued features 
time iglue treats symbolic valued domains 
mixed valued domains symbolic numeric attributes original continuous features normalized added new numerical ones obtained learning processes 
decision process applied 
starting point studies managing mixed valued data sets 
learning multi class problem 
basically iglue learns class time data positive examples target concept lattice construction negative examples lter 
considering data containing classes members class turn taken positive examples members classes negative examples class recognized 
system error rate weighted average errors obtained class 
unknown attribute values 
consider missing value symbolic feature particular value 
continuous features missing value maximal distance non missing value 
results discussion report previous results legal mnn datasets bc ss vo 
run legal datasets ls fl cr ch previous comparison systems legal iglue gives advantage iglue computational complexity classi cation accuracy mnn 
compare running time di erent systems 
classi cation accuracies analysed 
results reported tables 
case iglue system sf denotes selection function entropy improved entropy laplace dm distance measure manhattan euclidian lattice depth rst level second level 
table cpu time spent seconds di erent datasets 
systems data sets bc ss ls vo fl cr ch legal iglue sf dm ib pebls computational complexity table shows comparison time spent legal iglue ib pebls learn concept di erent datasets 
appears iglue faster legal generates relevant regularities comes fact lattice depth parameter iglue interacts system complexity 
limited parameter consider rst levels lattice 
pebls faster systems dataset 
ib better theoretical computational complexity compared iglue 
practice experiments point ectiveness iglue running time especially monks ss fl datasets 
due small number examples datasets description data dependencies example descriptions considerably reduce number lattice nodes 
lattice construction complexity exponential worst case experiments results show iglue running time ecient compared standard ml techniques 
prediction accuracy table presents error rates di erent iglue systems stopping lattice construction rst second levels error rates ml systems legal ib pebls 
goals carrying experiments determine selection function similarity measure plays signi cant role iglue learning process 
selection function xed results quite similar distance metrics 
similarity measures may signi cant ect iglue process 
table error rates obtained legal iglue ib pebls 
data legal iglue ib pebls sf entropy entropy laplace dm bc ss ls vo fl cr ch selection function improved entropy function generally outperforms entropy function 
best results bold generally obtained laplace formula function 
result similar reported clark niblett designing cn decision list system cb 
table indicates di erent results stopping rst levels join lattice 
adding relevant nodes level generally improve accuracy results 
average number nodes considerably increases consequently number relevant features increases level see table 
ect running time system 
consequently lattice level default value set 
ecient lattice level value chosen user di erent tests 
terms accuracy comparison see table iglue entropy function cn distance qs legal demonstrates iglue signi cantly outperformed legal monks problem breast table percentage relevant features 
iglue data entropy improved entropy laplace bc ss ls vo fl cr ch cancer data set 
sets results similar systems slight advantage iglue 
legal obtains better results di erence iglue results signi cant 
laplace formula iglue generally outperformed legal 
pebls system outperformed ib system dataset di erence signi cant 
advantage due distance pebls system 
pebls signi cantly outperformed datasets bc ss conversely signi cantly outperformed pebls datasets vo fl cr ch 
slight advantage tenth dataset ls 
laplace formula stopping lattice construction level iglue signi cantly takes advantage datasets ss fl cr obtains better error rates 
compared pebls iglue takes signi cant advantage datasets fl cr slight advantage vo dataset 
pebls signi cantly outperformed iglue datasets 
comparing systems pebls iglue iglue respectively takes advantage times systems pebls respectively iglue pebls pebls takes advantage times 
experiments point ectiveness iglue compared standard ml technique 
closer investigation prediction accuracy revealed pebls takes advantage data contains noise unknown attribute values bc ss vo 
iglue feature transformation process diculty treating unknown attribute value particular value 
related section compares approach existing works related feature selection feature transformation distance measure ibl respectively 
irrelevant attributes degrade prediction accuracy nn algorithms allowed equally uential high relevance 
di erent methods proposed deal selection relevant features 
feature weighting gives attribute weight proportional uence classi cation wam 
forward resp 
backward sequential selection algorithm constructs optimal set attributes iteratively including resp 
deleting new feature 
random selection attributes combined genetic algorithm proposed skalak ska 
methods consider dependence case galois lattice 
ng ng gives theoretical bound error rate ml systems feature selection process wrapper method 
done assuming selection process nds best subset features 
sebag ss proposed process similar approach described 
problem domain reformulated learned ruleset similarity measure de ned case retrieval 
new problem representation boolean valued 
friedman fri measure optimal problem characterized class distribution inside training set 
proposes exible algorithm resolve dicult choice suitable distance measure 
new instance algorithm evaluates local relevance training instance 
knowledge obtained adapt measure new object nearest neighbors 
distance measures symbolic domains value di erence metric vdm sw including modi ed value di erence metric pebls cs heterogeneous interpolated windowed vdm proposed wilson martinez wm 
original vdm take account numerical valued features 
solve problem introduced discretization process entails information loss 
hvdm ivdm wvdm sophisticated algorithms handle data described symbolic attributes numerical attributes 
results increasing computation cost 
iglue attributes transformation process numerical basic distance measure applied results classi cation 
lachiche marquis lm combines rules instance learning 
rule characterization de ne partial ordering inside training set 
notion closeness rede ned relevant consistent rules explicitly generated 
describes constructive approach machine learning combines feature transformation instance learning 
core approach able translate symbolic features numerical ones 
feature transformation domain reported systems transform numerical features symbolic ones 
feature transformation structure lattice suitable approach 
approach feature transformation generalised domain rule sets decision lists considering regularity semi lattice preconditions antecedent rule 
instance algorithms known best classi ers 
aha aka outlined major drawbacks approach proposed solution rst include reducing storage requirements tolerating noisy instances 
framework proposed able resolve major ibl problems tolerance irrelevant attributes feature selection process allows discard problem choosing appropriate measure nominal valued attributes transformed new numerical valued ones nally lattice framework provides intelligible information relation concepts inside data 
analyzed performance iglue system combines lattice feature transformation ibl techniques 
evaluation done combining selection functions similarity measures conduct experiments di erent known data sets uci irvine repository mm 
results indicate selection function important role process iglue di erent lattice nodes selected relevant chosen function ect similarity measure signi cant 
selection functions experiments laplace formula cb performs better improved entropy hut turn performs better standard entropy function qui 
new selection functions allow improve learning accuracy iglue 
experiments conducted demonstrate ectiveness iglue terms accuracy complexity compared standard ml systems ib pebls 
notion incrementality essential ibl approach drawbacks approach 
built lattice unseen examples represented initial context treated correctly system 
solutions problem 
rst solution include examples initial context rebuild lattice 
second concern current research adapt incremental algorithm lattice gma order build join semi lattice reasonable complexity 
ongoing research dealing prototype problem training set decision process system focus representative examples initial context 
join semi lattice may allow build prototypes 
financial support partially available project de plan etat nord pas de 
grateful 
english proofreading 
anonymous reviewers helpful comments signi cantly improved previous versions 
ag 
nearest neighbor classi cation feature projections 
proceedings th international conference icml pages bari italy july 
aka aha kibler albert 
instance learning algorithms 
machine learning 
bat 
pattern recognition ideas practice 
plenum press new york 
pp 

bir birkho lattice theory 
american mathematical society providence ri rd edition 
bm 
ordre classi cation 
paris universit 
bor bordat 
calcul du treillis de galois une correspondance 
math ematiques sciences 
cb clark boswell 
rule induction cn improvements 
yves editor proceedings fifth european conference ewsl pages berlin germany 
springer verlag 
cn clark niblett 
cn induction algorithm 
machine learning 
cr carpineto romano 
galois order theoretic approach conceptual clustering 
proceedings icml pages amherst july 
cs cost salzberg 
weighted nearest neighbor algorithm learning symbolic features 
machine learning 
ct cleary trigg 
instance learner entropic distance measure 
proceedings machine learning conference pages tahoe city ca usa 
dil dilworth 
decomposition theorem partially ordered sets 
ann 
math 
dom domingos 
unifying instance rule induction 
machine learning 
fri friedman 
flexible metric nearest neighbor classi cation 
unpublished manuscript available anonymous ftp 
stanford edu november 
gan ganascia 
charade rule system learning system 
ijcai pages milan italy 
gan ganascia 
algebraic formalization 
proceedings ijcai volume pages 
gma godin missaoui 
learning algorithms galois lattice structure 
proceedings pages san jos ca usa november 
ieee press 
gu 
construction du treillis de galois une relation 
math ematiques informatique sciences 
hut hutchinson 
algorithmic learning 
clarendon press oxford 
pp 
estimate entropy 
kj kohavi john 
wrappers feature subset selection 
arti cial intelligence december 
ks koller sahami 
optimal feature selection 
proceedings icml pages bari italy july 
lm lachiche marquis 
scope classi cation instance learning algorithm rule characterisation 
rouveirol editors proceedings ecml pages chemnitz germany april 
springer 
lm liu motoda editors 
feature extraction construction selection data mining perspective volume chapter lattice framework tool feature extraction 
kluwer academic publishers boston july 
ls ere 
structural machine learning galois lattice graphs 
proceedings icml pages madison wisconsin july 
mic michalski 
learning told learning examples 
international journal policy analysis information systems 
mm merz murphy 
uci repository machine learning databases 
mn 
galois lattice framework concept learning 
design evaluation re nement 
proceedings pages new orleans louisiana la november 
ieee press 
mnn 
lattice framework tool feature extraction 
proceedings european conference machine learning ecml lnai pages chemnitz germany april 
mw mangasarian wolberg 
cancer diagnosis linear programming 
siam news 
ng ng 
feature selection learning exponentially irrelevant features training examples 
proceedings icml 
university wisconsin madison 

iglue instance learning system lattice theory 
proceedings pages newport beach california usa november 
ieee press 
oos 
lattice knowledge discovery 
proceedings aaai knowledge discovery databases workshop pages anaheim 
qs 
expansion compression binary data build feature data 
proceedings international conference pattern recognition 
qui quinlan 
induction decisions trees 
machine learning 
qui quinlan 
programs machine learning 
morgan kaufmann publishers los altos california 
sah sahami 
learning classi cation rules lattices 
proceedings ecml pages crete greece april 
nada lavrac stefan wrobel eds 
sal salzberg 
distance metrics instance learning 
proceedings th international symposium methodologies intelligent systems pages 
sch schlimmer 
concept acquisition representational adjustment 
phd thesis department information computer science university california irvine ca 
sha shapiro 
role structured induction expert systems 
phd thesis university edinburgh 
ska skalak 
prototype feature selection sampling random mutation hill climbing algorithms 
proceedings icml pages new brunswick new jersey july 
renaud 
apprentissage automatique 
une exp acquisition de dans le domaine de la 
editor proceedings pages october 
inria press 
ss sebag schoenauer 
rule similarity measure 
richter editor proceedings rst european workshop case reasoning 
springer verlag 
sw stan ll waltz 
memory reasoning 
communications acm 
ta thrun monk problems 
performance comparison di erent learning algorithms 
technical report cmu cs carnegie mellon university december 

exon prediction genomes 

wam wettschereck aha mohri 
review empirical evaluation feature weighting methods class lazy learning algorithms 
arti cial intelligence review april 
special issue lazy learning kluwer academic publishers 
wet wettschereck 
study distance machine learning algorithms 
doctoral dissertation department computer science oregon state university 
wil wille 
concept lattices conceptual knowledge systems 
computer mathematic applied 
wm wilson martinez 
improved heterogeneous distance functions 
journal arti cial intelligence research 
zheng 
ects di erent types new attributes constructive induction 
proceedings pages 

