clusterings bad spectral ravi kannan department computer science yale university new haven 
email kannan cs yale edu santosh vempala adrian department mathematics cambridge 
email math mit edu july 
motivate develop new bicriteria measure assessing quality clustering avoids drawbacks existing measures 
simple recursive heuristic poly logarithmic worst case guarantees new measure 
main result analysis popular spectral algorithm 
variant spectral clustering turns ective worst case guarantees nds clustering exists 
supported part nsf ccr 
supported nsf career award ccr 
supported part nsf career award ccr part wolfe fellowship 
clustering partitioning dissimilar groups similar items problem variants mathematics applied sciences 
availability vast amounts data research problem 
years clever heuristics invented clustering 
heuristics problem speci method known spectral clustering applied successfully variety di erent situations 
roughly speaking spectral clustering technique partitioning rows matrix components top singular vectors matrix see section detailed description 
main motivation analyze performance spectral clustering 
evaluation inextricably linked question measure quality clustering 
justi cation provided practitioners typically case case experimental works data 
theoreticians busy studying quality measures simple de ne median minimum sum minimum diameter 
measures far analyzed theoreticians easy fool simple examples right clustering obvious optimizing measures produces undesirable solutions see section 
approach entirely satisfactory 
propose new bicriteria measure quality clustering expansion properties underlying pairwise similarity graph 
quality clustering parameters minimum conductance clusters ratio weight inter cluster edges total weight edges 
objective nd clustering maximizes minimizes 
note conductance provides measure quality individual cluster clustering whilst weight inter cluster edges provides measure cost clustering 
imposing lower bound quality individual cluster strive minimize cost clustering conversely imposing upper bound cost clustering strive maximize quality 
section motivate complex bicriteria measure showing drawbacks simpler quality measures 
new measure qualitatively attractive little optimizing computationally intractable 
section study recursive heuristic designed optimize new measure 
nding exact solution np hard algorithm shown simultaneous poly logarithmic approximations guarantees parameters bicriteria measure theorems 
section turn spectral algorithms clustering 
algorithms attractive part speed see section 
performance hitherto rigorous analysis 
new measure turns conducive theoretical analysis 
particular show recursive version spectral clustering ective worst case approximation guarantees respect bicriteria measure theorem 
variant spectral clustering guarantee input data clustering large small spectral algorithm nd clustering close optimal clustering theorem 
spectral clustering algorithms spectral clustering refers general technique partitioning rows matrix components top singular vectors matrix 
underlying problem clustering rows matrix ubiquitous 
mention special cases independent interest matrix encodes pairwise similarities vertices graph 
rows matrix points dimensional euclidean space 
columns coordinates 
rows matrix documents corpus 
columns terms 
entry encodes information occurrence jth term ith document 
matrix spectral algorithm clustering rows 
spectral algorithm find top right singular vectors matrix jth column au place row cluster ij largest entry ith row algorithm interpretation suppose rows points high dimensional space 
subspace de ned top right singular vectors rank subspace best approximates spectral algorithm projects points subspace 
singular vector de nes cluster obtain clustering map projected point cluster de ned singular vector closest angle 
section describe recursive variant algorithm 
clustering 
spectral algorithm 
intuitively clustering algorithm performs points similar assigned cluster points dissimilar assigned di erent clusters 
course may possible pair points compare clustering algorithm optimal matrix 
leads question exactly optimal clustering 
provide quantitative answer rst need de ne measure quality clustering 
years combinatorial measures clustering quality investigated detail 
include minimum diameter center median minimum sum example 
measures mathematically attractive due simplicity easy fool 
construct examples property best clustering obvious algorithm optimizes measures nds clustering substantially di erent unsatisfactory 
examples figures aim partition points clusters 
observe measures computationally useful note jth column jth singular vector jth left singular vector 
seek minimize objective function 
gures nearby points represent highly similar points induce low cost edges points farther apart dissimilar induce high cost edges 
optimizing diameter produces clearly desirable 
consider clustering minimizes maximum diameter clusters diameter cluster largest distance say points cluster 
np hard nd clustering main concern 
example shown optimal solution produces cluster contains points separated 
clustering respect minimum sum center measures produce result 
reason poor cluster produced minimized maximum dissimilarity points cluster expense creating cluster dissimilar points 
clustering hand leads larger maximum diameter say desirable better satis es goal similar points dissimilar points apart 
problem arises median measure see example case shown may produce clusters poor quality 
inferior clustering optimizing median measure 
purpose motivating proposed measure model clustering problem edge weighted complete graph vertices need partitioned 
weight edge ij represents similarity vertices points graph points space high edge weights points close low edge weights points far apart 
note di erence measures similar points induce low weight edges 
associated graph symmetric matrix entries ij assume ij non negative 
return question clustering quality cluster determined similar points cluster 
note cluster represented subgraph 
particular cut small weight divides cluster pieces comparable size cluster lots pairs vertices dissimilar low quality 
suggest quality subgraph cluster minimum cut subgraph 
misleading illustrated 
ii second subgraph higher quality cluster smaller minimum cut 
example edges represent high similarity pairs non edges represent pairs highly dissimilar 
minimum cut rst subgraph larger second subgraph 
second subgraph low degree vertices 
second subgraph higher quality cluster 
attributed fact rst subgraph cut weight small relative sizes pieces creates 
quantity measures relative cut size expansion 
expansion graph minimum ratio cuts graph total weight edges cut number vertices smaller part created cut 
formally denote expansion cut ij min jsj sj say expansion graph minimum expansion cuts graph 
rst measure quality cluster expansion subgraph corresponding 
expansion clustering minimum expansion clusters 
measure de ned gives equal importance vertices graph 
may lead requirement 
example order accommodate vertex little similarity vertices combined ij small low 
arguably prudent give greater importance vertices similar neighbors lesser importance vertices similar neighbors 
done direct generalization expansion called conductance subsets vertices weighted re ect importance 
conductance cut denoted ij min ij conductance graph minimum conductance cuts graph min 
order quantify quality clustering generalize de nition conductance 
take cluster cut cns say conductance cns ij min conductance cluster smallest conductance cut cluster 
conductance clustering minimum conductance clusters 
conductance measure extremely suited achieve intuitive goal clustering similar points separating dissimilar points 
obtain optimization problem graph integer nd clustering maximum conductance 
notice optimizing expansion conductance gives right clustering examples figures 
see assume example points induce unweighted graph zero edge weights 
pair vertices induces edge vertices close 
clustering obtain example 
problem clustering measure 
graph consist clusters high quality points create clusters poor quality clustering necessarily poor quality de ned quality clustering minimum clusters 
fact boost quality best clustering create clusters relatively low quality minimum large possible 
example shown 
assigning outliers leads poor quality clusters 
way handle problem avoid restricting number clusters 
lead situation points singleton extremely small clusters 
measure quality clustering criteria rst minimum quality clusters called second fraction total weight edges covered clusters called 
de nition 
call partition fc partition 
conductance 
total weight inter cluster edges fraction total edge weight 
obtain bicriteria measure quality clustering 
associated bicriteria measure optimization problem note number clusters restricted 
problem 
nd partition minimizes alternatively nd partition maximizes 
observe quality measure clustering induces maximization problem minimization problem inter cluster edge weights induce minimum cost problem 
measure cluster quality aware bad examples clustering problem examples solving bicriteria optimization problem gives clustering clearly inferior best clustering 
important question observation holds systematic experimental study 
focus rest consider measure theoretic standpoint examine detail performance spectral clustering algorithms 
may noted monotonic function represents optimal pairings 
example minimum value equal partition exists 
sections approximation algorithms clustering property 
nice characteristic algorithms single application obtain approximation entire function just evaluated single point 
user need specify desired value priori 
desired conductance cost trade may determined consideration approximation function approximation algorithm observe problem np hard 
see consider maximizing whilst setting zero 
problem equivalent nding conductance graph 
result may appropriate develop approximation guarantees problem 
simple heuristic outlined show poly logarithmic approximation algorithm clustering problem 
approximate cut algorithm find approximate sparsest cut recurse pieces induced cut 
idea algorithm simple 
nd cut minimum expansion 
recurse subgraphs induced note algorithm terminates set clusters consisting just vertex 
nding cut minimum conductance hard implement algorithm polynomial time current form 
simple modi cation allows polynomial implementation 
leighton rao gave polynomial time algorithm nds cut conductance log times minimum 
nding cut minimum conductance step apply leighton rao procedure nd approximate sparsest cut 
call cuts produced algorithm js jt note just subset partitioned particular stage 
may represent partitioning process tree nodes clusters induced stage algorithm tree cluster parent clusters may obtain approximation guarantees problem 
trace path leaf node root note 
mark node cluster path conductance maximal marked nodes created method form clustering 
order analyse performance algorithm observe equivalent way generate particular clustering viz run approximate cut algorithm additional condition recurse subgraphs conductance particular consider algorithm termination conductance produces clusters conductance log due fact approximate sparsest cuts 
obtain poly logarithmic guarantees problem 
consider situation quality cluster expansion measure 
theorem 
partition log log partition approximate cut algorithm quality cluster measured respect expansion measure proof 
consider cuts 
time algorithm induces partition vertex set graph 
clearly termination cluster desired expansion set termination expansion log remains examine performance clustering respect weight inter cluster edges 
optimal clustering 
observe twice total edge weight graph weight inter cluster edges optimal solution 
divide cuts groups rst group ones high expansion clusters 
second group consists remaining cuts 
formally de ning groups give brief motivation proof follows 
rst part proof involves bounding cost group high expansion cuts 
cut high expansion optimal clusters low expansion 
occur cut similar original cut close avoids optimal clusters 
allows bound cost high expansion cuts respect cluster avoiding cuts 
cost cluster avoiding cuts turn bound respect cost optimal clustering 
second part proof consists bounding cost group low expansion clusters cuts 
note intuitively low expansion cuts low cost 
actual proof consists bounding cost low expansion group terms cost high expansion group 
notation uv addition denote sum weights intra cluster edges cut 
de ne group cuts high expansion optimal clusters fj min js jt declared rst deal high expansion group 
js min js jt consequence min js jt js de ne cluster avoiding cut manner 
js jt place js jt place example process shown original cut solid line cluster avoiding cut dashed line 

recall approximate sparsest cut sparsity factor log optimal cut 
particular sparsity factor log sparsity cut 
additionally obtain min js jt js observations give js log min js jt js bound cost high expansion cuts respect cuts 
bound cost cluster avoiding cuts 
denote set inter cluster edges incident vertex subset edge inter cluster edge 
log nw claim allow bound sum costs cut claim 
vertex log values belongs prove claim consider vertex fj fj clearly jij log js js show jj log take fact follows partition subset 
jt js jt addition ii jt js concepts shown pictorially cuts represented solid lines cuts dashed lines 

ii obtain jt jt order values magnitude size halves successive indices follows jj log proving claim 
able bound cost group cuts terms cost optimal clustering 
log max jij jj log deal group cuts low expansion clusters suppose cuts induce partition order jp largest 
edge vertices belong di erent sets partition cut cut conversely edge cut points di erent sets partition 
expansion obtain min jp jc vertex log jc log values belongs smaller sets furthermore jp jc belongs smaller sets min jp jc jp log min js jt log min js jt de nition min js jt able bound intra cluster cost low expansion group cuts terms intra cluster cost high expansion group 
applying gives log addition inter cluster edge appears cut adding obtain theorem 
consider general case quality cluster measured respect conductance 
observe function 
additive function subsets property disjoint akin cardinality function 
observation may prove similar methods theorem guarantees general case 
theorem 
partition log log log partition approximate cut algorithm quality cluster measured respect conductance measure 
assess running times algorithms 
implementing approximate sparsest cut procedure 
fastest implementation procedure due karger runs time 
addition note algorithms cuts 
heuristics total running time 
may slow real world applications 
discuss faster practical algorithm section 
performance guarantees spectral clustering section describe analyse time variant spectral algorithm 
algorithm outlined eld computer vision eld web search engines 
note algorithm similar approximate cut algorithm described previous section 
spectral algorithm ii normalize find nd right eigenvector find best ratio cut wrt recurse pieces induced cut 
observe normalization matrix conductance measure corresponds familiar markov chain conductance measure ij min ij min stationary distribution markov chain 
elaborate basic description variant spectral algorithm 
initially normalize matrix rows sums equal 
stage algorithm clustering fc consider jc jc submatrix restricted normalise setting ii ij non negative row sums equal 
nd second eigenvector right eigenvector corresponding second largest eigenvalue bv order elements rows respect component direction ordering say fu nd minimum ratio cut cut minimises fu 
recurse pieces fu fu purpose analysis assume apply recursive step algorithm conductance piece addition simplicity analysis consider slightly modi ed algorithm 
point cluster property split singletons output vertices 
notice operation may performed small cost singletons little importance 
observe algorithm performs modi ed version 
worst case guarantees theorem prove worst case guarantee theorem 
result essentially proved sinclair jerrum proof lemma mentioned statement lemma 
completeness due fact theorem usually explicitly stated markov chain literature usually includes conditions relevant include proof result 
observe second eigenvalue theorem bounds conductance ratio cut respect optimal cut 
theorem 
suppose matrix non negative entries row sum equal suppose positive real numbers summing ij ji right eigenvector corresponding second largest eigenvalue ordering min ng ij min min min proof 
rst evaluate second eigenvalue 
diag 
time reversibility property symmetric 
eigenvalues largest eigenvalue equal 
addition left eigenvector corresponding eigenvalue 
max substituting obtain min min numerator rewritten ij ii ij ij ij denote nal term 
min prove rst inequality theorem cut minimum conductance 
de ne vector follows easy check obtain desired lower bound conductance 
prove second inequality 
suppose minimum attained equal dv eigenvector corresponding eigenvalue right eigenvector corresponding ordering respect accordance statement theorem 
assume simplicity notation indices reordered rows corresponding columns reordered 
de ne satisfy 



ij ij jz jz ij jz jz consider numerator nal term 
cauchy schwartz ij ij jz jz ij jz jz jz ij jz second inequality follows fact jz jz jz jz follows observations sign fi jg jz jz jz jz ii di erent signs jz jz jz jz jz ij jz jz ij result ij ij jz jz ij jz jz ij jz 
set kg jg min ij min obtain ij jz jz ij consequently ready provide approximation guarantees spectral algorithm 
worst case guarantees obtained similar methods applied theorems 
theorem 
partition spectral algorithm ii nd log log partition 
proof 
cuts produced algorithm adopt convention smaller side 
optimal clustering 
termination condition log gives clusters claimed conductance 
prove bound cost clustering produced rst divide cuts groups 
rst group ones high conductance clusters second group rest 
fj min bound cost high conductance group 
min consequently observe min de ne cluster avoiding cut manner 
place place notice ja min 
conductance theorem generate upper bound terms 
uv vu respect normalised matrix follows initial symmetry 
note algorithm order elements component second largest eigenvector take contiguous cut minimum conductance order 
applying theorem obtain min uv min uv min uv min uv min bound cost high conductance cuts respect cost cluster avoiding cuts 
bound cost cluster avoiding cuts 
denote set inter cluster edges incident vertex subset edge inter cluster edge 
claim allow bound sum costs cut claim 
vertex log values belongs log values belongs prove claim vertex fj fj clearly partition subset 
reduces factor greater successive times belongs maximum value minimum value rst statement claim follows 
suppose suppose subset partitioned 

halves successive times jj log proves second statement claim implies 
able bound cost group cuts high conductance clusters respect cost optimal clustering 
achieve cauchy schwartz inequality 
log log log deal group cuts low conductance clusters suppose cuts induce partition edge vertices belong di erent sets partition cut cut conversely edge cut points di erent sets partition 
conductance obtain min vertex log values belongs smaller 
sets min log min log min de nition min able bound intra cluster cost low conductance group cuts terms intra cluster cost high conductance group 
applying gives log addition inter cluster edge belongs cut sum 
note splitting singletons costs 
recalling equals twice total sum edge weights bound regarding cost inter cluster edge weights follows 
note part conductance log trivially true singleton sets 
theorem follows 
presence clustering section consider situation input matrix particularly clustering 
matrix partitioned blocks conductance block cluster high total weight inter cluster edges small 
result shows circumstance spectral algorithm nd clustering close optimal clustering small items number rows placed incorrect cluster 
show model situation 
terminology 
denote jxj norm kxk norm vector norm matrix max kxk assume normalized row 
addition assume written block diagonal matrix corresponds set edges run clusters 
consist blocks induce clusters optimal clustering 
conductance easier state result terms minimum eigenvalue gap blocks eigenvalue gap matrix closely related conductance 
ideally show large block total weight edges small spectral algorithm works 
expected typical input possible construct examples applying spectral algorithm 
handle problem consider modi ed version spectral algorithm alterations 
firstly assign clusters regards matrix rth column au 
assignment regards randomly chosen orthonormal matrix 
secondly adjust assignment process 
maximum entry column column chosen uniformly range 
cluster assignment follows 
pair rows placed separate clusters ir jr ir jr rows placed cluster 
note process may create clusters 
theorem shows large norm small spectral algorithm nds clusters close optimal ones 
analyses similar spirit see papadimitriou fiat 
theorem 
eigenvalue gap cluster optimal clustering 
addition di erence kth th eigenvalues suppose norm constant optimal cluster sizes bounded ratio spectral algorithm applied nds clustering di ers optimal rows 
proof 
matrix viewed perturbation matrix rst eigenvectors denoted similarly rst eigenvectors denoted ij 
similarly de ne ij ey diagonal matrix top eigenvalues de ne ax theorem stewart theorem page states matrix norm kek kek columns form orthonormal basis subspace invariant orthonormal matrix follows kfk kek kek suitable rotation take matrix jth column support rows recall partition rows respect matrix ax interested di erence clusterings induced denote ith row respectively 
kc kek kfk particular ith row kd fraction close assume rows misplaced consider rows 
knew matrix done 
fact know reason multiply orthonormal matrix clustering 
cases consider 
rows optimal cluster respect follows ka show close 
jc ir jr ja 

ju 
ku ju constant probability separated spectral algorithm jc ir jr jc ir jr sk ii rows di erent optimal clusters respect roughly roughly orthogonal 
expected magnitude ju 

constant addition log high probability 
probability separated log total number misclassi ed rows sk log 
theorem generalized case non symmetric matrices corresponding theorem stewart invariance left right singular subspaces 
basic aspects analyzing clustering algorithm quality clustering produced 
speed fast 
dealt issue care algorithms polynomial time 
spectral algorithms depend time takes nd top top singular vector 
done exactly polynomial time expensive applications information retrieval 
randomized algorithms low rank approximation addresses problem 
running time rst algorithm depends quality desired approximation size matrix assumes entries matrix sampled speci manner 
second algorithm needs assumptions running time linear number non zero entries 
observe approximation factors algorithms similar form 
function giving approximation sparsest cut log algorithms 
suppose run algorithms termination conductance obtain log clustering 

anna karlin pointing error earlier version 
azar fiat karlin mcsherry 
spectral analysis data 
proc 
rd stoc 
karger 
approximate min cuts time 
proc 
th stoc pp 
charikar chekuri feder motwani 
incremental clustering dynamic information retrieval 
proc 
th stoc 
charikar guha shmoys tardos 
constant factor approximation median problem 
proc 
st stoc pp 
drineas frieze kannan vempala vinay 
clustering large graphs matrices 
proc 
th soda 
dyer frieze 
simple heuristic center problem 
oper 
res 
pp 
frieze kannan vempala 
fast monte carlo algorithms nding low rank approximations 
proc 
th focs 
indyk 
sublinear time approximation scheme clustering metric spaces 
proc th focs pp 
jain vazirani 
primal dual approximation algorithms metric facility location median problems 
proc 
th focs pp 
leighton rao 
approximate max ow min cut theorem uniform multicommodity ow problems applications approximation algorithms 
proc 
th focs pp 
papadimitriou tamaki vempala 
latent semantic indexing probabilistic analysis 
proc 
th symposium principles database systems 
shi malik 
normalized cuts image segmentation 
ieee conf 
computer vision pattern recognition 
see www cs berkeley edu grouping sinclair jerrum 
approximate counting uniform generation rapidly mixing markov chains 
information computation pp 
spielman teng 
spectral partitioning works planar graphs nite element meshes 
proc 
th focs 
stewart error perturbation bounds subspaces associated certain eigenvalue problems 
siam review pp 
cluster cs yale edu 
