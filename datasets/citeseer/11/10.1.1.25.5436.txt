originally published proceedings usenix nd symposium operating systems design implementation seattle washington october information usenix association contact 
phone 
fax 
email office usenix org 
www url www usenix org synergy non blocking synchronization operating system structure michael greenwald david cheriton stanford university synergy non blocking synchronization operating system structure michael greenwald david cheriton computer science department stanford university stanford ca non blocking synchronization significant advantages blocking synchronization significant degree practice 
designed implemented multiprocessor operating system kernel run time library high performance reliability modularity 
nonblocking synchronization objective approach choice 
attractive approach synergy structuring techniques achieve primary goals benefits non blocking synchronization 
describes synergy structuring techniques facilitated non blocking synchronization experience implementation 
chose non blocking synchronization design implementation cache kernel operating system kernel supporting libraries reasons 
non blocking synchronization allows synchronized code executed asynchronous signal handler danger deadlock 
instance asynchronous rpc handler described directly store string synchronized data structure hash table may interrupting thread updating table 
locking signal handler deadlock thread 
second non blocking synchronization minimizes interference process scheduling synchronization 
example highest priority process access synchronized data structure delayed blocked lower priority process 
contrast blocking synchronization low priority process holding lock delay higher priority process effectively cs stanford edu 
sponsored part arpa army contract dabt 
michael greenwald supported rockwell fellowship 
defeating process scheduling 
blocking synchronization cause process delayed process encountered page fault cache delay hundreds thousands cycles case page fault 
type interference particularly unacceptable os cache kernel real time threads supported page faults non real time threads handled library level 
non blocking synchronization minimizes formation arise processes queued waiting single process holding lock gets delayed 
non blocking synchronization provides greater failures fail failing aborting leaving inconsistent data structures 
non blocking techniques allow small window inconsistency atomic compare sequence 
contrast lock window inconsistency spans entire locked critical section 
larger critical sections complex locking protocols introduce danger deadlock failure release locks certain code paths 
strong synergy non blocking synchronization design implementation cache kernel performance modularity reliability 
signals kernel supported form notification allowing simple efficient kernel implementation compared complex kernel message primitives 
class libraries implement higher level communication rpc terms signals shared memory regions 
non blocking synchronization allows efficient library implementation overhead disabling enabling signals part access needing carefully restrict code executed signal handlers 
second simplified kernel allows specialization facilities inheritance mechanism operating system mechanisms class library level particularly object oriented rpc system 
non blocking synchronization allows class library level tolerant user threads terminated fail stopped middle performing system library function re scheduling handling page fault 
isolation synchronization scheduling thread deletion provided non blocking synchronization separate class libraries user level implementation services leads modular reliable system design feasible conventional approaches 
synergy non blocking synchronization system design implementation carries forward detailed aspects cache kernel implementation 
describe aspects synergy detail experience date 
main techniques modularity performance reliability atomic dcas double compare swap type stable memory management tsm contention minimizing data structures cmds 
dcas discussed detail section defined 
dcas atomically updates locations addr addr values new new respectively addr holds value old addr holds old operation invoked 
section describes type stable memory management facilitates implementing non blocking synchronization providing independent benefits software structure 
section describes contention minimizing data structures benefits performance reliability lock non 
section describes approach minimizing window inconsistency systems benefits doing 
section describes non blocking synchronization implementation detail comparison blocking implementation 
section describes non blocking synchronization primitives assumed approach potential hardware implementation 
section describes performance implementation simulation show behavior high contention 
section describes effort relates previous current area 
close summary directions 
type stable memory management tsm type stable memory management tsm refers management memory allocation reclamation allocated portion memory descriptor change type time bound stable int dcas int addr int addr int old int old int new int new addr old addr old addr new addr new return true return false pseudo code definition dcas double compare swap fancy name extension old idea 
example process descriptors operating systems statically allocated system initialization type stable lifetime system execution 
notion tsm incorporates basic extensions conventional type implementation 
descriptor remains valid instance type active free list 
second tsm allows multiple memory allocation pools type 
example pool thread descriptors cluster processors large scale multiprocessor minimize contention clusters 
type portion memory change time long type stable time stable specifically descriptor inactive stable reallocated different type simplicity assume infinite stable discussion 
tsm simplifies implementation non blocking synchronization algorithms 
descriptor type type stable pointer type descriptor pointing descriptor type result area memory freed reallocated type 
consider example code shown non blocking deletion linked list delete operation searches linked list descriptors find desired element detect tsm implementation collection descriptors stored set page frames allocated released time 
descriptors required additional page frames allocated general pool number descriptors falls descriptors may consolidated smaller number pages excessive page frames returned general pool 
release page frames general pool delayed sufficiently ensure type stability property 
delay provides useful hysteresis movement pages descriptor collection general page pool 
list initialized dummy node head deletion element works correctly 
delete elt retry version list version list head elt null version list version goto retry changed return null really dcas list version version elt version elt deletion middle list protected dcas version number 
list 
element element atomically deleted list dcas operation 
dcas succeeds list modified delete operation started determined version field 
code checks conflicts reaches desired element list 
descriptors tsm pointer guaranteed point descriptor type 
tsm link pointer may point descriptor deleted reallocated different type 
type error cause random interpreted pointer cause search perform incorrectly raise exception due unaligned access read device register 
tsm simpler efficient way ensuring type safety techniques aware prevent reallocation automatic garbage collection mechanisms counts detect potential reallocation element version numbers 
benefits non blocking synchronization tsm important advantages construction modular reliable high performance operating systems 
tsm efficient memory allocator normally allocate instance type faster general purpose allocator 
example allocation new thread free list fixed size thread descriptors simple dequeue operation general purpose allocator malloc may search subdivision memory resources 
class specific new delete operators support clean source code representation tsm 
allocation efficient types free inactive descriptor instance type may require initialization allocation random portion memory 
second tsm aids reliability easier audit memory allocation locating descriptors type ensuring pointers supposed point descriptors type 
fixed size descriptors tsm avoids fragmentation memory arises general purpose allocators 
fragmentation cause failure poor performance 
tsm easier regulate impact type descriptor system resources 
example collection descriptors allocated dynamically page frame approach described number pages dedicated type controlled avoid exhausting memory available uses fragmentation memory 
tsm minimizes complexity implementing caching model descriptors operating system kernel 
approach number descriptors type limited allocation fails 
cache descriptor available dirty data written back higher level system management reused satisfy new allocation request 
mechanism relies number descriptors able locate allocated descriptor reclaim able determine dependencies descriptors 
tsm simplifies code cases 
tsm allows modular implementation 
object oriented programming standpoint base class descriptor manager class specialized type descriptor 
example class operating system kernel provides basic tsm allocation mechanism specialized derivation implement thread types types 
data structures minimize contention cache kernel designed implemented minimize logical physical contention provide efficient non blocking synchronization 
logical contention mean contention access data structures need controlled maintain consistency semantics data structures 
physical contention mean contention access shared memory needs controlled maintain consistency semantics memory system physical contention separate logical contention logical contention physical contention vice versa called false sharing 
example shared vari minimizing logical contention non blocking synchronization minimizes overhead conflicting operations failing retried 
avoids complexity complex backoff mechanisms part retry 
techniques contention minimization known 
example aspect contention minimization replicating data structures processor 
particular processor ready delay queues cache kernel contention structures limited signal interrupt handlers management operations load balance executed separate processor 
similarly signal delivery cache processor allows significant number signals delivered processor accessing shared signal mapping data structure replicating entire structure 
processor cache approach similar provided processor tlb address translation 
tlb reduces access real virtual address space mapping structure necessarily shared threads address space 
contention data structure reduced cases structuring multi level hierarchy 
example list searched frequently may revised hash table version number lock bucket 
searches updates localized single bucket portion list reducing conflict operations assuming hash different buckets 
upper levels hierarchy read read descriptors added leaves 
physical contention reduced descriptors 
tsm restricted allocation descriptors reduce number pages referenced part scan search operations reducing tlb rate source physical contention 
vein commonly updated fields placed contiguously aligned hopefully place cache line making updates efficient 
spatial locality data access achieved techniques provides significant benefit synchronization non blocking conventional locks 
spatial locality minimizes consistency overhead system running multiple processors caching portions shared data 
general experience suggests better re structure data structures reduce contention attempt improve behavior able reside cache line unit physical contention logical contention processor attempt update variables simultaneously processor updating separate variable 
synchronization techniques high contention 
algorithms simpler easier get right faster long contention low 
minimizing window inconsistency cache kernel structured minimize window data structure inconsistent 
provides temporal locality critical section 
familiar techniques 
basic pattern read values compute new values written write new values verifying values read changed 
structure generally inconsistent time write point write completes removing computation phase minimizes window inconsistency 
minimize cost verifying read values changed version number covers data structure updated data structure changes 
version number avoids keeping track actual location read part operation 
window inconsistency minimized structuring minimize physical contention part data structure access 
physical contention increases time processor perform operation increases effective memory access time 
techniques allow efficient non blocking synchronization 
particular update typically consists dcas operation updates version number plus location version number ensuring data structure changed concurrent update 
window inconsistency reduced execution dcas operation 
techniques benefits 
particular reduced window inconsistency reduces probability failure thread termination corrupting system data structures 
reduce complexity getting critical section code right shorter fewer separate control paths easier test 
structuring beneficial required implementation lock synchronization reduces lock hold time reducing contention 
non blocking synchronization implementation structuring cache kernel supporting class libraries described non blocking synchronization relatively simple implement 
data structures collections fixed size descriptors 
collections queues service 
example thread descriptors queued ready queue delay queue associated processor 
collections lookup search structures hash table linked list buckets 
example organize page descriptors lookup structure address space supporting virtual physical mapping address space 
base approach non blocking synchronization structures follows common base structure 
version number list 
dcas primitive atomically perform write descriptor list increment version number checking previous value changed conflicting access list 
illustrated structure deleting descriptor list single write descriptor change link field predecessor descriptor 
inserting new descriptor entails initializing locating descriptor linked list insert writing link field point descriptor performing dcas write link field prior descriptor increment version checking locations contention part update 
dequeuing descriptor tsm free list degenerate case deletion dequeue takes place head 
possible optimize case single cas dequeue version number 
efficient dcas support attractive dcas version number allow version number count number allocations take place 
special case operation requiring locations reads writes updated directly dcas 
approach array stacks fifo queues 
operations involve multiple writes descriptor performed creating duplicate descriptor performing modifications atomically replacing old descriptor new descriptor list changed duplicate descriptor created 
approach variant herlihy general methodology convert sequential implementation data structure wait free concurrent 
dcas ensure atomicity respect entire data structure scope version number copying single descriptor variant basic herlihy approach involves copying entire data structure modifying copy atomically replacing old copy new copy cas retrying entire copy modifying conflict 
approach reduces allocation copy cost single descriptor entire data structure requires dcas 
approach code duplicate just portion descriptor update dcas insert place original updating version number 
thread fails completing insertion rely audit reclaim partially initialized descriptor unclaimed stable time 
optimization data structures allow descriptor removed modified reinserted long deletion reinsertion done atomically 
optimization saves cost allocating freeing new descriptor compared previous approach 
approach requires operations tolerate inconsistency descriptor list period time 
example cache kernel signal delivery relies list threads signal delivered 
thread fails get signal list time signal generated 
defined signal delivery best effort reasons signal drop having signal delivery fail thread update violation signal delivery semantics 
programming higher level software best effort signal delivery required incorporating timeout retry mechanisms required distributed operation case add significant overhead 
techniques related transport layer network protocols system resilient faults 
note just having search mechanism retry search fails conjunction approach lead deadlock 
example signal handler attempts access descriptor retrying successful called stack thread removed perform update signal handler effectively deadlocks thread 
dealing multiple lists descriptor supposed multiple lists simultaneously complicates procedures 
far feasible program descriptor subset lists inserted deleted list atomically separate operations 
particular data structures allow descriptor absent list allow descriptor inserted incrementally 
major cache kernel data structures synchronized straightforward manner 
threads linked lists ready queue delay queue 
descriptor free lists operated stacks making allocation deallocation simple inexpensive 
virtual physical page maps stored tree depth widths respectively 
immediate descendants root deleted sub trees unloaded 
modifications map level synchronized dcas parent version number sure entire subtree modified conflict update 
cache kernel maintains dependency map records dependencies objects including physical virtual mappings 
implemented fixed size hash table linked lists bucket 
signal mapping cache structure optimization signal delivery active threads direct mapped hash table linked lists bucket 
majority uses single cas audit counters 
synchronization complex data structures encountered handled operation allocating initializing enqueuing message server process serially executes requested operations 
read operations proceed relying version number incremented server process 
server process run high priority include code back operation page fault really block operation anymore operation executed directly requesting process 
server process carefully protected failure data structure protected fail behavior random application thread may destroyed application 
approach pu massalin 
example general purpose memory page allocator synchronized manner relying tsm memory pool minimize access general allocator 
code date case queueing messages server module arises device structure avoids waiting device complete motivated synchronization issues 
investigated alternatives optimizations approach helper functions executed new thread left complete rollback previous thread accessing data structure 
example israeli describe non blocking heap implemented word ll sc lines performing multiple updates multiple distinct operations 
date needed employ called helper techniques comment actual practicality utility 
questionable reliability standpoint threads separate address spaces sharing access complex data structures 
data structures difficult program maintain provide marginal performance benefits practice particularly synchronization overhead taken account 
asymptotic performance benefits realized scale typical operating system data structures 
comparison blocking synchronization structuring described needed beneficial software blocking synchronization 
instance tsm strong set benefits contributing techniques minimizing contention reducing window inconsistency 
programming complexity non blocking synchronization similar conventional blocking synchronization 
differs experience programmers cas systems 
dcas plays significant part complexity reduction 
crude metric lines code cas implementation valois concurrent insertion deletion linked list requires lines corresponding dcas implementation requires non concurrent dcas implementation takes 
cas implementation fifo queue described requires lines dcas version 
dcas versions correspondingly simpler understand informally verify correct 
cases dcas translation understood blocking implementation non blocking straightforward 
simple case described initial read version number replaces acquiring lock dcas replaces releasing lock 
fact version numbers analogous locks ways 
version number scope shared data structure controls contention data structure just lock 
scope version number chosen degree concurrency balanced synchronization costs 
degree concurrency usually bounded memory contention concerns case 
deciding scope version number similar deciding granularity locking finer concurrency higher costs incurred 
version number modified data structure modified lock changed 
frequency read operations costs writeback dirty cache lines read synchronization read operations attractive 
version numbers count number times data structure modified time useful necessary statistic 
system complexity blocking synchronization appears higher code required get problems introduces compared non blocking synchronization 
particular special coding required signal handlers avoid deadlock 
special mechanisms thread scheduler required avoid priority inversion locks produce 
additional code complexity required achieve reliable operation thread terminated random time 
example operations may implemented separate server process 
primary concern non blocking synchronization excessive retries contending operations 
structuring reduced probability contention conditional load mechanism described section achieve behavior similar lock synchronization 
non blocking synchronization primitives approach assumes efficient implementation dcas functionality 
section briefly outline instruction set extension load linked store conditional instructions support dcas 
software implementation discussed section processor supporting load linked ll store conditional sc instructions add instructions 
llp load linked pipelined load link second address ll 
load linked scp 

scp store conditional pipelined store specified location provided modifications memory cells designated ll llp instructions cache lines invalidated cache processor performing scp 
llp scp sequence nested ll sc pair fails outer ll sc pair fails 
dcas implemented instruction sequence shown instructions addition ll sc instructions 
ll llp instructions lines link loads respective stores issued sc scp instructions 
lines verify contain respectively 
scp sc lines conditional 
issue stores unchanged lines 
guarantees results cas lines valid line sc fails 
store issued successful scp buffered pending successful sc 
sc line writes atomically comparison data structures protected version number te dcas compare double swap second value changed version number unchanged 
cases minor optimization possible line deleted 
atomically store dcas get contents addresses registers 
ll llp compare 
unequal fail 
bne fail bne fail equal unchanged load store new values scp sc success sc scp stored fail 
fail dcas implementation ll sc llp scp 
success failure sc dcas operation returned general register holds argument sc 
denotes success failure 
instruction tries read hardware interlocks ll sc result sc 
worked detailed design implementation instructions risc processor description omitted brevity 
software implementation dcas dcas functionality implemented software technique introduced bershad 
dcas implemented lock known operating system 
process holding locks delayed context switch operating system rolls back process dcas procedure releases lock 
rollback procedure relatively simple dcas implementation simple known operating system 
probability context switch middle dcas procedure low short typically instructions 
rollback cost incurred infrequently 
technique generally implement primitives location cas 
focus implementation primary relation offering software implementation dcas alternative proposed hardware support 
simpler just implement rollback dcas compared general primitives 
approach key advantage requiring hardware extensions facilities existing systems 
performance may comparable hardware extensions especially single processors small scale multiprocessors 
measurements required 
concerns 
cost locking 
straight forward implementation requires dcas procedure access common global lock processes 
multi level memory locks memory memory contention processors lock significant 
example data structure may shared segment mapped independent processes 
locks associated dcas instance cost complexity designate locks critical section operating system implement rollback 
locking unlocking modifies cache line containing lock increasing cost operation writeback required 
second bershad approach requires locations memory extra read write set lock write clear lock 
third multiprocessors care readers shared data structures want support unsynchronized reads 
depending lock readers see intermediate states dcas read tentative values part dcas fails 
requiring synchronization reads significantly increases contention global lock 
note cases tsm reduces danger unsynchronized reads reads cause type errors 
writes protected global lock final dcas detect unsynchronized reads suspect fail 
systems provide hardware dcas require additional read synchronization performed automatically memory system 
experience measurements required determine significant issue real systems 
bershad mechanism harder test conditions 
instance possible write operations rollback needs undo area memory paged addresses illegal 
system needs ensure thread rolled back dcas critical section terminated 
believe hardware implementation simpler verify naturally operates top virtual memory management system top directly accessible physical memory lowest level system software 
concern minor change software mechanisms bershad scheme result subtle errors execution go undetected system long period time 
hardware contention control extension processor provide conditional load instruction 
instruction load instruction succeeds location loaded advisory lock set setting advisory lock succeed 
available version number loaded initially normal load 
operation fails thread waits retries maximum uses normal load instruction proceeds 
waiting avoids performing update concurrently process updating data structure 
prevents potential starvation operation takes significantly longer operations causing frequently occuring operations perpetually abort 
appears particularly beneficial large scale shared memory systems time complete dcas governed operation significantly extended wait times memory contention increasing exposure time process perform interfering operation 
memory take times long contention misses 
process significantly delay execution process faulting data process possibly causing dcas fail 
cost common case simply testing succeeded load version number required case 
implemented advisory locking mechanism implemented paradigm 
briefly processor advises cache controller particular cache line locked 
normal loads stores ignore lock bit instruction tests sets cache level lock cache line fails set 
store operation clears bit 
implementation costs extra bits cache tags cache line plus logic cache controller 
judging experience paradigm quite feasible implement 
performance performance paradigm experimental multiprocessor discussed 
discuss results simulation indicating performance approach high contention 
discuss aspects system performance 
experimental implementation operating system kernel class libraries run paradigm architecture 
basic configuration consists processor motorola multiprocessors running mhz clocks 
processor dcas instruction cas 
software runs change software implementation dcas uniprocessor mhz powerpc 
implemented multiprocessor powerpc system date 
kernel synchronization uses dcas critical sections cas 
dcas uses performance critical insert deletion key queues ready queue delay queue 
case blocking synchronization machine startup allow processor complete initialization processors start execution 
overhead non blocking synchronization minimal extra instructions 
example deletion priority queue imposes synchronization overhead instructions compared synchronization whatsoever including instructions access version number test dcas success retry operation necessary 
instruction overhead comparable required locked synchronization lock access fail requiring test success retry 
motorola cas slow apparently inefficient handling chip cache synchronization takes microseconds processor time 
comparison spin locks take average secs take secs 
contrast extended instructions propose section provide performance comparable locking implementation 
particular requires extra instructions including required ops plus implicit sync processor 
careful implementation allow instructions sync execute normal memory speed 
performance comparable roughly instruction times required lock unlock sequence 
compares overhead terms instruction times 
operation instruction times dcas cas dcas llp scp sgi lock unlock lock unlock approximate instruction times extra overhead synchronize deletion priority queue 
overhead include backoff computation 
simulation evaluation actual contention kernel data structures current implementation low ability create high contention time writing 
understand system behaves heavy load simulated insertion deletion singly linked list loads far heavier encountered cache kernel 
simulation run proteus simulator simulating processors cache lines set shared bus goodman cache coherence protocol 
times reported cycles start test processor finishes executing 
memory latency modeled times cost cache 
cost dcas modeled extra cycles costs necessary memory 
additional cost cas unsynchronized instruction referencing shared memory cycles 
algorithms simulated 
dcas dcas algorithm contention controlled advisory locking implemented paradigm 

dcas dcas algorithm contention controlled os intervention proposed felten described section 
cas implementation cas supporting higher degree concurrency technique valois :10.1.1.41.9506
spinlock spin lock exponential back base case 
test performed total insertions deletions divided evenly processes 
varied number processors number processes processor 
controlled rate access list process doing local insertion deletion 
varied cycles 
simulations indicate cache kernel dcas algorithms perform better cas spin locks 
shows performance process processor minimal updates 
basic cost updates shown accesses serialized synchronization contention bus contention 
cache contention due collisions small hit rate cache algorithms 
processor assuming synchronization contention bus contention completion time significantly larger objects migrate cache processor 
processes processor processes preempted 
case difference non concurrent algorithms simply bus contention fixed overhead modelling page faults 
degrade comparably dcas suffers bus contention count active threads 
valois algorithm cas exploits concurrency number processors increase overhead large relative simpler algorithms 
bus memory contention greater necessary derive version algorithm algorithm strictly correct :10.1.1.41.9506
natural result complicated necessary cas 
dcas algorithm relatively straightforward 
cycles processors cycles dcas dcas spinlock cas performance synchronization algorithms local number processes processor concurrency gain offset loss due overhead 
synchronization contention causes deletion auxiliary nodes fail number nodes traversed increases larger number processes dcas algorithm performs substantially better cas concurrency 
displays results reducing rate access interleaving list accesses parallel local 
insertion delete pairs appear take cycles cache interference adding cycles local lets non concurrent algorithms processors concurrently useful parallel 
number processors accesses list serialized completion time dominated time insertion deletion pairs 
dcas form contention control performs comparably spin locks case delays performance significantly better cas algorithm 
shows results processes run processor 
scenario processes preempted possibly holding lock 
expected non competitive delays introduced 
valois simulation michael scott reports better asymptotic behavior 
difference appears authors simulating fifo queue 
fifo queue algorithm insertion occurs tail deletion head auxiliary nodes traversed general don affect completion time 
fully general lists auxiliary nodes increase execution time memory traffic 
cycles processors cycles performance synchronization algorithms local number processes processor contrast non blocking algorithms slightly affected preemption 
completion time cas unaffected variance shown figures increases due counts held preempted processes delaying deletion nodes process resumes delay spend time releasing hundreds nodes free list 
results indicate hardware advisory locking performs compared operating system support style felten 
normal case experiences delays notified immediately advisory lock released 
process preempted notified 
waiter backed certain maximum threshold uses normal load longer waits lock holder 
large number processes occasional occurence bounded delay enables dcas outperform cache advisory locking 
expected behavior cache kernel processor lock holder signal handlers local context switch 
case advisory lock prevent waiter making progress 
advantage operating system notification lower overhead advisory locking preferable 
dcas performs comparably better spin locks cas algorithms 
code considerably simpler cas algorithm valois 
simulations number processors accessing single data structure far higher occur real loads rate access shared data structure far higher expect real system 
previously noted contention levels indicative poorly designed system cycles procs processor processors cycles performance synchronization algorithms local number processes processor caused redesign data structure 
indicate techniques handle stress 
system performance ideal measurements show benefit non blocking synchronization system performance 
system performance shown benefit considerably ability execute code signal handlers exploited extensively cache kernel object oriented remote procedure call system 
system allows restricted procedures block executed directly part signal handler invocation handles new call 
optimization rpcs invoked directly signal handler overhead allocating dispatching separate thread execute rpc 
measurements reported cited indicate significant savings optimization particularly short execution calls common operating system services simulations 
related previous explored lock free operating systems implementations general techniques wait free concurrent data structures hardware operating system support non blocking synchronization 
lock free operating systems massalin pu describe lock free nonblocking implementation synthesis multiprocessor kernel just cas dcas 
supports contention dcas sufficient practical implementation large systems non blocking synchronization 
focused small number wait free lock free data structures inside operating system kernel 
reason emulated exploitation application specific optimizations implement data structures 
example implementation linked list insertion deletion middle list efficient usage synthesis kernel highly constrained single bit suffices count normally needed 
contrast implementation linked lists general usable arbitrary application code 
methodologies implementing concurrent data objects herlihy presents methodology converting sequential implementations data structures wait free concurrent implementations 
goal provide specification transformation provably correct applied automatically sequential code 
converts sequential implementation data structure wait free concurrent just cas slightly efficiently load linked store conditional 
method involves copying entire data structure modifying copy atomically replacing old copy new copy cas retrying entire copy modifying conflict 
performance improved ad hoc techniques techniques tend add hard catch subtle synchronization problems expensive 
regard approach impractically expensive copy overhead 
contrast contribution set general techniques programmer incorporates software design implementation allowing software sequential parallel execution modification acceptable performance 
barnes turek valois provide techniques increasing concurrency nonblocking synchronization :10.1.1.41.9506
cost concurrent updates appears outweigh actual benefit low rates contention system 
studies reported low level contention kernel data structures suggest phenomenon widely true just cache kernel 
hardware support processors provide single compare swap cas functionality support non blocking synchronization 
herlihy general methodology shows single cas adequate theory appears inefficient practice 
processors motorola provide multi word atomic instruction functionality rare risc processor knowledge 
risc extension propose section suggests feasible support modern processors 
cisc approach appear viable current processors die current processors support 
transactional memory provides hardware support multiple address atomic memory operations 
general dcas comes correspondingly higher cost 
proposed hardware implementation requires new instructions second set caches processor twice storage cache lines actively involved transaction complicated commit protocol 
double ll sc appears practical solution dcas functionality sufficient significantly simpler implement 
oklahoma update provides alternate implementation multiple address atomic memory operations 
duplicating entire cache lines involved transactions transactional memory oklahoma update requires reservation register word version load linked 
register contains flags plus words optionally 
contrasts implementation requires link address retained register synchronized word single cache line buffer delayed scp 
design word register entire cache line buffer scp 
approach adds complexity chip logic slows sc increases time cache locked savings questionable 
oklahoma update attempts implement features hardware exponential backoff better done software needlessly increase complexity size chip 
buffering certain requests come pre commit phase cause processors non interfering reservation sets delay different designs arise different assumptions regarding number memory locations atomically updatable time 
transactional memory conjectures oklahoma update places knee 
general locations better powerful 
implementation dcas far simplest extension existing processor designs 
key contribution experience indicates dcas sufficient practical performance making extra consider processors accesses cache lines addressed ascending alphabetical order 
interact 
holds holds asks stalls buffers request delays longer chains constructed 
hardware complexity schemes unnecessary 
operating system support felten reduce useless concurrency os support provide functionality support hardware cache advisory locking 
method variation technique bershad discussed section 
propose incrementing counter active threads entrance critical section decrementing exit 
os decrements counter active thread switched 
processes wait count active threads threshold case allowed proceed 
delayed processes excessively delay processes count decremented os 
techniques appear valuable systems hardware support advisory locking fact approach works better high contention 
hardware advisory locking conditional load resilient processor failure lower overhead low contention case 
hardware versus software dcas hardware implementation simple fast measurements required determine 
israeli rappaport implement way atomic compare swap way ll sc processors single cas 
approach primarily theoretical interest requires large amount space bits word shared memory requires words bits wide takes execute interlocks multi word atomic instructions 
anderson moir improve requiring realistic sized words time requiring prohibitively large amount space 
software transactional memory attempt implement transactional memory software depending ll sc unfortunately implementation correctly existing implementations ll sc code depends ability interleave sc simultaneously supported 
llp scp instructions proposed enable techniques provide software transactional memory multiple independently chosen words memory 
space computational overhead implementation excessive general stm operations atomic respect stm operations general reads writes 
scheme requires twice memory possibly shared location extra overhead factor reads writes case contention 
concluding remarks experience suggests powerful synergy non blocking synchronization structuring techniques design implementation operating system supporting run time libraries 
non blocking synchronization significantly reduces complexity improves performance software signal rich environment implemented cache kernel supporting class libraries 
structuring techniques achieve system design goals facilitate implementing non blocking synchronization 
biggest problem inadequate performance non blocking synchronization instructions 
contributions 
show careful design implementation operating system software efficiency reliability modularity implementing simple efficient non blocking synchronization far easier 
particular type stable memory tsm contention minimizing data structuring minimal inconsistency window structuring important reasons 
techniques beneficial blocking synchronization significantly reduce complexity improve performance non blocking synchronization 
conversely non blocking synchronization significant advantages signal centric design cache kernel associated libraries especially large amount conventional operating system functionality implemented library kernel level 
second describe number techniques implementing non blocking synchronization tsm version numbers dcas 
techniques simple write read understand perform 
contrast cas experience suggests dcas support sufficient practical non blocking os run time system implementation single cas sufficient 
fact lack efficient dcas support systems potential impediment techniques 
fortunately proposed hardware implementation indicates feasible implement efficient dcas functionality modern processor minimal additional complexity full compatibility architecture 
conditional load capability coupled cache advisory locking improves hardware support providing advantages locking lock free implementation 
existence software implementations dcas contention reduction demonstrates approach reasonable platforms lacking hardware support 
efficiently supported dcas allow standard libraries operating system software portable multiprocessors uniprocessors extra overhead code complication 
allow parallel architectures software developed uniprocessors relying nonblocking synchronization required signals handle serialization parallel processing context 
significantly reduce software bottleneck slowed deployment parallel processing date 
required evaluate merits hardware support dcas versus various software alternatives particularly system performance 
required validate experience dcas fact adequate practice 
experience date non blocking approach attractive practical way structure operating system software 
locks problematic signals extensively libraries synchronization finer grained cost memory delays higher relative processor speed 
hope encourages additional efforts area 
performance issues non blocking synchronization shared memory multiprocessors 
proceedings th annual acm symposium principles distributed computing pp august 
anderson moir universal constructions multi object operations proceedings th annual acm symposium principles distributed computing ottawa ont canada pp august barnes method implementing lock free shared data structures proceedings th acm symposium parallel algorithms architectures bershad practical considerations nonblocking concurrent objects 
proceedings th ieee international conference distributed computing systems los alamitos ca ieee computer society press pp may 
brewer dellarocas weihl proteus high performance parallel architecture simulator technical report mit lcs tr mit laboratory computer science september 
cheriton distributed system 
communications acm pp march cheriton duda 
caching model operating system kernel functionality 
proceedings st operation systems design implementation monterey ca pp nov 
cheriton boyle paradigm highly scalable shared memory multi computer architecture 
ieee computer february 
cheriton kutter 
optimizing memorybased messaging scalable shared memory multiprocessor architectures 
appear usenix computer systems journal 
available stanford computer science technical report cs december 
cheriton restructuring parallel simulation improve cache behavior shared memory multiprocessor experience 
proceedings international symposium shared memory multiprocessing pp tokyo april 
joseph heinrich 
mips user manual ptr prentice hall englewood cliffs nj herlihy moss 
transactional memory architectural support lock free data structures 
th annual symposium computer architecture san diego calif pp 

may 
herlihy 
wait free synchronization 
acm transactions programming languages systems pp january herlihy 
methodology implementing highly concurrent data objects acm transactions programming languages systems november israeli rappaport disjoint implementations strong shared memory primitives proceedings th annual acm symposium principles distributed computing los angeles ca pp august israeli rappaport efficient wait free implementation concurrent priority queue th intl workshop distributed algorithms lausanne switzerland lecture notes computer science springer verlag pp sept massalin pu 
lock free multiprocessor os kernel 
technical report cucs computer science department columbia university october 
michael scott simple fast practical non blocking blocking concurrent queue algorithms proceedings th annual acm symposium principles distributed computing philadelphia pa pp may 
shavit software transactional memory proceedings th principles distributed computing ottawa ont canada pp august sites ed dec alpha architecture digital press burlington mass stone stone turek 
multiple reservations oklahoma update 
ieee parallel distributed technology vol pp november torrellas gupta hennessy 
characterizing caching synchronization performance multiprocessor operating system 
fifth international conference architectural support languages operating systems pp october turek shasha prakash 
locking blocking making lock concurrent data structure algorithms non blocking 
proceedings principles database systems pp 
valois lock free linked lists swap proceedings th annual acm symposium principles distributed computing ottawa ont canada pp august cheriton specializing object oriented rpc functionality performance proceedings th ieee international conference ieee computer society press may :10.1.1.41.9506
family programmer manual motorola powerpc risc user manual motorola 
