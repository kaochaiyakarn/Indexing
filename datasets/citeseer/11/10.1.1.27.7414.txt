journal artificial intelligence research submitted published decision theoretic planning structural assumptions computational leverage craig boutilier cs ubc ca department computer science university british columbia vancouver bc canada thomas dean cs brown edu department computer science brown university box providence ri usa steve hanks hanks cs washington edu department computer science engineering university washington seattle wa usa planning uncertainty central problem study automated sequential decision making addressed researchers different fields including ai planning decision analysis operations research control theory economics 
assumptions perspectives adopted areas differ substantial ways planning problems interest researchers fields modeled markov decision processes mdps analyzed techniques decision theory 
presents overview synthesis mdp related methods showing provide unifying framework modeling classes planning problems studied ai 
describes structural properties mdps exhibited particular classes problems exploited construction optimal approximately optimal policies plans 
planning problems commonly possess structure reward value functions describe performance criteria functions describe state transitions observations relationships features describe states actions rewards observations 
specialized representations algorithms employing representations achieve computational leverage exploiting various forms structure 
certain ai techniques particular structured intensional representations viewed way 
surveys types representations classical decision theoretic planning problems planning algorithms exploit representations number different ways ease computational burden constructing policies plans 
focuses primarily abstraction aggregation decomposition techniques ai style representations 

planning decision theoretic notions represent domain uncertainty plan quality drawn considerable attention artificial intelligence ai 
decision theoretic planning dtp attractive extension classical ai planning paradigm allows model problems actions uncertain effects decision maker 
see example texts dean allen aloimonos dean wellman russell norvig research reported hanks russell wellman 
fl ai access foundation morgan kaufmann publishers 
rights reserved 
boutilier dean hanks incomplete information world factors resource consumption lead solutions varying quality may absolute defined goal state 
roughly aim dtp form courses action plans policies high expected utility plans guaranteed achieve certain goals 
ai planning viewed particular approach solving sequential decision problems type connections dtp models fields research decision analysis economics operations research apparent 
conceptual level sequential decision problems viewed instances markov decision processes mdps mdp framework connections explicit 
research dtp explicitly adopted mdp framework underlying model barto bradtke singh boutilier dearden boutilier dearden goldszmidt dean kaelbling kirman nicholson koenig simmons koenig russell allowing adaptation existing results algorithms solving mdps field applied planning problems :10.1.1.117.6173
doing departed traditional definition planning problem ai planning community goal explicit connection lines 
adopting mdp framework model posing solving planning problems illuminated number interesting connections techniques solving decision problems drawing ai planning reasoning uncertainty decision analysis 
interesting insights emerge body dtp problems exhibit considerable structure solved special purpose methods recognize exploit structure 
particular feature representations describe problems typical practice ai highlights problem special structure allows exploited computationally little effort 
general impediments widespread acceptance mdps ai general model planning 
absence explanations mdp model connections current planning research explicit conceptual computational level 
may due large part fact mdps developed studied primarily dominant concerns naturally different 
aim connections clear provide brief description mdps conceptual model planning emphasizes connection ai planning explore relationship mdp solution algorithms ai planning algorithms 
particular emphasize ai planning models viewed special cases mdps classical planning algorithms designed exploit problem characteristics associated cases 
second impediment skepticism ai researchers regarding computational adequacy mdps planning model techniques scale solve planning problems reasonable size 
difficulty solution techniques mdps tendency rely explicit state problem formulations 
problematic ai planning state spaces grow exponentially number problem features 
state space size dimensionality somewhat lesser concern decision analysis 
fields operations researcher decision analyst hand craft model ignores certain problem features deemed irrelevant define features summarize decision theoretic planning structural assumptions wide class problem states 
ai emphasis automatic solution problems posed users lack expertise decision analyst 
assuming crafted compact state space appropriate 
show specialized representations algorithms ai planning problem solving design efficient mdp solution techniques 
particular ai planning methods assume certain structure state space actions operators specification goal success criteria 
representations algorithms designed problem structure explicit exploit structure solve problems effectively 
demonstrate process identifying structure making explicit exploiting algorithmically brought bear solution mdps 
objectives 
provides overview dtp mdps suitable readers familiar traditional ai planning methods connections 
second describes types structure exploited ai representations methods facilitate computationally effective planning mdps 
suitable ai methods familiar classical presentation mdps 
surveys mdps ai suggests directions research regard interest researchers dtp 
general problem definition roughly speaking class problems consider involving systems dynamics modeled stochastic processes actions decision maker referred agent influence system behavior 
system current state choice action jointly determine probability distribution system possible states 
agent prefers certain system states goal states determine course action called plan policy lead target states possibly avoiding undesirable states way 
agent may know system state exactly making decision act may rely incomplete noisy sensors forced base choice action probabilistic estimate state 
help illustrate types problems interested consider example 
imagine robot agent designed help user office environment see 
activities undertake picking user mail getting coffee tidying user research lab 
robot move location location perform various actions tend achieve certain target states bringing coffee user demand maintaining minimal level lab 
associate certain level uncertainty effects robot actions tries move adjacent location succeed time fail move time 
robot incomplete access true state system sensors supply incomplete information tell mail available pickup mail room incorrect boutilier dean hanks hallway office coffee lab decision theoretic planning problem information mail room sensors occasionally fail detect presence mail 
performance robot measured various ways actions guarantee goal achieved 
maximize objective function defined possible effects actions 
achieve goal state sufficient probability avoiding disastrous states near certainty 
stipulation optimal acceptable behavior important part problem specification 
types problems captured general framework include classical goal oriented deterministic complete knowledge planning problems extensions conditional probabilistic planning problems general problem formulations 
discussion point assumed extensional representation system states state explicitly named 
ai research intensional representations common 
intensional representation states sets states described sets multi valued features 
choice appropriate set features important part problem design 
features include current location robot presence absence mail 
performance metric typically expressed intensionally 
serves example problem 
lists basic features describe states system actions available robot exogenous events occur intuitive description features actions events 
remainder organized follows 
section mdp framework introducing basic concepts terminology noting relationship model classical ai planning problem 
section surveys common solution techniques algorithms dynamic programming general mdp problems search algorithms planning problems points relationship problem assumptions solution techniques 
section turns algorithms representations showing various ways structured representations commonly ai algorithms represent mdps compactly 
section surveys decision theoretic planning structural assumptions features denoted description location loc location robot 
possible locations coffee room user office hallway laboratory degree lab 
possible values mail mail user mail box 
true false robot mail rhm rhm robot mail possession 
coffee request cr cr outstanding unfulfilled request coffee user 
robot coffee rhc rhc robot coffee possession 
actions denoted description move clockwise clk move adjacent location clockwise direction counterclockwise move adjacent location counterclockwise direction tidy lab tidy robot lab degree increased pickup mail pum robot mail robot takes mail rhm true false get coffee getc robot coffee room gets coffee rhc true deliver mail robot office mail hands mail user rhm false deliver coffee delc robot office coffee hands coffee user rhc cr false events denoted description mail arrival mail arrives causing true request coffee user issues coffee request causing cr true lab mess lab degree tidy elements robot domain 
abstraction aggregation problem decomposition methods shows connection traditional ai methods goal regression 
section demonstrates representational computational methods ai planning solution general mdps 
section points additional ways type computational leverage developed 

markov decision processes basic problem formulation section introduce mdp framework explicit relationship model classical ai planning models 
interested controlling stochastic dynamical system system point time number distinct states system state changes time response events 
action particular kind event agent order change system state 
assume agent control actions taken effects action perfectly predictable 
contrast exogenous events agent control occurrence may partially predictable 
view agent consistent ai view agent autonomous decision maker control view policy determined ahead time programmed device executed deliberation 
boutilier dean hanks states state transitions define state description system particular point time 
defines states vary particular applications notions natural 
common assume state captures information relevant agent decision making process 
assume finite state space fs possible system states 
cases agent complete information current state uncertainty incomplete information captured probability distribution states discrete time stochastic dynamical system consists state space probability distributions governing possible state transitions state system depends past states 
distributions constitute model system evolves time response actions exogenous events reflecting fact effects actions events may perfectly predictable prevailing state known 
generally concerned agent chooses appropriate course action remainder section assume agent course action fixed concentrating problem predicting system state occurrence predetermined sequence actions 
discuss action selection problem section 
assume system evolves stages occurrence event marks transition stage stage 
events define changes stage events necessarily cause state transitions equate stage transitions state transitions 
course possible event occur leave system state 
system progression stages roughly analogous passage time 
identical assume action possibly op taken stage action takes unit time complete 
speak loosely stages correspond units time refer interchangeably set stages set time points 
model uncertainty regarding system state stage random variable takes values assumption forward causality requires variable depend directly value variable 
roughly requires model system past history directly determines distribution current states knowledge states influence estimate current state indirectly providing evidence current state may lead states 
shows graphical perspective discrete time stochastic dynamical system 
nodes random variables denoting state particular time arcs indicate direct probabilistic dependence states previous states 
describe system completely supply conditional distributions pr js delta delta delta gamma times states thought descriptions system modeled question arises detail system captured state description 

discussion applies cases state space countably infinite 
see puterman discussion infinite continuous state problems 

deal topics considerable literature community continuous time markov decision processes puterman 
decision theoretic planning structural assumptions general stochastic process markov chain stationary markov chain 
detail implies information system turn translates better predictions behavior 
course detail implies larger set increase computational cost decision making 
commonly assumed state contains information predict state 
words information history system relevant predicting captured explicitly state 
formally assumption markov assumption says knowledge state renders information past irrelevant making predictions pr js gamma pr js markovian models represented graphically structure reflecting fact state sufficient predict state evolution 
common assume effects event depend prevailing state stage time event occurs 
distribution predicting state regardless stage model said stationary represented schematically just stages 
case single conditional distribution required 
generally restrict attention discrete time finite state stochastic dynamical systems markov property commonly called markov chains 
furthermore discussion restricted stationary chains 
complete model provide probability distribution initial states reflecting probability state stage 
distribution repre 
worth mentioning markov property applies particular model system 
non markovian model system finite order dynamics depend previous states converted equivalent larger markov model 
control theory called conversion state form luenberger 

course statement model detail saying state carries information stage irrelevant predicting transitions 
boutilier dean hanks state transition diagram 
sented real valued row vector size jsj entry state 
denote vector denote ith entry probability starting state represent stage nonstationary markov chain transition matrices size theta matrix captures transition probabilities governing system moves stage stage 
matrix consists probabilities ij ij pr js 
process stationary transition matrix stages matrix entries denoted ij suffice 
initial distribution states probability distribution states stages stationary markov process represented state transition diagram 
nodes correspond particular states stage represented explicitly 
arcs denote possible transitions non zero probability labeled transition probabilities ij pr js 
arc node node labeled ij ij 
size diagram depending number arcs 
useful representation transition graph relatively sparse example states immediate transitions neighbors 
example illustrate notions imagine robot executing policy moving counterclockwise repeatedly 
restrict attention variables location loc presence mail giving state space size 
suppose robot moves adjacent location probability 
addition mail arrive probability time independent robot location causing variable true 
true robot move state false action moving influence presence mail 
state transition diagram example illustrated 
transition matrix shown 
structure markov chain occasionally interest planning 
subset closed ij proper closed set proper subset enjoys property 
refer proper closed sets recurrent classes states 
closed set consists single state state called absorbing state 
agent enters closed set absorbing state remains 
important note nodes represent random variables earlier figures 
decision theoretic planning structural assumptions lm lm om om hm hm cm cm mm mm state transition diagram transition matrix moving robot 
forever probability 
example set states holds forms recurrent class 
absorbing states example program robot stay put state hm loc absorbing state altered chain 
say state transient belong recurrent class 
state holds transient eventually probability agent leaves state returns way remove mail arrives 
actions markov chains describe evolution stochastic system capture fact agent choose perform actions alter state system 
key element mdps set actions available decision maker 
action performed particular state state changes stochastically response action 
assume agent takes action stage process system changes state accordingly 
stage process state agent available set actions called feasible set stage describe effects supply state transition distribution pr js actions states stages case markov chain terms pr js true conditional distributions family distributions parameterized probability part model 
retain notation suggestive nature 
assume feasible set actions stages states case set actions fa ak executed time 
contrasts ai planning practice assigning preconditions actions defining states meaningfully executed 
model takes view action executed attempted state 
action effect executed state execution leads disastrous effects noted action transition matrix 
action preconditions computational convenience representational necessity planning process efficient identifying states planner consider selecting action 
preconditions represented mdps relaxing assumption set boutilier dean hanks lm lm om om hm hm cm cm mm mm transition matrix clk induced transition diagram action policy 
feasible actions states 
illustrate planning concepts assume actions preconditions 
restrict attention stationary processes case means effects action depends state stage 
transition matrices take form ij pr js capturing probability system moves state executed state stationary models action fully described single theta transition matrix important note transition matrix action includes direct effects executing action effects exogenous events occur stage 
example example extended agent available actions moving clockwise moving counterclockwise 
transition matrix assumption mail arrives probability shown 
matrix clk appears left 
suppose agent fixes behavior moves clockwise locations counterclockwise locations address agent come know location implement behavior 
defines markov chain illustrated transition diagram right 
exogenous events exogenous events events stochastically cause state transitions actions control decision maker 
correspond evolution natural process action agent 
notice effect action combines effects robot action exogenous event mail arrival state transition probabilities incorporate motion robot causing change location possible change mail status due mail arrival 
purposes decision making precisely combined effect 
possible assess effects actions exogenous events separately combine single transition matrix certain cases boutilier puterman 
discuss section 
decision theoretic planning structural assumptions important predicting distribution possible states resulting action taken 
call models actions implicit event models effects exogenous event folded transition probabilities associated action 
natural view transitions comprised separate events having effect state 
generally think transitions determined effects agent chosen action certain exogenous events agent control may occur certain probability 
effects actions decomposed fashion call action model explicit event model 
specifying transition function action zero exogenous events generally easy actions events interact complex ways 
instance consider specifying effect action pum pickup mail state mail possibility simultaneous mail arrival unit discrete time 
event occurs robot obtain newly arrived mail mail remain mailbox 
intuitively depends mail arrived pickup completed albeit time quantum 
state transition case viewed composition transitions precise description composition depends ordering agent action exogenous event 
mail arrives transition state mail waiting state mail waiting robot holding mail pickup action completed transition pum effect mail arrives remains box 
picture complicated actions events truly occur simultaneously interval case resulting transition need composition individual transitions 
example robot lifts side table glass water situated water spill similarly exogenous event causes side raised 
action event occur simultaneously result qualitatively different water spilled 
interleaving semantics described appropriate 
complications modeling exogenous events combination actions events approached ways depending modeling assumptions willing 
generally specify types information 
provide transition probabilities actions events assumption occur isolation standard transition matrices 
transition matrix decomposed matrices shown clk 
second exogenous event specify probability occurrence 
vary state generally require vector length indicating probability occurrence state 
occurrence vector 
fact individual matrices deterministic artifact example 
general actions events represented genuinely stochastic matrices 
boutilier dean hanks action clk event transition matrices action exogenous event explicit event model 
assume illustration mail arrives 
final requirement combination function describes compose transitions action subset event transitions 
indicated complex unrelated individual action event transitions 
certain assumptions combination functions specified reasonably concisely 
way modeling composition transitions assume interleaving semantics type alluded 
case needs specify probability action events take place occur specific order 
instance assume event occurs time discrete time unit continuous distribution exponential distribution rate 
information probability particular ordering transitions certain events occur computed resulting distribution possible states 
example probability composed transitions probabilities mail arrived respectively 
certain cases probability ordering needed 
illustrate combination function assume action occurs exogenous events 
furthermore assume events commutative initial state pair events distribution results applying event sequence delta identical obtained sequence delta occurrence probabilities intermediate states identical 
intuitively set events domain mess property 
conditions combined transition distribution action computed considering probability subset events applying subset order distribution associated generally construct implicit event model various components explicit event model natural specification converted form usually mdp solution algorithms 
assumptions instance form implicit event transition matrix pr action matrix pr assumes event occurrences matrices pr events occurrence vector pr event effective transition matrix 
probability different events may correlated possibly particular states 
case necessary specify occurrence probabilities subsets events 
treat event occurrence probabilities independent ease exposition 
decision theoretic planning structural assumptions event defined follows pr pr pr gamma pr equation captures event transition probabilities probability event occurrence factored 
denote diagonal matrices entries kk pr kk gamma pr pr pr assumptions implicit event matrix pr action pr pr delta delta delta pr en pr ordering possible events 
naturally different procedures constructing implicit event matrices required different assumptions action event interaction 
implicit models constructed specified directly explicit mention exogenous events assume stated action transition matrices take account effects exogenous events represent agent best information happen takes particular action 
observations effects action depend aspect prevailing state choice action depend agent observe current state remember prior observations 
model agent observational sensing capabilities introducing finite set observations fo agent receives observation set stage prior choosing action stage 
model observation random variable value taken probability particular generated depend ffl state system gamma ffl action taken gamma ffl state system action gamma effects exogenous events gamma realized action taken 
pr js gamma gamma probability agent observes stage performs state ends state actions assume observational distributions stationary independent stage pr js denote quantity 
view probabilistic dependencies state action observation variables graph time indexed variables shown nodes variable directly probabilistically dependent edge see 
model allows wide variety assumptions agent sensing capabilities 
extreme fully observable mdps agent knows exactly state stage model case letting setting pr js iff boutilier dean hanks graph showing dependency relationships states actions observations different times 
example means robot knows exact location mail waiting mailbox mail arrives 
agent receives perfect feedback results actions effects exogenous events noisy effectors complete noise free instantaneous sensors 
ai research adopts mdp framework explicitly assumes full observability 
extreme consider non observable systems agent receives information system state execution 
model case letting fog 
observation reported stage revealing information state pr js pr js 
open loop systems agent receives useful feedback results actions agent noisy effectors sensors 
case agent chooses actions plan consisting sequence actions executed unconditionally 
effect agent relying predictive model determine action choices execution time 
traditionally ai planning implicitly assumption non observability coupled omniscience assumption agent knows initial state certainty predict effects actions perfectly precisely predict occurrence exogenous events effects 
circumstances agent predict exact outcome plan obviating need observation 
agent build straight line plan sequence actions performed feedback plan execution depend information gathered execution time 
extremes special cases general observation model described allows agent receive incomplete noisy information system state partially observable mdps pomdps 
example robot able determine location exactly able determine mail arrives 
furthermore mail sensors occasionally report inaccurately leading incorrect belief mail waiting 
example suppose robot action change system state generates observation influenced presence mail provided decision theoretic planning structural assumptions pr obs mail pr obs loc loc loc loc observation probabilities checking mailbox 
robot time action performed 
robot sensor reports mail 
noisy sensor described probability distribution shown 
view error probabilities probability false positives false negatives 
system trajectories observable histories terms trajectory history interchangeably describe system behavior course problem solving episode initial segment thereof 
complete system history sequence states actions observations generated stage time point interest finite infinite length 
complete histories represented possibly infinite sequence tuples form hhs hs hs ii define alternative notions history contain complete information 
arbitrary stage define observable history sequence ho gamma gamma ii observation initial state 
observable history stage comprises information available agent history chooses action stage third type trajectory system trajectory sequence hhs hs gamma gamma describing system behavior objective terms independent agent particular view system 
evaluating agent performance generally interested system trajectory 
agent policy defined terms observable history agent access system trajectory fully observable case equivalent 
reward value problem facing decision maker select action performed stage decision problem making decision basis observable history 
agent needs way judge quality course action 
done defining boutilier dean hanks decision process rewards action costs 
value function delta function mapping set system histories reals agent prefers system history just case 
agent judges behavior bad depending effect underlying system trajectory 
generally agent predict certainty system trajectory occur best generate probability distribution possible trajectories caused actions 
case computes expected value candidate course action chooses policy maximizes quantity 
just system dynamics specifying value function arbitrary trajectories cumbersome unintuitive 
important identify structure value function lead parsimonious representation 
assumptions value functions commonly mdp literature time separability additivity 
time separable value function defined terms primitive functions applied component states actions 
reward function associates reward state costs assigned actions defining cost function theta associates cost performing action state rewards added value function costs subtracted 
value function time separable simple combination rewards costs accrued stage 
simple combination means value taken function costs rewards stage costs rewards depend stage function combines independent stage commonly linear combination product 
value function additive combination function sum reward cost function values accrued history stages 
addition rewards action costs system time separable value viewed graphically shown 
assumption time separability restrictive 
example certain goals involving temporal deadlines workplace tidy soon possible tomorrow morning maintenance allow mail sit 
technically set histories interest depends horizon chosen described 

term reward somewhat misnomer reward negative case penalty better word 
likewise costs positive negative beneficial admit great flexibility defining value functions 

see luenberger precise definition time separability 
decision theoretic planning structural assumptions undelivered minutes require value functions non separable current representation state 
note separability markov property property particular representation 
add additional information state example clock time interval time time achieved length time mail sits mail room robot picks 
additional information reestablish time separable value function expense increase number states ad hoc cumbersome action representation 
horizons success criteria order evaluate particular course action need specify long stages executed 
known problem horizon 
finite horizon problems agent performance evaluated fixed finite number stages commonly aim maximize total expected reward associated course action define finite horizon value length history bellman gamma fr gamma infinite horizon problem hand requires agent performance evaluated infinite trajectory 
case total reward may unbounded meaning policy arbitrarily bad executed long 
case may necessary adopt different means evaluating trajectory 
common introduce discount factor ensuring rewards costs accrued stages counted accrued earlier stages 
value function expected total discounted reward problem defined follows bellman howard fl gamma fl fixed discount rate fl 
formulation particularly simple elegant way ensure bounded measure value infinite horizon important verify discounting fact appropriate 
economic justifications provided discounted models reward earned sooner worth earned provided reward invested 
discounting suitable modeling process terminates probability gamma fl point time robot break case discounted models correspond expected total reward finite uncertain horizon 
reasons discounting finite horizon problems 
technique dealing infinite horizon problems evaluate trajectory average reward accrued stage gain 
gain history defined lim fr gamma 
see bacchus boutilier grove systematic approach handling certain types history dependent reward functions 
boutilier dean hanks refinements criterion proposed puterman 
problem ensures total reward infinite trajectory bounded expected total reward criterion defined 
consider case common ai planners agent task bring system goal state 
positive reward received goal reached actions incur non negative cost goal reached system enters absorbing state rewards costs accrued 
long goal reached certainty situation formulated infinite horizon problem total reward bounded desired trajectory bertsekas puterman 
general problems formulated fixed finite horizon problems priori bound number steps needed reach goal established 
problems called indefinite horizon problems practical point view agent continue execute actions finite number stages exact number determined ahead time 
solution criteria complete definition planning problem need specify constitutes solution problem 
see split explicit mdp formulations ai planning community 
classical mdp problems generally stated optimization problems value function horizon evaluation metric expected total reward expected total discounted reward expected average reward stage agent seeks behavioral policy maximizes objective function 
ai seeks satisficing solutions problems 
planning literature generally taken plan satisfies goal equally preferred plan satisfies goal plan satisfies goal preferable plan 
probabilistic framework seek plan satisfies goal maximum probability optimization lead situations optimal plan infinite length system state fully observable 
satisficing alternative kushmerick hanks weld seek plan satisfies goal probability exceeding threshold 
example extend running example demonstrate infinite horizon fully observable discounted reward situation 
adding new dimension state description boolean variable rhm robot mail giving system states 
provide agent additional actions pum pickup mail deliver mail described 
reward agent way mail delivery encouraged associate reward state rhm false states 
actions cost agent gets total reward stage system trajectory stay pum clk clk 
see haddawy hanks williamson hanks restatement planning optimization problem 
decision theoretic planning structural assumptions assign action cost gamma action stay cost total reward 
discount rate discount rewards costs initial segment infinite horizon history contribute gamma gamma gamma gamma gamma total value trajectory subsequently extended 
furthermore establish bound total expected value trajectory 
best case subsequent stages yield reward expected total discounted reward bounded similar effect behavior achieved penalizing states having negative rewards rhm true 
policies mentioned policies courses action plans informally point provide precise definition 
decision problem facing agent viewed generally deciding action perform current observable history 
define policy mapping set observable histories ho actions ho intuitively agent executes action ho gamma gamma stage performed actions delta delta delta gamma observations delta delta delta gamma earlier stages just observation current stage 
policy induces distribution pr hj set system histories probability distribution depends initial distribution define expected value policy ev hs pr hj agent adopt policy maximizes expected value satisficing context acceptably high expected value 
general form policy depending arbitrary observation history lead complicated policies policy construction algorithms 
special cases assumptions observability structure value function result optimal policies simpler form 
case fully observable mdp time separable value function optimal action stage computed information current state stage restrict policies simpler form theta danger acting 
due fact full observability allows state observed completely markov assumption renders prior history irrelevant 
non observable case observational history contains vacuous observations agent choose actions knowledge previous actions stage incorporates previous actions takes form boutilier dean hanks form policy corresponds linear unconditional sequence actions ha straight line plan ai nomenclature 
model summary assumptions problems computational complexity concludes exposition mdp model planning uncertainty 
generality allows capture wide variety problem classes currently studied literature 
section review basic components model describe problems commonly studied dtp literature respect model summarize known complexity results 
section describe specialized computational techniques solve problems problem classes 
model summary assumptions mdp model consists components ffl state space finite countable set states 
generally markov assumption requires state convey information necessary predict effects actions events independent information system history 
ffl set actions action represented transition matrix size jsj theta jsj representing probability ij performing action state move system state assume action model stationary meaning transition probabilities vary time 
transition matrix action generally assumed account exogenous events occur stage action executed 
ffl set observation variables set messages sent agent action performed provide execution time information current system state 
action pair states ij associate distribution possible observations km ij denotes probability obtaining observation action taken resulted transition state ffl value function value function maps state history real number just case agent considers history state history records progression states system assumes actions performed 
assumptions time separability additivity common particular generally reward function cost function defining value 
ffl horizon number stages state histories evaluated 
algorithms ai literature produce partially ordered sequence actions 
plans involve conditional nondeterministic execution 
represent fact linear sequence consistent partial order solve problem 
partially ordered plan concise representation particular set straight line plans 
decision theoretic planning structural assumptions ffl optimality criterion 
provides criterion evaluating potential solutions planning problems 
common planning problems general framework classify various problems commonly studied planning decision making literature 
case note modeling assumptions define problem class 
planning problems decision sciences tradition ffl fully observable markov decision processes extremely large body research studying basic algorithmic techniques detail section 
commonly formulation assumes full observability stationarity uses optimality criterion maximization expected total reward finite horizon maximization expected total discounted reward infinite horizon minimization expected cost goal state 
introduced bellman studied depth fields decision analysis including seminal howard 
texts include bertsekas puterman 
average reward optimality received attention literature blackwell howard puterman 
ai literature discounted total reward models popular barto dearden boutilier dean kaelbling kirman nicholson koenig average reward criterion proposed suitable modeling ai planning problems boutilier puterman mahadevan schwartz 
ffl partially observable markov decision processes pomdps pomdps closer general model decision processes described 
pomdps generally studied assumption stationarity optimality criteria identical average reward criterion widely considered 
discuss pomdp viewed state space consisting set probability distributions probability distributions represent states belief agent observe state belief system exact knowledge system state 
pomdps widely studied control theory astrom lovejoy smallwood sondik sondik drawn increasing attention ai circles cassandra kaelbling littman hauskrecht littman parr russell simmons koenig thrun fox burgard zhang liu :10.1.1.53.7233
influence diagrams howard matheson shachter popular model decision making ai fact structured representational method pomdps see section 
planning problems ai tradition boutilier dean hanks ffl classical deterministic planning classical ai planning model assumes deterministic actions action taken state successor important assumptions non observability value determined reaching goal state plan leads goal state preferred 
preference shorter plans represented discount factor encourage faster goal achievement assigning cost actions 
reward associated transitions goal states absorbing 
action costs typically ignored noted 
classical models usually assumed initial state known certainty 
contrasts general specification mdps assume knowledge distributional information initial state 
policies defined applicable matter state distribution states finds oneself action choices defined possible state history 
knowledge initial state determinism allow optimal straight line plans constructed loss value associated non observability unpredictable exogenous events uncertain action effects modeled consistently assumptions adopted 
overview early classical planning research variety approaches adopted see allen hendler tate yang text 
ffl optimal deterministic planning separate body retains classical assumptions complete information determinism tries recast planning problem optimization relaxes implicit assumption achieve goal costs 
time methods sort representations algorithms applied satisficing planning 
haddawy hanks multi attribute utility model planners keeps explicit information initial state goals allows preferences stated partial satisfaction goals cost resources consumed satisfying 
model allows expression preferences phenomena temporal deadlines maintenance intervals difficult capture time separable additive value function 
williamson see williamson hanks 
implements model extending classical planning algorithm solve resulting optimization problem 
haddawy implement model complete decision theoretic framework 
model planning refinement planning differs somewhat generative model discussed 
model set possible plans pre stored abstraction hierarchy problem solver job find hierarchy optimal choice concrete actions particular problem 
perez carbonell incorporates cost information classical planning framework maintains split classical satisficing planner additional cost information provided utility model 
cost information learn search control rules allow classical planner generate low cost goal satisfying plans 
decision theoretic planning structural assumptions ffl conditional deterministic planning classical planning assumption omniscience relaxed somewhat allowing state aspects world unknown 
agent situation certain system particular set states know 
unknown truth values included initial state specification actions cause proposition unknown 
actions provide agent information plan executed conditional planners introduce idea actions providing runtime information prevailing state distinguishing action proposition true action tell agent true action executed 
action causal informational effects simultaneously changing world reporting value propositions 
second sort information useful planning time allows steps plan executed conditionally depending runtime information provided prior information producing steps 
value actions lies fact different courses action may appropriate different conditions informational effects allow runtime selection actions observations produced general pomdp model 
examples conditional planners classical framework include early warren cnlp peot smith cassandra pryor collins goldman boddy uwl etzioni hanks weld draper lesh williamson systems 
ffl probabilistic planning feedback direct probabilistic extension classical planning problem stated follows kushmerick take input probability distribution initial states stochastic actions explicit implicit transition matrices set goal states probability success threshold objective produce plan reaches goal state probability initial state distribution 
provision execution time observation straight line plans form policy possible 
restricted case infinite horizon problem actions incur cost goal states offer positive reward absorbing 
special case objective find satisficing policy optimal 
ffl probabilistic planning feedback draper 
proposed extension probabilistic planning problem actions provide feedback exactly observation model described section 
problem posed building plan leaves system goal state sufficient probability 
plan longer simple sequence actions contain conditionals loops execution depends observations generated sensing actions 
problem restricted case general pomdp problem absorbing goal states cost free actions objective find policy conditional plan leaves system goal state sufficient probability 
boutilier dean hanks comparing frameworks task oriented versus process oriented problems useful point pause contrast types problems considered classical planning literature typically studied mdp framework 
problems ai planning literature emphasized goal pursuit shot view problem solving cases viewing problem infinite horizon decision problem results satisfying formulation 
consider running example involving office robot 
simply possible model problem responding coffee requests mail arrival keeping lab tidy strict goal satisfaction problem capturing possible nuances intuitively optimal behavior 
primary difficulty explicit persistent goal states exist 
simply require robot attain state lab tidy mail awaits unfilled coffee requests exist successful plan anticipate possible system behavior goal state reached 
possible occurrence exogenous events goal achievement requires robot bias methods achieving goals way best suits expected course subsequent events 
instance coffee requests point time requests highly penalized robot situate coffee room order satisfy anticipated request quickly 
realistic decision scenarios involve task oriented process oriented behavior problem formulations take account provide satisfying models wider range situations 
complexity policy construction defined planning problem different ways having different set assumptions state space system dynamics actions deterministic stochastic observability full partial value function time separable goal goal rewards action costs partially satisfiable goals temporal deadlines planning horizon finite infinite indefinite optimality criterion optimal satisficing solutions set assumptions puts corresponding problem particular complexity class defines worst case time space bounds representation algorithm solving problem 
summarize known complexity results problem classes defined 
fully observable markov decision processes fully observable mdps time separable additive value functions solved time polynomial size state space number actions size inputs 
common algorithms solving value iteration policy iteration described section 
finite horizon discounted infinite horizon problems require polynomial amount computation iteration jsj jaj jsj jaj jsj respectively converge polynomial number iterations factor gammafl discounted case 
hand problems shown complete papadimitriou tsitsiklis means efficient parallel solution algorithm 
space required store policy stage finite horizon problem 
precisely maximum number bits required represent transition probabilities costs 

see littman dean kaelbling summary complexity results :10.1.1.108.2266
decision theoretic planning structural assumptions 
interesting classes infinite horizon problems specifically involving discounted models time separable additive reward optimal policy shown stationary policy stored jsj space 
bear mind worst case bounds 
cases better time bounds compact representations 
sections explore ways represent solve problems efficiently 
partially observable markov decision processes pomdps notorious computational difficulty 
mentioned pomdp viewed infinite state space consisting probability distributions distribution representing agent state belief point time astrom smallwood sondik 
problem finding optimal policy pomdp objective maximizing expected total reward expected total discounted reward finite horizon shown exponentially hard jsj papadimitriou tsitsiklis 
problem finding policy maximizes approximately maximizes expected discounted total reward infinite horizon shown undecidable madani condon hanks 
restricted cases pomdp problem computationally difficult worst case 
littman considers special case boolean rewards determining infinite horizon policy nonzero total reward rewards associated states non negative 
shows problem exptime complete transitions stochastic pspace hard transitions deterministic 
deterministic planning recall classical planning problem defined quite differently mdp problems agent ability observe state perfect predictive powers knowing initial state effects actions certainty 
addition rewards come reaching goal state plan achieves goal suffices 
planning problems typically defined terms set boolean features propositions complete assignment truth values features describes exactly state partial assignment truth values describes set states 
set propositions induces state space size jpj space required represent planning problem feature representation exponentially smaller required flat representation problem see section 
ability represent planning problems compactly dramatic impact worstcase complexity 
bylander shows deterministic planning problem observation pspace complete 
roughly speaking means worst planning time increase exponentially size solution plan grow exponentially problem size 
results hold action space severely restricted 
example planning problem np complete cases action restricted precondition feature postcondition feature 
conditional optimal planning pspace complete 
results inputs generally compact generally exponentially terms complexity pomdp problems phrased 
probabilistic planning probabilistic goal oriented planning pomdps typically search solution space probability distributions states boutilier dean hanks formulas describe states 
simplest problem probabilistic planning admits observability undecidable worst madani 
intuition set states finite set distributions states worst agent may search infinite number plans able determine solution exists 
algorithm guaranteed find solution plan eventually exists guaranteed terminate finite time solution plan 
conditional probabilistic planning generalization non observable probabilistic planning problem undecidable 
interesting note connection conditional probabilistic planning pomdps 
actions observations problems equivalent expressive power reward structure conditional probabilistic planning problem quite restrictive goal states positive rewards states reward goal states absorbing 
put priori bound length solution plan conditional probabilistic planning viewed infinite horizon problem objective maximize total expected undiscounted reward 
note goal states absorbing guarantee total expected reward non negative bounded infinite horizon 
technically means conditional probabilistic planning problem restricted case infinite horizon positive bounded problem puterman section 
conclude problem solving arbitrary infinite horizon undiscounted positive bounded pomdp undecidable 
commonly studied problem infinite horizon pomdp criterion maximizing expected discounted total reward finding optimal near optimal solutions problem undecidable noted 
section noting results algorithm independent describe worst case behavior 
effect indicate badly algorithm perform arbitrarily unfortunate problem instance 
interesting question build representations techniques algorithms typically perform problem instances typically arise practice 
concern leads examine problem characteristics eye exploiting restrictions placed states actions observability value function optimality criterion 
algorithmic techniques focus value function particularly take advantage time separability goal orientation 
section explore complementary techniques building compact problem representations 

solution algorithms dynamic programming search section review standard algorithms solving problems described terms unstructured flat problem representations 
noted analysis fully observable markov decision processes far widely studied models general class stochastic sequential decision problems 
describing techniques solving focusing techniques exploit structure value function time separability additivity 
decision theoretic planning structural assumptions dynamic programming approaches suppose fully observable mdp time separable additive value function 
words state space action space transition matrix pr js action reward function cost function start problem finding policy maximizes expected total reward fixed finite horizon suppose policy action performed agent state stages remaining act 
bellman shows expected value policy state computed set stage go value functions define define fpr gamma definition value function dependence initial state clear 
say policy optimal policies optimal stage go value function denoted simply value function optimal horizon policy 
bellman principle optimality bellman forms basis stochastic dynamic programming algorithms solve mdps establishing relationship optimal value function th stage optimal value function previous stage max fc pr ja gamma value iteration equation forms basis value iteration algorithm finite horizon problems 
value iteration begins value function uses equation compute sequence value functions longer time intervals horizon action maximizes right hand side equation chosen policy element 
resulting policy optimal stage fully observable mdp shorter horizon important note policy describes done stage state system agent reach certain states system initial configuration available actions 
return point 
example consider simplified version robot example state variables cr rhc rhm movement various locations ignored actions getc pum delc 
actions getc pum rhc rhm respectively true certainty 
action rhm holds rhm false probability delc cr rhc false probability leaving state unchanged probability 
reward associated cr reward associated reward state sum rewards objective satisfied state 
shows optimal stage stage stage value functions various states 
recall aspects history relevant 
boutilier dean hanks state hm rhm cr pum hm rhm cr hm rhm cr delc delc hm rhm cr hm cr delc delc hm cr getc hm rhm cri hm rhm cri pum hm cri finite horizon optimal value policy 
optimal choice action state stage pairing values state missing variables hold instantiations variables 
note simply state illustrate application equation consider calculation 
robot choice delivering coffee delivering mail expected value option stage remaining ev delc ev value maximizing choice 
notice robot action perform aim lesser objective due risk failure inherent delivering coffee 
stages remaining state robot deliver mail certainty move stage go attempt deliver coffee delc 
illustrate effects fixed finite horizon policy choice note pum 
stages remaining choice getting mail coffee robot get mail subsequent delivery stage guaranteed succeed subsequent coffee delivery may fail 
compute see ev getc ev pum stages go robot retrieve coffee coffee chances successful delivery 
expected value course action greater guaranteed mail delivery 
note stages allow sufficient time try achieve objectives fact larger reward associated coffee delivery ensures greater number stages remaining robot focus coffee retrieval delivery attempt mail retrieval delivery coffee delivery successfully completed 
faced tasks fixed finite horizon 
example may want robot perform tasks keeping lab tidy picking mail decision theoretic planning structural assumptions arrives responding coffee requests 
fixed time horizon associated tasks performed need arises 
problems best modeled infinite horizon problems 
consider problem building policy maximizes discounted sum expected rewards infinite horizon 
howard showed exists optimal stationary policy problems 
intuitively case matter stage process infinite number stages remaining optimal action state independent stage 
restrict attention policies choose action state regardless stage process 
restriction policy size jsj regardless number stages policy executed policy form contrast optimal policies finite horizon problems generally nonstationary illustrated example 
howard shows value policy satisfies recurrence fc fl pr optimal value function satisfies max fc fl pr ja value fixed policy evaluated method successive approximation identical procedure described equation 
arbitrary assignment values define fl fpr gamma sequence functions converges linearly true value function alter value iteration algorithm slightly builds optimal policies infinite horizon discounted problems 
algorithm starts value function assigns arbitrary value value estimate state calculated max fc fl pr ja delta sequence functions converges linearly optimal value function 
finite number iterations choice maximizing action forms optimal policy approximates value 

far commonly studied problem literature argued boutilier puterman mahadevan schwartz problems best modeled average reward stage optimality criterion 
discussion average reward optimality variants refinements see puterman 

number iterations stopping criterion generally involves measuring difference discussion stopping criteria convergence algorithm see puterman 
boutilier dean hanks policy iteration howard policy iteration algorithm alternative value iteration problems 
iteratively improving estimated value function modifies policies directly 
begins arbitrary policy iterates computing iteration algorithm comprises steps policy evaluation policy improvement 
policy evaluation compute value function current policy 
policy improvement find action maximizes fl pr ja delta 
algorithm iterates states step evaluates current policy solving theta linear system represented equation equation computationally expensive 
algorithm converges optimal policy linearly certain conditions converges quadratically puterman 
practice policy iteration tends converge fewer iterations value iteration 
policy iteration spends computational time individual stage result fewer stages need computed 
modified policy iteration puterman shin provides middle ground policy iteration value iteration 
structure algorithm exactly policy iteration alternating evaluation improvement phases 
key insight need evaluate policy exactly order improve 
evaluation phase involves usually small number iterations successive approximation setting small equation 
tuning value iteration modified policy iteration extremely practice puterman 
value iteration policy iteration special cases modified policy iteration corresponding setting respectively 
number variants value policy iteration proposed 
instance asynchronous versions algorithms require value function constructed policy improved state lockstep 
case value iteration infinite horizon problems long state updated sufficiently convergence assured 
similar guarantees provided asynchronous forms policy iteration 
variants important tools understanding various online approaches solving mdps bertsekas tsitsiklis 
nice discussion asynchronous dynamic programming see bertsekas bertsekas tsitsiklis 

function defined equation called learning watkins dayan gives value performing action state assuming value function accurately reflects value 

see littman discussion complexity algorithm 
decision theoretic planning structural assumptions undiscounted infinite horizon problems difficulty finding optimal solutions infinite horizon problems total reward grow limit time 
problem definition provide way ensure value metric bounded arbitrarily long horizons 
expected total discounted reward optimality criterion offers particularly elegant way guarantee bound infinite sum discounted rewards finite 
discounting appropriate certain classes problems economic problems system may terminate point certain probability realistic ai domains difficult justify counting rewards rewards discounted reward criterion appropriate 
variety ways bound total reward undiscounted problems 
cases problem structured reward bounded 
planning problems example goal reward collected actions incur cost 
case total reward bounded problem legitimately posed terms maximizing total expected undiscounted reward cases goal reached certainty 
cases discounting inappropriate total reward unbounded different success criteria employed 
example problem posed wish maximize expected average reward stage gain 
computational techniques constructing gain optimal policies similar dynamic programming algorithms described generally complicated convergence rate tends quite sensitive communicating structure periodicity mdp 
refinements gain optimality studied 
example bias optimality distinguish gain optimal polices giving preference policy total reward initial segment policy execution larger 
algorithms complicated discounted problems variants standard policy value iteration 
see puterman details 
dynamic programming pomdps dynamic programming techniques applied partially observable settings smallwood sondik 
main difficulty building policies situations state fully observable past observations provide information system current state decisions information gleaned past 
result optimal policy depend observations agent execution 
history dependent policies grow size exponential length horizon 
history dependence precludes dynamic programming observable history summarized adequately probability distribution astrom policies computed function distributions belief states 
key observation sondik smallwood sondik sondik views pomdp time separable value function state space set probability distributions obtains fully observable mdp solved dynamic programming 
computational problem approach boutilier dean hanks state space dimensional continuous space special techniques solve smallwood sondik sondik 
explore techniques note currently practical small problems cassandra cassandra littman zhang littman lovejoy 
number approximation methods developed lovejoy white iii scherer ai brafman hauskrecht parr russell zhang liu increase range solvable problems techniques presently limited practical value 
pomdps play key role reinforcement learning natural state space consisting agent observations provides incomplete information underlying system state see mccallum :10.1.1.54.132
ai planning state search noted section classical ai planning problem formulated infinite horizon mdp solved algorithm value iteration 
recall assumptions classical planning specialize general mdp model determinism actions goal states general reward function 
third assumption want construct optimal course action starting known initial state counterpart model policy dictates optimal action state stage plan 
see interest online algorithms ai led revised formulations take initial current states account 
defined classical planning problem earlier non observable process solved fully observable 
set goal states init initial state 
applying value iteration type problem equivalent determining reachability goal states system states 
instance goal states absorbing assign reward transitions gamma set states exactly set states lead goal state 
particular init successful plan constructed extracting actions stage finite horizon policy produced value iteration 
determinism assumption means agent predict state perfectly stage execution fact observe state unimportant 
assumptions commonly classical planning exploited computationally value iteration 
terminate process iteration init interested plans init acting optimally possible start state 
second terminate value iteration jsj iterations jsj init point algorithm searched possible state guarantee solution plan exists 
view classical planning finite horizon decision problem horizon jsj 
value iteration 
accurately dimensional simplex gamma dimensional space 

specifically indicates probability reaches goal region optimal policy gamma stochastic settings 
deterministic case discussed value 
decision theoretic planning structural assumptions equivalent floyd warshall algorithm find minimum cost path weighted graph floyd 
planning search value iteration theory classical planning take advantage fact goal initial states known 
particular computes value policy assignment states stages 
wasteful optimal actions computed states reached init possibly lead state problematic jsj large iteration value iteration requires computations 
reason dynamic programming approaches extensively ai planning 
restricted form value function especially fact initial goal states advantageous view planning graph search problem 
general generally known priori states desirable respect long term value defined set target states classical planning problem search algorithms appropriate 
approach taken ai planning algorithms 
way formulate problem graph search node graph correspond state initial state goal states identified search proceed forward backward graph directions simultaneously 
forward search initial state root search tree 
node chosen tree fringe set leaf nodes feasible actions applied 
action application extends plan step stage generates unique new successor state new leaf node tree 
node pruned state defines tree 
search ends state identified member goal set case solution plan extracted tree branches pruned case solution plan exists 
forward search attempts build plan adding actions current sequence actions 
forward search considers states reached init backward search viewed different ways 
arbitrarily select root search tree expand search tree fringe selecting state fringe adding tree states action cause system enter chosen state 
general action give rise predecessor vertex actions deterministic 
state pruned appears search tree 
search terminates initial state added tree solution plan extracted tree 
search similar dynamic programming algorithms finding shortest path graph 
difference backward search considers states depth search tree reach chosen goal state steps 
dynamic programming algorithms contrast visit state stage search 
difficulty backward approach described commitment particular goal state 
course assumption relaxed algorithm simultaneously search paths goal states adding level search boutilier dean hanks tree vertex reach see section goal regression viewed doing implicitly 
generally thought regression backward techniques effective practice progression forward methods 
reasoning branching factor forward graph number actions feasibly applied state substantially larger branching factor reverse graph number operators bring system state 
especially true goal sets represented small set propositional literals section 
approaches mutually exclusive mix forward backward expansions underlying problem graph terminate forward path backward path meet 
important thing observe algorithms restrict attention relevant reachable states 
forward search states reached init considered provide benefit dynamic programming methods states reachable unreachable states play role constructing successful plan 
backward approaches similarly states lying path goal region considered significant advantages dynamic programming fraction state space connected goal region 
contrast dynamic programming methods exception asynchronous methods examine entire state space iteration 
course ability ignore parts state space comes planning stringent definition relevant states positive reward states matter extent move agent closer goal choice action states unreachable init interest 
state search techniques knowledge specific initial state specific goal set constrain search process forward search exploit knowledge goal set backward search exploit knowledge initial state 
graphplan algorithm blum furst viewed planning method integrates propagation forward reachability constraints backward goal informed search 
describe approach section 
furthermore partial order planning pop viewed slightly different approach form search 
described section discuss feature intensional representations mdps planning problems 
decision trees real time dynamic programming state search techniques limited deterministic goal oriented domains 
knowledge initial state exploited general mdps forming basis decision tree search algorithms 
assume finite horizon horizon initial state init decision tree rooted init constructed way search tree deterministic planning problem french 
action applicable init forms level tree 
states result positive probability actions occur applied init placed level arc 
see bacchus 
case progression search control bonet 
argue progression deterministic planning useful integrating planning execution 
decision theoretic planning structural assumptions init max initial stages decision tree evaluating action choices init value action expected value successor states value state maximum values successor actions indicated dashed arrows selected nodes 
labeled probability pr ja init relating level actions applicable states level tree grown depth point branch tree path consisting positive probability length trajectory rooted init see 
relevant part optimal stage value function optimal policy easily computed tree 
say value node tree labeled action expected value successor states tree probabilities labeling arcs value node tree labeled state sum maximum value successor actions 
rollback procedure value leaves tree computed values successively higher levels tree determined preceding values fact form value iteration 
value state level precisely gammat maximizing actions form optimal finite horizon policy 
form value iteration directed gamma stage go values computed states reachable init steps 
infinite horizon problems solved analogous fashion determine priori depth required number iterations value iteration needed ensure convergence optimal policy 
unfortunately branching factor stochastic problems generally greater deterministic problems 
troublesome fact construct entire decision tree sure proper values computed optimal policy constructed 
stands contrast classical planning search attention focused single branch goal state reached path constructed determines satisfactory plan 
worst case behavior planning may require searching tree decision tree evaluation especially problematic 
states level value 
boutilier dean hanks entire tree generated general ensure optimal behavior 
furthermore infinite horizon problems pose difficulty determining sufficiently deep tree 
way difficulty real time search korf 
particular real time dynamic programming rtdp proposed barto way approximately solving large mdps online fashion 
interleave search execution approximately optimal policy form rtdp similar evaluation follows 
imagine agent finds particular state init build partial search tree depth uniformly branches expanded deeply 
partial tree construction may halted due time pressure due assessment agent expansion tree may fruitful 
decision act rollback procedure applied partial possibly unevenly expanded decision tree 
reward values evaluate leaves tree may offer inaccurate picture value nodes higher tree 
heuristic information estimate long term value states labeling leaves 
value iteration deeper tree accurate estimated value root generally speaking fixed heuristic 
see section structured representations mdps provide means construct heuristics dearden boutilier 
specifically admissible heuristics upper lower bounds true values leaf nodes tree methods branch bound search 
key advantage integrating search execution actual outcome action taken prune tree branches rooted outcomes 
subtree rooted realized state expanded action choice 
algorithm hansen zilberstein viewed variant methods stationary policies state action mappings extracted search process 
rtdp formulated barto 
generally form online asynchronous value iteration 
specifically values rolled backed cached improved heuristic estimates value function states question 
technique investigated bonet dearden boutilier koenig simmons closely tied korf lrta algorithm 
value updates need proceed strictly decision tree determine states key requirement rtdp simply actual state init states value updated decision action iteration 
second way avoid computational difficulties arise large search spaces sampling methods 
methods sample space possible trajectories sampled information provide estimates values specific courses action 
approach quite common reinforcement learning sutton barto simulation models generate experience value function learned 
context kearns mansour ng kearns mansour ng investigated search methods infinite horizon mdps successor states specific action randomly sampled transition distribution 
expand successor states sampled states searched 
method exponential effective horizon mixing rate mdp required expand actions number states expanded required decision theoretic planning structural assumptions full search underlying transition graph sparse 
able provide polynomial bounds ignoring action branching horizon effects number trajectories need sampled order generate approximately optimal behavior high probability 
summary seen dynamic programming methods state search methods fully observable non observable mdps forward search methods interpretable directed forms value iteration 
dynamic programming algorithms generally require explicit enumeration state space iteration search techniques enumerate reachable states branching factor may require sufficient depth search tree search methods enumerate individual states multiple times considered stage dynamic programming 
overcoming difficulty search requires cycle checking multiple path checking methods 
note search techniques applied partially observable problems 
way search space belief states just dynamic programming applied belief space mdp see section 
specifically belief states play role system states stochastic effects actions belief states induced specific observation probabilities observation distinct fixed effect belief state 
type approach pursued bonet geffner koenig simmons 

factored representations point discussion mdps explicit extensional representation set states actions states enumerated directly 
cases advantageous representational computational point view talk properties states sets states set possible initial states set states action executed 
generally convenient compact describe sets states certain properties features enumerate explicitly 
representations descriptions objects substitute objects called intensional technique choice ai systems 
intensional representation planning systems built defining set features sufficient describe state dynamic system interest 
example state described set features robot location lab mail robot mail pending coffee request robot coffee 
second features take values take values true false 
assignment values features completely defines state state space comprises possible combinations feature values jsj 
feature factor typically assigned unique symbolic name indicated second column 
fundamental tradeoff extensional intensional representations clear example 
extensional representation coffee example views space possible states single variable takes possible boutilier dean hanks values intensional factored representation views state cross product variables takes substantially fewer values 
generally state space grows exponentially number features required describe system 
fact state system described set features allows adopt factored representations actions rewards components mdp 
factored action representation instance generally describes effect action specific state features entire states 
provides considerable representational economy 
instance strips action representation fikes nilsson state transitions induced actions represented implicitly describing effects actions features change value action executed 
factored representations compact individual actions affect relatively features effects exhibit certain regularities 
similar remarks apply representation reward functions observation models 
regularities factored representations suitable planning problems exploited planning decision making algorithms 
factored representations long classical ai planning similar representations adopted mdp models ai 
section section focus economy representation afforded exploiting structure inherent planning domains 
section section describe structure explicit factored representations exploited computationally plan policy construction 
factored state spaces markov chains examining structured states systems state described finite set state variables values change time 
simplify illustration potential space savings assume state variables boolean 
variables size state space jsj large specifying representing dynamics explicitly state transition diagrams theta matrices impractical 
furthermore representing reward function vector specifying observational probabilities similarly infeasible 
section define class problems dynamics represented space cases 
considering represent markov chains compactly consider incorporating actions observations rewards 
state variable take finite number values omega stand set possible values 
assume omega finite follows applied countable state action spaces 
say state space flat specified state variable variable denoted general model values 
state space factored state variable 
state possible assignment values variables 
letting represent ith state variable state space cross product value spaces individual state variables theta omega just denotes state process stage random variable representing value ith state variable stage 
variables called fluents ai literature mccarthy hayes 
classical planning atomic propositions describe domain 
decision theoretic planning structural assumptions bayesian network pearl representational framework compactly representing probability distribution factored form 
networks typically represent atemporal problem domains apply techniques represent markov chains encoding chain transition probabilities network structure dean kanazawa 
formally bayes net directed acyclic graph vertices correspond random variables edge variables indicates direct probabilistic dependency 
network constructed reflects implicit independencies variables 
network quantified specifying probability variable vertex conditioned possible values immediate parents graph 
addition network include marginal distribution unconditional probability vertex parents 
quantification captured associating conditional probability table cpt variable network 
independence assumptions defined graph quantification defines unique joint distribution variables network 
probability event space computed algorithms exploit independencies represented graphical structure 
refer pearl details 
figures page special cases bayes nets called temporal bayesian networks 
networks vertices graph represent system state different time points arcs represent dependencies time points 
temporal networks vertex parent temporal predecessor conditional distributions transition probability distributions marginal distributions distributions initial states 
networks reflect extensional representation scheme states explicitly enumerated techniques building performing inference probabilistic temporal networks designed especially application factored representations 
illustrates stage temporal bayes net tbn describing state transition probabilities associated markov chain induced fixed policy executing action repeatedly moving counterclockwise 
tbn set variables partitioned corresponding state variables time stage corresponding state variables time 
directed arcs indicate probabilistic dependencies variables markov chain 
diachronic arcs directed time variables time variables synchronic arcs directed variables time 
contains diachronic arcs synchronic arcs discussed section 
state time network induces unique distribution states 
quantification network describes state particular variable changes function certain state variables 
lack direct arc generally directed path synchronic arcs variables variable variable means knowledge irrelevant prediction immediate stage evolution variable markov process 
shows compact representation best circumstances potential links stage omitted 
graphical representation explicit fact policy action affect state variable loc exogenous events mess affect boutilier dean hanks rhc rhm rhm rhc rhc cr cr loc loc rhc loc cr cr loc time time factored tbn markov chain induced moving counterclockwise selected cpts shown 
variables cr tidy respectively 
furthermore dynamics loc variables described knowledge state parent variables instance distribution loc depends value loc previous stage loc loc probability loc probability 
similarly cr true probability due event true false simple policy rhc remains true false certainty true false previous stage 
effects relevant variables independent 
instantiation variables time distribution states computed multiplying conditional probabilities relevant variables 
ability omit arcs graph locality independence action effects strong effect number parameters supplied complete model 
full transition matrix size transition model requires parameters 
example shows exploit independence represent markov chains compactly example extreme effectively relationship variables chain viewed product independently evolving processes 

show cpts brevity 

fact exploit fact probabilities sum leave entry unspecified row cpt explicit transition matrix 
case tbn requires explicit parameters transition matrix requires delta entries 
generally ignore fact comparing sizes representations 
decision theoretic planning structural assumptions rhm rhm rhc rhc cr cr loc loc time time rhc loc loc rhc cr loc rhc loc cr rhc tbn markov chain induced moving counterclockwise delivering coffee 
boutilier dean hanks general subprocesses interact exhibit certain independencies regularities exploited tbn representation 
consider distinct markov chains exhibit different types dependencies 
illustrates tbn representing markov chain induced policy robot consistently moves counterclockwise office coffee case delivers coffee user 
notice different variables dependent instance predicting value rhc requires knowing values loc rhc cpt rhc shows robot retains coffee stage certainty stage locations executes delc losing coffee 
variable loc depends value rhc 
location change exception robot office coffee location remains robot move executes delc 
effect variable cr explained follows robot office delivers coffee possession fulfill outstanding coffee request 
chance cr remaining true conditions indicates chance spilling coffee 
dependencies additional diachronic arcs tbn requires parameters 
distribution resulting states determined multiplying conditional distributions individual variables 
variables related state known variables time loc rhc independent 
words pr loc cr rhc rhm js pr loc js pr js pr cr js pr rhc js pr rhm js pr js illustrates tbn representing markov chain induced policy assume act moving counterclockwise slightly different effect 
suppose robot moves hallway adjacent location chance spilling coffee possession fragment cpt rhc illustrates possibility 
furthermore robot carrying mail loses coffee accidentally intentionally delc action chance lose mail 
notice effects policy variables rhc rhm correlated accurately predict probability rhm determining probability rhc correlation modeled synchronic arc rhc rhm slice network 
independence variables hold synchronic arcs 
determining probability resulting state requires simple probabilistic reasoning example application chain rule 
example write pr rhc rhm js pr rhm pr rhc js joint distribution variables computed equation term replacing pr rhc js pr rhm js variables correlated remaining variables independent 
refer synchronic arcs simple 
general allow synchronic diachronic arcs 
decision theoretic planning structural assumptions rhm rhm rhc rhc cr cr loc loc time time rhc rhc rhm rhc loc pr rhc pr rhm tbn markov chain induced moving counterclockwise delivering coffee correlated effects 
factored action representations just extended markov chains account different actions extend tbn representation account fact state transitions influenced agent choice action 
discuss variety techniques specifying transition matrices exploit factored state representation produce representations natural compact explicit transition matrices 
implicit event models implicit event model section effects actions exogenous events combined single transition matrix 
consider models section 
saw previous section algorithms value policy iteration require transition models reflect ultimate transition probabilities including effects exogenous events 
way model dynamics fully observable mdp represent action separate tbn 
tbn shown seen representation action policy inducing markov chain example consists repeated application action 
network fragment illustrates interesting aspects tbn delc action including effects exogenous events 
robot satisfies outstanding coffee request delivers coffee office coffee chance shown conditional probability table cr 
effect rhc explained follows boutilier dean hanks loc loc rhc rhc cr cr cr cr cr rhc loc rhc loc cr cr time time loc rhc cr rhc loc pr rhc pr cr factored tbn action delc structured cpt representations 
robot loses coffee user delivers office attempts delivery chance random take coffee robot 
case markov chains effects actions different variables correlated case introduce synchronic arcs 
correlations thought ramifications baker finger lin reiter 
structured cpts conditional probability table cpt node cr rows assignment parents 
cpt contains number regularities 
intuitively reflects fact coffee request met successfully variable false time delc executed robot coffee right location user office 
cr remains true true true probability 
words distinct cases considered corresponding rules governing stochastic effect delc cr 
represented compactly decision tree representation branches summarize groups cases involving multivalued variables loc shown compactly decision graph 
tree graph representations cpts interior nodes labeled parent variables edges values variables leaves terminals distributions child variable values 
decision tree decision graph representations represent actions fully observable mdps boutilier hoey st aubin hu boutilier 
child boolean label leaves probability variable true probability variable false minus value 
decision theoretic planning structural assumptions described detail poole boutilier goldszmidt 
intuitively trees graphs embody rule structure family conditional distributions represented cpt settings consider yield considerable representational compactness 
rule representations directly poole context decision processes compact trees poole 
generically refer representations type structured cpts 
probabilistic strips operators tbn representation viewed oriented describing effects actions distinct variables 
cpt variable expresses stochastically changes persists function state certain variables 
long noted ai research planning reasoning action actions change state limited ways affect relatively small number variables 
difficulty variable oriented representations explicitly assert variables unaffected specific action persist value see cpt rhc instance infamous frame problem mccarthy hayes 
form representation actions called outcome oriented representation explicitly describes possible outcomes action possible joint effects variables 
idea underlying strips representation classical planning fikes nilsson 
classical strips operator described precondition set effects 
identifies set states action executed describes input state changes result action 
probabilistic strips operator pso hanks hanks mcdermott kushmerick extends strips representation ways 
allows actions different effects depending context second recognizes effects actions known certainty 
formally pso consists set mutually exclusive exhaustive logical formulae called contexts stochastic effect associated context 
intuitively context discriminates situations action differing stochastic effects 
stochastic effect set change sets simple list variable values probability attached change set requirement probabilities sum 
semantics stochastic effect described follows stochastic effect action applied state possible resulting states determined change sets occurring corresponding probability resulting state associated change set constructed changing variable values state match change set variables persist value 
note 
fact certain direct dependencies variables bayes net rendered irrelevant specific variable assignments studied generally guise context specific independence boutilier friedman goldszmidt koller see geiger heckerman shimony related notions 

conditional nature effects feature deterministic extension strips known adl pednault 
boutilier dean hanks cr rhc cr rhc rhc rhc rhc cr rhc cr rhc rhc cr cr nil cr cr nil loc rhc pso representation delc action 
loc nil loc nil loc nil loc nil loc rhc rhm loc rhc loc rhc rhm rhc nil loc pso representation simplified action 
context hold state transition distribution action state easily determined 
gives graphical depiction pso delc action shown tbn 
contexts rhc rhc loc represented decision tree 
leaf branch decision tree stochastic effect set change sets associated probabilities determined corresponding context 
example holds action possible effects robot loses coffee may may satisfy coffee request due chance mail may may arrive 
notice outcome spelled completely 
number outcomes contexts large due possible exogenous events discuss section 
key difference psos lies treatment persistence 
variables unaffected action cpts tbn model variables mentioned pso model compare variable loc representations delc 
way psos said solve frame problem unaffected variables need mentioned action description 

keep manageable ignore effect exogenous event mess variable 
discussion frame problem see boutilier goldszmidt 
decision theoretic planning structural assumptions rhm rhc loc cr rhm rhc loc cr rhm rhc cr loc rhm rhc cr mess loc simplified explicit event model delc 
psos provide effective means representing actions correlated effects 
recall description action captured robot may drop coffee moves hallway may drop mail drops coffee 
tbn representation rhc rhc parents rhm model dependence rhm change value variable rhc 
shows action pso format simplicity ignore occurrence exogenous events 
pso representation offer economical representation correlated effects possible outcomes moving hallway spelled explicitly 
specifically possible simultaneous change values variables question clear 
explicit event models explicit event models represented somewhat different form 
discussion section form taken explicit event models depends crucially assumptions interplay effects action exogenous events 
certain assumptions explicit event models concise 
illustrate shows deliver coffee action represented tbn exogenous events explicitly represented 
slice network shows effects action delc presence exogenous events 
subsequent slices describe effects events mess events illustration 
notice presence extra random variables representing occurrence events question 
cpts nodes reflect occurrence probabilities events various boutilier dean hanks conditions directed arcs event variables state variables indicate effects events 
probabilities depend state variables general tbn represents occurrence vectors see section compact form 
notice contrast event occurrence variables explicitly represent action occurrence variable network modeling effect system action taken 
example reflects assumptions described section events occur action takes place event effects commutative reason ordering events mess network irrelevant 
model system passes intermediate necessarily distinct states goes stage stage subscripts suggest process 
course described earlier actions events combined decomposable way complex combination functions modeled example see boutilier puterman 
equivalence representations obvious question ask concerns extent certain representations inherently concise 
focus standard implicit event models describing domain features different representations suitable 
tbn pso representations oriented representing changes values state variables induced action key distinction lies fact model influence variable separately pso model explicitly represents complete outcomes 
simple tbn network synchronic arcs represent action cases correlations action effect different state variables 
worst case effect variable differs state time variable time variables parents 
regularities exploited structured cpt representations action requires specification parameters assuming boolean variables compared entries required explicit transition matrix 
number parents variable bounded need specify conditional probabilities 
reduced cpts exhibit structure represented concisely decision tree 
instance cpt captured representation choice entries polynomial function number parents variable representation size delta polynomial number state variables 
case instance actions stochastic effects variable requires number pre conditions hold different effect comes play 
pso representation may concise tbn action multiple independent stochastic effects 
pso requires possible change list enumerated corresponding probability occurrence 
number changes grows exponentially number variables affected action 
fact evident 
sections discuss representations model choice action explicitly variable network 
decision theoretic planning structural assumptions rhc cr rhc rhc nil nil loc rhc nil factored pso representation delc action 
impact exogenous events affects number variables stochastically independently 
problem arise respect direct action effects 
consider action set parts spray painted part successfully painted probability successes uncorrelated 
ignoring complexity representing different conditions action take place simple tbn represent action parameters success probability part 
contrast pso representation require list distinct change lists associated probabilities 
pso representation exponentially larger number affected variables simple tbn representation 
fortunately certain variables affected deterministically cause pso representation blow 
furthermore pso representations modified exploit independence action effects different state variables boutilier dearden dearden boutilier escaping combinatorial difficulty 
instance represent delc action shown factored form illustrated simplicity show effect action exogenous event 
tbn determine effect combining change sets appropriate contexts multiplying corresponding probabilities 
simple defined original set state variables sufficient represent actions 
correlated action effects require presence synchronic arcs 
worst case means time variables gamma parents 
fact acyclicity condition assures worst case total number parents gamma specifying entries required explicit transition matrix 
number parents occurring time slice bounded regularities cpts allow compact representation profitably 
pso representations compare favorably cases action effects different variables correlated 
case psos provide somewhat economical representation action effects primarily needn worry frame conditions 
main advantage psos need enlist aid probabilistic reasoning procedures determine transitions induced actions correlated effects 
contrast explicit specification outcomes psos type reasoning required determine joint effects action represented tbn 
section discusses certain problem transformations render simple sufficient mdp 
boutilier dean hanks form synchronic arcs described section 
essentially correlated effects compiled explicit outcomes psos 
results littman shown simple psos represent action represented tbn exponential blowup representation size 
effected clever problem transformation new sets actions propositional variables introduced simple tbn pso representation 
structure original tbn reflected new planning problem incurring polynomial increase size input action descriptions description policy 
resulting policy consists actions exist underlying domain extracting true policy difficult 
noted representation automatically constructed general tbn specification provided directly actions variables transformed problem physical meaning original mdp 
transformations eliminate synchronic constraints discussion assumed variables propositions tbn pso action descriptions original state variables 
certain problem transformations ensure represent action simple long require original state variables 
transformation simply clusters variables action correlated effect 
new compound variable takes values assignments clustered variables tbn removing need synchronic arcs 
course variable domain size exponential number clustered variables 
intuitions underlying psos convert general tbn action descriptions simple tbn descriptions explicit events dictating precise outcome action 
intuitively event occur different forms corresponding different change list induced action change list respect variables question 
example convert action description explicit event model shown 
notice event takes values corresponding possible effects correlated variables rhc rhm 
specifically denotes event robot escaping hallway successfully losing cargo denotes event robot losing coffee denotes event losing coffee mail 
effect event space represents possible combined effects obviating need synchronic arcs network 
actions explicit nodes network difficulty tbn pso approach action description action represented separately offering opportunity exploit patterns actions 
instance fact location persists actions moving clockwise counterclockwise means frame axiom duplicated tbn actions case psos course 
addition ramifications correlated action 
describes markov chain induced policy representation easily extracted 
decision theoretic planning structural assumptions loc loc rhc rhc rhm rhm event rhc event event rhm loc loc hall rhc rhm time time explicit event model removes correlations 
effects duplicated actions 
instance coffee request occurs probability robot ends office correlation duplicated actions 
compelling example robot move briefcase new location number ways 
capture fact ramification contents briefcase move location briefcase regardless action moves briefcase 
circumvent difficulty introduce choice action random variable network conditioning distribution state variable transitions value variable 
state variables event variables explicit event models generally require distribution action variable intent simply model schematically conditional state transition distributions particular choice action 
choice action dictated decision maker policy determined 
reason anticipating terminology influence diagrams see section call nodes decision nodes depict network diagrams boxes 
variable take value action available agent 
tbn explicit decision node shown 
restricted example imagine decision node take values clk 
fact issuance coffee request depends robot successfully moved remained office represented arc loc cr repeated multiple action networks 
furthermore noisy persistence actions represented adding action pum undercuts advantage see try combine actions 
difficulty straightforward decision nodes standard representation influence diagram literature adding candidate actions cause explosion network dependency structure 
example consider boutilier dean hanks cr cr loc loc act time time influence diagram restricted process 
act act action action cpt influence diagram unwanted dependencies influence diagrams 
decision theoretic planning structural assumptions action networks shown 
action true probability true having effect true true 
combining actions single network obvious way produces influence diagram shown 
notice parent nodes inheriting union parents individual networks plus action node requiring cpt entries actions additional entries action affect individual networks reflect fact depends performed performed 
fact lost naively constructed influence diagram 
structured cpts recapture independence compactness representation tree captures distribution concisely requiring entries 
structured representation allows concisely express persists actions 
large domains expect variables generally unaffected substantial number actions requiring representations influence diagrams 
see boutilier goldszmidt deeper discussion issue relationship frame problem 
provide distributional information action choice hard see tbn explicit decision node represent markov chain induced particular policy natural way 
specifically adding arcs state variables time decision node value decision node choice action point dictated prevailing state 
influence diagrams influence diagrams howard matheson shachter extend bayesian networks include special decision nodes represent action choices value nodes represent effect action choice value function 
presence decision nodes means action choice treated variable decision maker control 
value nodes treat reward variable influenced usually deterministically certain state variables 
influence diagrams typically associated schematic representation stationary systems tool decision analysts sequential decision problem carefully handcrafted 
generic influence diagrams discussed shachter 
event theory plan construction associated influence diagrams choice possible actions stage explicitly encoded model 
influence diagrams usually model finite horizon decision problems explicitly describing evolution process stage terms state variables 
section decision nodes take values specific actions set possible actions tailored particular stage 
addition analyst generally include stage state variables thought relevant decision subsequent stages 
value nodes key feature influence diagrams discussed section 
usually single value node specified arcs indicating 
generally randomized policy represented specifying distribution possible actions conditioned state 
boutilier dean hanks rew rhm cr rhm cr representation reward function influence diagram 
influence particular state decision variables multiple stages value function 
influence diagrams typically model partially observable problems 
arc state variable decision node reflects fact value state variable available decision maker time action chosen 
words variable value forms part observation time prior action selected time policy constructed refer variable 
allows compact specification observation probabilities associated system 
fact probability observation depends directly certain variables mean far fewer model parameters required 
factored reward representation noted common formulating mdp problems adopt simplified value function assigning rewards states costs actions evaluating histories combining factors simple function addition 
simplification allows representation value function significantly parsimonious complex comparison complete histories 
representation requires explicit enumeration state action space motivating need compact representations parameters 
factored representations rewards action costs obviate need enumerate state action parameters explicitly 
action effect particular variable reward associated state depends values certain features state 
example robot domain associate rewards penalties undelivered mail unfulfilled coffee requests lab 
reward penalty independent variables individual rewards associated groups states differ values relevant variables 
relationship rewards state variables represented value nodes influence diagrams represented diamond 
conditional reward table crt node table associates reward combination values parents graph 
table shown locally exponential number relevant variables 
shows case stationary markovian reward function influence diagrams represent decision theoretic planning structural assumptions nonstationary history dependent rewards represent value functions finite horizon problems 
worst case crt take exponential space store cases reward function exhibits structure allowing represented compactly decision trees graphs boutilier strips tables boutilier dearden logical rules poole 
shows fragment possible decision tree representation reward function running example 
independence assumptions studied multiattribute utility theory keeney raiffa provide way reward functions represented compactly 
assume component attributes reward function independent contributions state total reward individual contributions combined functionally 
instance imagine penalizing states cr holds partial reward gamma penalizing situations undelivered mail rhm gamma penalizing gamma proportion things 
reward state determined simply adding individual penalties associated feature 
individual component rewards combination function constitute compact representation reward function 
tree fragment reflects additive independent structure just described considerably complex representation defines independent rewards individual propositions separately 
additive reward functions mdps considered boutilier brafman geib meuleau hauskrecht kim peshkin kaelbling dean boutilier singh cohn 
example structured rewards goal structure studied classical planning 
goals generally specified single proposition set literals achieved 
generally represented compactly 
haddawy hanks explore generalizations goal oriented models permit extensions partial goal satisfaction admit compact representations 
factored policy value function representation techniques studied far concerned input specification mdp states actions reward function 
components problem solution policy optimal value function candidates compact structured representation 
simplest case stationary policy fully observable problem policy associate action state nominally requiring representation size jsj 
problem exacerbated nonstationary policies pomdps 
example policy finite horizon stages generates policy size jsj 
finite horizon pomdp possible observable history length require different action choice histories generated fixed policy maximum number possible observations action 
fact policies require space motivates need find compact functional representations standard techniques tree structures discussed 
methods dealing pomdps conversion belief space see section complex 
boutilier dean hanks delc clk clk loc clk loc rhc cr getc pum pum hrm tree representation policy 
actions reward functions represent policies value functions 
focus stationary policies value functions logical function representation may 
example schoppers uses strips style representation universal plans deterministic plan policies 
decision trees policies value functions boutilier chapman kaelbling 
example policy robot domain specified decision tree 
policy dictates instance cr rhc true robot deliver coffee user office move office office mail case pickup mail way 
summary section discussed number compact factored representations components mdp 
began discussing intensional state representations temporal bayesian networks device representing system dynamics 
tree structured conditional probability tables cpts probabilistic strips operators psos introduced alternative transition matrices 
similar tree structures logical representations introduced representing reward functions value functions policies 
representations describe problem compactly offer guarantee problem solved effectively 
section explore algorithms factored representations avoid iterating explicitly entire set states actions 

abstraction aggregation decomposition methods greatest challenge mdps basis dtp lies discovering computationally feasible methods construction optimal approximately optimal satisficing policies 
course arbitrary decision problems intractable producing satisficing approximately optimal policies generally infeasible 
previous sections suggest realistic application domains may exhibit considerable structure furthermore structure modeled explicitly exploited typical problems solved effectively 
instance structure type lead compact decision theoretic planning structural assumptions factored representations input data output policies polynomial sized respect number variables actions describing problem 
suggests compact problem representations policy construction techniques developed exploit structure tractable commonly occurring problem instances 
dynamic programming state search techniques described section exploit structure different kind 
value functions decomposed state dependent reward functions state goal functions tackled dynamic programming regression search respectively 
algorithms exploit structure decomposable value functions prevent having search explicitly possible policies 
algorithms polynomial size state space curse dimensionality algorithms infeasible practical problems 
compact problem representations aid specification large problems clear large system specified compactly representation exploits regularities domain 
ai research dtp stressed regularities implicit compact representations speed planning process 
techniques focus optimal approximately optimal policy construction 
subsection focus abstraction aggregation techniques especially manipulate factored representations 
roughly techniques allow explicit implicit grouping states indistinguishable respect certain characteristics value optimal action choice 
refer set states grouped manner aggregate state cluster assume set states constitutes partition state space say state exactly state union states comprises entire state space 
grouping similar states state treated single state alleviating need perform computations state individually 
techniques approximation elements state approximately indistinguishable values states lie small interval 
look problem decomposition techniques mdp broken various pieces solved independently solutions guide search global solution 
subprocesses solutions interact minimally treated independent expect approximately optimal global solution 
furthermore structure problem requires solution particular subproblem solutions subproblems ignored altogether 
related reachability analysis restrict attention relevant regions state space 
reachability analysis communicating structure mdp form certain types decompositions 
specifically distinguish serial decompositions parallel decompositions 
result serial decomposition viewed partitioning state space blocks representing independent subprocess solved 
serial decomposition relationship blocks generally complicated case abstraction aggregation 
partition resulting decomposition 
group states non disjoint sets cover entire state space 
consider soft state aggregation see singh jaakkola jordan 
boutilier dean hanks states particular block may behave quite differently respect say value dynamics 
important consideration choosing decomposition possible represent block compactly compute efficiently consequences moving block subproblems corresponding subprocesses solved efficiently 
parallel decomposition somewhat closely related mdp 
mdp divided parallel sub mdps decision action causes state change sub mdp 
mdp cross product join sub mdps contrast union serial decomposition 
briefly discuss methods parallel mdp decomposition 
abstraction aggregation way problem structure exploited policy construction relies notion aggregation grouping states indistinguishable respect certain problem characteristics 
example group states optimal action value respect stage go value function 
aggregates constructed solution problem 
ai emphasis generally placed particular form aggregation abstraction methods states aggregated ignoring certain problem features 
policy illustrates type abstraction states cr rhc loc true grouped action selected state 
intuitively propositions hold problem features ignored abstracted away deemed irrelevant 
decision tree representation policy value function partitions state space distinct cluster leaf tree 
representations strips rules state space similarly 
precisely type abstraction compact factored representations actions goals discussed section 
tbn shown effect action delc variable cr cpt cr stochastic effect state parent variables value 
representation abstracts away variables combining states distinct values irrelevant non parent variables 
intensional representations easy decide features ignore certain stage problem solving implicitly aggregate state space 
dimensions abstractions type compared 
uniformity uniform abstraction variables deemed relevant irrelevant uniformly state space nonuniform abstraction allows certain variables ignored certain conditions 
distinction illustrated schematically 
tabular representation cpt viewed form uniform abstraction effect action variable distinguished clusters states differ value parent variable distinguished states agree parent variables disagree decision tree representation cpt embodies nonuniform abstraction 
second dimension comparison accuracy 
states grouped basis certain characteristics abstraction called exact states decision theoretic planning structural assumptions uniform nonuniform exact approximate adaptive fixed different forms state space abstraction 
cluster agree characteristic 
non exact abstraction called approximate 
illustrated schematically exact abstraction groups states agree value assigned value function approximate abstraction allows states grouped differ value 
extent states differ measure quality approximate abstraction 
third dimension adaptivity 
technically property abstraction abstractions particular algorithm 
adaptive abstraction technique abstraction change course computation fixed abstraction scheme groups states see 
example imagine abstraction representation value function revising abstraction represent accurately 
abstraction aggregation techniques studied literature mdps 
bertsekas develop adaptive aggregation opposed abstraction technique 
proposed method operates flat state spaces exploit implicit structure state space 
adaptive uniform abstraction method proposed schweitzer 
solving stochastic queuing models 
methods referred aggregation disaggregation procedures typically accelerate calculation value function fixed policy 
calculation requires computational effort quadratic size state space impractical large state spaces 
aggregation disaggregation procedures states aggregated clusters 
system equations solved series summations performed requiring effort cubic number clusters 
disaggregation step performed cluster requiring effort linear size cluster 
net result total linear total number states worst cubic size largest cluster 
dtp generally assumed computations linear size full state space infeasible 
important develop methods perform boutilier dean hanks polynomial log size state space 
problems amenable reductions unacceptable sacrifice solution quality 
section review techniques dtp aimed achieving reductions 
goal regression classical planning section introduced general technique regression backward search state space solve classical planning problems involving deterministic actions performance criteria specified terms reaching goal satisfying state 
difficulty search requires branch search tree lead particular goal state 
commitment goal state may retracted backtracking search process sequence actions lead particular goal state initial state 
goal usually specified set literals representing set states reaching state equally suitable may wasteful restrict search finding plan reaches particular element goal regression abstraction technique avoids problem choosing particular goal state pursue 
regression planner works searching sequence actions follows current set subgoals sg initialized iteration action ff selected achieves current subgoals sg deleting preconditions conflict subgoals 
subgoals achieved removed current subgoal set replaced formula representing context ff achieve current subgoals forming sg process known regressing sg ff 
process repeated conditions holds current subgoal set satisfied initial state case current sequence actions selected successful plan action applied case current sequence extended successful plan earlier action choice reconsidered 
example example consider simplified version robot planning example section illustrate value iteration robot actions pum getc delc deterministic obvious way 
initial state init hcr rhc goal set fcr mg 
regressing results sg fcr 
regressing sg delc results sg 
regressing sg pum results sg mg 
regressing sg getc results sg fmg 
note init sg sequence actions getc pum delc successfully reach goal state 
see algorithm implements form abstraction note goal provides initial partition state space dividing set states goal satisfied second set 
viewed partition zero stage go value function represents states value positive represents states value zero 
regression step thought revising partition 
planning algorithm attempts satisfy current subgoal set sg applying action ff uses decision theoretic planning structural assumptions cr cr rhm rhc rhm rhc getc pum delc goal example goal regression 
regression compute largest set states executing ff subgoals satisfied 
particular state space states sg sg way abstraction mechanism implemented goal regression considered adaptive 
viewed stage value function state satisfying sg reach goal state steps action sequence produced sg regression process stopped initial state member state sg illustrates repartitioning state space different regions sg steps example 
regression produces compact representation value function discussion deterministic goal dynamic programming section analogy exact regions produced regression record property goal reachability contingent particular choice action action sequence 
standard dynamic programming methods implemented structured way simply noticing number different regions produced ith iteration considering actions regressed stage 
union regressions form states positive values making representation stage go value function exact 
notice iteration costly regression actions attempted approach obviates need backtracking ensure shortest plan 
standard regression provide guarantees commitment particular search strategy breadthfirst dynamic programming strips action descriptions forms basic idea schoppers universal planning method schoppers 
general technique solving classical planning problems partial order planning pop chapman sacerdoti embodied popular planning algorithms snlp mcallester rosenblitt ucpop penberthy weld 
main motivation commitment approach comes realization regression techniques incrementally building plan temporal dimension 
iteration commit inserting step plan 
cases determined particular step appear plan necessarily step plan cases step 
case states sg reach goal region steps 
case specific sequence actions chosen far 

type planning called nonlinear commitment planning 
see weld survey nice overview 
boutilier dean hanks consideration appear fact recognized choices reveal inconsistency 
cases regression algorithm prematurely commit incorrect ordering eventually backtrack choice 
example suppose problem scenario robot hold item time coffee mail 
picking mail causes robot spill coffee possession similarly grasping coffee drop mail 
plan generated regression longer valid actions delc inserted plan action added achieve rhc rhm making false search plan backtrack 
ultimately discovered successful plan actions performed sequence 
partial order planning algorithms proceed regression algorithms choosing actions achieve subgoals regression determine new subgoals leaving actions unordered extent possible 
strictly speaking subgoal sets aren regressed goal action precondition addressed separately actions ordered relative action threatens negate desired effect 
example algorithm place actions delc plan leave unordered 
pum added plan achieve requirement rhm ordered unordered respect delc 
getc added plan achieve rhc action delc threats arise 
getc threatens desired effect rhm pum 
resolved ordering getc pum 
assume ordering chosen 
second pum threatens desired effect rhc getc 
threat resolved placing pum getc delc threat resolved ordering getc pum ordering consistent 
result plan getc delc pum 
backtracking required generate plan actions initially unordered orderings introduced discovery threats required 
terms abstraction incomplete partially ordered plan threat free certain open conditions preconditions subgoals viewed way partially completed regression plan state satisfying open conditions reach goal state executing total ordering plan actions consistent current set ordering constraints 
see kambhampati framework unifies various approaches solving classical plan generation problems 
techniques relying regression studied extensively deterministic setting applied probabilistic unobservable kushmerick partially observable draper hanks weld domains 
part techniques assume goal performance criterion attempt construct plans probability reaching goal state exceeds threshold 
augment standard pop methods techniques evaluating plan probability achieving goal techniques improving probability adding structure plan 
section consider regression related techniques solve mdps performance criteria general goals 
decision theoretic planning structural assumptions stochastic dynamic programming structured representations key idea underlying propositional goal regression need regress relevant propositions action extended stochastic dynamic programming methods value iteration policy iteration solve general mdps 
key difficulties overcome lack specific goal region uncertainty associated action effects 
viewing state space partitioned goal non goal clusters consider grouping states expected values 
ideally want group states value respect optimal policy 
consider somewhat difficult task grouping states value respect fixed policy 
essentially task performed policy evaluation step policy iteration insights construct optimal policies 
fixed policy want group states value policy 
generalizing goal versus non goal distinction partition groups states immediate rewards 
analogue regression developed stochastic case reason backward construct new partition states grouped value respect stage go value function 
iterate manner kth iteration produce new partition groups states stage go value function 
iteration perform polynomial number states size mdp representation lucky total number states bounded logarithmic factor size state space 
implement scheme effectively perform operations regression enumerating set states structured representations state transition value policy functions play role 
approaches type taken boutilier boutilier dearden boutilier boutilier dearden goldszmidt dietterich flann hoey :10.1.1.111.5793:10.1.1.158.8490
illustrate basic intuitions approach describing value iteration discounted infinite horizon 
assume mdp specified compact representation reward function decision tree actions 
value iteration produce sequence value functions delta delta delta representing utility optimal stage policy 
aim produce compact representation value function suitable produce compact representation optimal stationary policy 
compact representation reward function clear constitutes compact representation usual think leaf tree cluster states having identical utility 
produce compact form proceed phases 
branch tree provides intensional description conjunction variable values labeling branch state region comprising states identical value respect initial value function deterministic action ff perform regression step description determine conditions perform ff cluster 
furthermore determine region state space containing states identical value boutilier dean hanks time time example action 
respect execution ff stage go 
unfortunately nondeterministic actions handled quite way state action lead different regions non zero probability 
leaf tree representing region regress conjunction describing region action ff produce conditions true false specified probability 
words regressing standard fashion determine conditions true produce set distinct conditions true different probabilities 
piecing regions produced different labels description construct set regions state region transitions action ff particular part identical probability identical expected value boutilier 
view generalization propositional goal regression suitable decision theoretic problems 
example illustrate consider example action shown value function shown left 
order generate set regions consisting states value identical proceed steps see 
determine conditions fixed probability making true fixed probability moving left right subtree 
conditions tree representing cpt node portion tree representing see step 
notice tree leaves labeled probability making true implicitly false 
true know value value zero stages go false need know true 
ignore immediate reward cost distinctions region produced description recall value performing ff state ff expected value 
simply focus states elements identical expected value 
differences immediate reward cost added fact 
decision theoretic planning structural assumptions step step iteration decision theoretic regression 
step produces portion tree dashed lines step produces portion dotted lines 
determine value 
probability true tree representing cpt node step conditions cpt conjoined conditions required predicting probability grafting tree tree step 
grafting slightly different leaves tree full tree attached leaf tree simplified attached leaf removal redundant test variable notice need attach tree leaf true probability conditions relevant determination false 
leaves newly formed tree pr pr 
joint distributions effect variables independent semantics network tells probability having true zero stages go conditions labeling appropriate branch tree hold stage go 
words new tree uniquely determines state stage remaining probability making conditions labeling branches true 
computation expected value obtained performing stage go placed leaves tree expectation values leaves new set regions produced way describes function ff ff value associated performing ff state stage go acting optimally 
functions action ff see section determine course process repeated number times produce suitable optimal policy respect basic technique number different ways 
dietterich flann propose ideas similar restrict attention mdps goal regions boutilier dean hanks deterministic actions represented strips operators rendering true techniques directly applicable 
boutilier 
develop version modified policy iteration produce tree structured policies value functions boutilier dearden develop version value iteration described 
algorithms extended deal correlations action effects synchronic arcs boutilier 
abstraction schemes categorized nonuniform exact adaptive 
utility exact abstraction techniques tested real world problems date 
boutilier results series process planning examples reported scheme shown useful especially larger problems 
example specific problem states tree representation value function leaves indicating tremendous amount regularity value function 
schemes exploit regularity solve problems quickly example half time required modified policy iteration lower memory demands 
schemes involve substantial overhead tree construction smaller problems little regularity overhead time savings simple vector matrix representations methods faster generally provide substantial memory savings 
viewed best worst case behavior described boutilier 
series linear examples problems value functions represented trees size linear number problem variables tree scheme solves problems orders magnitude faster classical state techniques 
contrast problems exponentially distinct values tested distinct value state tree construction methods required construct complete decision tree addition performing number expected value maximization computations classical methods 
worst case tree construction overhead algorithm run times slower standard modified policy iteration 
hoey similar algorithm described uses algebraic decision diagrams adds pardo somenzi trees :10.1.1.111.5793
adds simple generalization boolean decision diagrams bdds bryant allow terminal nodes labeled real values just boolean values 
essentially add algorithms similar tree algorithms isomorphic subtrees shared 
lets adds provide compact representations certain types value functions 
highly optimized add manipulation evaluation software developed verification community applied solving mdps 
initial results provided hoey encouraging showing considerable savings tree algorithms problems :10.1.1.111.5793
example add algorithm applied state example described revealed value function distinct values cf 
tree leaves required produced add description value function internal nodes 
solved problem minutes times faster earlier reported timing results decision trees improvement due optimized add software packages 
similar results obtain problems problems states 
dietterich flann describe context reinforcement learning method solving mdps directly 
decision theoretic planning structural assumptions solved hours 
encouraging fact worst case exponential examples overhead associated adds compared classical vector methods trees factor compared flat modified policy iteration state variables lessens problems larger 
tree algorithms methods applied real world problems 
exact abstraction schemes clear examples resulting policies value functions may compact set regions may get large reaching level individual states boutilier precluding computational savings 
boutilier dearden develop approximation scheme exploits tree structured nature value functions produced 
stage value function pruned produce smaller accurate tree approximates specifically approximate value functions represented trees leaves labeled upper lower bound value function region decisiontheoretic regression performed bounds 
certain subtrees value tree pruned leaves subtree close value tree large computational constraints 
scheme nonuniform approximate adaptive 
approximation scheme tailored provide roughly accurate value function maximum tree size smallest value function respect tree size minimum accuracy 
results reported boutilier dearden show approximation small set examples including worst case examples tree algorithms allows substantial reduction computational cost 
instance variable worst case example small amount pruning introduced average error reduced computation time factor 
aggressive pruning tends increase error decrease computation time rapidly making appropriate tradeoffs dimensions addressed 
method remains tested evaluated realistic problems 
structured representations solution algorithms applied problems 
methods solving influence diagrams shachter exploit structure natural way shachter explore connection influence diagrams relationship influence diagram solution techniques dynamic programming 
boutilier poole show classic history independent methods solving pomdps conversion belief states exploit types structured representations described 
exploiting structured representations pomdps remains explored depth 
plans difficulties adaptive abstraction schemes suggested fact different abstractions constructed repeatedly incurring substantial computational overhead 
overhead compensated savings obtained policy construction reducing number backups problematic 
cases savings dominated time space required generate abstractions motivates development cheaper accurate approximate clustering schemes 
boutilier dean hanks way reduce overhead adopt fixed abstraction scheme abstraction produced 
approach adopted classical planning hierarchical abstraction planners pioneered sacerdoti system sacerdoti 
similar form abstraction studied knoblock see knoblock tenenberg yang 
variables case propositional ranked criticality roughly important variables solution planning problem abstraction constructed deleting problem description set propositions low criticality 
solution problem plan achieves elements original goal deleted 
preconditions effects actions deleted accounted solution solution original problem 
solution restrict search solution underlying concrete space 
hierarchies refined abstractions propositions introduced back domain stages 
form abstraction uniform propositions deleted uniformly fixed 
solution need solution problem tempted view approximate abstraction method 
best think plan solution form heuristic information help solve true problem quickly 
intuitions underlying knoblock scheme applied dtp boutilier dearden variables ranked degree influence reward function subset important variables deemed relevant 
subset determined variables influence relevant variables effects actions determined easily strips tbn action descriptions deemed relevant 
remaining variables deemed irrelevant deleted description problem action reward descriptions 
leaves mdp smaller state space fewer variables solved standard methods 
recall state space reduction exponential number variables removed 
view method uniform fixed approximate abstraction scheme 
output classical abstraction methods policy produced implemented value 
degree optimal policy true optimal policy differ value bounded priori abstraction fixed 
example simple illustration suppose reward satisfying coffee requests penalty satisfying substantially greater keeping lab tidy delivering mail 
suppose time pressure requires agent focus specific subset objectives order produce small state space 
case reward laden variables problem see cr judged important 
action descriptions determine variables directly indirectly affect probability achieving cr cr rhc loc deemed relevant allowing rhm ignored 
state space reduced size size 
addition action descriptions tidy trivial deleted 
decision theoretic planning structural assumptions advantage abstractions easily computed incur little overhead 
disadvantages uniform nature abstractions restrictive relevant reward variables determined policy constructed knowledge agent ability control variables 
result important variables large impact reward agent control may taken account important variables agent influence ignored 
series abstractions take account objectives decreasing importance posteriori valuable objectives dealt risk controllability taken account boutilier 
policies generated levels seed value policy iteration levels certain cases reducing time convergence dearden boutilier 
suggested dearden boutilier value function heuristic online search policies improve policy constructed discussed section 
error approximate value function overcome extent search heuristic function improved asynchronous updates 
different abstraction adopted drips planner haddawy haddawy doan 
actions abstracted collapsing branches possible outcomes maintaining probabilistic intervals disjunctive effects 
actions combined decomposition hierarchy hierarchical task networks 
planning done evaluating plans decomposition network producing ranges utility possible instantiations plans refining plans possibly optimal 
task networks means search restricted finite horizon open loop plans action choice restricted possible refinements network 
task networks offer useful way encode priori heuristic knowledge structure plans 
model minimization reduction methods abstraction techniques defined recast terms minimizing stochastic automaton providing unifying view different methods offering new insights abstraction process dean givan :10.1.1.42.6651
automata theory know finite state machine recognizing language exists unique minimal finite state machine recognizes exponentially smaller minimal machine called minimal model language captures relevant aspect machines said equivalent 
define similar notions equivalence mdps 
primarily concerned planning important equivalent mdps agree value functions policies 
practical standpoint may necessary find minimal model find reduced model sufficiently small equivalent 
apply idea model minimization model reduction planning follows algorithm takes input implicit mdp model factored form produces lucky explicit reduced model size polynomial factor size factored representation 
favorite state dynamic programming algorithms solve explicit model 
boutilier dean hanks think dynamic programming techniques rely structured representations discussed earlier operating reduced model explicitly constructing model 
cases building reduced model may appropriate cases save considerable effort explicitly constructing parts reduced model absolutely necessary 
potential computational problems model minimization techniques sketched 
small minimal model may exist may hard find 
look reduced model easier find necessarily minimal 
fail case look model small useful approximately equivalent original factored model 
careful mean approximate intuitively mdps approximately equivalent corresponding optimal value functions small factor 
order practical mdp model reduction schemes operate directly implicit factored representation original mdp 
lee yannakakis call online model minimization 
online model minimization starts initial partition states 
minimization iteratively refines partition splitting clusters smaller clusters 
cluster split states cluster behave differently respect transitions states clusters 
local property satisfied clusters partition model consisting aggregate states correspond clusters partition equivalent original model 
addition initial partition method splitting clusters satisfy certain properties guaranteed find minimal model 
case mdp reduction initial partition groups states reward nearly reward case approximation methods 
clusters partitions manipulated online model reduction methods represented intensionally formulas involving state variables 
instance formula rhc loc represents set states robot coffee located mail room 
operations performed clusters require conjoining complementing simplifying checking satisfiability 
worst case operations intractable successful application methods depends critically problem way represented 
illustrate basic idea simple example 
example depicts simple version running example single action 
boolean state variables corresponding rhc robot coffee rhc cr outstanding request coffee cr considering location possibilities loc robot coffee room loc 
outstanding coffee request depends request previous stage robot coffee room 
location depends location previous stage reward depends outstanding coffee request 

property required initial partition states cluster partition defining minimal model recall minimal model unique cluster initial partition 
decision theoretic planning structural assumptions cr cr cr pr cr loc loc pr loc rhc rhc pr rhc loc loc cr rhc loc loc cr rhc factored model illustrating model reduction techniques 
cr cr cr loc cr cr loc models involving aggregate states model corresponding initial partition minimal model 
initial partition shown defined terms immediate rewards 
say states particular starting cluster behave respect particular destination cluster probability destination cluster states starting cluster 
property satisfied starting cluster cr destination cluster cr split cluster labeled cr obtain model 
property satisfied pairs clusters model minimal model 
lee yannakakis algorithm non deterministic finite state machines extended givan dean handle classical strips planning problems givan dean mdps dean givan :10.1.1.42.6651:10.1.1.34.3923
basic step splitting cluster closely related goal regression relationship explored givan dean :10.1.1.34.3923
variants model reduction approach apply action space large represented factored form dean givan kim example action specified set parameters corresponding allocations different resources optimization problem 
exist algorithms computing approxi boutilier dean hanks reachability serial problem decomposition 
mate models dean givan leach efficient planning algorithms approximate models givan leach dean :10.1.1.34.3923
reachability analysis serial problem decomposition reachability analysis existence goal states exploited different settings 
instance deterministic classical planning problems regression viewed form directed dynamic programming 
uncertainty certain policy reaches goal state dynamic programming backups need performed goal states possible states 
regression implicitly exploits certain reachability characteristics domain special structure value function 
reachability analysis applied broadly forms basis various types problem decomposition 
decomposition problem solving mdp broken subprocesses solved independently roughly independently solutions 
subprocesses solutions interact marginally treated independent expect nonoptimal global solution result 
furthermore structure problem requires solution particular subproblem needed solutions subproblems ignored need computed 
instance regression analysis optimal action states reach goal region irrelevant solution classical ai planning problem 
shown schematically regions explored backward search state space states reach goal search horizon deemed relevant 
regions may reachable start state fact reach goal state means known irrelevant 
system dynamics stochastic scheme form basis approximately optimal solution method regions ignored transition regression goal region region 
similar remarks progression forward search start state apply illustrated 
decision theoretic planning structural assumptions schemes proposed ai literature exploiting reachability constraints apart usual forward backward search approaches 
peot smith introduce operator graph structure computed prior problem solving caches reachability relationships propositions 
graph consulted planning process deciding actions insert plan resolve threats 
graphplan algorithm blum furst attempts blend considerations forward backward reachability deterministic planning context 
difficulties regression may regress goal region sequence operators find region reached initial state 
example states region may reachable initial state 
graphplan constructs variant operator graph called planning graph certain forward reachability constraints posted 
regression implemented usual current subgoal set violates forward reachability constraints point subgoal set abandoned regression search backtracks 
conceptually think graphplan constructing forward search tree state space initial state root doing backward search goal region backward tree 
course process state constraints possible variable values hold simultaneously different planning stages recorded regression search backward planning graph 
sense graphplan viewed constructing abstraction forward reachable states distinguished unreachable states planning stage distinction states quickly identify infeasible regression paths 
note graphplan approximates distinction overestimating set reachable states 
overestimation opposed underestimation ensures regression search space contains legitimate plans 
reachability exploited solution general mdps 
dean 
propose envelope method solving goal mdps approximately 
assuming path generated quickly start state goal region mdp consisting states path neighboring states solved 
deal transitions lead envelope heuristic method estimates value states 
time permits set neighboring states expanded increasing solution quality accurately evaluating quality alternative actions 
ideas underlying graphplan applied general mdps boutilier brafman geib construction planning graph generalized deal stochastic conditional action representation offered 
initial state set initial states algorithm discovers reachability constraints form graphplan instance variable values obtain simultaneously action sequence starting initial state lead state values hold 
reachability constraints discovered process simplify action reward representation mdp refers reachable states 
case action 
approximate abstraction techniques described section generate heuristic information 

general ary constraints type considered boutilier 
boutilier dean hanks requires unreachable set values hold effectively deleted 
cases certain variables discovered immutable initial conditions deleted leading smaller mdps 
simplified representation retains original propositional structure standard abstraction methods applied reachable mdp 
suggested strong synergy exists abstraction reachability analysis techniques reduce size effective mdp solved dramatically isolation 
just reachability constraints prune regression paths deterministic domains prune value function policy estimates generated decision theoretic regression abstraction algorithms boutilier 
results reported boutilier limited single process planning domain show reachability analysis abstraction provide substantial reductions size effective mdp solved domains 
domain binary variables reachability considerations generally eliminated order variables depending initial state arity binary ternary constraints considered reducing state space size incorporating abstraction reachable mdp provided considerably reduction reducing mdp sizes ranging effectively zero states 
case occur discovered values variables impact reward altered case course action expected utility mdp needn solved solved applying null actions zero cost 
serial problem decomposition communicating structure communicating reachability structure mdp provides way formalize different types problem decomposition 
classify mdp markov chains induced stationary policies admits 
fixed markov chain group states maximal recurrent classes transient states described section 
mdp recurrent policy induces markov chain single recurrent class 
mdp policy induces single recurrent class possibly transient states 
mdp communicating pair states policy reach mdp weakly communicating exists closed set states communicating plus possibly set states transient policy 
call mdps 
notions crucial construction optimal average reward policies exploited problem decomposition 
suppose mdp discovered consist set recurrent classes delta delta delta matter policy adopted agent leave class enters class set transient states 
clear optimal policy restricted class constructed policy decisions states outside values 
essentially viewed independent subprocess 

simple way view classes think agent adopting randomized policy action adopted state positive probability 
classes induced markov chain correspond classes mdp 
decision theoretic planning structural assumptions observation leads suggestion optimal policy construction solve subprocesses consisting recurrent classes mdps remove states mdp forming reduced mdp consisting transient states 
break reduced mdp recurrent classes solve independently 
key doing effectively value function original recurrent states computed solving independent subproblems step take account transitions recurrent classes reduced mdp 
shows mdp broken classes constructed way 
original mdp classes recurrent solved independently 
removed mdp class recurrent reduced mdp 
course solved classes rely value states transitions class value function available purpose solve consisted jdj states 
hand solved solved 
lin dean provide version type decomposition employs factored representation 
factored representation allows dimensionality reduction different state subspaces aggregating states differ values irrelevant variables subspaces 
key decomposition discovery recurrent classes mdp 
puterman suggests adaptation fox landi algorithm fox landi discovering structure markov chains recall jsj 
alleviate difficulties algorithms explicit state representation boutilier puterman propose variant algorithm works factored tbn representation 
difficulty form decomposition reliance strongly independent subproblems recurrent classes mdp 
explored exact approximate techniques restrictive assumptions 
simple method approximation construct approximately recurrent classes 
imagine nearly independent sense transitions low probability high cost 
treating independent lead approximately optimal policies error bounded 
solutions interact strongly solutions constructed completely independently different approach solving decomposed problem taken 
optimal value function pointed calculate optimal value function thing note don need know value function states just value state reachable state single step 
set states outside reachable single step state inside referred states periphery values states intersection periphery summarize value exiting refer set states periphery block kernel mdp 
different blocks interact states kernel 

ross varadarajan related suggestion solving average reward problems 

slight correction suggested algorithm boutilier puterman 
boutilier dean hanks loc loc loc loc decomposition location 
loc loc loc loc kernel kernel decomposition depicting kernel states 
decision theoretic planning structural assumptions example spatial features provide natural dimension decompose domain 
running example location robot decompose state space blocks states block possible locations 
shows decomposition superimposed state transition diagram mdp 
states kernel shaded correspond entrances exits locations 
star shaped topology induced kernel decomposition kushner chen dean lin illustrated 
hallway location explicitly represented 
simplification may reasonable hallway conduit moving room case function hallway accounted dynamics governing states kernel 
figures idealized full set features running example kernel contain states 
technique computing optimal policy entire mdp involves repeatedly solving mdps corresponding individual blocks 
techniques works follows initially guess value state kernel 
current estimate values kernel states solve component mdps solution produces new estimate states kernel 
adjust values states kernel considering difference current new estimates iterate difference negligible 
iterative method solving decomposed mdp special case lagrangian method finding extrema function 
literature replete methods linear nonlinear systems equations winston 
possible formulate mdp linear program puterman 
dantzig wolfe developed method decomposing system equations involving large number variables set smaller systems equations interacting set coupling variables variables shared blocks 
dantzig wolfe decomposition method original large system equations solved iteratively solving smaller systems adjusting coupling variables iteration adjustment required 
linear programming formulation mdp values states encoded variables 
kushner chen exploit fact mdps modeled linear programs dantzig wolfe decomposition method solve mdps involving large number states 
dean lin describe general framework solving decomposed mdps pointing kushner chen special case addresses issue decompositions come 
dean 
investigate methods decomposing state space blocks reachable steps fewer reachable steps see discussion reachability 
set states reachable fewer steps construct mdp basis policy approximates optimal policy 
increases size block states reachable steps increases ensuring better solution amount time required compute 
ideally aggregate kernel states value provide compact representation 
remainder section won consider opportunities combining aggregation decomposition methods 
boutilier dean hanks solution increases 
dean 
discuss methods solving mdps time critical problems trading quality time 
ignored issue obtain decompositions expedite calculations 
ideally component decomposition yield simplification aggregation abstraction reducing dimensionality component avoiding explicit enumeration states 
lin presents methods exploiting structure certain special cases communicating structure revealed domain expert 
general finding decomposition minimize effort spent solving component mdps quite hard hard finding smallest circuit consistent input output behavior best hope heuristic methods 
unfortunately aware particularly useful heuristics finding serial decompositions markov decision processes 
developing heuristics clearly area investigation 
related form decomposition development macro operators mdps sutton 
macros long history classical planning problem solving fikes hart nilsson korf generalized mdps hauskrecht meuleau kaelbling dean boutilier parr parr russell precup sutton singh stone veloso sutton thrun schwartz 
macro taken local policy region state space block terminology 
mdp comprising blocks set macros defined block mdp solved selecting macro action block global policy induced set macros picked close optimal best combination macros set available 
sutton precup macros treated temporally actions models defined macro treated single action policy value iteration concrete actions 
hauskrecht parr parr russell models exploited hierarchical fashion high level mdp consisting states lying boundaries blocks macros actions chosen states 
issue macro generation constructing set macros guaranteed provide flexibility select close optimal global behavior addressed hauskrecht parr 
relationship serial decomposition techniques quite close problems discovering decompositions constructing sets macros exploiting intensional representations areas clearer compelling solutions required 
date area provided computational utility solution mdps cases hand crafted region decompositions macros provided little taken account factored nature mdps 
reason discuss detail 
general notion serial decomposition continues develop shows great promise 
multiattribute reward parallel decomposition form decomposition parallel decomposition mdp broken set sub mdps run parallel 
specifically stage global decision process state subprocess affected 
instance action decision theoretic planning structural assumptions mdp mdp mdp parallel problem decomposition 
affects state subprocess 
intuitively action suitable execution original mdp state reasonably sub mdps 
generally sub mdps form product join decomposition original state space contrast union decompositions state space determined serial decompositions state space formed cross product sub mdp state spaces join certain states subprocesses linked 
subprocesses may identical action spaces may action space global action choice factored choice subprocess 
case sub mdps may completely independent case global mdp solved exponentially faster 
challenging problem arises constraints legal action combinations 
example actions subprocesses require certain shared resources interactions global choice may arise 
parallel mdp decomposition wish solve sub mdps policies value functions generated help construct optimal approximately optimal solution original mdp highlighting need find appropriate decompositions mdps develop suitable merging techniques 
parallel decomposition methods involved decomposing mdp subprocesses suitable distinct objectives 
reward functions deal multiple objectives associated independent reward rewards summed determine global reward natural way decompose mdps 
ideas multiattribute utility theory seen play role solution mdps 
boutilier 
decompose mdp specified additive reward function abstraction technique described section 
component reward function abstraction generate mdp referring variables relevant component 
certain state variables may multiple sub mdps relevant objective original state space join subspaces 
decomposition tackled automatically 
merging tackled ways 
involves sum value functions obtained solving sub mdps heuristic estimate true value function 
heuristic guide online state search see section 
sub mdps interact heuristic perfect leads backtrack free optimal action selection interact search 
note existence factored mdp representation crucial abstraction method 
boutilier dean hanks required detect conflicts 
note sub mdp identical sets actions 
action space large branching factor search process may prohibitive 
singh cohn deal parallel decomposition assume global mdp specified explicitly set parallel mdps generating decompositions global mdp issue 
global mdp cross product state action spaces sub mdps reward functions summed 
constraints feasible action combinations couple solutions sub mdps 
solve global mdp sum sub mdp value functions upper bound optimal global value function maximum global state lower bound 
bounds form basis action elimination procedure value iteration algorithm solving global mdp 
unfortunately value iteration run explicit state space global mdp 
action space cross product potential computational bottleneck value iteration 
meuleau 
parallel decomposition approximate solution stochastic resource allocation problems large state action spaces 
singh cohn mdp specified terms number independent mdps involving distinct objective action choices linked shared resource constraints 
value functions individual mdps constructed offline set online action selection procedures 
approximation procedures discussed approach attempt construct policy explicitly similar real time search rtdp respect construct value function explicitly 
method applied large mdps state spaces size actions spaces larger solve problems roughly half hour 
solutions produced approximate size problem precludes exact solution estimates solution quality hard derive 
method applied smaller problems nature exact solution computed approximations high quality meuleau 
able solve large mdps large factored state action spaces model relies somewhat restrictive assumptions nature local value functions ensure solution quality 
basic approach appears generalizable offers great promise solving large factored mdps 
algorithms singh cohn meuleau seen rely implicitly structured mdp representations involving independent subprocesses 
approaches take advantage automatic mdp decomposition algorithms boutilier factored representations explicitly play part 
summary seen number ways intensional representations exploited solve mdps effectively enumeration state space 
include techniques abstraction mdps including relevance analysis goal regression decision theoretic regression techniques relying reachability analysis serial decomposition methods parallel mdp decomposition exploiting multiattribute nature 
singh cohn incorporate methods removing unreachable states value iteration 
decision theoretic planning structural assumptions reward functions 
methods fortunate circumstances offer exponential reduction solution time space required represent policy value function come guarantees reductions certain special cases 
methods described provide approximate solutions error bounds provided offer optimality guarantees general provide optimal solutions suitable assumptions 
avenue explored detail relationship structured solution methods developed mdps described techniques solving bayesian networks 
algorithms discussed section rely structure inherent tbn representation mdp natural ask embody intuitions underlie solution algorithms bayes nets solution techniques bayes nets directly indirectly applied mdps ways give rise algorithms similar discussed 
remains open question point undoubtedly strong ties exist 
shachter explored connections influence diagrams mdps 
kjaerulff investigated computational considerations involved applying join tree methods reasoning tasks monitoring prediction temporal bayes nets 
abstraction methods discussed section interpreted form variable elimination dechter zhang poole 
elimination variables occurs temporal order orderings time slice exploit tree graph structure cpts 
approximation schemes variable elimination dechter poole may related certain approximation methods developed mdps 
independence decompositions mdps discussed section clearly viewed exploiting independence relations explicit unrolling tbn 
development connections bayes net inference algorithms doubt prove useful enhancing understanding existing methods increasing range applicability pointing new algorithms 

concluding remarks search effective algorithms controlling automated agents long important history problem continue grow importance decisionmaking functionality automated 
disciplines ai decision analysis addressed problem carried different problem definitions different sets simplifying assumptions different viewpoints different representations algorithms problem solving 
assumptions historical reasons reasons convenience difficult separate essential assumptions accidental 
important clarify relationships problem definitions crucial assumptions solution techniques meaningful synthesis take place 
analyzed various approaches particular class sequential decision problems studied decision analysis ai literature 
started general reasonably neutral statement problem couched convenience language markov decision processes 
demonstrated various disciplines define problem assumptions effect boutilier dean hanks assumptions worst case time complexity solving problem defined 
assumptions regarding main factors distinguish commonly studied classes decision problems ffl observation sensing sensing tend fast cheap accurate laborious costly noisy 
ffl incentive structure agent behavior evaluated ability perform particular task ability control system interval time 
moving worst case analysis generally assumed pathological cases inevitably difficult agent able solve typical easy cases effectively 
agent needs able identify structure problem exploit structure algorithmically 
identified ways structural regularities recognized represented exploited computationally 
structure induced domain level simplifying assumptions full observability goal satisfaction time separable value functions 
second structure exploited compact domain specific encodings states actions rewards 
designer techniques structure explicit decision making algorithms exploit structural regularities apply particular problem hand 
third involves aggregation abstraction decomposition techniques structural regularities discovered exploited problem solving process 
developing framework allows comparison domains assumptions problems techniques drawn different disciplines discover essential problem structure required specific representations algorithms prove effective way insights techniques developed certain problems certain disciplines evaluated potentially applied new problems disciplines 
main focus elucidation various forms structure decision problems exploited representationally computationally 
part focused propositional structure commonly associated planning ai circles 
complete treatment included compact representations dynamics rewards policies value functions considered continuous real valued domains 
instance discussed linear dynamics quadratic cost functions control theory neural network representations value functions frequently adopted reinforcement learning community bertsekas tsitsiklis tesauro discussed partitioning continuous state spaces addressed reinforcement learning moore atkeson 
addressed relational quantificational structure order planning representations 
techniques cast framework described example piecewise linear value functions seen form abstraction different linear components applied different regions clusters state space 

bertsekas tsitsiklis provide depth treatment neural network linear function approximators mdps reinforcement learning 
decision theoretic planning structural assumptions certain cases indicated devise methods exploit types structure research lines limited 
extent representations algorithms described complementary pose obstacles combination 
remains seen interact techniques developed forms structure continuous state action spaces 
analysis raises opportunities challenges understanding assumptions techniques relationships designer decision making agents tools build effective problem solvers challenges lie development additional tools integration existing ones 
acknowledgments careful comments referees 
ron parr robert st aubin comments earlier draft 
students cs spring taught martha pollack university pittsburgh cpsc winter university british columbia deserve detailed comments 
boutilier supported nserc research ogp nce program project ic 
dean supported part national science foundation presidential young investigator award iri air force advanced research projects agency department defense contract 

hanks supported part arpa rome labs part nsf iri 
allen hendler tate 
eds 

readings planning 
morgan kaufmann san mateo 
astrom 

optimal control markov decision processes incomplete state estimation 
math 
anal 
appl 
bacchus boutilier grove 

rewarding behaviors 
proceedings thirteenth national conference artificial intelligence pp 
portland 
bacchus boutilier grove 

structured solution methods nonmarkovian decision processes 
proceedings fourteenth national conference artificial intelligence pp 
providence ri 
bacchus kabanza 

temporal logic control search forward chaining planner 
proceedings third european workshop planning italy 
available url ftp logos uwaterloo ca pub tlplan tlplan ps bacchus teh 

making forward chaining relevant 
proceedings fourth international conference ai planning systems pp 
pittsburgh pa boutilier dean hanks pardo somenzi 

algebraic decision diagrams applications 
international conference computer aided design pp 

ieee 
baker 

nonmonotonic reasoning framework situation calculus 
artificial intelligence 
barto bradtke singh 

learning act real time dynamic programming 
artificial intelligence 
bellman 

dynamic programming 
princeton university press princeton nj 
bertsekas 

adaptive aggregation infinite horizon dynamic programming 
ieee transactions automatic control 
bertsekas 

dynamic programming 
prentice hall englewood cliffs nj 
bertsekas tsitsiklis 

neuro dynamic programming 
athena belmont ma 
blackwell 

discrete dynamic programming 
annals mathematical statistics 
blum furst 

fast planning graph analysis 
proceedings fourteenth international joint conference artificial intelligence pp 
montreal canada 
bonet geffner 

learning sorting decision trees pomdps 
proceedings fifteenth international conference machine learning pp 
madison wi 
bonet geffner 

robust fast action selection mechanism 
proceedings fourteenth national conference artificial intelligence pp 
providence ri 
boutilier 

correlated action effects decision theoretic regression 
proceedings thirteenth conference uncertainty artificial intelligence pp 
providence ri 
boutilier brafman geib 

prioritized goal decomposition markov decision processes synthesis classical decision theoretic planning 
proceedings fifteenth international joint conference artificial intelligence pp 
nagoya japan 
boutilier brafman geib 

structured reachability analysis markov decision processes 
proceedings fourteenth conference uncertainty artificial intelligence pp 
madison wi 
boutilier dearden 

abstractions decision theoretic planning time constraints 
proceedings twelfth national conference artificial intelligence pp 
seattle wa 
decision theoretic planning structural assumptions boutilier dearden 

approximating value trees structured dynamic programming 
proceedings thirteenth international conference machine learning pp 
bari italy 
boutilier dearden goldszmidt 

exploiting structure policy construction 
proceedings fourteenth international joint conference artificial intelligence pp 
montreal canada 
boutilier dearden goldszmidt 

stochastic dynamic programming factored representations 
manuscript 
boutilier friedman goldszmidt koller 

context specific independence bayesian networks 
proceedings twelfth conference uncertainty artificial intelligence pp 
portland 
boutilier goldszmidt 

frame problem bayesian network action representations 
proceedings eleventh biennial canadian conference artificial intelligence pp 
toronto 
boutilier poole 

computing optimal policies partially observable decision processes compact representations 
proceedings thirteenth national conference artificial intelligence pp 
portland 
boutilier puterman 

process oriented planning average reward optimality 
proceedings fourteenth international joint conference artificial intelligence pp 
montreal canada 
brafman 

heuristic variable grid solution method pomdps 
proceedings fourteenth national conference artificial intelligence pp 
providence ri 
bryant 

graph algorithms boolean function manipulation 
ieee transactions computers 
bylander 

computational complexity propositional strips planning 
artificial intelligence 


linear stochastic systems 
wiley new york 
cassandra kaelbling littman 

acting optimally partially observable stochastic domains 
proceedings twelfth national conference artificial intelligence pp 
seattle wa 
cassandra littman zhang 

incremental pruning simple fast exact method pomdps 
proceedings thirteenth conference uncertainty artificial intelligence pp 
providence ri 
chapman 

planning conjunctive goals 
artificial intelligence 
boutilier dean hanks chapman kaelbling 

input generalization delayed reinforcement learning algorithm performance comparisons 
proceedings twelfth international joint conference artificial intelligence pp 
sydney australia 
dantzig wolfe 

decomposition principle dynamic programs 
operations research 
dean allen aloimonos 

artificial intelligence theory practice 
benjamin cummings 
dean givan 

model minimization markov decision processes 
proceedings fourteenth national conference artificial intelligence pp 
providence ri 
aaai 
dean givan kim 

solving planning problems large state action spaces 
proceedings fourth international conference ai planning systems pp 
pittsburgh pa dean givan leach 

model reduction techniques computing approximately optimal solutions markov decision processes 
proceedings thirteenth conference uncertainty artificial intelligence pp 
providence ri 
dean kaelbling kirman nicholson 

planning deadlines stochastic domains 
proceedings eleventh national conference artificial intelligence pp 

dean kaelbling kirman nicholson 

planning time constraints stochastic domains 
artificial intelligence 
dean kanazawa 

model reasoning persistence causation 
computational intelligence 
dean lin 

decomposition techniques planning stochastic domains 
proceedings fourteenth international joint conference artificial intelligence pp 

dean wellman 

planning control 
morgan kaufmann san mateo california 
dearden boutilier 

integrating planning execution stochastic domains 
proceedings tenth conference uncertainty artificial intelligence pp 
washington dc 
dearden boutilier 

abstraction approximate decision theoretic planning 
artificial intelligence 
dechter 

bucket elimination unifying framework probabilistic inference 
proceedings twelfth conference uncertainty artificial intelligence pp 
portland 
decision theoretic planning structural assumptions dechter 

mini buckets general scheme generating approximations automated reasoning probabilistic inference 
proceedings fifteenth international joint conference artificial intelligence pp 
nagoya japan 


sur un probl eme de production de dans 
management science 
dietterich flann 

explanation learning reinforcement learning unified approach 
proceedings twelfth international conference machine learning pp 
lake tahoe nv 
draper hanks weld 

probabilistic model action planning information gathering 
proceedings tenth conference uncertainty artificial intelligence pp 
washington dc 
draper hanks weld 

probabilistic planning information gathering contingent execution 
proceedings second international conference ai planning systems pp 

etzioni hanks weld draper lesh williamson 

approach planning incomplete information 
proceedings third international conference principles knowledge representation reasoning pp 
boston ma 
fikes hart nilsson 

learning executing generalized robot plans 
artificial intelligence 
fikes nilsson 

strips new approach application theorem proving problem solving 
artificial intelligence 
finger 

exploiting constraints design synthesis 
ph thesis stanford university stanford 
floyd 

algorithm shortest path 
communications acm 
fox landi 

algorithm identifying ergodic transient states stochastic matrix 
communications acm 
french 

decision theory 
press new york 
geiger heckerman 

advances probabilistic reasoning 
proceedings seventh conference uncertainty artificial intelligence pp 
los angeles ca 
givan dean 

model minimization regression propositional strips planning 
proceedings fifteenth international joint conference artificial intelligence pp 
nagoya japan 
boutilier dean hanks givan leach dean 

bounded parameter markov decision processes 
proceedings fourth european conference planning ecp pp 
toulouse france 
goldman boddy 

representing uncertainty simple planners 
proceedings fourth international conference principles knowledge representation reasoning pp 
bonn germany 
haddawy doan 

abstracting probabilistic actions 
proceedings tenth conference uncertainty artificial intelligence pp 
washington dc 
haddawy hanks 

utility models goal directed decision theoretic planners 
computational intelligence 
haddawy 

decision theoretic refinement planning abstraction 
proceedings second international conference ai planning systems pp 
chicago il 
hanks 

projecting plans uncertain worlds 
ph thesis yale university department computer science new haven ct hanks mcdermott 

modeling dynamic uncertain world symbolic probabilistic reasoning change 
artificial intelligence 
hanks russell wellman 
eds 

decision theoretic planning proceedings aaai spring symposium 
aaai press menlo park 
hansen zilberstein 

heuristic search cyclic graphs 
proceedings fifteenth national conference artificial intelligence pp 
madison wi 
hauskrecht 

heuristic variable grid solution method pomdps 
proceedings fourteenth national conference artificial intelligence pp 
providence ri 
hauskrecht 

planning control stochastic domains imperfect information 
ph thesis massachusetts institute technology cambridge 
hauskrecht meuleau kaelbling dean boutilier 

hierarchical solution markov decision processes macro actions 
proceedings fourteenth conference uncertainty artificial intelligence pp 
madison wi 
hoey st aubin hu boutilier 

spudd stochastic planning decision diagrams 
proceedings fifteenth conference uncertainty artificial intelligence stockholm 
appear 
howard 

dynamic programming markov processes 
mit press cambridge massachusetts 
decision theoretic planning structural assumptions howard matheson 

influence diagrams 
howard matheson 
eds principles applications decision analysis 
strategic decisions group menlo park ca 
kambhampati 

refinement planning unifying framework plan synthesis 
ai magazine summer 
kearns mansour ng 

sparse sampling algorithm nearoptimal planning large markov decision processes 
proceedings sixteenth international joint conference artificial intelligence stockholm 
appear 
keeney raiffa 

decisions multiple objectives preferences value tradeoffs 
john wiley sons new york 
kjaerulff 

computational scheme reasoning dynamic probabilistic networks 
proceedings eighth conference uncertainty ai pp 
stanford 
knoblock 

generating abstraction hierarchies automated approach reducing search planning 
kluwer boston 
knoblock tenenberg yang 

characterizing abstraction hierarchies planning 
proceedings ninth national conference artificial intelligence pp 
anaheim ca 
koenig 

optimal probabilistic decision theoretic planning markovian decision theory 
sc 
thesis ucb csd university california berkeley computer science department 
koenig simmons 

real time search nondeterministic domains 
proceedings fourteenth international joint conference artificial intelligence pp 
montreal canada 
korf 

macro operators weak method learning 
artificial intelligence 
korf 

real time heuristic search 
artificial intelligence 
kushmerick hanks weld 

algorithm probabilistic planning 
artificial intelligence 
kushner chen 

decomposition systems governed markov chains 
ieee transactions automatic control 
lee yannakakis 

online minimization transition systems 
proceedings th annual acm symposium theory computing pp 
victoria bc 
lin reiter 

state constraints revisited 
journal logic computation 
boutilier dean hanks lin 

exploiting structure planning control 
ph thesis department computer science brown university 
lin dean 

generating optimal policies high level plans conditional branches loops 
proceedings third european workshop planning pp 

littman 

probabilistic propositional planning representations complexity 
proceedings fourteenth national conference artificial intelligence pp 
providence ri 
littman dean kaelbling 

complexity solving markov decision problems 
proceedings eleventh conference uncertainty artificial intelligence pp 
montreal canada 
littman 

algorithms sequential decision making 
ph thesis cs brown university department computer science providence ri 
lovejoy 

computationally feasible bounds partially observed markov decision processes 
operations research 
lovejoy 

survey algorithmic methods partially observed markov decision processes 
annals operations research 
luenberger 

linear nonlinear programming 
addisonwesley reading massachusetts 
luenberger 

dynamic systems theory models applications 
wiley new york 
madani condon hanks 

undecidability probabilistic planning infinite horizon partially observable markov decision problems 
proceedings sixteenth national conference artificial intelligence orlando fl 
appear 
mahadevan 

discount discount reinforcement learning case study comparing learning learning 
proceedings eleventh international conference machine learning pp 
new brunswick nj 
mcallester rosenblitt 

systematic nonlinear planning 
proceedings ninth national conference artificial intelligence pp 
anaheim ca 
mccallum 

instance utile distinctions reinforcement learning hidden state 
proceedings twelfth international conference machine learning pp 
lake tahoe nevada 
mccarthy hayes 

philosophical problems standpoint artificial intelligence 
machine intelligence 
decision theoretic planning structural assumptions meuleau hauskrecht kim peshkin kaelbling dean boutilier 

solving large weakly coupled markov decision processes 
proceedings fifteenth national conference artificial intelligence pp 
madison wi 
moore atkeson 

parti game algorithm variable resolution reinforcement learning multidimensional state spaces 
machine learning 
papadimitriou tsitsiklis 

complexity markov chain decision processes 
mathematics operations research 
parr 

flexible decomposition algorithms weakly coupled markov decision processes 
proceedings fourteenth conference uncertainty artificial intelligence pp 
madison wi 
parr russell 

approximating optimal policies partially observable stochastic domains 
proceedings fourteenth international joint conference artificial intelligence pp 
montreal 
parr russell 

reinforcement learning hierarchies machines 
jordan kearns solla 
eds advances neural information processing systems pp 

mit press cambridge 
pearl 

probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann san mateo 
pednault 

adl exploring middle ground strips situation calculus 
proceedings international conference principles knowledge representation reasoning pp 
toronto canada 
penberthy weld 

ucpop sound complete partial order planner adl 
proceedings third international conference principles knowledge representation reasoning pp 
boston ma 
peot smith 

conditional nonlinear planning 
proceedings international conference ai planning systems pp 
college park md perez carbonell 

control knowledge improve plan quality 
proceedings second international conference ai planning systems pp 
chicago il 
poole 

exploiting rule structure decision making independent choice logic 
proceedings eleventh conference uncertainty artificial intelligence pp 
montreal canada 
poole 

independent choice logic modelling multiple agents uncertainty 
artificial intelligence 
boutilier dean hanks poole 

probabilistic partial evaluation exploiting rule structure probabilistic inference 
proceedings fifteenth international joint conference artificial intelligence pp 
nagoya japan 
poole 

context specific approximation probabilistic inference 
proceedings fourteenth conference uncertainty artificial intelligence pp 
madison wi 
precup sutton singh 

theoretical results reinforcement learning temporally behaviors 
proceedings tenth european conference machine learning pp 
chemnitz germany 
pryor collins 

cassandra planning contingencies 
technical report northwestern university institute learning sciences 
puterman 

markov decision processes 
john wiley sons new york 
puterman shin 

modified policy iteration algorithms discounted markov decision problems 
management science 
ross varadarajan 

markov decision processes sample path constraint decomposition approach 
mathematics operations research 
russell norvig 

artificial intelligence modern approach 
prentice hall englewood cliffs nj 
sacerdoti 

planning hierarchy abstraction spaces 
artificial intelligence 
sacerdoti 

nonlinear nature plans 
proceedings fourth international joint conference artificial intelligence pp 

schoppers 

universal plans reactive robots unpredictable environments 
proceedings tenth international joint conference artificial intelligence pp 
milan italy 
schwartz 

reinforcement learning method maximizing undiscounted rewards 
proceedings tenth international conference machine learning pp 
amherst ma 
schweitzer puterman 

iterative procedures discounted semi markov reward processes 
operations research 
shachter 

evaluating influence diagrams 
operations research 
shimony 

role relevance explanation irrelevance statistical independence 
international journal approximate reasoning 
decision theoretic planning structural assumptions simmons koenig 

probabilistic robot navigation partially observable environments 
proceedings fourteenth international joint conference artificial intelligence pp 
montreal canada 
singh cohn 

dynamically merge markov decision processes 
advances neural information processing systems pp 

mit press cambridge 
singh jaakkola jordan 

reinforcement learning soft state aggregation 
hanson cowan giles 
eds advances neural information processing systems 
morgan kaufmann san mateo 
smallwood sondik 

optimal control partially observable markov processes finite horizon 
operations research 
smith peot 

postponing threats partial order planning 
proceedings eleventh national conference artificial intelligence pp 
washington dc 
sondik 

optimal control partially observable markov processes infinite horizon discounted costs 
operations research 
stone veloso 

team partitioned opaque transition reinforcement learning 
asada 
ed robocup robot soccer world cup ii 
springer verlag berlin 
sutton 

td models modeling world mixture time scales 
proceedings twelfth international conference machine learning pp 
lake tahoe nv 
sutton barto 

reinforcement learning 
mit press cambridge ma 
russell 

control strategies stochastic planner 
proceedings twelfth national conference artificial intelligence pp 
seattle wa 
shachter 

dynamic programming influence diagrams 
ieee transactions systems man cybernetics 
tesauro 

td gammon self teaching backgammon program achieves play 
neural computation 
thrun fox burgard 

probabilistic approach concurrent mapping localization mobile robots 
machine learning 
thrun schwartz 

finding structure reinforcement learning 
tesauro touretzky leen 
eds advances neural information processing systems cambridge ma 
mit press 
warren 

generating conditional plans programs 
proceedings aisb summer conference pp 
university edinburgh 
boutilier dean hanks watkins dayan 

learning 
machine learning 
weld 

commitment planning 
ai magazine winter 
white iii scherer 

solutions procedures partially observed markov decision processes 
operations research 
williamson 

value directed approach planning 
ph thesis university washington department computer science engineering 
williamson hanks 

optimal planning goal directed utility model 
proceedings second international conference ai planning systems pp 
chicago il 
winston 

artificial intelligence third edition 
addison wesley reading massachusetts 
yang 

intelligent planning decomposition abstraction approach 
springer verlag 
zhang liu 

model approximation scheme planning partially observable stochastic domains 
journal artificial intelligence research 
zhang poole 

exploiting causal independence bayesian network inference 
journal artificial intelligence research 

