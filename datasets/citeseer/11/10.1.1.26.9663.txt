kluwer academic publishers boston 
manufactured netherlands 
automatic construction decision trees data multi disciplinary survey sreerama murthy murthy scr siemens com siemens corporate research princeton nj usa 
decision trees proved valuable tools description classi cation generalization data 
constructing decision trees data exists multiple disciplines statistics pattern recognition decision theory signal processing machine learning arti cial neural networks 
researchers disciplines working quite di erent problems identi ed similar issues heuristics decision tree construction 
surveys existing decision tree construction attempting identify important issues involved directions taken current state art 
keywords classi cation tree structured classi ers data compaction 
advances data collection methods storage processing technology providing unique challenge opportunity automated data exploration techniques 
enormous amounts data collected daily major scienti projects human genome project hubble space telescope geographical information systems stocks trading hospital information systems computerized sales records sources 
addition researchers practitioners diverse disciplines attempting automated methods analyze data 
quantity variety data available data exploration methods increases commensurate need robust cient versatile data exploration methods 
decision trees way represent rules underlying data hierarchical sequential structures recursively partition data 
decision tree data exploration ways description reduce volume data transforming compact form preserves essential characteristics provides accurate summary 
classi cation discovering data contains separated classes objects classes interpreted meaningfully context substantive theory 
generalization uncovering mapping independent dependent variables useful predicting value dependent variable 
automatic construction rules form decision trees attempted virtually disciplines data exploration methods developed 
sreerama murthy traditionally developed elds statistics engineering pattern recognition decision theory decision table programming 
renewed interest generated research arti cial intelligence machine learning neurosciences neural networks 
terminology emphases di er discipline discipline similarities methodology 
decision trees automatically constructed data successfully real world situations 
ectiveness compared widely automated data exploration methods human experts 
advantages decision tree classi cation pointed 
knowledge acquisition pre classi ed examples circumvents bottleneck acquiring knowledge domain expert 
tree methods exploratory opposed inferential 
nonparametric 
assumptions model data distribution trees model wide range data distributions 
hierarchical decomposition implies better available features computational ciency classi cation 
opposed statistical methods tree classi ers treat uni modal multi modal data fashion 
trees ease deterministic incomplete problems 
deterministic domains dependent variable determined perfectly independent variables incomplete problems 
trees perform classi cation sequence simple easy understand tests semantics intuitively clear domain experts 
decision tree formalism intuitively appealing 
reasons decision tree methodology provide important tool data mining researcher practitioner tool box 
fact existing data mining products constructing decision trees data 
order gain optimal bene existing methods develop improved algorithms crucial understanding existing subject 
existing decision tree lacks step step progress 
researchers system developers tried ad hoc variations basic methodology worked interesting 
due practice encounters instances redundant ort 
intent current point speci instances redundant careful reader may notice examples 
ad hoc nature obviously true decision trees 
counter example ross quinlan years 
progresses series carefully chosen steps advance understanding decision trees 
spite large body existing substantial practical success technique exist comprehensive multi disciplinary surveys results decision tree construction data 
see section discussion existing surveys 
current attempts ll gap 
summarize signi cant decision tree construction survey results related automatically constructing decision trees data elds pattern recognition statistics decision theory machine learning mathematical programming neural networks 
maintain conciseness survey guidelines limitations 
attempt tutorial overview speci topics 
main emphasis trace directions decision tree taken 
reason readers basic knowledge automatic decision tree construction methodology may bene survey readers completely new trees 
avoid repeating existing surveys 
partly surveys di erent emphases outlined section 
limit refereed journals published books conferences 
coverage decision tree applications falls far short comprehensive merely illustrative 
true coverage comparisons trees techniques 

outline survey overview brie outline motivate issues involved constructing decision trees 
issue mention corresponding section survey 
section aims establish structural organization large body existing literature trees 
terminology machine learning statistics 
alternative terminology may section 
greedy top construction commonly method tree growing today see section exceptions 
hierarchical model constructed top starting entire data partitioning subsets recursing partitioning procedure 
description tree growing reduces description techniques splitting data meaningful subsets 
section reviews dozens splitting rules proposed literature classi cation comparitive evaluations 
section covers detail multivariate splitting rules 
model intended description classi cation generalization better data capturing true characteristics data noise randomness 
context trees concern translates problem nding right sized trees 
techniques nd right sized trees including pruning evaluations comparisons topic section 
tree describe data set perfectly need metrics quantify goodness trees 
tree quality measures proposed literature summarized section 
sreerama murthy sample size versus dimensionality data set greatly uences quality trees constructed 
analysing uence reviewed section 
section covers methods preprocess data inducing trees feature subset selection removing redundant correlated features composite feature construction data subsampling 
real world data complex imperfect 
variable costs associated di erent features classes missing feature values rule exception 
review dealing issues sections respectively 
shortcomings decision tree models solutions alleviate extensively reported literature 
greedy splitting heuristics cient adequate applications essentially suboptimal 
situations processing resources important result ways improving greedy induction exist section 
crisp decisions decision trees usually output may adequate useful settings 
techniques tree models probability estimators suggested section 
individual decision trees high variance terms generalization accuracy authors suggested combining results multiple decision trees section 
trees cause data fragmentation reduces probabilistic signi cance near leaf nodes 
solution soft splits section 
discuss miscellaneous aspects tree construction section including incremental tree construction section 
natural questions ask context tree construction possible build optimal trees exactly speci algorithm researchers theoretically empirically analyzed tree construction methodology 
section reviews detail covering np completeness results analyses biases tree induction 
section devoted practical promise decision trees 
discuss real world applications available software packages comparisons alternative data analysis techniques establish decision trees versatile ective data analysis tools 
binning interestingly brings paucity decision trees 
dividing model construction individual subtasks risk losing track ofthe purpose exercise 
apparent improvements individual steps guaranteed lead better algorithms 
splitting rules example 
splitting rules de ned evaluated improved broader context tree construction method 
reduced mere ad hoc greedy heuristics 
surprising existing splitting rules functionally equivalent 
author acknowledges shortcoming organization 
papers dealing topic listed multiple times mention omitted decision tree construction survey places 
example relevance issues address referenced repeatedly sections 
section introduces brie basic concepts involved decision tree construction 
section discusses alternative terminology 
section summarizes high level pointers mentioning existing surveys text books historical origins 
sections comprise survey organization described detail 
section concludes general comments 

basics decision trees readers completely unfamiliar decision trees refer section ii summary basic de nitions 
decision tree constructed training set consists objects 
object completely described set attributes class label 
attributes ordered real unordered boolean values 
concept underlying data set true mapping attributes class 
noise free training set objects generated underlying concept 
decision tree contains zero internal nodes leaf nodes 
internal nodes child nodes 
internal nodes contain splits test value expression attributes 
arcs internal node children labeled distinct outcomes test leaf node class label associated 
task constructing tree training set called tree induction tree building tree growing 
existing tree induction systems proceed greedy top fashion 
section lists exceptions 
starting empty tree entire training set variant algorithm applied splits possible 

training examples current belong category create leaf node class 
score set possible splits goodness measure 

choose best split test current node 

create child nodes distinct outcomes label edges parent child nodes outcomes partition training data child nodes 

node said pure training samples belong class 
repeat previous steps impure child nodes 
discrimination process deriving classi cation rules samples objects classi cation applying rules new objects unknown class decision trees discrimination classi cation 
object classi ed passing tree starting root node 
test internal node path applied attributes sreerama murthy determine arc go 
label leaf node ends output classi cation 
object misclassi ed tree classi cation output tree object correct class label 
proportion objects correctly classi ed decision tree known accuracy proportion misclassi ed objects error 

terminology structures similar decision trees called classi cation trees branched testing sequences discriminant trees identi cation keys 
training sets consist objects known samples observations examples instances 
attributes referred features predictors independent variables 
ordered attribute space decision tree imposes partitioning geometrically represented collection hyper surfaces regions 
decision trees uses speci type surface hyper planes 
exceptions see neural trees methods paragraphs section reason splits referred hyper planes attributes dimensions objects points 
category dependent variable class label 
ordered domains equivalent comprise continuous integer real valued monotonous domains 
unordered domains categorical discrete free variables 
internal nodes non terminals test nodes 
leaf nodes referred terminal nodes decision nodes 
goodness measures known feature evaluation criteria feature selection criteria impurity measures splitting rules 

high level pointers decision tree performs multistage hierarchical decision making 
general rationale multistage classi cation schemes categorization schemes see 

origins decision tree induction statistics began due need exploring survey data 
statistical programs aid built binary segmentation trees aimed interactions predictor dependent variables 
pattern recognition decision trees motivated need interpret images remote sensing satellites landsat 
decision trees particular induction methods general arose machine learning avoid knowledge acquisition bottleneck expert systems 
sequential fault diagnosis inputs set possible tests associated costs set system states associated prior probabilities 
states fault free state states represent distinct faults 
aim decision tree construction survey build test algorithm unambiguously identi es occurrence system state tests minimizing total cost 
testing algorithms normally take form decision trees trees 
heuristics construct decision trees test sequencing 

surveys overview decision trees pattern recognition literature 
high level comparative perspective classi cation literature pattern recognition arti cial intelligence 
tree induction statistical perspective popularly today reviewed breiman excellent book classi cation regression trees 
review earlier statistical hierarchical classi cation see 
majority ofwork decision trees machine learning shoot breiman quinlan id algorithm 
quinlan book speci tree building program provides outline tree induction methodology machine learning perspective 
payne preece surveyed results constructing taxonomic identi cation keys attempted synthesis large widely dispersed literature elds biology pattern recognition decision table programming machine fault location coding theory questionnaire design 
taxonomic keys tree structures object leaf set available tests splits pre speci ed 
problem constructing identi cation keys problem constructing decision trees data common concerns exist optimal key construction choosing tests tree nodes 
moret provided tutorial overview representing boolean functions decision trees diagrams 
summarized results constructing decision trees discrete variable domains 
moret mentions pattern recognition constructing decision trees data primary emphasis 
landgrebe surveyed literature decision tree classi ers entirely pattern recognition perspective 
survey aim bringing disparate issues decision tree classi ers providing uni ed view casual users pitfalls method 
current di ers surveys ways 
substantial body done existing surveys written machine learning tree construction covered 
topics discussed existing surveys multivariate trees np completeness covered 
brings common organization decision tree multiple disciplines 
main emphasis automatically constructing decision trees parsimonious descriptions generalization data 
contrast example sreerama murthy main emphasis representing boolean functions decision trees 

covered years growing computational learning theory colt matters related decision tree induction 
cover little survey primarily due author ignorance 
proceedings annual colt conferences international conferences machine learning icml starting points explore 
papers get avor 
learning bayesian inference networks data closely related automatic decision tree construction 
increasing number papers topic similarities tree induction usually pointed 
discussion decision tree induction bayesian networks point view see 
literature learning bayesian networks see 
automatic construction hierarchical structures data dependent variable unknown unsupervised learning elds cluster analysis machine learning vector quantization covered 
hand constructed decision trees common medicine considered 
discuss regression trees 
rich body literature topic shares issues decision tree literature 
see 
discuss binary decision diagrams decision graphs 
discuss patents 

finding splits build decision tree necessary nd internal node test splitting data subsets 
case univariate trees nding split amounts nding attribute useful discriminating input data nding decision rule attribute 
case multivariate trees nding split seen nding composite feature combination existing attributes discriminatory power 
case basic task tree building rank features single composite usefulness discriminating classes data 

feature evaluation rules pattern recognition statistics literature features typically ranked feature evaluation rules single best feature feature subset chosen ranked list 
machine learning feature evaluation rules mainly picking single best feature node decision tree 
decision tree construction survey methods selecting subset features typically quite di erent 
postpone discussion feature subset selection methods section 
ben divides feature evaluation rules categories rules derived information theory rules derived distance measures rules derived dependence measures 
categories arbitrary distinct 
measures belonging di erent categories shown equivalent 
shown approximations 
rules derived information theory examples variety rules shannon entropy 
tree construction maximizing global mutual information expanding tree nodes contribute largest gain average mutual information tree explored pattern recognition 
tree construction locally optimizing information gain reduction entropy due splitting individual node explored pattern recognition sequential fault diagnosis machine learning 
mingers suggested statistic information theoretic measure close approximation distribution tree construction deciding 
de suggested attribute selection measure combined geometric distance information gain argued measures appropriate numeric attribute spaces 
rules derived distance measures distance refers distance class probability distributions 
feature evaluation criteria class measure separability divergence discrimination classes 
popular distance measure gini index diversity tree construction statistics pattern recognition sequential fault diagnosis 
breiman pointed gini index di culty relatively large number classes suggested rule remedy 
taylor silverman pointed gini index emphasizes equal sized spring purity children 
suggested splitting criterion called mean posterior improvement mpi emphasizes exclusivity spring class subsets 
bhattacharya distance kolmogorov distance statistic distance measures tree induction 
kolmogorov distance originally proposed tree induction class problems subsequently extended multiclass domains 
class separation metrics developed machine learning literature distance measures 
relatively simplistic method estimating class separation assumes values feature follow gaussian distribution class tree construction 
rules derived dependence measures measure statistical dependence random variables 
dependence measures interpreted belonging categories 
exist attribute selection criteria clearly belong category ben taxonomy 
combination mutual information measures 
rst measured sreerama murthy gain average mutual information due new split quanti ed probability gain due chance tables 
split minimized chosen methods 
permutation statistic univariate tree construction class problems 
main advantage statistic measures distribution independent number training instances 
seen section property provides natural measure tree growth 
measures activity attribute explored tree construction 
activity equal testing cost variable times priori probability tested 
computational requirements computing activity information measures 
quinlan rivest suggested minimum description length deciding splits prefer pruning 
pointed measures information gain gini index concave report worse goodness value trying split splitting natural way assessing expansion node 
remedy suggested upper bounds con dence intervals misclassi cation error attribute selection criterion 
total number misclassi ed points explored selection criterion authors 
examples heath sum minority inaccuracy 
cart book discuss measure tree induction 
additional tricks needed measure useful :10.1.1.44.6304
heath max minority maximum number misclassi ed points sides binary split sum impurities assigns integer class measures variance class numbers partition 
identical measure sum impurities earlier automatic interaction detection aid program 
feature evaluation criteria assume knowledge probability distribution training objects 
optimal decision rule tree node rule minimizes error probability considered assuming complete probabilistic information data known 
shang breiman argue trees built probability distributions turn inferred attribute values accurate trees built directly attribute values 
kak proposed method building multiattribute hash tables decision trees object localization detection 
decision trees built probability distributions attributes attribute values 
pal proposed variant id algorithm real data tests internal node genetic algorithms 

evaluations comparisons large number feature evaluation rules natural concern measure relative ectiveness constructing trees 
evaluations direction statistics pattern recognition machine learning predominantly empirical nature decision tree construction survey theoretical evaluations 
defer discussion section 
spite large number comparative studies far concluded particular feature evaluation rule signi cantly better 
majority studies concluded di erence di erent measures 
expected induction se rigorously justify performance unseen instances 
lot splitting rules similar functional perspective 
splitting rules essentially ad hoc heuristics evaluating strength dependence attributes class 
comparisons individual methods may interesting reader metric situations 
baker jain reported experiments comparing eleven feature evaluation criteria concluded feature rankings induced various rules similar 
feature evaluation criteria including shannon entropy divergence measures compared simulated data sequential multi class classi cation problem 
feature selection rule consistently superior speci strategy alternating di erent rules signi cantly ective 
breiman conjectured decision tree design insensitive large class splitting rules stopping rule crucial 
mingers compared attribute selection criteria concluded tree quality doesn depend speci criterion 
claimed random attribute selection criteria measures information gain 
claim refuted authors argued random attribute selection criteria prone tting fail noisy attributes 
compared activity measures loss analytically empirically 
showed chose non essential variables tree nodes produce trees th size trees produced loss 
fayyad irani showed measure sep performs better gini index information gain speci types problems 
researchers pointed information gain biased attributes large number possible values 
mingers compared information gain statistic growing tree splitting 
concluded corrected information gain bias multivalued attributes extent chosen produced trees extremely deep hard interpret 
quinlan suggested gain ratio remedy bias information gain 
mantaras argued gain ratio set problems suggested information theory distance partitions tree construction 
formally proved measure biased multiple valued attributes 
white liu experiments conclude information gain gain ratio mantaras measure worse statistical measure terms bias multiple valued attributes 
hyper geometric function proposed sreerama murthy means avoid biases information gain gain ratio metrics martin 
martin proposed examined alternatives quinlan measures including distance orthogonality beta function chi squared tests 
di erent martin proved time complexity induction post processing exponential tree height worst case fairly general conditions average case 
puts premium designs tend produce shallower trees multi way binary splits selection criteria prefer balanced splits 
kononenko pointed minimum description length feature evaluation criteria bias multi valued attributes 

multivariate splits decision trees popularly univariate splits single attribute internal node 
methods developed literature constructing multivariate trees body known 
multivariate splits considers linear oblique trees 
trees tests linear combination attributes internal nodes 
problem nding optimal linear split optimal respect feature evaluation measures section di cult nding optimal univariate split 
fact nding optimal linear splits known intractable feature evaluation rules see section heuristic methods required nding albeit suboptimal linear splits 
methods literature nding linear tests include linear discriminant analysis hill climbing search linear programming perceptron training 
linear discriminant trees authors considered problem constructing tree structured classi ers linear discriminants node 
fu linear discriminant node decision tree computing hyper plane coe cients fletcher powell descent method 
method requires best set features node prespeci ed 
friedman reported applying fisher linear discriminants atomic features internal nodes useful building better trees 
qing yun fu describe method build linear discriminant trees 
method uses multivariate stepwise regression optimize structure decision tree choose subsets features linear discriminants 
linear discriminants considered loh 
variables stage appropriately chosen data type splits desired 
features tree building algorithm yields trees univariate linear combination linear combination polar coordinate splits allows ordered unordered variables linear split 
linear discriminants decision tree considered remote sensing literature 
method building linear discriminant classi cation trees decision tree construction survey user decide node classes need split described 
john considered linear discriminant trees machine learning literature 
extension linear discriminants linear machines linear structures discriminate multiple classes 
machine learning literature explored decision trees linear machines internal nodes 
locally opposed clusters objects sklansky students developed piecewise linear discriminants principle locally opposed clusters objects 
sklansky suggested procedure train linear split minimize error probability 
procedure sklansky developed system induce piece wise linear classi er 
method identi es closest opposed pairs clusters data trains linear discriminant locally 
nal classi er produced method piecewise linear decision surface tree 
discovered resubstitution error rate optimized piece wise linear classi ers nearly monotonic respect number features 
result sklansky suggest ective feature selection procedure linear splits uses zero integer programming 
park sklansky describe methods induce linear tree classi ers piece wise linear discriminants 
main idea methods nd hyper planes cut maximal number tomek links 
tomek links data set connect opposed pairs data points circle uence points doesn contain points 
hill climbing methods cart linear combinations attributes chapter known 
algorithm uses heuristic hill climbing backward feature elimination nd linear combinations node 
murthy described signi cant extensions cart linear combinations algorithm randomized techniques 
perceptron learning perceptron linear function neuron trained optimize sum distances misclassi ed objects convergent procedure adjusting coe cients 
perceptron trees decision trees perceptrons just leaf nodes discussed 
decision trees perceptrons internal nodes described 
mathematical programming linear programming building adaptive classi ers late 
possibly intersecting sets points duda hart proposed linear programming formulation nding split distance misclassi ed points minimized 
mangasarian bennett linear quadratic programming techniques build machine learning systems general decision trees particular 
zero integer programming designing vector quantizers 
brown employed linear programming nding optimal multivariate splits classi cation tree nodes 
papers attempt minimize distance misclassi ed points decision boundary 
sense methods similar perceptron training methods decision tree splitting criteria 
mangasarian sreerama murthy described linear programming formulation minimize number misclassi ed points geometric distance 
neural trees neural networks community researchers considered hybrid structures decision trees neural nets 
techniques developed neural networks structure automatically determined outcome interpreted decision trees nonlinear splits 
techniques similar tree construction information theoretic splitting criteria pruning neural tree construction 
examples include 
sethi described method converting univariate decision tree neural net retraining resulting tree structured entropy nets sigmoidal splits 
extension entropy nets converts linear decision trees neural nets described 
decision trees small multi layer networks node implementing nonlinear multivariate splits described 
jordan jacobs described hierarchical parametric classi ers small experts internal nodes :10.1.1.136.9119
training methods tree structured boltzmann machines described 
methods polynomial splits tree nodes explored decision theory 
machine learning method suggested manufacturing second higher degree features inducing linear splits complex features get non linear decision trees 
information theory gelfand describe method build tree structured lter linear processing elements internal nodes 
heath simulated annealing nd best oblique split tree node 
chai suggested genetic algorithms search linear splits non terminal nodes tree 
attempted bivariate trees trees functions variables tests internal nodes 
considered linear cuts corner cuts rectangular cuts ordered unordered variables 

ordered vs unordered attributes elds pattern recognition statistics historically considered ordered numeric attributes default 
natural considering application domains spectral analysis remote sensing 
elds special techniques developed accommodate discrete attributes primarily algorithms ordered attributes 
fast methods splitting multiple valued categorical variables described 
machine learning sub eld arti cial intelligence turn dominated symbolic processing tree induction methods originally developed categorical attributes 
problem incorporating continuous attributes algorithms considered subsequently 
problem meaningfully discretizing continuous dimension considered 
fast methods splitting continuous dimension ranges considered machine learning literature 
extension id decision tree construction survey distinguishes attributes unordered domains attributes linearly ordered domains suggested 
quinlan discussed improved ways continuous attributes 
obtaining right sized trees see aha survey simplifying decision trees detailed account motivation tree simpli cation existing solution approaches 
main di culties inducing recursive partitioning structure knowing 
obtaining right sized trees important reasons depend size classi cation problem 
moderate sized problems critical issues generalization accuracy honest error rate estimation gaining predictive generalization structure data 
large tree classi ers critical issue optimizing structural properties height balance 
breiman pointed tree quality depends stopping rules splitting rules 
ects noise generalization discussed 
tting avoidance speci bias studied :10.1.1.56.1756
ect noise classi cation tree construction methods studied pattern recognition literature 
techniques suggested obtaining right sized trees 
popular pruning discussion defer section 
alternatives pruning attempted 
restrictions minimum node size node split smaller objects parameter tree induction algorithm 
strategy isknown robust early methods 
stage search variant tree induction divided subtasks rst structure tree determined splits nodes 
optimization method rst stage may related second stage 
lin fu means clustering stages qing yun fu multi variate stepwise regression rst stage linear discriminant analysis second stage 
thresholds impurity method threshold imposed value splitting criterion splitting criterion falls threshold tree growth aborted 
thresholds imposed local individual node goodness measures global entire tree goodness 
alternative 
problem method value splitting criteria section varies size training sample 
imposing single threshold meaningful nodes tree easy may possible 
feature evaluation rules distribution depend number training samples goodness value signi cance tree suggested sreerama murthy literature 
martin hirschberg argue pre pruning simple pruning linear tree height contrasted exponential growth complex operations 
key factor uences simple pruning su ce split selection pruning heuristics unbiased 
trees rules conversion quinlan gave cient procedures converting decision tree set production rules 
simple heuristics generalize combine rules generated trees act substitute pruning quinlan univariate trees 
tree reduction cockett herrera suggested method reduce arbitrary binary decision tree irreducible form discrete decision theory principles 
irreducible tree optimal respect expected testing cost criterion tree reduction algorithm worst case complexity greedy tree induction methods 

pruning pruning method widely obtaining right sized trees proposed breiman 
chapter 
suggested procedure build complete tree tree splitting leaf node improve accuracy training data remove subtrees contributing signi cantly generalization accuracy 
argued method better splitting rules compensate extent suboptimality greedy tree induction 
instance node levels node splitting rule tree growth pruning may give high rating retain subtree 
kim koehler analytically investigate conditions pruning bene cial accuracy 
main result states pruning bene cial increasing skewness class distribution increasing sample size 
breiman pruning method cost complexity pruning weakest link pruning error complexity pruning proceeds stages 
rst stage sequence increasingly smaller trees built training data 
second stage trees chosen pruned tree classi cation accuracy pruning set 
pruning set portion training data set aside exclusively pruning 
separate pruning set fairly common practice 
pruning method needs separate data set quinlan reduced error pruning 
method cost complexity pruning build sequence trees claimed faster 
requirement independent pruning set problematic especially small training samples involved 
solutions suggested get problem 
breiman describe cross validation procedure avoids reserving part training data pruning large computa decision tree construction survey tional complexity 
quinlan pessimistic pruning away need separate pruning set statistical correlation test 
crawford analyzed breiman cross validation procedure pointed large variance especially small training samples 
suggested bootstrap method ective alternative 
gelfand claimed cross validation method ine cient possibly ine ective nding optimally pruned tree 
suggested cient iterative tree growing pruning algorithm guaranteed converge 
algorithm divides training sample halves iteratively grows tree half prunes half exchanging roles halves iteration 
quinlan rivest minimum description length tree construction pruning 
error coding method ect main pointed 
pruning method viewing decision tree encoding training data suggested forsyth 
dynamic programming prune trees optimally ciently explored 
studies done study relative ectiveness pruning methods 
just case splitting criteria single ad hoc pruning method superior 
choice pruning method depends factors size training set availability additional data pruning 

issues tree construction involves issues nding splits knowing recursive splitting 
section bundles issues 

sample size versus dimensionality relationship size training set dimensionality problem studied extensively pattern recognition literature 
researchers considered problem sample size vary dimensionality vice versa 
intuitively number samples number features samples attributes samples attributes induction di cult 
papers summarized informally follows nite sized data little priori information ratio sample size large possible suppress optimistically biased evaluations performance classi er 
sample size training classi er exists optimum feature size quantization complexity 
optimality terms tree size predictive accuracy 
quantization complexity refers number sreerama murthy ranges dimension split 
result true class problems multi class problems 
ratio sample size dimensionality vary inversely proportional amount knowledge class conditional densities 
tasks features optimal available decision tree quality isknown ected redundant irrelevant attributes 
avoid problem feature subset selection method section method form small set composite features section preprocessing step tree induction 
orthogonal step feature selection instance selection 
training sample large allow cient classi er induction subsample selection method section employed 

feature subset selection large body choosing relevant subsets features see texts 
developed context tree induction lot direct applicability 
components method attempts choose best subset features 
rst metric feature subsets compared determine better 
feature subsets compared literature direct error estimation feature evaluation criteria discussed section bhattacharya distance comparing subsets features 
direct error estimation similar wrapper approach advocates induction algorithm black box feature subset selection method 
second component feature subset selection methods search algorithm space possible feature subsets 
existing search procedures heuristic nature exhaustive search best feature subset typically prohibitively expensive 
exception optimal feature subset selection method zero integer programming suggested sklansky 
heuristic commonly greedy heuristic 
stepwise forward selection start empty feature set add stage best feature criterion 
stepwise backward elimination start full feature set remove step worst feature 
feature greedily added removed beam search said performed 
combination forward selection backward elimination bidirectional search attempted 
comparisons heuristic feature subset selection methods studies comparing feature evaluation criteria studies comparing pruning methods feature subset selection heuristic far superior 
showed heuristic sequential feature selection methods arbitrarily worse optimal strategy 
compared feature subset selection techniques empirically concluded technique uniformly superior 
surge interest feature subset selection methods machine learning community resulting empirical evaluations 
studies provide interesting insights decision tree construction survey increase ciency ectiveness heuristic search feature subsets 

composite features aim choose subset features nd composite features arithmetic logical combinations atomic features 
decision tree literature fu probably rst discuss features features generated original attributes 
friedman tree induction method consider equal ease atomic composite features 
techniques search multivariate splits section seen ways constructing composite features 
linear regression nd feature combinations explored 
discovery combinations boolean features tests tree nodes explored machine learning literature aswell signal processing 
rendell describe method constructs boolean features lookahead uses constructed feature combinations tests tree nodes 
lookahead construction boolean feature combinations considered 
linear threshold unit trees boolean functions described 
decision trees having rst order predicate calculus representations horn clauses tests internal nodes considered 

subsample selection feature subset selection attempts choose useful features 
similarly subsample selection attempts choose appropriate training samples induction 
quinlan suggested windowing random training set sampling method programs id 
initially randomly chosen window iteratively expanded include important training samples 
ways choosing representative samples nearest neighbor learning methods exist see examples 
techniques may helpful inducing decision trees large samples provided cient 
oates jensen analyzed ect training set size decision tree complexity 

incorporating costs real world domains attributes costs measurement objects misclassi cation costs 
measurement misclassi cation costs identical di erent attributes classes decision tree algorithms may need explicitly prefer cheaper trees 
attempts tree construction cost sensitive 
involve incorporating attribute measurement costs machine learning pattern recognition statistics incorporating misclassi cation costs 
methods incorporate attribute measurement costs typically include cost term feature evaluation criterion variable misclassi cation costs accounted prior probabilities cost matrices 
sreerama murthy 
missing attribute values real world data sets case attribute values missing data 
researchers addressed problem dealing missing attribute values training testing sets 
training data friedman suggested objects missing attribute values ignored forming split node 
feared discrimination information lost due ignoring missing values may substituted mean value particular feature training subsample question 
split formed objects missing values passed child nodes training testing stages 
classi cation object missing attribute values largest represented class union leaf nodes object ends 
breiman cart system implemented friedman suggestions 
quinlan considered problem missing attribute values 

improving greedy induction tree induction systems greedy approach trees induced top node time 
authors pointed inadequacy greedy induction di cult concepts 
problem inducing globally optimal decision trees addressed time 
early dynamic programming branch bound techniques convert decision tables optimal trees see 
tree construction partial exhaustive lookahead considered statistics pattern recognition tree structured vector quantizers bayesian class probability trees neural trees machine learning 
studies indicate lookahead cause considerable improvements greedy induction 
murthy salzberg demonstrate level lookahead help build signi cantly better trees worsen quality trees causing pathology 
seemingly unintuitive behavior caused way feature selection heuristics de ned greedy framework 
constructing optimal near optimal decision trees stage approach attempted authors 
rst stage su cient partitioning induced reasonably greedy method 
second stage tree re ned close optimal possible 
re nement techniques attempted include dynamic programming fuzzy logic search multi linear programming 
build re ne strategy seen search space possible decision trees starting greedily built suboptimal tree 
order escape local minima search space randomized search techniques genetic programming simulated annealing attempted 
methods search space decision trees random perturbations additions deletions splits 
deterministic hill climbing search procedure decision tree construction survey suggested searching optimal trees context sequential fault diagnosis 
discusses strategies algorithm improvements needed generate optimal classi cation trees 
inducing topologically minimal trees trees number occurrences attribute path minimized topic 
suen wang described algorithm attempted minimize entropy ofthe tree class overlap simultaneously 
class overlap measured number terminal nodes represent class 

estimating probabilities decision trees crisp decisions leaf nodes 
contrary class probability trees assign probability distribution classes terminal nodes 
breiman 
chapter proposed method building class probability trees 
quinlan discussed methods extracting probabilities decision trees 
buntine described bayesian methods building smoothing averaging class probability trees 
smoothing process adjusting probabilities node tree probabilities nodes path 
averaging improves probability estimates considering multiple trees 
smoothing context tree structured vector quantizers described 
approach re nes class probability estimates greedily induced decision tree local kernel density estimates suggested 
assignment probabilistic goodness splits decision tree described 
uni ed methodology combining uncertainties associated attributes test systematically propagated decision tree 

multiple trees known decision tree construction variance especially samples small features 
variance caused random choice training pruning samples equally attributes chosen node due cross validation reasons 
authors suggested collection decision trees just reduce variance classi cation performance 
idea build set correlated uncorrelated trees training sample combine results 
multiple trees built randomness di erent subsets attributes tree 
classi cation results trees combined simplistic voting methods statistical methods combining evidence 
relationship correlation errors individual classi ers error combined classi er explored 
alternative trees hybrid classi er uses small parts larger classi er 
brodley describes system automatically sreerama murthy selects suitable univariate decision tree linear discriminant instance classi er node hierarchical recursive classi er 

incremental tree induction tree induction algorithms batch training entire tree needs recomputed accommodate new training example 
crucial property neural network training methods incremental network weights continually adjusted accommodate training examples 
incremental induction decision trees considered authors 
friedman binary tree induction method adaptive features splits 
adaptive split depends training subsample splitting 
overly simple example adaptive split test mean value feature 
proposed incremental tree induction methods context univariate decision trees aswell multivariate trees 
crawford argues approaches attempt update tree best split updated sample taken node su er repeated restructuring 
occurs best split node widely sample node small 
incremental version cart uses signi cance thresholds avoid problem described 

soft splits common criticisms decision trees decisions lower levels tree increasingly smaller fragments data may probabilistic signi cance data fragmentation 
leaf nodes represent class unnecessarily large trees may result especially number classes large high class overlap 
researchers considered soft splits data decision trees 
hard split divides data mutually exclusive partitions 
soft split hand assigns probability point belongs partition allowing points belong multiple partitions 
uses simple form soft splitting 
soft splits pattern recognition literature 
jordan jacobs describe parametric hierarchical classi er soft splits :10.1.1.136.9119
multivariate regression trees soft splitting criteria considered 
induction fuzzy decision trees considered 

tree quality measures fact trees correctly represent data raises question decide tree better 
measures suggested quantify tree quality 
moret summarizes measures tree size expected testing cost worst case testing cost 
shows measures pairwise incompatible implies algorithm mini decision tree construction survey measure guaranteed minimize tree 
fayyad irani argue concentrating optimizing measure number leaf nodes achieve performance improvement measures 
generalization accuracy popular measure quantifying goodness learning systems 
accuracy tree computed testing set independent training set estimation techniques cross validation bootstrap 
fold cross validation generally believed honest assessment tree predictive quality 
kononenko bratko pointed comparisons basis classi cation accuracy unreliable di erent classi ers produce di erent types estimates produce output class probabilities accuracy values vary prior probabilities classes 
suggested information metric evaluate classi er remedy problems 
martin argued information theoretic measures classi er complexity practically computable severely restricted families classi ers suggested generalized version cart standard error rule means achieving tradeo classi er complexity accuracy 
description length number bits required code tree data compact encoding suggested means combine accuracy complexity classi er 
miscellaneous existing tree induction systems proceed greedy top fashion 
bottom induction trees considered 
bottom tree induction common problems building identi cation keys optimal test sequences 
hybrid approach tree construction combined top bottom induction 
concentrate decision trees constructed labeled examples 
problem learning trees decision rules examples addressed 
problem learning trees solely prior probability distributions considered 
learning decision trees qualitative causal models acquired domain experts topic 
trained network learned model craven algorithm trepan uses queries induce decision tree approximates function represented model 
attempts generalizing decision tree representation exist 
chou considered decision directed acyclic graphs class probability vectors leaves tests internal nodes 
option trees internal node holds optional tests respective subtrees discussed 
oliver suggested method build decision graphs similar chou decision minimum length encoding principles 
suggested se trees set enumeration structures embed decision trees 
cox argues classi cation tree technology implemented commercially available systems useful pattern recognition sreerama murthy decision support 
suggests ways modifying existing methods prescriptive descriptive 
interesting method displaying decision trees multidimensional data block diagrams proposed 
block diagrams point features data de ciencies classi cation method 
parallelization tree induction algorithms discussed detail 
hardware architectures implement decision trees described 

analyses researchers tried evaluate tree induction method precisely answer questions possible build optimal trees speci feature evaluation rule 
investigations theoretical empirical ones 

np completeness aspects optimal tree construction shown intractable 
rivest proved problem building optimal decision trees decision tables optimal sense minimizing expected number tests required classify unknown sample np complete 
sequential fault diagnosis cox showed arbitrary distribution attribute costs arbitrary distribution input vectors problem constructing minimum expected cost classi cation tree represent simple function linear threshold function np complete 
show problem identifying root node optimal strategy np hard 
problem building optimal trees decision tables considered murphy proved cases construction storage optimal trees np complete 
proved optimal decision tree construction decision tables np complete variety measures 
measures considered earlier papers np completeness appear subset measures 
problem constructing smallest decision tree best distinguishes characteristics multiple distinct groups shown np complete 
comer sethi studied asymptotic complexity trie index construction document retrieval literature 
megiddo investigated problem polyhedral separability separating sets points hyper planes proved variants problem np complete 
results papers throw light complexity decision tree induction 
lin discussed np hardness problem designing optimal pruned tree structured vector quantizers 
results consider univariate decision tree construction 
intuitively linear multivariate tree construction di cult univariate tree construction larger space splits searched 
heath proved problem nding split minimizes number misclassi ed points sets mutually exclusive points np complete 
decision tree construction survey hoe gen proved general problem np hard proved problem nding hyper plane misclassi es opt examples opt minimum number misclassi cations possible hyper plane np hard 
problem nding single linear split np hard surprise problem building optimal linear decision trees np hard 
hope reducing size decision tree dimensionality data possible problem tractable 
case blum rivest showed problem constructing optimal node neural network np complete 
goodrich proved optimal smallest linear decision tree construction np complete dimensions 

theoretical insights goodman smyth showed greedy top induction decision trees directly equivalent form shannon fano pre coding 
consequence result top tree induction mutual information necessarily suboptimal terms average tree depth 
trees maximal size generated cart algorithm shown error rate bounded twice bayes error rate asymptotically bayes optimal 
considered problem converting decision tables optimal trees studied properties optimal variables class attributes members root optimal tree 
eades staples showed optimality search trees terms worst case depth closely related regularity 
irregular trees optimal splitting rules section tend slice small corners attribute space building highly unbalanced trees nd optimal trees 
authors pointed similarity equivalence problem constructing decision trees existing seemingly unrelated problems 
insights provide valuable tools analyzing decision trees 
wang suen show entropy reduction point view powerful theoretically bounding search depth classi cation error 
chou gray view decision trees variablelength encoder decoder pairs show rate equivalent tree depth distortion probability misclassi cation 
suggested universal technique lower bound size characteristics decision trees arbitrary boolean functions 
technique power spectrum coe cients dimensional fourier transform function 
zhao proved equivalence pseudo boolean analysis id algorithm 

assumptions biases tree induction methods heuristic nature 
assumptions biases hoping heuristics produce trees 
authors sreerama murthy attempted evaluate validity relevance assumptions biases tree induction 
assumption multi stage classi ers may accurate single stage 
analysis data fragmentation caused stage hierarchical classi ers may compensate gain accuracy 
michie argues topdown induction algorithms may provide overly complex classi ers real conceptual structure encoding relevant knowledge 
solution problem gray suggested induction method generates single disjuncts conjuncts rule time complexity tree induction 
cacy multi level decision trees compared holte simple level classi cation rules 
concluded real world data sets commonly machine learning community decision trees perform signi cantly better level rules 
refuted elomaa grounds 
elomaa argued holte observations may peculiarities data slight di erences accuracy holte observed signi cant 
bias smaller consistent decision trees higher generalization accuracy larger consistent trees occam razor 
analysis murphy pazzani empirically investigated truth bias 
experiments indicate conjecture true 
experiments indicate smallest decision trees typically lesser generalization accuracy trees slightly larger 
extension study murphy evaluated size bias function concept size 
concluded bias smaller trees generally bene cial terms accuracy larger trees perform better smaller ones high complexity concepts better guess correct size randomly pre speci ed size bias 
assumption locally optimizing information distance splitting criteria section tends produce small shallow accurate trees 
analysis class binary splits data set said complete informally partition data exists member induces partition 
zimmerman considered problem building identi cation keys complete classes splits arbitrary class distributions 
garey graham analyze properties recursive greedy splitting quality trees induced decision tables showed greedy algorithms information theoretic splitting criteria perform arbitrarily worse optimal 
showed globally optimum performance decisions node emphasize decision leads greater joint probability correct classi cation level decisions di erent nodes tree independent 
loveland analyzed performance variants gini index context sequential fault diagnosis 
goodman smyth analyzed greedy tree induction information theoretic view point 
proved mutual information induction equivalent form shannon fano pre coding insight argued greedily induced trees nearly optimal terms depth 
conjecture substantiated empirically shown expected depth decision tree construction survey trees greedily induced information gain gini index isvery close optimal variety experimental conditions 
relationship feature evaluation shannon entropy probability error investigated 

practical promise discussion far concentrated techniques analysis decision tree construction 
vain technique practically useful outperforms competitive techniques 
section address issues 
argue decision trees practically useful technique examples diverse real world applications 
brie discuss existing software packages building decision trees data 
summarize comparing decision trees alternative techniques data analysis neural networks nearest neighbor methods regression analysis 

selected real world applications section lists real world applications decision trees 
aim give reader feel versatility usefulness decision tree methods data exploration useful readers interested nding potential tree classi ers speci domains 
coverage applications necessity limited 
application papers cited published refereed journals ph theses 
restrict application domains domain scientists tried decision trees decision tree researchers tested algorithm application domains 
application areas listed alphabetical order 
agriculture application range machine learning methods including decision trees problems agriculture described 
astronomy astronomy active domain automated classi cation techniques 
decision trees reported ltering noise hubble space telescope images star galaxy classi cation determining galaxy counts discovering second sky survey 
biomedical engineering identifying features devices 
control systems control nonlinear dynamical systems control plants 
financial analysis asserting attractiveness buy writes data mining applications 
sreerama murthy image processing interpretation digital images radiology recognizing objects high level vision outdoor image segmentation 
language processing medical text classi cation acquiring statistical parser set parsed sentences 
law discovering knowledge international con ict con ict management databases possible avoidance termination crises wars 
manufacturing production non destructively test welding quality semiconductor manufacturing increasing productivity material procurement method selection accelerate printing process optimization electro chemical machining schedule printed circuit board assembly lines uncover aws boeing manufacturing process quality control 
review machine learning decision trees techniques scheduling see 
medicine medical research practice long important areas application decision tree techniques 
uses automatic induction decision trees cardiology study tooth psychiatry detecting cations mammography analyze sudden infant death sid syndrome diagnosing thyroid disorders 
molecular biology initiatives human genome project genbank database er fascinating opportunities machine learning data exploration methods molecular biology 
decision trees analyzing amino acid sequences 
pharmacology tree classi cation drug analysis 
physics detection physical particles 
plant diseases assess hazard mortality pine trees 
power systems power system security assessment power stability prediction 
remote sensing remote sensing strong application area pattern recognition decision trees see 
uses tree classi cation remote sensing 
software development estimate development ort software module 
decision trees building personal learning assistants classifying sleep signals 
decision tree construction survey 
software packages today research codes commercial products purpose constructing decision trees data 
addition decision tree construction primary function provided general purpose data mining tool suites 
interest survey decision tree software tools 
list current software section knowledge discovery nuggets web page www com html 
addition decision tree entries listed software suites classi cation multiple approaches relevant 
available decision tree software varies terms speci algorithms implemented sophistication auxiliary functions visualization data formats supported speed 
web page just lists decision tree software packages 
evaluate 
objective comparative evaluation decision tree software terms available functionality programmability ciency user friendliness visualization support database interface price interesting relevant necessarily easy straightforward exercise 
author unaware existing comparisons 
important point single available software program implements known decision trees 
package chooses favorite algorithms heuristics implement 
choices seen shortcomings packages implementing known signi cant task mayhave primarily research value 

trees versus data analysis methods section section comprehensive merely illustrative 
provide pointers compared decision trees competing techniques data analysis statistics machine learning 
brown compared back propagation neural networks decision trees problems known multi modal 
analysis indicated di erence methods method performed vanilla state 
performance decision trees improved study multivariate splits back propagation networks better feature selection 
comparisons symbolic connectionist methods 
multi layer perceptrons cart linear combinations compared nd di erence accuracy 
similar reached id back propagation compared 
compared classi cation trees neural networks analyzing ecg concluded technique superior 
contrast id slightly better connectionist bayesian methods 
compared stepwise linear discriminant analysis stepwise logistic regression cart senior predicting patient die year discharged acute myocardial sreerama murthy 
results showed di erence physicians computers terms prediction accuracy 
van compared statistical multivariate methods heuristic decision tree methods domain ecg analysis 
comparisons show decision tree classi ers comprehensible exible incorporate change existing categories 
comparisons cart linear regression discriminant analysis argued cart suitable methods noisy domains lots missing values 
comparisons decision trees statistical methods linear discriminant function analysis automatic interaction detection aid argued machine learning methods outperform statistical methods ignored 
feng comparison machine learning methods including decision trees neural networks statistical classi ers part european statlog project 
statlog project initiated european commission comparative testing statistical logical learning algorithms large scale applications classi cation prediction control 
feng main method uniformly superior machine learning methods superior multimodal distributions statistical methods computationally cient 
thrun compared learning algorithms simulated monk problems 
long compared quinlan logistic regression problem diagnosing acute cardiac concluded methods came fairly close expertise physicians 
experiments logistic regression outperformed 
mingers compare decision trees neural networks discriminant analysis real world data sets 
comparisons reveal linear discriminant analysis fastest methods underlying assumptions met decision trees methods presence noise 
dietterich argue inadequacy trees certain domains may due fact trees unable take account statistical information available methods neural networks 
show decision trees perform signi cantly better text speech conversion problem extra statistical knowledge provided 
jackson compare expert system developed traditional knowledge engineering methods quinlan id domain 
quinlan empirically compared decision trees genetic classi ers neural networks 
gordon compared decision tables decision trees decision rules determine formalism best decision analysis 
methods learning examples compared early study dietterich michalski 

attempted multi disciplinary survey automatically constructing decision trees data 
pointers elds pattern decision tree construction survey recognition statistics decision theory machine learning mathematical programming neural networks 
attempted provide concise description directions decision tree taken years 
goal provide overview existing decision trees taste usefulness newcomers practitioners eld data mining knowledge discovery 
hope overviews help avoid redundant ad hoc ort researchers system developers 
hierarchical recursive tree construction methodology powerful repeatedly shown useful diverse real world problems 
simple intuitively appealing 
simplicity methodology lead practitioner take slack attitude decision trees 
just case statistical methods neural networks building successful tree classi er application requires thorough understanding problem deep knowledge tree methodology 
acknowledgments simon kasif rst pointed multi disciplinary survey decision trees worthwhile exercise undertake 
simon steven salzberg lewis stiller reading commenting manuscript 
am grateful wray buntine writing great review helped improve 
notes 
adapted similar suggested general framework searching structure data 

earlier data mining products old machine learning methods just new titles 

considered trees internal nodes just child 
nodes data split residuals taken single variable regression 

converting decision tables trees common leaf nodes decision label 
decision tables classi cation 

decision tree said perform classi cation class labels discrete values regression class labels continuous 
restrict entirely classi cation trees 

interesting early patent decision tree growing assigned ibm patent 

desirable properties measure entropy include symmetry additivity 
shannon entropy possesses properties 
insightful treatment reduction common theme underlying pattern recognition problems see 

goodman smyth report idea mutual information features classes select best feature originally put forward lewis 

named italian economist gini quinlan uses naive version con dence intervals doing pessimistic pruning 
sreerama murthy scha er stated proved conservation theorem states essentially positive performance learning situations set equal degree negative performance 
clarify non intuitive consequences conservation theorem scha er gave example concept information loss gives better generalization accuracy information gain 
scha er draws heavily wolpert earlier results 
trees node children considered vector quantization literature 
techniques start su cient partitioning optimize structure thought converse approach 
bootstrapping independent learning samples size created random sampling replacement original learning sample cross validation divided randomly mutually exclusive equal sized partitions 
efron showed cross true result bootstrap variance especially small samples 
exist arguments cross validation clearly preferable bootstrap practice 
van argues increasing amount information measurement subset enlarging size complexity worsens error probability truly bayesian classi er 
guarantee cost complexity due additional measurements may worth slight improvement accuracy 
real world classi ers truly bayesian 
lot exists neural networks literature committees ensembles networks improve classi cation performance 
see example 
regular tree tree nodes children child internal node leaf children 
tree regular regular argued empirically variance decision tree methods reason bias poor performance domains 
general description modern classi cation problems astronomy prompt pattern recognition machine learning techniques see 
considerable ongoing discussion exists appropriateness internet scholarly publications 
critics argue assume availability internet www readership relative permanence continued correctness referenced articles 
acknowledging merits criticism resort referencing web site 
partly reasonable survey decision tree software tools involved long relatively brief life span evolving nature market 

aaai 
aaai proc 
tenth national conf 
arti cial intelligence san jose ca th july 
aaai press mit press 

aaai 
aaai proc 
eleventh national conf 
arti cial intelligence washington dc th july 
aaai press mit press 

aaai 
aaai proc 
twelfth national conf 
arti cial intelligence volume seattle wa st july th august 
aaai press mit press 

aczel 
measures information characterizations 
academic pub new york 

david aha richard bankert 
comparitive evaluation sequential feature selection algorithms 
ai statistics pages 
ai stats preliminary papers fourth int 
workshop arti cial intelligence statistics ft lauderdale fl rd th january 
society ai statistics 

ai stats preliminary papers fifth int 
workshop arti cial intelligence statistics ft lauderdale fl th january 
society ai statistics 

aldrich schmitz 
machine learning strategies control plants 
control eng 
practice february 
decision tree construction survey 
kamal ali michael pazzani 
link error correlation error reduction decision tree ensembles 
technical report ics tr university california irvine department information computer science september 

hussein almuallim thomas dietterich 
learning boolean concepts presence irrelevant features 
arti cial intelligence 

peter roland chin paul 
automated approach design decision tree classi ers 
ieee trans pattern analysis machine intelligence pami january 

les atlas ronald cole alan lipman jerome connor dong park el robert marks ii 
performance comparison trained multilayer perceptrons trained classi cation trees 
proc 
ieee 

siddhartha bhattacharya gary koehler jane 
review machine learning scheduling 
ieee trans 
eng 
management may 

bahl brown de souza mercer 
tree statistical language model natural language speech recognition 
ieee trans 
speech signal processing 

baker jain 
feature ordering practice nite sample ects 
proc 
third int 
joint conf 
pattern recognition pages san diego ca 

baker david hodges jr ross 
classi cation regression tree analysis assessing hazard pine mortality caused hetero 
plant disease february 


matching prediction principle biological classi cation 
applied statistics 

moshe ben 
myopic policies sequential classi cation 
ieee trans 
computing february 

moshe ben 
distance measures information measures error bounds feature evaluation 
krishnaiah kanal pages 

bennett mangasarian 
robust linear programming discrimination oftwo linearly inseparable sets 
optimization methods software 

bennett mangasarian 
multicategory discrimination linear programming 
optimization methods software 

kristin bennett 
decision tree construction linear programming 
inproc 
th midwest arti cial intelligence cognitive science society conf pages 

kristin bennett 
global tree optimization non greedy decision tree algorithm 
proc 
interface th symposium interface research triangle north carolina 

blum rivest 
training node neural network np complete 
proc 
workshop computational learning theory pages boston ma 
morgan kaufmann 

marko ivan bratko 
trading accuracy simplicity decision trees 
machine learning 

david chao debra 
comparison binary decision trees neural networks top detection 
physical review particles fields march 

boyce 
optimal subset selection 
springer verlag 

anna gregor henry davis 
statistical learning accurate heuristics 
ijcai pages 
editor bajcsy 

hennessy 
spectral lower bound technique size decision trees level circuits 
ieee trans 
comp february 

leo breiman 
bagging predictors 
technical report department statistics univ california berkeley ca 

leo breiman jerome friedman richard olshen charles stone 
classi cation regression trees 
wadsworth int 
group 
sreerama murthy 
richard brent 
fast training algorithms multilayer neural nets 
ieee trans 
neural networks may 

leonard david aha 
simplifying decision trees survey 
technical report aic navy center applied research arti cial intelligence naval research lab washington dc 
aha aic nrl navy mil 

carla brodley 
recursive automatic algorithm selection inductive learning 
phd thesis univ massachusetts amherst ma 

carla brodley paul multivariate decision trees 
machine learning 

donald brown vincent louis 
comparison decision tree classi ers backpropagation neural networks multimodal classi cation problems 
pattern recognition 

donald brown louis 
classi cation trees optimal multivariate splits 
proc 
int 
conf 
systems man cybernetics volume pages le france th october 
ieee new york 

bucy 
decision tree design simulated annealing 
mathematical numerical analysis 
rairo 
bullock wang fairchild patterson 
automated training morphology algorithm object recognition 
proc 
spie int 
society optical eng 
issue title automatic object recognition iv 

bruce draper 
non parametric classi cation pixels varying illumination 
spie int 
society optical eng november 

buntine niblett 
comparison splitting rules decision tree induction 
machine learning 

buntine 
decision tree induction systems bayesian analysis 
kanal levitt lemmer editors uncertainty arti cial intelligence 
elsevier science publishers amsterdam 

wray buntine 
theory learning classi cation rules 
phd thesis univ technology sydney australia 

wray buntine 
learning classi cation trees 
statistics computing 

wray buntine 
guide literature learning probabilistic networks data 
ieee trans 
knowledge data engineering 

janice callahan stephen sorensen 
rule induction group decisions statistical data example 
operational research society march 

jan van 
topics measurement selection 
krishnaiah kanal pages 

rich caruana dayne freitag 
greedy attribute selection 
ml pages 
editors william cohen haym hirsh 

richard casey george nagy 
decision tree design probabilistic model 
ieee trans 
information theory january 

jason catlett 

phd thesis basser department computer science univ sydney australia 

jason catlett 
tailoring rulesets misclassi cation costs 
ai statistics pages 

bing bing chai zhuang zhao jack sklansky 
binary linear decision tree genetic algorithm 
proc 
th int 
conf 
pattern recognition 
ieee computer society press los alamitos ca 

chandrasekaran 
numbers symbols knowledge structures pattern recognition arti cial intelligence perspectives classi cation task 
volume pages 
elsevier science amsterdam netherlands 

chandrasekaran jain 
quantization complexity independent measurements 
ieee trans 
comp january 

chaudhuri lo loh yang 
generalized regression trees 
statistica sinica 
decision tree construction survey 
philip chou 
applications information theory pattern recognition design decision trees 
phd thesis stanford univ 

philip chou 
optimal partitioning classi cation regression trees 
ieee trans 
pattern analysis machine intelligence april 

philip chou robert gray 
decision trees pattern recognition 
proc 
ieee symposium information theory page ann arbor mi 

krzysztof cios ning liu 
machine learning method generation neural network architecture continuous id algorithm 
ieee trans 
neural networks march 


cid extension id attributes ordered domains 
south african computer march 

cockett herrera 
decision tree reduction 
acm october 

cohen 
cient pruning methods separate conquer rule learning systems 
ijcai pages 
editor bajcsy 

douglas comer ravi sethi 
complexity trie index construction 
acm july 

cover van 
possible orderings measurement selection problems 
ieee trans 
systems man cybernetics smc 

louis anthony cox 
causal knowledge learn useful decision rules data 
ai statistics pages 

louis anthony cox qiu 
minimizing expected costs classifying patterns sequential costly inspections 
ai statistics 

louis anthony cox qiu warren 
heuristic cost computation discrete classi cation functions uncertain argument values 
annals operations research 

mark craven 
extracting comprehensible models trained neural networks 
technical report cs tr university wisconsin madison september 

stuart crawford 
extensions cart algorithm 
int 
man machine studies august 

stephen john mingers 
neural networks decision tree induction discriminant analysis empirical comparison 
operational research society april 

matcher 
statistical decision tree tool studying eeg ects cns active drugs 


florence buc didier jean pierre nadal 
trio learning new strategy building hybrid neural trees 
int 
neural systems december 

das 
decision tree approach selecting demand reorder jit methods material procurement 
production planning control 

dasarathy editor 
nearest neighbor nn norms nn pattern classi cation techniques 
ieee computer society press los alamitos ca 

dasarathy 
minimal consistent set mcs identi cation optimal nearest neighbor systems design 
ieee trans 
systems man cybernetics 

kanal 
decision trees pattern recognition 
kanal rosenfeld editors progress pattern recognition volume pages 
elsevier science 

sarma 
bayesian decision tree approaches pattern recognition including feature measurement costs 
ieee trans 
pattern analysis machine intelligence pami 

thomas dietterich hermann hild bakiri 
comparison id backpropagation english text speech mapping 
machine learning 

thomas dietterich eun bae kong 
machine learning bias statistical bias statistical variance decision tree algorithms 
ml 
appear 
sreerama murthy 
thomas dietterich michalski 
comparitive view selected methods learning examples 
michalski carbonell mitchell editors machine learning arti cial intelligence approach volume pages 
morgan kaufmann san mateo ca 

justin 
evaluation search algorithms feature selection 
technical report graduate group computer science univ california davis safeguards systems group los alamos national lab january 

dowe 
decision tree models bush re activity 
ai applications 

draper carla brodley paul goal directed classi cation linear machine decision trees 
ieee trans 
pattern analysis machine intelligence 

draper smith 
applied regression analysis 
wiley new york 
nd edition 

duda hart 
pattern classi cation scene analysis 
wiley new york 

eades staples 
optimal trees 
journal algorithms 

bradley efron 
estimating error rate prediction rule improvements crossvalidation 
american statistical association june 

john elder iv 
heuristic search model structure 
ai statistics pages 

elomaa 
defence notes learning level decision trees 
ml pages 
editors william cohen haym hirsh 


classi cation trees prove useful testing quality 
welding september 
issue title special emphasis rebuilding america roads bridges 

esposito donato malerba giovanni semeraro 
study pruning methods decision tree induction 
ai statistics pages 

bob evans doug fisher 
overcoming process delays decision tree induction 
ieee expert pages february 

brian everitt 
cluster analysis rd edition 
arnold press london 

judith falconer bruce naughton dorothy dunlop elliot roth dale 
predicting stroke rehabilitation outcome classi cation tree approach 
archives physical medicine rehabilitation june 


decision tree induction process optimization knowledge re nement industrial process 
arti cial intelligence eng 
design analysis manufacturing ai winter 

fano 
transmission information 
mit press cambridge ma 

usama fayyad irani 
minimized decision tree 
aaai proc 
national conf 
arti cial intelligence volume pages 
aaai 

usama fayyad irani 
attribute speci cation problem decision tree generation 
aaai pages 

usama fayyad irani 
handling continuous valued attributes decision tree generation 
machine learning 

usama fayyad irani 
multi interval discretization continuous valued attributes classi cation learning 
ijcai pages 
editor bajcsy 

edward feigenbaum 
expert systems 
bond editor state art machine intelligence 
pergamon 

feng sutherland king muggleton 
comparison machine learning classi ers statistics neural networks 
ai statistics pages 

fielding 
binary segmentation automatic interaction detector related techniques exploring data structure 
payne pages 

file houston 
evaluation induction medical expert system 
comp 
biomedical research october 
decision tree construction survey 
douglas fisher 
knowledge acquisition incremental conceptual clustering 
machine learning 

douglas fisher kathleen mckusick 
empirical comparison id back propagation 
ijcai 
editor sridharan 

fletcher powell 
rapidly convergent descent method minimization 
computer iss 

foley 
considerations sample feature size 
ieee trans 
information theory 

prabhu 
induction multivariate regression trees design optimization 
aaai pages 


feature selection piecewise linear classi ers 
phd thesis univ california irvine ca 

jack sklansky 
feature selection automatic classi cation non gaussian data 
ieee trans 
systems man cybernetics march april 

richard forsyth david clarke richard wright 
tting revisited information theoretic approach simplifying discrimination trees 
experimental theoretical arti cial intelligence july september 

jerome friedman 
recursive partitioning decision rule nonparametric classi ers 
ieee trans 
comp april 

hayes 
ect sample size classi er design 
ieee trans 
pattern analysis machine intelligence 

fulton simon kasif steven salzberg 
cient algorithm nding multi way splits decision trees 
ml 
appear 

furnkranz trappl 
knowledge discovery international con ict databases 
applied arti cial intelligence 

michael garey ronald graham 
performance bounds splitting algorithm binary testing 
acta informatica 


gelfand ravishankar 
tree structured piecewise linear adaptive lter 
ieee trans 
information theory november 

saul gelfand ravishankar edward delp 
iterative growing pruning algorithm classi cation tree design 
ieee transaction pattern analysis machine intelligence february 

gelsema kanal editors 
pattern recognition practice iv multiple paradigms comparative studies hybrid systems volume machine intelligence pattern recognition 
series editors kanal elsevier 

gennari pat langley douglas fisher 
models incremental concept formation 
arti cial intelligence september 

allen gersho robert gray 
vector quantization signal compression 
kluwer academic pub 

auslander gri selection myocardial features devices 
ieee trans 
biomedical eng august 


honeywell program survey analysis 
behavioral science 

elizabeth richard olshen chatterjee john arthur moss henning robert engler robert howard dittrich john ross jr predicting year outcome acute myocardial 
comp 
biomedical research february 

malcolm morris 
automated medical decisions 
comp 
biomedical research april 

marchand 
growth algorithm neural network decision trees 
euro physics letters june 

rodney goodman padhraic smyth 
decision tree design communication theory standpoint 
ieee trans 
information theory september 

rodney goodman smyth 
decision tree design information theory 
knowledge acquisition 
sreerama murthy 
michael goodrich vincent mark je ery salowe 
decision tree xed dimensions global hard local greed 
technical report tr johns hopkins univ department computer science baltimore md may 

gordon olshen 
asymptotically cient solutions classi cation problem 
annals statistics 

gray 
capturing knowledge top induction decision trees 
ieee expert june 

kak 
interactive learning multi attribute hash table classi er fast object recognition 
computer vision image understanding may 

heng guo saul gelfand 
classi cation trees neural network feature extraction 
ieee trans 
neural networks november 

guo 
distinguishing mean variance autocorrelation changes statistical quality control 
int 
production research february 

ali william wallace 
induction rules subject quality constraint probabilistic inductive learning 
ieee trans 
data eng december 
special issue learning discovery knowledge databases 


linear function neurons structure training 
biological cybernetics 

hand 
discrimination classi cation 
wiley chichester uk 


design optimization hierarchical classi er 
new generation computer systems 

hansen salomon 
neural network ensembles 
ieee trans 
pattern analysis machine intelligence 

hart 
experience inductive system knowledge eng 
editor research development expert systems 
cambridge univ press cambridge ma 

carlos hartmann varshney mehrotra carl 
application information theory construction cient decision trees 
ieee trans 
information theory july 

haskell 
design hierarchical classi ers 
de editors computing great lakes computer science conf 
proc pages berlin 
springer verlag 
conf 
held mi th th october 


decision tree method line steady state security assessment 
ieee trans 
power systems 

heath 
framework machine learning 
phd thesis johns hopkins univ baltimore md 

heath kasif salzberg 
dt multi tree learning method 
proc 
second int 
workshop multistrategy learning pages ferry wv 
george mason univ 
heath kasif salzberg 
learning oblique decision trees 
ijcai pages 
editor bajcsy 

helmbold schapire 
predicting nearly best pruning decision tree 
machine learning pages 
earlier version colt 

ernest jr king sun fu 
nonparametric partitioning procedure pattern classi cation 
ieee trans 
comp july 

gabor herman daniel yeung 
piecewise linear classi cation 
ieee trans 
pami july 

klaus hoe gen hans simon kevin van horn 
robust single neurons 
computer system sciences 

holte 
simple classi cation rules perform commonly datasets 
machine learning 

hughes 
mean accuracy statistical pattern recognition 
ieee trans 
information theory january 

hunt 
classi cation induction applications modelling control non linear dynamic systems 
intelligent systems eng winter 
decision tree construction survey 
laurent ronald rivest 
constructing optimal binary decision trees npcomplete 
information processing letters 

ibaraki 
adaptive linear classi ers linear programming 
technical report department computer science univ illinois urbana champaign 

jack sklansky 
optimum feature selection zero integer programming 
ieee trans 
systems man cybernetics smc september october 


utilization best linear discriminant function designing binary decision tree 
int 
journal remote sensing january 

ijcai proc 
eleventh int 
joint conf 
arti cial intelligence 
morgan kaufmann pub 
san mateo ca 
editor sridharan 

ijcai proc 
thirteenth int 
joint conf 
arti cial intelligence volume chambery france th august rd september 
morgan kaufmann pub 
san mateo ca 
editor bajcsy 

ijcai proc 
fourteenth int 
joint conf 
arti cial intelligence montreal canada th st august 
morgan kaufmann pub 
san mateo ca 
editor chris mellish 

michalski 
decision trees learned examples decision rules 
methodologies intelligent systems th int 
symposium 
ismis volume lncs pages 
springer verlag trondheim norway june 

irani cheng jie usama fayyad qian 
applying machine learning semiconductor manufacturing 
ieee expert february 

israel 
hybrid electro optical architecture classi cation trees associative memory mechanisms 
int 
arti cial intelligence tools architectures languages algorithms september 

andreas ittner michael schlosser 
non linear decision trees ndt 
int 
conf 
machine learning 


jain chandrasekaran 
dimensionality sample size considerations pattern recognition 
krishnaiah kanal pages 

george john 
robust linear discriminant trees 
ai statistics pages 

george john ron kohavi karl eger 
irrelevant features subset selection problem 
ml pages 
editors william cohen haym hirsh 

michael jordan 
statistical approach decision tree modeling 
proceedings seventh annual acm conference computational learning theory pages new brunswick new jersey 
acm press 

michael jordan jacobs 
hierarchical mixtures experts em algorithm 
neural computation 

weiss wachter 
role assessing disease activity disease classi cation regression trees 
american may 


application con dence interval error analysis design decision tree classi ers 
pattern recognition letters may 

kanal 
patterns pattern recognition 
ieee trans 
information theory 

kanal 
problem solving methods search strategies pattern recognition 
ieee trans 
pattern analysis machine intelligence pami 

kanal chandrasekaran 
dimensionality sample size statistical pattern classi cation 
pattern recognition 

kass 
exploratory technique investigating large quantities categorical data 
applied statistics 

michael kearns 
boosting theory practice developments decision tree induction weak learning framework 
proceedings thirteenth national conference arti cial intelligence eighth innovative applications arti cial intelligence conference pages menlo park 
aaai press mit press 

michael kearns mansour 
boosting ability top decision tree learning algorithms 
proceedings eighth annual acm symposium theory computing pages philadelphia pennsylvania 
sreerama murthy 
davis kennedy 
decision tree bears fruit 
products finishing july 

ck carvalho dickson weir fayyad 
discovery second sky survey 
astronomical 

randy kerber 
chimerge discretization numeric attributes 
aaai pages 

kim david landgrebe 
hierarchical decision tree classi ers highdimensional large class data 
ieee trans 
geoscience remote sensing july 

kim koehler 
investigation conditions pruning induced decision tree 
european operational research august 

sung ho kim 
general property nested pruned subtrees decision support tree 
communications statistics theory methods april 

kenji kira larry rendell 
feature selection problem traditional methods new algorithm 
aaai pages 


generalization noise 
int 
man machine studies 


machine learning object recognition scene analysis 
pattern recognition ai 

ron kohavi 
bottom induction oblivious read decision graphs strengths limitations 
aaai 

ron kohavi 
power decision tables 
european conference machine learning 

ron kohavi 
study cross validation bootstrap accuracy estimation model selection 
ijcai pages 
editor chris mellish 

ron kohavi 
wrappers performance enhancements oblivious decision graphs 
ph thesis cs tr stanford university department computer science september 


decision trees automatic learning cardiology 
journal medical systems 

igor kononenko 
biases estimating multi valued attributes 
ijcai pages 
editor chris mellish 

igor kononenko ivan bratko 
information evaluation criterion classi er performance 
machine learning january 

van 
classi cation methods computerized interpretation 
methods information medicine september 


problem character recognition point view mathematical statistics 
editor character readers pattern recognition 
spartan new york 

koza 
concept formation decision tree induction genetic programming paradigm 
schwefel manner editors parallel problem solving nature proc 
st workshop ppsn volume lncs pages dortmund germany october 
springer verlag berlin germany 

rama krishnaiah kanal editors 
classi cation pattern recognition reduction dimensionality volume handbook statistics 
north holland publishing amsterdam 

srinivasan krishnamoorthy douglas fisher 
machine learning approaches estimating software development ort 
ieee trans 
software eng february 


optimization classi cation trees strategy algorithm improvement 
computer physics communications december 

kubat pfurtscheller 
ai approach automatic sleep classi cation 
biological cybernetics 

ashok kulkarni 
mean accuracy hierarchical classi ers 
ieee trans 
comp august 
decision tree construction survey 
michael kurtz 
astronomical object classi cation 
gelsema kanal editors pattern recognition arti cial intelligence pages 
elsevier science pub amsterdam 


optimal strategy tree classi er 
pattern recognition 


multi stage bayes classi er 
pattern recognition 


identity optimal strategies multi stage classi ers 
pattern recognition letters july 

kwok carter 
multiple decision trees 
levitt kanal lemmer editors uncertainty arti cial intelligence volume pages 
elsevier science amsterdam 

holmes 
addressing geographical data errors classi cation tree soil unit prediction 
int 
geographical information science march 

timmers bins 
binary tree versus single level tree classi cation white blood cells 
pattern recognition 

pat langley stephanie sage 
scaling domains irrelevant features 
thomas petsche amd stephen jose hanson russell greiner editor computational learning theory natural learning systems volume vol iv 
mit press 

lee 
noisy character recognition fuzzy tree classi er 
proc 
spie 
volume title machine vision recognition industrial inspection 
conf 
location san jose ca 
th th february 

wendy lehnert stephen soderland david feng shmueli 
inductive text classi cation medical applications 
journal experimental theoretical arti cial intelligence january march 

lewis 
characteristic selection problem recognition systems 
ire trans 
information theory 

li richard dubes 
tree classi er design permutation statistic 
pattern recognition 

lin storer 
design performance tree structured vector quantizers 
information processing management 

lin storer cohn 
optimal pruning tree structured vector quantizers 
information processing management 

han lin vitter 
nearly optimal vector quantization linear programming 
storer cohn editors dcc 
data compression conf pages los alamitos ca march th th 
ieee computer society press 

lin king sun fu 
automatic classi cation cells binary tree classi er 
pattern recognition 

liu white 
importance attribute selection measures decision tree induction 
machine learning 

wei yin loh 
tree structured classi cation generalized discriminant analysis 
american statistical association september 

william long john gri th harry selker ralph agostino 
comparison logistic regression decision tree induction medical domain 
comp 
biomedical research february 

loveland 
performance bounds binary testing arbitrary weights 
acta informatica 

david 
algorithmic speedups growing classi cation trees additive split criterion 
ai statistics pages 

david 
bivariate splits consistent split criteria dichotomous classi cation trees 
phd thesis department computer science rutgers univ new brunswick nj 

david 
classi cation trees bivariate splits 
applied intelligence int 
arti cial intelligence neural networks complex problem solving technologies july 
sreerama murthy 
david 
tree structured interpretable regression 
ai statistics pages 

ren luo ralph mark 
object identi cation automated decision tree construction approach robotics applications 
robotic systems june 


simulated annealing construction near optimal decision trees 
ai statistics 

david magerman 
natural language parsing statistical pattern recognition 
thesis cs tr stanford university department computer science february 

mangasarian 
mathematical programming neural networks 
orsa computing fall 

mangasarian 
misclassi cation minimization 
unpublished manuscript 

mangasarian setiono wolberg 
pattern recognition linear programming theory application medical diagnosis 
siam workshop optimization 

lopez de mantaras 
technical note distance selection measure decision tree induction 
machine learning 

kent martin 
evaluating comparing classi ers complexity measures 
ai statistics pages 

kent martin 
exact probability metric decision tree splitting stopping 
machine learning 

kent martin daniel hirschberg 
time complexity decision tree induction 
technical report ics tr university california irvine department information computer science august 

dean mckenzie lee hun low 
construction computerized classi cation systems machine learning algorithms overview 
comp 
human behaviour 

dean mckenzie wallace lee hun low singh 
constructing minimal diagnostic decision tree 
methods information medicine april 

mcqueen garner nevill manning witten 
applying machine learning agricultural data 
comp 
electronics agriculture june 

nimrod megiddo 
complexity polyhedral separability 
discrete computational geometry 

william 
partitioning algorithm application pattern classi cation optimization decision trees 
ieee trans 
comp january 

joseph 
tree hedge 
financial analysts pages november december 

donald michie 
phenomenon context software manufacture 
proc 
royal society london 

spiegelhalter michie taylor 
machine learning neural statistical classi cation 
ellis horwood 
statlog project 

miller 
subset selection regression 
chapman hall 

john mingers 
expert systems rule induction statistical data 
operational research society 

john mingers 
empirical comparison pruning methods decision tree induction 
machine learning 

john mingers 
empirical comparison selection measures decision tree induction 
machine learning 

minsky papert 
perceptrons 
mit press cambridge ma 

tom mitchell rich caruana dayne freitag john mcdermott david zabowski 
experience learning personal assistant 
communications acm july 


optimum decision trees optimal variable theorem related applications 
acta informatica 
decision tree construction survey 

criteria selecting variable construction cient decision trees 
ieee trans 
comp january 

machine learning proc 
tenth int 
conf univ massachusetts amherst ma th june 
morgan kaufmann pub 
editor paul 
machine learning proc 
eleventh int 
conf rutgers univ new brunswick nj th july 
morgan kaufmann pub 
editors william cohen haym hirsh 

machine learning proc 
twelfth int 
conf tahoe city ca th july 
morgan kaufmann pub 
san mateo ca 
editor je rey schlimmer 

robert james keller krishnapuram 
uncertainty management rule systems application image analysis 
ieee trans 
systems man cybernetics march 

andrew moore mary lee 
cient algorithms minimizing cross validation error 
ml pages 
editors william cohen haym hirsh 

bernard moret thomason gonzalez 
relation decision trees 
acm trans 
programming language systems october 

bernard moret 
decision trees diagrams 
computing surveys december 

morgan messenger 
sequential search program analysis nominal scale dependent variables 
technical report institute social research univ michigan ann arbor mi 

morris 
decision trees domain knowledge pattern recognition 
gelsema kanal pages 


comparison techniques choosing subsets pattern recognition properties 
ieee trans 
comp september 

muller wysotzki 
automatic construction decision trees classi cation 
annals operations research 

murphy 
designing storage cient decision trees 
ieee trans 
comp march 

patrick murphy 
empirical analysis bene decision tree size biases function concept distribution 
submitted machine learning journal july 

patrick murphy david aha 
uci repository machine learning databases machine readable data repository 
maintained department information computer science univ california irvine 
anonymous ftp ics uci edu directory pub machine learning databases 

patrick murphy michael pazzani 
exploring decision forest empirical investigation occam razor decision tree induction 
arti cial intelligence research 

sreerama murthy kasif salzberg beigel 
oc randomized induction oblique decision trees 
aaai pages 

sreerama murthy simon kasif steven salzberg 
system induction oblique decision trees 
arti cial intelligence research august 

sreerama murthy steven salzberg 
decision tree induction ective greedy heuristic 
proc 
int 
conf 
knowledge discovery databases montreal canada august 

sreerama murthy steven salzberg 
lookahead pathology decision tree induction 
ijcai 
appear 

narendra 
branch bound algorithm feature subset selection 
ieee trans 
comp 

dana nau 
decision quality function search depth game trees 
association computing machinery october 


np completeness problems construction optimal decision trees 
soviet physics doklady april 

niblett 
constructing decision trees noisy domains 
bratko lavrac editors progress machine learning 
sigma press england 
sreerama murthy 
nilsson 
learning machines 
morgan kaufmann 

nilsson lundgren 
computerized induction analysis possible variations di erent elements human tooth 
arti cial intelligence november 

steven norton 
generating better decision trees 
ijcai pages 
editor sridharan 

nu nez 
background knowledge decision tree induction 
machine learning 

tim oates david jensen 
ects training set size decision tree complexity 
proceedings th international conference machine learning pages 
morgan kaufmann 

oliver 
decision graphs extension decision trees 
ai statistics 


statistical analysis context survey research 
payne pages 

payne editors 
analysis survey data volume john wiley sons chichester uk 

pagallo haussler 
boolean feature discovery empirical learning 
machine learning march 

page muggleton 
learnability ts machine learning practice learnability result decision tree learner cart 
proceedings conference applied decision technologies adt 
volume computational learning probabilistic reasoning pages uk april 
seminars 

pal chakraborty bagchi 
rid id algorithm real data 
information sciences february 

steven gordon 
tables trees formulas decision analysis 
communications acm october 

park 
comparison neural net classi ers linear tree classi ers similarities di erences 
pattern recognition 

park jack sklansky 
automated design linear tree classi ers 
pattern recognition 

park jack sklansky 
automated design multiple class piecewise linear classi ers 
classi cation 

krishna mark 
application heuristic search information theory sequential fault diagnosis 
ieee trans 
systems man cybernetics july august 

payne preece 
identi cation keys diagnostic tables review 
royal statistical society series 

pearson stokes 
vector evaluation induction algorithms 
int 
high speed computing march 


knowledge acquisition symbolic decision tree induction interpretation digital images radiology 
lecture notes computer science 

de jong spears 
arti cial intelligence approach analog systems diagnosis 
wen liu editor testing diagnosis analog circuits systems 
van nostrand reinhold new york 

narayan raman michael shaw 
learning scheduling exible manufacturing ow line 
ieee trans 
eng 
management may 

jackson 
comparitive review knowledge eng 
inductive learning data medical domain 
proc 
spie int 
society optical eng april 

shi qing yun king sun fu 
method design binary tree classi ers 
pattern recognition 

john ross quinlan 
discovering rules induction large collections examples 
donald michie editor expert systems micro electronic age 
edinburgh univ press edinburgh uk 
decision tree construction survey 
john ross quinlan 
ect noise concept learning 
michalski carbonell mitchell editors machine learning arti cial intelligence approach volume 
morgan kau man san mateo ca 

john ross quinlan 
induction decision trees 
machine learning 

john ross quinlan 
simplifying decision trees 
int 
man machine studies 

john ross quinlan 
empirical comparison genetic decision tree classi ers 
fifth int 
conf 
machine learning pages ann arbor michigan 
morgan kaufmann 

john ross quinlan 
unknown attribute values induction 
proc 
sixth int 
workshop machine learning pages san mateo ca 
morgan kaufmann 

john ross quinlan 
probabilistic decision trees 
michalski editors machine learning arti cial intelligence approach volume 
morgan kaufmann san mateo ca 

john ross quinlan 
programs machine learning 
morgan kaufmann pub san mateo ca 

john ross quinlan 
comparing connectionist symbolic learning methods 
hanson rivest editors computational learning theory natural learning systems constraints prospects 
mit press 

john ross quinlan 
improved continuous attributes 
arti cial intelligence research march 

john ross quinlan ronald rivest 
inferring decision trees minimum description length principle 
information computation march 

larry rendell 
lookahead feature construction learning hard concepts 
ml pages 
editor paul 
larry rendell 
improving design induction methods analyzing algorithm functionality data concept complexity 
ijcai pages 
editor bajcsy 

alfred renyi laszlo 
probability theory 
north holland publishing amsterdam 

riddle segal etzioni 
representation design brute force induction boeing manufacturing domain 
applied arti cial intelligence january march 


stochastic complexity statistica enquiry 
world scienti 

eve robert gray 
lookahead growing tree quantizers 
icassp int 
conf 
speech signal processing volume pages toronto ontario may th th 
ieee 

rounds 
combined non parametric approach feature selection binary decision tree design 
pattern recognition 

steven stein james donald brown 
decision trees real time transient stability prediction 
ieee trans 
power systems august 

ron 
se tree characterization induction problem 
ml pages 
editor paul 
ron short jr automatic cataloging characterization earth science data set enumeration trees 
telematics informatics fall 

david landgrebe 
survey decision tree classi er methodology 
ieee trans 
systems man cybernetics may june 

sahami 
learning non linearly separable boolean functions linear threshold unit trees madaline style networks 
aaai pages 

steven salzberg 
locating protein coding regions human dna decision tree algorithm 
computational biology 
appear fall 

steven salzberg holland ford sreerama murthy rick white 
decision trees automated identi cation cosmic ray hits hubble space telescope images 
publications astronomical society paci march 

anant sankar richard 
growing pruning neural tree networks 
ieee trans 
comp march 
sreerama murthy 
lawrence saul michael jordan 
learning boltzmann trees 
neural computation november 

cullen scha er 
tting avoidance bias 
machine learning 

cullen scha er 
conservation law generalization performance 
ml pages 
editors william cohen haym hirsh 

cullen scha er 
conservation generalization case study 
technical report department computer science cuny hunter college february 

robert gray 
unbalanced non binary tree structured vector quantizers 
singh editor conf 
record seventh asilomar conf 
signals systems comp volume pages los alamitos ca november st rd 
ieee computer society press 
conf 
held paci grove ca 


decision theoretic approach hierarchical classi er design 
pattern recognition 

krishnan sethi 
entropy nets decision trees neural networks 
proc 
ieee october 

krishnan sethi chatterjee 
cient decision tree design discrete variable pattern recognition problems 
pattern recognition 

krishnan sethi 
hierarchical classi er design mutual information 
ieee trans 
pattern analysis machine intelligence pami july 

krishnan sethi yoo 
design multicategory split decision trees perceptron learning 
pattern recognition 

shang leo breiman 
distribution trees accurate 
proc 
int 
conf 
neural information processing pages 


shannon 
mathematical theory communication 
bell system technical 

jude shavlik mooney towell 
symbolic neural learning algorithms empirical comparison 
machine learning 

shinohara shinohara miyano arikawa 
knowledge acquisition amino acid sequences machine learning system 
trans 
information processing society japan october 

seymour 
multiple binary decision tree classi ers 
pattern recognition 

seymour 
nonparametric classi cation matched binary decision trees 
pattern recognition letters february 

siedlecki 
automatic feature selection 
int 
pattern recognition arti cial intelligence 


nadal 
neural trees new tool classi cation 
network computation neural systems october 

jack sklansky leo 
locally trained piecewise linear classi ers 
ieee trans 
pattern analysis machine intelligence pami march 

jack sklansky nicholas 
pattern classi ers trainable machines 
springer verlag new york 

padhraic smyth alexander gray usama fayyad 
retro tting decision tree classi ers kernel density estimation 
proc 
th international conference machine learning pages 
morgan kaufmann 

baker morgan 
searching structure 
institute social research univ michigan ann arbor mi 

schwartz wiles gough philips 
connectionist rule bayesian decision aids empirical comparison 
pages 
chapman hall london 

suen qing ren wang 
interactive clustering algorithm new objectives 
pattern recognition 

sun qiu louis anthony cox 
hill climbing approach construct near optimal decision trees 
ai statistics pages 

swain 
decision tree classi er design potential 
ieee trans 
geoscience electronics ge 
decision tree construction survey 
jan 
multiclass nonparametric partitioning algorithm 
pattern recognition letters 

jan willem vincent 
neural nets classi cation trees comparison domain ecg analysis 
gelsema kanal pages 

jan mcnair 
ect noise biases performance machine learning algorithms 
int 
bio medical computing july 

ming tan 
cost sensitive learning classi cation knowledge applications robotics 
machine learning 

paul taylor bernard silverman 
block diagrams splitting criteria classication trees 
statistics computing december 

sebastian thrun monk problems performance comparison di erent learning algorithms 
technical report cmu cs school computer science carnegie mellon univ pittsburgh pa 


linear discriminant classi cation tree user driven multicriteria classi cation method 
chemometrics intelligent lab 
systems 

pei lei tu jen yao chung 
new decision tree classi cation algorithm machine learning 
proc 
ieee int 
conf 
tools ai pages arlington virginia november 

zhao 
equivalence inductive learning pseudo boolean logic simpli cation rule generation reduction scheme 
ieee trans 
systems man cybernetics may june 

peter turney 
cost sensitive classi cation empirical evaluation hybrid genetic decision tree induction algorithm 
journal arti cial intelligence research march 

paul incremental induction decision trees 
machine learning 

paul perceptron trees case study hybrid concept representations 
connection science 

paul improved algorithm incremental induction decision trees 
ml pages 
editors william cohen haym hirsh 

paul neil berkman je ery clouse 
decision tree induction cient tree restructuring 
machine learning 

paul carla brodley 
incremental method nding multivariate splits decision trees 
proc 
seventh int 
conf 
machine learning pages los altos ca 
morgan kaufmann 

van 
problem measurement selection 
phd thesis stanford univ dept electrical eng 

thierry van de 
decision trees numerical attribute spaces 
ijcai pages 
editor bajcsy 

varshney hartmann de jr applications information theory sequential fault diagnosis 
ieee trans 
comp 

walter van de velde 
incremental induction topologically minimal trees 
bruce porter ray mooney editors proc 
seventh int 
conf 
machine learning pages austin texas 

wallace boulton 
information measure classi cation 
computer 

wallace patrick 
coding decision trees 
machine learning april 

qing ren wang suen 
analysis design decision tree entropy reduction application large character set recognition 
ieee trans 
pattern analysis machine intelligence 

qing ren wang ching suen 
large tree classi er heuristic search global training 
ieee trans 
pattern analysis machine intelligence pami january 

nicholas jack sklansky training dimensional classi er minimize probability error 
ieee trans 
systems man cybernetics smc september 
sreerama murthy 
larry watanabe larry rendell 
learning structural decision trees examples 
volume pages harbour sydney australia th august 
morgan kaufmann pub 
san mateo ca 
editors john mylopoulos ray reiter 

watanabe 
pattern recognition quest minimum entropy 
pattern recognition 

nicholas weir usama fayyad 
initial galaxy counts digitized poss ii 
astronomical 

nicholas weir usama fayyad 
automated star galaxy classi cation digitized poss ii 
astronomical 

weiss 
empirical comparison pattern recognition neural nets machine learning classi cation methods 
ijcai pages 
editor sridharan 

allan white wei zhang liu 
technical note bias information measures decision tree induction 
machine learning june 

wilks english 
accurate segmentation respiration waveforms infants enabling identi cation classi cation irregular breathing patterns 
medical eng 
physics january 

wirth catlett 
experiments costs bene ts windowing id 
fifth int 
conf 
machine learning pages ann arbor michigan 
morgan kaufmann 

david wolpert 
tting avoidance bias 
technical report sfi tr santa fe institute 

david wolpert 
connection sample testing generalization error 
complex systems 

woods jr kegelmeyer 
pattern recognition techniques detection cations mammography 
int 
pattern recognition arti cial intelligence december 

king sun fu 
approach design linear binary tree classi er 
proc 
third symposium machine processing remotely sensed data west lafayette 
purdue univ 
yuan shaw 
induction fuzzy decision trees 
fuzzy sets systems 

wang lin yan 
new inductive learning algorithm separability inductive learning algorithm 
acta automatica sinica 
translated chinese automation 

xiao jia zhou dillon 
statistical heuristic feature selection criterion decision tree induction 
ieee trans 
pattern analysis machine intelligence pami august 

seth zimmerman 
optimal search procedure 
american mathematical monthly march 
kluwer academic publishers boston 
manufactured netherlands 
contributing authors sreerama murthy received ph computer science johns hopkins university baltimore md 
prior studied indian institute technology madras india regional engineering college india 
dr murthy working imaging visualization department siemens corporate research princeton nj 
dr murthy unifying decision tree multiple disciplines nding new applications decision trees particularly image analysis computer aided diagnosis 
