compiler techniques code compaction saumya debray william evans university arizona robert muth compaq computer bjorn de university ghent years increasing trend incorporation computers variety devices amount memory available limited 
desirable try reduce size applications possible 
article explores compiler techniques accomplish code compaction yield smaller executables 
main contribution article show careful aggressive interprocedural optimization procedural abstraction repeated code fragments yield signi cantly better reductions code size previous approaches generally focused abstraction repeated instruction sequences 
show equivalent code fragments detected factored conventional compiler techniques having resort purely linear treatments code sequences sux tree approaches setting framework code compaction exible treatment code fragments considered equivalent 
ideas implemented form binary rewriting tool reduces size executables average 
categories subject descriptors programming languages processors code generation compilers optimization coding information theory data compaction compression program representation general terms experimentation performance additional key words phrases code compaction code compression code size reduction saumya debray robert muth supported part national science foundation ccr cda asc 
bjorn de supported part fund scienti research flanders 
authors addresses debray evans department computer science university arizona tucson az email cs arizona edu muth alpha development group compaq computer ma email robert muth compaq com de department electronics information systems university ghent gent belgium email rug ac 
permission digital hard copy part material fee granted provided copies distributed pro commercial advantage acm copyright server notice title publication date appear notice copying permission association computing machinery 
acm 
copy republish post servers redistribute lists requires prior speci permission fee 
acm 

saumya debray 
years increasing trend incorporation computers wide variety devices palm tops telephones embedded controllers devices amount memory available limited due considerations space weight power consumption price 
time increasing desire sophisticated software devices encryption software telephones speech image processing software laptops palm tops 
unfortunately application requires memory available particular device able run device 
desirable try reduce size applications possible 
article explores compiler techniques accomplish code compaction 
previous reducing program size explored wide range program representations source languages intermediate representations machine codes van de 
resulting compressed form decompressed compiled execution ernst franz franz kistler executed interpreted fraser proebsting proebsting decompression cooper mcintosh fraser :10.1.1.147.3931
rst method results smaller compressed representation second requires overhead decompression execution 
decompression time may negligible fact may compensated savings transmission retrieval time franz kistler 
severe problem space required place decompressed code 
somewhat mitigated techniques partial decompression bene ernst techniques require altering run time operation hardware computer :10.1.1.147.3931
article explore compaction compression executable form 
resulting form larger smallest compressed representation program pay decompression overhead require space order execute 
earlier code compaction yield smaller executables treated executable program simple linear sequence instructions procedural abstraction eliminated repeated code fragments 
early fraser sux tree construction identify repeated sequences sequence assembly instructions abstracted functions 
applied range unix utilities vax processor technique managed reduce code size average 
shortcoming approach relies purely textual interpretation program sensitive super cial di erences code fragments due di erences register names may ect behavior code 
shortcoming addressed baker parameterized sux trees cooper mcintosh register renaming baker manber discuss similar approach parameterized procedural abstractions 
main idea rewrite instructions hard coded register names register operands instruction expressed possible terms previous basic block register 
branch instructions rewritten possible pc relative compiler techniques code compaction 
form 
transformations allow sux tree construction detect repetition similar lexically identical instruction sequences 
cooper mcintosh obtain code size reduction average techniques classically optimized code implementation classical optimizations achieve code size reduction compared unoptimized code 
approaches su er weaknesses 
rst focusing solely eliminating repeated instruction sequences ignore potentially pro table sources code size reduction 
second approach treats program simple linear sequence instructions sux tree approaches mentioned su er disadvantage having particular ordering instructions 
problem equivalent computations may map di erent instruction sequences di erent parts program due di erences register usage branch labels instruction scheduling pro le directed code layout improve instruction cache utilization pettis hansen 
article describes somewhat di erent approach code compaction system approach problem 
main contribution show aggressive interprocedural optimization procedural abstraction repeated code fragments possible obtain signi cantly greater reductions code size achieved date 
identi cation abstraction repeated code fragments shows equivalent code fragments detected factored having resort purely linear treatments code sequences sux tree approaches 
treating program simple linear sequence instructions interprocedural control ow graph 
sux tree construction identify repeated instruction sequences scheme identify similar basic blocks 
sets framework code compaction exible treatment code fragments considered equivalent 
notions dominators detect identical subgraphs control ow graph larger single basic block abstracted procedure 
identify take advantage architecture speci code idioms saving restoring speci sets registers entry return functions 
bene ts approach simpli es development code compaction systems information available compilers control ow graph dominator postdominator trees making unnecessary resort extraneous structures sux trees 
ideas implemented form binary rewriting tool alto post link time code optimizer muth 
resulting system called squeeze able achieve signi cantly better compaction previous approaches reducing size classically optimized code 
ideas incorporated fairly easily compilers capable interprocedural code transformations 
code size reductions achieve come sources aggressive interprocedural application classical compiler analyses optimizations code factoring refers variety techniques identify factor repeated instruction sequences 
section discusses classical optimizations supporting analyses useful reducing code size 

saumya debray followed section discussion code factoring techniques squeeze 
section discuss interactions classical optimizations factoring transformations 
section contains experimental results 
prototype system available www cs arizona edu alto squeeze 

classical analyses optimizations code compaction context code compaction binary rewriting little sense allow compiler ate size program transformations procedure inlining loop unrolling keep obviously unnecessary code failing perform example common subexpression elimination register allocation 
assume code compaction carried link time compiler invoked appropriate options generate reasonably compact code 
opportunities exist code transformations reduce program size 
section discusses classical program analyses optimizations useful code size reduction 
general optimizations implemented squeeze engineered avoid increases code size 
example procedure inlining limited procedures single call site alignment ops inserted instruction scheduling instruction cache optimization 
optimizations code compaction classical optimizations ective reducing code size include elimination redundant unreachable dead code certain kinds strength reduction 
redundant code elimination 
computation program redundant program point computed previously result guaranteed available point 
computations identi ed obviously eliminated ecting behavior program 
large portion code size reductions link time squeeze comes application optimization computations hardware register called global pointer gp register points collection bit constants called global address table 
alpha processor squeeze implemented bit architecture bit instructions 
bit constant loaded register appropriate global address table accessed gp register bit displacement 
accessing global object loading storing global variable jumping procedure involves steps loading address object global address table accessing object loaded address 
procedure executable program associated global address table di erent procedures may share table 
di erent procedures generally compiled typical bit architecture bit instruction words bit registers bit constant loaded register instructions load high bits register low bits instructions bits loaded encoded part instruction word 
alpha bit instructions bit registers mechanism adequate loading bit constant address procedure global variable register 
compiler techniques code compaction 
independently may need di erent global pointer values value gp register computed function entered control returns call function 
link time possible determine set functions gp value recomputation gp necessary 
turns functions program able value gp making recomputation gp redundant cases 
computation gp involves just register operations signi cant latency 
superscalar processor alpha corresponding instructions generally issued simultaneously computations incur signi cant performance penalty 
elimination gp computations generally lead signi cant improvements speed 
recomputations gp program elimination redundant gp computations yield signi cant reductions size 
unreachable code elimination 
code fragment unreachable control ow path rest program 
code unreachable executed eliminated ecting behavior program 
link time unreachable code arises primarily propagation information procedure boundaries 
particular propagation values actual parameters function call body called function possible statically resolve outcomes conditional branches callee 
nd result interprocedural constant propagation conditional branch function taken control ow path code branch taken code unreachable eliminated 
unreachable code analysis involves straightforward depth rst traversal control ow graph performed soon control ow graph program computed 
initially basic blocks marked unreachable entry block program dummy block called unknown edge basic block predecessors known see section 
analysis traverses interprocedural control ow graph identi es reachable blocks basic block marked reachable reached block reachable 
function calls corresponding return blocks handled context sensitive manner basic block follows function call marked reachable corresponding call site reachable 
dead code elimination 
dead code refers computations results 
notion results considered broadly 
example possible computation generate exceptions raise signals handling ect behavior rest program consider computation dead 
code dead eliminated ecting behavior program 
link time opportunities dead code elimination arise primarily result unreachable code elimination transforms partially dead computations computations results execution paths program point fully dead ones 

saumya debray strength reduction 
strength reduction refers replacement sequence instructions equivalent cheaper typically faster sequence 
general cheaper instruction sequence may shorter original sequence multiplication division operations operands known constant replaced cheaper longer sequence operations shifts adds 
bene ts code compaction come situations replacement sequence happens shorter original sequence 
squeeze code size improvements strength reduction come primarily application function calls 
processors alpha di erent function call instructions bsr branch subroutine instruction uses pc relative addressing able access targets xed displacement current location jsr jump subroutine instruction branches indirectly register target address 
compiler typically processes programs function time generates code function calls knowledge far away memory callee function calls translated jsr instructions 
turn requires bit address callee loaded register prior jsr 
discussed section done loading address callee global address table 
code generated function call consists load instruction followed jsr instruction 
strength reduced bsr instruction obtain savings code size improvement execution speed 
program analyses code compaction program analyses turn fundamental importance transformations discussed discussed section 
control flow analysis 
control ow analysis essential optimizations discussed section 
necessary redundant code elimination order identify computation redundant program point verify computed execution path point 
necessary unreachable code elimination dead code elimination classi cation code unreachable dead relies fundamentally knowing control ow behavior program 
strength reduction transformation function calls discussed section relies knowledge targets calls 
traditional compilers generally construct control ow graphs individual functions intermediate representation program straightforward way aho 
things somewhat complex link time machine code harder decompile 
squeeze construct interprocedural control ow graph program follows start address program appears xed location header le location may di erent di erent le formats 
starting point standard algorithm aho identify leaders basic blocks function entry blocks 
relocation information executable identify additional leaders jump table targets detected mark basic blocks compiler techniques code compaction 
relocatable 
stage assumptions function single entry block basic blocks function laid contiguously 
rst assumption turns incorrect repair ow graph stage 
second assumption hold constructed control ow graph may contain safe imprecisions may cause ective size optimizations 
add edges ow graph 
exact target control transfer instruction resolved assume transfer special block unknown case indirect jumps function unknown case indirect function calls 
conservatively assume unknown unknown de ne registers basic block start address marked relocatable may target unresolved indirect jump 
add edge unknown block 
function entry point marked relocatable may target unresolved indirect function call 
add call edge unknown 
safe overly conservative 
discuss improved 
carry interprocedural constant propagation resulting control ow graph described section 
results determine addresses loaded registers 
information turn resolve targets indirect jumps function calls 
resolve targets unambiguously replace edge unknown unknown edge appropriate target 
far assumed function call returns caller instruction immediately call instruction 
level executable code assumption violated ways 
rst involves escaping branches ordinary non function call jumps function arise due tail call optimization code sharing hand written assembly code example numerical libraries 
second involves nonlocal control transfers functions setjmp longjmp 
cases handled insertion additional control ow edges call compensation edges control ow graph 
case escaping branches function function result single compensation edge exit node exit node case function containing setjmp edge unknown exit node function containing longjmp compensation edge exit node unknown ect compensation edges force various data ow analyses approximate safely control ow ects constructs 
squeeze attempts resolve indirect jumps jump tables arise case switch statements 
essential idea constant propagation identify start address jump table bounds architectures callee may explicitly manipulate return address circumstances 
example sparc calling convention allows extra word follow call instruction 
case callee increments return address skip word 
grateful anonymous referee pointing 
situations arise alpha architecture handled squeeze 

saumya debray check instruction determine extent jump table 
edge indirect jump unknown replaced set edges entry jump table 
indirect jumps function resolved way remaining edges unknown basic blocks function deleted 
potentially procedure entry point address stored data section relocatable address program target indirect function call 
mentioned step procedures assumed reachable indirect calls long program contains call target unknown 
safe overly conservative 
discussed section code generated compiler function call typically consists load global address table followed indirect call 
compiler principle optimize direct call caller callee module scheme necessary inter module calls 
means procedure accessible outside module relocatable address stored global address table data section considered called unknown indication conservative simple technique note programs specint benchmark suite functions average considered called unknown alpha executables contain function relocation information improve precision control ow analysis 
compiler uses special relocation entries referred literal relocations tag instruction loads function address global address table instruction uses loaded address 
relocation entries play purely informational role ignored linker ecting program behavior 
load function address simply jump address remove edge unknown function replace call edges basic blocks contain jump instructions 
load function address followed jump address may stored may equal unresolved target 
case preserve edge unknown function 
specint benchmarks results fewer procedures having call unknown resulting improvement control ow information signi cant ect amount code eliminated unreachable leads signi cant improvement amount code compaction realized 
interprocedural constant propagation 
mentioned assume standard compiler analyses optimizations including constant propagation carried prior link time code compaction 
opportunities link time constant propagation arise 
turns surprisingly constant values propagated compile time source level compilation units propagated link time values available compile time addresses global names compiler unable propagate compilation unit boundaries caller callee 
link time constant propagation opportunities arise architecture speci computations visible compiler techniques code compaction 
intermediate code representation level typically compilers optimizations 
example computation gp register alpha processor 
analysis squeeze essentially standard iterative constant propagation limited registers carried control ow graph entire program 
ect communicating information constant arguments calling procedure callee 
improve precision squeeze attempts determine registers saved entry function restored exit 
register saved restored function manner contains constant just function called inferred contain value return call 
constant propagation turns fundamental importance rest system control data ow analyses rely knowledge constant addresses computed program 
example code generated compiler function call typically rst loads address called function register uses jsr instruction jump indirectly register 
constant propagation determines address loaded xed value callee far away indirect function call replaced direct call bsr instruction discussed section 
cheaper vital improving precision interprocedural control ow graph program lets replace pair call return edges unknown pair edges known callee 
example constant address information involves identi cation possible targets indirect jumps jump tables 
done assume indirect jump capable jumping basic block function signi cantly hamper optimizations 
knowledge constant addresses useful optimizations removal unnecessary memory 
nd average link time constant propagation able determine values arguments results instructions program 
mean evaluated instructions removed represent address computations indexing arrays structures calling functions 
interprocedural register liveness analysis 
code factoring discussed section involves abstracting repeated instruction sequences procedures 
call procedures necessary nd register hold return address 
squeeze implements relatively straightforward interprocedural liveness analysis restricted registers determine registers live program point 
analysis context sensitive maintains information return edges correspond call sites propagates information realizable call return paths 
standard data ow equations liveness analysis extended deal alpha instruction set 
example call pal instruction acts interface host operating system handled specially registers may instruction visible explicit operands instruc precisely basic block marked relocatable discussed section 

saumya debray tion 
implementation currently uses node unknown target calls 
conditional move instruction requires special attention destination register considered source register 
order propagate data ow information realizable call return paths squeeze computes summary information function models ect function calls summaries 
site call function consisting call node return node ects function call liveness information summarized pieces information mayuse set registers may register may realizable path entry node intervening de nition mayuse describes set registers live entry independent calling context necessarily live call node 
bypass set registers liveness depends calling context consists registers live live analysis proceeds phases 
rst phases compute summary information functions mayuse bypass sets 
third phase uses information actual liveness computation 
turns context sensitive liveness analyses may overly conservative careful handling register saves restores function call boundaries 
consider function saves contents register restores register returning 
register saved manner appear operand store instruction appear function 
subsequent restore operation register appear destination load instruction appear de ned function 
straightforward analysis infer function de ned cause inferred live call site handle problem squeeze attempts determine function set registers saves restores 
set callee saved registers function determined improve precision analysis removing set mayuse adding bypass values updated xpoint computation 

code factoring code factoring involves nding multiply occurring sequence instructions making representative sequence place occurrences arranging occurrence program executes representative occurrence 
third step achieved explicit control transfer call jump moving representative occurrences point dominates occurrence 
rst exploit form code factoring involves added control transfer instructions 
assume program necessarily respect calling conventions regard callee saved registers conventions respected libraries containing hand written assembly code 
approach safe overly conservative 
compiler techniques code compaction 
stq sub stq ldq xor xor cmp add beq sub stq ldq xor ldq stq ldq stq stq stq ldq xor stq xor add sub sub stq ldq xor stq add cmp beq fig 

local code factoring 
local factoring transformations inspired idea knoop try merge identical code fragments moving point pre postdominates occurrences fragments 
implemented local variant scheme describe example depicted 
left hand side gure shows assembly code conditional branch beq block blocks contain instruction add 
instructions backward dependencies instruction safely move block just beq instruction shown right hand side 
similarly blocks share store instruction stq instructions forward dependencies instruction safely moved block case possible move store instruction due lack aliasing information backward dependencies load instructions ldq general possible move instruction 
case prefer move moving way branch eliminate copy moving block predecessors eliminate copies 
scheme uses register reallocation transformation ective 
example sub instructions write di erent registers 
rename making instructions identical 
opportunity rests xor instructions identical move write register conditional branch 
register dead transformation possible 
procedural abstraction single entry single exit code fragment procedural abstraction involves creating procedure fc body copy replacing appropriate occurrences program text function call fc rst step dicult second step level assembly 
saumya debray machine code involves little 
order create function call form jump link instruction transfers control callee time puts return address register necessary nd free register purpose 
simple method calculate register number occurrences code fragment return register 
register highest gure merit chosen return register fc single instance fc particular return register occurrences program may create multiple instances fc di erent return registers 
complicated scheme abstracting function see section regions multiple basic blocks see section 
procedural abstraction individual basic blocks central approach ability apply procedural abstraction individual basic blocks 
section discuss candidate basic blocks procedural abstraction identi ed 
fingerprinting 
reduce cost comparing basic blocks determine identical similar ngerprint function compute ngerprint basic block blocks di erent ngerprints guaranteed di erent 
general ngerprint functions de ned respect notion equality basic blocks 
example current implementation blocks considered equal instruction sequences 
ngerprint function block sequence instructions block 
hand code compaction scheme de nes equality basic blocks respect de nition chains ngerprint number occurrences type opcode may 
current implementation ngerprint bit value formed concatenating bit encodings opcodes rst instructions block 
systems applications tend short basic blocks characterizing rst instructions basic blocks 
means blocks di erent sequence opcodes rst instructions ngerprint discover di erent compare instruction instruction 
bits instruction encode di erent opcodes reserve code 
decide explicitly represented considering static instruction count program 
frequently occurring opcodes distinct bit patterns 
remaining pattern represents opcodes top frequency 
reduce number pairwise comparisons ngerprints carried hashing scheme basic blocks di erent hash buckets guaranteed di erent ngerprints need compared 
register renaming basic blocks 
nd basic blocks similar ngerprint number instructions identical attempt rename registers identical 
basic idea simple rename compiler techniques code compaction 
live live fig 

example basic block level register renaming 
registers locally basic block necessary insert register moves new basic blocks inserted immediately block renamed preserve program behavior 
example shown block renamed block 
soundness ensure renaming alter nition relationships 
keeping track set registers live point basic block set registers subjected renaming 
sets detect disallow renamings alter program behavior 
pseudocode renaming algorithm appendix renaming algorithm keeps track number explicit register register moves inserted basic block renamed 
renaming undone renaming process cost renaming number register moves required function call instruction exceeds savings renaming number instructions block 
cooper mcintosh describe di erent approach register renaming 
carry register renaming level entire live ranges 
renaming register di erent register renaming applied entire live range advantage requiring additional register moves renamed block approach 
problem register renaming allow abstraction particular pair basic blocks may interfere abstraction di erent pair blocks 
illustrated solid double arrows indicate identical basic blocks dashed double arrows indicate blocks identical identical register renaming 
blocks comprise live range register comprise live range 
rename live range blocks identical cause blocks identical function 
rename block identical interfere abstraction blocks 
interference ects clear live range level renaming produces results necessarily superior basic block level renaming 
notice 
saumya debray load 
live range live range fig 

interference ects live range level register renaming 
problem addressed judiciously splitting live ranges 
local renaming seen limiting case live range level renaming splitting applied live range spans basic block 
control flow separation 
approach described typically able basic blocks identical explicit control transfer instruction 
reason control transfers di erent targets blocks considered di erent abstracted 
control transfer instruction conditional branch procedural abstraction complicated fact possible return addresses communicated 
avoid problems basic blocks explicit control transfer instruction split blocks block containing instructions block control transfer block contains control transfer instruction 
rst pair blocks subjected renaming procedural abstraction usual way 
section describes code fragments larger single basic block subjected procedural abstraction 
single entry single exit regions discussion far focused procedural abstraction individual basic blocks 
general may able nd multiple occurrences code fragment consisting basic block 
order apply procedural abstraction region occurrence program able identify single program point control enters single program point control leaves isn hard see set basic blocks single entry point single exit point corresponds pair points dominates block postdominates block conversely pair program points dominates postdominates uniquely identi es set basic blocks single entry point single exit point 
single entry single exit regions considered identical possible set correspondence compiler techniques code compaction 
members identical immediate successor condition immediate successor condition algorithm determine regions identical works recursively traversing regions starting entry node verifying corresponding blocks identical 
squeeze apply procedural abstraction individual basic blocks identify pairs basic blocks dominates postdominates pair de nes single entry single exit set basic blocks 
partition sets basic blocks groups identical regions candidates procedural abstraction 
case basic blocks compute ngerprint region regions di erent ngerprints necessarily di erent 
ngerprints bit values 
bits number basic blocks region bits total number instructions bit pattern represent values larger 
remaining bits encode rst particular preorder traversal region basic blocks region block encoded bits bits type block bits number instructions block 
case basic blocks number pairwise comparisons ngerprints reduced distributing regions hash table 
turns applying procedural abstraction set basic blocks straightforward single basic block especially binary rewriting implementation 
reason general procedure corresponding single entry single exit region called return address put register value guaranteed preserved entire procedure region may contain function calls region may contain paths register overwritten 
means return address register saved stack 
allocating extra word stack hold return address cause problems careful 
allocating space top stack frame cause changes displacements variables stack frame relative top stack pointer allocating bottom stack frame change displacements arguments passed stack 
address arithmetic involving stack pointer address computations local arrays computations may ected changes displacements stack frame 
problems somewhat easier handle procedural abstraction carried code generation level syntax trees franz 
level assembly code cooper mcintosh fraser machine code considerably complicated 
simple cases possible avoid complications associated having save restore return address introducing procedural abstractions 
identify situations 
cases de ne essence type block describes control ow behavior contains procedure call conditional branch indirect jump jump table 
saumya debray return return return fig 

merging regions returns cross jumping 
identical regions 
rst case involves situations return blocks blocks control returns caller 
case need procedural abstraction create separate function regions 
transformation known cross jumping muchnick code region simply replaced branch transformation illustrated 
second case suppose possible nd register live entry region value guaranteed preserved regions may general purpose register de ned region callee saved register saved restored functions regions occur 
case abstracting regions procedure necessary add code explicitly save restore return address instruction call simply put return address return instruction simply jump indirectly return caller 
conditions satis ed squeeze tries determine return address register safely saved stack entry restored 
uses conservative analysis determine function may arguments passed stack registers may pointers stack frame 
set candidate regions abstracted representative procedure checks function contains candidate region safe respect problems mentioned allocate word stack frame function register free entry regions consideration register free regions consideration calls setjmp functions ected change structure stack frame 
compiler techniques code compaction 
conditions satis ed entry allocates additional word stack saves return address passed location exit loads return address location restores stack frame 
current implementation safety check described quite conservative treatment function calls region 
principle nd space allocated stack free registers return address entry exit abstracted function possible allocate extra word stack order free register implemented 
architecture speci idioms apart general purpose techniques described earlier detecting abstracting repeated code fragments machine speci idioms pro exploited 
particular instructions save restore registers return address callee saved registers prolog epilog function generally predictable structure saved predictable locations stack frame 
example standard calling convention compaq alpha axp architecture tru unix treats register return address register ra registers callee saved registers 
saved locations sp sp sp 
abstracting instructions yield considerable savings code size 
save restore sequences recognized handled specially squeeze reasons rst instructions form contiguous sequence code stream second handling specially allows basic blocks may identical 
abstracting register saves 
order register save instructions prolog function separate function necessary identify register hold return address call register rst compute savings obtained return address calls 
done totaling function free entry number registers saved prolog 
choose register maximum savings exceed generate family functions save save save ra save callee saved registers return address register return register idea function save saves register falls function save example suppose functions saves registers 
saves register 
assume register free entry functions chosen return address register 
code resulting transformation described shown 
may turn functions subjected transformation callee saved registers 
example suppose functions return address register save register 
case code function save unreachable subsequently eliminated 
tru unix known digital unix 

saumya debray save save save save ra save sp sp bsr save bsr sp sp stq sp stq sp stq sp stq ra sp ret fig 

example code abstraction register save actions function 
particular choice return address register described may account functions program 
process repeated choices return address registers bene obtained functions accounted 
abstracting register restores 
code abstracting register restore sequences function conceptually analogous described di erences 
simply opposite done register saves function code resulting procedural abstraction return block function structure instructions manage control transfers stack pointer update bsr restore call function restores registers sp sp deallocate stack frame ret ra return move instruction deallocating stack frame function restores saved registers need return function epilog abstracting control return directly caller ect realizing tail call optimization 
problem code restore saved registers di erent functions general stack frames di erent sizes need adjust stack pointer di erent amounts 
solution problem pass argument function restores registers amount stack pointer adjusted 
return address register ra guaranteed free point overwritten return address prior returning control caller pass argument 
need control return registers restored return directly caller simply jump function function restores registers function call 
resulting code requires instructions function return block practice functions guaranteed follow standard calling convention necessary verify register ra fact return address register compiler techniques code compaction 
caller caller ra ldq sp ldq sp ldq sp restore restore restore restore ra ra sp sp ra stq ra sp ldq ra sp ldq sp sp ret ra fig 

example code abstraction register restore actions function 
ra sp needs adjusted br restore jump function restores registers code function restores registers pretty expect 
situation register save sequences discussed section need function restoring registers 
reason need call function control jump directly discussed 
means generate di erent versions function di erent return address registers 
structure code analogous saving registers chain basic blocks restores callee saved register control falling block saves lower numbered callee saved register 
member chain adjusts stack pointer appropriately loads return address register returns 
minor twist 
amount stack pointer adjusted passed register ra register overwritten adjust stack pointer 
hand memory location memory address restored stack frame adjust stack pointer return address loaded ra 
rst glance problem addressed instruction sequence sp sp ra sp sp ra new sp ra sp ra ra sp ra old sp ra load ra ra return address ret ra code incorrect stack pointer updated stack frame deallocated return address loaded stack frame 
result interrupt occurs rst instruction third instruction return address may overwritten resulting incorrect behavior 
avoid ensure stack pointer update instruction ret instruction 
rst computing new value stack pointer storing stack frame slot rst callee saved register originally stored updating return address 
saumya debray register nally loading new value stack pointer memory ra sp ra ra sp ra new sp sp store ra new sp saved location sp ra load sp ra return address sp load sp sp new sp ret ra resulting code restoring saved registers functions considered example illustrated shown 
go order minimize number registers 
nd register free function load return address register resulting somewhat simpler code 
general easy nd register free function 
reason go lengths eliminate single instruction return block lot return blocks input programs typically amounting basic blocks program excluding return blocks leaf routines allocate deallocate stack frame usually block function elimination instruction block translates code size reduction 
may small put perspective consider cooper mcintosh report code size reduction sux tree techniques 
abstracting partially matched blocks discussed preceding sections smallest code unit considered procedural abstraction squeeze basic block 
words squeeze attempt carry form procedural abstraction blocks may signi cant amount partial match blocks may share common subsequences instructions 
illustrated pair basic blocks shown matched instructions indicated lines drawn 
experiments described section indicate abstraction partially matched blocks computationally quite expensive adds little additional savings code size 
reason chosen include partial matching squeeze 
issues addressed considering procedural abstraction partially matched blocks rst identify partially matched blocks second transform code ect abstraction 
experiments abstraction partially matched blocks carried procedural abstraction fully matched blocks discussed section 
general particular basic block may partially matched di erent blocks may match di erent subsequences instructions 
savings obtained procedural abstraction case depends block chosen match 
block partially matched subjected procedural abstraction available partial matching basic blocks 
indebted anders lindgren pointing problem original code suggesting solution shown 
compiler techniques code compaction 
ld st sp st ld sp st sp pair partially matched blocks 
ld st ld sp st sp return ld st ld sp st sp return procedure obtained maximal matching procedure obtained table instructions fig 

procedural abstraction partially matched blocks 
means perspective may yield largest savings procedural abstraction carried may best choice globally may obtained greater savings matching block 
problem computing globally optimal set partial matches set basic blocks maximizes savings obtained procedural abstraction computationally dicult related longest common subsequence problem np complete garey johnson 
take greedy approach processing basic blocks decreasing order size 
processing block compare blocks choose block yields maximal savings computed discussed procedural abstraction carried partial matching put partition associated blocks processed manner blocks partition abstracted single procedure 
bene obtained procedural abstraction partially matched blocks determined follows 
dynamic programming determine minimum edit distance blocks best 
saumya debray match 
consider second issue mentioned carrying program transformation 
partial match blocks multiple execution paths resulting procedure call take path take 
passing argument abstracted procedure indicating call call site originated instructions execute 
scanning blocks nd mismatched sequence instructions block generate code abstracted procedure test argument execute appropriate instruction sequence outcome 
shows control ow graph resulting procedure 
addition instructions shown manage control ow 
need conditional branch blocks general blocks partition abstracted may need explicit comparison operations determine set alternatives execute unconditional branch pairs blocks fb fb total instructions 
notice designating instruction block match original blocks having common execution paths call sites procedure save single copy instruction pay penalty branch instructions managing control ow 
case turns better determining original partial match ignore fact instructions matched 
yield code shown total instructions 
hand single matched instruction sequence say matched instructions savings incurred combining single block abstracted procedure outweigh cost additional instructions needed manage control ow 
example illustrates minimal edit distance blocks necessarily yield greatest savings better ignoring matches 
obvious dynamic programming algorithm computing minimum edit distance modi ed straightforward way accommodate 
postprocessing phase instructions incur great control ow penalty 
improvement instructions match deemed pro table cost control ow management signi cantly lowers bene ts procedural abstraction partial matches 
example shown example call site resulting procedure need additional instructions set argument register identifying call site carry control transfer total instructions 
contrast original basic blocks shown contain total instructions 
despite signi cant partial match blocks pro table case procedure 
general procedural abstraction partial matches incurs large computational cost yields code size savings 
obtained similar results number variations scheme factoring common suxes pre xes blocks 
high computational cost transformation low bene produces compiler techniques code compaction 
decided include squeeze 

interactions classical optimizations code factoring considerable evidence appropriately controlled optimization yield signi cant reductions code size 
compiler folklore amount peephole optimization speed compilation process resulting reduction number instructions processed phases 
cooper mcintosh observe code size reductions due compiler optimizations experiments discussed section indicate enabling optimizations increase code size yield code size reduction average 
classical compiler optimizations aimed primarily increasing execution speed reductions size produce cases happy coincidental outcome transformations primary goal reduction execution time 
examples transformations situations lead increase code size include machine independent optimizations partial redundancy elimination procedure inlining shrink wrapping machine dependent optimizations instruction scheduling instruction cache optimization result insertion ops alignment purposes 
transformations lead code size reductions execution speed improvement primary goal optimization yield smaller size reductions possible 
example local factoring transformation discussed section instruction hoisted upward downward preferable hoist downward yield greater size reductions 
primary goal increasing execution speed prefer hoist upward hide latencies 
discussion take account interactions classical optimizations primary goal reduction execution time code factoring transformations primary goal reduction code size 
simple example consider code sequences basic blocks block block load sp load sp add add load sp add add add mul mul add add store sp store sp blocks di erent subjected procedural abstraction procedure 
compiler determines instructions block marked dead due code eliminating optimizations cause dead block eliminates blocks identical factored believe observation due wulf 

saumya debray procedure 
compiler better job optimization able nd free register block allows eliminate load instruction block blocks di erent abstracted procedure 
notice case compiler decision eliminate load instruction locally decision reduces code size instruction improve speed standpoint code compaction decision globally 
interactions give rise phase ordering problem speed oriented transformations 
possible way deal iterate transformations xpoint 
satisfactory solution transformations code factoring require lot code sequence comparisons identify repeated instruction sequences factored quite expensive iterating expensive impractical 
currently perform iteration 

experimental results evaluate ideas spec integer benchmarks embedded applications adpcm epic gsm mpeg dec mpeg enc rasta obtained mediabench benchmark suite ucla www cs ucla edu mediabench 
evaluated squeeze code obtained di erent compilers vendor supplied compiler cc invoked cc gnu compiler gcc version optimization level 
programs compiled additional ags instructing linker retain relocation information produce statically linked executables 
optimization level chosen compiler selected allow standard optimizations procedure inlining loop unrolling increase code size 
optimization level vendor supplied compiler cc carries local optimizations recognition common subexpressions global optimizations including code motion strength reduction test replacement split lifetime analysis code scheduling size increasing optimizations inlining integer multiplication division expansion shifts loop unrolling code replication eliminate branches 
similarly level optimization gcc compiler carries supported optimizations involve space speed trade particular loop unrolling function inlining carried 
baseline measurements code optimized compiler discussed unreachable code ops removed pro le guided code layout improve performance signi cantly carried compilers experiments carried 
eliminates library routines referenced program get linked program routines library excludes size reductions trivially obtained traditional compiler 
include pro le directed code layout baseline allow fair comparison requirement statically linked executables result fact squeeze relies presence relocation information control ow analysis 
tru unix linker ld refuses retain relocation information executables statically linked 
compiler techniques code compaction 
table code size improvements due di erent transformations transformation savings redundant computation elimination basic block region abstraction useless code elimination register save restore abstraction inter procedural optimizations squeeze carries optimization want resulting performance improvements unduly ate execution speed resulting executables 
obtain instruction counts rst disassemble executable les discard unreachable code op instructions 
eliminates library routines linked called op instructions may inserted compiler instruction scheduling alignment purposes 
identify unreachable code construct control ow graph entire program carry reachability analysis 
course constructing control ow graph discard unconditional branches 
reinsert necessary code transformations carried code layout just transformed code written 
get accurate counts generate nal code layout case compaction count total number instructions 
code size code size reductions achieved techniques summarized 
corresponding raw data debray 
shows ects squeeze code compiled vendor supplied compiler cc shows ects squeeze code compiled gnu compiler gcc 
columns labeled unoptimized refer programs compiled optimization level optimization carried serve point indicate code size reduction realized optimizations carried compiler columns labeled base refer code optimized appropriate level discussed unreachable code ops removed 
seen classical compiler optimizations compilers able achieve signi cant improvements code size compared unoptimized code cc obtains size reduction just average gcc able achieve average size reduction 
importantly seen optimized executables input squeeze able achieve signi cant reductions size 
cc compiled programs achieves average size reduction just gcc compiled programs average size reduction little 
greatest reduction size adpcm program smallest go program 
table gives breakdown average contribution di erent kinds code transformations code size reductions achieve 
classes transformations account savings 
third savings comes 
saumya debray compress gcc go ijpeg li ksim perl vortex adpcm epic gsm mpeg dec mpeg enc rasta average normalized code size unoptimized base squeezed compiler cc compress gcc go ijpeg li ksim perl vortex adpcm epic gsm mpeg dec mpeg enc rasta average normalized code size unoptimized base squeezed compiler gcc fig 

ects compaction code size normalized 
elimination redundant computations global pointer register gp comes ordinary procedural abstraction architecture speci abstraction register save restore sequences accounts useless code elimination accounts savings 
recall baseline programs unreachable code ops removed 
gure refers code subsequently useless due interprocedural optimization discussed section remainder savings arise due variety interprocedural optimizations 
measured extent basic blocks di erent sizes contribute savings due procedural abstraction 
small basic blocks savings block abstracted tend small likelihood nding similar blocks increasing total resulting savings large 
compiler techniques code compaction 
savings savings basic block size fig 

contribution procedural abstraction savings basic blocks di erent sizes 
opposite true large blocks basic block abstracted accrues signi cant savings likelihood nding similar identical blocks abstracted high 
distribution average savings observed benchmarks shown 
seen small blocks account signi cant amount savings savings comes blocks containing just instructions close comes blocks containing instructions 
savings generally drop number instructions increases large bump basic blocks size 
reason turns large number return blocks restore callee saved registers return address register memory deallocate stack frame return function 
actions require instructions processor 
contribution large basic blocks exceeding instructions length average quite small occasionally able blocks quite long 
gcc vortex benchmarks basic blocks instructions abstracted 
rasta benchmark blocks instructions long 
mentioned earlier experiments statically linked executables code library routines linked executable linker prior compaction 
possible library code compressible user code 
happen example libraries compiled di erent compilers compiler optimization levels 
desirable identify extent presence library code uences results 
example turns library code highly compressible user code results readily applicable executables statically linked 
instrumented squeeze record addition deletion code run function size change associated 
classical optimizations implemented squeeze straightforward 
procedural abstraction approach 
suppose di erent instances particular code fragment abstracted procedure resulting net savings code size function containing instances credited savings instructions necessarily integral quantity 
list functions user code obtained modi 
saumya debray compress gcc go ijpeg li ksim perl vortex adpcm gsm mpeg dec mpeg enc rasta user code libraries fig 

contributions code size reduction user code versus libraries 
ed version lcc compiler fraser hanson estimate total size user code code savings attributable 
measurements account indirect ects having library code available inspection improved precision data ow analyses may give rise additional opportunities optimization 
information useful obtaining qualitative estimates uence library code numbers 
results shown 
bars labeled user code represent fraction instructions user code relative total number user code instructions deleted process code compaction labeled libraries give corresponding gures library code 
user code libraries amount reduction code size typically ranges average reduction user code library code 
programs li perl vortex adpcm user code noticeably compressible libraries go gsm rasta libraries compressible 
general user library code comparable contribution code size reduction measured 
code speed intuitively expects programs resulting code compaction techniques described slower original code primarily additional function calls resulting procedural abstraction occurs 
careful consideration indicates situation may simple analysis suggests number reasons 
code size reduction due aggressive interprocedural optimizations improve execution speed 
second transformations pro le directed code layout need large ect code size signi cant positive ect speed 
hand superscalar processor alpha slow downs occur compressed code reasons procedural abstraction due elimination ops inserted instruction scheduler order numbers refer control ow graph prior code layout unconditional branches added linearizing graph 
compiler techniques code compaction 
align instructions increase number instructions issued cycle 
determine actual ect transformations benchmarks compared execution times original optimized executables resulting application squeeze executables 
execution pro les form basic block execution counts obtained program pixie fed back squeeze code compaction 
spec benchmarks pro led spec training inputs subsequently timed spec inputs 
remaining benchmarks input pro ling subsequent timing 
timings obtained lightly loaded compaq alpha workstation mhz alpha processor split primary direct mapped cache kb instruction data cache kb chip secondary cache mb chip secondary cache mbytes main memory running tru unix 
results shown 
corresponding raw data debray 
case execution time measured smallest time runs 
columns labeled original refer execution times inputs optimized appropriate level compiler discussed earlier elimination unreachable code ops 
provided point 
columns labeled base refer executables obtained removing unreachable code ops original executables performing pro le directed code layout 
execution times executables produced squeeze correspond columns labeled squeezed 
results timing experiments indicate means squeezed code slower original code 
benchmarks squeezed code runs signi cantly faster original 
example compress benchmark compiled cc squeezed executable faster base original executables gcc faster base original executables 
ksim compiled cc squeezed executable faster base faster original gcc faster base original 
perl compiled cc faster base faster original gcc faster base original 
programs su er slow downs result code compaction vortex epic gcc compiler 
slows 
reasons slow downs discussed section 
set benchmarks considered average speedup compared base original programs cc compiled executables executables obtained gcc 
words code compaction yields signi cant speed improvements compressed code performs favorably performance original code enhanced pro le guided code layout 
reasons explored section generally benchmarks squeezed code experiences signi cant decreases number instruction cache misses average amount instruction level parallelism sustained 

saumya debray compress gcc go ijpeg li ksim perl vortex adpcm epic gsm mpeg dec mpeg enc rasta average normalized execution time original base squeezed compiler cc compress gcc go ijpeg li ksim perl vortex adpcm epic gsm mpeg dec mpeg enc rasta average normalized execution time original base squeezed compiler gcc fig 

ects compaction execution time normalized 
low level dynamic behavior better understand dynamic behavior programs subjected code compaction examined various aspects low level execution characteristics 
results summarized obtained hardware counters processor case smallest runs program 
total instructions executed 
code size reductions code compaction come sources interprocedural optimization code factoring 
interprocedural optimizations reduce number instructions executed example elimination unnecessary gp register computations elimination ops inserted alignment instruction scheduling dead code elimination inlining procedures called single call site 
optimizations particular elimination unreachable code ect number instructions executed 
code factoring hand leads execution additional compiler techniques code compaction 
compress gcc go ijpeg li ksim perl vortex adpcm epic gsm mpeg dec mpeg enc rasta total instructions executed cc compress gcc go ijpeg li ksim perl vortex adpcm epic gsm mpeg dec mpeg enc rasta total instructions executed gcc instructions executed normalized compress gcc go ijpeg li ksim perl vortex adpcm epic gsm mpeg dec mpeg enc rasta instruction cache misses cc compress gcc go ijpeg li ksim perl vortex adpcm epic gsm mpeg dec mpeg enc rasta instruction cache misses gcc instruction cache misses normalized compress gcc go ijpeg li ksim perl vortex adpcm epic gsm mpeg dec mpeg enc rasta instructions cycle cc compress gcc go ijpeg li ksim perl vortex adpcm epic gsm mpeg dec mpeg enc rasta instructions cycle gcc instruction level parallelism original base squeezed key fig 

low level dynamic behavior 
branch instructions procedure calls returns results increase number instructions executed 
shows relative number instructions executed original squeezed programs compared base program 
expect di erence original base programs base program unreachable code ops eliminated base program executes fewer instructions original 
di erence due entirely eliminated ops typically large ranging averaging 
interestingly consider code generated squeeze nd programs squeezed version 
saumya debray executes fewer instructions base programs 
programs reduction instructions executed resulting optimizations squeeze set dynamic increases due factoring 
programs ects code factoring outweigh due optimizations result net increase number instructions executed 
nd benchmarks considered squeezed versions code obtained cc execute fewer instructions average base versions gcc compiled binaries execute little instructions average 
instruction cache misses 
modern cpus signi cantly faster memory delivering instructions major bottleneck 
high instruction cache hit rate essential performance 
primary instruction caches order fast tend relatively small low associativity 
advantageous lay basic blocks program way frequently executed blocks positioned close lead cache con icts pettis hansen 
code factoring undo ects pro le directed code layout pulling code fragment procedure positioned close call site 
problem arises example instances repeated code fragment close code fragments frequently executed 
code fragments factored procedure frequently executed call sites resulting procedure may possible lay code way positions body procedure close call sites 
lead increase instruction cache misses 
shows ect code compaction instruction cache misses 
cc compiled programs compress benchmark experiences large increase number instruction cache misses result factoring 
binaries obtained gcc programs ijpeg vortex su er large increases number cache misses gcc go experience smaller noticeable increases 
number instruction cache misses goes remaining programs cases notably compress li ksim epic mpeg dec quite dramatically 
squeezed programs incur fewer instruction cache misses average cc compiled binaries fewer misses gcc compiled binaries corresponding base programs 
instruction level parallelism 
alpha processor experiments run superscalar machine execute instructions cycle provided various scheduling constraints satis ed 
example integer oating point instructions issued cycle instruction group simultaneously issued instructions try access memory access functional unit 
instructions fetched groups group examined opportunities multiple issues evaluating extent satisfy constraints 
means possible plausible code transformation deletion op instruction alter instruction sequence way opportunities multiple instruction issues reduced dramatically corresponding loss performance conversely judicious insertion ops lead increase level instruction level parallelism exploited 
compiler techniques code compaction 
address problem squeeze carries instruction scheduling transformations applied nal code layout determined 
squeeze eliminates ops inserted compiler scheduling alignment purposes potential signi cant loss instruction level parallelism code produces 
evaluate case measured average number instructions issued cycle various executables 
results shown 
seen elimination ops incurs price base program average number instructions issued cycle slightly smaller cc gcc original program 
instruction scheduler squeeze able overcome problem programs tested able attain higher number instructions cycle 
average instructions issued cycle squeezed programs compared base programs improves cc compiled binaries gcc compiled binaries 
summary 
shows benchmarks vortex epic compiled gcc su er slowdown result code compaction 
low level execution characteristics indicate possible reasons 
programs code compaction causes increase total number instructions executed programs 
programs generally able compensate improvements vortex su ers increase instruction cache misses epic su ers reduction average number instructions issued cycle 
programs incur degradations dynamic execution characteristics able compensate improvements characteristics 
example compress cc ijpeg gcc su er dramatic increases number instruction cache misses able eke improvements speed due combination reduction total number instructions executed ijpeg compiled gcc increase average number instructions issued cycle 
ects code factoring shows ect code factoring code size execution time 
raw data debray 
graphs compare squeeze performing code transformations code factoring squeeze code factoring enabled 
seen factoring reduces size programs 
interesting aspect comparison elimination code due various optimizations squeeze ect reducing apparent ecacy code factoring code factored eliminated useless unreachable 
result greater code shrinking ects classical optimizations smaller nd bene ts due factoring 
smallest code unit consider procedural abstraction basic block approach pick instruction sequences subparts block 
comparison sux tree approaches cooper mcintosh able repeated instruction sequences subsequences block 
despite limitation approach code 
saumya debray compress gcc go ijpeg li ksim perl vortex adpcm epic gsm mpeg dec mpeg enc rasta average cc compress gcc go ijpeg li ksim perl vortex adpcm epic gsm mpeg dec mpeg enc rasta average gcc code size normalized compress gcc go ijpeg li ksim perl vortex adpcm epic gsm mpeg dec mpeg enc rasta average cc compress gcc go ijpeg li ksim perl vortex adpcm epic gsm mpeg dec mpeg enc rasta average gcc execution time normalized factoring factoring key fig 

relative impact code factoring code size execution time 
factoring relative size reductions obtain factoring essentially cooper mcintosh 
possible explanation ability subsequences basic block di erence large basic blocks proportion blocks generally tends small programs 
expect factoring causes increase number instructions executed 
average results increase execution time cc compiled binaries gcc compiled binaries 
gcc compiled binaries experience signi cant slow downs vortex slowing epic perl 

article focuses problem code compaction yield smaller executables 
describes system approach problem aggressive interprocedural optimization procedural abstraction repeated code fragments yields signi cantly greater reductions code size achieved date 
identi cation abstraction repeated code compiler techniques code compaction 
fragments departs classical sux tree approaches 
uses information available compilers control ow graph dominator postdominator trees 
treat program simple linear sequence instructions exible treatment code fragments may considered equivalent 
simpli es implementation sets framework code compaction exible treatment code fragments considered equivalent 
results system able obtain considerably greater compaction optimized code previous approaches incurring signi cant performance penalties 
appendix local register renaming algorithm suppose want rename registers basic block possible identical block pseudocode algorithm squeeze shown 
simplicity exposition assume instructions form reg reg op reg ith operand instruction op 
assume operands source operands operand destination operand 
addition instruction elds keep track operand register renaming 
elds undo renaming necessary initialized 
algorithm maintains global arrays keep track register moves inserted entry exit block respectively renaming successful 
element arrays initialized 
main routine carries renaming illustrated 
basic idea instruction try rename operands identical corresponding instruction violating semantic constraints 
done total number move instructions inserted block exceeds savings obtained procedural abstraction block renaming abandoned 
case control transferred label renaming instruction block undone 
pseudocode renaming individual operands shown 
idea record original value operand appropriate eld instruction renamed rename operand propagate renaming forward basic block register renamed rede ned block reached 
grateful anders lindgren johan iar systems sweden pointing errors earlier version 
due nathaniel mcintosh helpful discussions pointing ucla mediabench benchmark programs 
comments anonymous reviewers helpful improving contents article 

saumya debray function return fail livein fr live entry fr live entry fr callee saved register saved function containing forbidden fr callee saved ins reg reg op reg ins reg reg op reg ins ins reg reg reg livein reg goto reg reg ins ins livein fail goto od de nition ins reaches de nition ins reach goto reg reg ins ins forbidden fail goto ins ins goto livein livein od bsr added return success ins ins ins op ins ins ins op ins ins ins op ins od return fail fig 

algorithm local register renaming 
compiler techniques code compaction 
function ins ins forbidden ins op ins op return success forbidden return fail ins ins op instruction ins block op return fail op od op break od return success function multiple predecessors create new basic block redirect edges entering enter add edge insert instruction od multiple successors create new basic block redirect edges add edge insert instruction od fig 

pseudocode operand replacement move insertion 

saumya debray aho sethi ullman 
compilers principles techniques tools 
addison wesley reading mass baker 
theory parameterized pattern matching algorithms applications extended 
proc 
acm symposium theory computing 
acm press new york 
baker manber 
deducing similarities java sources bytecodes 
proc 
usenix annual technical conference 
usenix berkeley ca 
bene wolfe 
fast asynchronous hu man decoder compressed code embedded processors 
proc 
international symposium advanced research asynchronous circuits systems 
ieee computer society washington cooper mcintosh 
enhanced code compression embedded risc processors 
acm conference programming language design implementation 
acm press new york 
debray evans muth de 
compiler techniques code compaction 
tech 
rep dept computer science university arizona 
mar ernst evans fraser lucco proebsting 
code compression 
acm conference programming language design implementation 
acm press new york franz 
adaptive compression syntax trees iterative dynamic code optimization basic technologies mobile object systems 
mobile object systems programmable internet vitek tschudin eds 
number springer lecture notes computer science 
springer heidelberg germany 
tech 
report department information computer science university california irvine 
franz kistler 
slim binaries 
commun 
acm dec 
fraser proebsting 
custom instruction sets code compression 
unpublished manuscript 
research microsoft com papers pldi ps 
fraser myers wendt 
analyzing compressing assembly code 
proc 
acm sigplan symposium compiler construction 
vol 

acm press new york 
fraser hanson 
retargetable compiler design implementation 
addison wesley reading mass garey johnson 
computers intractability guide theory np completeness 
freeman new york knoop steffen 
optimal code motion theory practice 
acm trans 
program 
lang 
syst 
july 
muchnick 
advanced compiler design implementation 
morgan kaufman san francisco ca 
muth debray 
alto link time optimizer dec alpha 
tech 
rep dept computer science university arizona 
dec appear software practice experience 
pettis hansen 
pro le guided code positioning 
acm conference programming language design implementation 
acm press new york 
proebsting 
optimizing ansi interpreter superoperators 
proc 
symp 
principles programming languages 
acm press new york 
van de 
code compaction bibliography 
www win tue nl cs pa html 

compacting object code parameterized procedural abstraction 
thesis dept computing science university victoria 
