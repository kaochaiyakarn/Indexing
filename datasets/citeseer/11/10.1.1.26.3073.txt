hierarchical statistical language models experiments domain adaptation introduce hierarchical statistical language model represented collection local models plus general sentence model 
provide example mixes trigram general model pfsa local model class decimal numbers described terms sub word units graphemes 
model practically extends vocabulary model infinite size better performance compared word model 
domain language model adaptation experiments show local models encode linguistic information trained may ported new language models re estimation 

language models important component speech recognizers 
gram models currently widely despite obvious shortcomings 
models reached limits variations proposed years provided improvement 
language structured hierarchical way able take advantage structural constraints progress statistical language modelling hard achieve 
propose model inherently structural allowing incorporating linguistic knowledge better model context integrating sub language models 
model propose building block strategy similar ways models literature 
assumes language models derived certain classes constituents broad sense independently rest linguistic data modelled 
constituent model incorporate constituent models linguistic units 
example model currency phrases may sub model numbers 
topmost model effectively connects various blocks block simple single word 
intuitively hierarchical statistical language model collection sub models organized hierarchy 
sub model generate strings symbols alphabet insert output sub models lower hierarchy 
want model probabilistic requirement sub model returns probability word word sequence cover 
usual restrictions may imposed reasons efficiency availability training data estimating models parameters 
due modularity model restrictions separately sub model need global conventional models 
reason basic units james allen university rochester usa james cs rochester edu models need words 
model employing sub word units generate infinite vocabulary 
contrast approaches similar fact language models currently finite coverage 
hierarchical framework facilitates linguistic structural information gram models allows incremental adaptation various sub models re training model 
results domain language model adaptation experiment wsj corpus 
goal prove local models may capture linguistic generalizations transferred adaptation 
model layers trigram top layer probabilistic state automaton pfsa local model sublanguage numbers infinite 
adaptation trigram component adapted interpolation sublanguage model kept unchanged 
show adapted model compares favorably word language model adapted interpolation 

model model proposed call hierarchical hybrid statistical language model thought generalization class phrase gram models 
traditional class models assume simple uniform unigram distribution words class 
modelling done level word phrases accounted usually fact provides rudimentary linguistic model pointed 
believe beneficial allow richer structure inside class model sophisticated probability distributions inside classes 
example class modelled pfsa probabilistic context free grammar pcfg small raise questions efficiency 
note pcfgs may approximated 
gram models 
regular expressions immediately convertible fsas 
call model hybrid don require components model architecture appropriate structure sublanguage tries model amount training data available 
practical reasons ask return probability word word sequence cover model remains probabilistic 
course hybrid models need special decoding mechanisms think approach feasible care taken sub model lends efficient decoding 
input utterance segmented sequence tag stands subsequence ut words trivial case subsequence composed word identify tag word 
hierarchical model likelihood segmentation ti 
likelihood word sequence tag local probability models pt ti expressed terms models lower hierarchy 
lends model hierarchical structure 
general segmentations level corresponding word sequence case total probability obtained summing segmentations 
restrict model segmentation 
modelling doesn level word 
may constructs better modelling done sub word level 
exemplify model numbers text principle applied entities 
direction pursue modelling proper names sub word level syllable 
general purpose language models sub word units proved word ones retain rich statistical information frequent words infrequent ones include components smaller units 
effect increasing coverage language model just small size increase alleviating data sparseness problem model 
local model possible gather training data different sources making reliable estimation 
architecture local models obvious cases designed humans 
models linguistically sound allow intuitive parameterization 
automatic clustering phrases syntactic semantic categories assigned parser 
distributional clustering class phrases done models represented layer hierarchical models 
construction model proceeds identifying training data tokenizer parser words phrases belong certain class replace occurrences class specific tags 
top level model trained usual way tokenized corpus 
data collected class training local model class 

domain adaptation simple domain adaptation technique linear interpolation 
adapted model mixture components general model trained large background corpus domain specific model trained small adaptation corpus 
detailed description adaptation techniques refer reader 
technique proved useful accounting variations topic style 
cross domain adaptation background corpus adaptation corpus dissimilar results especially level trigrams 
restrict easier problem goal improving adaptation techniques showing local models portable things equal adapted hierarchical model performs better conventional adapted model 
train background corpus simple hierarchical model composed trigram general model local pfsa model sub word units 
adapt explained general component small adaptation corpus 
local model transferred change architecture parameters 

experimental results performed experiments newspaper text wsj 
training background model selected sentences words wsj corpus 
adaptation testing selected wsj corpus 
expect fairly similar background corpus style topics different quite vocabulary differences 
half wsj corpus sentences words adaptation 
third remaining data held development rest sentences words testing seen 
known adaptation linear interpolation technique helps adaptation data small adaptation available 
tested performance models full adaptation corpus smaller subset 

models adaptation procedure baseline model word trigram model witten bell discounting cutoffs built cmu cambridge slm toolkit 
background model word trigram vocabulary comprising frequent words 
vocabulary expanded words appear frequent words adaptation corpus 
basic vocabulary fixed word models top level components hierarchical models vocabulary included non number words basic vocabulary plus tag class numbers 
adaptation data trigram estimated interpolated background model give baseline adapted model 
interpolation weights optimized held data expectation maximization algorithm 
app pp adapt interp adapt interp adapt interp adapt interp baseline reduction table app pp results word baseline hierarchical model relative app reductions achieved 
models layers trigram top layer pfsa model sublanguage numbers infinite 
adaptation trigram component adapted interpolation trigram model estimated tokenized adaptation data sublanguage model kept unchanged 
adaptation conditions trigram component word model 
local model describes decimal numbers level basic units characters 
identified decimal numbers regular expression 
built equivalent state deterministic fsa 
automaton turned pfsa probability model parameters trained numbers training data including basic vocabulary 
fsa model class numbers 
omit arc labels note arcs labeled multiple symbols 
model assigns numbers estimate probability random number estimate normalized training data 
model takes account fact finite amount test data certain proportion words numbers 
note probability depends fact word seen test data number number particular numbers encountered training data receive probability ones encountered 
comparison probability model conventional see 

results hierarchical model recognize numbers basic vocabulary perplexity pp compare performance models 
computed pp values models give curiosity reader 
actual comparisons compare ones 
terms adjusted perplexity app measure introduced adjusts value pp quantity dependent number unknown words test set number occurrences 
compute app value full test set compare models different vocabularies 
refer reader details evaluation procedure 
main results depicted table 
significance headings follows models trained background corpus adapt models trained adaptation corpora 
marked models trained smaller adaptation corpus ones trained full adaptation corpus interp adapted models resulting interpolation appropriate adapt models described 
models performed better word models 
adapted model brought significant app improvements course better coverage compared baseline oov rate reduced relative 
expected measures benefit larger adaptation data 
reductions may large numbers account test data quite impressive 
interesting see largest reduction observed models trained small adaptation corpora 
suggests local models hierarchical model reasonable initial language model new domain little training data background corpora similar target domain requires training data achieve performance word model 
interesting test hypothesis conjunction techniques building initial models 
results show local model able capture generalizations data ported effectively new models 
part improvements due better context modelling words numbers going predicted class tag number 

introduced hierarchical statistical language model generalizes previous variations gram models 
hinted advantages expect provided example mixes different models trigram general model pfsa local model class decimal numbers described terms sub word units graphemes 
model practically extends vocabulary model infinite size better performance compared word model 
experimented domain language model adaptation showed hierarchical models compare favorably word language models 
suggests local models encode linguistic information may ported new language models re estimation 
simple sub models developed integrated easily 
structural information introduced model expect additional improvements 
spoken language recognition experiments scheduled near 

supported onr research 
darpa research 

air force rome labs research contract 
nsf research 
iri 


federico 
dynamic language models interactive speech applications 
proc 
eurospeech pp 


clarkson rosenfeld statistical language modelling cmu cambridge toolkit proc 
eurospeech pp 


federico language model adaptation 
computational models speech pattern processing keith pointing ed nato asi series springer verlag 

dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society series 

allen 
evaluating hierarchical hybrid statistical language models 
proc icslp 

allen 
rapid language model development new task domains 
proc 
lrec 

automatic training stochastic finitestate language models speech understanding 
proc 
icassp pp 


ward 
language model combining trigrams stochastic context free grammars 
proc 
icslp pp 


grefenstette tapanainen 
word sentence 
problems tokenization 
proc 
rd int 
conf 
comp 
lexicography pp 


guyon pereira 
design linguistic postprocessor variable memory length markov models 
proc 
rd int 
conf 
document anal 
recognition pp 


meteer rohlicek 
statistical language modeling combining gram context free grammars 
proc 
icassp vol 
ii pp 


moore appelt dowding moran 
combining linguistic statistical knowledge sources natural language processing atis 
proc 
spoken language systems technology workshop pp 

morgan kaufmann 

nasr est chet de mori 
language model combining grams stochastic finite state automata 
proc 
eurospeech pp 


pereira wright 

finite state approximation phrase structure grammars 
emmanuel roche yves schabes editors finite state language processing 
mit press cambridge pp 

riccardi pieraccini 
stochastic automata language modeling 
computer speech language 

ries bu waibel 
class phrase models language modeling 
proc 
icslp pp 


measuring perplexity language models speech recognizers 
technical report bbn laboratories 

seneff linguistic hierarchies speech understanding 
proc 
icslp pp 


analyzing improving statistical language models speech recognition 
phd thesis simon fraser university vancouver canada 

wang mahajan huang 
unified context free grammar gram model spoken language processing 
proc 
icassp 

ward 
class language model speech recognition 
proc 
icassp pp 

