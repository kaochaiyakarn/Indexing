journal artificial intelligence research submitted published cached sufficient statistics efficient machine learning large datasets andrew moore cs cmu edu mary soon lee cs cmu edu school computer science robotics institute carnegie mellon university pittsburgh pa introduces new algorithms data structures quick counting machine learning datasets 
focus counting task constructing contingency tables approach applicable counting number records dataset match conjunctive queries 
subject certain assumptions costs operations shown independent number records dataset loglinear number non zero entries contingency table 
provide sparse data structure adtree minimize memory 
provide analytical worst case bounds structure models data distribution 
empirically demonstrate tractably sized data structures produced large real world datasets sparse tree structure allocates memory counts zero allocating memory counts deduced counts bothering expand tree fully near leaves 
show adtree accelerate bayes net structure finding algorithms rule learning algorithms feature selection algorithms provide number empirical results comparing adtree methods traditional direct counting approaches 
discuss possible uses adtrees machine learning methods discuss merits adtrees comparison alternative representations kd trees trees frequent sets 

caching sufficient statistics computational efficiency important concern machine learning algorithms especially applied large datasets fayyad mannila piatetsky shapiro fayyad uthurusamy real time scenarios 
earlier showed kd trees multiresolution cached regression matrix statistics enable fast locally weighted instance regression moore schneider deng :10.1.1.27.7911
attempt accelerate predictions symbolic attributes kind kd tree splits dimensions nodes 
machine learning algorithms operating datasets symbolic attributes need frequent counting 
applicable online analytical processing olap applications data mining operations large datasets multidimensional database access datacube operations harinarayan rajaraman ullman association rule learning agrawal mannila srikant toivonen verkamo accelerated fast counting 
establishing notation 
data set records attributes 
attributes called am value attribute fl ai access foundation morgan kaufmann publishers 
rights reserved 
moore lee kth record small integer lying range called arity attribute gives example 
record record record record record record arity attributes aa simple dataset example 
records attributes 
queries query set attribute value pairs left hand sides pairs form subset fa am arranged increasing order index 
examples queries dataset notice total number possible queries pi 
attribute appear query values may take may omitted equivalent giving don care value 
counts count query denoted query simply number records dataset matching attribute value pairs query 
example dataset find contingency tables subset attributes associated contingency table denoted ct 
table row possible sets values row corresponding records count 
example dataset attributes contingency tables exist depicted 
cached sufficient statistics efficient machine learning ct ct ct ct ct ct ct ct possible contingency tables dataset 
conditional contingency table written ct contingency table subset records dataset match query right symbol 
example ct contingency tables variety machine learning applications including building probability tables bayes nets evaluating candidate conjunctive rules rule learning algorithms quinlan clark niblett 
desirable able perform counting efficiently 
prepared pay time cost building caching data structure easy suggest mechanism doing counting constant time 
possible query precompute contingency table 
total amount numbers stored memory data structure pi humble dataset revealed 
real dataset attributes medium arity fifteen binary attributes far large fit main memory 
retain speed precomputed contingency tables incurring intractable memory demand 
subject 

cache reduction dense adtree caching sufficient statistics describe adtree data structure represent set possible counts 
initial simplified description obvious tree representation yield immediate memory savings provide opportunities 
se tree similar data structure 
moore lee cutting zero counts redundant counts 
structure shown 
adtree node shown rectangle child nodes called vary nodes shown ovals 
represents query stores number records match query field 
vary child child values attribute kth child represents query vary parent additional constraint vary vary vary 



vary vary vary 
vary 




top adtree described text 
notes regarding structure ffl drawn diagram description query leftmost second level explicitly recorded 
contents simply count set pointers vary children 
contents vary node set pointers 
ffl cost looking count proportional number instantiated variables query 
example look follow path tree vary vary vary 
count obtained resulting node 
ffl notice node adn vary parent adn children vary vary 
vary am necessary store vary nodes indices information obtained path tree 
cached sufficient statistics efficient machine learning cutting nodes counts zero described tree sparse contain exactly pi nodes 
sparseness easily achieved storing null node query matches zero records 
specializations query count zero appear tree 
datasets reduce number numbers need stored 
example dataset previously needed numbers represent contingency tables need numbers 

cache reduction ii sparse adtree easy devise datasets benefit failing store counts zero 
suppose binary attributes records kth record bits binary representation query count zero tree contains nodes 
reduce tree size despite take advantage observation counts stored tree redundant 
vary node adtree stores subtrees subtree value find common values call mcv store null place subtree 
remaining gamma subtrees represented 
example simple dataset 
vary node records values common mcv field 
appendix describes straightforward algorithm building adtree 
see section possible build full exact contingency tables give counts specific queries time slightly longer full adtree section 
examine memory consequences representation 
appendix shows binary attributes attributes records number nodes needed store tree bounded worst case 
contrast amount memory needed dense tree section worst case 
notice mcv value context dependent 
depending constraints parent nodes mcv 
context dependency provide dramatic savings frequently case correlations attributes 
discussed appendix 
computing contingency tables sparse adtree adtree wish able quickly construct contingency tables arbitrary set attributes fa notice conditional contingency table ct query built recursively 
build ct query ct query 
ct query moore lee null mcv null mcv null null mcv null mcv mcv vary vary vary mcv count vary vary mcv vary vary mcv sparse adtree built dataset shown bottom right 
common value subtree vary child root node null 
vary nodes common child set null child common depends context 
example build ct dataset build ct ct combine 
ct ct ct example numbers contingency tables combined recursively form larger contingency tables 
building conditional contingency table adtree need explicitly specify query condition 
supply adtree implicitly equivalent information 
algorithm cached sufficient statistics efficient machine learning adn vn vary subnode adn 
mcv vn mcv 
mcv adn subnode vn 
ct fa adn 
calculated explained return concatenation ct ct base case recursion occurs argument empty case return element contingency table containing count associated current adn 
omission algorithm 
iteration unable compute conditional contingency table mcv subtree deliberately missing section 

take advantage property contingency tables ct query ct query value ct query computed algorithm calling fa adn missing conditional contingency table algorithm computed row wise subtraction fa adn gamma mcv ct frequent sets agrawal traditionally learning association rules computing counts 
mannila toivonen employs similar subtraction trick calculates counts frequent sets 
section discuss strengths weaknesses frequent sets comparison adtrees 
complexity building contingency table cost computing contingency table 
consider theoretical worstcase cost computing contingency table attributes arity note cost unrealistically pessimistic contingency tables sparse discussed 
assumption attributes arity simplify calculation worst case cost needed code 
moore lee contingency table attributes entries 
write cost computing contingency table 
top level call calls build contingency tables gamma attributes gamma calls build ct query kg mcv final call build ct query 
gamma subtractions contingency tables require gamma numeric subtractions 
kc gamma gamma gamma solution recurrence relation gamma gamma cost loglinear size contingency table 
comparison cached data structure simply counted dataset order build contingency table need nr operations number records dataset 
cheaper standard counting method interested large datasets may 
case method order magnitude speedup say contingency table binary attributes 
notice cost independent total number attributes dataset depends smaller number attributes requested contingency table 
sparse representation contingency tables practice represent contingency tables multidimensional arrays tree structures 
gives slow counting approach adtree approach substantial computational advantage cases contingency table sparse zero entries 
shows sparse contingency table representation 
mean average case behavior faster worst case contingency tables large numbers attributes high arity attributes 
experiments section show costs rising slowly nk gamma increases 
note sparse representation worst case min nr nk gamma maximum possible number non zero contingency table entries 

cache reduction iii leaf lists introduce scheme reducing memory 
worth building adtree data structure small number records 
example suppose records binary attributes 
analysis appendix shows worst case adtree require nodes 
computing contingency tables resulting adtree records faster conventional counting approach merely require retain dataset memory 
aside concluding adtrees useful small datasets leads final method saving memory large adtrees 
adtree node fewer min records expand subtree 
maintains list pointers original dataset explicitly listing records match current 
list pointers called leaf list 
gives example 
cached sufficient statistics efficient machine learning null null null null null null null null ct right hand sparse representation contingency table left 
leaf lists minor major consequences 
minor consequence need include straightforward change contingency table generating algorithm handle leaf list nodes 
minor alteration described 
major consequence dataset retained main memory algorithms inspect leaf lists access rows data pointed leaf lists 
second major consequence adtree may require memory 
documented section worst case bounds provided appendix 
adtrees machine learning see section adtree structure substantially speed computation contingency tables large real datasets 
machine learning statistical algorithms take advantage 
provide examples feature selection bayes net scoring rule learning 
algorithms benefit example stepwise logistic regression text classification 
decision tree quinlan breiman friedman olshen stone learning may benefit 
examine ways speed nearest neighbor memory queries adtrees 

depends cost initially building adtree amortized runs decision tree algorithm 
repeated runs decision tree building occur wrapper model feature selection john kohavi pfleger intensive search tree structures traditional greedy search quinlan breiman moore lee see rows null mcv vary mcv vary mcv null mcv null mcv null mcv see rows row vary mcv vary mcv see rows adtree built leaf lists min 
node matching fewer records expanded simply records set pointers dataset shown right 
datasets experiments datasets table 
dataset supplied continuous attributes discretized ranges 
adtrees feature selection attributes output wish predict interesting ask subset attributes best predictor output distribution datapoints reflected dataset 
kohavi 
ways scoring set features particularly simple information gain cover thomas 
attribute wish predict set attributes inputs 
set possible assignments values write assign kth assignment 
gamma jxj assign assign assign number records entire dataset gammax log counts needed computation read directly ct 
searching best subset attributes simply question search attribute sets size specified user 
simple example designed test cached sufficient statistics efficient machine learning name num 
num 
records attributes adult small adult income dataset placed uci repository ron kohavi kohavi 
contains census data related job wealth nationality 
attribute arities range 
uci repository called test set 
rows missing values removed 
adult kinds records different data 
training set 
adult adult adult concatenated 
census larger dataset different census provided ron kohavi 
census data census addition extra high arity attributes 
birth records concerning wide number readings factors recorded various stages pregnancy 
attributes binary attributes sparse values false 
synth synthetic datasets entirely binary attributes generated bayes net 
table datasets experiments 
counting methods practical feature selector need penalize number rows contingency table high arity attributes tend win 
adtrees bayes net structure discovery possible bayes net learning tasks entail counting speeded adtrees 
experimental results particular example scoring structure bayes net decide matches data 
maximum likelihood scoring penalty number parameters 
compute probability table associated node 
write parents parent attributes node write set possible assignments values parents 
maximum likelihood estimate estimated estimates node probability tables read ct parents 
step scoring structure decide likelihood data probability tables computed penalize number parameters network penalty likelihood increase time link added moore lee bayes net generated synth datasets 
kinds nodes 
nodes marked triangles generated 
square nodes deterministic 
square node takes value sum parents takes value 
circle nodes probabilistic functions single parent defined parent parent 
provides dataset fairly sparse values interdependencies 
network 
penalized log likelihood score friedman yakhini gamma params log asgn asgn log asgn params total number probability table entries network 
search structures find best score 
experiments stochastic hill climbing operations random addition removal network link randomly swapping pair nodes 
operation necessary allow search algorithm choose best ordering nodes bayes net 
stochastic searches popular method finding bayes net structures friedman yakhini 
probability tables affected nodes recomputed step 
shows bayes net structure returned bayes net structure finder iterations hill climbing 
adtrees rule finding output attribute distinguished value rule finders search conjunctive queries form assign cached sufficient statistics efficient machine learning relationship pars class pars relationship sex pars relationship class capital gain pars class hours week pars relationship class sex marital status pars relationship sex education num pars class capital loss pars class age pars marital status race pars relationship education num education pars relationship education num pars relationship hours week education num native country pars education num race pars occupation pars class sex education score search took seconds 
output bayes structure finder running adult dataset 
score contribution sum equation due specified attribute 
np number entries probability table specified attribute 
find query maximizes estimated value assign assign assign avoid rules significant support insist assign number records matching query threshold min experiments implement brute force search looks possible queries involve user specified number attributes build ct turn choose tables look rows table queries greater minimum support min return priority queue highest scoring rules 
instance adult dataset best rule predicting class attributes score private education num married civ spouse capital loss class 
experimental results examine memory required adtree datasets 
table shows example adult dataset produced adtree nodes 
tree required megabytes memory 
adult datasets size tree varied approximately linearly number records 
specified experiments section adult datasets leaf lists 
birth synthetic datasets leaf lists size min default 
birth dataset large number sparse attributes required modest megabytes store tree magnitudes worst case bounds 
synthetic datasets tree size increased sublinearly dataset size 
indicates moore lee dataset nodes megabytes build time census census adult adult adult birth syn syn syn syn syn table size adtrees various datasets 
number attributes 
number records 
nodes number nodes adtree 
megabytes amount memory needed store tree 
build time number seconds needed build tree nearest second 
dataset gets larger novel records may cause new nodes appear tree frequent 
table shows costs performing iterations bayes net structure searching 
experiments performed mhz pentium pro machine megabytes main memory 
recall bayes net iteration involves random change network requires recomputation contingency table exception iteration nodes computed 
means time run iterations essentially time compute contingency tables 
adult datasets advantage adtree conventional counting ranges factor 
unsurprisingly computational costs adult increase sublinearly dataset size adtree linearly conventional counting 
computational advantages sublinear behavior pronounced synthetic data 
table examines effect leaf lists adult birth datasets 
adult dataset byte size tree decreases factor leaf lists increased 
computational cost running bayes search increases indicating worth tradeoff memory scarce 
bayes net scoring results involved average cost computing contingency tables different sizes 
results tables savings fixed size attribute sets easier discern 
tables give results feature selection rule finding algorithms respectively 
biggest savings come small attribute sets 
computational savings sets size particularly interesting counts cached straightforward methods needing tricks 
cases see large savings especially birth data 
datasets larger numbers rows course reveal larger savings 
cached sufficient statistics efficient machine learning dataset adtree time regular time speedup factor census census adult adult adult birth syn syn syn syn syn table time seconds perform hill climbing iterations searching best bayes net structure 
adtree time time adtree regular time time taken conventional probability table scoring method counting dataset 
speedup factor number times adtree method faster conventional method 
adtree times include time building adtree place table 
typical adtrees build tree able data analysis operations building cost amortized 
case including tree building cost minor impact results 
adult birth min mb nodes build search mb nodes build search secs secs secs secs table investigating effect min parameter adult dataset birth dataset 
mb memory adtree 
nodes number nodes adtree 
build secs time build adtree 
search secs time needed perform iterations bayes net structure search 
moore lee adult birth number number adtree regular speedup number adtree regular speedup attributes attribute time time factor attribute time time factor sets sets table time taken search attribute sets size number attributes set gives best information gain predicting output attribute 
times seconds average evaluation times attribute set 
adult birth number number adtree regular speedup number adtree regular speedup attributes rules time time factor rules time time factor table time taken search rules size number attributes highest scoring rules predicting output attribute 
times seconds average evaluation time rule 
cached sufficient statistics efficient machine learning 
alternative data structures kd tree 
kd trees accelerating learning algorithms omohundro moore 
primary difference kd tree node splits attribute attributes 
results memory linear number records 
counting expensive 
suppose example level tree splits level splits case binary variables query involving attributes higher explore paths tree level 
datasets fewer records may cheaper performing linear search records 
possibility trees guttman roussopoulos store databases dimensional geometric objects 
context offer advantages kd trees 
frequent set finder 
frequent set finders agrawal typically large databases millions records containing sparse binary attributes 
efficient algorithms exist finding subsets attributes occur value true fixed number chosen user called support records 
research mannila toivonen suggests frequent sets perform efficient counting 
case support frequent sets gathered counts frequent set retained equivalent producing adtree performing node cutoff common value cutoff occurs value false 
frequent sets way similar adtrees advantage disadvantage 
advantage efficient algorithms developed building frequent sets small number sequential passes data 
adtree requires random access dataset built leaf lists 
impractical dataset large reside main memory accessed database queries 
disadvantage frequent sets comparison adtrees circumstances may require memory 
assume value rarer attributes dataset assume reasonably choose find frequent sets 
unnecessarily sets produced correlations 
extreme case imagine dataset values attributes perfectly correlated values record identical 
attributes frequent sets 
contrast adtree contain nodes 
extreme example datasets weaker inter attribute correlations similarly benefit adtree 
leaf lists technique reduce size adtrees 
frequent set representation 
moore lee hash tables 
knew small set contingency tables requested possible contingency tables adtree unnecessary 
better remember small set contingency tables explicitly 
kind tree structure index contingency tables 
hash table equally time efficient require space 
hash table coding individual counts contingency tables similarly allow space proportional number non zero entries stored tables 
representing sufficient statistics permit fast solution contingency table request adtree structure remains memory efficient hash table approach method stores non zero counts memory reductions exploit ignoring common values 

discussion numeric attributes 
adtree representation designed entirely symbolic attributes 
faced numeric attributes simplest solution discretize fixed finite set values treated symbols little help user requests counts queries involving inequalities numeric attributes 
evaluate structures combining elements multiresolution kd trees real attributes moore adtrees 
algorithm specific counting tricks algorithms count conventional linear method algorithm specific ways accelerating performance 
example bayes net structure finder may try remember contingency tables tried previously case needs re evaluate 
deletes link deduce new contingency table old needing linear count 
cases appropriate adtree may lazy caching mechanism 
birth adtree consists root node 
structure finder needs contingency table deduced current adtree structure appropriate nodes adtree expanded 
adtree takes role algorithm specific caching methods general memory contingency tables remembered 
hard update incrementally tree built cheaply see experimental results section built lazily adtree updated cheaply new record 
new record may match nodes tree worst case 
scaling adtree representation useful datasets rough size shape 
datasets looked ones described cached sufficient statistics efficient machine learning shown empirically sizes adtrees tractable real noisy data 
included dataset attributes 
extent attributes skewed values correlated enables adtree avoid approaching worse case bounds 
main technical contribution trick allows prune common values 
correlation hardly help 
empirical contribution show actual sizes adtrees produced real data vastly smaller sizes get worst case bounds appendix despite savings adtrees represent sufficient statistics huge datasets hundreds non sparse poorly correlated attributes 
dataset adtree fit main memory 
case simply increase size leaf lists trading decreased memory increased time build contingency tables 
inadequate possibilities remain 
build approximate adtrees store information nodes match fewer threshold number records 
approximate contingency tables complete error bounds produced mannila toivonen 
second possibility exploit secondary storage store deep rarely visited nodes adtree disk 
doubtless best achieved integrating machine learning algorithms current database management tools topic considerable interest data mining community fayyad 
third possibility restricts size contingency tables may ask refuse store counts queries threshold number attributes 
cost building tree 
practice adtrees ways ffl 
traditional algorithm required build adtree run fast version algorithm discard adtree return results 
ffl amortized 
new dataset available new adtree built 
tree shipped re wishes real time counting queries multivariate graphs charts machine learning algorithms subset attributes 
cost initial tree building amortized times 
database terminology process known materializing harinarayan suggested desirable datamining researchers john lent mannila toivonen 
option useful cost building adtree plus cost running adtree algorithm cost original counting algorithm 
intensive machine learning methods studied condition safely satisfied 
decided intensive bayes net structure finder 
table 
pruning datasets ran memory megabyte machine built tree easy show birth dataset needed store nodes 
moore lee dataset speedup ignoring build time iterations speedup allowing build time iterations speedup allowing build time iterations census census adult adult adult birth syn syn syn syn syn table computational economics building adtrees search bayes net structures experiments section 
shows run iterations account adtree building cost relative speedup adtrees declines greatly 
conclude data analysis intense benefit adtrees fashion 
adtree multiple purposes build time amortized resulting relative efficiency gains traditional counting exhaustive searches non exhaustive searches 
algorithms non exhaustive searches include hill climbing bayes net learners greedy rule learners cn clark niblett decision tree learners quinlan breiman 
sponsored national science foundation career award andrew moore 
authors justin boyan scott davies nir friedman jeff schneider suggestions ron kohavi providing census datasets 
appendix memory costs appendix examine size tree 
simplicity restrict attention case binary attributes 
worst case number nodes adtree dataset attributes records worst case adtree occur possible records exist dataset 
subset attributes exists exactly node adtree 
example consider attribute set fa 
suppose node tree corresponding 
unsurprisingly resulting bayes nets highly inferior structure 
cached sufficient statistics efficient machine learning query fa values definition adtree remembering considering case binary attributes state ffl common value ffl common value records match 
ffl common value records match 
node 
worst case assumption possible records exist database see adtree contain node 
worst case number nodes number possible subsets attributes worst case number nodes adtree reasonable number rows frequently case dataset fewer records lower worst case bound adtree size 
node kth level tree corresponds query involving attributes counting root node level 
node match gammak records node ancestors tree pruned half records choosing expand common value attribute introduced ancestor 
tree nodes level blog rc tree nodes match fewer rc gamma records 
match records making null 
nodes adtree exist level blog rc higher 
number nodes level node level involves attribute set size binary attributes attribute set node adtree 
total number nodes tree summing levels blog rc bounded blog rc blog rc gamma 
number nodes assume skewed independent attribute values imagine values attributes dataset independent random binary variables value probability value probability gamma smaller expect adtree 
average common value vary node match fraction min gamma parent records 
average number records matched kth level tree min gamma maximum level tree may find node matching records approximately log gamma log min gamma 
total number nodes tree approximately log gamma log bounded log gamma log log gamma log gamma 
moore lee exponent reduced factor log attributes brings enormous savings memory 
number nodes assume correlated attribute values adtree benefits correlations attributes way benefits 
example suppose record generated simple bayes net random variable hidden included record 
gamma 
adn node resulting adtree number records matching node levels adn tree fraction gamma number records matching adn 
see number nodes tree approximately log gamma log bounded log gamma log log gamma log gamma 
gamma 
correlation attributes bring enormous savings memory case example marginal distribution individual attributes uniform 

bayes net generates correlated boolean attributes am number nodes dense adtree section dense adtrees cut tree common value vary node 
worst case adtree occur possible records exist dataset 
dense adtree require nodes possible query attribute values count tree 
number nodes kth level dense adtree worst case 
number nodes leaf lists leaf lists described section 
tree built maximum leaf list size min node adtree matching fewer min records leaf node 
means formulae re replacing min important remember leaf nodes contain room min numbers single count 
cached sufficient statistics efficient machine learning appendix building adtree define function subset rg total number records dataset adtree rows specified represent queries attributes higher 
new called adn 
adn count jth vary node adn 
uses function define new vary node called vn 
fg 
ij value attribute record add set ij vn mcv argmax mcv set subtree vn null 
set subtree vn build entire tree call rg 
assuming binary attributes cost building tree records attributes bounded blog rc agrawal mannila srikant toivonen verkamo 

fast discovery association rules 
fayyad piatetsky shapiro smyth uthurusamy 
eds advances knowledge discovery data mining 
aaai press 
moore lee breiman friedman olshen stone 

classification regression trees 
wadsworth 
clark niblett 

cn induction algorithm 
machine learning 
cover thomas 

elements information theory 
john wiley sons 
fayyad mannila piatetsky shapiro 

data mining knowledge discovery 
kluwer academic publishers 
new journal 
fayyad uthurusamy 

special issue data mining 
communications acm 
friedman yakhini 

sample complexity learning bayesian networks 
proceedings th conference uncertainty artificial intelligence 
morgan kaufmann 
guttman 

trees dynamic index structure spatial searching 
proceedings third acm sigact sigmod symposium principles database systems 
computing machinery 
harinarayan rajaraman ullman 

implementing data cubes efficiently 
proceedings fifteenth acm sigact sigmod sigart symposium principles database systems pods pp 

computing machinery 
john kohavi pfleger 

irrelevant features subset selection problem 
cohen hirsh 
eds machine learning proceedings eleventh international conference 
morgan kaufmann 
john lent 

data 
proceedings third international conference knowledge discovery data mining 
aaai press 
kohavi 

power decision tables 
wrobel 
eds machine learning ecml th european conference machine learning crete greece 
springer verlag 
kohavi 

scaling accuracy naive bayes classifiers decision tree hybrid 
simoudis han fayyad ed proceedings second international conference knowledge discovery data mining 
aaai press 


inductive learning algorithms complex systems modeling 
crc press boca raton 
mannila toivonen 

multiple uses frequent sets condensed representations 
simoudis han fayyad ed proceedings second international conference knowledge discovery data mining 
aaai press 
cached sufficient statistics efficient machine learning moore schneider deng 

efficient locally weighted polynomial regression predictions 
fisher ed proceedings international machine learning conference 
morgan kaufmann 
omohundro 

efficient algorithms neural network behaviour 
journal complex systems 
quinlan 

learning efficient classification procedures application chess games 
michalski carbonell mitchell 
eds machine learning artificial intelligence approach 
tioga publishing palo alto 
quinlan 

learning logical definitions relations 
machine learning 
roussopoulos 

direct spatial search pictorial databases packed trees 
navathe 
ed proceedings acm sigmod international conference management data 
computing machinery 


se tree characterization induction problem 
utgoff ed proceedings th international conference machine learning 
morgan kaufmann 

