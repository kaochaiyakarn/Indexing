improving simple bayes ron kohavi barry becker dan sommer eld data mining visualization group silicon graphics shoreline blvd mountain view ca ronnyk engr sgi com 
simple bayesian classi er sbc called naive bayes built conditional independence model attribute class 
model previously shown surprisingly robust obvious violations independence assumption yielding accurate classi cation models clear conditional dependencies 
examine di erent approaches handling unknowns zero counts estimating probabilities 
large scale experiments datasets conducted determine ects approaches interesting insights including new variant laplace estimator outperforms methods dealing zero counts 
bias variance decomposition show sbc performed common benchmark datasets accuracy scale dataset sizes grow 
limitations mind sbc serve excellent tool initial exploratory data analysis especially coupled visualizer structure comprehensible 
simple bayesian classi er supervised classi cation learning labelled training set learning algorithm 
learner uses training set build model maps unlabelled instances class labels 
model serves purposes predict labels unlabelled instances provide valuable insight people trying understand domain 
simple models especially useful model understood non experts machine learning 
simple bayes classi er sbc called naive bayes built conditional independence model attribute class formally probability class label value ci unlabelled instance ha ani consisting attribute values ci ci ci bayes rule ci ci label values 
ny aj ci ci conditional independence assumption probability computed class prediction class largest posterior probability 
model robust continues perform face obvious violations independence assumption 
probabilities formulas estimated training set 
address separate issues related sbc treat unknown values estimate probabilities especially counts zero 
large scale comparison variants datasets uci repository done 
emphasize extreme cases led interesting insights 
bias variance decomposition show sbc performed common benchmark datasets accuracy scale dataset sizes grow 
improving naive simple bayesian classi er investigate various options choose sbc 
options conducted experiments show di erences error 
explain cases di erences arise option preferable 
describing di erent options describe methodology 
experimental methodology chose datasets reported domingos pazzani labor negotiations soybean small fewer instances 
added datasets especially larger ones segment mushroom letter adult total 
speci datasets shown table 
main concern estimating accuracy estimate precise 
ran di erent inducers datasets forms 
dataset large arti cial indicating single test set yield accurate estimates training set test set de ned source dataset statlog de ned splits dna letter satimage cart de ned training size waveform led split ran inducer performed fold cross validation improve reliability estimate 
extreme interesting results shown graphically 
show absolute di erence error bars relative error rates error rate divided symbols pluses 
relative error rates especially useful error low 
example reducing error rate may signi cant terms absolute errors initial error rate error halved 
error costs signi cant error ratio important 
note types information shown graph left axis error diff echo card auto audiology cleve prim tumor unknown value minus ignore unknown ignore unknown unknown value fig 

comparison ignoring unknown values considering separate value 
left axis shows scale bars absolute error di erences bars zero indicate ignoring unknowns better 
right scale pluses relative error ratio pluses line show better performance ignoring unknowns 
showing scale bars right axis showing scale relative errors 
basic classi er simple sbc model 
continuous attributes discretized bins uniform size frequency counts estimate probabilities 
class label value zero counts class ignored predicted 
zero count class label attribute value conditional probability zero 
ties broken favor class instances original dataset 
important especially simple version classes zero posterior probability predict majority class 
horse colic unknowns treated 
rst option investigate handle unknowns 
consider unknowns separate value done domingos pazzani ignored instance including matching term product 
optimal treatment unknowns depend meaning domain 
unknown special meaning blank army rank person treating separate value better 
unknowns represent truly missing information data corrupted entry mistakenly left blank approach better matches bayesian de nition marginalizing appropriate attribute 
shows experimental results datasets di ered 
anneal hypothyroid adult error ratio datasets average error rate considering unknowns separate value ignoring 
datasets shown unknown important 
di ered generally better ignoring unknowns anneal dataset signi cant increase error observed 
encoding anneal dataset uci repository appears awed dataset converted unknowns dashes called le anneal rest 
reason ignore unknowns algorithm users map unknowns separate values unknowns considered separate value users cause certain values ignored 
conclude better algorithms ignore unknowns cases unknowns represent anneal unknowns converted separate value 
rest experiments unknowns treated true missing values 
estimating probabilities class probabilities conditional probabilities experiments pure frequency counts 
attribute value occur class label value produce zero estimate eliminating class consideration 
overcome problem single value controlling outcome examine general approaches literature match approaches replace zero count match factor inversely proportional number instances di erent approaches di erent numerator idea 
clark niblett domingos pazzani mlc default laplace approaches prede ned factor matches instances value problem estimate kf 
valued problems known laplace law succession 
table summarizes average errors average error ratios relative matches pc match approach numerator factor set datasets 
see frequency counts worst performer laplace law succession second worst 
matches pc middle 
small settings matches similar description le says values applicable values missing values treated legal discrete values showing absence discrete value dashes le 
addition tested original new encoding anneal dataset 
original encoding fold cross validation error encoding dashes decreased 
approach average error ratio average error relative matches pc laplace matches laplace matches pc matches matches laplace laplace law succession matches frequency counts table 
comparison di erent methods estimating probabilities 
match denotes replacing zeroes factor number instances 
laplace denotes adding numerator times number possible values denominator 
laplace denotes adding factor instances 
corrections laplace perform best 
laplace sets adjustment making smaller le size grows 
shows errors error ratios variants datasets signi cant di erences 
see frequency counts performs generally worse matches pc cars mushroom datasets performs signi cantly better 
laplace take best worlds 
tracks matches pc datasets cars mushroom tracks matches 
error di erences explained distinct opposite ects 
explanation frequency counts performs poorly 
conditional probability set zero frequency counts possible rule class single attribute value classes ruled 
opposite ect happens probabilities biased far away zero laplace law succession 
cases single strong predictor weakened 
correcting zero counts hurt performance cars mushroom datasets datasets rely single strong predictor able override weaker predictors classes 
methods correcting frequency best small correction values knowledge previously reported 
addition unknown handling zero counts discretize data entropy minimization absolute error datasets decreases average relative error ratio 
limitations sbc shows performance datasets uci limited classi er 
global classi er local predictions nearest neighbors decision trees 
simple error diff zoo glass wine audiology matches minus matches pc laplace minus matches pc cars soybean anneal mushroom matches matches pc laplace matches pc fig 

comparison probability estimation methods 
baseline chosen matches pc 
absolute errors relative error ratios shown respect baseline 
left axis shows scale bars absolute error di erences bars zero show worse performance matches pc 
right axis shows scale pluses asterisks relative error ratios symbols worse performance 
bayesian inducer consistent statistical sense additional strong assumptions inducer consistent classi ers produces approach bayes optimal error dataset size grows nity 
proofs decision tree inducers nearest neighbor inducers mild assumptions 
bias variance decomposition error error sum terms squared bias variance 
bias measures induced classi ers data low values variance measures stability indicate stability 
sbc usually low variance perturbations training sets rarely cause large changes predictions probabilities 
contrast decision tree inducers unstable attributes ranked closely root subtree order change training set perturbed cause subtree di er 
sbc usually high bias inability locally data 
shows bias variance decomposition described kohavi wolpert large datasets inducers simple bayesian mc decision tree inducer mlc 
evaluation set sampling compute bias variance 
internal sample process generate training sets half remaining training sets original dataset samples generated 
datasets fewer instances process repeated times averaged total runs 
gure shows performance sbc generally inferior large bias variance decomposition algorithm mlc requires support routines unavailable mc 
error ratio error german dna segment chess hypo sbc bias sbc var mc bias mc var mc sbc fig 

bias variance decomposition larger datasets sbc mc mlc decision tree inducer similar 
lower bar denotes bias upper bar denotes variance sum indicates total error 
left axis shows error bars lower better 
right axis shows ratio mc sbc lower indicates mc better 
datasets dna better 
looking decomposed terms variance simple bayesian inducer lower chess hypothyroid 
data change bias simple bayesian model conclude error decrease dataset size grows 
comparison classi ers previous sections proposed solutions decisions required sbc 
reduced error basic sbc relative improvement 
table shows dataset characteristics absolute errors rules sbc 
average error rules sbc 
ignore big datasets datasets dna adult table error rules error sbc error 
simple bayesian inducer fast inducers minutes 
rules took hours build ruleset adult dataset 
sbc fast algorithm 
accuracy small datasets may asymptote high error rate making useful classi er large databases 
related sbc model simple explanatory power previously noted kononenko wrote physicians explanations conditional probabilities natural similar classi cation 
summed evidence diagnosis satimage mushroom letter adult error ratio dataset train data rules sbc test set attr error error error size size cont nom zoo cv echocardiogram cv lymphography cv iris cv hepatitis cv glass cv wine cv auto cv sonar cv glass cv led audiology cv breast cv cleve cv solar cv waveform primary tumor cv liver disorder cv ionosphere cv horse colic cv cars cv vote cv soybean cv crx cv breast cv pima cv vehicle cv anneal cv german cv dna segment cv chess hypothyroid cv satimage mushroom letter adult table 
characteristics datasets comparison rules sbc 
datasets sorted training set size 
cv indicates fold cross validation 
numbers error indicate standard deviation mean error 
sbc model discretizes entropy estimates probabilities laplace ignores unknown values classi cation 
versions sbc notably version described cestnik alternative formulation mathematically equivalent requires estimating cja ajc 
comparisons reported showed insigni cant di erences accuracy methods 
researchers noted performance sbc including clark niblett kononenko langley sage domingos pazzani 
proposed extensions generally resulted little improvements proposals promising 
summary studied di erent options handling unknowns estimating probabilities discretizing 
large scale comparison datasets able pinpoint interesting datasets error di erences signi cant explained reasons di erent error results 
proposed new method estimating probabilities laplace outperformed methods datasets tested 
bias variance decomposition showed sbc performs small datasets generally scale larger datasets strong bias component 
compared sbc rules showed accurate outperforms inducers smaller datasets uci repository 
acknowledgments pedro domingos jim kelly mehran sahami joel excellent comments suggestions 
eric bauer clay kunz mlc 
experiments described done mlc 

leo breiman 
heuristics instability model selection 
technical report statistics department university california berkeley 

leo breiman 
bias variance arcing classi ers 
technical report statistics department university california berkeley 
available www stat berkeley edu users breiman 

cestnik 
estimating probabilities crucial task machine learning 
carlucci aiello editor proceedings ninth european conference arti cial intelligence pages 

clark niblett 
cn induction algorithm 
machine learning 

pedro domingos michael pazzani 
independence conditions optimality simple bayesian classi er 
saitta editor machine learning proceedings thirteenth international conference pages 
morgan kaufmann july 

james dougherty ron kohavi mehran sahami 
supervised unsupervised discretization continuous features 
armand prieditis stuart russell editors machine learning proceedings twelfth international conference pages 
morgan kaufmann july 

richard duda peter hart 
pattern classi cation scene analysis 
wiley 

fix hodges 
discriminatory analysis nonparametric discrimination consistency properties 
technical report report usaf school aviation medicine randolph field tex february 

nir friedman moises goldszmidt 
building classi ers bayesian networks 
saitta editor proceedings thirteenth national conference arti cial intelligence pages 
morgan kaufmann july 

stuart geman eli bienenstock doursat 
neural networks bias variance dilemma 
neural computation 

irving john 
estimation probabilities essay modern bayesian methods 
press 

louis gordon richard olshen 
sure consistent nonparametric regression recursive partitioning schemes 
journal multivariate analysis 

ron kohavi 
scaling accuracy naive bayes classi ers decision tree hybrid 
proceedings second international conference discovery data mining pages 

ron kohavi dan sommer eld james dougherty 
data mining mlc learning library 
tools arti cial intelligence pages 
ieee computer society press 
received best award 
www sgi com technology mlc 

ron kohavi david wolpert 
bias plus variance decomposition zero loss functions 
saitta editor machine learning proceedings thirteenth international conference 
morgan kaufmann july 
available robotics stanford edu users ronnyk 

igor kononenko 
semi naive bayesian classi ers 
proceedings sixth european working session learning pages 

igor kononenko 
inductive bayesian learning medical diagnosis 
applied arti cial intelligence 

pat langley 
induction recursive bayesian classi ers 
proceedings european conference machine learning pages april 

pat langley stephanie sage 
scaling domains irrelevant features 
russel greiner editor computational learning theory natural learning systems 
mit press appear 

christopher merz patrick murphy 
uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 

ross quinlan 
programs machine learning 
morgan kaufmann san mateo california 

singh gregory provan 
comparison induction algorithms selective non selective bayesian classi ers 
machine learning proceedings twelfth international conference pages july 
article processed latex macro package llncs style 
