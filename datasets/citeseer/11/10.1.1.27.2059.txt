nonlinear programming algorithm solving semidefinite programs low rank factorization samuel renato monteiro march nonlinear programming algorithm solving semidefinite programs sdps standard form 
algorithm distinguishing feature change variables replaces symmetric positive semidefinite variable sdp rectangular variable factorization rr rank factorization number columns chosen minimally enhance computational speed maintaining equivalence sdp 
fundamental results concerning convergence algorithm derived encouraging computational results large scale test problems 
keywords semidefinite programming low rank factorization nonlinear programming augmented lagrangian limited memory bfgs 
past years topic semidefinite programming sdp received considerable attention optimization community interest sdp included investigation theoretically efficient algorithms development practical implementation codes exploration numerous applications 
terms applications intriguing arise combinatorial optimization sdps serve tractable convex relaxations np hard problems 
progress area somewhat slow due fact practice theoretically efficient algorithms developed sdp quite time memory intensive fact especially true sdp relaxations large scale combinatorial optimization problems 
attempting address issues trend sdp development practically efficient algorithms strong theoretical guarantees 
follows trend introducing new experimental nonlinear programming algorithm sdp exhibits strong practical performance 
research supported part national science foundation ccr int 
school mathematics georgia tech atlanta ga usa math gatech edu 
school georgia tech atlanta ga usa monteiro gatech edu 
straightforward terms semidefinite programming generalization linear programming lp linear function symmetric matrix variable minimized affine subspace real symmetric matrices subject constraint positive semidefinite 
sdp shares features lp including large number applications rich duality theory ability solved precisely approximated polynomial time 
nice survey sdp refer reader 
successful polynomial time algorithms lp class methods shown efficient theory practice 
advent sdp interior point algorithms lp extended solve sdp polynomialtime small medium scale problems interior point algorithms proven robust obtaining highly accurate optimal solutions short amounts time 
performance sparse large scale problems impressive main reason difficult preserve sparsity computing second order search directions common types interior point algorithms 
algorithms issues surrounding classical second order interior point methods sdp see selection papers deal sparsity types interior point methods see 
recognizing practical disadvantages classical interior point methods researchers proposed alternative approaches solving sdps 
common new approaches attempt exploit sparsity effectively large scale sdps relying order gradient information 
rendl introduced order bundle method solve special class sdp problems trace primal matrix fixed 
special case graph theoretic maximum cut sdp relaxation homer shown change variables real square matrix having size allows recast sdp unconstrained optimization problem standard nonlinear method particular order method 
monteiro improved idea homer simply noting loss generality required lower triangular accordance cholesky factorization 
series papers monteiro zhang showed apply idea cholesky factorization dual sdp space transform sdp nonlinear optimization problem simple feasible set 
provided globally convergent order log barrier algorithm solve sdps method key features preservation sparsity 
fukuda kojima introduced interior point technique lagrangian duality solves class sdps studied allows order methods unrestricted space lagrange multipliers 
current follows path laid alternative methods specifically motivated consider order methods solving nonlinear reformulation sdp obtained replacing positive semidefinite variable appropriate factorization 
standard form primal sdp min ai bi 
data matrices ai real symmetric matrices data vector dimensional operator denotes inner product matrices matrix variable required symmetric positive semidefinite indicated constraint 
generally speaking constraint challenging aspect solving objective function constraints linear hoping simply circumvent difficult constraint introduce change variables real matrix required symmetric 
terms new variable resulting nonlinear program min ai bi 
easily seen equivalent factored positive constraint eliminated significant advantage benefit corresponding cost objective function constraints longer linear quadratic general nonconvex 
practical optimize place 
answer certainly immediate important questions addressed number variables large number variables managed efficiently 
optimization method best suited 
particular optimization method exploit sparsity problem data 
nonconvex programming problem expect find global solution practice 
answer appeal theorem posits existence optimal solution having rank satisfying inequality terms reformulation existence implies existence satisfying having columns equal zero 
idea manage variables simply set columns zero taken large eliminate optimal solutions 
words ignore columns optimization 
consequence resulting optimization equivalent original sdp having far fewer variables 
answering develop effective limited memory bfgs augmented lagrangian algorithm solving major computations require computer time space directly proportional number nonzeros data matrices ai computational results show method finds optimal solutions quite reliably able derive amount theoretical justification belief method strongly affected inherent nonconvexity largely experimental 
positively addressing questions primary optimizing place practical especially large sparse sdps 
organized follows 
section discuss detail standard form sdp nonlinear reformulation 
particular analyze optimality conditions consequences low rank factorization theorem mentioned 
section describe optimization technique focusing particular exploit sparsity data 
section discuss demonstrate implementation proposed algorithm classes large scale sdps 
compare method classical interior point methods algorithm rendl conclude method outperforms methods terms time solution quality 
lastly section conclude final remarks ideas research 
notation terminology denote space real numbers real dimensional column vectors real matrices respectively 
denote euclidean norm vectors 
denote space real symmetric matrices define sp subsets sp consisting positive semidefinite positive definite matrices respectively 
matrix sp write respectively 
trace denote trace indicate sp matrix sum diagonal elements define trace 
diag operator defined diag diagonal matrix having uii ui 
diag defined diag ui uii 
matrix ai aij denote th row th entry respectively 
nonlinear formulation section explore standard form primal sdp nonlinear formulation detail highlight important features semidefinite programming state assumptions 
consider consequences low rank theorem mentioned derive various optimality conditions 
sdp problem low rank reformulation stated standard form primal sdp specified data sn ai sn variable symmetric matrix required positive semidefinite sn 
assume constraint matrices ai linearly independent sn dimension means particular 
associated primal sdp dual sdp variables sn max 
assume nonempty optimal solution sets zero duality gap assume existence feasible solutions fundamental proposition proof example corollary useful analysis nonlinear reformulation primal sdp 
proposition feasible solutions simultaneously optimal equivalently xs sx 
factored primal sdp reformulated nonlinear program terms unrestricted variable note variations obtained requiring structure factorization example impose requirement lower triangular nonnegative diagonal elements case factorization represent cholesky factorization 
possibility require symmetric positive semidefinite making matrix square root fact type factorization valid feasible place generic factorization reformulate sdp 
slightly different perspective reformulating sdp factorization factorization valid feasible solutions valid optimal solutions 
reformulation clearly optimal solution set factorization subset sdp optimal solution set 
pursuing idea consider theorem proven concurrently theorem exists optimal solution rank satisfying inequality matrix rank factored having columns equal zero theorem impose structure factorization valid sdp optimal solutions valid feasible solutions 
particular define min rank optimal nonlinear program max min ai bi 

equivalent sdp sense previous paragraph 
advantage clear number variables typically smaller number 
find useful discuss ideas slightly general context slightly different notation 
introduce nonlinear program dependent positive integer nr min rr ai rr bi 

note distinguishing feature nr rectangular shape matrix rows columns 
note nn equivalent equivalent 
optimality conditions wish analyze optimality conditions nonlinear program nr fixed define usual lagrangian function rr yi ai rr bi vector unrestricted lagrange multipliers equality constraints nr 
introducing auxiliary variable defined lagrangian rewritten simply rr important note relationship dual sdp lagrange multipliers auxiliary variable comparing see cases relationship 
case lagrangian happens positive semidefinite constitutes feasible solution dual sdp 
consider easily derived formulas ai rr bi air rl sr rr dd combining formulas standard results nonlinear programming proposition concerning local minimizers nr 
proposition local minimum nr suppose regular point gradients air constraints linearly independent 
exists unique lagrange multiplier vector corresponding inequality holds matrices satisfying 
dd air 

proposition states second order necessary conditions feasible point local minimizer nr 
possible formulate standard sufficient conditions strict local minimum 
easy see nr strict local minima 
see feasible point arbitrary orthogonal matrix 
point rq feasible objective value equals equations demonstrate ai rq rq ai ai rr bi rq rq rr 
chosen rq arbitrarily close follows strict local minimum 
standard sufficiency conditions relevant current context interesting sufficient conditions feasible point nr yield optimal sdp solution rr said differently conditions irrespective integer guarantee feasible point optimal solution nn equivalently included feasible set nn obvious way 
conditions stated propositions 
proposition stationary point nr exists rl 
associated matrix positive semidefinite optimal respectively 
proof 
note feasible feasible nr feasible related positive semidefinite 
second condition rl rewritten equation 
easily 
claim proposition follows combining facts just derived proposition 
proposition suppose satisfies hypotheses proposition nr associated 
injection local minimum nr optimal respectively 
proof 
linear independence air implies linear independence ai ai simply injection air local minimum nr satisfies hypotheses proposition nr exists unique associated fulfill proposition 
proposition applies nr see 
simply additional zero column appended see 
uniqueness conclude st column zero st column ai zero 
follows matrix having columns equal zero satisfies ai proposition dd particular arbitrary vector consider matrix formed replacing st column zero matrix dd dd sd 
arbitrary inequality proves positive semidefinite 
positive semidefinite 
proposition follows proposition 
proposition special significance consider solve primal sdp practically reformulations nr 
example suppose attempt solve formulation nr algorithm computes local minimum explicitly computing lagrange multipliers local minimum obtained current positive semidefinite conclude proposition current rr optimal primal dual sdp solutions 
important note choice large sdp optimal solution rank big defined 
nr equivalent sdp implying positive semidefinite stationary point 
proposition presents intriguing algorithmic possibility 
suppose solving know exact value best solve nr specifically choose nr equivalent choose solving larger program necessary 
course know value ahead time theorem guarantees solving nr solve sdp 
happens bigger solving larger program necessary 
proposition suggests scheme solve sdp avoids solving nr 
choose small compute local minimum nr 

optimization technique determine injection local minimum nr compute better local minimum nr 

holds rr sdp optimal proposition repeat step 
step local minimum saddle point escaped 
goal scheme solve sdp nr equivalent equals course theoretical computational considerations scheme example linear independence assumptions proposition optimization technique step section show adaptation scheme allows solve sdp faster just solving directly 
optimization method section describe practical algorithm obtaining local minimizer nonlinear program nr 
key features algorithm ability handle nonconvex equality constraints nr exploitation sparsity problem data order search directions 
augmented lagrangian algorithm mentioned price pay elimination difficult constraint factorization rr difficult constraints ai rr bi 
assume structure ai constraints general nonconvex 
optimization method choose solving nr address difficult constraints 
nonlinear programming method believe candidate solving nr augmented lagrangian method called method multipliers 
basic idea method idea penalization method ignores constraints optimizes objective includes additional terms penalize infeasible points 
penalization lead ill conditioning optimization feature augmented lagrangian method introduce explicit lagrange multipliers yi constraint help balance ill conditioning induced penalization 
addition mitigating penalization multipliers serve test optimality discussed section direct connection dual problem 
description augmented lagrangian algorithm solving nr assume chosen nr feasible 
key component augmented lagrangian algorithm function called augmented lagrangian function rr yi ai rr bi ai rr bi variables unrestricted parameter positive 
comparing see augmented lagrangian function differs usual lagrangian addition term involving 
term measures euclidean norm infeasibility respect nr scaled real number 
called penalty parameter 
motivation augmented lagrangian algorithm appropriate fixed choice optimal solution nr simply optimizing function respect see example details 
course trick determine augmented lagrangian algorithm attempts forming sequence converges 
done minimizing respect order find optimal solution determine new pair process continued 
exact method updated important theoretical practical detail algorithm 
implementations update rule typically follows parameters auxiliary scalar vk introduced compute ai bi ii vk set iii set yk ai bi vk yk vk vk 
words quantity represents infeasibility quantity vk best infeasibility obtained point prior iterations algorithm 
hope smaller vk meaning obtained new best infeasibility 
fact smaller vk item ii details update lagrange multipliers penalty parameter stays vk carries new best infeasibility hand vk item iii increases penalty parameter factor keeps parameters fixed goal reducing infeasibility target level vk 
typical choices respectively 
addition methods choose update parameters dynamically course algorithm 
approach take numerical experiments see section details 
reasonable assumptions sequence converges sequence converges obtaining exact optimal solution practical implementation sequence replaced sequence stationary points approximate stationary points proven converge stationary point see example 
practice probably expect just stationary point local minimum nr 
fact computational results section demonstrate global solution nr 
method perform unconstrained minimization respect naturally plays critical role efficiency augmented lagrangian algorithm 
chosen order limited memory bfgs approach employs strong wolfe powell line search see example number limited memory bfgs updates store 
prefer gradient algorithm function gradient evaluations performed quickly especially small data matrices sparse detailed subsection 
addition difficult see computing factoring hessian application newton method consume large amounts space time 
function gradient evaluations stated goals optimization nr ability exploit sparsity problem structure wish discuss briefly augmented lagrangian algorithm exactly 
main computational algorithm lies solution subproblems min limited memory bfgs algorithm chosen perform unconstrained minimization uses function gradient evaluations main computational engine 
focus attention involved computing function value gradient respect augmented lagrangian function fixed 
difficult see main involved evaluating computation quantities rr ai rr 
particular quantities computed function value easily obtained roughly additional flops 
dot products performed efficiently 
consider cases arise example sdps section concerning computational results 
suppose data matrices sparse arbitrary structure 
letting represent data matrices rr wij rr ij wij wij 
final summation clearly shows needed compute rr proportional number nonzeros times involved computing dot product th th rows involved 
augmented lagrangian function value computed am flops 
quantity represents redundant computations 
particular data matrices share nonzero position dot product th th rows computed 
fix compute store rr ij th entry nonzero data matrix 
data matrix rr computed additional flops 
nonzero pattern matrix defined precisely combined nonzero patterns data matrices proposition 
proposition data matrices arbitrary time required evaluate augmented lagrangian function am 
second suppose data matrices rank matrix data matrix equals ww case may sparse structured 
main evaluating function computing rr arranged rr ww rr calculated nr flops proposition 
proposition data matrices rank equal time required evaluate augmented lagrangian function mnr 
couple remarks concerning propositions order 
propositions joined straightforward way handle case data matrices sparse arbitrary rank 
second easily seen space requirements function evaluation needed data matrices point order arbitrary data matrices rank matrices vectors stored convenient gradient computation 
turning gradient evaluation point formula gradient respect rl cr ai rr bi air sr yi yi ai rr bi ai rr bi ai yi ai rr bi ai 
notice yi computed directly reasonable assumption function value computed 
case data matrices arbitrary structure reasonable approach compute gradient simply form compute gradient making sure take advantage sparsity inherits data matrices 
procedure forming computing matrix product easily seen cost am flops 
case data matrices rank data matrix computed flops formula assuming stored function evaluation 
practical way form gradient compute data matrix combine resulting matrices linear combination matrices defining procedure gives flop count mn mnr 
putting cases proposition 
proposition data matrices arbitrary time required evaluate gradient augmented lagrangian function respect am 
data matrices rank equal time mnr mn 
space requirements excessive particular managed nr case arbitrary matrices nr case rank matrices 
gradient preserve zeros iterates wish draw attention characteristic gradient potentially undesirable effects augmented lagrangian algorithm handled cause little trouble 
problem arises observation pattern zeros propagate pattern zeros gradient sr turn propagate pattern zeros points obtained performing gradient line search example suppose augmented lagrangian algorithm nr initialized point having final column equal zero 
gradient sr final column equal zero 
point obtained performing line search gradient zero final column 
continuing algorithm way difficult see limited memory bfgs algorithm uses combinations gradients compute search directions ensure final column ensuing iterate zero 
algorithm meant solve nr fact solving nr 
net effect zero preserving characteristic gradient may imposing unintended structure iterates turn mean solving restriction nr 
practical way alleviate problem simply initialize algorithm zeros 
computational results section describe computational experiences low rank augmented lagrangian approach 
comparison computational results spectral bundle method rendl dual scaling interior point method benson ye zhang 
implementation method written ansi experiments code performed sgi origin mhz processors gigabytes ram 
stress codes parallel code uses processor 
lov sz theta sdp set computational results lov sz theta sdp introduced lov sz seminal 
simple undirected graph lov sz theta number defined negative optimal value lov sz theta sdp min ee trace xij vector ones dimension 
note lov sz theta sdp usually stated maximization implying simply optimal value chosen state sdp minimization standard form 
interesting properties satisfies inequality stability chromatic numbers respectively 
regard polynomial time computable number lies numbers np hard compute 
terms semidefinite program ee set ai consists identity matrix matrices form eje ek position vector entry equal entries 
computations treat rank matrix data matrices handled arbitrary sparse matrices 
tailor propositions case hand obtaining proposition 
proposition lov sz theta sdp function gradient evaluations augmented lagrangian nr performed nr nr flops respectively 
optimize lov sz sdp apply augmented lagrangian algorithm stated section problem defined 
case approximate value 
particular employ idea dynamically increasing rank described section 
reasons 
firstly guarantee regularity assumption proposition holds arbitrary stationary point nr obtained algorithm theoretical base dynamically changing rank strong 
secondly computational experience shows problem nr corresponding lov sz sdp difficult solve small values particular convergence method problems strong convergence quite 
result find computationally potential benefits nr small realized due slow convergence 
addition chosen test optimality checking positive algorithm 
reason fold firstly desired test stopping criterion described secondly efficient routine testing positive readily available 
particular cholesky factorization typical way test require large amount computational nonzero fill cholesky factor large 
numerical tests lov sz theta sdp important monitor increase penalty parameter 
early testing tendency increase high level resulted ill conditioning augmented lagrangian function consequently poor performance algorithm 
related observation progress lagrange multipliers optimal extremely important accuracy method 
fact necessary allow updating possible 
order penalty parameter emphasizing multipliers chose penalty update factor opposed usual value dynamically updated infeasibility reduction parameter allow updating requiring moderate amount reduction infeasibility 
fundamental property optimal value lies 
mind initialize augmented lagrangian algorithm specific rr primal feasible objective value dual feasible objective value primal matrix having diagonal position zeros certainly suffice matrix appropriate choice large pattern zeros cause problems algorithm section 
alter suggested matrix perturbing dense matrix small norm 
particular initial choose defined ij nm nm 
initial choice note dual sdp written follows max ee yij eje 
departed usual notation denoting lagrange multipliers vector denote multiplier trace constraint yij denote multiplier constraint edge 
choose components ij comprise initial lagrange multiplier 
seen objective value assumption isolated nodes reasonable assumption context computing lov sz theta number graph easy see diagonally dominant diagonal entry equal diagonal entry equal 
positive definite constitutes feasible solution dual sdp having objective value desired 
stated section augmented lagrangian algorithm stopping criterion 
computational results stopping criterion chosen follows algorithm terminated points obtained max words algorithm stops relative difference augmented lagrangian function regular objective function sufficiently small 
discussion low rank augmented lagrangian algorithm referred lr compare method methods spectral bundle method version rendl interior point method dsdp version benson ye zhang 
spectral bundle method simply sb dual ascent method maximizes dual computing sequence feasible points objective values monotonically approach optimal value 
dual scaling algorithm simply dsdp hand maintains primal dual feasibility obtains optimality forcing primal dual objective values converge 
sb dsdp run default parameters 
firstly compare methods set graphs seventh dimacs implementation challenge semidefinite related optimization problems characteristics listed table 
table give graph name number vertices number edges edge density graph nonzero density dual matrix addition column table gives value graph 
table list objective values obtained method problems 
note columns dsdp dsdp gives dsdp primal objective table hamming theta graphs graph dens rank hamming hamming hamming hamming hamming hamming table objective values hamming theta graphs graph lr sb dsdp dsdp hamming hamming hamming hamming hamming hamming value dsdp gives dual objective value 
note dsdp unable run efficiently graphs results available indicated symbol table helps interpret objective values providing estimates accuracy method 
column gives euclidean norm final infeasibility obtained augmented lagrangian algorithm particular shows lr able obtain points nearly feasible 
second column compares objective values lr sb relative sense absolute value difference numbers divided larger absolute value numbers 
lr primal method sb dual method numbers indicate methods computed highly accurate optimal value 
column compares dsdp dsdp similar way numbers column indicate dsdp obtained accurate optimal value 
table give times seconds taken method problem final row give total time method 
table shows sb outperformed lr dsdp class graphs 
noted hamming graphs particular structure allows sb solve iteration 
type performance quite atypical demonstrated tables see paragraph 
tables give computational results lr sb additional set graphs come sources called collection graphs introduced rendl second dimacs implementation challenge maximum clique maximum stable set problem 
data tables organized table accuracies hamming theta graphs graph lr feas lr sb dsdp hamming hamming hamming hamming hamming hamming table times seconds hamming theta graphs note graph sb took atypical iteration 
graph lr sb dsdp hamming hamming hamming hamming hamming hamming totals table second dimacs theta graphs graph dens rank mann brock brock brock fat hamming hamming johnson johnson keller hat san similar way tables 
regarding table note method upper limit hours seconds computation time single instance 
tables support method accurate lr outperforms sb large margin terms time 
maximum cut sdp relaxation maximum cut problem simple undirected edge weighted graph problem partitioning vertices sets total weight edges crossing maximized 
maximum cut problem referred simply maxcut assume 
symmetric matrix entry wij weight edge wij sdp relaxation maxcut problem introduced goemans table objective values accuracies second dimacs theta graphs graph lr sb lr feas lr sb mann brock brock brock fat hamming hamming johnson johnson keller hat san table times seconds second dimacs theta graphs graph lr sb mann brock brock brock fat hamming hamming johnson johnson keller hat san totals williamson min diag diag :10.1.1.3.9509
vector ones 
note state maxcut sdp relaxation minimization accordance opposed maximization :10.1.1.3.9509
terms sdp 
diag ai ei vector position 
notice sparsity pattern exactly graph full details maxcut sdp relaxation refer reader :10.1.1.3.9509
terms nonlinear reformulation nr maxcut sdp relaxation constraint diag ri 
words means point feasible nr row norm equal 
norm constraints separable easy see nr reformulated unconstrained problem having objective function ri rj cij ri rj ri rj cij ri rj simplicity handling constraints choose optimize nr unconstrained formulation augmented lagrangian algorithm section 
advantage applying limited memory bfgs algorithm unconstrained problem obtain feasible descent method exploit sparsity evident final expression 
implementation maxcut sdp relaxation able test idea dynamically increasing rank discussed section 
specific case maxcut sdp relaxation easy see constraint gradients linearly independent linear independence assumption proposition satisfied 
obtain local minimum unconstrained problem clearly give corresponding local minimum nr satisfies hypotheses proposition 
maxcut sdp relaxation ideal case testing idea dynamically increasing rank 
procedure testing dynamic update slight modification ideas proposition 
recall smallest rank theoretically guarantees equivalence nr sdp 
case approximately equal 
procedure define rj 
solve nr nr successively terminating process difference successive optimal values sufficiently small nr solved 
note problem nr initialized defined normalizing rows matrix ij nm nm nm 
lov sz theta sdp motivation choice starting point obtain reasonable initial objective value avoiding structured pattern zeros table torus maxcut graphs graph dens rank initial point 
similar manner nr solved local minimum obtained initial point nr computed appending columns small norm normalizing 
initial points nr nr nr calculated similarly necessary 
tests problem considered solved point satisfying fj max fj obtained fj objective function space rj addition consider difference successive optimal values sufficiently small max comparison idea dynamic rank test solving directly 
initial point chosen computation chosen similarly nr stopping criterion algorithm terminated point obtained relative gradient norm discussion methods described referred lr lr respectively 
compare methods methods comparison previous subsection spectral bundle method sb dual scaling interior point method dsdp 
stated previously sb dual ascent method dsdp maintains primal dual feasibility 
compare methods set graphs seventh dimacs implementation challenge semidefinite related optimization problems 
called torus problems characteristics listed table 
format table follows table 
tables provide objective values accuracies methods 
columns table similar columns table compares primal method dual method 
column hand compares primal versions lr lr scaled difference numbers 
result column indicates values lr lr differ significantly indicates lr lr converged value predicted theory 
tables method able compute accurate solutions 
table objective values torus maxcut graphs graph lr lr sb dsdp dsdp table accuracies torus maxcut graphs graph lr lr lr sb lr sb dsdp table give times taken method 
table shows lr outperforms methods 
particular conclude idea dynamically increasing rank works lr outperforms lr 
addition see low rank methods outperform sb dsdp dsdp order magnitude slower sb 
tables give computational results lr lr sb additional set graphs come collection graphs mentioned previous subsection 
data tables organized similar way tables tables support method accurate lr outperforms lr outperforms sb 
particular see lr lr stronger performance largest graphs 
example case graph having vertices edges lr times faster sb 
table times seconds torus maxcut graphs graph lr lr sb dsdp totals table maxcut graphs graph dens rank table objective values accuracies maxcut graphs graph lr lr sb lr lr lr sb lr sb table times seconds maxcut graphs graph lr lr sb totals table minimum bisection graphs graph dens rank bm industry table objective values minimum bisection graphs graph lr lr sb dsdp dsdp bm industry minimum bisection sdp relaxation minimum bisection problem simple undirected edge weighted graph similar maxcut problem partition vertices required satisfy 
particular number vertices 
minimum bisection problem relaxed min diag diag xe scalars vectors matrices 
fact difference negated objective function additional constraint xe 
solve handle constraint diag handled alter objective function eliminate constraint 
handle additional constraint xe augmented lagrangian techniques section 
addition test idea dynamically changing rank maxcut sdp relaxation 
easily checked regularity assumptions hold reformulation nr minimum bisection sdp 
choices various parameters stopping criteria similarly done lov sz theta sdp maxcut sdp relaxation 
tables organized similarly previous subsection show performance algorithms collection minimum bisection sdps obtained seventh dimacs implementation challenge 
maxcut sdp results demonstrates lr outperforms methods 
remarks concerning tables order 
dsdp unable perform satisfactorily problems indicated symbol second lr lr sb upper bound hours running time instance 
third important note time limit affects accuracies table sb able converge time 
table accuracies minimum bisection graphs graph lr feas lr feas lr lr lr sb lr sb dsdp bm industry final remarks table times seconds minimum bisection graphs graph lr lr sb dsdp bm industry totals introduced new nonlinear algorithm solving semidefinite programs standard form 
algorithm combines ideas factorization positive semidefinite matrices ii rank optimal sdp solutions iii order nonlinear optimization algorithms 
ideas contributes success algorithm 
particular item allows eliminate difficult constraint item ii allows greatly reduce number variables item iii allows take advantage sparsity problem data 
regarding optimality conditions low rank nonlinear formulation nr developed interesting sufficient conditions shown incorporated practical algorithm solving sdps 
addition practical behavior augmented lagrangian algorithm variants indicate likelihood convergence global optimal solution underlying nonlinear program nonconvex 
algorithm proposed compares favorably efficient algorithms solving sdps 
particular low rank approach outperformed spectral bundle method dual scaling interior point method successful codes solving large scale sdps 
maxcut sdp feel performance algorithm lr strong believe lr solution maxcut sdps sparse graphs tens thousands vertices routine activity 
performance algorithm lov sz theta sdp strong best knowledge computational results represent best progress solving lov sz sdp date 
ways try improve method 
important areas improvement solution augmented lagrangian subproblems lov sz theta sdp minimum bisection sdp 
currently subproblems solved slowly due mainly poor convergence 
convergence improved method benefit greatly 
area improvement theoretical convergence augmented lagrangian algorithm optimal sdp solution 
theoretical justification explains observed practical convergence currently formal convergence proof 
idea combine method dual approach guarantees dual feasibility pair algorithm 
way improve algorithm extend solve classes sdps example having inequality constraints 
authors express gratitude professor takashi tsuchiya tokyo institute statistical mathematics hosting authors japan partial research project conducted 
addition authors wish professor yin zhang rice university making available computer computational results performed 
computer acquired part support nsf dms 
authors debt rendl benson ye zhang making code freely available comparison 
seventh dimacs implementation challenge semidefinite related optimization problems november 
see website dimacs rutgers edu challenges seventh instances 

problems distance geometry convex properties quadratic maps 
discrete computational geometry 
benson ye zhang 
solving large scale sparse semidefinite programs combinatorial optimization 
research report department management science university iowa iowa 
appear siam journal optimization 
monteiro 
projected gradient algorithm solving maxcut sdp relaxation 
manuscript school georgia tech atlanta ga usa december 
appear optimization methods software 
monteiro zhang 
interior point algorithms semidefinite programming nonlinear programming formulation 
manuscript school georgia tech atlanta ga usa december 
appear computational optimization applications 
monteiro zhang 
solving semidefinite programs nonlinear programming 
part transformations derivatives 
manuscript school georgia tech atlanta ga usa september 
submitted mathematical programming 
monteiro zhang 
solving semidefinite programs nonlinear programming 
part ii interior point methods subclass sdps 
manuscript school georgia tech atlanta ga usa october 
submitted mathematical programming 
fletcher 
practical methods optimization 
john wiley sons new york second edition 
fukuda kojima 
interior point methods lagrangian dual semidefinite programs 
manuscript department mathematical computing sciences tokyo institute technology oh ku tokyo japan december 
fukuda kojima nakata 
exploiting sparsity semidefinite programming matrix completion general framework 
siam journal optimization 
goemans williamson :10.1.1.3.9509
improved approximation algorithms maximum cut satisfiability problems semidefinite programming 
journal acm pages 
rendl 
spectral bundle method semidefinite programming 
siam journal optimization 
homer 
design performance parallel distributed approximation algorithms maxcut 
manuscript dept computer science center computational science boston university street boston ma usa 
johnson trick 
cliques coloring satisfiability second dimacs implementation challenge 
ams 
lov sz 
shannon capacity graph 
ieee transactions information theory january 
monteiro todd 
path methods semidefinite programming 
vandenberghe wolkowicz editors handbook semidefinite programming 
kluwer academic publishers 

rank extreme matrices semidefinite programs multiplicity optimal eigenvalues 
mathematics operations research 
vandenberghe wolkowicz 
handbook semidefinite programming 
kluwer academic publishers 

