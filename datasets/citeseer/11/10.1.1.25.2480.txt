block orthogonalization procedure constant synchronization requirements andreas wu 
consider problem skinny long matrices 
propose alternative method computes orthonormal basis right singular vectors matrix 
advantage operations matrix matrix multiplications cache ecient synchronization point required parallel implementations typically stable classical gram schmidt 
second consider problem block vectors previously orthonormal set vectors 
solve problem alternating iteratively phase gram schmidt phase new method 
provide error analysis derive bounds accurately successive phases performed minimize total performed 
experiments con rm favorable numerical behavior new method ectiveness modern parallel computers 
key words 
gram schmidt orthogonalization householder qr factorization singular value decomposition poincar ams subject classi cation 


computing orthonormal basis set vectors basic computation common scienti applications 
computationally demanding procedures vectors large dimension computation scales square number vectors involved 
techniques ones ensure high accuracy expensive ones 
skinny long matrices row dimension far exceeds column dimension arise naturally various scienti contexts 
examples include statistical analysis observations variables iterative methods small subspace vectors span required solutions 
variety reasons orthonormal basis vectors computed 
traditionally obtained qr factorization quite matrix primary interest orthonormal basis qr factorization computed householder transformations qr algorithm classical gram schmidt gs modi ed version mgs 
stable numerically gs reorthogonalization usually preferred qr algorithm better computational properties 
methods performance limitations modern cache processors parallel computers 
implementations level level blas operations low cache reuse 
level blas implementations possible suitable skinny matrices 
parallel platforms increasingly popular clusters workstations reduction communication synchronization overheads kept explosive growth network bandwidth processor speed 
result global synchronization required frequent inner products scale number processors 
supported nersc performed computational facilities nersc college william mary 
facility enabled national science foundation eia sun microsystems sar edu 
department computer science college william mary williamsburg virginia andreas cs wm edu 
nersc lawrence berkeley national laboratory berkeley california lbl gov 
method needed operates blocks vectors requires number synchronizations independent size matrix 
applications occurs incremental fashion new set vectors call internal set orthogonalized previously orthonormal set vectors call external 
computation typical block krylov methods krylov basis expanded block vectors 
typical certain external orthogonalization constraints applied vectors iterative method 
locking converged eigenvectors eigenvalue iterative methods example 
nature applications suggests usually internal set skinny matrix fewer vectors external set 
conceptually problem viewed update qr factorization produced orthonormal set external vectors modi ed 
computationally problem usually tackled phase process rst internal vectors external ones external phase second internal vectors internal phase 
external phase block gs mgs competitive choices internal phase ecient procedure skinny matrices needed 
previous orts address problems considered blocks vectors hybrids scalable gs blocks accurate mgs blocks 
performance improves number synchronization points linear number vectors blas level kernels dominant despite blocking 
interestingly orts focused full qr factorization set vectors phase problem 
introduce method singular value decomposition svd uses right singular vectors produce orthonormal basis skinny matrix 
idea new dating back poincar encountered chemistry wavelet literature 
received attention computationally viable orthogonalization method knowledge analysis numerical properties 
method call svqb uses exclusively level blas kernels constant number synchronization points 
show accurate numerically mgs better gs absence special sparsity structure 
interestingly show stable alternatives mgs householder overkill phase problem 
coupling svqb method internal phase block gs reorthogonalization external phase results method constant synchronization requirements 
organized follows 
describe svqb method skinny matrices analyze numerical stability con rm theory numerical experiments comparisons methods 
second couple svqb block gs phase problem 
analyze numerical interaction methods theory tune phases avoid unnecessary 
timings series experiments cray ibm sp cluster sun workstations 
verify block computations small number synchronizations help new method achieve accurate orthogonality faster competitive methods 

problem 
set orthonormal vectors set vectors practice expect referred skinny matrix 
rst problem consider obtaining orthonormal set vectors span span 
second problem obtain orthonormal span span problems orthonormal basis necessarily qr factorization 
nite precision equality spans relaxed orthonormality requirement remain 
distinction problems computational reasons 
skinny matrices problem important allows ecient solutions qr factorization 
presence external matrix methods classical gs orthogonalize vector previous orthogonal vectors case di erence phases blurred 
algorithms allow level blas computational kernels introduce number synchronization points parallel computers 
improve computational performance need consider block subblocks 
block gs method orthogonalize block previous subblocks 
case orthogonality vectors block resolved di erent time distinct internal phase 
modi ed distinguishing problems allows non qr factorization methods internal phase 
internal phase gs mgs popular qr factorization methods incur number arithmetic operations level blas kernels implemented parallel modest number synchronization points 
mgs numerically stable error orthogonality bounded condition number 
householder re ections yield matrix orthogonal machine precision require twice arithmetic gs 
practice matrices second orthogonalization gs typically producing orthogonality machine precision gs preferred methods 
context phase problem producing internal set vectors orthogonality close machine precision unnecessary interdependence phases 
example external orthogonalization may spoil internal orthogonality vice versa 
phase problem obviates expensive stable methods householder 

svqb method 
especially interesting orthonormal basis span derived right singular vectors assume vectors normalized 
singular values square roots eigenvalues right singular vectors corresponding eigenvectors su eigendecomposition de ne wu obviously span span vectors normalized diagonal diag contains squares norms ii 
implicitly normalized wd scaling columns rows inexpensive numerically stable explicit normalization 
resulting factorization qr qb factorization orthonormal full matrix 
exact arithmetic algorithm singular vector qb factorization call svqb follows algorithm 
svqb 

scale diag 
solve su eigenpairs 
compute wd vectors linearly dependent eigenvalues corresponding vectors zero 
nite precision similar ect caused linearly dependent vectors eigenvalues close zero 
numerical noise eigenvalues bounded away zero 
prevent normalization ows avoid explicit computation norm vectors set minimum threshold eigenvalues 
machine precision insert steps max ii ii set ii strategies dealing linear dependencies possible 
example consider eigenvectors eigenvalues greater safe threshold 
resulting basis smaller guaranteed orthonormal numerically span subspace original vectors 
nite precision arithmetic algorithm may applied iteratively svqb orthonormal set obtained 
solution eigenvalue problem implicit normalization involve matrices inexpensive 
parallel computers duplicated processor 
matrix matrix multiplication computing multiplication contribute nm oating point operations algorithm twice expensive gs 
operations level blas kernels performed eciently cache computers 
alternatively matrix multiplication computing symmetric performed half operations level blas 
parallel implementation svqb method method requires synchronization point computing matrix 
cholesky qr method 
similarly svqb method derive block qr factorization cholesky factorization 
note cholesky factor wr de nes qr factorization 
method denoted rarely computationally attractive characteristics qr factorization level blas kernel triangular system solution involves arithmetic gs requires synchronization point parallel implementations 
researchers noticed stable mgs stable gs 
problematic issues ill conditioning stable cholesky factorization 
regularizing ectively straightforward case svqb smallest singular pairs simply left computation 
cache performance usually inferior svqb triangular solve 

stability analysis svqb 
applications block krylov iterative methods orthonormality resulting vectors importance upper triangular qr factorization 
example krylov methods progress albeit slower provided slightly di erent orthonormal set common literature measure stability departure resulting orthonormality backward error 
block non sequential nature expect svqb procedure stable mgs 
show stable gs 
theorem 
set linearly independent vectors oating point representation matrix computed applying svqb procedure condition number ki qk min denotes norm machine round constant depending proof 
oating point representation ksk ksk write ksk kwk ect performing scaling matrix corresponds vector having norm implicitly case ksk computed eigenvectors eigenvalues small symmetric matrix standard backward error analysis considered exact eigendecomposition nearby matrix relation express error sk ksk letting matrices related sk sk ksk min min ii max max ii consider similar notation eigenvalues matrices 
algorithm sets eigenvalues smaller max equal threshold de ne diagonal matrix ii ii ii max max ii max oating point representation matrix returned svqb procedure 
kqk kwk uk denote exact eigenvalues max largest kwk max note uk 
min min max symmetric eigenproblems error eigenvalue bounded error matrix sk ksk max constants min min max max max max note max min substitution yields min min max rst term chosen minimum ii order max smaller 
means singular values larger represented eigenvalues despite squaring 

bound min min max setting give bound kqk kqk kw kqk min kqk min emphasize backward error exact result svqb product computed matrices yields exact backward error relevant orthogonality interest 
consider departure computed orthonormality ki qk ki qk ki relations orthonormality ki qk ki ki kk sk de nition rst term zero 
equal max max max 
case terms 
bounds setting obtain ki qk min min min bound showing applying svqb iteratively converges fast orthonormal basis 
rst state lemma see 
lemma 
ki qk kq qk kqk 
ki qk ki proposition 
ki qk condition number proof 
de nition kq proof follows lemma kq qk kqk ki ki theorem 
set linearly independent vectors condition number 
oating point representation matrix computed applying svqb procedure machine round proof 
theorem ki qk proposition proves bound case bounded away 
case bounded general case numerical error dominates 
intuition gained considering structure obtained bounds 


max 
element wise 
note scaling 
element wise 
de nition entries diagonal matrix eigenvalues max threshold max rest 
eigenvalues perturbations diagonal values smallest eigenvalue bounded away zero 
assume nite precision errors occur inability represent eigenvalues smaller max 

obviously 
note case vectors exactly orthogonal condition number far 
proved general observed numerical experiments 
plausible explanation perturbations vector scaling introduce random noise expect linearly independent relatively conditioned directions 

convergence comparisons 
theorems suggest situations applying svqb twice produce orthogonal vectors 
case extremely ill conditioned vectors third application procedure necessary 
akin behavior gs reorthogonalization expected better iterative gs internal reorthogonalization 
accurate cholesky decomposition computed procedure identical svqb 
fact possible prove bounds similar ones previous section 
accurate decomposition large condition numbers expect stable eigenvalue svqb method 
demonstrate relative ectiveness methods apply sets vectors report improvements condition numbers 
rst set krylov vectors generated vector ones laplacean regular nite di erence square mesh neumann conditions 
dimension matrix initial vector considered set 
second set consists columns hilbert matrix size 
third set arti cial show bene ts mgs gs 
variation shown matlab notation ones diag rand eps eps eps tests run matlab sun ultra workstation 
condition numbers computed matlab function cond inaccurate exceed results cases shown tables respectively 
compare svqb gs mgs printing condition number vectors methods produce iteration 
svqb gs mgs iteration table krylov laplacean size initial vector ones 

scaling numerical error 
svqb gs mgs iteration table hilbert matrix size 

implicit normalization svqb guarantee normality ill conditioned problems print condition number unscaled vectors condition number vectors explicitly scaling norms 
note iteration equal condition number vectors application svqb 
example rst iteration 
results tables con rm developed theory 
condition number smaller reduction obeys closely bound theorem 
application step svqb reduces condition number set vectors 
reduction sharp examples tables vectors explicitly normalized reduction larger see table 
expected method behaves similarly svqb table 
cases orthogonality inferior svqb see table possible takes iterations produce fully orthonormal set see table 
spite fact implementation rst compute lowest eigenvalue shift cholesky decomposition applied numerically positive de nite matrix 
discussed earlier gs reorthogonalization vector ective large condition numbers 
cases gs may er improvement successive iterations see rst gs iteration table may require iterations produce set relatively small conditioner number see tables 
achieved iterations provide fully orthonormal set 
exception example table gs requires svqb gs mgs iteration table ones diag rand eps eps eps reorthogonalization 
reason gs takes advantage sparse structure matrix performing computations small elements achieving low relative error 
mgs clearly stable rest methods 
departure orthogonality mgs bounded relatively small second mgs needed see tables 
note bound svqb virtually identical mgs close diminishing advantages svqb 

phase problem 
theory examples establish svqb competitive choice internal phase problem 
denote ortho procedure external phase wd diagonal matrix normalizing norms vectors phase algorithm described algorithm 
ortho svqb 
ortho 
svqb common choices ortho gs mgs 
eciency important especially number vectors large block gs form reorthogonalization usually employed 
show accuracy close machine precision critical orthogonality achieved phase may preserved 
nite precision algorithm compute accurately orthonormal set applied iterative fashion 
develop ecient iterative version ortho svqb rst examine numerical interaction phases 

numerical interplay phases 
reason full orthogonality necessary phase svqb procedure may destroy previous orthogonality ortho may destroy orthogonality lemma 
set vectors kv wk kwk 
svqb result internal step ortho svqb algorithm 
kv qk min proof 
notation theorem bounds kv qk kv qk kqk min lemma states exactly orthogonal svqb procedure step may destroy orthogonality maximum 
example consider matrices matlab computation shows kv wk 
step svqb observe kv qk agrees lemma 
important chose accurate ortho methods mgs obtain orthogonality may lost 
ortho procedure rst step ortho svqb algorithm worse ect orthogonality vectors 
lemma 
set normal vectors kw ik kv wk 
ortho normalized result external orthogonalization 
assume oating point error computing kq ik proof 
ii se note diagonal elements holds se ks sk 
result diagonal elements ii holds min ii kd 
addition see ii 
compute kq ik kd kd kd ik kd kks sk lemma states vectors orthonormal lose mutual orthogonality ortho step suciently orthogonal kv wk 
nite precision additional orthogonality loss expected 
bound sharp constant 
example consider initially ortho verify kq ik 
lemma gives bound small times larger actual loss orthogonality 

iterative gs svqb algorithm 
section suggests gs suf cient external phase rest focuses gs svqb algorithm 
shows possible gs svqb implementations steps gs svqb carried iteratively 
choose appropriate algorithm computational considerations developed theory 
algorithm repeat gs svqb algorithm repeat repeat gs repeat svqb algorithm repeat repeat gs svqb algorithm repeat gs repeat svqb fig 

possible iterative implementations gs svqb algorithm 
outer loop repeated numerically orthonormal orthogonal inner loops repeated full orthogonalization achieved speci ed number steps 
theory suggests algorithm preferable 
note gs expensive size usually larger try minimize number times repeated 
second applications gs usually sucient produce full orthogonality lemma orthogonality wasted svqb step 
algorithms apply gs repeatedly inappropriate 
algorithm may result wasted svqb gs reach synergistic levels accuracy 
algorithm appropriate choice 
note iterating svqb produce orthogonality justi ed outer step gs destroy kv wk lemma 
especially kv wk obtained current step orthogonality maintained fully 

tuning algorithm 
step identify ecient practical conditions terminating outer inner repeat loops algorithm 
avoid unnecessary lemmas suggest steps gs svqb balanced keeping kv wk comparably small 
seek conditions nal outer iteration require svqb applications 
orthonormal gs destroyed internal orthogonality lemma implies test checked inexpensively gs product kv need test orthonormal gs step 
known singular values obtained svqb 
theorem implies svqb produced orthonormal 
condition tested kv exit outer loop repeat gs procedure managed orthogonalize perform reorthogonalization popular test due daniel norm times norm svqb cause loss orthogonality assume inner loop svqb produces 
obviously gs outer iteration reduce condition number gs 
application svqb svqb destroy orthogonality versus lemma 
making third outer iteration necessary 
avoid svqb inner loop iterated produce 
inner loop executes guarantee full orthonormality second outer iteration needed 
summarize analysis algorithm 
condition number computed eigenvalues matrix corresponds matrix application svqb 
bound resulting inferred theorem condition checks 
note kv computed gs 
algorithm 
igs svqb iterative gs svqb method large number repeat kv gs break skip nal svqb daniel test repeat svqb false 
optimizations 
block algorithm targets performance 
tests performs pessimistic apply block 
may wasteful individual vectors may orthogonal vectors exempt vectors performing computations smaller block 
fortunately perform tests individual vectors adjust block dynamically ecting block structure number synchronizations algorithm 
interestingly large bounds lemmas result couple ill conditioned vectors 
grouping vectors individual tests bounds apply smaller blocks provide better direction algorithm 
speci cally gs phase additional computation sepa gg gs qg gg gg gg gg gs daniel test gg qg synchronization blas flops svqb new new repeat svqb svqb fig 

rate vectors need reorthogonalization vectors need bad vectors 
vectors may close vectors svqb phase mixes vectors identity ones disappear 
solve problem inexpensively 
regular svqb compute bg bg perform eigenvalue decomposition subdivide group gg gb 
group gg consists singular vectors corresponding large singular values gg vectors orthogonal appended iterations 
note gg vector 
group gb consists singular vectors small singular values may lost orthogonality 
combine gb vectors apply svqb gb 
resulting needs orthogonalized versus gg implementation details fall scope 
shows conceptual steps algorithm 
notice eigenvalue decompositions vector groupings tests performed vectors matrices eigenvectors negligible computational cost 
main operation blas incurs synchronization 

variable block algorithm 
number vectors large applying svqb method full block may cache ecient numerically stable 
typically optimal block size cache performance decreases 
addition conditioning bound deteriorate block size 
reasons want variable block size tuned machine problem 
desirable block size 
partition sets vectors apply igs svqb individually 
subblock orthonormal orthogonal previous locked targeted 
algorithm follows algorithm 
bgs svqb variable block igs svqb method partition igs svqb number synchronization points bgs svqb algorithm larger percentage computation spent gs procedure 
algorithm reduces classical gs method algorithm igs svqb method 
expect identify range block sizes cache performance optimal synchronization requirements excessive 

timing experiments 
tested implementation bgs svqb variety orthogonalization alternatives 
provide common comparison framework methods bgs method algorithm identical bgs svqb di erent method orthogonalize block 
rst method classical gs algorithm reorthogonalization 
method structure di ers slightly bgs method 
gs ecient orthogonalize vector block vectors previously orthogonalized vectors block size ect behavior gs gures simply refer gs 
compare qr factorization householder re ections 
method denoted bgs qr uses qr implementation library single multiprocessor platforms 
compare computationally similar method 
method denoted uses sequential cholesky decomposition library 
algorithms implemented fortran mpi interface run cray ibm sp parallel computers nersc national lab node cluster sun ultra college william mary 
mb memory available node machines sp nodes processor smp nodes assigned individual mpi processes 
network considerably faster sp cow networks power switch fast ethernet respectively 
nersc platforms link mpich libraries machine optimized libraries blas 
sun cluster lam mpi blas kernels automatically optimized atlas university tennessee 
rst numerical example easily reproducible set krylov vectors diagonal matrix diag matlab notation initial vector log 
build set normalized vectors ax condition number resulting set computed matlab cond function 
goal set accurately possible various orthogonalization methods block sizes bgs method 
initially computation spent method blocks get gs takes reducing computational di erences methods 
illustrates single node oating point performance mflop rate achieved algorithms function block size left block size bgs svqb bgs cg bgs qr block size mflops processor ibm sp bgs svqb bgs cg bgs qr fig 

single node mflops function block size methods 
clear performance advantage block methods expected counter balance increase number flops 
methods vectors dimension 
left graph depicts cray results 
right graph depicts ibm sp results 
graph sp right graph 
expected gs rate constant regardless block size 
single node performance bgs qr improve block size machine points implementation inherent block limitations qr 
hand block structure svqb allows outperform gs signi cantly small blocks vectors 
cholesky back solve implementation exploit better architecture sp 
platforms bgs svqb improves gs performance small block sizes 
mention block mgs method expected worse single node performance gs phases involves blas kernels 
node performance important leads accurate faster orthogonalization 
algorithms tested produced nal orthonormal set kq ik shows execution times block methods superior gs method 
graphs plot execution time function block size methods various numbers processors 
left graph corresponds cray right ibm sp 
bgs bgs svqb consistently faster gs block size sp block sizes 
clear logarithmic time scale relative improvement gs timings persists large number processors despite smaller local problem sizes 
bgs svqb faster gs faster sp 
note performance bgs sp faster gs carry 
experiment measures ects synchronization number nodes increases xing problem size processor constant block size 
test set krylov vectors generated matrix discretized laplacean dimensional cube 
processor holds uniform grid locally matrix size proportional number nodes uses create krylov space 
vectors generated chunks successive krylov vectors chunk call 
plots execution times methods wide range processor numbers 
experiment large number available nodes 
absence communication synchronization block size nodes nodes nodes nodes nodes gs bgs bgs svqb block size gs bgs bgs svqb nodes nodes nodes nodes nodes fig 

time versus block size methods various numbers processors 
results obtained left graph ibm sp right graph 
time scale logarithmic 
number processors gs bgs qr bgs bgs svqb fig 

scalability methods constant problem size vector rows processor block size 
ideal scalability show horizontal line 
costs times equal processors 
time increase observed gure relatively small methods extremely fast network 
ects apparent gs bgs qr curves increase faster respective ones bgs bgs svqb 
problem bgs svqb faster gs improvement previous numerical problem 
superscalar processors higher latency network expect gs bgs bgs svqb block size gs bgs bgs svqb number processors fig 

left graph execution time methods versus block size single sun ultra 
blas libraries optimized atlas 
right graph scalability methods cluster sun ultra constant problem size vector rows processor block size 
ideal scalability show horizontal line 
block algorithms perform better sun cluster 
test case krylov vectors uniform grid laplacean processor storing subgrid 
left graph gure shows ect blocking single node execution time algorithms 
ects dramatic machines execution time reduced half 
attributed atlas ne tuning blas kernels 
note improves single node performance architecture 
time variability typical caching ects 
right graph gure shows scalability algorithms constant computational load processor 
block size bgs method 
proposed methods clearly outperform gs time substantially smaller gs increase slower number processors 


introduced analyzed new method called svqb computes orthonormal basis skinny matrix right singular vectors 
method attractive computationally involves blas kernels requires synchronization point parallel implementations 
proved departure orthonormality resulting vector set bounded 
considered problem phase block vectors previously orthonormal set vectors gs svqb 
computational eciency suggests independent block application methods 
phase impairs orthogonality produced phase 
provided bounds describe numerical interdependence balance performed orthogonalization phases iterative scheme 
matlab examples demonstrated theoretical bounds accordance practice parallel implementations cray ibm sp sun cow shown method improves performance alternatives 
bj 
solving linear squares problems gram schmidt orthogonalization 
bit 
bj 
numerics gram schmidt orthogonalization 
linear algebra applications 
bj paige 
loss recapture orthogonality modi ed gram schmidt algorithm 
siam matrix anal 
appl 
choi cleary azevedo demmel dhillon dongarra hammarling henry stanley walker whaley 
user guide 
siam 
kapoor srinivasan 
new orthogonalization procedure extremal property 
technical report quant ph lacs stanford palo alto 
chui 
wavelet analysis applications 
academic press san diego 
daniel kaufman stewart 
reorthogonalization stable algorithms updating gram schmidt qr factorization 
math 
comp october 
dongarra du hammarling 
set level basic linear algebra subprograms 
acm trans 
math 
soft 
dongarra hammarling hanson 
extended set fortran basic linear algebra subprograms 
acm trans 
math 
soft 

algorithms qr decomposition 
technical report tr angewandte mathematik eth 
golub van loan 
matrix computations 
john hopkins university press baltimore md 
golub underwood 
block lanczos method computing eigenvalues 
rice editor mathematical software iii pages new york 
academic press 
ho man iterative algorithms gram schmidt orthogonalization 
computing 
jalby philippe 
stability analysis improvement block gram schmidt algorithm 
siam sci 
statist 
comput 
lawson hanson kincaid krogh 
basic linear algebra subprograms fortran usage 
acm trans 
math 
soft 

chem 
phys 

adv 
quant 
chem 
parlett 
symmetric eigenvalue problem 
siam philadelphia pa 
saad 
iterative methods sparse linear systems 
pws publishing 
wigner 
math 
phys 
singh culler gupta 
parallel computer architecture 
hardware software 
morgan kaufmann publishers san francisco 

parallel block jacobi davidson implementation solving large eigenproblems coarse grain environments 
international conference parallel distributed processing techniques applications pages 
csrea press 
clint whaley jack dongarra 
automatically tuned linear algebra software 
technical report ut cs university tennessee 
wilkinson 
algebraic eigenvalue problem 
press oxford england 

