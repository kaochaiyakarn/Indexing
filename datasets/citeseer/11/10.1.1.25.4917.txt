appears proceedings th international conference supercomputing sorrento italy june 
evaluating impact memory system performance software prefetching locality optimizations abdel aggarwal donald yeung chau wen tseng electrical computer engineering dept computer science dept university maryland college park eng umd edu tseng cs umd edu software prefetching locality optimizations techniques overcoming speed gap processor memory 
evaluate impact memory trends effectiveness software prefetching locality optimizations types applications regular scientific codes irregular scientific codes pointer chasing codes 
find applications software prefetching outperforms locality optimizations sufficient memory bandwidth locality optimizations outperform software prefetching bandwidth limited conditions 
break point ghz processors occurs roughly gbytes sec today memory systems increase memory systems 
study interactions software prefetching locality optimizations applied concert 
naively combining techniques provides robustness changes memory bandwidth latency yield additional performance gains 
propose evaluate algorithms better integrate software prefetching locality optimizations including modified tiling algorithm padding prefetching index prefetching 
current microprocessors spend large percentage execution time memory access stalls large chip caches 
processor speeds growing greater rate memory speeds expect memory access costs important 
computer architects memory wall problem designing larger sophisticated caches 
caches extremely effective complete solution 
techniques required fully address memory wall research supported part nsf computer systems architecture ccr nsf career awards ccr asc 
problem 
promising approaches improving memory performance software prefetching locality optimizations 
executes explicit prefetch instructions loading data memory cache 
long prefetching begins early data evicted prior memory access latency completely hidden 
processor throughput improves due memory latency tolerance memory bandwidth increased prefetching increases memory traffic 
comparison locality optimizations compiler run time transformations change computation order data layout program increase probability accesses data cache 
successful average memory latency bandwidth usage reduced fewer main memory accesses 
software prefetching locality optimizations studied isolation 
examine approach works types data intensive applications 
evaluation uses single unified environment enable meaningful comparison 
primary focus compare importance latency tolerance provided prefetching latency reduction provided locality optimizations high performance memory systems 
addition investigates interactions software prefetching locality optimizations applied concert 
contributions follows ffl compare efficacy software prefetching locality optimizations types data intensive applications 
ffl quantify impact bandwidth latency scaling memory systems relative effectiveness software prefetching locality optimizations 
ffl examine performance integrated software prefetching locality optimizations propose evaluate enhancements increase combined effectiveness 
rest look memory access patterns examine software prefetching locality optimizations type application affine array accesses indexed array accesses pointer structures jacobi kernel molecular dynamics linked list traversal struct node val ptr list time 
ptr malloc node 
ptr ptr ptr val 
force ptr force 
force ptr ptr example affine array indexed array pointer chasing codes 
tal evaluations application develop improved algorithms discuss related conclude 
memory access patterns types software prefetching locality optimizations may applied dependent type memory access pattern program 
presenting important types memory access patterns 
affine array accesses basic memory access pattern affine linear accesses multidimensional arrays 
instance consider jacobi code typically multigrid solvers partial differential equations pdes 
value point calculated average values neighboring points dimensions stencil pattern repeatedly applied point resulting smoother solution 
array accesses affine array subscripts combinations loop index variables constant coefficients additive constants 
practice coefficients small additive constants 
programs called regular codes memory access patterns regular defined 
affine array accesses common dense matrix linear algebra finite difference pde solvers database scans image processing 
major feature affine array accesses allow memory access patterns entirely computed compile time assuming array dimension sizes known 
allows software prefetching locality transformations calculated precisely compile time 
indexed array accesses memory access pattern called indexed array accesses main data array accessed separate index array value unknown compile time 
example consider molecular dynamics code calculates forces pairs atoms molecule 
accesses index array affine array sequentially 
accesses arrays indexed contents programs called irregular memory accesses fixed 
irregular accesses typically difficult keep data cache resulting cache misses low performance 
indexed array accesses arise scientific application domains computational scientists attempt complex simulations 
computational fluid dynamics meshes modeling large problems sparse reduce memory computation requirements 
body solvers arise astrophysics molecular dynamics example data structures irregular model positions particles interactions 
unfortunately irregular computations poor temporal spatial locality utilize processor caches efficiently 
applications affine accesses compile time transformations improve locality values index array known compile time 
pointer chasing accesses consider pointer programs dynamically allocate memory pointer data structures linked lists ary trees graph structures 
example consider list traversal code creates singly linked list data structure node containing pointer element list 
traverse list program determine pointer value stored node 
indexed array accesses programs utilizing pointers irregular memory access patterns determined compile time 
additionally node traversed pointer stored current node memory accesses 
programs known pointer chasing codes 
pointer chasing codes occur application domains including scientific programs advanced data structures 
software prefetching software prefetching relies programmer compiler insert explicit prefetch instructions application code memory cache 
run time inserted prefetch instructions bring data processor cache advance overlapping cost memory access useful processor 
section briefly describe software prefetching techniques previously proposed prefetching different types memory 
affine array prefetching perform software prefetching affine array commonly scientific codes known compiler algorithm inserting prefetches proposed mowry gupta followed 
algorithm locality analysis determine array suffer cache misses 
cache missing memory isolated performing loop unrolling loop peeling transformations 
prefetch instructions inserted isolated cache missing 
inserted prefetch properly scheduled exists ample time initiation prefetch consumption data processor known prefetch distance overlap latency memory access 
indexed array prefetching indexed array accesses form common irregular scientific codes 
prefetch algorithm indexed array accesses originally proposed similar algorithm affine array prefetching 
main difference lies prefetch requests scheduled 
affine array prefetching prefetch scheduled early tolerate latency single cache indexed array memory indirection index array data array requires sophisticated prefetch scheduling 
index array data array cache memory latency serialized cache misses just tolerated 
prefetch algorithm schedule prefetch index array access cache times prior iteration consumes data schedule prefetch data array cache time prior iteration consumes data 
pointer chasing prefetching prefetching pointer data structures challenging due memory serialization effects associated traversing pointer structures 
memory operations performed array traversal issue parallel individual array elements referenced independently 
contrast memory operations performed pointer traversal dereference series pointers purely sequential operation 
sequentiality pointer chasing prevents conventional prefetching techniques overlapping cache misses suffered pointer chain limiting effectiveness 
jump pointer prefetching promising approach addressing pointer chasing problem 
jump pointer prefetching additional pointers inserted dynamic data structure connect non consecutive link elements 
prefetching linked list prefetch arrays jump pointers prefetch list prefetch array ptr list head ptr prefetch ptr jump ptr ptr prefetch array jump pointers code 
jump pointers allow prefetch instructions name link elements pointer chain prefetch distance away sequentially traversing intermediate links 
consequently prefetch instructions overlap fetch multiple link elements simultaneously issuing prefetches memory addresses stored jump pointers 
illustrates loop instrumented jump pointer prefetching 
jump pointer prefetching prefetch prefetch distance number link elements linked list jump pointers point early nodes 
enable prefetching early nodes jump pointer prefetching extended prefetch arrays 
technique array prefetch pointers added linked list point prefetch distance number link elements 
prefetches issued memory addresses prefetch arrays traversing linked list cover early nodes illustrated 
addition inserting prefetch instructions jump pointer prefetching prefetch arrays require insertion code create maintain prefetch pointers data structure modified shown 
locality optimizations software prefetching tries hide memory latency retaining original program structure 
alternative reduce memory costs changing computation order data layout program compile run times 
locality optimizations try improve data locality ability application reuse data cache 
reuse may form temporal locality cache line accessed multiple times spatial locality nearby data accessed cache line 
previous researchers developed locality optimizations 
section consider optimizations types data intensive applications investigating 
tiling affine accesses programs affine array accesses easiest compilers apply locality optimizations memory access patterns fully analyzed compile time 
useful program transformation tiling blocking uses loop transformations form small blocks loop iterations executed exploit data locality 
demonstrates jacobi code tiled 
rearranging loop structure innermost loops fit cache due fewer iterations tiling allows reuse exploited tiled dimensions 
major problem tiling limited cache associativity may cause data tile mapped cache lines sufficient space cache 
conflict misses result causing tile data evicted cache may reused :10.1.1.133.892
effect shown 
previous research tile size selection array padding applied avoid conflict misses tiles 
tile size selection algorithms carefully select tile dimensions tailored individual array dimensions conflicts occur 
array padding expands leading array dimensions increasing range non conflicting tile shapes improving performance tiled codes range problem sizes 
apply combination algorithms tile linear algebra pde solvers 
reordering indexed accesses index arrays arise scientific applications sparse mesh pde solvers molecular dynamics codes access pattern determined run time 
unfortunately index arrays cause data accessed irregular manner making spatial locality reuse data cache line data larger cache 
researchers discovered run time data computation transformations improve locality irregular computations 
computations typically commutative loop iterations safely reordered bring accesses data closer time 
data layout transformed data accesses cache line 
compiler run time transformations automated inspector executor approach developed messagepassing machines 
apply efficient partitioning algorithms input data bring reuse closer follow lexicographically sorting loop iterations data access patterns algorithms specified 
partitioning algorithm works viewing data accesses graph applying series graph coarsening passes connected data put cluster 
resulting programs achieve better processor caches 
memory allocation pointers pointer programs frequently suffer poor locality 
notoriously difficult analyze transform reliance pointers heap allocated recursive data structures 
researchers developed heap allocation transformation techniques improve locality pointer programs :10.1.1.37.5419
algorithms tiled jacobi kk tk ti tj tk tile jj tj ii ti kk kk tk tiled loops jj jj tj ii ii ti tiled jacobi example 
include run time tree optimization routines place parent nodes child nodes improved locality coloring placing tree nodes avoid conflict root 
particular interest ccmalloc customized memory allocator allocates memory location near user specified address 
heuristic proved effective reserves space allocation requests allocating new blocks data 
memory allocator multiple members linked list adjacent memory locations 
take advantage hardware prefetching long cache lines cache line utilization increases fragmentation reduced decreasing probability useful cache lines evicted cache 
applied optimization pointer chasing benchmark codes 
experimental evaluation section evaluates performance software prefetching locality optimizations independently concert 
describe experimental methodology 
compare software prefetching locality optimizations different memory bandwidths latencies study combination 
methodology experimental evaluation employs benchmarks representing classes data intensive applications described section 
table lists benchmarks problem sizes memory access patterns 
applications table perform affine array accesses 
matmult multiplies matrices performs red black successive relaxation ja performs jacobi relaxation 
jacobi frequently multigrid pde solvers mgrid spec nas benchmark suite 
applications perform indexed array accesses 
iterative pde solver irregular mesh abstracted non bonded force calculation charmm key molecular dynamics application nih model macromolecular systems 
applications cache array layout causes conflict cache array layout avoids conflicts example conflict misses array layouts 
perform pointer chasing accesses 
health simulates health care system mst computes minimum spanning tree 
pointer chasing codes olden benchmark suite 
application applied software prefetching locality optimizations hand isolation combination 
followed algorithms described sections applying appropriate algorithm application memory access pattern 
measured performance optimized codes detailed architectural simulator 
simulator simplescalar tool set models ghz way issue dynamically scheduled processor 
simulator models aspects processor including instruction fetch unit branch predictor register renaming functional unit pipelines reorder buffer 
addition simulator models memory system detail 
assume split kbyte direct mapped cache byte cache blocks unified kbyte way cache byte cache blocks 
caches small match input data sets required simulation 
study sensitivity software prefetching locality optimization techniques available memory bandwidth modified simplescalar simulator model bus contention memory bus varied memory bus bandwidth gbytes sec note bandwidth gbyte sec equivalent processor loading byte cycle varied memory latency cycles 
parameters capture characteristics architectures processors faster large dram memories 
experiments transfers bus incur cycle latency experience contention 
varying memory bandwidth evaluate performance software prefetching locality optimizations memory bandwidth scaling 
plot execution time axis vary memory bandwidth gbytes sec axis keeping memory latency fixed cycles 
execution time bar broken memory stall software overhead computation components 
groups bars represent original application problem size access pattern matmult matrices affine array jacobi grid affine array grid affine array node mesh indexed array molecules indexed array health levels iters pointer chasing mst nodes pointer chasing table benchmark summary 
version program versions optimized software prefetching locality optimization 
section focus applying techniques isolation 
section examine techniques combination 
affine array indexed array benchmarks software prefetching locality optimizations provide significant performance gains improving performance average respectively unoptimized codes 
comparing techniques see major differences 
software prefetching suffers overhead due prefetch related address computation instructions 
tiling incurs overhead extra levels loops minimal 
include preprocessing overhead data reordering amortized loop iterations 
second relative effectiveness software prefetching locality optimizations eliminate memory stalls depends available memory bandwidth 
high memory bandwidths software prefetching eliminates practically memory stalls memory system sustain simultaneous memory requests necessary hide memory latency 
memory bandwidth reduced memory requests serialize software prefetching loses effectiveness 
contrast locality optimizations reduce memory latency memory traffic 
highly effective low bandwidths reduced traffic pays 
locality optimizations eliminate memory stalls achieve lower maximum performance compared software prefetching 
consequently array benchmarks software prefetching outperforms locality optimizations high memory bandwidths locality optimizations outperform software prefetching low memory bandwidths 
quantify effect table reports memory bandwidths software prefetching locality optimiza execution time cycle sec memory overhead busy orig pref opt pref opt matmult execution time cycle sec memory overhead busy orig pref opt pref opt jacobi execution time cycle sec memory overhead busy orig pref opt pref opt execution time cycle sec memory overhead busy orig pref opt pref opt execution time cycle sec memory overhead busy orig pref opt pref opt execution time cycle sec memory overhead busy orig pref opt pref opt health execution time cycle sec memory overhead busy orig pref opt pref opt mst execution time breakdown memory bandwidth scaling optimizations orig software prefetching pref locality optimization opt combined optimizations opt pref 
memory latency fixed cycles 
latency matmult jacobi average table equi performance bandwidths cycle memory latencies 
column reports average benchmarks 
memory bandwidths gbytes sec 
bandwidth gbytes sec execution time mcycles matmult orig bandwidth gbytes sec execution time mcycles matmult pref bandwidth gbytes sec matmult opt bandwidth gbytes sec execution time mcycles matmult opt pref bandwidth gbytes sec execution time mcycles jacobi orig bandwidth gbytes sec execution time mcycles jacobi pref bandwidth gbytes sec jacobi opt bandwidth gbytes sec execution time mcycles jacobi opt pref bandwidth gbytes sec execution time mcycles orig bandwidth gbytes sec execution time mcycles pref bandwidth gbytes sec opt bandwidth gbytes sec execution time mcycles opt pref execution time memory bandwidth latency scaling affine array benchmarks optimizations orig software prefetching pref locality optimization opt combined optimizations opt pref 
tions achieve equal performance 
memory systems providing memory bandwidths higher equi performance bandwidth favor software prefetching providing lower memory bandwidths favor locality optimizations 
cycle memory latency corresponding data table shows average equi performance bandwidth gbytes sec 
large equi performance bandwidth underscores importance latency reduction techniques implies memory systems provide significant memory bandwidth prefetching outperform locality optimizations data intensive applications 
pointer chasing benchmarks locality optimization outperforms software prefetching memory bandwidths 
due factors 
pointer prefetching incurs high software overhead create manage jump pointers 
software overhead health mst respectively busy component 
contrast ccmalloc memory allocation incurs measurable overhead 
second traversal loops pointer chasing codes short particularly mst provide sufficient hide memory latency 
software prefetching eliminate memory stalls 
pointer prefetching requires jump pointer storage increases cache rate memory bandwidth consumption making optimized code data intensive original code 
shows software prefetching health mst performs worse original code low memory bandwidths 
varying memory latency figures evaluate software prefetching locality optimizations memory latency scaling 
similar plot execution time versus memory bandwidth 
addition results cycle memory latencies separate lines graph 
version program original prefetch optimized displayed separate graph 
focus applying techniques isolation leaving discussion combined techniques section 
surprisingly execution time program versions increase scale memory latency 
affine array indexed array benchmarks software prefetching effectively hi bandwidth gbytes sec execution time mcycles orig bandwidth gbytes sec execution time mcycles pref bandwidth gbytes sec opt bandwidth gbytes sec execution time mcycles opt pref bandwidth gbytes sec execution time mcycles orig bandwidth gbytes sec execution time mcycles pref bandwidth gbytes sec execution time mcycles opt bandwidth gbytes sec execution time mcycles opt pref bandwidth gbytes sec execution time mcycles health orig bandwidth gbytes sec execution time mcycles health pref bandwidth gbytes sec health opt bandwidth gbytes sec execution time mcycles health opt pref bandwidth gbytes sec execution time mcycles mst orig bandwidth gbytes sec execution time mcycles mst pref bandwidth gbytes sec execution time mcycles mst opt bandwidth gbytes sec execution time mcycles mst opt pref execution time memory bandwidth latency scaling indexed array pointer chasing benchmarks optimizations orig software prefetching pref locality optimization opt combined optimizations opt pref 
des increasing memory latencies sufficient memory bandwidth 
contrast locality optimizations suffer performance degradation memory latencies grow enjoy benefit reduced traffic low memory bandwidths 
result software prefetching outperforms locality optimizations high memory bandwidths locality optimizations outperform software prefetching low memory bandwidths memory latencies simulated 
table shows equi performance bandwidths generally increase memory latency 
consequently systems high memory latencies greater memory bandwidth required software prefetching demonstrates performance advantage locality optimizations 
pointer chasing benchmarks locality optimization outperforms software prefetching memory latencies bandwidths 
reasons section reduced effectiveness software prefetching pointer comparing average performance different versions programs relative memory bandwidth latency 
performance normalized relative original program gbyte sec bandwidth cycle latency 
data structures explain locality optimization performance advantage higher memory latencies 
combined techniques section evaluates software prefetching locality optimizations combination 
created combined versions benchmarks manner 
affine array benchmarks applied software prefetching innermost tiled loops 
indexed array pointer chasing benchmarks software prefetching locality optimizations modify distinct parts code 
programs simply merge modified portions software prefetching locality optimization program versions 
results reported figures pref opt 
summarize average performance version program relative memory bandwidth latency 
performance normalized relative original program bandwidth gbyte sec latency cycles averaged programs memory bandwidth latency 
simulations show results vary depending memory bandwidth latency 
software prefetching sensitive available memory bandwidth 
bandwidth low software prefetching increases overhead reducing memory costs 
combined algorithm performs slightly worse locality optimizations 
comparison memory latencies high combining software prefetching locality optimizations usually yields better performance applying 
shows combining better prefetching slightly better locality optimizations 
certain conditions combining techniques encounters high overhead 
affine array benchmarks tiling significantly reduces number iterations innermost loop 
prefetching applied short tiled loops software pipeline startup overhead incurred prefetching significant reducing amount memory latency hidden 
effect apparent high cpu overhead pref opt versions matmult jacobi 
combining inherits overheads software prefetching tiling reducing performance relative software prefetching 
pointer chasing benchmarks combining ccmalloc memory allocation low memory bandwidths 
extra jump pointers prefetch arrays required pointer prefetching increase demand memory bandwidth partially negating reduced traffic benefits achieved ccmalloc memory allocation combined version 
combined version ccmalloc memory allocation high memory bandwidths mst 
described previously software prefetching short list traversal loops mst ineffective combining software prefetching ccmalloc memory allocation adds overhead reducing memory stalls 
combining exploits latency tolerance latency reduction sensitive variations memory bandwidth latency technique isolation 
robust performance valuable bandwidth latency parameters target system available compiler compiler produce single optimized code heterogeneous systems 
algorithm enhancements addition evaluating effects memory bandwidth latency scaling performance simulations point number ways enhance software prefetching locality optimizations 
bandwidth gbytes sec execution time mcycles square tile bandwidth gbytes sec execution time mcycles tall tile bandwidth gbytes sec square tile pref bandwidth gbytes sec execution time mcycles tall tile pref matmult bandwidth gbytes sec execution time mcycles square tile bandwidth gbytes sec execution time mcycles tall tile bandwidth gbytes sec square tile pref bandwidth gbytes sec execution time mcycles tall tile pref jacobi bandwidth gbytes sec execution time mcycles square tile bandwidth gbytes sec execution time mcycles tall tile bandwidth gbytes sec square tile pref bandwidth gbytes sec execution time mcycles tall tile pref comparing square tiles tall tiles prefetching 
tiling prefetching problem combining tiling software prefetching naively high startup overhead prefetching short tiled loops 
improve performance modifying tiling algorithm select tiles iterations innermost loop 
tiling heuristic uses euclidean gcd algorithm generate series non conflicting tile shapes 
tiles square aspect ratio typically achieve best cache utilization bias selection taller tiles greater height width aspect ratio 
tall tiles iterations innermost loop compared square tiles reducing startup overheads combination software prefetching 
table reports square tall tile sizes affine array benchmarks 
presents tall tile results prefetching matmult jacobi compares corresponding square tile results fig application square tall matmult theta theta jacobi theta theta theta theta table tile sizes square tall tile versions affine array benchmarks 
ure 
notice tall tiles square tiles achieve similar performance 
combined software prefetching tall tiles significantly reduce short loop overheads suffered high bandwidths square tiles matching performance software prefetching 
simulation results demonstrate tall tiles allow fully exploit benefits software prefetching tiling simultaneously 
bandwidth gbytes sec execution time mcycles orig bandwidth gbytes sec execution time mcycles orig pad bandwidth gbytes sec execution time mcycles pref bandwidth gbytes sec execution time mcycles pref pad padding prefetching jacobi 
padding software prefetching software prefetching hide memory latency sufficient memory bandwidth conflict misses prefetched data degrade completely eliminate benefits 
experiments prefetching affine array codes may require array padding particularly set associativity cache low 
problem applications problematic data sizes severe conflict misses may result prefetched data mapped cache lines 
problem especially acute affine accesses arrays dimensions near multiple cache size adjacent array elements conflict cache 
compilers avoid problem pad arrays avoid prefetch conflicts loops tiled 
approach employ treat distance prefetch data actual data height tile variable determining tile width 
compiler analysis euclidean gcd algorithm determine cache conflicts occur tile padding leading array dimensions conflicts eliminated 
ensures prefetched data able stay cache processor 
presents experiments demonstrating utility combining array padding prefetching 
versions program jacobi created padding prefetching 
way associative cache simulations purpose illustration way caches eliminate conflicts jacobi complicated programs 
chose problem size theta theta 
power problem sizes occur frequently multigrid codes due need series meshes increasing granularity 
prefetch distance euclidean algorithm chose pad array theta theta eliminate conflicts 
simulation results show jacobi experiences cache conflicts array padding eliminate improving performance 
software prefetching help conflicts evict prefetched cache lines 
padding applied prefetching improve performance achieved padding 
ccmalloc prefetching software prefetching pointer chasing codes suffers high overhead create manage jump pointers 
jump pointers may necessary prefetching combined ccmalloc memory allocation 
intelligent allocation places link nodes contiguously memory prefetch instructions access link nodes indexing just affine array accesses 
approach refer index prefetching originally proposed 
index prefetching jump pointers removed eliminating overhead associated jump pointer prefetching 
quantify benefit created index prefetching versions health mst 
results shown 
upper portion compares index prefetching original versions prefetch arrays ccmalloc allocation combination assuming memory latency cycles 
data shows index prefetching eliminates software overheads incurred prefetch arrays 
result index prefetching outperforms optimized versions high memory bandwidths health mst 
index prefetching performs slightly worse ccmalloc allocation especially mst due prefetching link nodes conditionally accessed increasing memory bandwidth consumption 
index prefetching reduces software overheads effective eliminating memory stalls prefetch arrays health 
health link nodes deleted re inserted linked lists frequently 
contiguous allocation index prefetching dynamic lists useless layout link nodes random delete insert operations 
upper portion shows health index prefetching hides memory latency prefetch arrays due frequent delete insert operations 
larger memory latencies increased memory stalls outweigh reduced software overheads combining ccmalloc prefetch arrays naively outperforms index prefetching 
related similar saavedra evaluated unimodular transformations tiling software prefetching matrix multiply 
mowry evalu execution time cycle sec memory overhead busy pref ccmalloc ccmalloc pref index pref health execution time cycle sec memory overhead busy pref ccmalloc ccmalloc pref index pref mst bandwidth gbytes sec execution time mcycles pref bandwidth gbytes sec execution time mcycles ccmalloc bandwidth gbytes sec ccmalloc pref bandwidth gbytes sec execution time mcycles index pref health bandwidth gbytes sec execution time mcycles pref bandwidth gbytes sec execution time mcycles ccmalloc bandwidth gbytes sec execution time mcycles ccmalloc pref bandwidth gbytes sec execution time mcycles index pref mst comparing index prefetching index pref prefetch arrays pref ccmalloc memory allocation ccmalloc combined optimizations ccmalloc pref 
top graphs memory latency fixed cycles 
ated software prefetching tiling scientific applications 
comparison focuses memory trends quantifies impact software prefetching locality optimizations 
prior considered single technology point 
furthermore examine classes applications requiring different types optimizations study memory trend effects broader context 
propose enhancements address problems arise combining techniques 
compared cache simulator evaluate performance detailed simulation modern processor 
relatively little compared software prefetching locality optimizations large body studied techniques isolation 
software prefetching affine array accesses studied 
hardware prefetching similarly limited affine array accesses uses hardware identify access pattern automatically 
prefetch engines affine array accesses provide hardware support prefetching rely programmer compiler identify access pattern 
prefetching pointer chasing traversals uses approaches 
approach inserts additional pointers called jump pointers dynamic data structures connect nonconsecutive link elements described section 
second approach uses natural pointers prefetching 
techniques prefetch pointer chains sequentially schedule prefetch early loop iteration possible maximize memory latency overlap 
third approach uses hardware table called markov predictor predict link node addresses prefetching 
fourth approach uses special allocation technique allocate nodes contiguously memory enables indexed access link nodes 
approach proposed called data linearization prefetching 
index prefetching technique evaluated section identical data linearization prefetching 
data locality recognized significant performance issue modern processor architectures 
computation reordering transformations loop permutation tiling primary optimization techniques loop fis sion distribution loop fusion helpful 
data layout optimizations padding transpose shown useful eliminating conflict misses improving spatial locality 
cache estimation techniques proposed help guide data locality optimizations 
tiling proven useful linear algebra codes multiple loop nests time step loops 
comparison apply tiling stencil codes tiled existing methods 
researchers examined irregular computations context parallel computing run time compiler support support accesses message passing multiprocessors 
looked techniques improving locality 
researchers investigated data layout transformations pointer data structures 
chilimbi investigated allocation time run time techniques improve locality linked lists trees 
propose extensions 
calder profiling guide layout global stack variables avoid conflicts 
carlisle investigate parallel performance pointer codes olden 
drawn 
relative effectiveness software prefetching locality optimizations depends available memory bandwidth 
array benchmarks software prefetching outperforms locality optimizations high memory bandwidths locality optimizations outperform software prefetching low memory bandwidths 
equi performance bandwidth gbytes sec today memory systems increase memory latencies increase 
locality optimizations outperform software prefetching benchmarks memory bandwidths latencies due reduced effectiveness prefetching data structures 
second combining software prefetching locality optimizations inherits merits techniques 
combining yields better performance software prefetching locality optimizations memory latency high exploits 
combining robust changes memory system parameters latency tolerance latency reduction techniques isolation 
naively combining techniques outperform best choice software prefetching locality optimizations bandwidths latencies 
combined effectiveness software prefetching locality optimizations enhanced new algorithms 
affine array benchmarks tall tile selection reduces prefetch startup overheads allowing combining outperform software prefetching locality optimizations practically memory bandwidths latencies 
padding remove conflicts prefetched data affine array benchmarks crucial prefetching problem sizes suffer cache conflicts 
benchmarks combining index prefetching ccmalloc memory allocation reduce prefetch overheads effective large number link nodes contiguously allocated health ccmalloc allocation gets gain mst 
current processor speeds maintaining memory bandwidths gbytes sec probably achievable 
simulation results relevant bandwidth low 
processors faster memory wall increase reducing available memory bandwidth relative processor speed 
locality optimizations important 
similarly processor speeds increase memory latencies increase past cycles making results simulation higher memory latencies relevant 
architects switch processor memory pim architectures increase memory bandwidth dramatically 
chip data available memory bandwidth high simulations 
experiments show pim systems benefit significantly software prefetching 
pim systems require locality optimizations reduce accesses chip data 
acknowledgments authors gabriel rivera providing insightful discussions tiling padding techniques providing affine array codes 
han providing indexed array codes 
ranka 
memory hierarchy management iterative graph structures 
proceedings th international parallel processing symposium orlando fl april 
burger austin 
simplescalar tool set version 
cs tr university wisconsin madison june 
calder john austin 
cache conscious data placement 
proceedings eighth international conference architectural support programming languages operating systems asplos viii san jose ca october 
callahan kennedy porterfield 
software prefetching 
proceedings fourth international conference architectural support programming languages operating systems asplos iv santa clara ca april 
carlisle rogers reppy hendren 
early experiences olden 
proceedings sixth workshop languages compilers parallel computing portland august 

chen 
effective programmable prefetch engine onchip caches 
proceedings th annual symposium microarchitecture pages 
ieee 

chen 
baer 
effective hardware data prefetching high performance processors 
transactions computers may 

chi 
compiler optimization technique data cache prefetching small cam array 
proceedings international conference parallel processing pages august 
chilimbi hill larus 
cache conscious structure layout 
proceedings sigplan conference programming language design implementation atlanta ga may 
chiueh 
sunder programmable hardware prefetch architecture numerical loops 
proceedings supercomputing pages 
acm november 
coleman mckinley 
tile size selection cache organization data layout 
proceedings sigplan conference programming language design implementation la jolla ca june 
das saltz 
hwang 
communication optimizations irregular scientific computations distributed memory architectures 
journal parallel distributed computing september 
ding kennedy 
improving cache performance dynamic applications computation data layout transformations 
proceedings sigplan conference programming language design implementation atlanta ga may 
fu patel 
data prefetching multiprocessor vector cache memories 
proceedings th annual symposium computer architecture pages toronto canada may 
acm 
fu patel janssens 
stride directed prefetching scalar processors 
proceedings th annual international symposium microarchitecture pages december 
ghosh martonosi malik 
cache equations analytical representation cache misses 
proceedings acm international conference supercomputing vienna austria july 
han 
tseng 
comparison locality transformations irregular codes 
proceedings th workshop languages compilers run time systems scalable computers rochester ny may 
han 
tseng 
improving locality adaptive irregular codes 
proceedings thirteenth workshop languages compilers parallel computing white plains ny august 
joseph grunwald 
prefetching markov predictors 
proceedings th international symposium computer architecture pages denver june 
acm 
jouppi 
improving direct mapped cache performance addition small fully associative cache prefetch buffers 
proceedings th annual symposium computer architecture pages seattle wa may 
acm 
karlsson dahlgren 
prefetching technique irregular accesses linked data structures 
proceedings th international conference high performance computer architecture toulouse france january 
levy 
architecture software controlled data prefetching 
proceedings th international symposium computer architecture pages toronto canada may 
acm 
lam rothberg wolf :10.1.1.133.892
cache performance optimizations blocked algorithms 
proceedings fourth international conference architectural support programming languages operating systems asplos iv santa clara ca april 
lu cox dwarkadas zwaenepoel 
compiler software distributed shared memory support irregular applications 
proceedings sixth acm sigplan symposium principles practice parallel programming las vegas nv june 

luk mowry 
compiler prefetching recursive data structures 
proceedings seventh international conference architectural support programming languages operating systems asplos vii boston ma october 
mckinley carr 
tseng 
improving data locality loop transformations 
acm transactions programming languages systems july 
mehrotra harrison 
examination memory access classification scheme pointer intensive numeric programs 
proceedings th acm international conference supercomputing philadelphia pa may 
acm 
mellor crummey whalley kennedy 
improving memory hierarchy performance irregular applications 
proceedings acm international conference supercomputing rhodes greece june 
mitchell carter ferrante 
localizing non affine array 
proceedings international conference parallel architectures compilation techniques newport beach la october 
mowry 
tolerating latency software controlled data prefetching phd thesis 
technical report stanford university march 
mowry 
tolerating latency multiprocessors compiler inserted prefetching 
ieee transactions computer systems february 
mowry gupta 
tolerating latency prefetching shared memory multiprocessors 
journal parallel distributed computing june 
mowry lam gupta 
design evaluation compiler algorithm prefetching 
proceedings th international conference architectural support programming languages operating systems asplos pages 
acm october 
palacharla kessler 
evaluating stream buffers secondary cache replacement 
proceedings st annual international symposium computer architecture pages pp 
chicago il may 
acm 
panda nakamura dutt nicolau 
augmenting loop tiling data alignment improved cache performance 
ieee transactions computers february 
rivera 
tseng 
data transformations eliminating conflict misses 
proceedings sigplan conference programming language design implementation montreal canada june 
rivera 
tseng 
comparison compiler tiling algorithms 
proceedings th international conference compiler construction cc amsterdam netherlands march 
rivera 
tseng 
tiling optimizations scientific computations 
proceedings sc dallas tx november 
rogers carlisle reppy hendren 
supporting dynamic data structures distributed memory machines 
acm transactions programming languages systems march 
roth sohi 
dependence prefetching linked data structures 
proceedings eigth international conference architectural support programming languages operating systems october 
roth sohi 
effective jump pointer prefetching linked data structures 
proceedings th international symposium computer architecture atlanta ga may 
saavedra mao park moon 
combined effectiveness unimodular transformations tiling software prefetching 
proceedings th international parallel processing symposium pages april 
song li 
new tiling techniques improve cache temporal locality 
proceedings sigplan conference programming language design implementation atlanta ga may 

streaming prefetch 
proceedings europar lyon france 
wolf lam 
data locality optimizing algorithm 
proceedings sigplan conference programming language design implementation toronto canada june 
wulf mckee 
hitting memory wall implications obvious 
computer architecture news march 
