classification news stories support vector machines robert cooley cooley cs umn edu department computer science engineering university minnesota eecs bldg union st se minneapolis mn usa april data set data mining task classification main reasons performing feature space reduction 
improve accuracy algorithm 
domain text mining common technique parameterizing document vector words results thousands dimensions 
performance learning algorithms decreases dimensionality input space increases 
support vector machines svms vap vapnik statistical learning theory classification technique shown joachims joa reasonably immune high dimensionality text feature spaces 
second reason performing feature space reduction decrease size data set order conserve storage space minimize amount time required handle data run mining algorithms 
svms large data sets may warrant feature space reduction second class problems 
describes results experiment train svms classify print television radio news sources 
tests performed compare full text versus feature space reduction natural language processing technique reduction information gain 
results show size data set reduced order magnitude natural language processing results significant loss recall precision 
precision recall achieved svms trained full text information gain representations higher achieved nearest neighbors algorithm 
term weighting methods tfidf tf binary compared svms 
representations tfidf tf weights produced similar results binary weighting method resulted significant loss recall 
portion completed author employed mitre 
author normally supported nsf ehr 
text classification important component large information retrieval text mining system 
domains news media world wide web www infeasible manually classify new documents added system timely manner 
automatic methods document classification needed 
task especially difficult domains unstructured text documents titles abstracts keywords help determine document category 
systems version vector space model sm order parameterize corpus documents allow quantifiable calculations similarity documents 
number vector dimensions equal number unique words corpus dimensionality vector model quite large corpus reasonable size 
joa text classification problems linearly separable nature high dimensionality sparse representation 
unique word corpus represents dimension document contains small percentage words corpus 
support vector machines svms ideally suited problem text classification complexity model learned independent dimensionality feature space 
joachims shown svms outperform classification techniques feature space reduction reasons desirable reduce size data set 
large text collections storage space efficiency handling data concerns 
feature space reduction yang pedersen yp concluded methods information gain mit favor common terms provide best results text categorization :10.1.1.32.9956
due zipf distribution word frequency feature space reduction technique favors common terms result equivalent reduction total number terms data set 
techniques information gain unsuitable data set reduction 
natural language processing techniques identification proper nouns method feature space reduction 
methods biased common terms reduction feature space size data set achieved 
term frequency inverse document frequency tfidf sm common term weighting method text classification 
purpose inverse document frequency idf term weight indicate terms important classification 
calculating idf term requires count documents corpus support vector machines built method determining terms important identification support vectors 
simpler term weighting method produce similar results computational cost 
describes results experiments run text corpus broadcast print news stories test hypotheses ffl performance support vector machines adversely affected feature space data set reduction technique named entity identification 
ffl performance support vector machines degrade term weighting methods simpler tfidf 
ffl confirmation joachim results support vector machines superior methods nearest neighbors text classification 
svms trained tested full text text reduced identification named entities locations organizations people information gain 
benchmark results svm tests compared results nearest neighbors knn algorithm fri 
addition different term weighting schemes tfidf tf binary compared 
full text information gain svms outperform knn algorithm precision recall 
full text svms result significantly higher precision recall precision recall break point named entity svms disproving hypothesis listed 
information gain svms provided similar performance full text svms 
performance representations created tfidf tf weighting methods similar significant loss recall experienced binary term weighting 
rest organized follows section describes text classification process including feature space reduction term weighting section provides details regarding svms classification section presents experimental setup results section gives 
text classification virtually modern information retrieval systems full text documents classification storage retrieval rely version salton vector space model sm 
vector space model works defining unique word corpus documents separate dimension 
document described creating vector word document represented weight 
words appear document particular dimension vector set zero 
major decisions vector space model text classification reduce feature space reduction required weight terms classification method 
feature space reduction feature space reduction performed improve performance data mining learning algorithms 
important improving accuracy algorithm controlling computation time 
previously mentioned svms need feature space reduction order improve performance 
confirmed experimental results section 
feature space reduction desirable reasons limits storage space providing line processing capabilities 
usually referred feature space reduction techniques stopwords stemming algorithms effect limiting number unique terms vector space 
stopwords defined words known generic distinguishing documents 
articles prepositions frequently included stopword lists 
addition removing stopwords suffixes removed order prevent counting plural active forms words separately normal form 
referred stemming 
instance words running runs converted run avoid unnecessary feature space expansion 
text classification yang pedersen yp shown feature space reduction techniques favor common terms outperform favor rare terms :10.1.1.32.9956
information gain ig mit term weight meets criteria calculated conditional probability document category depending presence absence term document 
information gain ig term calculated follows categories ig gamma log jt log jt log artificial intelligence natural language communities employed attempt intelligent feature space reduction 
natural language techniques particular part speech identification pick important words document 
example nouns proper nouns verbs selected document ignoring parts speech 
subset proper nouns consisting people locations organizations referred named entities 
news stories usually contain named entities specific topic 
named entities biased commonly appearing words feature space reduction identification named entities result similar reduction size data set 
term weighting common weighting schemes referred term frequency inverse tfidf sm 
weighting scheme starts frequency term document tf multiplies inverse document frequency idf term corpus 
idf term lower documents appears 
tfidf formula term follows number documents corpus df document frequency term tf idf tf log df idea documents word appears measure distinguishing document 
added information contained idf needed particular algorithm just term frequency tf weighting scheme 
advantage tf calculated just single document having maintain update corpus wide document frequencies 
simpler term weighting scheme advantage binary weighting terms assigned weight missing assigned weight zero 
support vector machines support vector machine learning method introduced vapnik statistical learning theory vap structural risk minimization principle 
svms vc dimension problem characterize complexity independent dimensionality problem 
svms classification basic idea find optimal separating hyperplane positive negative examples 
optimal hyperplane defined giving maximum margin training examples support vector machine classification example closest hyperplane 
group examples vectors lie closest separating hyperplane referred support vectors 
separating hyperplane new examples classified simply checking side hyperplane fall 
simple linearly separable example shown dimensions 
dotted lines represent limits linear planes correctly classify training examples 
bold line labeled represents plane provides maximum separation classes 
support vectors shaded training examples completely define svms referred universal learners meaning various types representations polynomials radial basis functions neural networks 
support vector machine combines concepts described detail cm ffl structural risk minimization srm inductive principle 
ffl mapping input samples high dimensional space set predefined nonlinear basis functions 
ffl linear functions constraints complexity approximate input samples high dimensional space 
ffl duality theory optimization estimates model parameters highdimensional feature space computationally tractable 
classification optimal separating hyperplane defined values decision function equal zero 
set training data input space decision function defined ff ff maximal solution quadratic optimization problem ff ff gamma ff ff subject constraints ff ff inner product kernel regularization parameter 
polynomials degree inner product kernel equal delta radial basis functions form sin ff expf gamma jx gamma oe oe equal width inner product kernel equal expf gamma jx gamma oe separation model complexity dimensionality expected svms perform relative learning methods domains inherently high dimensionality 
complete discussion svms cm 
experimental results data set data set experiments comes news media sources 
print sources new york times associated press wire television sources abc world news tonight cnn headline news radio sources public radio international voice america collected sgml format months 
topics identified training development sets covering approximately documents documents topics identified 
news stories topics classified hand part second topic detection tracking initiative tdt sponsored linguistic data consortium nist 
experiments described data months training set data second months test set 
topics news story training test sets 
topics number documents set shown table 
documents training testing 
full text named entities ig text total training set terms test set unique terms table size full reduced representations number terms representations data set created full text reduced named entity identification reduced information gain different term weighting schemes 
full text representation created approach discussed joa 
consisted applying list standard stopwords porter stemming algorithm por 
named entity approach people organizations locations document 
named entities identified alembic workbench toolkit dah 
alembic uses rule set obtained supervised learning identify parts speech terms text document adds appropriate sgml tags 
table shows size full reduced text representations 
shown full text order magnitude larger named entity representation feature space total number terms 
information gain representation chosen feature space size similar named entity representation 
feature space reduction total size information gain representation large full text 
order get size information gain representation size named entity representation feature space reduced terms effectively classify categories 
notice number reported number unique words representation 
list unique words generated training set order meaningful comparison text vectors test set list words 
new words introduced test set play part classification documents 
shown full text named entities loaded relational database order separate documents terms training test sets calculate vector weights calculate information gain term 
test training sets representations weighting schemes output flat files order run classification algorithms 
size training test file full representation mb mb respectively 
reduced representations file sizes mb mb information gain representation mb mb named entity representation 
analysis svms full reduced representations different weights trained tested different parameters 
polynomials degrees tested radial basis functions gammas 
precision recall full representation tfidf weights shown tables 
bold values represent model lowest vc dimension topic 
shown topics linear model lowest vc dimension confirming joachim claim text categorization problems linearly separable 
method representation weighting recall precision break svm named entity tfidf svm named entity tf svm full text tfidf svm full text tf svm full text binary svm information gain tfidf knn named entity tfidf calculated knn information gain tfidf calculated table micro averaged recall precision break points text classified svms knn 
order compare representations methods svm lowest vc dimension topic taken break point precision recall 
precision recall break point varying classification threshold values precision recall equal 
break point topic micro averaged produce single value representation data 
micro averaging yan accomplished sgml relational conversion natural language processing alembic workbench named entity table tagged files dtd porter stemmer stopword removal stoplist sgml source files sgml relational conversion full text table term weight information gain calculation support vector machine training testing examples natural language ruleset preprocessing architecture sum true positive false positive false negative classifications topics 
single value precision recall calculated sums 
values summarized table 
shown svms trained full text representation significantly better precision recall compared named entity representation 
information gain representation performs better named entity representation svms knn 
results tf weighting scheme virtually tfidf weighting scheme break point recall lower binary weighting scheme 
notice micro averaged break point higher micro averaged precision recall conditions seemingly violating precision recall tradeoff 
artifact micro averaging topics large range number positive examples topic 
highlights fact method micro averaged break point useful comparing performance algorithms represent real value precision recall 
table shows precision topic full representation reduced representations knn neighbors information gain representation 
table displays recall categories 
knn algorithm run neighbors topics full information gain named entity representations 
results neighbors information gain representation consistently produced higher precision recall 
shown feature space reduction text categorization support vector machines 
news stories television radio print sources support vector machines trained tested full text minus stopwords stemming outperformed text representations created named entities information gain 
named entity representation providing feature space data set size reduction resulted lower precision recall information gain representation svms knn algorithm 
performance information gain representation slightly lower full text data set size reduction accompanying feature space reduction reason favor information gain full text 
shown unnecessary maintain tfidf weights text vectors svms simple tf weights provide similar results lower computational cost 
cm cherkassky 
learning data concepts theory methods 
john wiley sons new york 
dah day aberdeen hirschman kozierok robinson vilain 
mixed initiative development language processing systems 
fifth conference applied natural language processing washington 
fri friedman 
flexible nearest neighbor classification 
technical report stanford university 
joa thorsten joachims 
text categorization support vector machines learning relevant features 
european conference machine learning ecml 
mit tom mitchell 
machine learning 
mcgraw hill 
por porter 
algorithm suffix stripping 
automated library information systems 
sm salton mcgill 
smart sire experimental retrieval systems 
pages 
mcgraw hill new york 
tdt topic detection tracking project tdt 
vap vapnik 
nature statistical learning theory 
springer verlag new york 
yan yiming yang 
evaluation statistical approaches text categorization 
technical report cmu cs carnegie mellon university 
yp yiming yang jan pedersen :10.1.1.32.9956
comparative study feature selection text categorization 
international conference machine learning 
topic training set test set current conflict iraq asian economic crisis scandal winter olympics visit cuba violence algeria alabama clinic bombing lawsuit cable car crash sgt gene mckinney trial india elections florida tornado tucker execution casey martin pga law suit national tobacco settlement china airlines crash john glenn space assassination attempt navy dismissal balloon ride nyc quality life elections arrest buys silver palestinian raids james earl ray grossberg baby murder dr spock death african leaders meet world bank pres 
tourism table number documents training test sets topic polynomial radial basis function current conflict iraq asian economic crisis scandal winter olympics visit cuba violence algeria alabama clinic bombing lawsuit cable car crash sgt gene mckinney trial india elections florida tornado tucker execution casey martin pga law suit national tobacco settlement china airlines crash john glenn space assassination attempt navy dismissal balloon ride nyc quality life elections arrest buys silver palestinian raids james earl ray grossberg baby murder dr spock death african leaders meet world bank pres 
tourism table precision full representation svms topic polynomial radial basis function current conflict iraq asian economic crisis scandal winter olympics visit cuba violence algeria alabama clinic bombing lawsuit cable car crash sgt gene mckinney trial india elections florida tornado tucker execution casey martin pga law suit national tobacco settlement china airlines crash john glenn space assassination attempt navy dismissal balloon ride nyc quality life elections arrest buys silver palestinian raids james earl ray grossberg baby murder dr spock death african leaders meet world bank pres 
tourism table recall full representation svms topic full text named entities info 
gain knn current conflict iraq asian economic crisis scandal winter olympics visit cuba violence algeria alabama clinic bombing lawsuit cable car crash sgt gene mckinney trial india elections florida tornado tucker execution casey martin pga law suit national tobacco settlement china airlines crash john glenn space assassination attempt navy dismissal balloon ride nyc quality life elections arrest buys silver palestinian raids james earl ray grossberg baby murder dr spock death african leaders meet world bank pres 
tourism table precision svms knn tfidf weighting topic full text named entities info 
gain knn current conflict iraq asian economic crisis scandal winter olympics visit cuba violence algeria alabama clinic bombing lawsuit cable car crash sgt gene mckinney trial india elections florida tornado tucker execution casey martin pga law suit national tobacco settlement china airlines crash john glenn space assassination attempt navy dismissal balloon ride nyc quality life elections arrest buys silver palestinian raids james earl ray grossberg baby murder dr spock death african leaders meet world bank pres 
tourism table recall svms knn tfidf weighting 
