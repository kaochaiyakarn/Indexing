mdps semi mdps framework temporal abstraction reinforcement learning richard sutton precup university massachusetts amherst satinder singh university colorado boulder learning planning representing knowledge multiple levels temporal abstraction key longstanding challenges ai 
consider challenges addressed mathematical framework reinforcement learning markov decision processes mdps 
extend usual notion action framework include options closed loop policies action period time 
examples options include picking object going lunch traveling distant city primitive actions muscle joint torques 
show options enable temporally knowledge action included reinforcement learning framework natural general way 
particular show options may interchangeably primitive actions planning methods dynamic programming learning methods learning 
formally set options defined mdp constitutes embedded semi markov decision process smdp theory smdps provides foundation theory options 
interesting issues concern interplay underlying mdp smdp smdp theory 
results cases show results planning options execution interrupt options perform better planned introduce new intra option methods able learn option fragments execution propose notion subgoal improve options 
results precursors existing literature contribution establish simpler general setting fewer changes existing reinforcement learning framework 
particular show results mentioned obtained committing ruling particular approach state abstraction hierarchy function approximation utility problem 
everyday decisions people foresee consequences possible courses action multiple levels temporal abstraction 
consider traveler deciding undertake journey distant city 
decide go benefits trip weighed expense 
having decided go choices leg fly drive take taxi arrange ride 
steps involves foresight decision way smallest actions 
example just call taxi may involve finding telephone dialing digit individual muscle contractions lift receiver ear 
human decision making routinely involves choice courses action broad range time scales 
understand automate ability flexibly multiple overlapping time scales 
temporal abstraction explored ai early primarily context strips style planning fikes hart nilsson newell simon sacerdoti korf laird rosenbloom newell minton iba etzioni tambe drescher nilsson 
temporal abstraction focus appealing aspect qualitative modeling approaches ai kuipers de kleer brown dejong say brafman tennenholtz 
explored robotics control engineering maes brooks brockett grossman araujo grupen colombetti dorigo sastry 
consider temporal abstraction framework reinforcement learning markov decision processes mdps 
framework popular ai ability deal naturally stochastic environments integration learning planning barto bradtke singh dean boutilier brafman geib simmons koenig geffner bonet 
reinforcement learning methods proven effective number significant applications nie haykin appear singh bertsekas tesauro 
mdps conventionally conceived involve temporal abstraction temporally extended action 
discrete time step unitary action taken time affects state reward time 
notion course action persisting variable period time 
consequence conventional mdp methods unable take advantage efficiencies available higher levels temporal abstraction 
hand temporal abstraction introduced reinforcement learning possible ways sutton watkins ring wixson schmidhuber mahadevan connell singh lin dayan dayan hinton kaelbling singh chrisman sutton thrun schwartz asada ar ari dietterich huber grupen wiering schmidhuber precup sutton singh mcgovern sutton parr russell drummond hauskrecht meuleau :10.1.1.9.313
generalize simplify previous works form compact unified framework temporal abstraction reinforcement learning mdps 
answer question minimal extension reinforcement learning framework allows general treatment temporally knowledge action 
second part new framework develop new results generalizations previous results 
keys treating temporal abstraction minimal extension reinforcement learning framework build theory decision processes smdps see puterman 
smdps special kind mdp appropriate modeling continuous time systems 
actions smdps take variable amounts time intended model temporally extended courses action 
existing theory smdps specifies model results actions plan 
smdps reinforcement learning pioneered crites barto mahadevan 
parr 
existing smdp limited temporally extended actions treated indivisible unknown units 
attempt smdp theory look inside temporally extended actions examine modify comprised lower level actions 
tried suggest essence analyzing temporally actions ai applications goal directed behavior involves multiple overlapping scales decisions modified 
explore interplay mdps smdps 
base problem consider conventional discrete time mdp consider courses action mdp results state transitions extended variable duration 
term options fact base system smdp technical changes framework larger step away standard framework 
choice term may deserve explanation 
previous terms including macro actions behaviors actions structures closely related options 
introduce new term avoid confusion previous formulations informal terms 
term options meant generalization actions formally primitive choices 
inappropriate option course action large temporally extended fact exactly point 
wish treat smdp time mdp state options mdp state trajectory mdp small discretetime transitions smdp comprises larger continuous time transitions 
options enable mdp trajectory analyzed way 
courses action include primitive actions special case 
fixed set options defines discrete time smdp embedded original mdp suggested 
top panel shows state trajectory discrete time mdp middle panel shows larger state changes continuous time smdp panel shows levels analysis superimposed options 
case underlying base system mdp regular transitions options define potentially larger transitions smdp number discrete steps 
usual smdp theory applies superimposed smdp defined options addition explicit interpretation terms underlying mdp 
smdp actions options longer black boxes policies base mdp examined changed learned planned right 
part develops ideas formally fully 
sections review reinforcement learning framework generalization temporally extended action 
section focuses link smdp theory illustrates speedups planning learning possible temporal abstraction 
rest concerns ways going smdp analysis options change learn internal structure terms mdp 
section considers problem effectively combining set options single primitive temporally extended actions similarly prefer name refer 
policy 
example robot may pre designed controllers servoing joints positions picking objects visual search face difficult problem coordinate switch behaviors mahadevan connell matari sastry maes brooks koza rice dorigo colombetti ar 
sections concern intra option learning looking inside options learn simultaneously options consistent fragment experience 
section define notion subgoal shape learn new options 
reinforcement learning mdp framework section briefly review standard reinforcement learning framework discrete time finite markov decision processes mdps forms basis extensions temporally extended courses action 
framework learning agent interacts environment discrete lowest level time scale time step agent perceives state environment basis chooses primitive action response action environment produces step numerical reward state convenient suppress differences available actions states possible denote union action sets 
finite environment transition dynamics modeled step state transition probabilities ss prfs ag step expected rewards efr ag understood ss 
sets quantities constitute step model environment 
agent objective learn optimal markov policy mapping states probabilities available primitive action theta maximizes expected discounted reward state flr fl delta delta delta fi fi fi fi fi fi fl ss probability policy chooses action state fl discount rate parameter 
quantity called value state policy called state value function optimal state value function gives value state optimal policy max max fi fi fi max fl ss policy achieves maximum definition optimal policy 
optimal policy easily formed choosing state action achieves maximum 
planning reinforcement learning refers models environment compute value functions optimize improve policies 
particularly useful regard bellman equations recursively relate value functions 
treat values unknowns set bellman equations forms system equations unique solution fact 
fact key way temporal difference dynamic programming methods estimate value functions 
learning methods particular importance parallel set value functions bellman equations state action pairs states 
value action state policy denoted expected discounted reward starting henceforth flr fl delta delta delta fi fi fi fl ss fl ss known action value function policy optimal function max fl ss max tasks episodic nature involving repeated trials episodes reset standard state state distribution 
episodic tasks include single special terminal state arrival terminates current episode 
set regular states plus terminal state denoted ss general ranges set just stated earlier 
episodic task values defined expected cumulative reward termination infinite equivalently consider terminal state transition forever reward zero 
details background reinforcement learning framework sutton barto 
options term options generalization primitive actions include temporally extended courses action 
options consist components policy theta termination condition fi initiation set option hi fii available state option taken actions selected option terminates stochastically fi 
particular option taken state markov action selected probability distribution delta 
environment transition state option terminates probability fi continues determining delta possibly terminating fi 
option terminates agent opportunity select option 
example option named open door consist policy reaching grasping turning door knob termination condition recognizing door opened initiation set restricting consideration open door states door 
episodic tasks termination episode terminates current option fi maps terminal state options 
initiation set termination condition option restrict range application potentially useful way 
particular limit range option policy need defined 
example handcrafted policy mobile robot dock battery charger defined states battery charger sight 
termination condition fi defined outside termination condition fi plays role similar fi fi models sutton opposite sense 
fi corresponds gamma fi earlier 
robot successfully 
servoing robot arm particular joint configuration similarly set allowed starting states controller applied termination condition indicating target configuration reached tolerance unexpected event taken outside domain application 
markov options natural assume states option continue states option taken fs fi 
case need defined useful options timeout terminate period time elapsed failed reach particular state 
possible markov options termination decisions solely basis current state long option executing 
handle cases interest consider generalization semi markov options policies termination conditions may choices dependent prior events option initiated 
general option initiated time say determines actions selected number steps say terminates intermediate time decisions markov option may depend decisions semi markov option may depend entire sequence events prior 
call sequence history denote tt denote set histories omega gamma semi markov options policy termination condition functions possible histories omega theta fi omega 
semi markov case useful cases options detailed state representation available policy selects options hierarchical machines parr parr russell 
note hierarchical structures options select options give rise higher level options semi markov lowerlevel options markov 
semi markov options include general range possibilities 
set options initiation sets implicitly define set available options state sets available actions unify kinds sets noting actions considered special case options 
action corresponds option available available fs lasts exactly step fi selects 
consider agent choice time entirely options persist single time step temporally extended 
refer single step primitive options multi step options 
just case actions convenient suppress differences available options states 
denote set available options 
definition options crafted actions possible adding possibility temporally extended 
options terminate defined way consider sequences way consider sequences actions 
consider policies select options primitive actions model consequences selecting option model results action 
consider turn 
options consider sequence consider terminates terminates omitting altogether terminates state outside initiation set 
say options composed yield new option denoted ab corresponding way behaving 
composition markov options general semi markov markov actions chosen differently option terminates 
composition semi markov options semi markov option 
actions special cases options compose producing deterministic action sequence words classical macro operator 
interesting policies options 
initiated state markov policy options theta selects option probability distribution delta 
option taken determining actions terminates point new option selected delta 
way policy options determines conventional policy actions flat policy lat 
henceforth unqualified term policy policies options include flat policies special case 
note policy markov options selects markov corresponding flat policy markov options multi step temporally extended 
action selected flat policy state depends just option followed time depends stochastically entire history tt policy initiated time analogy semi markov options call policies depend histories way semi markov policies 
note semi markov policies specialized nonstationary policies 
nonstationary policies may depend arbitrarily preceding events policies may depend events back particular time 
stochastics decisions determined solely event subsequence time independent events preceding time 
ideas lead natural generalizations conventional value functions policy 
value state semi markov flat policy define expected return initiated def flr fl delta delta delta fi fi fi denotes event initiated time value state general policy defined value state corresponding flat policy def flat action value functions generalize option value functions 
define value option state policy def flr fl delta delta delta fi fi fi composition denotes semi markov policy follows terminates starts choosing resultant state 
semi markov options useful define event continuing time history continuing actions selected history preceded selected delta terminates probability fi ha doesn terminate selected ha delta 
definition holds just states history state 
section define generalizations optimal value functions 
smdp option option methods options closely related actions special kind decision problem known semi markov decision process smdp see puterman 
fact mdp fixed set options smdp state formally 
theorem really result simple observation follows immediately definitions 
theorem highlight state explicitly conditions consequences theorem mdp options smdp mdp set options defined mdp decision process selects options executing termination smdp 
proof sketch smdp consists set states set actions pair state action expected cumulative discounted reward defined joint distribution state transit time 
case set states set actions just set options 
expected reward state transit time distributions defined state option mdp option policy termination condition fi 
expectations distributions defined mdps markov options semi markov state reward time dependent option state initiated 
transit times options discrete simply special case arbitrary real intervals permitted smdps 
pi relationship mdps options smdps provides basis theory planning learning methods options 
sections discuss limitations theory due treatment options indivisible units internal structure section focus establishing benefits assurances provides 
establish theoretical foundations survey smdp methods planning learning options 
formalism slightly different results essence taken adapted prior including classical smdp bradtke duff parr parr russell singh sutton precup sutton precup sutton singh mcgovern sutton :10.1.1.32.9030
result similar theorem proved detail parr 
sections new methods improve smdp methods 
planning options requires model consequences 
fortunately appropriate form model options analogous ss defined earlier actions known existing smdp theory 
state option may started kind model predicts state option terminate total reward received way 
quantities discounted particular way 
option denote event initiated state time reward part model state flr delta delta delta fl gamma fi fi fi random time terminates 
state prediction part model state ss fl probability option terminates steps 
ss combination likelihood state terminates measure delayed outcome relative fl 
call kind model multi time model precup sutton describes outcome option single time potentially different times appropriately combined 
multi time models write bellman equations general policies options 
markov policy state value function written delta delta delta fl gamma fl fi fi fi duration option selected os ss bellman equation analogous 
corresponding bellman equation value option state delta delta delta fl gamma fl fi fi fi delta delta delta fl gamma fl os fi fi fi ss os note equations specialize earlier special case conventional policy conventional action 
note 
generalizations optimal value functions optimal bellman equations options policies options 
course conventional optimal value functions affected options ultimately just primitive actions options 
interesting know restricted set options include actions 
example planning consider high level options order find approximate plan quickly 
denote restricted set options set policies selecting options pi 
optimal value function select def max pi max os delta delta delta fl gamma fl fi fi fi duration taken note definition state predictions options differs slightly earlier primitive actions 
new definition model transition state primitive action simply corresponding transition probability transition probability times fl 
henceforth new definition 
max os ss max os fl fi fi fi denotes option initiated state conditional event usual random variables state terminates cumulative discounted reward way number time steps value functions bellman equations optimal option values def max pi delta delta delta fl gamma fl fi fi fi duration delta delta delta fl gamma fl max os fi fi fi ss max os fl max os fi fi fi reward number steps state due set options corresponding optimal policy denoted policy achieves states models options known optimal policies formed choosing proportion maximizing options 
known optimal policies model choosing state proportion options max 
way computing approximations key goals planning learning methods options 
smdp planning definitions mdp set options formally comprises smdp standard smdp methods results apply 
bellman equations options defines system equations unique solution corresponding value function 
bellman equations update rules dynamic planning methods finding value functions 
typically solution methods problem maintain approximation states options example synchronous value iteration svi options starts arbitrary approximation computes sequence new approximations fv max os ss gamma action value form svi starts arbitrary approximation computes sequence new approximations fq ss max gamma note algorithms reduce conventional value iteration algorithms special case standard results smdp theory guarantee processes converge general semi markov options lim lim sets options plans policies temporally options approximate sense achieve may maximum possible hand models find correct guaranteed achieve call value achievement property planning options 
contrasts planning methods state space generally guaranteed achieve planned values models correct dean lin 
simple illustration planning options consider rooms example gridworld environment rooms shown 
cells grid correspond states environment 
state agent perform actions left right stochastic effect 
probability actions cause agent move cell corresponding direction probability agent moves directions probability 
case movement take agent wall agent remains cell 
consider case rewards zero state transitions 
rooms provide built hallway options designed take agent room hallway cells leading room 
hallway option policy follows shortest path room target hallway minimizing chance stumbling hallway 
example policy hallway option shown 
termination condition fi hallway option zero states room states outside room including hallway states 
initiation set comprises states hallways multi step options right left room hallways stochastic primitive actions fail time rooms example gridworld environment stochastic cell cell actions room room hallway options 
hallway options suggested arrows labeled labels indicate locations goals experiments described text 
target hallway policy underlying hallway options 
room plus non target hallway state leading room 
note options deterministic markov option policy defined outside initiation set 
denote set hallway options option provide priori accurate model ss assuming goal state see 
note transition models ss nominally large order jij theta jsj fact sparse relatively little memory order jij theta needed hold nonzero transitions state adjacent hallway states 
consider sequence planning tasks navigating grid designated goal state particular hallway state labeled 
formally goal state state actions lead terminal state reward 
discount fl rooms example 
planning method svi various sets options initial value function goal state initialized correct value shown leftmost panels 
contrasts planning original actions planning hallway options original actions 
upper part shows value function iterations svi just primitive actions 
region accurately valued states moved cell iteration iterations states initial arbitrary value zero 
lower part shown corresponding value functions svi hallway options 
iteration states rooms adjacent goal state accurately valued second iteration states accurately valued 
values continued change small amounts subsequent iterations complete optimal policy known time 
planning step step hallway options enabled planning proceed higher level room room faster 
example particularly favorable case multi step options goal state hallway target state options 
consider case coincidence goal lies middle room state labeled 
hallway options models just previous experiment 
case planning models hallway options completely solve task take agent hallways goal state 
shows value functions target hallway states exceptions possible outcomes target hallway neighboring state target room 
iteration initial values iteration primitive options hallway options value functions formed iterations planning synchronous value iteration primitive actions hallway options 
hallway options enabled planning proceed room room cell cell 
area disk cell proportional estimated value state disk just fills cell represents value 
iteration initial values iteration iteration iteration iteration hallway options example goal different subgoal hallway options 
planning svi options initial progress due models primitive actions third iteration room room planning dominated greatly accelerated planning 
iterations svi hallway options options corresponding primitive actions 
iterations accurate values propagated cell iteration models corresponding primitive actions 
iterations hallway state reached subsequently room room planning temporally extended hallway options dominated 
note state lower right corner nonzero value iteration 
value corresponds plan going hallway state goal overwritten larger value corresponding direct route goal iteration 
options close approximation correct value function fourth iteration states steps goal non zero values time 
svi example particularly simple planning method potential advantage multi step options particularly clear 
large problems svi impractical number states large complete iterations 
practice necessary selective states updated options considered states considered 
issues resolved multi step options greatly aggravated 
options provide tool dealing flexibly 
planning options need complex planning actions 
svi experiments primitive options hallway options state hallway options needed considered 
addition models primitive actions generate possible successors non zero probability multi step options generate 
planning multi step options computationally cheaper conventional svi case 
second experiment case multi step options greatly increase computational costs 
general course guarantee multi step options reduce expense planning 
example hauskrecht 
shown adding multi step options may slow svi initial value function optimistic 
research deterministic macro operators identified related utility problem macros see etzioni minton tambe newell rosenbloom greiner gratch dejong 
temporal abstraction provides flexibility greatly reduce computational complexity opposite effect indiscriminately 
issues scope consider 
smdp value learning problem finding optimal policy set options addressed learning methods 
mdp augmented options smdp apply smdp learning methods developed bradtke duff parr russell parr mahadevan 
mcgovern sutton 
planning methods discussed option viewed indivisible opaque unit 
execution option started state jump state terminates 
experience approximate option value function updated 
example smdp version step qlearning bradtke duff call smdp learning updates option termination ff fl max gamma denotes number time steps denotes cumulative discounted reward time implicit step size parameter ff may depend arbitrarily states option time steps :10.1.1.32.9030:10.1.1.32.9030
estimate converges conditions similar conventional learning parr easy determine optimal policy described earlier 
illustration applied smdp learning rooms example goal case planning different sets options cases options selected set ffl greedy method 
options usually selected random maximal option value max os probability ffl option selected randomly available options 
probability random action ffl experiments 
initial state trial upper left corner 
shows learning curves goals sets options 
cases multi step options caused goal reached quickly trial 
goal methods maintained advantage conventional learning experiment presumably exploration 
results similar goal method performed worse long term 
best solution requires steps primitive actions hallway options find best solution running hallways 
reason advantages method method reduced 
episodes episodes steps episode goal goal performance smdp learning rooms example various goals sets options 
episodes data points averages groups episodes trends clearer 
step size parameter optimized nearest power goal set options 
results shown ff cases ff ff 
interrupting options smdp methods apply options treated opaque indivisible units 
interesting potentially powerful methods possible looking inside options altering internal structure rest 
section take step altering options useful 
area working simultaneously terms mdps smdps relevant 
analyze options terms smdp mdp interpretation change produce new smdp 
particular section consider interrupting options termination conditions choose effect creating new termination condition new option 
note treating options indivisible units smdp methods limiting unnecessary way 
option selected methods require policy followed option terminates 
suppose determined option value function policy state option pairs encountered function tells committing option reevaluate commitment step 
suppose time midst executing option markov compare value continuing value interrupting selecting new option 
highly valued interrupt allow switch 
section prove new way behaving better 
demonstration effect improved performance interrupting temporally extended substep value function planning higher level may kaelbling 
formally prove improvement general setting 
theorem characterize new way behaving policy original policy new set options new option corresponding old option terminates switching better continuing words termination condition fi fi 
call interrupted policy theorem slightly general require interruption state done 
weakens requirement completely known 
important generalization theorem applies semi markov options just markov options 
generalization may result intuitively accessible reading 
fortunately result read restricted markov case simply replacing occurrence history state set histories omega gamma set states theorem interruption mdp set options markov policy theta define new set options mapping option sets follows hi fii define corresponding hi fi fi fi history ends state may choose set fi 
histories termination conditions changed way called interrupted histories 
interrupted policy option corresponding 

state non zero probability encountering interrupted history initiating 
proof shortly show arbitrary start state executing option interrupted policy policy worse policy words show inequality holds ss ss true expand left hand side repeatedly replacing occurrence left corresponding xx 
limit left hand side proving prove inequality note show ss ss follows 
gamma denote set interrupted histories gamma fh omega fi fi ss fl fi fi fi gamma fl fi fi fi gamma state cumulative reward number elapsed steps option history trajectories encountering history gamma encounter history gamma occur probability expected reward executing option state continue trajectories encountering history gamma option termination follow policy get fl fi fi fi gamma fi fl gamma fi fl fi fi fi gamma ss option semi markov 
proves gamma 
note strict inequality holds history gamma ends trajectory generated non zero probability 
pi application result consider case optimal policy set markov options discussed planning learning determine optimal value functions optimal policy achieves 
best done changing smdp defined best possible achievable mdp course typically wish directly primitive options computational expense 
interruption theorem gives way improving little additional computation stepping outside step interrupt current option switch new option valued highly checking options typically done vastly expense time step involved combinatorial process computing sense interruption gives nearly free improvement smdp planning learning method computes intermediate step 
extreme case interrupt step switch greedy option option state highly valued polling execution dietterich :10.1.1.9.313
case options followed step superfluous 
options play role determining basis greedy switches recall multi step options may enable quickly section 
multi step options followed step provide substantial advantages computation theoretical understanding 
shows simple example 
task navigate start location goal location continuous dimensional state space 
actions movements direction current state 
low level actions infinite number introduce landmark locations space 
landmark define controller takes landmark direct path cf 
moore 
controller applicable limited range states case certain distance corresponding landmark 
controller defines option circular region controller landmark option initiation set controller policy arrival target landmark termination condition 
denote set landmark options action goal location transitions terminal state discount rate fl reward gamma transitions minimum time task 
landmarks coincides goal possible reach goal picking optimal policy runs landmark landmark shown thin line upper panel 
optimal solution smdp defined best picking options 
course better options followed way landmark 
trajectory shown thick line cuts corners shorter 
interrupted policy respect smdp optimal policy 
interrupted policy takes steps start goal optimal policy primitive actions steps better nominal additional cost smdp optimal policy takes steps 
state value functions policies shown lower part 
note values interrupted policy greater values smdp solution steps interrupted solution steps range input set run landmark controller landmarks smdp value function landmarks problem values interruption interruption improve navigation landmark directed controllers 
task top navigate minimum time options controllers run landmarks black dots 
circles show region landmark controllers operate 
thin line shows smdp solution optimal behavior uses controllers interrupting thick line shows corresponding solution interruption cuts corners 
lower panels show state value functions smdp interruption solutions 
position velocity interrupted solution steps smdp solution steps phase space plot smdp interrupted policies simple dynamical task 
system mass moving dimension gamma position velocity coefficient friction action applied force 
controllers provided options drives position zero velocity 
whichever option followed time target position determines action taken gamma 
original policy 
shows results example controllers options dynamics 
task move mass dimension rest position rest position minimum time 
option takes system way option takes option takes position greater 
options control system precisely target position zero velocity terminating correct ffl 
just options best done move precisely rest option re accelerate move second option 
smdp optimal solution slower corresponding interrupted policy shown 
need slow near zero velocity takes time steps interrupted policy takes steps 
intra option model learning section introduce new method learning model ss deterministic markov option experience knowledge fi 
semi markov option general approach execute option termination times state recording case resultant state cumulative discounted reward elapsed time outcomes averaged approximate expected values ss 
example incremental learning rule update model execution ff gamma sx sx ff fl ffi xs gamma sx ffi sx step size parameter ff may constant may depend state option time 
example ff divided number times experienced updates maintain estimates sample averages experienced outcomes 
averaging done call smdp model learning methods smdp value learning methods jumping initiation termination option ignoring happens way 
special case primitive action smdp model learning methods reduce learn conventional step models actions 
drawback smdp model learning methods improve model option option terminates 
nonterminating options applied option time option executing time 
markov options special temporal difference methods learn usefully model option option terminates 
call intra option methods learn experience single option 
intra option methods learn model option executing option long selections consistent option 
intra option methods examples policy learning methods sutton barto learn consequences policy behaving potentially different policy :10.1.1.32.7692
intra option methods simultaneously learn models different options experience 
intra option methods introduced sutton prediction problem single unchanging policy full control case consider 
just bellman equations value functions bellman equations models options 
consider intra option learning model markov option hi fii 
correct model related fl gamma fi reward state action taken state ss gamma fi sx fle gamma fi fi ffi ss gamma fi fi ffi turn bellman equations update rules learning model 
consider action taken way selected consistent hi fii selected distribution delta 
bellman equations suggest temporal difference update rules ff fl gamma fi gamma ff fl gamma fi ffi gamma ss estimates ss respectively ff positive step size parameter 
method call step model learning applies updates option consistent action taken course just simplest intra option method 
may possible eligibility traces standard tricks policy learning sutton 
illustration consider smdp intra option model learning rooms example 
assume hallway options assume models learned 
experiment rewards selected normal probability distribution standard deviation mean different state action pair 
means selected randomly run uniformly gamma interval 
experience generated selecting randomly state options executed options executed smdp intra smdp smdp intra smdp reward prediction error state prediction error max error avg 
error smdp smdp smdp intra intra smdp max error avg 
error model learning smdp intra option methods 
shown average maximum absolute errors learned true models averaged hallway options repetitions experiment 
lines labeled smdp smdp method sample averages ff 
possible options possible actions goal state 
smdp model learning method equations applied option terminated intra option model learning method equations applied step options consistent action taken step 
example options deterministic consistency action selected means simply option selected action 
method tried range values step size parameter ff results shown value best method happened ff cases 
smdp method show results step size parameter set model estimates sample averages give best possible performance method lines labeled 
shows average maximum errors state option space method averaged options repetitions experiment 
expected intra option method able learn significantly faster smdp methods 
intra option value learning turn intra option learning option values optimal policies options 
options semi markov smdp methods described section probably feasible methods semi markov option completed evaluated way 
options markov willing look inside consider intra option methods 
just case model learning intra option methods value learning potentially efficient smdp methods extract training examples experience 
example suppose learning approximate markov 
execution smdp methods extract single training example 
markov sense initiated steps jumps intermediate valid experiences experiences improve estimates 
consider option similar selected actions terminated step formally different option formally executed experience learning relevant 
fact option learn experience slightly related occasionally selecting actions generated executing option 
idea policy training full experience occurs learn possible options irrespective role generating experience 
best experience policy intra option version learning 
convenient introduce new notation value state option pair option markov executing arrival state gamma fi fi max write bellman equations relate expected values immediate successor initiating markov option hi fii flu fi fi fi ss immediate reward arrival consider learning methods bellman equation 
suppose action taken state produce state reward selected way consistent markov policy option hi fii 
suppose selected distribution delta 
bellman equation suggests applying policy step update ff flu gamma gamma fi fi max method call step intra option learning applies update rule option consistent action taken note algorithm potentially dependent order options updated 
theorem convergence intra option learning set deterministic markov options step intra option learning converges optimal values option regardless options executed learning provided primitive action gets executed state infinitely 
proof sketch experiencing transition option picks action state intra option learning performs update ff flu gamma action selection deterministic markov option hi fii 
result follows directly theorem jaakkola jordan singh observation expected value update operator flu yields contraction proved flu gamma jr ss gamma jr ss gamma gamma ss ss gamma fi gamma fi max gamma max ss max jq gamma fl max jq gamma pi episodes episodes option values average value greedy policy learned value learned value upper hallway option left hallway option true value true value value optimal policy learning option values intra option methods selecting options 
experience generated selecting randomly primitive actions goal shown left value greedy policy averaged states repetitions experiment compared value optimal policy 
right panel shows learned option values state approaching correct values 
illustration applied intra option method rooms example time goal rightmost hallway cell 
actions selected randomly equal probability primitives 
update applied primitive options hallway options consistent action 
hallway options updated clockwise order starting hallways faced current state 
rewards experiment previous section 
shows learning curves demonstrating effective learning option values selecting corresponding options 
intra option versions reinforcement learning methods sarsa td eligibility trace versions sarsa learning straightforward experience 
bellman equation intra option planning 
learning options achieve subgoals important aspect working mdps smdps options making smdp actions may changed 
seen way done changing termination conditions 
fundamental changing policies consider briefly section 
natural think options achieving subgoals kind adapt option policy better achieve subgoal 
example option open door natural adapt policy time effective efficient opening door may generally useful 
subgoals options relatively straightforward design policy intra option learning methods adapt policies better achieve subgoals 
example may possible simply apply learning learn independently subgoal option singh lin dorigo colombetti thrun schwartz 
hand clear best way formulate subgoals associate options basis evaluation 
important considerations extent models options constructed achieve subgoal transferred aid planning achieve 
long lived learning agent face continuing series subtasks result capable 
section briefly simple approach associating subgoals suffices illustrate possibilities problems arise 
larger issue address source subgoals 
assume subgoals focus options learned tuned achieve learning different subgoals aid 
simple way formulate subgoal assigning subgoal value state subset states values indicate desirable terminate state example learn hallway option rooms task target hallway assigned subgoal value hallway states outside room assigned subgoal value 
denote set options terminate states defined fi fi 
function define new state value function denoted options expected value cumulative reward option initiated state plus subgoal value state terminates 
similarly define new action value function ao actions options define optimal value functions subgoal max og max og 
finding option achieves optimal option subgoal defined subtask 
markov options subtask bellman equations methods learning planning just original task 
example time steps rms error hallway subtask values time steps upper hallway task true values learned values lower hallway task values tasks learning subgoal achieving hallway options random behavior 
shown left error averaged repetitions 
right panel shows learned values options state maximum action values approaching correct values 
step tabular learning method updating estimate ff fl max gamma ff flg gamma simple example applied method learn policies hallway options rooms example 
option assigned subgoal values target hallway states outside option room including target hallway 
initial state upper left corner actions selected randomly equal probability goal state 
parameters fl ff 
rewards zero 
shows learned action values subgoals options reliably approaching ideal values 
interesting note general optimal policies achieving subgoals depend detail precise values assigned subgoal states 
example suppose nonzero expected rewards introduced rooms task distributed uniformly gamma 
subgoal value target hallway results optimal policy goes different optimal policies options different subgoal values target hallway 
subgoal value left results direct policy subgoal 
directly target hallway away hallway shown left subgoal value may result optimal policy goes indirectly target hallway shown right 
roundabout path may preferable case avoid unusually large penalties 
extreme may optimal head target hallway spend infinite amount time running corner reach subgoal state 
problem merely illustrates flexibility subgoal formulation 
example may want options open door opens door easy example unlocked opens door matter example breaking need 
option able break door necessary second able choose open door committing breaking locked greatly diminish option usefulness 
ability learn represent options different intensities subgoals different balances outcome values important flexibility 
subgoals options models options enable interesting new possibilities reinforcement learning agents 
example agent series tasks subgoals graded difficulty 
agent directed find option achieves subgoal learn model option 
option model constructed task note transferred task 
option just says behaving way useful substep task help task 
similarly model just predicts consequences behaving way way behaving useful substep task model help planning substep 
long model accurate option may useful planning solution task 
singh lin subgoal hallway option transfer 
option passing lower left room state subgoal value longer works state subgoal value gamma 
original model option respect subgoal 
provide simple examples learning solutions subtasks transferring help solve new task 
hand assuring models options remain accurate changes tasks subgoals far immediate 
severe problem arises new subgoal prevents successful completion option model previously learned 
illustrates problem rooms example 
assume options models learned new subgoal considered assigns high value state lower right room low value gamma state passed enter room room 
gamma subgoal state impossible pass rooms subgoal considers options terminate subgoal states low value state undesirable try 
prior model indicates possible travel lower left room gamma state hallway state valued state 
planning model lead inevitably highly valued poor policy 
problems arise new subgoal involves states may passed option executed 
may feasible detect prevent problems 
idea keep track states option passes invalidate options models pass subgoal states 
idea alter subgoal formulation subgoal states passed stopping collecting subgoal value optional required 
note require models accurate just non precup sutton predict correct outcome just outcome equal expected value correct outcome 
finesse may enable important special cases handled simply 
example new subgoal involving states subgoal value singleton probably safely transferred 
sort problem shown occur cases 
representing knowledge flexibly multiple levels temporal abstraction potential greatly speed planning learning large problems 
introduced framework doing context reinforcement learning mdps 
context enables handle stochastic environments closed loop policies goals general way possible classical ai approaches temporal abstraction 
framework clear learned interpreted mechanically shown exhibiting simple procedures learning planning options learning models options creating new options subgoals 
compared formulations temporal abstraction reinforcement learning general requires fewer deviations standard reinforcement learning framework 
possible focused exclusively temporal abstraction simultaneously proposing solutions issues state abstraction hierarchical execution utility problems 
minimalist strategy maximizes generality results 
having committed particular approach results apply possibilities 
foundation theory options provided existing theory smdps associated learning methods 
fact set options defines smdp provides rich set planning learning methods convergence theory immediate natural general way analyzing mixtures actions different time scales 
theory offers lot interesting cases involve interrupting constructing decomposing options constituent parts 
intermediate ground mdps smdps richest possibilities new algorithms results 
broken ground touched issues far left done 
key issues transfer subtasks source subgoals integration state abstraction remain incompletely understood 
connection options smdps provides foundation addressing issues 
emphasized temporally extended action interesting note may implications temporally extended perception 
common recognize action perception intimately linked 
see objects room label locate know opportunities afford action door open chair sit book read person talk 
temporally extended actions modeled options models options correspond perceptions 
consider robot learning recognize battery charger 
useful concept set states successfully dock charger 
exactly concept produced model docking option 
kinds action oriented concepts appealing tested learned robot external supervision shown 
authors gratefully acknowledge substantial help received colleagues shared related results ideas long period preparation especially amy mcgovern ron parr tom dietterich andrew fagg ravindran manfred huber andy barto 
leo ari paul cohen moll mance harmon engelbrecht ted perkins 
supported nsf ecs afosr andrew barto richard sutton 
precup acknowledges support fulbright foundation 
satinder singh supported nsf iis 
araujo grupen 

learning control composition complex environment 
proceedings fourth international conference simulation adaptive behavior pp 

asada noda 

purposive behavior acquisition real robot vision reinforcement learning 
machine learning 
barto bradtke singh 

learning act realtime dynamic programming 
artificial intelligence 
boutilier brafman geib 

prioritized goal decomposition markov decision processes synthesis classical decision theoretic planning 
proceedings fifteenth international joint conference artificial intelligence pp 

bradtke duff 

reinforcement learning methods continuous time markov decision problems 
advances neural information processing systems 
mit press cambridge ma 
brafman tennenholtz 

modeling agents qualitative decision makers 
artificial intelligence 
brockett 

hybrid models motion control systems 
essays control perspectives theory applications pp 

birkhauser boston 
chrisman 

reasoning probabilistic actions multiple levels granularity aaai spring symposium decision theoretic planning stanford university 
colombetti dorigo 

behavior analysis training methodology behavior engineering 
ieee transactions systems man cybernetics part crites barto 

improving elevator performance reinforcement learning 
advances neural information processing systems 
mit press cambridge ma 
dayan 

improving generalization temporal difference learning successor representation 
neural computation 
dayan hinton 

feudal reinforcement learning 
advances neural information processing systems 
san mateo ca morgan kaufmann 
de kleer brown 

qualitative physics 
artificial intelligence 
dean kaelbling kirman nicholson 

planning time constraints stochastic domains 
artificial intelligence 
dean lin 

decomposition techniques planning stochastic domains 
proceedings fourteenth international joint conference artificial intelligence pp 

morgan kaufmann 
see technical report cs brown university department computer science 
dejong 

learning plan continuous domains 
artificial intelligence 
dietterich 

maxq method hierarchical reinforcement learning 
machine learning proceedings fifteenth international conference pp 

morgan kaufman 
dorigo colombetti 

robot shaping developing autonomous agents learning 
artificial intelligence 
drescher 

minds constructivist approach arti intelligence 
mit press 
drummond 

composing functions speed reinforcement learning changing world 
proceedings tenth european conference machine learning 
springer verlag 
etzioni 

prodigy ebl works 
proceeding eighth national conference artificial intelligence pp 

boston mit press 
fikes hart nilsson 

learning executing generalized robot plans 
artificial intelligence 
geffner bonet 

high level planning control incomplete information pomdps 
proceedings aips workshop integrating planning scheduling execution dynamic uncertain environments 
gratch dejong 

statistical approach adaptive problem solving 
artificial intelligence 
greiner 

statistical approach solving ebl utility problem proceedings aaai 
grossman nerode ravn rischel 

hybrid systems 
springer verlag new york 
hauskrecht meuleau boutilier kaelbling dean 

hierarchical solution markov decision processes macro actions 
uncertainty intelligence proceedings fourteenth conference pp 

huber grupen 

feedback control structure line learning tasks 
robotics autonomous systems 
iba 

heuristic approach discovery macro operators 
machine learning 
jaakkola jordan singh 

convergence stochastic iterative dynamic programming algorithms 
neural computation 
kaelbling 

hierarchical learning stochastic domains preliminary results 
proc 
tenth int 
conf 
machine learning pp 
morgan kaufmann 
ar zs ari cs 

module reinforcement learning experiments real robot 
machine learning autonomous robots special joint issue 
korf 

learning solve problems searching 
boston pitman publishers 
koza rice 

automatic programming robots genetic programming 
proceedings tenth national conference artificial intelligence pp 

kuipers 

commonsense knowledge space learning experience 
proc 
ijcai pp 

laird rosenbloom newell 

chunking soar anatomy general learning mechanism 
machine learning 
lin 

reinforcement learning robots neural networks 
phd thesis carnegie mellon university 
technical report cmu cs 
maes brooks 

learning coordinate behaviors 
proceedings aaai pp 

mahadevan connell 

automatic programming robots reinforcement learning 
artificial intelligence 
mahadevan das 

self improving factory simulation continuous time average reward reinforcement learning 
proceedings th international conference machine learning 
schulte tsitsiklis 

reinforcement learning call admission control routing integrated service networks 
advances neural information processing systems 
san mateo morgan kaufmann 
mcgovern sutton 
macro actions reinforcement learning empirical analysis 
technical report university massachusetts department computer science 
meuleau hauskrecht kim peshkin kaelbling dean boutilier 

solving large weakly coupled markov decision processes 
proceedings fifteenth national conference artificial intelligence 
minton 

learning search control knowledge approach 
kluwer academic 
minton 

quantitative results concerning learning 
artificial intelligence 
moore 

parti game algorithm variable resolution reinforcement learning multidimensional spaces advances neural information processing systems mit press cambridge ma 
newell simon 

human problem solving 
prentice hall englewood cliffs nj 
nie haykin 
appear 
learning dynamic channel assignment technique mobile communication systems 
ieee transactions vehicular technology 
nilsson 

reactive programs agent control 
journal artificial intelligence research 
parr russell 

reinforcement learning hierarchies machines 
advances neural information processing systems 
mit press cambridge ma 
parr 

hierarchical control learning markov decision processes 
phd thesis university california berkeley 
precup sutton 

multi time models reinforcement learning 
proceedings icml workshop modeling reinforcement learning 
precup sutton 

multi time models temporally planning 
advances neural information processing systems 
mit press cambridge ma 
precup sutton singh 

planning closed loop macro actions 
working notes aaai fall symposium model directed autonomous systems 
precup sutton singh 

theoretical results reinforcement learning temporally options 
proceedings tenth european conference machine learning 
springer verlag 
puterman 

markov decision problems 
wiley new york 
ring 

incremental development complex behaviors automatic construction sensory motor hierarchies 
proceedings eighth international conference machine learning pp 
morgan kaufmann 
sacerdoti 

planning hierarchy abstraction spaces 
artificial intelligence 
sastry 

algorithms design hybrid systems 
proceedings international conference information sciences 
say 

qualitative system identification deriving structure behavior 
artificial intelligence 
schmidhuber 

neural sequence 
technische universitat munchen tr 
simmons koenig 

probabilistic robot navigation partially observable environments 
proceedings fourteenth international joint conference artificial intelligence pp 

morgan kaufmann 
singh 

reinforcement learning hierarchy models 
proceedings tenth national conference artificial intelligence pp 

mit aaai press 
singh 

scaling reinforcement learning learning variable tem resolution models 
proceedings ninth international conference machine learning pp 
morgan kaufmann 
singh 

transfer learning composing solutions elemental sequential tasks 
machine learning 
singh 

efficient learning multiple task sequences 
advances neural information processing systems morgan kaufmann 
singh barto grupen connolly 

robust reinforcement learning motion planning 
advances neural information processing systems morgan kaufmann 
singh bertsekas 

reinforcement learning dynamic channel allocation cellular telephone systems 
advances neural information processing systems 
mit press 
sutton 

td models modeling world mixture time scales 
proceedings twelfth international conference machine learning pp 
morgan kaufmann 
sutton barto 

reinforcement learning 
mit press cambridge ma 
sutton 

learning world models connectionist networks 
proc 
seventh annual conf 
cognitive science society pp 

tambe newell rosenbloom 

problem expensive chunks solution restricting expressiveness 
machine learning 
tesauro 

temporal difference learning td gammon 
communications acm 
thrun schwartz 

finding structure reinforcement learning 
advances neural information processing systems 
san mateo morgan kaufmann 
asada 

behavior coordination mobile robot modular reinforcement learning 
proceedings ieee rsj international conference intelligent robots systems pp 

watkins 

learning delayed rewards 
phd thesis cambridge university 
wiering schmidhuber 

hq learning 
adaptive behavior 
wixson 

scaling reinforcement learning techniques modularity proc 
eighth int 
conf 
machine learning pp 
morgan kaufmann 

