discovering structure multiple learning tasks tc algorithm sebastian thrun joseph sullivan computer science department carnegie mellon university pittsburgh pa www cs cmu edu thrun proceedings thirteenth international conference machine learning saitta ed morgan kaufmann san mateo ca increased interest lifelong machine learning methods transfer knowledge multiple learning tasks 
methods repeatedly outperform conventional single task learning algorithms learning tasks appropriately related 
increase robustness approaches methods desirable reason relatedness tasks order avoid danger arising tasks unrelated potentially misleading 
describes task clustering tc algorithm 
tc clusters learning tasks classes mutually related tasks 
facing new learning task tc determines related task cluster exploits information selectively task cluster 
empirical study carried mobile robot domain shows tc outperforms non selective counterpart situations small number tasks relevant 
exciting new developments field machine learning algorithms gradually improve ability learn applied sequence learning tasks 
motivated observation humans encounter just single learning task lifetime successfully improve ability learn researchers proposed algorithms able acquire domain specific knowledge re learning tasks 
example context face recognition methods developed improve recognition accuracy significantly learning recognize face transferring face specific invariances learned previous face recognition tasks 
similar results context object recognition robot navigation chess reported 
author affiliated university bonn germany part research carried 
technically speaking problem learning multiple tasks stated follows 

training data current learning task training set 
training data previous learning tasks called support sets 
performance measure find hypothesis maximizes performance current th learning task 
notice item list data previous learning tasks support sets appear usual formulation machine learning problems 
support data indirectly related carry different class labels 
utilize data mechanisms required acquire re domain specific knowledge order guide generalization knowledgeable way 
date available variety strategies transfer domain specific knowledge multiple learning tasks see detailed survey comparison ffl learning internal representations artificial neural networks ffl learning distance metrics ffl learning re represent data ffl learning invariances classification ffl learning algorithmic parameters choosing algorithms ffl learning domain models :10.1.1.30.5817
approaches demonstrated empirically reduce sample complexity learning task 
weigh previous learning tasks equally strongly transferring knowledge may fail small subset learning tasks related appropriately 
example approaches object recognition described generalize better previous learning tasks involve recognition objects learner faced previously unrelated learning tasks stock market prediction approaches fail due non selective nature transfer mechanisms 
consequently common practice human designers pick set tasks known related appropriately 
overcome ous limitation current approaches desirable design algorithms discover relation multiple learning tasks transfer knowledge selectively related learning tasks 
describes algorithm called tc task clustering algorithm 
previous methods tc transfers knowledge selectively related set learning tasks 
order tc estimates mutual relatedness tasks builds entire hierarchy classes learning tasks 
new learning task arrives tc determines related task cluster hierarchy previous learning tasks 
knowledge transferred selectively single cluster task clusters employed 
clustering strategy enables tc handle multiple classes tasks may exhibit different characteristics 
elucidate tc practice reports results series experiments carried mobile robot domain 
key results empirical study 
sample complexity reduced significantly domain specific knowledge transferred previous learning tasks 

tc reliably succeeds partitioning task space surprisingly meaningful hierarchy related tasks 

selective transfer significantly improves results cases support tasks relevant hurt performance support tasks appropriately related 
tc algorithm tc algorithm designed support fast learning large sets binary classification tasks defined input space 
research driven interest fast data efficient robot learning algorithms 
tc introduced steps adopted literature 
nearest neighbor generalization underlying function approximation level tc algorithm uses nearest neighbor generalization see 
determine proximity data points tc uses globally weighted euclidean distance metric dist gamma denotes adjustable vector weighting factors superscript refer th component vector 
parameterizes space euclidean distance metrics 
obviously determines generalization properties nearest neighbor 
adjusting distance metric tc transfers knowledge learning tasks adjusting tasks re 
ideas done minimizing distance training examples belong class maximizing distance training examples opposite class labels en ffi xy dist gamma 
min ffi xy ae class class gamma class class subscript denotes particular learning task 
argmin denote parameter vector minimizes henceforth called optimal dist corresponding optimal distance metric 
minimizing distance class class simultaneously maximizing distance class class dist focuses relevant input dimensions th learning task 
implementation gradient descent 
notice optimized simultaneously multiple learning tasks 
ae ng denote subset support tasks 
argmin en optimal parameter vector dist corresponding distance metric task set task transfer matrix optimal distance metric obtained task learning task improve results tasks demand similar feature weighting 
determine degree tasks related tc computes matrix called task transfer matrix 
task transfer matrix contains value pair learning tasks expected generalization accuracy obtained task optimal distance metric 
element estimated fold cross validation optimal distance metric task training set task task transfer matrix basis clustering tasks building task hierarchies 
clustering tasks task hierarchy tc clusters learning tasks tn disjunct bins denoted done maximizing functional ja measures averaged estimated generalization accuracy obtained task uses optimal distance metrics task cluster 
words maximizing groups tasks related transferring optimal distance metrics leads largest performance gain 
notice resulting task clusters defines cluster specific optimal distance metric argmin en obtained minimizing ea cf 

repeating clustering process different values 
figures page show examples task hierarchies explained detail section 
selective transfer novel tasks new learning task arrives tc identifies related task cluster hierarchy previous learning tasks 
done minimizing task clusters denotes task transfer coefficient optimal distance metric task cluster 
notice appropriate number clusters unknown entire hierarchy consulted searching related task cluster 
having determined appropriate task cluster tc uses optimal distance metric task cluster nearest neighbor generalization new task 
summarize steps tc algorithm 
tc classifies nearest neighbor globally weighted distance metric 

transfers knowledge multiple learning tasks learning distance metric tasks re nearest neighbor generalization 

focus transfer related tasks tc computes task transfer matrix measures mutual relation learning tasks 

constructs task hierarchy clustering learning tasks task transfer matrix 

facing new thing learn distance metric transferred selectively related task cluster 
experimental results setup evaluate tc variety circumstances applied different families binary classification tasks obtained set databases shown 
databases collected mobile robot shown color camera top robot sonar proximity sensors arranged ring robot input 
database consists snapshots show examples counterexamples particular concept persons landmarks objects locations 
datasets constructed particular person front notice maximizing defined pairwise matrix understood combinatorial data clustering problem various algorithms exist see example 
mobile robot xavier equipped camera sonar sensors 
robot 
different persons wore different clothes recognition involved spotting certain colors 
remaining datasets contained snapshots landmark blue trash bin situations obstacle front robot doors open closed doors collection random snapshots taken particular location lab vs hallway 
defined families learning tasks ffl task family consists thirteen tasks involving people landmarks locations databases 
new task testing task task recognizing status doors open vs closed 
input space dimensional camera images subsampled matrix yielding total rgb pixels image 
addition sonar measurements 
dataset contains exactly examples 
ffl aimed test tc situations tasks unrelated 
contains groups tasks group called consists tasks adopted task family involving recognition people 
second group called consists tasks input pixels permuted randomly permutation tasks 
third group consists tasks time input space task permuted differently randomly 
testing task task family task family contains tasks potentially related testing tasks 
tasks mutually related unrelated testing task tasks related testing task mutually related 
see non selective transfer suffers unrelated tasks 
ffl task family consists tasks correspond databases 
uses sophisticated input representation 
ideas images encoded view approach scene recognition 
approach feature dimensions chosen large changes object pose orientation produce small changes feature space small changes experiments performed stages task families utilized data set 
person joseph person sean person lisa person sebastian person greg person trash bin distant obstacle nearby obstacle open door closed door lab vs hallway examples learning tasks image sonar scan 
actual images color 
object quality shape texture color produce relatively large differences feature space 
particular representation comprises features corresponding color intensity texture correlation augmented sonar measurements 
datasets generally smaller contained examples 
clustering tasks value mn estimated fold cross validation training set examples dataset testing set examples depending dataset 
distance metric sect 
optimized gradient descent iterated steps step size momentum 
convergence consistently observed earlier epochs 
results sensitive learning parameters 
bounding distance metric observe noticeable fitting tasks distance metrics optimized testing task 
experimental results reported test set results performance measured data points part training set 
averaged experiments different sets training examples 
illustrate effect transfer tasks compare optimal distance metric transfer non optimized equally weighted distance metric alternatively distance metric optimal training set 
metrics rely support sets transfer 
appropriate diagrams show confidence bars true value 
performance graphs show generalization accuracy testing set accuracy testing task 
non selective transfer question investigated addresses effectiveness learning distance metric support sets step tc algorithm cf 
section 
learner benefit distance metric previously optimized related tasks 
conducted experiments non selectively support tasks computing optimal distance metric 
transfer understood special case tc algorithm number clusters set 
key empirical result shown compares accuracy nearest neighbor function number training examples 
grey thin black curve illustrate nearest neighbor absence support tasks grey curve shows generalization accuracy equally weighted distance metric thin curve shows generalization accuracy distance metric optimal training set 
thick curve depicts generalization accuracy transferring knowledge metric optimal distance support tasks 
seen graphs approach shows significantly better results approaches particularly early phase learning 
result illustrates benefit transferring knowledge tasks 
ways quantify results 

relative generalization accuracy 
generalization error obtained averaging curves 
support set optimal distance metric infers average classification error equally weighted distance metric distance metric optimal training set 

relative sample complexity 
second quantity measures reduction sample complexity 
shows result statistical tests generalization accuracy training set optimal distance metric versus support set optimal metric different numbers training examples 
white region training examples generalization error generalization error equally weighted metric transfer training set optimal metric transfer support set optimal metric non selective transfer gamma gamma optimal metric equally weighted euclidean metric sample complexity non selective transfer task family 
error function number training examples 
reduction sample complexity statistical comparison error best non transfer metric optimal distance metric varying numbers training examples 
white grey area non transfer transfer approach superior confidence level 
dividing dark region methods generalize equally 
results shown averaged experiments 
bars indicate confidence intervals 
training examples generalization error generalization error support set optimal metric non selective transfer training set optimal metric transfer phi phi optimal metric equally weighted euclidean metric sample complexity non selective transfer task family obviously transfer increases need training data training set optimal distance metric transfer outperforms support set optimal metric transfer confidence 
large grey region opposite case 
methods equally generalization accuracies differ significantly level 
notice average support set optimal distance metric uses number training examples required training set optimal metric training examples required equally weighted metric 
transfer cuts sample complexity roughly half 
summarize generalization error transferring knowledge inferred best approach requires samples required transfer 
results apply task family positive impact knowledge transfer depends crucially fact support tasks sufficiently related testing task 
task family majority tasks unrelated shows quite opposite effect 
seen optimal distance metric transferring knowledge non selectively fact inferior best non transfer approach 
transferring knowledge average generalization error larger best non transfer approach 
sample complexity increases non selective transfer knowledge 
findings support claim transfer hurts performance tasks appropriately related 
shown sections selectively transferring knowledge right cluster tasks avoid damaging effects stemming poorly related tasks 
clustering tasks shows normalized version transfer matrix task family 
row depicts particular task including testing task benefits knowledge transferred task white boxes indicate generalization accuracy task improves optimal distance metric task equally weighted distance metric 
black boxes indicate opposite case meaning tasks 
size box visualizes magnitude effect 
task family tasks related testing task unrelated notably row 
diagram diverse task family shows tasks particular anti related testing task 
words respective optimal distance metrics hurt performance testing tasks 
shows non permuted tasks related testing task showing exists opportunity synergy knowledge transfer 
test support task test performance task task transfer matrix test support task test performance task task transfer matrix shy 
greg jos 
seb 
clo 
open lab far near support task test greg joseph sebastian closed door open door lab vs hallway obstacle far obstacle near performance task task transfer matrix task transfer matrices cn different task families 
white values indicate error task reduced optimal distance metric task 
black values indicate opposite tasks anti related 
relation individual tasks testing tasks depicted 
task hierarchies figures depict task hierarchies task families 
figures illustrate second key result empirical study task families tc manages discover surprisingly meaningful tasks clusters 
apparent task family 
early starting clusters major clusters split set tasks people recognition determining obstacle proximity landmarks locations 
class split class containing door related tasks containing single location related task 
notice information type learning task communicated explicitly tc algorithm discovered importance individual input features different learning tasks 
similar results hierarchy task family 
major task families encoding grouped 
example partitions tc generates task clusters different task types clustered separate clusters 
tc groups exactly tasks rely input encoding 
clusters task family differences different tasks subtle interesting note tasks involving recognition people form similar subgroup particularly involving different people cf 

tasks involve recognition person arranged different clusters 
findings clearly illustrate second key result research tc manages find meaningful clusters 
experiments tc discovered structure inherently exists different tasks 
selective transfer shows performance results obtained tc algorithm task family clusters thick black curve entire hierarchy grey curve 
experiments clusters appropriately related testing task leads results better obtained equally weighted distance function 
board tc selects best task cluster considerably generalizes 
example training examples test task example open door closed door tc picks experiments correct task cluster 
experiments tc selects task cluster task cluster task cluster situation changes training data arrives 
training examples tc correctly guesses best task cluster experiments patterns reliably identifies best cluster 
illustrates tc error training data scarce manages identify relevant tasks 
performance results obtained family illustrate third key result empirical study selective transfer superior non selective transfer situations tasks unrelated irrelevant 
example tc achieves average generalization error test task knowledge transferred selectively support tasks 
relatively speaking average error observed non selective approach cf 
thick curve considerably close best possible distance metric see 
relative improvement sample complexity significant sample complexity test set tc transfers knowledge selectively compared non selective counterpart 
tc compared best non transfer approach tc uses samples reach level generalization accuracy generalization accuracy average inferred level task hierarchy best worst best tasks involving people task hierarchy task family hierarchy obtained clustering task space different numbers clusters right part diagram depicts corresponding value difference best worst partitioning 
level best worst best regular encoding permutation group ii permutation group task hierarchy task hierarchy task family notice early task hierarchy separates different task types 
related task cluster identified clusters available 
closed open lab greg joseph sebastian near far level task hierarchy best worst best people recognition joseph sebastian closed open closed open lab greg joseph sebastian near far greg joseph sebastian near far greg joseph sebastian closed open lab near far obstacle door status proximity location task hierarchy task family despite small size datasets tc reliably discovers different types learning problems groups different types learning problems different branches hierarchy 
equally metric 
results remarkably close achieved knew advance ones support sets appropriately related 
experiments observed number task clusters weakly impacts results long 
smaller values number task clusters insufficient tc performance degrades regular nearest neighbor equally weighted distance metric 
shows results obtained applying tc task family results basically match results obtained notable difference optimizing distance metric training set lead improvement non optimized distance metric finding attribute fact input features int appropriate image classification tasks see section 
compared equally weighted distance metric relative generalization accuracy tc relative sample complexity 
shown results obtained task family case tc performs approximately non selective counterpart see 
discussion considers situations learner faces entire collection learning tasks 
shows hierarchical structure discovered space learning tasks selectively transfer knowledge new learning tasks order boost generalization 
tc algorithm proposed employs nearest neighbor algorithm transfers knowledge adjusting distance metric tasks re 
transfer knowledge selectively tc clusters tasks bins related tasks 
relatedness defined context tc knowledge transfer mechanisms 
facing new learning task tc determines related task cluster selectively transfers knowledge form cluster 
experimental comparison conducted mobile robot perceptual domain shown 
tasks appropriately related tc transfer mechanisms successfully reduces sample complexity 
example task family tc consumes training examples best non transfer approach requires 

tc clustering mechanisms manages discover meaningful task clusters build hierarchies tasks 
example task family tc consistently groups tasks involving people doors obstacles locations different bins clusters available 
task family tc groups different task types separate clusters 

tasks appropriately related selectively transferring knowledge related task cluster improves results significantly 
example task family tasks appropriately related testing task selective transfer requires training examples generalization error generalization error equally weighted metric transfer tc entire hierarchy delta delta deltaff tc gamma gamma optimal metric equally weighted euclidean metric sample complexity selective transfer tc task family clusters entire task hierarchy 
depicts statistical comparison sample complexity tc clusters 
training examples generalization error generalization error transfer approaches tc entire hierarchy gamma gamma optimal metric equally weighted euclidean metric sample complexity selective transfer tc task family uncertainty results large confidence bars due small size datasets task family amount training data required corresponding non selective transfer mechanisms 
results tune results obtained robot perception robot control game playing domains illustrate lifelong learner generalize accurately data transfers knowledge acquired previous learning tasks 
key assumption tc approach existence groups tasks tasks related group 
little known cases class boundaries smoother 
cases smoother arbitration schemes weighting impact task cluster proportion produce superior results 
results illustrate hard class boundaries consistently improve generalization accuracy 
hard boundaries advantage cluster optimal distance metric computed line arrival new learning tasks tc fast practice 
main potential limitations tc arises fact task clustering pairwise comparisons 
tc capture effects transfer arise tasks involved 
remains seen pairwise comparisons prevent tc finding useful clusters different application domains 
full evaluation transfer subsets tasks requires time exponentially number tasks tc time requirements quadratic 
appears feasible design incremental strategies time requirements nt efficient current implementation tc small 
third limitation current implementation arises fact space partitions searched exhaustively done number tasks sufficiently small case experiments clearly complexity exhaustive search prohibits global optimization large values view principal limitation tc algorithm heuristic stochastic optimization methods certainly applicable 
learning tasks arrive task clusters may learned incrementally determining cluster membership task arrives 
little known concerning results depend fact partitioning represents global minimum reader may notice general scheme underlying tc approach may applicable approaches transfer knowledge multiple learning tasks surveyed see section 
approaches learn transfer just global weighting vector 
course approaches computationally infeasible general scheme underlying tc algorithm requires order comparisons involving repeated experiments transfer tasks 
key difference tc approach previous approaches lies tc ability transfer knowledge selectively 
weighting previous learning tasks equally learning bias new tc structures space learning tasks reasons relatedness 
light experimental findings conjecture tc approach scales better application domains diverse tasks learned domains learning tasks just single type 
acknowledgment authors wish anonymous reviewers thoughtful comments 
research sponsored part national science foundation award iri wright laboratory aeronautical systems center air force materiel command usaf agency arpa number 
views contained document author interpreted necessarily representing official policies endorsements expressed implied nsf wright laboratory united states government 
abu mostafa 
method learning hints 
hanson cowan giles editors advances neural information processing systems pages san mateo ca 
morgan kaufmann 

ahn brewer 
psychological studies explanation learning 
dejong editor investigating explanation learning 
kluwer academic publishers boston dordrecht london 
atkeson 
locally weighted regression robot learning 
proceedings ieee international conference robotics automation pages sacramento ca 
baxter 
learning internal representations 
phd thesis university australia 
beymer shashua poggio 
example image analysis synthesis 
memo 
brodley 
recursive automatic algorithm selection inductive learning 
phd thesis university massachusetts amherst ma 
buhmann 
data clustering learning 
arbib editor handbook brain theory neural networks pages 
books mit press 
caruana 
multitask learning knowledge source inductive bias 
utgoff editor proceedings tenth international conferenceon machine learning pages san mateo ca 
morgan kaufmann 
franke 
scattered data interpolation tests methods 
mathematics computation 
friedman 
flexible metric nearest neighbor classification 

hastie tibshirani 
discriminant adaptive nearest neighbor classification 
submitted publication 
hild waibel 
multi speaker speaker independent architectures multi state time delay neural network 
proceedings international conference acoustics speech signal processing pages ii 
ieee 
edelman 
generalizing single view face recognition 
technical report cs tr department applied mathematics computer science weizmann institute science rehovot israel 
murase nayar 
visual learning recognition objects appearance 
international journal computer vision 
mel 
view approach object recognition multiple visual cues 
mozer touretzky hasselmo editors advances neural information processing systems 
mit press 
moore hill johnson 
empirical investigation brute force choose features smoothers function approximators 
hanson judd petsche editors computational learning theory natural learning systems volume 
mit press 
moses ullman edelman 
generalization changes illumination viewing position upright inverted faces 
technical report cs tr department applied mathematics computer science weizmann institute science rehovot israel 
sullivan mitchell thrun 
neural network learning mobile robot perception 
ikeuchi veloso editors symbolic visual learning 
oxford university press 
pratt 
transferring previously learned backpropagation neural networks new learning tasks 
phd thesis rutgers university department computer science new brunswick nj 
rendell 
layered dynamically variable bias management 
proceedings ijcai pages 
sharkey sharkey 
adaptive generalization transfer knowledge 
proceedings second irish neural networks conference belfast 
silver mercer 
model consolidation retention transfer neural net task knowledge 
proceedings inns world congress neural networks pages volume iii washington dc 
stanfill waltz 
memory reasoning 
communications acm 
suddarth holden 
symbolic neural systems hints developing complex systems 
international journal machine studies 
sutton 
adapting bias gradient descent incremental version delta bar delta 
proceeding tenth national conference artificial intelligence aaai pages menlo park ca july 
aaai aaai press mit press 
thrun 
explanation neural network learning lifelong learning approach 
kluwer academic publishers boston ma 
appear 
thrun 
learning th thing easier learning 
advances neural information processing systems mit press 
thrun mitchell 
learning thing 
proceedings ijcai 
appeared cmu technical report cmu cs 
thrun sullivan 
clustering learning tasks selective cross task transfer knowledge 
technical report cmu cs carnegie mellon university school computer science pittsburgh pa 
utgoff 
machine learning inductive bias 
kluwer academic publishers 
