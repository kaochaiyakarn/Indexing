massachusetts institute technology artificial intelligence laboratory center biological computational learning department brain cognitive sciences memo december learning incomplete data zoubin ghahramani michael jordan zoubin psyche mit edu publication retrieved anonymous ftp publications ai mit edu 
real world learning tasks involve high dimensional data sets complex patterns missing features 
review problem learning incomplete data statistical perspectives likelihood bayesian 
goal fold place current neural network approaches missing data statistical framework describe set algorithms derived likelihood framework handle clustering classification function approximation incomplete data principled efficient manner 
algorithms mixture modeling distinct appeals expectation maximization em principle dempster estimation mixture components coping missing data 
copyright fl massachusetts institute technology report describes research done center biological computational learning artificial intelligence laboratory massachusetts institute technology 
support center provided part national science foundation contract asc 
support laboratory artificial intelligence research provided part advanced research projects agency department defense 
authors supported part atr auditory visual perception research laboratories siemens iri national science foundation office naval research 
zoubin ghahramani supported mcdonnell pew foundation 
michael jordan nsf presidential young investigator 
computational biological learning environment provide complete information learner 
example vision system may encounter partially occluded examples object recover model unoccluded object 
similarly adaptive controller may required learn mapping sensor readings actions sensors unreliable fail give readings 
examples data sets missing values abound machine learning 
review problem learning incomplete data statistical perspective 
goal fold place current neural network treatments missing data statistical framework derive framework set algorithms handle incomplete data principled manner 
maintain breadth review discuss classification function approximation clustering problems 
missing data arise input target variables treat missing inputs unlabeled data 
statistical framework adopt cf 
little rubin distinction environment assume generate complete data missing data mechanism renders output environment unobservable learner 
supervised learning problem consists forming map inputs targets 
unsupervised learning process generally consists extracting compact statistical description inputs 
cases learner may benefit knowledge constraints data generation process falls certain parametric family mechanism caused pattern incompleteness independent data generation process 
statistical theory allows formalize consequences constraints provides framework deriving learning algorithms consequences 
developing framework incomplete data motivate problem simplest statistical example illustrates interaction missing data data generation mechanisms 
imagine wish estimate mean covariance matrix sigma bivariate normal distribution data set observations missing see fig 

estimate mean mean observed values underestimate ignored covariance structure observed data 
intelligent heuristic covariance structure fill values missing regressing heuristic yield biased estimate covariance matrix filled data points fall regression line 
filling techniques known mean imputation regression imputation yield unsatisfactory see example uci repository machine learning databases murphy aha 
results simple parameter estimation problem 
organized follows 
section outline statistical framework defines missing data data generation mechanisms 
section proceed describe likelihood approach learning incomplete data 
section approach derive set learning algorithms function approximation classification clustering 
section describes alternative likelihood approach bayesian approach algorithms implement 
section discusses boltzmann machines incomplete data 
conclude section 
simple example 
complete data generated gaussian mean covariance matrix 
data points missing values denoted hollow circles line 
solid square indicates mean calculated observed data 
hollow square ellipse indicate mean standard deviation calculated incomplete data maximum likelihood ml algorithm 
note ml estimate higher observed values 
framework statistical framework little rubin 
assume data set fx divided observed component missing component data vector may different patterns missing features 
distinguish input target components data vector 
formalize notion missing data mechanism defining missing data indicator matrix ij ae ij observed ij missing 
data generation process missing data mechanism considered random processes joint probability distribution rj oe rjx oe parameters data generation process distinct set parameters oe missing data mechanism 
data probability model decomposed distinguish nested types missing data mechanism 
data 
missing completely random rjx oe 
probability ij missing independent values data vector 

missing random mar rjx oe rjx oe 
probability ij missing independent values missing components data may depend values observed components 
example ij may missing certain values ik provided ik observed 
illustrates case 

missing random 
rjx oe may depend value ij ij jx ij oe function ij data said censored 
example sensor fails input exceeds range output censored 
type missing data mechanism critical evaluating learning algorithms handling incomplete data 
full maximum likelihood bayesian approaches handle data missing random completely random 
simpler learning approaches handle data fail mar data general 
general approaches exist data 
bayesian maximum likelihood techniques estimates parameters oe linked observed data rj oe 
maximum likelihood methods likelihood rj oe bayesian methods posterior probability rj oe oe wish ascertain conditions parameters data generation process estimated independently parameters missing data mechanism 
rj oe rjx oe dx note rjx oe rjx oe succinctness non bayesian phrase estimating parameters section replaced calculating posterior probabilities parameters parallel bayesian argument 
rj oe rjx oe dx rjx oe equation states data mar likelihood factored 
maximum likelihood methods implies directly maximizing jx function equivalent maximizing 
parameters missing data mechanism ignored purposes estimating little rubin 
bayesian methods missing data mechanism ignored additional requirement prior oe oe results imply data sets censored data handled bayesian likelihood methods model missing data mechanism learned 
positive side imply mar condition weaker condition sufficient bayesian likelihood learning 
likelihood methods feedforward networks previous section showed maximum likelihood methods utilized estimating parameters data generation model ignoring missing data mechanism provided data missing random 
turn problem estimating parameters model incomplete data 
focus feedforward neural network models turning class models missing data incorporated naturally estimation algorithm 
feedforward neural networks know descent error cost function interpreted ascent model parameter likelihood white 
particular target vector assumed gaussian jx oe log likelihood equivalent sum squared error weighted output variances max log jx min oe gamma target missing unknown variance output taken infinite oe 
similarly certain components target vector missing assume variance component infinite 
missing targets drop likelihood minimization proceed simply certain targets replaced don cares 
components input vector missing likelihood properly defined jx depends full input vector 
conditional observed inputs needed likelihood requires integrating missing inputs 
turn requires model input density explicitly available feedforward neural network 
tresp 
proposed solving problem separately estimating input density gaussian mixture model conditional density yjx feedforward network 
approach seen maximizing joint input output log likelihood log oe log jx log joe feedforward network parametrized mixture model parametrized oe 
components input vector missing observed data likelihood expressed jx jx jx oe dx input decomposed observed missing components 
mixture model integrate likelihood missing inputs feedforward network 
gradient likelihood respect network parameters jx jx jx oe gamma dx exhibits error terms weight completion missing input vector jy oe 
term bayes rule proportional product probability completion input jx oe posterior probability output completion jx 
integral approximated monte carlo method missing input completions generated input distribution 
intuitively appealing aspect method weight placed error gradients input completions better approximate target tresp buntine weigend 
arguments imply computing maximum likelihood estimates missing inputs requires model joint input density 
principle achieved multiple feedforward networks learning particular conditional density inputs 
example pattern missing inputs monotone input dimensions ordered ij observed ik observed missing data completed cascade gamma networks 
network trained predict input dimension completed instances lower index input dimensions models particular conditional density cf 
regression imputation monotone multivariate normal data little rubin 
accommodate general patterns missing inputs targets approach multiple feedforward networks practically cumbersome number networks grows exponentially data dimensionality 
problem avoided modeling input output densities mixture model 
mixture models incomplete data mixture modeling framework allows learning data sets arbitrary patterns incompleteness 
learning framework classical estimation problem requiring explicit probabilistic model algorithm estimating parameters model 
possible disadvantage parametric methods lack flexibility compared nonparametric methods 
mixture models largely circumvent problem combine flexibility nonparametric methods certain analytic advantages parametric methods mclachlan basford 
mixture models utilized supervised learning problems form mixtures experts architecture jacobs jordan jacobs 
architecture parametric regression model modular structure similar nonparametric decision tree adaptive spline models breiman friedman 
approach differs regression approaches goal learning estimate density data 
distinction input output variables joint density estimated estimate form input output map 
similar density estimation approaches discussed specht nonparametric models nowlan tresp 
gaussian mixture models 
estimate vector function joint density estimated particular input conditional density yjx formed 
obtain single estimate full conditional density evaluate yjx expectation appealing feature mixture models context deal naturally incomplete data 
fact problem estimating mixture densities viewed missing data problem labels component densities missing expectation maximization em algorithm dempster developed handle kinds missing data 
em algorithm mixture models section outlines estimation algorithm finding maximum likelihood parameters mixture model dempster 
model data fx generated independently mixture density 

component mixture denoted parametrized start assuming complete data 
equation independence assumption see log likelihood parameters data set jx log 

maximum likelihood principle best model data parameters maximize jx 
function easily maximized numerically involves log sum 
intuitively credit assignment problem clear component mixture generated data point parameters adjust fit data point 
em algorithm mixture models iterative method solving credit assignment problem 
intuition access hidden random variable indicating data point generated component maximization problem decouple set simple 
binary indicator variables fz defined im ij iff generated gaussian complete data log likelihood function written jx ij log jz involve log summation 
unknown utilized directly expectation denoted 
shown dempster jx maximized iterating steps step jx jx step arg max expectation step computes expected complete data log likelihood maximization finds parameters maximize likelihood 
practice densities exponential family step reduces computing expectation missing data sufficient statistics required step 
steps form basis em algorithm mixture modeling 
incorporating missing values em algorithm previous section aspect em algorithm learning mixture models 
important application em learning data sets missing values little rubin dempster 
application pursued statistics literature non mixture density estimation problems 
show combining exceptions mixture densities context contaminated normal models robust estimation little rubin context mixed categorical continuous data missing values little 
missing data application em learning mixture parameters results set clustering classification function approximation algorithms incomplete data 
previously defined notation divided data vector different patterns missing components 
denote missing observed components data vector ordinarily introduce superscripts simplified notation sake clarity 
handle missing data rewrite em algorithm incorporating indicator variables algorithm missing inputs step jx jx step arg max expected value step taken respect sets missing variables 
proceed illustrate algorithm classes models mixtures gaussians mixtures building blocks classification function approximation 
real valued data mixture gaussians real valued data modeled mixture gaussians 
start estimation algorithm complete data duda hart dempster nowlan 
model step simplifies computing ij jx 
binary nature ij ij jx denote ij probability gaussian generated data point ij sigma gamma expf gamma gamma sigma gamma gamma sigma gamma expf gamma gamma sigma gamma gamma step re estimates means covariances gaussians data set weighted ij ij ij sigma ij gamma gamma ij incorporate missing data rewriting log likelihood complete data jx ij log jz ij log ignore second term estimating parameters jz 
specializing equation mixture gaussians note derivation assumes equal priors gaussians priors viewed mixing parameters learned maximization step 
indicator variables missing step reduced estimating ij jx 
case interested missing expand equation superscripts denote subvectors submatrices parameters matching missing observed components data obtain jx ij log log sigma gamma gamma sigma gamma oo gamma gamma gamma sigma gamma om gamma gamma gamma sigma gamma mm gamma note expectation sufficient statistics parameters include unknown terms ij ij ij compute ij jx ij jx ij jx intuitive approach dealing missing data current estimate data density compute expectation missing data step complete data expectations completed data re estimate parameters 
seen section intuition fails dealing single dimensional gaussian expectation missing data lies line biases estimate covariance 
hand approach arising application em algorithm specifies current density estimate compute expectation incomplete terms appear likelihood maximization 
mixture gaussians incomplete terms interactions indicator variable ij second moments simply computing expectation missing data model substituting values step sufficient guarantee increase likelihood parameters 
compute expectations define ij jz ij sigma mo sigma oo gamma gamma squares linear regression predicted gaussian expectation ij jx ij probability defined measured observed dimensions similarly get ij jx ij ij example sigma divided sigma oo sigma om sigma mo sigma mm corresponding note superscript gamma oo denotes inverse followed submatrix operations oo gamma denotes reverse order 
ij jx ij sigma mm gamma sigma mo sigma oo gamma sigma mo ij mt ij step uses expectations substituted equations re estimate means covariances 
re estimate mean vector substitute values ij missing components equation 
re estimate covariance matrix substitute values bracketed term outer product matrices involving missing components equation 
discrete valued data mixture binary data modeled mixture bernoulli densities 
dimensional vector xd modeled generated mixture bernoulli densities xj 
xd jd gamma jd gammax model complete data step computes ij id jd gamma jd gammax id id ld gamma ld gammax id step re estimates parameters ij ij incorporate missing data compute appropriate expectations sufficient statistics step 
bernoulli mixture include incomplete terms ij jx ij jx 
equal ij calculated observed subvector second assume class individual dimensions bernoulli variable independent simply ij step uses expectations substituted equation 
generally discrete categorical data modeled generated mixture multinomial densities similar derivations learning algorithm applied 
extension data mixed real binary categorical dimensions readily derived assuming joint density mixed components types 
mixed models serve solve classification problems discussed section 
clustering gaussian mixture model estimation form soft clustering nowlan 
furthermore full covariance model principal axes gaussians align principal components data soft cluster 
binary categorical data soft clustering algorithms obtained bernoulli multinomial mixture models 
illustrate extension clustering algorithms missing data problems simple example character recognition 
learning digit patterns 
row theta templates generate data set 
second row templates gaussian noise added 
third row templates noise added missing pixels 
training set consisted noisy incomplete samples digit 
fourth row means twelve gaussians asymptote passes data set patterns mean imputation heuristic 
fifth row means twelve gaussians asymptote passes incomplete data set em algorithm 
gaussians constrained diagonal covariance matrices 
example fig 
gaussian mixture algorithm training set dimensional noisy greyscale digits pixels missing 
em algorithm approximated cluster means highly deficient data set quite 
compared em mean imputation common heuristic missing values replaced unconditional means 
results showed em outperformed mean imputation measured distance gaussian means templates see fig 
likelihoods log likelihoods sigma em gamma sigma mean imputation gamma sigma 
function approximation far alluded data vectors inputs targets 
supervised learning generally wish predict target variables set input variables wish approximate function relating sets variables 
decompose data vector input subvector target output subvector relation input target variables expressed conditional density jx 
conditional density readily obtained joint input target density density mixture models seek estimate 
framework distinction supervised learning function approximation unsupervised learning density estimation semantic resulting data considered composed inputs targets 
focusing gaussian mixture model note conditional density jx gaussian mixture 
particular input estimated output summarize density 
require single estimate output natural candidate squares estimate lse takes form jx 
expanding expectation get ij sigma ti sigma ii gamma gamma convex sum squares linear approximations gaussian 
weights sum ij vary nonlinearly input space viewed corresponding output classifier assigns point input space probability belonging gaussian 
squares estimator interesting relations models cart breiman mars friedman mixtures experts jacobs jordan jacobs mixture gaussians competitively partitions input space learns linear regression surface partition 
similarity noted tresp 

gaussian covariance matrices constrained diagonal squares estimate simplifies ij ij equation computed substituting equation evaluating exponentials dimensions input space 
average output means weighted proximity gaussian input means 
expression form identical normalized radial basis function rbf networks moody darken poggio girosi algorithms derived disparate frameworks 
limit covariance matrices gaussians approach zero approximation nearest neighbor map 
learning problems lend squares estimates problems involve learning mapping input target variables ghahramani 
resulting conditional densities multimodal single value output input appropriately reflect fact ghahramani bishop 
problems stochastic estimator output sampled jx preferred squares estimator 
learning problems involving discrete variables lse stochastic estimators different interpretation 
wish obtain posterior probability output input lse estimator 
hand wish obtain output estimates fall discrete output space stochastic estimator 
classification classification missing inputs missing features em correct classification mi classification iris data set 
data points training testing 
data point consisted real valued attributes class labels 
shows classification performance sigma standard error function proportion missing features em algorithm mean imputation mi common heuristic missing values replaced unconditional means 
classification strictly speaking special case function approximation merits attention 
classification problems involve learning mapping input space attributes set discrete class labels 
mixture modeling framework lends readily classification problems modeling class label multinomial variable 
example attributes real valued class labels mixture model gaussian multinomial components dj 
jd sigma expf gamma gamma sigma gamma gamma denotes joint probability data point attributes belongs class jd parameters multinomial class variable 
jd dj 
jd 
missing attributes missing class labels unlabeled data points readily handled em algorithm 
step missing attributes completed formulas gaussian mixture ij dj 
jd 
ld 
hand class label missing ij 

exactly gaussian mixture 
class label completed probability vector th component ij jd classification model estimated label particular input may obtained computing 
similarly class conditional densities computed evaluating xjc 
classes way yields class conditional densities turn mixtures gaussians 
shows performance em algorithm sample classification problem varying proportions missing features 
mixture approach classification closely related mixture discriminant analysis mda approach proposed hastie tibshirani 
mda classes fit mixture densities em algorithm optimal discriminant obtained 
hastie tibshirani extend basic mda procedure combining reduced rank discrimination 
fisher rao linear discriminant analysis results interpretable low dimensional projection data leads improved classification performance 
authors mention missing data em methods context algorithm 
previous approaches classification incomplete patterns proceeded different lines 
cheeseman 
describe bayesian classification method class modeled having gaussian real valued attributes multinomial discrete attributes 
learning procedure finds maximum posteriori parameters model differentiating posterior probability class parameters setting zero 
yields coupled set nonlinear equations similar em steps iterated find posterior mode parameters dempster 
handle missing data authors state discrete attributes shown correct procedure treating unknown value equivalent adding unknown category value set 
real valued attributes add known unknown category attribute set value appropriately 
comments approach 
unknown category added multinomial value set results extra parameter estimated 
furthermore adding unknown category reflect fact unobserved data arises original multinomial value set argument quinlan see example data set attribute unknown algorithm may form class attribute value unknown situation clearly undesirable classifier 
class modeled single gaussian multinomial data points assumed unlabeled cheeseman 
algorithm fact form soft clustering 
approached problem classification incomplete data approximation em clustering 
step observed data classified current mixture model data point assigned class 
parameters class re estimated step 
notation approximation corresponds setting highest ij data point 
compared method neural network algorithm missing input varied possible range discrete attribute values find completion resulting minimum classification error 
reported approximation em outperformed neural network algorithm algorithm linear discriminant analysis 
include exact em algorithm comparison 
quinlan discusses problem missing data context decision tree classifiers 
quinlan decision tree framework uses measure information gain build classifier resulting tree structure queries attribute values set leaves representing class membership 
author concludes treating unknown separate value solution missing value problem querying attributes unknown values higher apparent information gain quinlan 
approach advocates compute expected information gain assuming unknown attribute distributed observed values subset data node tree 
approach consistent information theoretic framework adopted parallels em bayesian treatments missing data suggest integrating possible missing values 
alternative method handling missing data decision trees breiman 
cart algorithm 
cart initially constructs large decision tree splitting criterion closely related measure information gain 
tree pruned recursively measure model complexity proportional number terminal nodes resulting smaller interpretable tree better generalization properties 
case missing value attribute considered evaluating goodness splits attribute 
cases assigned branches split attribute missing values best surrogate split split attribute partitions data similarly original split 
method works single highly correlated attribute predicts effects split missing attribute 
single attribute predict effects split method may perform 
approach computing expected split observed variables similar quinlan suitable statistical perspective may provide improved performance missing data 
bayesian methods bayesian learning parameters treated unknown random variables characterized probability distribution 
bayesian learning utilizes prior distribution parameters may encode world knowledge initial biases learner constraints probable parameter values 
learning proceeds bayes rule multiplying prior probability parameters likelihood data parameters normalizing integral parameter space resulting posterior distribution parameters 
information learned unknown parameters expressed form posterior probability distribution 
context learning incomplete data bayesian priors impact arenas 
prior may reflect assumptions initial distribution parameter values described 
learning procedure converts prior posterior data likelihood 
seen perform conversion independently missing data mechanism requires mechanism missing random prior 
second prior may reflect assumptions initial distribution missing values 
prior distribution input values complete missing data sampling distribution 
complete data problems simple models judicious choice conjugate priors parameters allows analytic computation posterior distribution box tiao 
incomplete data problems usual choices conjugate priors generally lead recognizable posteriors making iterative simulation sampling techniques obtaining posterior distribution indispensable schafer 
data augmentation gibbs sampling technique closely related form em algorithm data augmentation tanner wong 
iterative algorithm consists steps 
imputation step computing expectations missing sufficient statistics simulate random draws missing data conditional distribution jx 
posterior step sample times posterior distribution parameters easily computed imputed data jx 
obtain samples joint distribution jx alternately conditioning unknown variables technique known gibbs sampling geman geman 
mild regularity conditions algorithm shown converge distribution posterior tanner wong 
note augmented data chosen simplify step way indicator variables chosen simplify step em 
data augmentation techniques combined metropolis hastings algorithm schafer 
metropolis hastings metropolis hastings creates monte carlo markov chain drawing probability distribution meant approximate distribution interest accepting rejecting drawn value acceptance ratio 
acceptance ratio ratio probabilities drawn state previous state chosen easy calculate involve computation normalization factor 
transition probabilities allow state reached eventually state chain ergodic markov chain approach stationary distribution chosen distribution interest initial distribution 
combination data augmentation metropolis hastings example problems posterior difficult sample 
problems may generate markov chain stationary distribution jx 
multiple imputation bayesian backpropagation multiple imputation rubin technique missing value replaced simulated values reflect uncertainty true value missing data 
multiple imputation completed data sets exist analyzed complete data methods 
results combined form single inference 
multiple imputation requires sampling jx may difficult iterative simulation methods context schafer 
bayesian backpropagation technique missing data buntine weigend special case multiple imputation 
bayesian backpropagation multiple values input imputed prior distribution approximate integral turn compute gradient required backpropagation 
procedure similar tresp 
completes data sampling prior distribution inputs estimates distribution directly data 
boltzmann machines incomplete data boltzmann machines networks binary stochastic units symmetric connections learning corresponds minimizing relative entropy probability distribution visible states target distribution hinton sejnowski 
relative entropy cost function rewritten reveal target distribution taken empirical distribution data equivalent model likelihood 
boltzmann learning rule implements maximum likelihood density estimation binary variables 
boltzmann learning procedure estimates correlations unit activities stage input target units clamped stage target units 
correlations modify parameters network direction relative entropy cost gradient 
moves output unit distribution phase closer target distribution clamped phase 
reformulated terms maximum likelihood conditional density estimation boltzmann learning rule instance generalized em algorithm gem dempster laird rubin estimation unit correlations current weights clamped values corresponds step update weights corresponds step hinton sejnowski 
generalized em sense step maximize likelihood simply increases gradient ascent 
incomplete variables boltzmann machine states hidden units denoted visible input output units 
suggests principled way handling missing inputs targets boltzmann machine treat hidden units leave 
exactly formulation mixture models em algorithm estimate appropriate sufficient statistics order correlations step 
sufficient statistics increase model likelihood step 
ways handling missing data learning 
heuristics filling missing data unconditional conditional means efficient discarding information latent data set 
principled statistical approaches yield interpretable results providing guarantee find maximum likelihood parameters despite missing data 
statistical approaches argue convincingly missing data integrated estimate data density 
class models strictly bayesian point view procedures improper don take account variability parameters integration 
performed naturally efficiently mixture models 
models described applications clustering function approximation classification real discrete data 
particular shown missing inputs targets incorporated mixture model framework essentially making dual ubiquitous em algorithm 
principal virtually incomplete data techniques reviewed neural network machine learning literatures placed basic statistical framework 
rubin helpful discussions missing data 
iris data set obtained uci repository machine learning databases 
bishop 

mixture density networks 
technical report ncrg aston university birmingham box tiao 

bayesian inference statistical analysis 
addison wesley reading ma 
breiman friedman olshen stone 

classification regression trees 
wadsworth international group belmont ca 
buntine weigend 

bayesian backpropagation 
complex systems 
cheeseman kelly self stutz taylor freeman 

autoclass bayesian classification system 
proceedings fifth international conference machine learning university michigan ann arbor 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
royal statistical society series 
duda hart 

pattern classification scene analysis 
wiley new york 
friedman 

multivariate adaptive regression splines 
annals statistics 
geman geman 

stochastic relaxation gibbs distributions bayesian restoration images 
ieee transactions pattern analysis machine intelligence 
ghahramani 

solving inverse problems em approach density estimation 
proceedings connectionist models summer school 
erlbaum hillsdale nj 
hastie tibshirani 

discriminant analysis gaussian mixtures 
technical report bell laboratories murray hill nj 
hastings 

monte carlo sampling methods markov chains applications 
biometrika 
hinton sejnowski 

learning relearning boltzmann machines 
rumelhart mcclelland editors parallel distributed processing explorations microstructure cognition 
volume foundations 
mit press cambridge ma 
jacobs jordan nowlan hinton 

adaptive mixture local experts 
neural computation 
jordan jacobs 

hierarchical mixtures experts em algorithm 
neural computation 
little rubin 

statistical analysis missing data 
wiley new york 
little 

maximum likelihood estimation mixed continuous categorical data missing values 
biometrika 
mclachlan basford 

mixture models inference applications clustering 
marcel dekker 
metropolis rosenbluth rosenbluth teller teller 

equation state calculations fast computing machines 
journal chemical physics 
moody darken 

fast learning networks locally tuned processing units 
neural computation 
murphy aha 

uci repository machine learning databases machine readable data repository 
university california department information computer science irvine ca 
nowlan 

soft competitive adaptation neural network learning algorithms fitting statistical mixtures 
cmu cs school computer science carnegie mellon university pittsburgh pa poggio girosi 

theory networks approximation learning 
lab memo mit cambridge ma 
quinlan 

induction decision trees 
machine learning 
quinlan 

unknown attribute values induction 
proceedings sixth international conference machine learning 
rubin 

multiple imputation surveys 
wiley new york 
schafer 

analysis incomplete multivariate data simulation 
chapman hall london 


multi valued standard regularization theory regularization networks learning algorithms approximating multi valued functions 
technical report tr atr kyoto japan 


classification incomplete data neural networks 
proceedings fourth australian conference neural networks sydney australia 
specht 

general regression neural network 
ieee trans 
neural networks 
tanner wong 

calculation posterior distributions data augmentation 
journal american statistical association 
tresp ahmad 

training neural networks deficient data 
cowan tesauro alspector editors advances neural information processing systems san francisco ca 
morgan kaufman publishers 
white 

learning artificial neural networks statistical perspective 
neural computation 

