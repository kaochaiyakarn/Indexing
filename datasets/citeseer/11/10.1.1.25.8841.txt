advances neural information processing systems eds 
touretzky mozer hasselmo mit press 
gaussian processes regression christopher williams neural computing research group aston university birmingham uk williams aston ac uk carl edward rasmussen department computer science university toronto toronto ont canada carl cs toronto edu bayesian analysis neural networks difficult simple prior weights implies complex prior distribution functions 
investigate gaussian process priors functions permit predictive bayesian analysis fixed values hyperparameters carried exactly matrix operations 
methods optimization averaging hybrid monte carlo hyperparameters tested number challenging problems produced excellent results 
bayesian approach neural networks prior distribution weights induces prior distribution functions 
prior combined noise model specifies probability observing targets function values yield posterior functions predictions 
neural networks prior functions complex form means implementations approximations mackay monte carlo approaches evaluating integrals neal 
neal argued reason believe real world problems neural network models limited nets containing small number hidden units 
shown sensible consider limit number hidden units net tends infinity predictions obtained models bayesian machinery 
shown large class neural network models converge gaussian process prior functions limit infinite number hidden units 
gaussian processes specified parametrically regression problems 
advantage gaussian process formulation combination prior noise models carried exactly matrix operations 
show hyperparameters control form gaussian process estimated data maximum likelihood bayesian approach leads form automatic relevance determination mackay neal 
prediction gaussian processes stochastic process collection random variables fy jx xg indexed set case input space dimension number inputs 
stochastic process specified giving probability distribution finite subset variables consistent manner 
gaussian process stochastic process fully specified mean function covariance function gamma gamma finite set points joint multivariate gaussian distribution 
consider gaussian processes 
section show parameterise covariances hyperparameters consider form covariance 
training data consists pairs inputs targets ng 
input vector test case denoted superscript 
inputs dimensional targets scalar 
predictive distribution test case obtained dimensional joint gaussian distribution outputs training cases test case conditioning observed targets training set 
procedure illustrated case training point test point 
general predictive distribution gaussian mean variance gamma oe gamma gamma covariance matrix training cases ij matrix inversion step equations implies algorithm time complexity standard methods matrix inversion employed data points certainly feasible workstation computers larger problems iterative methods approximations may needed 
parameterizing covariance function choices covariance functions may reasonable 
formally required specify functions generate non negative definite covariance matrix set points 
modelling point view wish specify covariances points nearby inputs give rise similar predictions 
find covariance function works expf gamma gamma ffi illustration prediction gaussian process 
training case test case wish predict ellipse lefthand plot standard deviation contour plot joint distribution dotted line represents observation right hand plot see distribution output test case obtained conditioning observed target 
axes scale plots 
log plays role hyperparameters define hyperparameters log variables equation positive scale parameters 
covariance function parts term linear regression term involving noise term ffi 
term expresses idea cases nearby inputs highly correlated outputs parameters allow different distance measure input dimension 
irrelevant inputs corresponding small model ignore input 
closely related automatic relevance determination ard idea mackay neal mackay neal 
variable gives scale local correlations 
covariance function valid input dimensionalities compared splines integrated squared mth derivative valid regularizer see wahba 
variables controlling scale bias linear contributions covariance 
term accounts noise data variance noise 
covariance function log likelihood training data gamma log det gamma gamma gamma log section discuss hyperparameters adapted response training data 
relationship previous gaussian process view provides unifying framework regression methods 
arma models time series analysis spline smoothing wahba earlier correspond gaussian process prediction call hyperparameters correspond closely hyperparameters neural networks effect weights integrated exactly 
particular choice covariance function gaussian processes field cressie known kriging literature concentrated case input space dimensional considering general input spaces 
similar regularization networks poggio girosi girosi jones poggio derivation uses smoothness functional equivalent covariance function 
poggio suggested hyperparameters set cross validation 
main contributions emphasize maximum likelihood solution possible recognize connections ard hybrid monte carlo method bayesian treatment see section 
training gaussian process partial derivative log likelihood training data respect hyperparameters computed matrix operations takes time 
section methods adapt hyperparameters derivatives 
maximum likelihood maximum likelihood framework adjust hyperparameters maximize likelihood training data 
initialize hyperparameters random values reasonable range iterative method example conjugate gradient search optimal values hyperparameters 
small number hyperparameters relatively small number iterations usually sufficient convergence 
approach susceptible local minima advisable try number random starting positions hyperparameter space 
integration hybrid monte carlo bayesian formalism start prior distribution hyperparameters modified training data produce posterior distribution jd 
predictions integrate posterior example predicted mean test input jd predicted mean equation particular value 
feasible integration analytically markov chain monte carlo method hybrid monte carlo hmc duane promising application 
assign broad gaussians priors hyperparameters hybrid monte carlo give samples posterior 
hmc works creating fictitious dynamical system hyperparameters regarded position variables augmenting momentum variables purpose dynamical system give hyperparameters inertia random walk behaviour space avoided 
total energy system sum kinetic energy function momenta potential energy potential energy defined jd exp gammae 
sample joint distribution exp gammae gamma technically splines require generalized covariance functions 
marginal distribution required posterior 
sample hyperparameters posterior obtained simply ignoring momenta 
sampling joint distribution achieved steps finding new points phase space near identical energies simulating dynamical system discretised approximation hamiltonian dynamics ii changing energy doing gibbs sampling momentum variables 
hamiltonian dynamics hamilton order differential equations approximated discrete step specifically leapfrog method 
derivatives likelihood equation enter derivative potential energy 
proposed state accepted rejected metropolis rule depending final energy necessarily equal initial energy discretization 
step size hyperparameters large possible keeping rejection rate low 
gibbs sampling momentum variables momentum variables updated modified version gibbs sampling allowing energy change 
persistence new value momentum weighted sum previous value weight value obtained gibbs sampling weight gamma 
form persistence momenta change approximately times slowly increasing inertia hyperparameters help avoiding random walks 
larger values persistence increase inertia reduce rate exploration practical details priors hyperparameters set gaussian mean gamma standard deviation 
simulations step size produced low rejection rate 

hyperparameters corresponding initialised gamma rest 
apply method rescale inputs outputs mean zero variance training set 
sampling procedure run desired amount time saving values hyperparameters times thirds run 
third run discarded burn intended give hyperparameters time come close equilibrium distribution 
predictive distribution mixture gaussians 
squared error loss mean distribution point estimate 
width predictive distribution tells uncertainty prediction 
experimental results report results prediction gaussian process modified version mackay robot arm problem ii real world data sets 
robot arm problem consider version mackay robot arm problem introduced neal 
standard robot arm problem concerned mappings cos cos sin sin method 
inputs sum squared test error gaussian process gaussian process mackay neal neal table results robot arm task 
bottom lines data obtained neal 
mackay result test error net highest evidence 
data generated picking uniformly picking uniformly 
neal added inputs copies corrupted additive gaussian noise standard deviation irrelevant gaussian noise inputs zero mean unit variance 
independent zero mean gaussian noise variance added outputs datasets neal mackay examples training set test set 
theory described section deals prediction scalar quantity predictors constructed outputs separately joint prediction possible gaussian process framework see kriging cressie 
experiments conducted true inputs second inputs 
section report results maximum likelihood training similar results obtained hmc 
log log initialized values chosen uniformly adapted separately prediction early experiments linear regression terms covariance function involving 
conjugate gradient search algorithm allowed run iterations time likelihood changing slowly 
results reported run gave highest likelihood training data fact runs performed similarly 
results shown table encouraging indicate gaussian process approach giving similar performance respected techniques 
methods obtain level performance quite close theoretical minimum error level 
interesting look values obtained optimization task values theta gamma theta gamma theta gamma respectively 
values show nicely inputs important followed corrupted inputs irrelevant inputs 
training irrelevant inputs detected quite quickly corrupted inputs shrink slowly implying input noise relatively little effect likelihood 
real world problems gaussian processes described compared regression algorithms real world data sets rasmussen volume 
data sets training examples input dimension ranged 
length hmc sampling gaussian processes minutes smallest training set size hour largest ones machine 
results rank methods order lowest error full blown bayesian treatment neural networks hmc gaussian processes ensembles neural networks trained cross validation weight decay evidence framework neural networks mackay mars 
currently working assessing statistical significance ordering 
discussion method regression gaussian processes shown performs suite real world problems 
conducted experiments approximation neural nets finite number hidden units gaussian processes space limitations allow described 
directions currently investigation include gaussian processes classification problems outputs regression surfaces class classification problem ii non stationary covariance functions jx gamma iii covariance function containing sum terms form line equation 
hope code gaussian process prediction publically available near 
check www cs utoronto ca neuron delve delve html details 
radford neal useful discussions david mackay generously providing robot arm data chris bishop peter dayan radford neal zhu comments earlier drafts 
cw partially supported epsrc gr 
cressie 

statistics spatial data 
wiley 
duane kennedy 

hybrid monte carlo 
physics letters 
girosi jones poggio 

regularization theory neural networks architectures 
neural computation 
mackay 

practical bayesian framework backpropagation networks 
neural computation 
mackay 

bayesian methods backpropagation networks 
van hemmen domany schulten editors models neural networks ii 
springer 
neal 

bayesian learning stochastic dynamics 
hanson cowan giles editors neural information processing systems vol 
pages 
morgan kaufmann san mateo ca 
neal 

bayesian learning neural networks 
phd thesis dept computer science university toronto 
poggio girosi 

networks approximation learning 
proceedings ieee 
rasmussen 

practical monte carlo implementation bayesian learning 
touretzky mozer hasselmo editors advances neural information processing systems 
mit press 
wahba 

spline models observational data 
society industrial applied mathematics 
cbms nsf regional conference series applied mathematics 

