active learning structure bayesian networks simon tong computer science department stanford university simon tong cs stanford edu daphne koller computer science department stanford university koller cs stanford edu task causal structure discovery empirical data fundamental problem areas 
experimental data crucial accomplishing task 
experiments typically expensive selected great care 
uses active learning determine experiments informative uncovering underlying structure 
formalize causal learning task learning structure causal bayesian network 
consider active learner allowed conduct experiments intervenes domain setting values certain variables 
provide theoretical framework active learning problem algorithm actively chooses experiments perform model learned far 
experimental results show active learning substantially reduce number observations required determine structure domain 
determining causal structure domain frequently key issue situations 
bayesian networks bns pearl compact graphical representation joint probability distributions 
viewed providing causal model domain pearl assume graphical structure bn represents causal structure domain formalize problem discovering causal structure domain task learning bn structure data 
years substantial discovering bn structure purely observational data 
inherent limitations ability discover structure randomly sampled data 
experimental data intervene model vital full determination causal structure 
obtaining experimental data time consuming costly 
experiments chosen care 
provide active learning algorithm selects experiments informative revealing causal structure 
active learning choice data case results seen far 
possibility active learning arise naturally variety domains variants 
interventional active learning learner ask experiments involving interventions performed 
type active learning norm scientific studies ask rat fed sort food 
intervention model causes certain probabilistic dependencies model replaced intervention pearl rat longer eats normally eat choose 
observing results experiment determine direction causal influence cases purely observational data inadequate 
active learning settings ability actively select experiments need mechanism tells experiment perform 
formal framework active learning bayesian networks principles bayesian learning 
maintain distribution bayesian network structures updated data 
define notion quality distribution provide algorithm selects queries greedy way designed improve model quality possible 
provide experimental results variety domains showing active learning algorithm provide substantially accurate estimates bn structure amount data 
interestingly active learning algorithm provides significant improvements cases intervene model select instances certain types 
applicable problem learning structure non causal setting 
learning bayesian networks fx xng set random variables variable values finite domain dom 
bayesian network bn pair represents distribution joint space directed acyclic graph nodes correspond random variables structure encodes conditional independence properties joint distribution 
denote set parents set parameters quantify network specifying conditional probability distributions cpds 
bayesian network represents joint distribution set variables chain rule bayesian networks xn 
viewed probabilistic model answer query form sets variables assignment values bn viewed causal model pearl perspective bn answer interventional queries specify probabilities intervene model forcibly setting variables take particular values 
pearl framework intervention causal model sets single node replaces standard causal mechanism forced take value graphical terms intervention corresponds model cutting incoming edges intuitively new model depend parents original model fact give information evidential reasoning parents experiment fact tells values parents 
example fault diagnosis model car observe car battery charged conclude belt possibly defective deliberately drain battery fact empty obviously gives information belt 
set resulting model distribution eliminate incoming edges nodes set cpds nodes probability 
goal learn bn structure data 
standard assumptions ffl causal markov assumption data generated underlying bayesian network ffl faithfulness assumption distribution induced satisfies independences implied structure goal reconstruct data 
clearly data reconstruct general uniquely determine example network form equally consistent samples best hope identify markov equivalence class pearl set network structures induce precisely independence assumptions 
markov equivalence class skeleton network set connected pairs fixed pairs direction edge fixed edges directed way spirtes experimental observational data ability identify structure larger cooper yoo intuitively assume trying determine direction edge provided experimental data intervenes see distribution change intervening change distribution conclude assumptions edge bayesian learning experimental data discussed goal active learning learn bn structure learning data allowed control certain variables intervening values 
formalize idea assuming subset variables query variables 
learner select particular instantiation request called query 
result query called response randomly sampled instance non query variables conditioned words result experiment model setting take values assumptions imply sampled model described 
bayesian framework learn bn structure 
precisely maintain distribution possible structures associated parameters 
prior structures parameters bayesian conditioning update new data obtained 
heckerman standard assumptions prior ffl structure modularity prior written form pa 
ffl parameter independence ju ffl parameter modularity graphs ju ju 
assume cpd parameters multinomials associated parameter distributions conjugate dirichlet distributions 
analysis holds distribution satisfying parameter modularity assumption 
give complete randomly sampled instance bayes rule posterior distribution proportional 
marginal likelihood data integrating possible parameter values having complete random sample suppose interventional query resulting response need define update distribution query response 
break problems identity delta 
need determine update parameter density structure update distribution structures 
term expression consider particular network structure prior distribution parameters clear resulting complete instance update parameters nodes 
interventional queries node query forced ancestors incoming edges cut 
example parent interventional query sampled original distribution easily information define variable updateable context interventional query update rule parameter density simple 
prior density response query standard bayesian updating parameters case randomly sampled instances update dirichlet distributions updateable nodes 
denote distribution obtained algorithm read density performing query obtaining complete response 
note quite different density denotes standard bayesian conditioning 
note performing interventional update parameters preserves parameter modularity 
consider distribution structures 
denote posterior distribution structures performing query obtaining response 
theorem tells easily update posterior interventional query theorem query complete response satisfies parameter independence parameter modularity score score ju ju ju active learning goal merely update distribution interventional data 
want actively select instances allow learn structure better 
myopic active learner function selects query current distribution takes resulting response uses update distribution repeats process 
described update process previous section 
task construct algorithm deciding query current distribution loss function tong koller key step approach definition measure quality distribution graphs parameters 
measure evaluate extent various instances improve quality distribution providing approach selecting query perform 
formally distribution graphs parameters loss function loss measures quality distribution graphs parameters 
query define expected posterior loss query xp loss definition immediately leads simple algorithm candidate query evaluate expected posterior loss select query lowest 
note expected loss appears computationally expensive evaluate 
need maintain distribution set structures number structures super exponential number nodes 
furthermore query compute expected posterior loss perform computation set structures exponential number possible responses query 
high level framework concrete pick loss function 
recall goal learn correct structure interested presence direction edges graph 
nodes possible edge relationships distribution graphs parameters induces distribution possible edge relationships 
measure extent sure relationship entropy induced distribution gammap log gammap log gammap log larger entropy sure relationship expression forms basis edge entropy loss function loss certain domains may especially interest determining relationship particular pairs nodes 
reflect desire loss function introducing scaling factors front different terms 
defined loss function distribution task find efficient algorithm computing expected posterior loss query relative note current distribution conditioned data obtained far 
initially prior get data bayesian conditioning described update apply algorithm posterior 
approach obtaining tractable algorithm ideas friedman koller consider simpler problem restricting attention network structures consistent total ordering oe 
introduce distribution orderings 
analysis fixed ordering oe total ordering restrict attention network structures consistent oe edge oe friedman assume node set possible candidate parents fixed query round 
certain domains prior knowledge construct data point nodes directly related define set candidate parents node consistent ordering oe fu oe oe oe represent set structures induced oe oe note number structures oe exponential number variables key impact restriction fixed ordering choice parents node independent choice parents node buntine friedman koller important consequences theorems give closed form efficiently computable expressions key quantities theorem query write probability response query oe oe pa score oe pa score oe pa 
theorem query completion write probability edge oe oe pa score oe pa score define score consider expected posterior loss eq 
oe oe xp oe oe compute oe theorem 
notice theorem expression oe depends values give fact applying theorem rewrite expected posterior loss oe xp oe oe oe oe oe theta oe pa xk score xk oe xk wk oe oe xk wk oe pa xk score xk xk wk expression involves summations exponential number possible completions query 
notice eq 
summation completions resembles expression computing marginal probability bayesian network inference marginalizing fact oe regarded factors standard graphical model inference procedures lauritzen spiegelhalter evaluate expression effectively 
restriction candidate set parents node ensures factor oe variables factor variables 
applying bayesian network inference factor variables possible query value expression xk oe 
need perform inference pair 
restricted candidate parents number possible edges kn 
computational cost computing expected posterior loss possible queries cost kn applications bayesian network inference 
analysis unrestricted orderings previous section obtained closed form expression computing expected posterior loss query ordering 
generalize derivation removing restriction fixed ordering 
expression expected posterior loss rewritten xp loss oe xp oe loss oe xp oe expectation orderings approximated sampling possible orderings current distribution graphs parameters 
shown friedman koller sampling orderings done effectively markov chain monte carlo mcmc techniques 
expression inside expectation orderings similar expected posterior loss query fixed ordering eq 

difference compute entropy terms restricting single ordering 
entropy term probability expressions relationships nodes oe terms inside expectation computed theorem 
naively compute expectation query completion sampling orderings oej computing oe 
clearly approach impractical 
simple approximation substantially reduces computational cost 
general mcmc algorithm generates set orderings sampled oe 
cases single data instance small effect distribution orderings samples oe reasonably approximation samples distribution oej 
current set sampled orderings approximate eq 

note fixed ordering case entropy term depends values variables bayesian network inference method compute expression xp oe 
algorithm summary properties summarize algorithm sample set orderings current distribution graphs parameters 
set orderings compute entropy terms 
ordering compute xp oe standard bayesian network inference algorithm obtain factor possible queries 
average query factors obtained ordering 
final result query factor possible query gives expected posterior loss asking query 
choose ask query gives lowest expected posterior loss 
consider computational complexity algorithm 
ordering need compute xp oe 
involves kn bayesian network inferences 
inference returns factor possible queries inference take time exponential number query variables 
time complexity algorithm generate query sampled orderings delta kn delta cost bn inference 
addition need generate sampled orderings 
friedman koller provide techniques greatly reduce cost process 
show markov chain mixes fairly rapidly reducing number steps chain required generate random sample 
setting reduce number steps required 
initially start uniform prior orderings easy generate random orderings 
starting point markov chain 
single query get response new posterior distribution orderings similar previous 
old set orderings fairly close new stationary distribution 
small number mcmc steps current orderings give new set orderings close sampled new posterior 
experimental results evaluated ability algorithm reconstruct network structure data generated known network 
experimented commonly networks cancer nodes asia nodes car twelve nodes 
test network maintained orderings described 
restricted set candidate parents size 
selected candidate parents node random node true parents generating network candidate parent sets 
typically took minutes active method generate query 
compared active learning method random sampling uniform querying choose setting query nodes uniform distribution 
method produces estimates probabilities edges pair variables domain 
compared number queries error random uniform active number queries error random uniform active number queries random uniform active number queries weighted error random uniform active number queries error random uniform active number queries edge entropy random uniform active cancer query node 
asia query nodes 
car query nodes 
car query nodes weighted edge importance 
cancer pairs single nodes queries 
cancer edge entropy 
legends reflect order curves appear 
axes zoomed resolution 
method estimate true network edge error estimate error ig gamma ig gamma ig gamma holds zero 
considered active method provides benefit random sampling obvious additional power having access queries intervene model 
set experiments eliminated advantage restricting active learning algorithm query roots 
algorithms informed nodes roots setting candidate parent sets empty 
query root causal query equivalent simply selecting data instance matches query give male need causal intervention create response 
situation able query root nodes arises domains medical domains example ability select subjects certain age gender ethnicity variables typically assumed root nodes 
figures show learning curves networks 
experimented uniform dirichlet priors informed priors simulated sampling data instances true network 
cancer car type prior little qualitative difference comparative performance learning methods graphs shown uniform priors 
asia domain uniform prior tended cause methods perform similarly 
possible reason approximation eq 
assumes single query significantly alter distribution orderings poor initial phases 
asia graph displayed slightly informed prior 
graphs see active method performs significantly better random sampling uniform querying 
domains determining existence direction causal influence particular nodes may special importance 
experimented car network 
modified edge error function eq 
edge entropy eq 
active method determining relationship particular nodes node times important regular pair nodes 
nodes network query nodes results shown fig 

active learning method performs substantially better 
note true causal interventions methods power identify model asymptotically identify skeleton edges direction forced markov equivalence class 
setting active learning algorithm allows derive information significantly faster 
considered ability active learning algorithm exploit ability perform interventional queries 
simple extension analysis permitted active algorithm choose set pair nodes single node nodes 
compared random sampling uniformly choosing set single pair nodes 
experiment performed cancer network informed prior random observations 
fig 
shows active method significantly outperforms methods 
see fig 
prediction error graphs similar graphs edge entropy eq 
distribution structures 
shows edge entropy reasonable surrogate predictive accuracy 
discussion introduces problem active learning bayesian network structure interventional queries 
formal framework task resulting algorithm adaptively selecting queries perform 
shown active method provides substantially better predictions regarding structure random sampling process interventional queries selected random 
somewhat surprisingly algorithm achieves significant improvements approaches restricted querying roots network exploit advantage intervening model 
significant body design experiments field optimal experimental design atkinson bailey focus learning causal structure domain experiment design typically fixed advanced selected actively 
active learning bayesian networks considered tong koller parameter estimation structure assumed known 
studies learning causal models purely observational data spirtes heckerman cooper yoo consider learning structure causal networks mixture experimental observational data non active learning setting 
interesting directions extend treatment continuous variables temporal processes 
interesting problem dealing hidden variables missing data 
active learning decide extra variable observe extra piece missing data try obtain order best learn model 
exciting direction potential active learning order try uncover existence hidden variable domain 
atkinson bailey atkinson bailey 
years design experiments pages biometrika 
biometrika 
press 
buntine buntine 
theory refinement bayesian networks 
proc 
uai 
cooper yoo cooper yoo 
causal discovery mixture experimental observational data 
proc 
uai 
friedman koller friedman koller 
bayesian network structure 
proc 
uai 
friedman friedman nachman pe er 
learning bayesian network structure massive datasets sparse candidate algorithm 
proc 
uai 
heckerman heckerman geiger chickering 
learning bayesian networks combination knowledge statistical data 
machine learning 
heckerman heckerman meek cooper 
bayesian approach causal discovery 
technical report msr tr microsoft research 
lauritzen spiegelhalter lauritzen spiegelhalter 
local computations probabilities graphical structures application expert systems 
royal statistical society 
pearl pearl 
probabilistic reasoning intelligent systems 
morgan kaufmann 
pearl pearl 
causality models reasoning inference 
cambridge university press 
spirtes spirtes glymour scheines 
causation prediction search 
mit press 
tong koller tong koller 
active learning parameter estimation bayesian networks 
proc 
nips 
appear 
