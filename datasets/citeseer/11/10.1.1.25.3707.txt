dynamic itemset counting implication rules market basket data sergey brin rajeev motwani jeffrey ullman department computer science stanford university rajeev cs stanford edu shalom tsur division hitachi america tsur hitachi com consider problem analyzing market basket data important contributions 
new algorithm finding large itemsets uses fewer passes data classic algorithms uses fewer candidate itemsets methods sampling 
investigate idea item reordering improve low level efficiency algorithm 
second new way generating implication rules normalized antecedent consequent truly implications simply measure occurrence show produce intuitive results methods 
show different characteristics real data opposed synthetic data dramatically affect performance system form results 
area data mining problem deriving associations data received great deal attention 
problem formulated agrawal ais ais sa mar toi referred market basket problem 
problem set items large collection transactions subsets baskets items 
task find relationships presence various items baskets 
numerous applications data mining fit framework 
canonical example problem gets name supermarket 
items products baskets customer purchases checkout 
determining products customers buy useful planning marketing 
applications varied data characteristics 
example student enrollment classes word occurrence text documents users visits done division hitachi america supported fellowship nsf 
web pages 
applied market basket analysis census data see section 
address performance functionality issues market basket analysis 
improve performance past methods introducing new algorithm finding large itemsets important subproblem 
enhance functionality introducing implication rules alternative association rules see 
common formalization problem finding association rules support confidence 
support itemset set items fraction transactions itemset occurs subset 
itemset called large support exceeds threshold oe 
association rule written itemsets confidence rule fraction transactions containing contain association rule hold large confidence rule exceed confidence threshold fl 
probability terms write fi jg oe ji fl 
existing methods deriving rules consist steps 
find large itemsets oe 

construct rules exceed confidence threshold large itemsets step 
example abc large itemset check confidence ab ac bc address tasks step performance perspective devising new algorithm step semantic perspective developing conviction alternative confidence 
algorithms finding large itemsets research focussed deriving efficient algorithms finding large itemsets step 
known algorithm apriori ais algorithms finding large itemsets relies property itemset large subsets large 
proceeds level wise 
counts itemsets finds counts exceed threshold large itemsets 
combines form candidate potentially large itemsets counts determines large itemsets 
continues combining large typically restricted just item doesn 
itemset itemset items 
itemsets form candidate itemsets counting determining large itemsets forth 
lk set large itemsets 
example contain ffa cg fa dg fa fg ck set candidate itemsets superset lk algorithm result set itemsets ck create counter itemset ck forall transactions database increment counters itemsets ck occur transaction lk candidates ck exceed support threshold result result lk ck itemsets item subsets lk algorithm performs passes data maximum number elements candidate itemset checking pass support candidates ck important factors govern performance number passes data efficiency passes 
address issues introduce dynamic itemset counting dic algorithm reduces number passes data keeping number itemsets counted pass relatively low compared methods sampling toi 
intuition dic works train running data stops intervals transactions apart 
parameter experiments tried values ranging 
train reaches transaction file pass data starts pass 
passengers train itemsets 
itemset train count occurrence transactions read 
consider apriori metaphor itemsets get start pass get 
itemsets take pass itemsets take second pass see 
dic added flexibility allowing itemsets get long get time train goes 
itemset seen transactions file 
means start counting itemset soon suspect may necessary count waiting previous pass 
example mining transactions count itemsets transactions read 
counting itemsets transactions read 
counting itemsets transactions 
assume itemsets need count 
get file counting itemsets go back start file count itemsets 
transactions finish counting itemsets transactions finish counting itemsets 
total passes data passes level wise algorithm 
dic addresses high level issues count itemsets substantial speedup apriori particularly apriori requires passes 
deal low level issue increment appropriate counters transaction section considering sort order items data structure 
implication rules contribution functionality market basket analysis implication rules conviction believe useful intuitive measure confidence interest see discussion section 
confidence conviction normalized antecedent consequent rule statistical notion correlation 
furthermore interest directional measures actual implication opposed occurrence 
features implication rules produce useful intuitive results wide variety data 
example rule past active duty military service vietnam high confidence 
clearly misleading having past military service increases chances having served vietnam 
tests census data advantages conviction rules confidence interest evident 
section results generating implication rules census data census 
census data considerably difficult mine supermarket data performance advantages dic finding large itemsets particularly useful 
counting large itemsets itemsets form large lattice empty itemset bottom set items top see example 
itemsets large denoted boxes rest small 
example empty itemset ab ac bc bd cd abc large 
show itemsets large count 
fact generally want know counts 
infeasible count small itemsets 
fortunately sufficient count just minimal ones itemsets include small itemsets itemset small supersets small 
minimal small itemsets denoted circles example ad bcd minimal small 
form top side boundary large small itemsets toivonen calls negative boundary lattice theory minimal small itemsets called prime implicants 
algorithm counts large itemsets find count large itemsets minimal small itemsets boxes circles 
dic algorithm described marks itemsets different possible ways ffl solid box confirmed large itemset itemset finished counting exceeds support threshold 
ffl solid circle confirmed small itemset itemset finished counting support threshold 
ffl dashed box suspected large itemset itemset counting exceeds support threshold 
course example assumed best possible circumstances estimated correctly exactly itemsets count 
realistically itemsets added little 
considerable savings 
transactions transactions itemsets itemsets itemsets itemsets itemsets itemsets dic apriori passes passes apriori dic abcd abc abd acd bcd ac ab cd bd ad bc itemsets lattice 
ffl dashed circle suspected small itemset itemset counting support threshold 
dic algorithm works follows 
empty itemset marked solid box 
itemsets marked dashed circles 
itemsets unmarked 
see 

read transactions 
experimented values ranging 
transaction increment respective counters itemsets marked dashes 
see section 
dashed circle count exceeds support threshold turn dashed square 
immediate superset subsets solid dashed squares add new counter dashed circle 
see figures 

dashed itemset counted transactions solid counting 

transaction file rewind 
see 

dashed itemsets remain go step 
way dic starts counting just itemsets quickly adds counters itemsets 
just passes data usually small values finishes counting itemsets 
ideally abcd abc abd acd bcd ab ac bc ad bd cd start dic algorithm 
abcd abc abd acd bcd ab ac bc ad bd cd transactions 
abcd abc abd acd bcd ab ac bc ad bd cd transactions 
abcd abc abd acd bcd ab ac bc ad bd cd pass 
want small possible start counting itemsets early step 
steps incur considerable overhead reduce 
data structure implementation dic algorithm requires data structure keep track itemsets 
particular support operations 
add new itemsets 

maintain counter itemset 
transactions read increment counters active itemsets occur transaction 
fast bottleneck process 
attempt optimize operation section 
maintain itemset states managing transitions active counted dashed solid small large circle square 
detect transitions occur 

itemsets large determine new itemsets added dashed circles potentially large 
data structure exactly hash tree apriori little extra information stored node 
trie properties 
itemset sorted items sort order discussed section 
itemset counting counted node associated prefixes 
empty itemset root node 
itemsets attached root node branches labeled item represent 
itemsets attached prefix containing item 
labeled item 
shows sample trie form 
dotted path represents traversal trie transaction abc encountered ab abc ac bc incremented hash tree data structure order 
exact algorithm described section 
node stores item itemset represents counter marker file started counting state branches interior node 
significance dic number benefits dic 
main performance 
data fairly homogeneous file interval reasonably small algorithm generally order passes 
algorithm considerably faster apriori passes maximum size candidate itemset 
data fairly homogeneous run random order section 
important relevant done toivonen sampling toi 
technique sample data reduced threshold safety count necessary itemsets data just pass 
pays added penalty having count itemsets due reduced threshold 
quite costly particularly datasets census data see section 
conservative algorithm assumption come back missed little penalty 
performance dic provides considerable flexibility having ability add delete counted itemsets fly 
result dic extended parallel incremental update versions see section 
non homogeneous data weakness dic sensitive homogeneous data particular data correlated may realize itemset large counted database 
happens shift hypothetical boundary start counting itemset supersets finished counting itemset 
turns census data ordered census district exactly problem occurs 
test impact effect randomized order transactions re ran dic 
turned significant difference performance see section 
cost associated randomizing transaction order small compared mining cost 
randomization may impractical 
example may expensive data may stored tape insufficient space store randomized version 
considered ways addressing problem ffl virtually randomize data 
visit file random order making sure pass order 
incur high seek cost especially data tape 
case may sufficient jump new location transactions 
ffl support threshold 
start support threshold considerably lower 
gradually increase threshold desired level 
way algorithm begins fairly conservative confident data collected 
experimented technique somewhat little success 
careful control slack different dataset useful technique 
ffl thing note data correlated location file may useful detect report 
possible local counter kept itemset measures count current interval 
interval checked considerable discrepancies support data set 
dic algorithm addresses high level strategy itemsets count 
lower level performance issues increment appropriate counters particular transaction 
address section 
item reordering low level problem increment appropriate counters transaction interesting 
recall data structure trie structure apriori see section 
collection time implement test toivonen algorithm compared 
tests lowered support thresholds suspect dic quite competitive 
ab bc total table increment cost abc ac ad bcd bd cd cost leaves 
ab bc total table increment cost itemsets form structure heavily dependent sort order items 
note sample data structure order items occurs trie occurs times 
determine optimize order items important understand counter incrementing process works 
transaction items certain order 
increment appropriate counters starting root node trie increment increment node counter counter leaf forall increment branches necessary branches exists increment branches return 
cost running subroutine equal gamma index ranges non leaf itemsets occur gamma index number items left element items checked inner loop 
advantageous items occur itemsets sort order items items left items occur itemsets 
example consider structure 
suppose items add respective itemsets data structure 
singletons hanging tree 
insert cost insert see table 
change order items note tree structure remains change order cost see table 
considerably cheaper 
want order items inverse popularity counted non leaf itemsets 
reasonable approximation inverse inverse popularity transactions 
interval transactions counting itemsets tree structure depends order 
transactions change order items build tree 
transactions resorted new ordering 
technique incurs overhead due re sorting data beneficial 
implication rules traditional measures interestingness support combined confidence interest 
consider measures probabilistic model 
fa bg itemset 
support fa bg write 
sure items rule applies occur frequently care 
task computationally feasible limiting size result set usually conjunction measures 
confidence conditional probability equal 
flaw ignores 
example equal occurrence unrelated high rule hold 
example people buy milk time supermarket purchase milk completely unrelated purchase salmon confidence salmon milk 
confidence quite high generate rule 
key weakness confidence particularly evident census data items occur items 
interest defined factors essentially measure departure independence 
measures occurrence implication completely symmetric 
fill gap define 
intuition useful logically rewritten see far deviates independence invert ratio take care outside negation believe concept useful number reasons ffl confidence conviction factors value relevant items completely unrelated salmon milk example 
ffl interest rules hold time vietnam veteran years old highest possible conviction value 
confidence property rules confidence 
interest useful property 
example people vietnam veterans years old get interest slightly interest completely independent items 
practice invert ratio search low values ratio 
way deal 
short conviction truly measure implication directional maximal perfect implications properly takes account 
results tested dic algorithm reordering different types data synthetic data generated ibm test data generator census data 
tested implication rules sets data results tests interesting census data 
synthetic data designed test association rules support confidence 
rules generated interesting evaluated utility items involved actual meaning 
results tests sets data justified dic tests census data justified implication rules 
test data synthetic test data generator documented convenient 
transactions average size items chosen items average large itemsets size 
census data bit challenging 
chose look files public microdata samples 
contain actual census entries constitute percent sample state file represents 
tests file washington consists roughly entries 
entry attributes represented decimal number 
example sex field uses digit male female 
income field uses digits actual dollar value person income year 
selected attributes study took possible field value pairs 
numerical attributes income took logarithm value rounded nearest integer reduce number possible answers total yielded different items 
data important differences synthetic data 
considerably wider versus items transaction 
second items extremely popular worked 
third entire itemset structure far complex synthetic data correlations directional example birth children female reverse true rules true time example attributes correlated degree 
factors mining census data considerably difficult supermarket style data 
test implementations implemented dic apriori different unix platforms 
implementations apriori thought special case dic case interval size size note done apriori years optimize impossible perform optimizations 
optimizations equally applicable dic 
www almaden ibm com cs quest html done numerical parameters 
focus research took simple approach 
dic apriori support threshhold execution time sec performance apriori dic synthetic data randomized dic dic apriori support threshhold execution time sec performance apriori dic census data relative performance dic apriori dic apriori run synthetic data census data 
running algorithms synthetic data fairly straightforward 
tried range support values produced large itemsets relatively easily 
apriori beat dic high support graph dic outperformed apriori tests running faster support threshold 
running dic apriori census data tricky 
number items census data appeared time huge number large itemsets 
address problem items support dropped 
fair number large itemsets manageable high support thresholds 
reader notice tests run support levels order magnitude higher support levels supermarket analysis 
far large itemsets generated 
support threshold mining proved time consuming nearly half hour just records 
support threshhold execution time sec effect varying interval size performance performance census data reasons mining census data difficult synthetic data 
census data times wider synthetic data 
counting itemsets take times longer transaction times pairs row census data 
counting itemsets take times longer itemsets take times longer 
course counting itemsets counting counting higher cardinality itemsets 
furthermore items support left popular items works hours week 
tend combine form long itemsets 
performance graphs show curves 
apriori dic dic shuffled order transactions effect apriori 
tests dic 
results clearly showed dic runs noticeably faster apriori randomized dic runs noticeably faster dic 
support level randomized dic ran times faster apriori 
varying value achieved slightly higher speedups times faster support times faster see section 
varying interval size experiment tried find optimal value interval size 
tried values values middle range worked best coming second respectively 
interval size proved worst choice due overhead incurred 
value somewhat slow took passes data lower interval values 
tried varying non randomized data 
experiments failed miserably able complete 
terms number passes support threshold apriori passes data simple dic randomized dic passes values reordering reordering support threshhold execution time sec performance item reordering respectively 
shows dic combined randomization sufficiently low finish small number passes 
effect item reordering item reordering nearly successful hoped 
small difference tests played negligible role performance 
tests census data difference wrong direction 
disappointment better analysis optimal order fly modification yield better results 
tests implication rules difficult quantify implication rules 
due high support threshold considered rules minimal small itemsets large itemsets 
total rules conviction conviction 
learned year olds don unemployed residents don earn income men don give birth interesting facts 
looking list conviction level find military looking year year census currently employed civilians 
list sample rules table 
note problem rules long involving say items complicated interesting 
list shorter ones 
comparison tests confidence produced misleading results 
example confidence women state looking job personal care limitations high scale 
turned simply respondents personal care limitations 
interest produced useful results 
example interest male birth considerably lower itemsets consider related appears way list rules interest greater 
conviction implication rule year olds don unemployed people don earn income men don give birth people military looking year year census currently civilian employment people military worked week limited disability heads household personal care limitations people school personal care limitations worked year african american women military african americans reside state born people moved past years table sample implication rules census data finding large itemsets dic algorithm particularly combined randomization provided significant performance boost finding large itemsets 
item reordering hoped 
isolated earlier tests big difference 
suspect different method determining item ordering technique useful 
selecting interval big difference performance warrants investigation 
particular may consider varying interval depending itemsets added checkpoint 
number possible extensions dic 
dynamic nature flexible adapted parallel incremental mining 
parallelism efficient known way parallelize finding large itemsets divide database nodes node count itemsets data segment 
key performance issues load balancing synchronization 
apriori necessary wait pass get results nodes determine new candidate sets pass 
dic dynamically incorporate new itemsets added necessary wait 
nodes proceed count itemsets suspect candidates adjustments get results nodes 
incremental updates handling incremental updates involves things detecting large itemset small detecting small itemset large 
difficult 
small itemset large may new potentially large itemsets new prime implicants count entire data just update 
determine new itemset counted go back count prefix data missed 
way dic goes back count prefixes itemsets missed straightforward extend dic way 
consideration useful find large itemsets data mine just data order months may correspond small updates 
recall train analogy section 
solution trains reading current data coming incrementing appropriate counts reading months old data decrementing appropriate counts remove effects 
order necessary able add remove itemset counters fly quickly efficiently dic handles static data 
extension may particularly useful 
census data census data particularly challenging 
market basket analysis techniques designed deal kind data 
difficult reasons see section data wide items transaction items varied support close close lot mine things highly correlated 
difficult mine supermarket data uniform ways 
believe data sets similarly challenging mine done handling efficiently 
may useful develop measures difficulty market basket data sets 
implication rules looking implication rules generated census data educational 
educational rules 
rules came top things obvious 
interesting things rules high conviction value high 
example people earned year poverty line 
interesting rules middle range extremely high obvious low insignificant 
believe generally true data mining application 
extremely correlated effects generally known obvious 
truly interesting ones far correlated 
big problem number rules generated 
impossible unnecessary deal rules 
considered techniques pruning 
ffl prune rules minimal 
example may prune rule 
somewhat nontrivial longer rule may hold higher conviction value may want keep 
implemented pruning rules subsets high conviction value 
proven quite effective tests cuts number rules generated factor 
rules pruned typically long misleading 
example pruned rule employed civilian looking job leave absence primary language english worked year 
implied rule employed civilian worked year 
kind pruning effective produce concise output user 
ffl second prune transitively implied rules 
rules hold time may want prune difficulties 
minimal set rules pick 

second rules don hold time handled 
conviction value expected carry 
conviction proven useful new measure having benefits perfect rules completely uncorrelated rules 
generally ranks rules reasonable intuitive way 
confidence assign high values rules simply consequent popular 
interest directional strongly affected direction arrow truly generate implication rules 
ranking system rules 
needs done rule pruning filtering 
experiments learned data fits market basket framework behaves nearly supermarket data 
believe wrong choice framework 
simply doing kind analysis data census data difficult 
correlations redundancies census data 
aware idiosyncrasies probably simplify problem considerably example collapsing redundant attributes find specialized solution 
want build general system capable detecting reporting interesting aspects data may throw 
goal developed dic task painful impatient developed conviction results mining market basket data usable 
members stanford data mining group helpful discussions 
wish hitachi america new information technology group supporting 
grateful roussopoulos help 
ais agrawal swami 
database mining performance perspective 
ieee transactions knowledge data engineering december 
ais agrawal swami 
mining association rules sets items large databases 
proc 
acm sigmod int conf 
management data pages may 
agrawal lin sawhney shim 
fast similarity search presence noise scaling translation time series databases 
proc 
int conf 
large data bases vldb 
agrawal srikant 
fast algorithms mining association rules 
proceedings th vldb conference santiago chile 
agrawal srikant 
mining sequential patterns 
proceedings th international data engineering taipei taiwan 
mar mehta agrawal rissanen 
sliq fast scalable classifier data mining 
march 
sa srikant agrawal 
mining generalized association rules 

toi toivonen 
sampling large databases association rules 
proc 
int conf 
large data bases vldb 
