information theoretic methods probability statistics csiszar budapest ideas information theory fruitful applications various elds science engineering mathematics pure applied 
illustrated typical applications information theory speci cally probability statistics 
early years information theory importance actual accomplishments shannon applied biology psychology linguistics fundamental physics economics theory organizations 
super cial applications shannon believe serious applications concepts elds forthcoming results quite promising establishing applications trivial matter slow tedious process hypothesis experimental veri cation 
shannon loc 
cit 
emphasized hard core essentially branch mathematics thorough understanding mathematical foundation surely prerequisite applications 
hard core branch mathematics expect natural way interaction branches mathematics addition enriching leads signi cant applications ideas mathematics 
applications kullback information theoretic approach statistics follow soon 
celebrated example kolmogorov fact stationary coding increase entropy rate show stationary processes di erent entropy rate isomorphic sense ergodic theory 
demonstrated processes mutually isomorphic solving long standing problem 
kolmogorov initiated spectacular developments ergodic theory entropy basic concept eld 
times scientists regarded panacea long passed today informa tion theorists aware established substantial applications discipline partially supported hungarian national foundation scienti research 
quite ones 
author mathematician particularly applications various branches pure applied mathematics including combinatorics ergodic theory algebra operations research systems theory primarily probability statistics 
goal give avor applications surveying typical ones speci cally probability statistics 
applications treated main tools properties information measures method types concept coding 
applications probability treated section statistics section 
preliminaries divergence divergence information divergence called relative entropy kullback leibler distance probability distributions pd nite set de ned kq log natural logarithms 
divergence pd arbitrary measurable space probability measures de ned kq sup kq sup taken measurable partitions denotes quantization de ned pd kg 
awell known integral formula kq kq log dx densities respect arbitrary dominating measure divergence non symmetric information theoretic measure distance key property kq equality stronger property known pinsker inequality variation distance jp qj kq jp qj jp dx true metric divergence respects analogue squared euclidean distance 
particular convex set pd minimum kq subject attained minimizer called projection unique kq kp kq csiszar 
de ned nite number linear constraints holds equality 
analogue pythagorean theorem analogue cosine theorem euclidean geometry 
applications probability gaussian measures dichotomy theorem consist functions algebra spanned cylinder sets 
pd called gaussian measure nite dimensional distributions pt tk gaussian 
ft pt tk image mapping 

dichotomy theorem says gaussian measures equiv mutually absolutely continuous orthogonal singular detection possible exists 
rst proof important result obtained hajek showing gaussian measures kq qkp implies orthogonality 
course kq qkp implies equivalence non gaussian 
sketch hajek proof 
kq pt tk easy consequence eq 

ii 
divergence invariant transformations permits reduce calculation pt tk easy case pt tk dimensional standard gaussian qt tk product form 
iii 
easy case direct calculation shows pt tk qt tk large pt tk qt tk concentrated disjoint sets 
large deviations sanov theorem gibbs conditioning principle random variables values arbitrary set equipped algebra common distribution empirical pn xn random probability measure de ned pn jfi xi 
sanov theorem sanov says intuitively pd nite set exactly pn close expf nd kq pn pg expf nd kq log possible type block length basic fact method types cf 
csiszar korner 
clearly implies lim log pn inf set pd types dense 
kq arbitrary general form sanov theorem says set pd probabilities pn de ned lim inf lim sup log pn inf log pn inf kq kq denote interior closure topology topology convergence probability measures topology weaker topology variation dis tance 
course equality right hand sides su cient condition limit relation 
particular holds convex kq particular interest choice fp fdp cg possibly vector valued function subset range pn means np xi parlance large deviations theory cf 
dembo asymptotic bounds easily checked compactness topology divergence balls fp kp ag mean pn satis es large deviation principle rate function kq 
simplest available proof important result inherently information theoretic boom acknowledge suggestion author proving 
sketch proof di cult part 
measurable partition pn bounded sum probabilities pd kg type 
implies gives follows checks pn expf inf kq log lim sup log pn sup inf kq sup non trivial hard 
inf kq inf sup kq suppose convex set pd conditional distribution pn belongs case convex set ef exists 
hypotheses asymptotic bound may sharpened non asymptotic bound 
importantly mild additional hypotheses strong form gibbs conditioning principle cf 
dembo may established simple reasoning csiszar 
identity kq kp nd kq holds pde marginals equal pd take conditional distribution pn assumption 
follows log pn pkq pkp nd kq inf kq proving claimed non asymptotic bound 
projection obtain log pn pkp nd kp nd kq kp nd kq step follows replaced supposing nally satis es follows px kp says intuitively conditionally pn random variables xn behave independent ones common distribution particular implies xed kp conditional joint distribution pn converges product distribution stronger sense variation distance cf 

promised strong version gibbs conditioning principle 
result may extended case projection exist 
unique necessarily exists inf kq pn implies holds generalized projection csiszar 
measure concentration measure concentration currently hot topic probability theory cf 
talagrand 
general description problems pertinent topic lead far result considered early example measure concentration theorem known information theorists 
blowing lemma margulis ahlswede korner says intuitively slightly blowing set exponentially small probability gets set probability close 
blowing lemma originally just mathematical tool particularly useful multiuser shannon theory cf 
csiszar korner 
today integral part due information theoretic proof rst simple proof 
extended approach prove measure concentration results providing examples applications probability theory 
sketch main ideas approach restricting attention simplest case 
pd nite set qn fy ag denote blow set denotes normalized hamming distance 
proved distance minfd bg subsets bounded terms probabilities log log choice gives strong version blowing lemma sketch proof exp log key idea show qn pd necessarily product form exist random variables xn distribution yn distribution ed kq obvious pinsker inequality minimum prfx yg subject px py equals jp qj 
proof goes induction coupling argument extend satisfying induction hypothesis suitable new components xn yn 
ii result implies triangle inequality arbitrary pd ande exist ande distributions ande ed kq kq iii follows letting ande conditional distributions obtained conditioning respectively 
choice left hand side lower bounded probability 
approach derive bounds extended distributions certain processes memory role including mixing markov chains 
far approaches measure concentration extended 
noted weaker asymptotic form blowing lemma established broad class processes substantial ideas shields 
topics applications probability covered 
just mention various limit theorems probability theory informa tion theoretic proofs 
include central limit theorem barron ergodicity markov chains renyi kendall fritz limit theorem convolution powers pd topological group csiszar 
promising idea prove bounds recurrence matching problems utilizing non existence codes beating entropy bound shields section ii 
information theoretic methods statistics statistics science extracting information data appears natural eld ap plications communication theory 
historically information measure statisticians prior shannon fisher information fisher 
divergence rst explicitly introduced purposes statistics motivated shannon kullback leibler 
implicitly played role earlier statistical works wald kullback soon developed uni ed approach testing statistical hypotheses information measure kullback 
results considered retrospect applications statistics statisticians independently 
wald explicitly mention information treatment sequential analysis noted considered major contribution statistical applications kullback 
author shares view considers results subsection applications typical tool viz 
method types 
proof results preceded development method types represented origins method 
prefer speak context interplay statistics statistical applications 
major inference methods motivated methods maximizing entropy minimizing divergence minimizing description length 
coverage impossible lack space necessary information theorists familiarity methods 
illustrate simple examples 
early results start wald inequality relating expected sample size sequential test type type error probabilities 
assuming sampling distribution known sequential test accepts hypotheses basis sample xn random length stopping time knowledge xn determines wald proved ep kq log log probability accepting probability accepting wald showed sequential probability ratio test nearly attains equality 
interpretation result easy understand denoting distribution left hand side equals kq checked wald identity right hand side divergence quantizations acceptance regions likelihood ratio constant equality hold 
test achieve exactly general sequential probability ratio test comes close 
early result statistical celebrated stein lemma cherno stein apparently 
provides operational meaning divergence testing simple hypothesis simple alternative best test sample size type error probability type error probability expf nd kq notice type error required go zero just special case const wald inequality imply type error probability exponent exceed kq 
hypothesis testing exponential rate optimal tests nite set pd suppose null hypothesis sample size comes tested alternative hypothesis speci ed 
test accepts null hypothesis empirical pn sample belongs divergence ball fp kp ag universally exponential rate optimal sense 
probability type error goes exponentially exponent alternative hypothesis positive probability type error goes zero exponent min kq alternative hypothesis speci ed tests type error probability exponent type error probability decreases larger exponent particular alternative exponential decrease type error achievable 
interest modi cation test replacing constant sequence log nan 
type error probability goes longer exponentially probability type error alternative goes zero exponent kq 
best possible stein lemma 
results due hoe ding 
today information theorists proof easy exercise method types cf 
csiszar korner 
extension testing composite hypotheses straightforward 
test null hypothesis true distribution belongs set pd take union divergence balls accept null hypothesis pn union 
type error probability goes exponent simple alternative type error probability exponent mum subject best possible 
notice acceptance pn equivalent supp exp na universally rate optimal tests statisticians call likelihood ratio tests 
consider hypothesis testing pd arbitrary set equipped algebra 
previous acceptance criterion sense continuous distribution 
way consider re ning sequence partitions nm generates accept null hypothesis true distribution belongs set pd inf pn kp assuming number types alphabet size exp type error probability test exp na type error probability exp bn inf min kq approach due 
showed compactness assumption bn approaches test universally exponential rate optimal 
particular rate optimality holds null hypothesis simple 
notice relationship results subsection subsection 
iterative scaling em algorithm iterative scaling familiar procedure infer matrix non negative entries pij row column sums pi pij known say pi prior guess ij qij available 
procedure consists iteratively adjusting row column sums setting limit ij pi ij pi ij odd pij pij lim author knowledge iterative scaling rst estimate telephone tra viz 
number pij calls exchange exchange day counts pi outgoing incoming calls exact knowledge tra qij previous day 
statistics procedure rst proposed deming stephan infer dimensional distribution pij known marginals pi empirical distribution observed sample contingency table prior guess nature procedure recognized ireland kullback 
suppose pd denote set dimensional pd rst marginal respectively second marginal belongs odd kp kp kp odd 
due pythagorean identity projection respectively 
show csiszar lling gap proof ireland kullback fp kq limits exist ij equals projection 
uniqueness projection su ces show convergent subsequence ni say kq kp kq holds summing identities ni letting gives kq kp kp kq implies kp consequently lim ni lim ni establishing 
applies yielding sum equals kq 
completes proof 
similar convergence result claimed kullback iterative scaling densities 
case harder proof essentially relies continuity divergence function second variable longer holds underlying set nite 
convergence proof iterative scaling densities appears available additional assumptions 
large variety problems requiring infer pd generally non negative valued function available information consists certain linear constraints 
popular maximum entropy method suggests take feasible closest divergence default model projection feasible set satisfying constraints 
divergence non negative valued functions necessarily pd nite set de ned extension eq 
kq log feasible set represented projection easy compute 
iterative scaling desired projection maxent solution computed iterating successive projections starting convergence projection follows way pythagorean theorem projections eq 
equality provided fp pkq 
iterative algorithms statistics intuitive interpretations 
algorithm designed compute projection arbitrary feasible set de ned linear constraints underlying set nite generalized iterative scaling darroch ratcli known smart algorithm byrne 
shown csiszar equivalent iterative projection performed suitable product space 
called em algorithm dempster laird rubin designed compute maximum likelihood estimates incomplete data shown csiszar equivalent alternating minimization kq suitably constructed 
convergence proved technical conditions important convexity 
general result implies convergence em algorithm particular case decomposition mixtures 
remarkably general result implies convergence familiar algorithms computing channel capacity blahut rate distortion functions blahut optimum portfolios cover 
works related topics subsection include byrne della pietra della pietra la erty 
minimum description length mdl mdl statistical inference principle motivated rissanen 
says various possible stochastic models model classes data sequence xn select yielding shortest code account bits needed describe model model class encoding 
mdl naturally lead strong interplay statistics theory universal data compression 
deserve detailed coverage limited space consider just example mdl approach markov order estimation 
binary sequences consider model classes rst order markov second order markov order markov 
pre condition code may regarded optimal model class maximum model class mean redundancy max redundancy smallest possible 
known code optimal sense order markov model class meaning codeword length log xn log maximum probability xn order markov sources 
disregarding term order yielding minimum codelength arg max log xn log taken mdl estimate markov order notice description length model class constant considered nite number classes enter comparison 
eq 
instance general result chose model classes involving di erent number parameters criterion mdl maximized log likelihood minus number parameters times log statistics known bic criterion enjoys desirable properties 
interesting note previous penalized maximum likelihood criterion known aic derived considerations akaike 
mutual information statistics ideas statistics comes naturally adopting bayesian approach 
suppose joint distribution xn depends unknown parameter say xn distribution 
order amount information provided observation parameter viz 
mutual information de ned necessary assigned distribution called prior distribution 
plays role input distribution channel de ned possible distributions corresponding possible values 
course input distribution assigned just technical tool prior assigned 
reason statistical applicability mutual information related tools fano inequality means restricted bayesian statistics 
sixties renyi studied papers asymptotics set possible values nite cf 
renyi 
showed exponentially fast related asymptotic behavior error probability bayesian maximum posteriori probability estimate 
subset ir positive lebesgue measure mutual information typically goes nity 
asymptotics studied seventies russian researchers pinsker 
clarke barron obtained sharp results 
bayesian statistics bernardo suggested called prior selected criterion yielding maximum limit 
argued criterion leads familiar je reys prior clarke barron provide rigorous proof restrictive hypotheses family fp statistical problems parameter set nite dimensional may set probability densities ir ir class densities satisfying smoothness conditions 
context idea consider restricted having uniform distribution suitable nite subset suppose problem estimate observations estimator evaluated supremum expected loss loss function subject suitable assumptions may bounded terms tn tn valued approximation estimator sup expected loss may bounded terms sup tn nj tn right hand side equals uniformly distributed fano inequality bounded log nj log tn log nj xn log nj log log nj log log nj log nj arrives useful lower bound sup ed valid estimator 
ideas hinted derive risk bounds tight constant factor non parametric density estimation 
works direction include pinsker yu yang barron results particularly impressive 
topics section fraction statistical applications covered 
information tangentially mentioned refer excellent survey barron 
just mention eld jointly belongs statistics obviously requires methods disciplines hypothesis testing estimation remote observations subject rate constraints permissible communication 
works hypothesis testing estimation problems respectively communication constraints include ahlswede csiszar han respectively zhang berger ahlswede han amari 
ahlswede 
minimax estimation presence side information remote data ann 
statist vol pp 
ahlswede csiszar 
hypothesis testing communication constraints ieee trans 
vol pp 
ahlswede korner 
bounds conditional probabilities applications multiuser communication 
verw 
gebiete vol pp 
akaike 
information theory extension maximum likelihood principle second int symp 
inform 
theory pp petrov eds budapest 

algorithm computing capacity discrete memoryless channels ieee trans 
vol pp 
barron 
entropy central limit theorem ann 
probab vol pp 
barron 
information theory probability statistics learning neural nets manuscript 
bernardo 
posterior bayesian inference roy 
statist 
soc 
vol pp 
blahut 
computation channel capacity rate distortion functions ieee trans 
vol pp 
byrne 
iterative image reconstruction algorithms cross entropy minimization ieee trans 
image processing vol pp 
byrne 
alternating minimization generalized orthogonality pythagorean identities iterative image reconstruction siam optimization submitted 
cherno 
measure asymptotic ciency tests hypothesis sum observations ann 
math 
statist vol pp 

clarke barron 
je reys prior asymptotically favorable entropy risk statist 
planning inference vol pp 

cover 
algorithm maximizing expected log investment return ieee trans 
vol pp 

csiszar 
note limiting distributions topological groups publ 
math 
inst 

acad 
sci vol pp 
csiszar 
divergence geometry probability distributions minimization problems ann 
probab vol pp 
csiszar 
sanov property generalized projections conditional limit theorem ann 
probab vol pp 
csiszar 
geometric interpretation darroch ratcli generalized iterative scal ing ann 
statist vol pp 
csiszar korner 
information theory coding theorems discrete memoryless systems academic 
csiszar 
information geometry alternating minimization proce statistics decisions suppl pp 
darroch ratcli 
generalized iterative scaling log linear models ann 
math 
statist vol pp 
della pietra della pietra la erty 
bregman distances iterative scaling auxiliary functions manuscript 
dembo 
large deviations techniques applications jones bartlett 
deming stephan 
squares adjustment sampled frequency table expected marginal totals known ann 
math 
statist vol pp 
dempster laird rubin 
maximum likelihood incomplete data em algorithm roy 
statist 
soc vol pp 
pinsker 
estimation square integrable probability density random variable russian probl 

inform vol pp 
fisher 
theory statistical estimation proc 
camb 
phil 
soc vol pp 
fritz 
information theoretic proof limit theorems reversible markov processes trans 
sixth prague conference inform 
theory pp academia prague 

probability weighing evidence gri london 

large deviation theorems em probability measures ann 
probab vol pp 
hajek 
property normal distributions stochastic process russian math 
vol pp 
han 
hypothesis testing multiterminal data compression ieee trans 
vol pp 
han amari 
parameter estimation multiterminal data compression ieee trans 
vol pp 

lower bound risks nonparametric estimates densities uniform metric theory probab 
appl vol pp 
hoe ding 
asymptotically optimal tests multinomial distributions ann 
math 
statist vol pp 

information sample parameter second int symp 
inform 
theory pp 
petrov eds budapest 

bounds risks non parametric regression estimates theory probab 
appl vol pp 

ireland kullback 
contingency tables marginals biometrika vol pp 
kendall 
information theory limit theorem markov chains processes countable nity states ann 
inst 
stat 
math vol pp 
kolmogorov 
new invariant transitive dynamical systems russian dokl 
sssr vol pp 

de vol pp 
kullback 
information theory statistics wiley 
kullback 
probability densities marginals ann 
math 
statist vol pp 

kullback leibler 
information su ciency ann 
math 
statist vol pp 
yu 

information theoretic proof central limit theorem lindeberg conditions russian 

vol pp 

margulis 
probabilistic characteristics graphs large connectivity russian probl 

inform vol pp 

simple proof blowing lemma ieee trans 
vol pp 

bounding distance informational divergence method prove measure concentration ann 
probab vol pp 
shields 
positive divergence blowing properties israeli math vol pp 

iterated averages projections ann 
statist submitted 
pinsker 
information contained observations asymptotically su cient statis tics russian probl 

inform vol pp 
renyi 
measures entropy information proc 
fourth berkeley symposium math 
statist 
probab vol pp univ calif press 
renyi 
problems statistics point view information theory proc 
coll 
inform 
theory pp bolyai math 
soc budapest 
rissanen 
modeling shortest data description automatica vol pp 

rissanen 
stochastic complexity statistical inquiry world scienti 
convergence iterative proportional tting procedure ann 
statist vol pp 
sanov 
probability large deviations random variables russian math 
vol pp 

multiterminal detection zero rate data compression ieee trans 
vol pp 
shannon 
ire trans 
vol 
shields 
ergodic theory discrete sample paths graduate studies math vol amer 
math 
soc 
talagrand 
concentration measure isoperimetric inequalities product spaces publ 
math 
vol pp 
talagrand 
new look independence special invited ann 
probab vol pp 

asymptotically optimal tests ann 
statist vol pp 
wald 
sequential analysis wiley 
yang barron 
information theoretic determination minimax rates con vergence appear ann 
statist 
yu 
fano le cam appear festschrift honor le cam 
zhang berger 
estimation compressed information ieee trans 
vol pp 

