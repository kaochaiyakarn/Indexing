nonlinear black box modeling system identification unified overview jonas sjoberg zhang lennart ljung albert benveniste bernard pierre yves june nonlinear black box structure dynamical system model structure prepared describe virtually nonlinear dynamics 
considerable interest area structures neural networks radial basis networks wavelet networks hinging hyperplanes wavelet transform methods models fuzzy sets fuzzy rules 
describes approaches common framework user perspective 
focuses common features different approaches choices considerations relevant successful system identification application techniques 
pointed nonlinear structures seen concatenation mapping observed data regression vector nonlinear mapping regressor space output space 
mappings discussed separately 
mapping usually formed basis function expansion 
basis functions typically formed simple scalar function modified terms scale location 
expansion scalar argument regressor space achieved radial ridge type approach 
basic techniques estimating parameters structures criterion minimization step procedures relevant basis functions determined data linear squares step determine coordinates function approximation 
particular problem deal large number potentially necessary parameters 
handled making number parameters considerably number offered parameters regularization shrinking pruning regressor selection 
mathematically comprehensive treatment companion 
keywords nonlinear system model structures parameter estimation wavelets neural networks fuzzy modeling 
key problem system identification find suitable model structure model 
fitting model structure parameter estimation ll hh js department electrical engineering linkoping university linkoping sweden ab bd aj qz irisa inria campus de beaulieu rennes cedex france avenue des de rennes cedex france mail name liu se name irisa fr cases lesser problem 
basic rule estimation estimate know 
words utilize prior knowledge physical insight system selecting model structure 
customary distinguish levels prior knowledge color coded follows 
ffl white box models case model perfectly known possible construct entirely prior knowledge physical insight 
ffl grey box models case physical insight available parameters remain determined observed data 
useful consider sub cases physical modeling model structure built physical grounds certain number parameters estimated data 
state space model order structure 
semi physical modeling physical insight suggest certain nonlinear combinations measured data signal 
new signals subjected model structures black box character 
ffl black box models physical insight available chosen model structure belongs families known flexibility successful past 
black box models black box linear models task really describe approximate system frequency response impulse response just mapping pm number outputs number inputs 
typically nice functions dominate applications modest approximation problem extensively successfully handled known linear black box structures 
typical structures reviewed section 
nonlinear black box situation difficult 
main reason excluded rich spectrum possible model descriptions handled 
shall discuss possibilities limitations nonlinear black box identification 
area quite diverse covers topics mathematical approximation theory estimation theory non parametric regression algorithms currently concepts neural networks wavelets fuzzy models 
important links classical statistical approaches non parametric regression density estimation kernel methods nearest neighbor techniques 
rich literature subject 
general treatments may refer books neural networks kung haykin books fuzzy models brown harris wang books surveys non parametric regression density estimation stone silverman devroye background material wavelets multi resolution techniques daubechies chui meyer organization take position practical user nonlinear black box models describe essential features available approaches discuss issues deal successfully arrive model observed data 
companion complements material theoretical aspects 
papers read independently 
organized follows 
shall look modeling question find general non linear black box model seen concatenation mapping past observed data regressor space non linear function expansion type mapping space system outputs 
done section 
mappings dealt separately sections respectively 
check bearings discuss basic model properties section giving important insights deal potentially large number parameters required handle arbitrary non linear dynamical systems 
estimation techniques criterion optimization direct methods dealt sections respectively 
fuzzy modeling fits general framework discussed section 
numerical examples real data section user choices attitudes discussed section 
glossary take classical statistical approach problem 
earlier treatments particular neural networks fuzzy models perspectives developed special terms traditional statistical concepts 
provide glossary commonly terms estimate train learn validate generalize model structure network estimation data training set validation data generalization set overfit overtraining nonlinear black box structures system identification problem follows observed inputs outputs dynamical system looking relationship past observations gamma gamma outputs gamma gamma additive term accounts fact output exact function past data 
goal small may think gamma gamma prediction past data 
equation models general discrete time dynamic systems 
static systems viewed particular case dynamic systems mainly focus dynamic systems 
find function 
way search family functions 
parameterize function family finite dimensional parameter vector gamma gamma parameterizing function finite dimensional vector usually approximation 
main topic find parameterization deal 
decided structure collected data set quality naturally assessed means fit model data record ky gamma gamma gamma norm actual way achieving trying achieve minimum may differ system identification schemes follow concept 
model structure family really general turns useful write concatenation mappings takes increasing number past observations maps finite dimensional vector fixed dimension takes vector space outputs gamma gamma gamma gamma shall call vector regression vector components referred regressors 
allow general case formation regressors parameterized gamma gamma short write 
regression vector depends model parameters 
simplicity extra argument explicitly essential discussion 
choice nonlinear mapping decomposed partial problems dynamical systems 
choose regression vector past inputs outputs 

choose nonlinear mapping regressor space output space 
shall address possibilities choices sections 
regressors possibilities get guidance choice regressors review linear case 
review linear black box models simplest dynamical model finite impulse response model fir gamma gamma denote shift operator polynomial gamma corresponding predictor tj regression vector gamma gamma gamma tends infinity may describe dynamics nice linear systems 
character noise term modeled way 
linear black box structures practice variants different ways picking poles system different ways describing noise characteristics 
common models ljung summarized general family special cases known box jenkins bj model model output error oe model arx model 
predictor associated pseudo linear regression form see eq ljung tj regressors components general case 
gamma associated polynomial 
gamma associated polynomial 
gamma kj simulated outputs past associated polynomial 
gamma gamma gamma gamma kj prediction errors associated polynomial 
gamma gamma gamma gamma kj simulation errors associated polynomial remarked case simulated output refers quantity 
linear state space model predictor form ax bu gamma cx cx described pseudo linear regression predictor tj cx states regressors 
note component obtained linear filtering past inputs outputs filters depend matrices model output error type 
essential difference state space regressors input output regressors described earlier contain blocks regressor time shifted number steps 
characteristic state space models echelon type 
state space regressors restricted internal structure 
implies possible obtain efficient model smaller number regressors state space model 
state space models connection neural nets discussed rivals matthews 
regressors nonlinear black box dynamical models described regressors give necessary freedom linear black box case natural nonlinear case 
structures kind tj nonlinear function parameterized components similar just described regressors 
input output case ones gamma gamma measured variables cause problems include 
remaining previous outputs black box model gamma kj write 
question arises simulated output gamma kj computed network produces predicted outputs gamma kj 
answer output model equal tj measured outputs gamma regressors replaced computed gamma kj 
nomenclature linear models natural coin similar names nonlinear models 
line 
chen chen billings 
distinguish ffl models gamma regressors ffl models gamma gamma regressors ffl noe models gamma gamma kj regressors 
case output model tj 
ffl models gamma gamma gamma kj regressors ffl models gamma gamma kj gamma kj gamma kj regressors 
case simulated output obtained output structure replacing zeros regression vector ffl non linear state space models past components virtual outputs signal values internal nodes network see correspond output variable 
narendra parthasarathy notation models conjunction neural networks 
model called series parallel model noe called parallel model 
model structures noe non linear state space model correspond recurrent structures see subsection parts regression vector consist past outputs model 
general harder recurrent structures 
things difficult check conditions obtained predictor model stable takes extra effort calculate gradients model parameter estimation 
choices regressors far discussed regressors just linear functions measured inputs measured outputs model outputs 
physical insight system hand utilize information form new variables transformations raw measurements 
practical point view sufficient regard called input output suitable transformations raw measurements formed view known system 
semi physical regressors example power signal formed voltage current measurements believe essential stimulus system 
nonlinear structures applied reason waste parameters estimate facts known 
type preprocessing raw data light prior knowledge filtered input regressors gamma filters tailored application 
laguerre kautz filters extensively discussed applications 

van den hof interesting generalizations regressor choices described 
structural questions actual way regressors combined clearly reflects structural assumptions system 
example consider assumption system disturbances additive necessarily white noise denotes past inputs disturbance need spectral description 
described white sequence fe predictor gamma gamma gamma term filter gamma equally subsumed general mapping 
structure leads noe structure complemented linear term containing past narendra parthasarathy related neural network model suggested 
described consists delayed outputs delayed inputs 
parameterized functions chosen linear nonlinear neural net 
motivation model easier develop controllers models discussed earlier 
suggested build linear model system 
residuals model contain unmodeled nonlinear effects 
neural net model applied residuals treating inputs residuals input output pick nonlinearities 
attractive step obtain linear model robust leads reasonable models 
second neural net step assured obtain model linear 
nonlinear mappings possibilities function expansions basis functions basic features turn nonlinear mapping goes point matter regression vector constructed 
just vector lives natural think parameterized function family function expansions ff refer basis functions role play similar functional space basis 
particular situations constitute functional basis 
typical examples wavelet bases see subsection 
going show expansion different basis functions possible choice regression vector previous section plays role unified framework investigating known nonlinear black box model structures 
key question choose basis functions know nonlinear black box model structures composed obtained parameterizing single mother basis function generically denote 
situations generally write fi fl fi gamma fl equation interpreted symbolically specified precisely 
stresses fi fl denote parameters different nature 
typically fi related scale directional property fl position translation parameter 
scalar example fourier series take cos 
fourier series expansion fi frequencies fl phases 
scalar example piecewise constant functions take unit interval indicator function take example fl fi delta ff delta 
gives piecewise constant approximation function clearly obtained quite similar result smooth version indicator function gaussian bell gammax variant piece wise constant case take unit step function just variant indicator function obtained difference steps 
smooth version step sigmoid function oe gammax course give quite similar results 
classification single variable basis functions classes single variable basis functions distinguished depending nature ffl local basis functions functions having gradient bounded support vanishing rapidly infinity 
loosely speaking variations concentrated interval 
ffl global basis functions functions having infinitely spreading bounded gradient 
clearly fourier series example global basis function local functions 
construction multi variable basis functions multi dimensional case multi variable functions 
practice constructed single variable function simple manner 
recall methods constructing multi variable basis functions single variable basis functions 

tensor product 
single variable functions identical tensor product construction multi variable basis function product delta delta delta 

radial construction 
single variable function radial construction multivariable basis function form fi fl gamma fl fi delta fi denotes chosen norm space regression vector 
norm typically quadratic norm fi fi fi possibly dependent positive definite matrix dilation scale parameters 
simple cases fi may just scaled versions identity matrix 

ridge construction 
single variable function 
fi fl ridge function fi fl fi fl ridge function constant sub space fi 
consequence mother basis function local support basis functions unbounded support subspace 
resulting basis said semi global term ridge function precise 
comment different possibilities 
evaluating function constructed tensor product factor functions evaluated separately computational cost roughly proportional dimension function constructed methods dimension dependent computational cost stays evaluation norm gamma fl inner product fi consequently dimension dependence weaker 
reason tensor product rarely large dimensional case 
hand methods yield different forms multi variable functions 
factors different natures tensor product construction allows build functions behave differently different directions 
radial construction ensures directional homogeneity 
ridge construction offers direction selective feature basis functions necessarily constant directions 
turns quite useful property practical cases 
note particular situations methods may lead result multi variable gaussian function obtained tensor product construction radial construction 
connection named structures briefly review popular model structures 
structures related interpolation techniques discussed 
general form function expansions composed basis functions obtained parameterizing particular mother basis function described previous section 
wavelets 
wavelet decomposition typical example local basis functions 
loosely speaking mother basis function usually referred mother wavelet wavelet literature denoted dilated translated form wavelet basis context common expansion doubly indexed scale location specific choices dimensional case fi fl gives notation gamma multi variable wavelet functions constructed tensor products scalar wavelet functions preferred method 
see section 
compared simple example piece wise constant function approximation section multi resolution capabilities different scale parameters simultaneously 
suitably chosen mother wavelet appropriate translation dilation parameters wavelet basis orthonormal easy compute coordinates ff 
shall discuss detail subsection extensively 
strictly speaking dilated translated wavelets may frame basis 
see daubechies 
wavelet radial basis networks 
choice local basis functions combination radial construction multi variable case orthogonalization wavelet networks zhang benveniste radial basis neural networks poggio girosi 
kernel estimators 
known example local basis functions kernel estimators nadaraya watson 
kernel function delta typically bell shaped function kernel estimator form ff gamma fl small positive number fl points space regression vector 
clearly special case 
nearest neighbors interpolation 
models produce outputs depending closest estimation data points interpolation models described expansions basis functions 
assume data drawn values form uniform lattice take indicator function expanded hypercube radial approach max norm 
choose location scale parameters cubes gamma fl fi tightly laid exactly data point falls center cube 
corresponding expansions equivalent nearest neighbor model consists value output estimate value data point value closest 
splines 
splines local basis functions piecewise polynomials 
connections pieces polynomials continuous derivatives certain order depending degree polynomials de boor schumaker 
splines nice functions computationally simple smooth desired 
reasons widely classic interpolation problems 
sigmoid neural networks 
combination model expansion ridge basis function sigmoid choice mother function gives celebrated hidden layer feed forward sigmoid neural net 
hinging hyperplanes 
hinging hyperplanes model breiman closely related neural network corresponds choice hinge function sigmoid mother basis function hinge function form open book see defined breiman sigma max phi fi fl fi gamma fl gamma psi fi fi gamma row vectors fl fl gamma scalars 
sjoberg shown hinging hyperplane model parameterized original form 
introducing basis functions sigmax usually kernel function noted delta 
hinge function building block hinging hyperplane models hinging hyperplanes model expressed fi fl fl parameter vector dimension 
hinging hyperplane model ridge constructions additional linear term 
hinge functions basic functions yields kind piecewise linear model proposed sontag 
projection pursuit regression 
example ridge type basis functions projection pursuit regression huber friedman stuetzle having form ff fi fl fi matrices smooth fitted functions 
connection framework obvious 
term projection pursuit derives fact selected dimensions represent projections regressor space show significant patterns 
words happens subspaces 
partial squares 
ridge basis function approaches connection conceptually partial squares pls techniques chemometrics wold 
pls employs techniques select significant subspaces larger regressor space reduce number parameters estimate 
fuzzy models 
called fuzzy models belong model structures class 
case basis functions constructed fuzzy set membership functions inference rules 
works discussed section 
network questions far viewed model structures basis function expansions albeit adjustable basis functions 
structures referred networks primarily typically mother basic function repeated large number times expansion 
graphical illustrations structure look networks 
multi layer networks network aspect function expansion pronounced basic mappings convolved manner outputs basis functions denoted fi fl collect vector linear combination output model treat new regressors insert layer basis functions forming second expansion ff fi fl denotes collection involved parameters ff fi fl ff fi fl neural network terminology called hidden layer network 
basis functions fi fl constitute hidden layer fi fl give second 
layers hidden show explicitly output course available user 
see illustration 
clearly repeat procedure arbitrary number times produce multi layer networks 
term primarily sigmoid neural networks applies basis function expansion 
question layers easy 
principle basis functions hidden layer sufficient modeling practically reasonable systems 
see example cybenko barron 
sontag contains useful interesting insights importance second hidden layers nonlinear structure 
recurrent networks important concept applications dynamical systems recurrent networks 
refers situation regressors time outputs model structure previous time instants gamma see illustration 
case component regressor time obtained value interior node just output layer previous time instant 
model dependent regressors structure considerably complex offer time quite useful flexibility 
distinguish input output networks state space networks difference distinct non linear case 
past outputs network recurrent regressors may feed back interior point network input layer recurrent regressor 
experience state space networks quite favorable rivals matthews 
input layer hidden layers output layer feedforward network hidden layers 
example recurrent network 
gamma delays signal time sample 
formidable task finding black box non linear model description reduced subproblems 
select regressors 

select scalar mother basis function 
expansion mother function regressor space radial ridge type possibly specific multidimensional function 

determine number basis functions number hidden layers 

determine values dilation location parameters fi fl 
determine coordinate parameters ff 
remainder article deal steps 
shall discuss user aspects issues section 
combined effects choices affect approximating power model structure 
companion specifically devoted question 
issues turn steps estimation questions 
basically possibilities dilation location parameters step ffl fi fl continuous variables estimate time ff parameters 
ffl treat fi fl separately example offering predetermined values wavelet approach 
estimation coordinates ff linear regression problem layer networks 
shall deal approaches sections respectively 
section shall review general aspects model estimation 
model estimation model properties different techniques developed estimating models 
discuss methods detail sections shall point basic general features affect model properties 
turn important implications choice basis functions actual estimation process 
models model estimation consider general black box model ff fi parameters fl previously included fi brevity discussion 
chosen basis functions main goal model estimation choose parameters model fit 
assume finite set measured regressor output pairs ng refer estimation data set model parameter estimation rely 
note part parameters ff fi need estimated data depending choice basis functions estimation method 
denote abuse notation estimated part parameters vector 
non estimated parameters subsumed basis functions note dimension proportional number basis functions 
actual number estimated parameters dim denoted leading guideline estimating minimize error output model measured output min vn ky gamma actual method may perform minimization explicitly detailed section constructive methods discussed section 
model quality suppose actual data described unknown true model white noise variance estimate denoted want close 
measures model quality measure quality model 
course possible measures suitable different applications 
shall focus allows important analytical results 
measure fit model true system gamma gamma recall variance noise 
expression regressors assumed stationary process 
practical purposes quite general conditions interpreted sample mean lim ky gamma conditions imposed indicated limit exists 
important realize measure depends properties regressors 
model depends regressor sequence applied 
follows shall assume regressors measure properties distribution important restriction 
model structure parameterized dimension define best model chosen quality measure arg min showed explicitly dependence note depend properties 
measure quality model shall expectation respect model measure describes model expected fit true system applied new data set properties distribution regressors 
notation stress measure regressor properties model structure family depends model size basic facts bias variance shall quote quite general results model quality chapter ljung 
entirely independent model structure valid quite general conditions 
assume estimate obtained minimization 
assume model quite sense model residuals white noise 
model quality criterion defined expressed gamma noise gamma bias gamma variance indicated approximately decomposed parts due bias due variance estimation 
examined 
bias tends infinity involve bias part 
estimate converge best possible approximation true system model structure model size measured prediction performance regressor properties estimation data set 
variance estimated parameter vector certain covariance matrix describes deviation 
matrix direct interest parameters physical significance 
translate variation resulting variation prediction performance 
gives gamma ee variance true prediction errors defined 
approximate equality asymptotic expression case scalar 
multivariate case quadratic norm taken inverse covariance matrix 
factor omitted subsumed norm 
combining gives gamma useful interpretation displays expected loss model applied new data set 
important realize expected value minimized loss function model performance applied estimation data quite different 
vn defined gamma basic consequences spurious parameters model structure family non increasing function potential approximation degree increases number basis functions 
approximation capabilities different structures respect commented shortly section 
model estimated direct penalty parameters manifested variance contribution 
added parameter increased useful decreases 
long decrease addition parameter harmful model quality parameter included 
call parameter spurious 
term overfit describe happens spurious parameters employed 
model structure flexibility having bias small parameter dimension matter having efficient function basis small bias achieved basis functions 
great deal attention paid quality basis function terms function approximation regardless statistical issues 
black box model structures reviewed earlier flexible identify reasonable systems practice 
concerns nonlinear mapping regression vector output contains extensive discussions 
just mention examples 
known orthonormal wavelets form orthonormal basis mallat daubechies 
authors shown hidden layer sigmoid network approximate continuous functions arbitrary accuracy provided number basis functions net sufficiently large error bounds known see 
cybenko barron 
similar results obtained hidden layer networks similar techniques 
parameters offered parameters natural way approach problem minimizing respect try sequence models increasing estimate testing model validation data set modifying obtained loss estimation data view 
essence akaike criteria aic fpe 
simple model structures natural ordering parameters 
true example black box models single input single output dynamical systems model order serves ordering entity 
non linear black box models discussion case 
easy carry mentioned program testing astronomical amount cases 
leads idea offering model structure lot parameters try decide important non spurious ones 
number correspond number parameters 
subsection shall review possibilities achieve feature 
regularization pull origin common useful technique distinguish important parameters add penalty term criterion wn vn ffik ffi small number 
intuitively idea parameter influence term kept close zero second term 
parameter important model fit affected second term 
suppose minimize shown see 
sjoberg ljung moody hold important change number reduced ffi oe oe ffi oe eigenvalues singular values second derivative matrix hessian criterion 
interpret 
redundant parameter lead zero eigenvalue hessian 
small eigenvalue interpreted corresponding parameter combination essential spurious parameter 
regularization parameter ffi threshold spurious parameters 
eigenvalues oe widely spread see neural network case ffi eigenvalues larger ffi think efficient number parameters parameterization 
regularization decreases variance typically increases bias contribution total error 
parameter ffi acts knob affects efficient number parameters 
plays similar role model size ffl large ffi small model structure small variance large bias ffl small ffi large model structure large variance small bias means offer large number parameters fit ffi tune actual number parameters 
tuning done checking model prediction performance applying validation data set 
added regularization term ffik changed wn vn ffik gamma changing beneficial effects variance error 
penalty term corresponds prior gaussian distribution parameters viz mean covariance matrix ffii 
mackay bayesian approach introduced parameters may belong different gaussian distributions 
means spurious parameters excluded fit associating large prior time important parameters connected small prior receive small bias 
additional gaussian distributions describing parameters estimated parameters 
described mackay 
regularization include prior knowledge black box model 
penalizing size parameters add complexity term penalizes distance nominal model 
suykens example approach 
omitting basis functions alternative way find important parameters select regressors carefully guided data 
classical topic statistical regression shall review techniques section 
variant shrinking 
means components certain noise level set zero pulled zero soft threshold 
relationship regularization obvious 
reduces variances significantly changing bias 
difficulty know estimate noise level 
discussed subsection extensively case wavelets spectacular results obtained 
equivalent shrinking connection neural nets called pruning attracted interest lately 
see reed overview 
pruning difference shrinking dilation parameters considered possibly deleted 
estimation algorithms optimization methods section review methods parameter estimation number chosen basis functions deal issues estimate unknown parameters model 
components unknown basic approach minimize vn defined respect parameters 
short review algorithms minimizing vn topics connected minimization discussed 
methods minimization criterion scalar valued criterion parameter estimate defined minimizing argument arg estimate unknown function general non quadratic norm vn gamma estimate maximum likelihood estimate specific noise assumption depends choice norm 
quadratic norm corresponds assumption white gaussian noise 
entropy interpretation probabilities estimated classification problems common choose criterion relative entropy 
see cover thomas 
gives maximum likelihood estimate probability baum 
relative entropy defined entropy delta log estimated true probability belonging class entropy non negative zero 
removing parameter independent terms gives gammap delta log expectation value gamma log 
expectation value replaced sample mean obtain criterion vn gamma log probabilities considered time class problem criterion sum terms 
nonlinear optimization methods general minimum vn computed analytically minimization done numerical search procedure 
called nonlinear optimization classical treatment problem minimize sum squares dennis schnabel 
survey methods nn application kung van der 
generally speaking numerical minimization criteria fit identification purposes established topic treated general model structures ljung ljung glad 
general consensus damped gauss newton algorithm regularization features ill conditioned hessians defined shortly line manner application demands line recursive algorithms 
surprising amount applications neural network area gradient search line fashion 
contributed popular opinion neural networks require large amounts time training parameter estimation 
basic search algorithm discussion follows quadratic norm choices minor modifications done 
efficient search routines iterative local search downhill direction current point 
iterative scheme kind gamma gamma parameter estimate iteration number search scheme entities ffl step size ffl estimate gradient ffl matrix modifies search direction useful distinguish different minimization situations line batch update gamma available data record 
ii line recursive update data sample typically done gradient estimate data just sample 
shall concentrate line case 
general aspects recursive techniques refer sjoberg 
search directions gradient newton basis local search gradient gamma gamma theta gamma vector dim 
assume scalar 
known gradient search minimum inefficient especially ill conditioned problems close minimum 
optimal newton search direction gamma gamma true newton direction require second derivative computed 
far minimum need positive semidefinite 
alternative search directions common practice gradient direction 
simply take gauss newton direction 
levenberg marquardt direction 
ffii defined ffi may step size 
large ffi gives small step gradient direction small zero ffi gives gauss newton step 
conjugate gradient direction 
construct newton direction sequence gradient estimates 
loosely think constructed difference approximation gradients 
direction constructed directly explicitly forming inverting generally considered dennis schnabel gauss newton search direction preferred 
ill conditioned problems levenberg marquardt modification recommended 
ideal step size underlying criterion really quadratic 
typically done values tested new parameter value gives lower value criterion 
referred damped gauss newton method 
results conjugate gradient methods reported nn applications van der 
methods approximation true hessian referred quasi newton methods 
equation describes parameter update done basic numerical method find minimum 
straight forward approach estimate parameter iteration 
exist stage multi stage algorithms parameters updated iteration 
considering subset parameters computational burden iteration lower 
usually compensated larger number iterations 
advantage approach depends nature specific problem considered 
example parameters connected nonoverlapping basis functions updated independent 
example multi stage methods breiman algorithm parameter estimation hinging hyperplanes model breiman 
breiman suggests scheme parameters connection hinge function updated iteration 
obvious algorithm fall general description covered shown algorithm equivalent multi stage newton algorithm applied quadratic criterion 
see sjoberg 
back propagation calculation gradient model structure dependent quantity general scheme gradient model structure 
connection neural networks celebrated back propagation error algorithm bp compute gradient 
backpropagation described contexts see werbos rumelhart 
hidden layer sigmoid neural network straightforward compute gradient omitting subscript dff ffg fi fl fi fl dfl ffg fi fl ffg fi fl dfi ffg fi fl ffg fi fl ff fl scalars fi row vector 
bp algorithm case means factor ffg fi fl derivative respect fl re calculation derivative respect fi 
back propagation algorithm general limited sigmoid neural network models 
applies network models described chain rule differentiation applied expression smart re intermediate results needed places algorithm 
ridge construction models fi parameter vector neural nets complicated thing algorithm keep track indexes 
fi parameter matrix wavelet model calculation somewhat complicated basic procedure remains 
shifting multi layer network models possibilities re intermediate result increase importance bp algorithm 
described illustrative way see 
recurrent models calculation gradient complicated 
gradient time instant depend regressor gradient previous time instant gamma 
see discussion topic 
additional problem calculate gradient change essential minimization algorithm 
neural network literature referred back propagation time 
implicit regularization stopped iterations overtraining stated estimation performed minimizing criterion 
iterations basic scheme run improvement fit nn literature entire search algorithm called back propagation 
consistent keep notation just algorithm calculate gradient 
pattern pattern illustrating backpropagation 
shows graphs 
graph left encodes formula way expressions encoded graphs syntactic analyzers computer science instance graph encodes ff fi denotes multiplication general layer network formula 
nodes graph interpreted operators evaluation expression encoded subtree located node 
triangle pattern indicates component result evaluating function encoded graph 
immediately encode multi layer network 
right hand side expanded pattern showing case layers 
shown thick paths linking root particular parameter say fi semantic thickening node thick path operator replaced partial derivative respect node just thick path regard node evaluation expression encoded subtree located 
instance replaced fi chain rule differentiation turns graph right hand side encodes partial derivative fi graphical representation explains intermediate calculations shared different partial derivatives nice feature backpropagating gradient 
presentation due cybenko 
local minimum vn reached 
noted early neural network literature model evaluated validation data improved number iterations started deteriorate increasing number iterations despite fact value vn estimation data course continued improve 
phenomenon termed overtraining 
effect explained follows words see wahba sjoberg ljung formal treatments suppose iterative search started iterations pull parameters minimizing values 
parameters substantial influence fit feel stronger pull adjusted quicker important parameters 
iterations aborted vn minimized important parameters hang initial value pretty result minimized regularized criterion 
increasing number iterations corresponds decreasing regularization parameter ffi 
precisely link follows quadratic approximations applicable gamma gamma ffi ffii gamma iteration number increases corresponds regularization parameter decreases zero log ffi gammai increasing number iterations equivalent larger model structure parameters 
concept overtraining consequently just reflection known concept overfit defined section 
know iterations 
value criterion vn course continue decrease certain point corresponding regularization parameter small increased variance starts dominate decreased bias 
visible model tested fresh set validation data generalization data 
evaluate criterion function fresh data set plot fit function iteration number 
typical plot shown section 
method terminating iterations model fit evaluated validation data starts increase called stopped search 
regularization implemented stopped search called implicit regularization contrast explicit regularization obtained minimizing modified criterion 
local minima fundamental problem minimization tasks vn may local non global minima local search algorithms may get caught 
easy solution problem 
usually effort spend time come initial value start iterations 
realistic option nonlinear black box problems little prior knowledge available 
best thing cases usually choose random way support basis functions covers interesting domain input space 
model structures constructive estimation methods give options described section 
various global search strategies left random search random restarts simulated annealing genetic algorithms 
estimation algorithms constructive methods recall general model structure total parameter vector composed different parts coordinate parameters ff hand dilation location parameters fi fl 
fixed parameters fi fl minimizing collects ff case linear squares problem 
problems nice efficient algorithms exist search performed problem local minima 
assumes values fi fl parameterized basis functions chosen efficient manner 
approach feasible particular basis functions come natural choice values fi fl 
instance wavelets suitable applying approach 
fact situations choice fi fl partially influenced observed data 
algorithms considered section data selecting values parameters practically finite set 
finite set values depends chosen basis functions possibly prior knowledge application 
refer approaches model estimation constructive methods 
wavelets play important role constructive methods section mainly concentrated wavelet models 
orthonormal wavelet decomposition shrinking algorithms wavelets interesting class functions due special properties 
subsection introduce basic concepts orthonormal wavelet basis describe wavelet shrinking techniques wavelets powerful nonlinear estimators 
orthonormal bases wavelets multiresolution analysis introduced yves meyer stephane mallat developed ingrid daubechies provides orthonormal bases form gamma zg element basis translated dilated version single mother wavelet 
time consider scalar function inner product hf performs zooming gammaj width interval centered point gammaj large corresponds checking function fine scales 
implies local singularity function affect small part coefficients wavelet basis 
main difference fourier basis local singularity affect fourier representation 
basis expanded ff jk jk ff jk hf jk decomposed doubly infinite orthogonal sum spanf zg 
expansion scale index ranges infinitely coarse infinitely fine translation index 
useful practice consider double sided expansion sided expansion scales collapsed single basic low resolution subspace set achieved associating mother wavelet called father wavelet termed scale function oe translated versions suffice span scales 
expansion shall form ff oe zero scale ff jk jk finer scales wavelet literature wavelets usually considered functions noted 
consider wavelets particular basis functions denote wavelets functions regression vector 
wavelet coefficient hf usually denoted fi jk ff jk order keep consistent notations ff fi 
note denotes inner product gamma oe oe gamma ff jk hf ff hf oe orthonormal expansion 
refer reader formal matter discussion smoothness properties constructed wavelets 
dilation parameter translation parameter wavelet correspond fi fl parameters generic mother basis function introduced 
strong point orthonormal wavelet expansions coarse scale coefficients recursively computed fine scale ones vice versa 
explain 
jo clearly jo family foe spans foe gamma spans finer scale phi oe oe gamma oe gamma suitable 
equations imply ff jk hf oe jk ff jk hf jk obey fine coarse recursions ff jk gamma ff ff jk gamma ff recursions compute recursively fine scales coarse scales orthonormal wavelet decomposition ff initial condition index denotes finest scale recursions 
assume addition scale function oe selected computation inner product hf oe efficiently performed 
formulas build highly efficient procedure computing wavelet decomposition see efficient computation inner product hf oe jk addition equations inverted yield coarse fine recursion ff jk gamma ff gamma gamma ff gamma definition space ff oe gamma ff oe ff jk jk formulas allow switch representation representation 
generally compact smooth ff jk negligible 
move discussing multidimensional case 
exist main types constructions wavelet basis dilation factor daubechies 
guess simply consists tensor product functions generated dimensional bases psi theta theta construction drawback mixing different resolution levels alternatively mixing desired proceed follows 
introduce scale function phi oe theta theta oe gamma mother wavelets psi gamma obtained substituting oe 
family orthonormal basis ae phi psi jk psi gamma jk oe phi jk jd phi gamma gamma psi jk jd psi gamma gamma note formula shows constructing storing orthonormal wavelet bases prohibitive cost large dimension main limitation efficient techniques rely orthonormal wavelet bases generalizations 
wavelet shrinking algorithm assume sample estimation data available ng unknown true model sequences random variables ee ee assume time uniformly distributed recall multidimensional wavelet expansion ff phi gamma ff jk psi jk ff phi ff jk psi jk construct estimate idea consists law large numbers replacing expansion coefficients ff ff jk empirical estimates ff phi ff jk psi jk note assumption uniformly distributed point 
brute force estimate impractical points gently uniformly distributed real life psi jk just sample support empirical averages ff jk defined remaining ff jk significantly different noise level probably discarded 
issues addressed procedure see mathematical justification 
select relevant scales 
obviously order compute empirical coefficient ff jk need observations hit support psi jk 
statistical laws loglog type guarantee generically hold scales fine specifically max ln jmax ln brute force set ff jk max note assumption uniformly distributed point 

collapse data synthetic regularly sampled record 
assuming smooth density shall approximate constant bin delta gammaj max gammaj max theta theta gammaj max gammaj max length gammad jmax statistically know bin data coarsely collapse data bin single representative data point simple averages delta delta synthetic output associated th bin delta set ff jmax jmax identify scaling factor father wavelet coefficients unknown function estimated taken finest scale max 
fine coarse recursions get wavelet expansion 
point constructed synthetic input output pairs input considered bin output associated ff jmax estimate 
getting full wavelet expansion performed applying synthetic data recursion formulae 
multidimensional version filters compute ff jk ff jk max gamma gamma ff jk gamma ff ff jk gamma ff indicator function equals true wise 

shrink junk noise level 
point estimate full wavelet expansion unknown function scale max due local nature wavelets expansion coefficients significantly different zero wavelets having significant variations supports 
coefficients expansion basically contain noise relevant information 
wavelet basis orthonormal shown significance wavelet coefficients tested separately coefficient comparing suitably selected thresholds cf 

shrink estimates ff jk ff jk ff jk fj ff jk properly selected threshold 

coarse fine recursions reconstruct finest scale 
ready inverse filter obtain ff jmax ff gamma ff gamma gamma ff gamma set gammaj max delta max ff steps constitute algorithm 
note computational burden concentrated fine coarse coarse fine recursions packages available see 

altogether extremely efficient algorithm small dimensions typically 
techniques basis functions selection orthonormal wavelet bases really nice restricted class basis functions related fast estimation algorithms efficient shrinking algorithms 
practically applicable problems small number regressors reasonably distributed data 
subsection introduce techniques basis function selection applicable restricted class basis functions including non orthogonal wavelets 
techniques handle applications moderately large number regressors sparse data 
techniques shall able observed data select values fi fl typically dilation location parameters finite set equivalently select basis functions fi fl finite set basis functions fg fi fl fi fl bg shall refer basis function set 
discuss construct basis function set introducing techniques basis function selection 
construction basis function set simplicity consider case basis functions parameterized versions single mother basis function fi fl fi fl 
construction depends form typically fi fl correspond dilation translation parameters respectively model estimated finite domain regression vector resolution level 
suggest choice discuss typical examples 
hidden layer sigmoid networks oe fi fl 
parameters fi fl chosen non flat part oe fi fl stays inside domain interest values fi fl distributed 
clear idea distribution 
reason basis function selection techniques suited sigmoid networks 
radial basis function rbf networks fi gamma fl radial function 
possibilities choosing values fl centers rbfs take values fl uniform lattice regression vector space values fl equal observed values regression vector 
difficult choose values fi adaptive clustering vector quantization techniques purpose poggio girosi 
wavelet networks fi gamma fl wavelet function 
choice fi fl dilation translation parameters suggested wavelet transform 
typically values fi fl form regular lattice wavelet bases frames 
chosen corresponding basis function set 
construction may practical limitations dimension regression vector large typically size increases exponentially applications large regressor dimension estimation data sparse space regression vector 
particularity data taken account construction instance basis functions generated local mother basis function delta basis functions constructed regular way contain contain estimation data effective support basis functions immediately rejected 
limit size basis function selection algorithms assume basis function set chosen 
problem set estimation data defined select basis functions classical problem regression analysis draper smith 
value selecting optimal basis functions principle performed exhaustive search consist examining possible combinations basis functions number possible combinations usually large 
term effective support support deal case non compactly supported basis functions 
special constructions result orthogonal basis functions 
situations basis function selection problem solved efficient way 
wavelet shrinking algorithm described subsection spectacular example 
basis functions strictly orthogonal close orthogonal applying shrinking technique give reasonable results 
near tight wavelet frames daubechies typical examples orthogonal basis functions 
general case basis functions orthogonal order overcome combinatorial complexity exhaustive search different heuristics reviewed details algorithms zhang 
residual selection rbs 
idea method select stage basis function best fits estimation data repeatedly select basis function remainder best fits residual previous fitting 
literature classical regression analysis method referred stagewise regression procedure 
see example draper smith 
matching pursuit algorithm mallat zhang adaptive signal representation qian chen 
stepwise selection orthogonalization sso 
rbs method explicitly consider non orthogonality basis functions idea alternative method select stage basis function best fits estimation data repeatedly select basis function remainder best fits estimation data combining previously selected basis functions 
computational efficiency selected basis functions orthogonalized earlier selected ones 
radial basis function rbf networks nonlinear modeling problems chen chen 
backward elimination 
contrast previous methods backward elimination method starts building model basis functions eliminates basis function stage trying deteriorate model fit little possible 
recursive scheme elimination stages reduce computational cost 
method computationally expensive large 
continuous wavelet transform combination basis function selection applying mentioned techniques basis function selection non orthogonal wavelets yields interesting family models called wavelet networks zhang benveniste zhang 
computationally efficient wavelet shrinking algorithms small dimensional case allow handle problems moderately large dimensions 
software package wavelet networks matlab language available anonymous ftp zhang 
need recall basic concepts continuous wavelet transform point 
consider radial wavelets 
continuous wavelet transform inverse transform function respectively equations 
transforms functions oe radial depending delta denotes euclidean norm known synthesis analysis wavelets 
specifically oe radial functions satisfying 
gamma oe da 
oe 
denote fourier transform oe respectively 
function formulae define isometry subspace theta daubechies gamma oe gamma gamma gamma da dt respectively dilation translation parameters 
discussed detail reconstruction formula immediately explains dilated translated versions synthesis wavelet candidate basis functions 
rewrite formula gamma gamma da dt gamma sign gamma ju da dt gamma sign da dt renormalized constant factor function ca gamma ju considered probability density function 
draw independent random samples density 
build gamma sign law large numbers converges true function 
justifies dilated translated versions synthesis wavelet build basis function set implementing procedure require estimation density function computationally expensive 
results reported daubechies zhang called wavelet frames justify wavelet families form ff gamma kfi practice wavelet basis function sets introduced constructed wavelet frames 
applying algorithms basis function selection yields wavelet networks zhang 
encoding prior information syntactic fuzzy models claimed section fuzzy modeling seen particular choice basis functions 
shall point clear section 
addition discuss detail add provided fuzzy modeling 
introduce fuzzy models typically fuzzy control lee 
presentations possible see instance zadeh takagi sugeno sugeno 
presentation give slightly simple consistent 
fuzzy logic fuzzy sets consider scalar input variables generically written 
fuzzy set defined linguistic label membership function 

membership function mathematical meaning fuzzy set 
actual value statement value equal statements premises called fuzzy rules 
typical form statements large 
careful statement convey information membership function fuzzy set large specified 
frequently form membership functions just symmetric triangle parameterized width dilation location illustrated 
small medium large large fuzzy sets 
picture shows fuzzy membership functions corresponding small corresponding membership function fact form parameter specifying exact location width parameterized family basis functions 
fuzzy operators fuzzy sets combined operators order predicate logic 
allows describe combination membership functions syntax 
instance fuzzy set involving vector 
keyword combinator fuzzy sets defined formally terms combination membership functions 
similarly operators accordingly defined 
choices proposed various authors dubois prade widely ones min max uv gamma uv max gamma min corresponding definitions written line gamma try generalize boolean implication operator continuous valued logic usual logic implication point deviate usual presentation fuzzy literature implication encoded modus ponens mechanism modified accordingly 
preferred presentation fully consistent accordance usual predicate calculus 
implies written macro expands sequel shall encode product uv corresponding codings 
implication expanded follows 
denote membership function associated fuzzy set membership function 
formulas gamma gamma gamma gamma uv obtain expression implication reichenbach implication literature gamma gamma gamma implication models certainty rule form certain see dubois prade 
fuzzy reasoning modelling fuzzy maps fuzzy rules fuzzy rules statements form note complex premises 
modus ponens mechanism logic maps predicates predicates 
counterpart fuzzy logic allows approximate application drawn fact exactly agree part rule 
written rule fact fact seen input output fuzzy rule map 
modus ponens mechanism combines membership functions yields membership function 
typical example temperature high corresponds temperature 
general case mathematical translation fuzzy modus ponens rule dubois prade defined max elimination performed maximization 
note general facts ordinary numbers fuzzy sets 
inference crisp inputs discuss particular case fuzzy reasoning crisp input fact statement directly related general nonlinear black box model formulation 
fact statement crisp membership function ordinary value 
case modus ponens mechanism reduces gamma gamma note case crisp input fact general fuzzy set 
fuzzy rule bases models mean modeling identification context 
shall describe sets rules state behavior system 
goal show connection conventional models see fuzzy models parameterized 
fuzzy rule bases fuzzy rule basis collection fuzzy rules form say isa fuzzy sets doubly indexed index input coordinate index rule 
denote membership functions respectively 
simple example dc motor consider electric motor input voltage output angular velocity explain angular velocity time depends applied voltage velocity previous time sample 
regressors gamma gamma 
device rule base kind choose low voltage high voltage 
choose slow speed fast speed 
membership function low voltage taken ae ae gamma gamma gamma membership function high voltage taken gamma membership functions slow fast speed chosen analogously breaking points rad sec 
statements outputs chosen triangles vertices respectively located rad sec 
obtain rule base low slow low low fast medium high slow medium high fast high 
combining rules rule base sight quite different models discussed sections article 
see connections shall give mathematical translation 
combining fuzzy rules fuzzy rule basis interpreted fuzzy rule basis means fuzzy sets defined 
expressing combinator product membership functions get gamma gamma gamma gamma approximation gamma gamma valid small large 
assume membership functions rule basis subject identity true membership functions defined input domain form strong fuzzy partition holds rule basis complete covers cases terms fuzzy sets defined input domains easy verify dc motor example obeys requirement 
case defuzzification point setting formula defines function mapping points fuzzy sets 
get function usual setting 
perform defuzzification called height method see lee dubois prade 
property get ordinary function delta choice implication implication encoded intermediate results aggregated 
point reaches maximum value definition weight functions obvious 
property hold defuzzification formula modified accordingly wang rule basis may directly built crisp ordinary values 
case defuzzification needed 
back general black box formulation back predictor model form discussed section mapping regression vector predicted output 
rules rule base need tuning may introduce parameters tuned 
parameters numbers 
example unknown replaced adjustable parameter ff parameters introduced membership functions 
usually fuzzy set membership functions parameterized functions form fi fl fi gamma fl function values fi dilation factor fl translation factor pair fi fl encodes fuzzy set piecewise linear function outside interval 
parameters introduced way model takes form ff fi fl basis functions obtained parameterized membership functions fi fl fi ji gamma fl ji back basic situation difference basis functions created dilation translation basic function complex way 
estimation free parameters follows general theory 
fuzzy partition fixed adjustable fi fl fixed get particular case kernel estimate 
identified fuzzy models referred neuro fuzzy models literature back propagation procedure training neural networks 
proved fuzzy models universal approximators wang surprising 
summarize fuzzy models described fuzzy rule bases plus additional parameters vague statements large small precise terms membership functions 
fuzzy rule basis exhibits structure model plus coarse features related location elementary functions decomposition 
fuzzy models just particular instances general model structure advantage providing fuzzy rules way describe possibly available output input measured values oil pressure top valve position bottom 
prior knowledge 
experiments reported section neuro fuzzy modelling sense 
extension classical fuzzy modelling syntax proposed encompass multiresolution model structures wavelet decompositions networks 
experiments section application examples nonlinear black box modeling order give reader practical insights 
cover dynamic system modeling static system modeling fuzzy system modeling 
modeling hydraulic robot actuator section shall study identification hydraulic actuator 
shall consider linear models nonlinear black box ones neural networks wavelet networks 
data 
position robot arm controlled hydraulic actuator 
oil pressure actuator controlled size valve opening oil flows actuator 
position robot arm function oil pressure 
thorough description particular hydraulic system 
shows measured values valve size oil pressure input output signals respectively 
seen oil pressure settling period step change valve size 
oscillations caused mechanical resonances robot arm 
linear model 
principle try simple things gives arx model predicts output past outputs past inputs output fit simulation linear arx model validation data 
solid line simulated signal 
dashed line true oil pressure 
number iterations rms error sum squared error training model 
solid line validation data 
dashed line estimation data 
regression vector gamma gamma gamma gamma gamma output input system respectively 
result simulation obtained linear model validation data shown 
result impressive 
neural network model 
model hidden layer sigmoid neural network hidden units considered described section 
regressor linear model gives model parameters 
shown quadratic criterion develops estimation estimation validation data respectively 
validation data criterion decreases starts increase 
overtraining described section 
best model obtained minimum means parameters nonlinear model converged efficient number parameters smaller dim 
parameters give minimum nonlinear model 
model simulated validation data gives root mean square error rms considerably smaller obtained linear model 
simulation nonlinear wavelet network model validation data 
solid line simulated signal 
dashed line true oil pressure 
wavelet network model 
model wavelet network considered model hydraulic actuator similar way regressors 
wavelet function gamma gamma dim 
dilation matrices fi chosen multiples identity matrix 
apply sso procedure see subsection iteratively selects wavelets model 
akaike final prediction error criterion number wavelets chosen corresponding model parameters 
model issue iterative construction simulate output robot arm validation data 
corresponding rms validation data 
model refined iterations levenberg marquardt procedure simulation way 
result depicted rms 
observe levenberg marquardt procedure slightly improved result 
suggests iterative construction method model parameter close local minima searched levenberg marquardt procedure 
non linear structures 
non linear models considered far obtained just plugging regressor non linear structure 
try structures suggested section 
structure assumption additive noise gives model linear past 
parameter estimation easier model structure 
parameters speeds numerical search model linear regressors problem local minima 
turns structure advantageous include regressors predictor model gamma gamma gamma delta delta delta gamma modeled neural net hidden units 
gives model parameters third compared number parameter neural net model 
time problems overlearning estimation parameters 
simulating model manner models gave rms 
linear part model replaced neural net obtain model consisting neural nets 
gives model kept linear past quite model regressors fed large network 
units neural net obtain model parameters 
rms error simulation validation data simulation depicted 
output fit simulation nonlinear neural network model validation data 
solid line simulated signal 
dashed line true oil pressure 
modeling gas turbine gas power motors typically electrical power generators 
usually gas turbine system mainly composed compressor combustion chambers expansion turbine illustrated 
purposes joint study european gas turbine sa recherche develop monitoring diagnostics system joint system chambers expansion 
purpose semi physical model developed predicts temperature profile exhaust gas 
due phase shift gas turbine semi physical model strongly nonlinear zhang zhang 
installed exhaust turbine measure output temperature profile 
compression rate compressor rotation velocity turbine measured 
suggested semi physical model chosen average measurements regressors deviations average gamma outputs black box model 
experimented approach data taken gas turbine european gas turbine sa 
training data collected hours 
re sampled data kept measurement points model estimation 
sake brevity theta gamma delta theta gamma delta theta gamma delta theta gamma delta compressor combustion chambers turbine exhaust air gas turbine system models rbs net sso net net semi physical linear init 
rms opt 
rms init 
flops theta theta theta opt 
flops theta theta theta theta theta init 
time sec 
opt 
time sec 
table performance evaluation turbine models 
rbs net sso net net mean wavelet network models initialized rbs sso procedures respectively 
rms means square root mean square error 
rmss evaluated validation data network models init 
rms corresponds initialized model opt 
rms corresponds model optimized iterations levenberg marquardt procedure 
flops matlab measure computational burden 
computation time programs matlab language executed sun sparc workstation 
shall show results concerning 
obtained models tested set measured data refer validation data tested semi physical model linear regression model wavelet network models 
wavelet network model chosen radial wavelet function gamma gamma dim 
number wavelets networks corresponding model parameters 
initialize wavelet networks constructive procedures rbs sso described subsection optimize procedure 
results summarized table 
outputs corresponding validation data predicted linear regression model wavelet network model plotted 
values square root mean square errors rms table linear regression model bad compared models plots show wavelet model significantly improve prediction accuracy 
sigmoid neural networks tested example results similar obtained wavelet networks 
nonlinear black box models perform better semi physical model terms output prediction 
price higher computational complexity 
hand semi physical model accurate allows perform physical diagnosis system faults mathis contrast black box models give possibility implement finer global alarm monitoring system see mathis model parameters provide physical information fault diagnosis 
modeling variations medical example illustrating fuzzy models 
describing problem 
variations depend factors easily quantifiable may vary time 
food diet physical activity stress emotions proximity meal effects doctors know qualitatively assess 
healthy person regulation ensured insulin 
case organic deficiency persons insulin artificially injected 
deciding amount injection difficult morphology physical activity time comparison validation data predictions linear regression model left wavelet network initialized procedure right 
solid lines represent true measurements dashed lines represent outputs models 
meal richness meal glucose concentration results previous day taken account 
injected insulin acts delay efficiency reduces glucose concentration gets higher 
lastly followed 
optimum control better anticipate glucose level rises occurs insulin healthy persons 
summarize deal nonlinear unstable system time delay 
doctors devised empirical rules allowing persons approximatively compute insulin level injection 
persons pump insulin injection rate parts basic flow rate denoted providing daily insulin needs variable part denoted flash injection assimilate meal 
despite doctor experience difficult manually obtain constant level part control take account input variables far human control capability 
motivated propose predictive model basis automatic injection control 
model uses basis empirical rules doctors takes account qualitative nature available data 
proposal self supervision note books daily support control context treatment insulin dependent patients pump operation 
day writes note book time actual time importance quality meal activity insulin injection 
experimental results case study reported 
variables interest qualitative labels 
knowledge expressed form rule thumb advice 
knowledge build hour ahead predictive model variations 
predictive model subsequently control system 
restricted model inputs current instant omitted simplicity described table 
output predicted variation time hours dg pb pm ps ns nm nb means positive negative small big shows membership functions parameters determined learning optimal value depends patient 
membership functions represented simple order fuzzy partition item symbol fuzzy values gl low low normal high high vl vh basis insulin injection rate ba low normal high flash insulin injection rate bo low normal high elapsed time previous meal dr far near just far diet nr fiber normal expected activity ac low normal high table fuzzy input variables splines free knots 
method follows steps 
start initial guess model available qualitative prior knowledge 
tune model particular patient consideration performing learning optimization available data 
expressing prior knowledge 
combining possible qualitative values different inputs yields different cases corresponding amount candidate fuzzy rules 
fact rules considered prior model reflecting actual domain input variables meaningful knowledge exists 
example rules gl vl nr dg pb gl ba dg ns shows predicted ffi time ffi hours learning prior model 
solid line shows actual dashed line predicted 
doctor rules quite efficient predicting effect insulin injections 
spikes occur prediction error 
prediction error mean gamma standard deviation oe 
tuning model patient 
data patient note book divided data file parts learning validation testing 
shows prior model hour ahead prediction dashed line vs actual solid line predicted time learning subsequent learning parameters data 
prediction error mean gamma standard deviation oe 
improvement seen note improvement patient dependent 
errors time steps due changes marked note book usually lead inject insulin expected 
model learning hour ahead prediction dashed line vs actual solid line comments example 
drawn example ffl fuzzy rules turned convenient way express prior knowledge doctors part prior knowledge mainly qualitative 
important notice fuzzy rule basis far equivalent exhaustive table describing input output map percent table described rules 
restriction useful prior information range validity modelling 
ffl subsequent tuning prior model performed preserving structure model fuzzy rules modified parameters hidden splines adjusted 
possible prior model initial guess allow rules introduced learning corresponding experiments progress 
ffl advantage describing model fuzzy rules possibility decompile model learning form fuzzy rules return user doctor patient 
returning mathematical model little average user having training mathematics 
summary recommendations system identification fully formalized automated 
user blend experience common sense established theory methodology 
section take position user relevant software support available 
assume collected input output data system shall estimate model 
things consider successful result 
general concerns look data 
obvious step 
revealing 
nonlinear effects detected ocular inspection responses similar different levels different directions 
time constants seen try simple things engineering principle try simple things 
identification context simple may mean size computational complexity models 
practice certainly means try linear models see solve problem get insight shortcomings 
theoretical estimation point view simplicity primarily refers number estimated parameters 
searching simpler complex models valid typically trade bias variance achieved 
look physics physical insights may suggest linearly nonlinearly transform raw measurements new regressors 
try semi physical regressors cf section linear black box structures 
gives unsatisfactory results physical insight completely lacking time move nonlinear black box structures described 
models sense semi physical regressors 
bias variance trade bias variance trade tells excessively increase number estimated parameters number basis functions model 
ultimate improvement model quality obtained suitably choosing basis functions require physical knowledge lead physical models 
validation estimation data best way evaluating identified model test fresh data model estimation 
pointed validation generalization data determining model complexity bias variance trade terms model structure complexity size regularization parameter iterations 
checking potential model validation data clear pragmatic appeal reproduce previously unseen data satisfactory manner 
notion efficient number parameters variance contribution model output error principally proportional number parameters model structure estimated minimization 
means parameter important model fit contribute important parameter variance error 
intuitive explanation un important parameter estimated inaccurately influence small fit large errors damaging 
tempting estimation schemes reward important parameters 
number possibilities 
regularization reviewed subsection classical method 
variant regularization iterations minimization true minimum described section 
third way focusing important parameters estimate discard small possibly reestimate values remaining ones 
called shrinking basis function selection section 
remaining number parameters essentially determine model variance error 
structural issues consider regressor selection rational question ask am prepared regressors distribute possible regressor choices listed section 
easy quantitative answer question may point general aspects ffl choice consider consists trying static models regressor 
ffl including gamma requires dynamic response time covered past inputs 
maximum response time change input upsilon sampling time number regressors upsilon large number 
hand models finite number past inputs unstable simulation advantage 
variant approach form regressors laguerre filtering 
retains advantages fir approach time making possible fewer regressors 
discussed context nonlinear black boxes 
ffl adding gamma list regressors possible cover slow responses fewer regressors 
quite important nonlinear models trying achieve objective delayed inputs prohibitive linear models 
disadvantage past outputs bring past disturbances model 
model additional task sort noise properties 
model past outputs may unstable simulation input 
caused fact past measured outputs replaced past model outputs 
ffl bringing past predicted simulated outputs gamma kj past values nodes network may interpreted state variables may quite useful 
typically increases model flexibility leads non trivial difficulties related recurrent nature resulting network 
see subsection 
problems handled may lead instability network nonlinear model problem easy monitor 
regressors fed back depend 
order minimization iterations true gradient direction dependence taken account straightforward 
dependence neglected convergence local minima criterion function guaranteed 
balance discussion probably regressors gamma gamma ones test 
choice basis functions regression vector decided question function expansion 
return choices listed section 
difficult decision collected experience substantial 
described model structures capable approximating reasonable function 
question pick suits application sense terms needed 
curse dimensionality 
dimension regression vector denoted function approximated domain 
moderate size observations necessity sparse bounded region practical interest 
example takes observations fill unit cube coarse component wise grid granularity 
consideration important choice basis functions obtained radial constructions ridge constructions 
see 
radial constructions 
view curse dimensionality local basis functions prime choice dimension regression vector small 
wavelet basis function expansion excellent choice wavelet coefficients estimated efficiently 
somewhat larger values natural try wavelet networks radial basis networks 
large values model structures local basis functions simply support model statements outside areas observations unreasonable 
multi resolution aspects useful feature wavelet models scale parameters chosen differently 
certain areas data space covered basis functions large support covered finer 
region may covered types pick fine details course trends 
useful way deal lack data certain regions 
may note curse dimensionality relate possible lack supporting data 
prior seed basis functions screened help data huge high dimensions may major obstacle 
solution proposed section scan available data pick basis functions contain data points supports 
ridge constructions 
ridge constructions ones sigmoidal neural networks hinging hyperplanes networks deal curse dimensionality extrapolation 
means functions identify certain directions space happens 
words projection directions show clear data patters projected picture 
directions chosen global ones 
approach clear connections projection pursuit friedman 
advantage higher regression vector dimensions handled extrapolation unsupported data regions 
reasonable depends course application 
experience indicates approach successful 
basis functions prior verbal information 
building basis functions fuzzy logic fuzzy rules way dealing curse dimensionality 
extrapolation unsupported data regions done prior knowledge right wrong system behavior 
regions model supported data modified information observations 
important aspect choice basis functions fuzzy sets specify domain interest areas input data expected 
quite appealing way deal partial data information 
toolbox system identification techniques black box models nonlinear dynamical systems available 
true preferred physical insight build nonlinear effects model typically done fewer parameters 
insight available linear approximative models choice turn black box structures 
topic new 
classical literature subject concentrated global basis function expansions volterra expansions 
apparently limited success 
topic really revived neural network applications 
treated possibilities black box nonlinear dynamical models common framework 
pointed similarities different approaches tried pinpoint real choices 
bottom line choice basis functions 
basis functions carry parameters adjust observed data 
parameters typically correspond scale location function support 
scale location function coordinates estimated joint minimization process separate step fix location scale 
perspective user 
details approximation theory properties function expansions 
focused choices user successful application 
mathematical investigations companion 
barron 

universal approximation bounds superpositions sigmoidal function 
ieee trans 
information theory 
baum 

supervised learning probability distributions neural networks 
andersson editor neural information processing systems pages 
amer 
inst 
physics new york 
breiman 

hinging hyperplanes regression classification function approximation 
ieee trans 
information theory 
brown harris 

adaptive modelling control 
prentice hall new york 
chen billings 

neural networks nonlinear dynamic system modelling identification 
int 
control 
chen billings 

non linear system identification neural networks 
int 
control 
chen billings luo 

orthogonal squares methods application non linear system identification 
int 
control 
chen cowan 

orthogonal squares learning algorithm radial basis function networks 
ieee trans 
neural networks 
chui 

wavelets tutorial theory applications 
academic press boston san diego 
cover thomas 

information theory 
wiley new york 
cybenko 

approximation superposition sigmoidal function 
mathematics control signals systems 
daubechies 

wavelet transform time frequency 
ieee trans 
information theory 
daubechies 

lectures wavelets 
cbms nsf regional series applied mathematics 
de boor 

practical guide splines 
applied mathematical sciences 
heidelberg springer new york berlin 
benveniste 

accuracy analysis wavelet approximations 
ieee transactions neural networks 
dennis schnabel 

numerical methods unconstrained optimization nonlinear equations 
prentice hall englewood cliffs new jersey 
devroye 

nonparametric density estimation 
john wiley sons new york 
draper smith 

applied regression analysis 
series probability mathematical statistics 
wiley 
second edition 
dubois prade 

fuzzy sets approximate reasoning part 
fuzzy sets systems 
friedman 

projection pursuit regression 
amer 
statist 
assoc 
friedman stuetzle 

projection pursuit regression 
amer 
stat 
assoc 


general class fuzzy inference systems 
proc 
ces conf prague 


modelling flexible mechanical system containing actuators 
technical report dep 
electrical engineering linkoping university linkoping sweden 
haykin 

neural networks comprehensive foundation 
macmillan college publishing third avenue ny 


partial squares regression statistical models 
scand 
statist 
huber 

projection pursuit discussion 
ann 
statist 
benveniste ljung sjoberg zhang 

nonlinear black box models system identification mathematical foundations 
automatica 
available anonymous ftp 
zhang benveniste 
may 
wavelets identification 
technical report irisa 
zhang 

multi dimensional wavelet frames 
ieee trans 
neural networks 
accepted publication january 
kung 

digital neural networks 
prentice hall englewood cliffs new jersey 
lee 

fuzzy logic control systems parts ii 
ieee trans 
systems man cybernetics 
ljung 

system identification theory user 
prentice hall englewood cliffs nj 
ljung glad 

modeling dynamic systems 
prentice hall englewood cliffs ljung 

theory practice recursive identification 
mit press cambridge massachusetts 
mackay 

bayesian methods adaptive models 
phd thesis caltech pasadena ca 
mackay 

baysian interpolation 
neural computation 
mallat 

multiresolution approximation wavelets orthonormal bases 
trans 
amer 
math 
soc 
mallat zhang 

matching pursuit time frequency dictionaries 
technical report new york university computer science department 
mathis 

surveillance de turbine 
phd thesis universit de rennes 
french 
matthews 

uniform approximation nonlinear discrete time systems neural network models 
phd thesis eth zurich switzerland 


personal communication 
meyer 

ondelettes op erateurs 
hermann 
moody 

effective number parameters analysis generalization regularization nonlinear learning systems 
moody hanson lippmann editors advances neural information processing systems 
morgan kaufmann publishers san mateo ca 
nadaraya 

estimating regression 
theory prob 
appl 
narendra parthasarathy 

identification control dynamical systems neural networks 
ieee trans 
neural networks 
personnaz 

neural networks nonlinear adaptive filtering unifying concepts new algorithms 
neural computation 
personnaz 

training recurrent neural networks 
illustration dynamical process modeling 
ieee trans 
neural networks 
poggio girosi 

networks approximation learning 
proceedings ieee 
poggio girosi 

regularization algorithms learning equivalent multilayer networks 
science 
sjoberg 

hinge finding algorithm hinging hyperplanes 
technical report report dep 
electrical engineering linkoping university linkoping sweden 
available anonymous ftp 
sjoberg 

parameterization hinging hyperplane models 
technical report report dep 
electrical engineering linkoping university linkoping sweden 
available anonymous ftp 
qian chen 

signal representation adaptive normalized gaussian functions 
signal processing 
reed 

pruning algorithms survey 
ieee trans 
neural networks 
rivals 

mod de processus par de neurones application au un 
phd thesis ecole sup erieure de physique de la ville de paris rue paris france 
rumelhart hinton williams 

learning representations backpropagating errors 
nature 
beylkin coifman daubechies mallat meyer raphael editors 
wavelets applications 
jones bartlett books mathematics boston 
cybenko 

ill conditioning neural network training problems 
siam journal scientific computing 
schumaker 

spline functions basic theory 
pure applied mathematics 
chichester wiley sons new york brisbane 
silverman 

density estimation statistics data analysis 
chapman hall london 
sjoberg ljung 

neural networks system identification 
preprint th ifac symposium system identification copenhagen volume pages 
available anonymous ftp 
sjoberg ljung 

overtraining regularization searching minimum neural networks 
preprint th ifac symposium adaptive systems control signal processing pages grenoble france 
sontag 

nonlinear regulation piecewise linear approach 
ieee trans 
automatic control 
sontag 

neural networks control 
willems editors essays control perspectives theory applications volume progress systems control theory pages 
birkhauser 
stone 

optimal global rates convergence nonparametric regression 
annals statistics 
sugeno 

fuzzy logic approach qualitative modelling 
ieee trans 
fuzzy systems 
suykens moor vandewalle 

static dynamic stabilizing neural controllers applicable transition equilibrium points 
neural networks 
takagi sugeno 

fuzzy identification systems application modelling control 
ieee trans 
syst 
man cybernetics 



public domain matlab toolbox anonymous ftp simplicity 
stanford edu pub 
van den hof 

identification basis functions statistical analysis error bounds 
editors preprint th ifac symposium system identification volume pages 
van der 

minimisation methods training feedforward neural networks 
neural networks 
wahba 

topics ill posed problems 
engl editors inverse ill posed problems 
academic press 


system identification laguerre models 
ieee trans 
automatic control 


system identification kautz models 
ieee trans 
automatic control 
wang 

fuzzy systems universal approximators 
proc 
ieee conf 
fuzzy systems san diego 
wang 

adaptive fuzzy systems control design stability analysis 
prenticehall englewood cliffs nj 
watson 

smooth regression analysis 
series 
werbos 

regression new tools prediction analysis behavioral science 
phd thesis harvard university 
wold ruhe wold dunn 

collinearity problem linear regression partial squares approach generalized inverses 
siam sci 
stat 
comput 
zadeh 
march 
fuzzy logic neural networks soft computing 
communications acm 
zhang 

contribution la surveillance de proc ed es 
thesis universit de rennes zhang 


public domain matlab toolbox anonymous ftp ftp irisa fr local 
zhang 

wavelet network nonparametric estimation 
technical report irisa 
zhang basseville benveniste 

early warning slight changes systems plants application condition maintenance 
automatica 
special issue statistical methods signal processing control 
zhang benveniste 

wavelet networks 
ieee trans 
neural networks 

