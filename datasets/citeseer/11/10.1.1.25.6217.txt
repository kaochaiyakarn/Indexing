automated lip sync background techniques john lewis computer graphics laboratory new york institute technology summary problem creating mouth animation synchronized recorded speech discussed 
review model speech sound generation indicates automatic derivation mouth movement speech soundtrack tractable problem 
automatic lip sync techniques compared method described detail 
method common speech synthesis method linear prediction adapted provide simple accurate phoneme recognition 
recognized phonemes associated mouth positions provide keyframes computer animation speech 
experience technique indicates automatic produce useful results 
key words facial animation speech synchronization movement lips tongue speech important component facial animation 
mouth movement speech ongoing relatively rapid movement encompasses number visually distinct positions 
movement synchronized speech 
adequate performance lip sync problem defined 
example accurate mouth movement timing order satisfying accurate pass reality test 
people read lips identify speech mouth movement viewers passive notion correct mouth movement speech know bad lip sync see 
lip sync problem traditionally handled ways 
animations realistic movement desired mouth motion general character movement may obtained 
technique live action footage actors performing desired motion obtained frames footage provide guide corresponding frames animation 
second approach commonly cartoons adopt canonical mapping subset speech sounds corresponding mouth positions 
animation handbooks tables illustrating mouth positions corresponding small number key sounds 
animator approximately segment soundtrack key sounds 
example word happy segmented sequence vowels ee 
approach neglects sounds vowel sounds correspond visually distinctive mouth positions typically greater duration non vowel sounds 
lip sync produced approach satisfactory generally realistic 
viable computer face models developed including 
ideally control face models high level animation script intelligent front face model automatically translate script appropriate sequence facial expressions movements 
considers limited problem automatically obtaining mouth movement recorded soundtrack 
section describe speech production reasons automatic lip sync feasible 
subsequent sections review approaches automatic lip sync 
concludes discussion important poorly defined problem matching realism lack realism facial model lip sync motion speech sounds 
source filter speech model excellent textbooks speech principles available 
relevant points mentioned 
fig 
shows envelope waveform phrase come quietly trouble 
difficult visually segment waveform words 
example gap quietly gap ly quietly 
appear visualization computer animation annotated waveform envelope phrase come quietly trouble 
speech sound generation may modeled broadband sound source passed filter 
sound source vibrations vocal cords case voiced sounds air turbulence case sounds 
case voiced sounds vocal cords collide periodically producing pitched sound slowly decaying spectrum harmonics fig 

sound produced passes vocal tract consists throat mouth tongue lips optionally nasal cavity 
effect vocal tract filter sound introducing resonances peaks spectrum called formants 
vowel sounds characterized frequencies formants 
locations formants varied moving jaw tongue lips change shape vocal tract 
formants appear dark bands speech spectrogram plot fig 

formant trajectories curve slowly vowels change rapidly disappear consonants vowel consonant transitions 
source filter description speech sound generation diagrammed fig 

plots energy spectra frequency increasing zero left plot 
fig 
source shows harmonics periodic roughly triangular pulse produced vocal cords 
fig 
filter shows vocal tract filter transfer function containing formants 
fig 
output shows spectrum resulting speech 
formants superimposed harmonic spectrum vocal cords 
note formant peak frequencies independent harmonic frequencies 
important feature source filter model separates intonation phonetic information 
intonation characteristics including pitch amplitude voiced quality features sound source vocal tract filtering determines phoneme phoneme somewhat loosely term atomic perceptual unit speech sound 
human speech production perception likewise separate intonation phonetic information 
demonstrated sounding fixed vowel varying pitch voiced quality conversely maintaining constant pitch sounding different vowels mouth position vowel entirely independent pitch 
emphasized various qualifications details source filter model described qualifications invalidate separation intonation phonetic information 
order automatic lip sync feasible position lips tongue related identifiable way characteristics speech sound 
source filter model indicates lip tongue positions functions phoneme independent intonation characteristics speech sound 
procedure results representation speech timed sequence phonemes phonetic script suitable starting point automated lip sync approach 
automated lip sync techniques loudness jaw rotation naive approach automatic lip sync open mouth proportion loudness sound 
evident poor approach nasal loud mouth closed 
mouth assumes variety visually distinct positions appear visualization computer animation smoothed speech spectrogram pitch harmonics removed 
plot shows energy frequencies zero bottom hz 
time zero left second 
primary vowel formants visible dark bands 
diagram source filter speech generation model frequency domain vocal cords generate periodic sound harmonics 
vocal tract acts filter introducing resonances spectrum 
resulting speech sound resonant peaks formants superimposed harmonic spectrum generated vocal cords 
appear visualization computer animation speech simply open close 
facial animation produced approach robotic quality 
spectrum matching sophisticated approach pass speech signal bank filters sample spectra output filters desired animation frame rate compare spectra spectra set sounds squares match example 
approach transmission presence low bandwidth teleconferencing experiments mit early 
approach produce acceptable lip sync accurate produce fully realistic lip motion 
problem formant frequencies quantized available filter frequencies 
significant difficulty approach spectrum describes vocal tract formants pitch case voiced speech lip tongue positions related formants independent pitch 
pitch natural voiced speech varies utterance pitch particular portion utterance match pitch sounds 
mismatch degrades accuracy sound matching 
pitch contamination reduced designing filter bank smooth pitch harmonics 
tradeoff smoothing spectrum accurately localizing formant peaks 
best results obtainable filter bank approach extended point fourier transform sufficient resolve pitch harmonics frequency samples hz 
magnitude high resolution transform smoothed sophisticated technique smoothing spline 
speech synthesis different approach lip sync problem involves computer synthesized speech starting recorded speech 
approach phonetic script specified directly animator generated text phoneme synthesizer 
phonetic script drives phoneme speech synthesizer read generate lip motion resulting speech 
approach successfully facial animation systems 
advantage approach generates accurate lip sync speech lip motion specified script 
appropriate desired speech specified textually recording speech content informative intonation secondary consideration case computerized voice information system 
drawback approach difficult achieve natural rhythm articulation synthetic speech 
current speech synthesis algorithms produces speech having slightly robotic quality older systems produce speech unintelligible 
typically intonation improved adding information pitch loudness indicators text refining phonetic script 
requires additional required animate mouth directly 
linear prediction approach lip sync described lip sync approach linear prediction special case wiener filtering 
approach speech effectively sound source vocal tract filtering components 
filtering component phonetic script required lip sync processing required remove pitch harmonics 
algorithm efficient maps available matrix algorithms hardware 
section describe linear prediction lip sync algorithm implementation considerations 
linear prediction speech model linear prediction models speech signal broadband excitation signal ffx input linear autoregressive filter weighted sum input past output filter ffx gammak realization source filter model speech production described previously 
excitation signal approximated pulse train resulting pitched vowel sounds uncorrelated noise resulting consonants vowels depending filter 
filter coefficients vary time constant short interval analysis frame vocal tract shape assumed constant 
analysis frame time appear visualization computer animation fast track perceptible speech events somewhat longer voice pitch period permit deconvolution pitch information 
analysis frame time milliseconds satisfies conditions 
corresponds frames second suggesting sampling mouth movement standard animation rate frames second may fast speech events fig 

purpose lip synchronized animation convenient choose analysis frame rate twice film video frame playback rate 
case speech analysis frames reduced desired animation frame rate simple low pass filter 
alternative generate animation higher frame rate frames second apply filter frames generated animation analysis frames 
approach reduces temporal aliasing resulting quantizing mouth movement keyframes animation frame rate source difficulty previous 
algorithm frame digitized speech coefficients determined minimizing squared error actual predicted speech number samples 
number formulations leastsquares linear prediction simple derivation results autocorrelation method linear prediction 
derivation views speech signal random process stationary statistics analysis frame time 
expected squared estimation error gamma ffx gammak minimized setting proof determine minimum involves rewriting quadratic form obtaining gammaj gamma ffx gammaj gammak gammaj excitation time uncorrelated previous speech signal expectation product ffx gammaj zero 
expectation terms gammaj gammak gamma th value autocorrelation function 
substitutions result system gamma matrix form delta delta delta gamma delta delta delta gamma delta delta delta delta delta delta delta delta delta delta delta delta gamma gamma delta delta delta delta delta delta ap delta delta delta solved analysis frame autocorrelation function estimated directly speech signal gamma gamma length analysis frame samples 
autocorrelation stationary process function gamma symmetric toeplitz matrix having equal elements diagonals 
permits efficient inversion algorithms levinson recursion 
number formulations linear prediction choice particular approach depends largely mathematical preferences 
provide speech oriented overviews autocorrelation covariance formulation exhaustive interesting treatment subject 
solution algorithms published 
fortran implementation levinson algorithm version routine auto included ieee signal processing library 
efficient solution obtained durbin algorithm fact right hand vector composed data matrix 
algorithm described pascal algorithm 
alternatively solved standard symmetric general matrix inversion routine extra computational cost 
synchronized speech coefficients resulting linear prediction analysis describe short term speech spectrum pitch information convolved 
analyzed speech converted phonetic script classifying speech frame minimum euclidean distance short term spectrum spectra set phonemes 
spectrum appear visualization computer animation obtained evaluating magnitude ff gamma gammak transform points complex half unit circle case denominator effectively discrete fourier transform negated zero extended coefficient sequence gammaa gammaa gammaa length permitting implementation fft 
resolution appears sufficient linear prediction spectra smooth 
direct identification approach compare coefficients coefficients phonemes squares matching coefficients performs poorly appears norm required 
selection phonemes involves compromise robust identification phonetic visual resolution 
various read lips books books animation describe visually distinctive mouth positions corresponding sounds fig 

previous synchronized speech animation typically approximately distinct mouth keyframes synthetic speech approaches distinct mouth positions 
current phoneme set consists vowels words hate hat hot heed head hit hoe hug pronounced american english consonants phonemes spoken english counting combination sounds set includes vowels 
approach lip sync profits fact vowels easily identified linear prediction speech model visually distinctive mouth positions correspond vowels cases fig 
consonants generally shorter vowels 
necessary distinct mouth position phoneme consonants distinguished voicing lip tongue position 
fact key sounds mouth positions required represent consonant sounds consonants fairly similar spectra mouth positions mouth closed slightly open 
accurate vowel identification possible linear prediction identification approach twelve phonemes 
currently khz audio sampling rate 
number coefficients chosen rule thumb pole conjugate zero pair denominator polynomial khz plus insert fig 
near computer face model positioned vowel word hot 
extra coefficients model spectrum shape 
semantically important information speech lies gamma hz demonstrated intelligibility telephones audio sample rate khz sufficient analysis applications lip sync 
higher sample rate allows speech data manipulated resynthesized reasonably high quality sound track 
consonant transitions area theoretical difficulty 
cases example pronouncing consonant word mouth remain open aspiration period silence leading word 
purely acoustically lip sync technique incorrectly cause mouth closed period 
fig 
shows raw output linear prediction procedure applied phrase begins greetings media consumers 
columns left right time excitation volume voiced unvoiced indicator best phoneme match starred column associated error second best match error 
example annotated corresponding speech right hand column 
annotation seen vowels plausibly identified consonants mapped consonants 
example sound word greetings matched sound labeled es 
sound greetings matched vowel word hit vowel heed due pronunciation 
sound variation vowel sound word heed 
parametric face model parametric human face model developed parke lip sync experiments 
model extended full head versions mcdermott 
parametric modeling approach allows face directly intuitively manipulated limited fairly natural set parameters bypassing effort involved modeling digitizing keyframes keyframe approach 
face model parameters relevant mouth positioning lip sync include controlling jaw rotation mouth opening points lower lip appear visualization computer animation portion lip reading chart 
top row left right vowels words hat hot sound 
bottom row vowels words head hit 
insert fig 
near computer face model positioned vowel word 
sound movement corners mouth 
parametric model allows expressive parameters manipulated animated independently geometric features animation script including lip sync expressive parameters applied available character geometric database 
fig 
fig 
show character positioned vowels words hot 
tongue motion automatically derived phonetic script manner lips capability parke face model currently include tongue 
parameter smoothing mouth move rapidly vowel consonant transitions vowel vowel transitions generally smooth seen formant trajectories fig 

automated lip sync effect performs vector quantization high dimensional acoustic space onedimensional discrete space phonemes 
quantization results abrupt transitions phonemes 
necessary smooth mouth motion 
phoneme space discrete possible smooth phoneme sequence directly 
approach date convert phonetic script set parameter tracks face model smooth tracks 
fairly sophisticated smoothing technique needed 
finite impulse response filter provide suitable smoothing blurred rapid vowel consonant transitions attenuated extremes parameter movement 
smoothing spline currently implemented provides somewhat better results 
examination formant trajectories suggests need smoothing technique preserves large discontinuities 
linear prediction speech resynthesis linear prediction software implemented original speech 
enables manipulations may useful animation 
faithful synthesis approach difference signal residual original speech output linear prediction filter synthesis excitation signal gamma gammak residual signal approximates uncorrelated noise consonants vowels approximates appear visualization computer animation gain err sil sil silence gain err sil sil gain err em gr gain err hit hit gain err hit hit gain err hit hit gain err es sil gain err es es gain err heed heed gain err heed es gain err heed heed gain err ng gain err gain err gain err em error gain err es es gain err es es gain err sil sil silence gain err em 
em gain err em em gain err hit heed gain err es heed gain err es es gain err es heed gain err es hit gain err hit heed gain err hit es gain err head head gain err head head gain err gain err sil sil silence annotated output linear prediction lip sync procedure words greetings media 
appear visualization computer animation pulse train voiced vowels 
linear prediction analysis residual encode information original speech 
synthesized speech highly intelligible retains original inflection rhythm subtle synthetic quality may appropriate computer animation 
variations form synthesis commonly speech compression reader doubt heard examples produced dedicated linear prediction chips 
quality robot speech obtained excitation signal synthetically generated signal may pulse train random sequence 
levinson durbin algorithms return frame prediction error magnitude compared threshold determine form excitation normalized errors greater typically reflect consonants voice 
important manipulation easily possible case synthetic excitation speed slow speech 
accomplished simply accessing linear prediction analysis frames faster slower rate 
voice pitch controlled excitation speech rate changed producing mouse effect 
linear prediction software implemented general purpose lisp computer music system additional sonic manipulations reverberation gender age change spectrum shifting directly obtainable 
evaluation linear prediction lip sync approach described previous section produces mouth motion tightly synchronized speech 
quality lip sync falls short full realism characterized better lip sync obtained lazy approach employed film footage guided creation mouth keyframes frames 
animator trained traditional animation techniques characterized linear prediction lip sync method producing data 
characterization consistent recommendations animation handbooks generally suggest lengthy stressed syllables animated 
gestalt specificity animator uses computer face model faces strong poorly defined perceptual phenomenon 
fig 
attempt elucidate phenomenon 
drawing easily recognized face face sketch specifies geometric information computer face model 
infer character despite fact drawing specifies far geometric information existing computer face models 
information clearly omitted perceptually ignored completed 
contrast dimensional shaded renderings objects cars extremely realistic comparable renderings computer face models appear mechanical 
face model detailed specific inaccuracies specified information perceptually prominent 
view problem results fact computer models generally specify unknown information 
example set vertices control points geometric model may known detail surface constructed points may plausible surfaces 
shaded rendering model realize surfaces 
case computer face model surface interpolation required computer rendering asserts face quite smooth rendering fig 
rule possibility skin imperfections unspecified locations 
phenomenon may affect automated lip sync computerized character animation 
lip sync motion derived recorded soundtrack quite specific fully realistic 
speculate animation successful motion filtered subsampled detailed reducing perceptual expectations 
similar considerations applied soundtrack 
animator consider viewers appear visualization computer animation accept slightly mechanical face speech slightly mechanical case lip sync approaches synthetic speech 
recorded speech may resynthesized linear prediction order achieve slight synthetic quality preserving intelligibility intonation 
hand successful real voices traditional animation invalidate principle realism soundtrack match images 
preceding comments philosophical scientific successful application facial animation require understanding similar issues 
directions facial animation generated automated lip sync looks unnatural head eyes moving 
head movement speech probably quite possible generate stereotypical head eye movement automatically soundtrack 
reduce animator load enable automated talking head presentations audio monologues 
explored possible variations lip movement utterance 
correct pronunciation considerably constrains possible deviations standard lip movement obvious effect increased volume corresponds greater mouth opening 
possible effect emotional expression mouth movement speech considered 
may important effect mouth position primary indicators emotion 
related problem attempt derive emotional state directly speech soundtrack 
sean curran provided evaluation automatic technique animator viewpoint 
walther lipreading nelson hall chicago 
mcgovern live action footage tool animator siggraph tutorial notes character animation computer acm new york 
blair animation learn draw animated cartoons foster laguna beach california 
parke parameterized models facial animation ieee computer graphics applications nov 
bergeron controlling facial expressions body movements animated short tony de siggraph tutorial notes acm new york 
magnenat thalmann thalmann synthetic actors computer generated threedimensional films springer verlag tokyo 
waters muscle model animating threedimensional facial expression computer graphics july 
rabiner schafer digital processing speech signals prentice hall englewood cliffs 
witten principles computer speech academic press london 
flanagan speech analysis synthesis perception springer verlag new york 
weil face synthesis manipulation facial imagery thesis massachusetts institute technology 
lewis purcell soft machine interface proceedings graphics interface ottawa may 
pearce wyvill wyvill hill speech expression computer solution face animation proceedings graphics interface 
hill pearce wyvill animating speech automated approach speech synthesis rules visual computer 
magnenat thalmann thalmann muscle action procedures human face animation visual computer 
lewis parke automated lip synch speech synthesis character animation proceedings chi acm new york toronto 
appear visualization computer animation wiener extrapolation interpolation smoothing stationary time series wiley new york 
gray linear prediction speech springer verlag new york 
parke parametric model human faces ph dissertation utah 
levinson wiener rms root mean square error criterion filter design prediction journal mathematical physics 
programs digital signal processing ieee press 
implementation parameterized facial modeling animation system siggraph course notes state art facial animation acm new york 
de boor practical guide splines springer verlag new york 
lewis manual manual internal documentation 
parke personal communication 
critical reality computer animation siggraph tutorial notes character animation computer acm new york 
appear visualization computer animation 
