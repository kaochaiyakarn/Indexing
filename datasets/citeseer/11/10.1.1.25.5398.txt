ieee transactions pattern analysis machine intelligence vol 
december automatic analysis facial expressions state art maja pantic student member ieee leon rothkrantz humans detect interpret faces facial expressions scene little effort 
development automated system accomplishes task difficult 
related problems detection image segment face extraction facial expression information classification expression emotion categories 
system performs operations accurately real time form big step achieving human interaction man machine 
surveys past solving problems 
capability human visual system respect problems discussed 
meant serve ultimate goal guide determining recommendations development automatic facial expression analyzer 
index terms face detection facial expression information extraction facial action encoding facial expression emotional classification 
pointed bruce takeuchi nagao hara kobayashi human face face communication ideal model designing multimodal media human computer interface hci 
main characteristics human communication multiplicity multimodality communication channels 
channel communication medium modality sense perceive signals outside world 
examples human communication channels auditory channel carries speech auditory channel carries vocal intonation visual channel carries facial expressions visual channel carries body movements 
senses sight hearing touch examples modalities 
usual face face communication channels different modalities activated 
result communication highly flexible robust 
failure channel recovered channel message channel explained channel 
multimedia modal hci developed facilitating robust natural efficient effective man machine interaction 
relatively existing works combine different modalities single system human communicative reaction analysis 
examples works chen de silva studied effects combined detection facial vocal expressions emotions 
far majority studies treat various human communication channels separately indicated 
examples systems emotional interpretation human voices authors department media engineering mathematics delft university technology po box aj delft netherlands 
mail pantic rothkrantz cs tudelft nl 
manuscript received june revised march accepted sept 
recommended acceptance bowyer 
information obtaining reprints article please send mail tpami computer org ieeecs log number 
ieee emotion recognition physiological signals pattern recognition detection interpretation hand gestures recognition body movements facial expression analysis survey 
terms face face interface indicate face plays essential role interpersonal communication 
face mean identify members species interpret said means lipreading understand emotional state intentions basis shown facial expression 
personality attractiveness age gender seen face 
considerable research social psychology shown facial expressions help coordinate conversation considerably effect listener feels liked disliked speaker spoken words 
indicated verbal part spoken words message contributes percent effect message vocal part voice intonation contributes percent facial expression speaker contributes percent effect spoken message 
implies facial expressions form major modality human communication 
advances image analysis pattern recognition open possibility automatic detection classification emotional conversational facial signals 
automating facial expression analysis bring facial expressions man machine interaction new modality interaction tighter efficient 
system classification facial expressions widely accessible tool research behavioral science medicine 
goal survey done automating facial expression analysis facial images image sequences 
section identifies basic problems related facial expression analysis 
problems face detection facial image image sequence facial expression data extraction facial expression classification 
capability pantic rothkrantz automatic analysis facial expressions state art human visual system respect problems described 
defines way expectations automated system 
characteristics ideal automated system facial expression analysis section 
section surveys techniques literature past decade facial expression analysis computer 
characteristics summarized respect requirements posed design ideal facial expression analyzer 
attempt provide exhaustive review past problems related automatic facial expression analysis 
selectively discuss systems deal problems 
possible directions research discussed section 
section concludes 
facial expression analysis aim explore issues design implementation system perform automated facial expression analysis 
general main steps distinguished tackling problem 
facial expression analyzed face detected scene 
devise mechanisms extracting facial expression information observed facial image image sequence 
case static images process extracting facial expression information referred localizing face features scene 
case facial image sequences process referred tracking face features scene 
point clear distinction terms facial features face model features 
facial features prominent features face eyebrows eyes nose mouth chin 
face model features features represent model face 
face represented various ways unit holistic representation set features analytic representation combination hybrid approach 
applied face representation kind input images determine choice mechanisms automatic extraction facial expression information 
final step define set categories want facial expression classification facial expression interpretation devise mechanism categorization 
automated facial expression built decide system functionality 
point functionality human visual system 
best known facial expression analyzer 
section discusses basic problems related process facial expression analysis capability human visual system respect 
face detection works automatic facial expression analysis conditions facial image image sequence obtained controlled 
usually image face frontal view 
presence face scene ensured global location face scene known priori 
determining exact location face digitized facial image complex problem 
scale orientation face vary image image 
taken fixed camera faces occur images various sizes angles due movements observed person 
difficult search fixed pattern template image 
presence noise occlusion problem difficult 
humans detect facial pattern casual inspection scene 
detect faces effortlessly wide range conditions bad lightning conditions great distance 
generally believed gray levels images pixels form lower limit detection face human observer 
characteristic human visual system face perceived collection facial features 
presence features geometrical relationship appears important details features 
face partially occluded hand perceive face perceptual system fills missing parts 
difficult possible achieve computer 
facial expression data extraction presence face detected observed scene step extract information encountered facial expression automatic way 
extraction performed automatically fully automatic facial expression analyzer developed 
applied face representation kind input images affect choice approach facial expression information extraction 
fundamental issues facial expression analysis representation visual information examined face reveal 
results johansson point light display experiments gave clue problem 
experiments suggest visual properties face regarding information shown facial expression clear describing movements points belonging facial features eyebrows eyes mouth analyzing relationships movements 
triggered researchers vision facial gesture analysis different attempts define point visual properties facial expressions 
various analytic face representations yielded face modeled set facial points fig 
fig 
set templates fitted facial features eyes mouth 
approach face representation holistic approach face represented unit 
wire frame mapped texture spatio temporal model facial image motion fig 
typical examples holistic approaches face representation 
face modeled socalled hybrid approach combination analytic holistic approaches face representation 
approach set facial points usually determine initial position template models face fig 
irrespectively kind face model applied attempts model extract information displayed facial expression losing information 
factors task complex 
presence facial ieee transactions pattern analysis machine intelligence vol 
december hair glasses obscure facial features 
problem variation size orientation face input images 
disables search fixed patterns images 
noise occlusion extent 
indicated ellis human encoding visual stimulus face expression may form primal sketch may hardwired 
known terms nature internal representation face human brain 
facial expression classification face appearance perceived step automated expression analyzer identify facial expression conveyed face 
fundamental issue facial expression classification define set categories want deal 
related issue devise mechanisms categorization 
facial expressions classified various ways terms facial actions cause expression terms expressions raised brows terms expressions emotional expressions 
facial action coding system facs probably known study facial activity 
system developed facilitate objective measurement facial activity behavioral science investigations face 
facs designed human observers detect independent subtle changes facial appearance caused contractions facial muscles 
form rules facs provides linguistic description possible visually detectable facial changes terms called action units aus 
rules trained human facs coder decomposes shown expression specific aus describe expression 
automating facs widely accessible research tool behavioral science furthermore theoretical basis multimodal media user interfaces 
triggered researchers computer vision field take different approaches tackling problem 
explicit attempts automate facial action coding applied automated facs encoding see review existing methods table survey 
studies automated expression analysis perform emotional classification 
indicated known commonly study emotional classification facial expressions cross cultural study existence universal categories emotional expressions ekman defined categories referred basic emotions happiness sadness surprise fear anger disgust 
described basic emotion terms facial expression uniquely characterizes emotion 
past years questions arose study 
basic emotional expressions universal merely stressing verbal communication relation actual emotional state 
certain facial expression able displayed face classified basic emotion categories 
fig 

expressions blended emotions surprise happiness 
studies vision facial expression analysis rely ekman emotional categorization facial expressions 
problem automating facial expression emotional classification difficult handle number reasons 
ekman description facial expressions emotion linguistic ambiguous 
uniquely defined description terms facial actions terms universally defined facial codes 
validation verification classification scheme difficult crucial tasks 
second classification facial expressions multiple emotion categories feasible raised eyebrows smiling mouth blend surprise happiness fig 

psychological scrutiny topic 
issues related facial expression classification general 
system capable analyzing subject male female age ethnicity 
words classification mechanism may depend variability observed person 
hand person maximal intensity displaying particular facial expression 
obtained classification quantified achieve quantified encoding facial actions quantified emotional labeling blended expressions systems start generic expression classification adapt particular individual advantage 
second important realize interpretation body language situation dependent 
information context facial expression appears difficult obtain automatic way 
issue handled currently existing systems 
growing psychological research argues timing facial expressions critical factor interpretation expressions 
researchers automated vision expression analysis suggests moving real time face analysis facial expression dynamics 
human mechanisms face detection robust case interpretation facial expressions 
difficult determine exact nature expression person face 
trained observer correctly classify faces pantic rothkrantz automatic analysis facial expressions state art showing basic emotions average percent 
ratio varies depending factors familiarity face familiarity personality observed person general experience different types expressions attention face nonvisual cues context expression appears 
interesting note appearance upper face features plays important role face interpretation opposed lower face features 
system facial expression analysis developing automated system facial expression analysis decide functionality 
point best known facial expression analyzer human visual system 
may possible incorporate features human visual system automated system features may undesirable certainly serve point 
requirement posed developing ideal automated facial expression analyzer stages facial expression analysis performed automatically face detection facial expression information extraction facial expression classification 
actual implementation integration stages system constrained system application domain 
instance system tool research behavioral science real time performance essential property system 
hand crucial system form part advanced user interface 
long delays interaction desynchronized efficient 
having explanation facility elucidate facial action encoding performed system useful system employed train human experts facs 
facility superfluous system employed videoconferencing stress monitoring tool 
mainly concerned major application domains automated facial expression analyzer behavioral science research multimodal media hci 
section propose ideal automated facial expression analyzer table employed application domains properties human visual system 
considering potential applications automated facial expression analyzer involve continuous observation subject time interval facial image acquisition proceed automatic way 
order universal system capable analyzing subjects age ethnicity 
constraints set outlook observed subjects 
system perform robustly despite changes lightning conditions distractions glasses changes hair style facial hair beard grown eyebrows 
similarly human visual system ideal system fill missing parts observed face perceive face part occluded hand 
real life situations complete table properties ideal analyzer observed subject assumed 
system able deal rigid head motions 
ideally system perform robust facial expression analysis despite large changes viewing conditions capable dealing range head movements frontal view profile view acquired fixed camera 
achieved employing fixed cameras acquiring different facial views examined face frontal view right left profile views approximating actual view interpolation acquired views 
having constraints set rigid head motions subject achieved having camera mounted subject head placed front face 
ideal system perform robust automatic face detection facial expression information extraction acquired images image sequences 
considering stateof art image processing techniques inaccurate noisy missing data expected 
ideal system capable dealing inaccuracies 
addition certainty extracted facial expression information taken account 
ideal system able perform analysis visually distinguishable facial expressions 
defined face representation prerequisite achieving 
face representation particular alteration face model uniquely reveals particular facial expression 
general ideal system able distinguish 
possible facial expressions point total facial actions defined facs ieee transactions pattern analysis machine intelligence vol 
december combinations form complete set facial expressions 
bilateral unilateral facial change 
facial expressions similar facial appearance upward pull upper lip nose causes upward pull upper lip 
practice may possible define face model satisfy reflect change facial appearance features detectable facial image image sequence 
set distinct facial expressions system distinguish possible 
system behavioral science research purposes perform facial expression recognition applied automated facs encoding 
explained bartlett means accomplish multiple quantified expression classification terms aus defined facs 
system part advanced multimodal media hci system able interpret shown facial expressions terms emotions 
psychological researchers disagree existence universal categories emotional facial displays ideal system able adapt classification mechanism user subjective interpretation expressions suggested 
definitely case facial expression able displayed face classified emotion class 
think blended emotional displays raised eyebrow smiling mouth fig 

expression classified emotion categories defined ekman friesen surprise happiness 
descriptions expressions ekman friesen left hand side facial expression shown fig 
belongs surprise happiness class 
instance left hand side image percentage shown surprise higher percentage shown happiness percentages table early methods automatic facial expression analysis approximately case right hand side image 
order obtain accurate categorization ideal analyzer perform quantified classification facial expression multiple emotion categories 
automatic facial expression analysis utility application domains human behavior interpretation multimodal media hci automatic facial expression analysis attracted interest computer vision researchers 
mid different approaches proposed facial expression analysis static facial images image sequences 
samal iyengar gave overview early works 
explores compares approaches automatic facial expression analysis developed late 
surveying works detail giving short overview systems facial expression analysis proposed period 
table summarizes features systems respect requirements posed design ideal facial expression analyzer 
systems performs quantified expression classification terms facial actions 
system proposed moses system listed table performs real time 
properties columns excluded table 
stands stands represents missing entry 
missing entry means reported issue issue applicable system question 
inapplicable issue instance issue dealing rigid head motions inaccurate facial data cases input data hand measured 
methods listed table perform automatic facial data extraction see column achieve facial motion analysis 
method proposed kearney mckenzie performs facial expression classification pantic rothkrantz automatic analysis facial expressions state art terms facial actions terms interpretation categories defined users systems listed table perform expression classification number basic emotion categories 
utilized techniques include holistic spatial analysis spatio temporal analysis analytic spatial analysis 
correct recognition rates reported range percent recognizing emotion categories 
survey works literature divided parts problems discussed section face detection facial expression information extraction facial expression classification 
attempt provide exhaustive review past problems related automatic facial expression analysis 
selectively discuss developed systems deal facial expression detection classification 
table summarizes properties surveyed facial expression analyzers respect requirements posed design ideal facial expression analyzer table 
systems able perform facial expression information extraction images occluded faces quantified expression classification terms facial actions 
properties columns excluded table 
case systems facial expression information manually extracted declaring deal subjects ethnicity 
case automatic facial expression data extraction value column table represents range ethnicity testing subjects 
number testing images number subjects testing images performance surveyed systems summarized table 
table approaches automatic facial expression analysis approaches explored lately include systems automatic analysis synthesis facial expressions broader list see 
image analysis techniques systems relevant goals systems limited behavioral science investigations face multimodal media hci 
systems primarily concern facial expression animation attempt classify observed facial expression terms facial actions terms emotion categories 
reason similar methods scope goal explore compare image approaches facial expression detection classification 
face detection automatic facial expression analysis conditions image obtained controlled 
camera mounted device worn subject placed way image face frontal view 
presence face scene ensured global location face scene known priori 
real life situations automated facial expression analyzer employed multimodal media hci location face image known priori 
problem automatic face detection arbitrary scene drawn great attention see 
independently kind input images facial images arbitrary images detection exact face position observed image image sequence approached ways 
holistic approach face determined unit 
second analytic ieee transactions pattern analysis machine intelligence vol 
december approach face detected detecting important facial features irises nostrils 
location features correspondence determines location face 
table provides classification facial expression analyzers kind input images applied method 
face detection facial images represent face huang huang apply point distribution model pdm 
order achieve correct placement initial pdm input image huang huang utilize canny edge detector obtain rough estimate face location image 
valley pixel intensity lies lips symmetrical vertical edges representing outer vertical boundaries face generate rough estimate face location 
face facial hair glasses rigid head motion may encountered illumination variations linear system correctly 
pantic rothkrantz detect face unit 
input system dual view facial images 
determine vertical horizontal outer boundaries head analyze vertical horizontal histogram frontal view image 
localize contour face algorithm hsv color model similar algorithm relative rgb model 
profile view image apply profile detection algorithm represents spatial approach sampling profile contour thresholded image 
thresholding input profile table summary methods automatic face detection image value hsv model exploited 
facial hair glasses allowed 
kobayashi hara apply analytic approach face detection 
ccd camera monochrome mode obtain brightness distribution data human face 
base brightness distribution calculated average brightness distribution data obtained subjects 
system extracts position irises utilizing crosscorrelation technique base data currently examined data 
irises identified location face determined relative locations facial features face 
observed subject face camera approximately distance front 
analytic approach face detection 
outer corners eyes height eyes height mouth extracted automatic way 
features identified size examined facial area normalized rectangular grid placed image 
stated method applied limitation method reported kimura yachida utilize potential net face representation 
input image normalized centers eyes center mouth tracked method proposed wu 
algorithm applies integral projection method synthesizes color edge information 
potential net fitted normalized image model face movement 
face facial hair glasses direct face face position camera 
pantic rothkrantz automatic analysis facial expressions state art face detection arbitrary images works surveyed perform automatic face detection arbitrary scene 
hong utilize system order perform realtime tracking head 
box bounding head image initial labeled graph fitted 
head tracking module system applies spatio temporal filtering input image sequence 
stereo algorithm determines stereo disparities pixels changed due movement 
inspecting local disparity histogram image regions confined certain disparity interval selected 
skin color detector convex region detector applied regions 
bounding boxes confining clusters outputs detectors correspond heads hands 
case person moving longer module memorizes position person 
estimation current position velocity head achieved linear predictive filter 
reported system performs presence background motion fails case covered rotated faces 
essa pentland eigenspace method pentland locate faces arbitrary scene :10.1.1.40.359
method employs eigenfaces approximated principal component analysis pca sample facial images 
eigenfaces define subspace sample images called face space 
detect presence face single image distance observed image face space calculated projection coefficients signal energy 
detect presence faces image sequence spatio temporal filtering performed filtered image thresholded order analyze motion blobs motion blob represent human head evaluated single image 
method pentland real time successfully tested database images people ranged age ethnicity having varying head positions facial hair 
facial expression data extraction presence face detected observed scene step extract information shown facial expression 
applied face representation kind input images affect choice approach facial expression data extraction 
general types face representation mainly facial expression analysis holistic maps analytic deformable templates hybrid analytic holistic approach 
face representations surveyed systems listed table 
depending face model template feature method applied facial expression data extraction 
template methods fit holistic face model input image track input image sequence 
feature methods localize features analytic face model input image track input table utilized face models sequence 
methods utilized surveyed systems listed table 
facial data extraction static images template methods shown table surveyed systems classified methods facial expression analysis static images 
category utilizes holistic hybrid approach face representation table applies template method facial expression information extraction input image 
edwards utilize holistic face representation refer active appearance model aam 
build model facial images manually labeled points localized facial features 
generate statistical model shape variation edwards aligned training images common coordinate frame applied pca get mean shape 
build statistical model gray level appearance warped training image triangulation algorithm control points match mean shape 
applying pca gray level information extracted warped images obtained mean normalized gray level vector 
applying pca vector appearance parameters controlling shape gray levels model obtained 
fit aam input image edwards apply aam search algorithm implies stages 
training stage training images labeled landmark points known model displacements applied corresponding difference vector recorded 
training data generated multivariate multiple regression analysis applied model relationship model displacement ieee transactions pattern analysis machine intelligence vol 
december table methods automatic facial expression data extraction pantic rothkrantz automatic analysis facial expressions state art fig 

small model graph small dense model graph big 
image difference 
recognition stage learned regression model determine movement face model 
aam search algorithm tested hand labeled face images 
percent failed converge satisfactory result 
method works images faces facial hair glasses hand labeled landmark points approximated proposed aam 
hong utilize labeled graph represent face 
node graph consists array called jet 
component jet filter response certain gabor wavelet extracted point input image 
hong wavelets different frequencies different orientations 
defined different labeled graphs called general face knowledge 
big labeled graph nodes fig 
node component jet corresponding landmark extracted individual faces assigned 
small labeled graph nodes fig 

node contains component jet wave field orientations frequencies extracted set faces 
small find exact face location input facial image big localize facial features 
hong utilize system method elastic graph matching proposed wiskott fit model graph surface image 
small moved scaled input image place best match 
matching performed exact face position derived canonical graph size value mean euclidean distance nodes center gravity 
big fitted cropped face region node weighting method applied 
low weight assigned nodes face hair boundary high weight assigned nodes facial features 
big weighted nodes emotionally classify shown facial expression 
hong utilize system deals real time processing video sequences perform facial expression analysis static images 
dense model graph suitable facial action coding extracted deformations graph 
issue discussed hong 
represent face huang huang utilize point distribution model pdm 
pdm fig 

aligned training set generation pdm model reprinted permission academic press academic press 
generated facial feature points manually localized images chinese subjects showing basic emotions figs 

mouth included model approximating contour mouth parabolic curves 
proposed model combination pdm mouth template arguably close feature model template model 
classified holistic face model pdm models face interacts estimated face region input image entire 
initial placement pdm input image section method huang huang moves deforms entire pdm simultaneously 
gradient shape parameters estimation minimizes gray level model fitness measure applied 
search mouth starts defining appropriate search region basis fitted pdm 
darkest point vertical strip search region 
gray level thresholding applied eliminate misleading points parabolic curve approximate mouth line 
edges strongest gradient located line approximated parabolic curve represent upper lip 
method applied find lower lip 
method teeth visible dark region lips 
method strongly constrained section 
fig 

fitted pdm model reprinted permission academic press academic press 
ieee transactions pattern analysis machine intelligence vol 
december fig 

fiducial grid facial points 
cottrell holistic face representation deal facial expression information extraction automatic way 
facial emotion database assembled ekman friesen digitized images basic emotional facial expressions scaled prominent facial features located image region 
image area eye divided vertically overlapping pixel blocks area mouth divided horizontally overlapping pixel blocks 
pca pixel blocks randomly taken entire image applied order generate eigenvectors 
input nn emotional classification expression normalized projection extracted blocks top principal components 
hybrid approach face representation 
fit quadratic grid normalized facial image see section 
averaged optical flow calculated regions 
calculate optical flow neutral examined facial expression image optical flow algorithm proposed horn schunck 
magnitude direction calculated optical flows simplified ternary value magnitude vertical direction 
information horizontal movement excluded 
method fail recognize facial appearance change involves horizontal movement facial features 
face facial hair glasses rigid head motion may encountered method correctly 
zhang hybrid approach face representation deal facial expression information extraction automatic way 
facial points fig 
set gabor wavelet coefficients extracted 
wavelets spatial frequencies orientations utilized 
zhang deal pixels frontal view images female japanese subjects manually normalized distance eyes pixels 
similar face representation lyons expression classification basic plus neutral emotion categories 
fiducial grid manually positioned nodes pixels images apply wavelets spatial frequencies angular orientations 
fig 

facial characteristic points 
facial data extraction static images feature methods second category surveyed methods automatic facial expression analysis static images uses analytic approach face representation table table applies feature method expression information extraction input image 
earlier kobayashi hara proposed geometric face model fig 

utilize ccd camera monochrome mode obtain set brightness distributions vertical lines crossing 
normalize input image affine transformation distance irises pixels 
distance irises length vertical lines empirically determined 
range acquired brightness distributions normalized data trained nn expression emotional classification 
shortcoming proposed face representation facial appearance changes encountered horizontal direction modeled 
real time system developed kobayashi hara works online taken images subjects facial hair glasses facing camera approximately distance 
pantic rothkrantz utilizing point model composed facial views frontal side view 
frontal view face model composed features 
features defined correspondence set facial points fig 
rest specific shapes mouth chin 
utilized side view face model consists profile points correspond peaks valleys curvature profile contour function fig 

localize contours prominent facial features extract model features input dual view pantic rothkrantz apply multiple feature detectors pantic rothkrantz automatic analysis facial expressions state art fig 

facial points frontal view face model side view face model 
prominent facial feature eyebrows eyes nose mouth profile 
example localize eyes methods proposed 
best acquired redundant results chosen 
done knowledge facial anatomy check correctness result certain detector confidence performance specific detector assigned testing results 
performance detection scheme tested dual views 
human observers percent approved visually inspected achieved localization facial features 
system deal minor inaccuracies extracted facial data deals merely images faces facial hair glasses 
zhao utilize point frontal view face model deal automatic facial expression data extraction 
utilize facial distances manually measure images selected facial emotion database assembled ekman friesen 
data expression emotional classification 
facial data extraction image sequences template methods category surveyed approaches automatic facial expression analysis image sequences uses holistic hybrid approach face representation table table applies template method facial expression information extraction input image sequence 
black yacoob local parameterized models image motion facial expression analysis 
utilize affine planar affine plus curvature flow model 
planar model represent rigid facial motions 
motion plane stabilize frames examined image sequence motions facial features estimated relatively stabilized face 
nonrigid motions facial features local facial areas eyebrows eyes mouth fig 
represented affine plus curvature model 
recover parameters flow models robust regression scheme brightness constancy assumption employed 
cope large motions fig 

planar model representing rigid face motions affine model representing nonrigid facial motions 
coarse fine gradient descent strategy 
approach proposed black yacoob initial regions head facial features selected hand automatically tracked 
applying adapted gradient optical flow algorithm ohya estimating motion local facial areas right eye mouth fig 

input facial images taken camera mounted helmet worn subject subsampled directions 
optical flow algorithm applied fourier transform utilized horizontal vertical velocity field lower frequency coefficients extracted feature vector facial expression emotional classification 
ohya advantage face symmetry estimating motion just local facial areas right eye mouth 
consequence method sensitive unilateral appearance changes left eye 
essa pentland utilizing hybrid approach face representation :10.1.1.40.359
applied eigenspace method automatically track face scene section extract positions eyes nose mouth 
method extracting prominent facial features employs eigenfeatures approximated pca sample images 
eigenfeatures define socalled feature space detect location prominent facial features image distance feature image relevant feature space computed fft local energy computation 
extracted position prominent facial features normalize input image 
spatiotemporal motion energy representation facial motion estimated consecutive normalized frames dynamic face model 
essa pentland optical flow computation method proposed simoncelli 
approach uses multiscale coarse fine kalman filter obtain motion estimates error covariance information 
method computes mean velocity vector represents estimated flow consecutive normalized facial images video sequence 
flow covariances different frames stored ieee transactions pattern analysis machine intelligence vol 
december fig 

motion vector field represented deformation grids 
recursive continuous time kalman filter calculate error predictions previous data obtain corrected noise free motion field 
method applied frontal view facial image sequences 
system hybrid approach face representation utilized proposed kimura yachida 
utilizing potential net 
fit potential net normalized facial image see section compute edge image applying differential filter 
order extract external force smooth gradient edge image applying gaussian filter 
filtered image referred potential field elastic net model fig 
placed 
net deforms governed elastic force potential field 
method suitable facial action encoding extracted deformations net 
kimura yachida discussed issue 
wang hybrid approach face representation 
utilize facial feature points preserve local topology depicted fig 
facial expression recognition 
treated nodes labeled graph interconnected links representing euclidean distance nodes 
links weighted empirically set parameters denoting properties facial features belong 
example links mouth nodes weighted smaller weight mouth deform 
initial location frame input image sequence assumed known 
track rest frames wang system consists layers memory layer fig 

potential field corresponding potential net 
input layer 
correspondence tracked consecutive frames treated labeled graph matching problem proposed buhmann antecedent frame treated memory layer current frame input layer 
graph matching realized dynamic process node diffusion minimizes cost function simulated annealing procedure 
observed faces facial hair glasses rigid head motion may encountered frame examined image sequence represent face marked frame method correctly 
face model wang represents way improving labeled graph models include intensity measurement encountered facial expressions information stored links nodes 
facial data extraction image sequences feature methods surveyed methods automatic facial expression analysis image sequences utilizes analytic face representation table table applies feature method facial expression information extraction 
cohn model facial landmark points localized facial features hand marked mouse device frame examined image sequence 
rest frames hierarchical optical flow method track optical flows windows surrounding landmark points 
displacement landmark point calculated subtracting normalized position frame current normalized position frames input fig 


pantic rothkrantz automatic analysis facial expressions state art sequence manually normalized 
displacement vectors calculated initial peak frame represent facial information recognition displayed facial actions 
face facial hair glasses rigid head motion may encountered frame face facial landmark points marked frame method correctly 
facial expression classification step facial expression analysis classify identify interpret facial display conveyed face 
surveyed facial expression analyzers classify encountered expression extracted facial expression information particular facial action particular basic emotion 
systems perform 
independent classification categories mechanism classification applied particular surveyed expression analyzer template neural network rule classification method 
applied methods expression classification terms facial actions summarized table 
table summarizes utilized methods facial expression emotional classification 
template classification method applied encountered facial expression compared templates defined expression category 
best match decides category shown expression 
general difficult achieve template quantified recognition facial expression 
infinitely lot combinations different facial actions intensities modeled finite set templates 
problem difficult due fact everybody maximal intensity displaying certain facial action 
neural networks represent black box approach arguably classified methods classifying neural networkbased methods separately 
doing typical neural network perform quantified facial expression categorization multiple classes table facial expression classification terms facial actions general template methods achieve performance 
neural network classification approach facial expression classified categorization process network learned training phase 
neural network classification methods utilized surveyed systems perform facial expression classification single category 
recognition facial expressions feasible neural network output unit associated weight interval proposed associated 
seen table classified expression classifiers methods utilize neural network 
doing characteristics methods fit better properties template expression classification approaches 
rule classification methods utilized surveyed systems classify examined facial expression basic emotion categories previously encoded facial actions table table 
expressions characterize emotion categories described terms facial actions 
shown expression described terms facial actions compared expressions defined emotion categories classified optimal fitting category 
expression classification static images template methods category surveyed methods automatic expression analysis static images applies template method expression classification 
methods category perform expression classification single basic emotion category 
new example face extracted parameters aam section main aim edwards identify observed individual way invariant confounding factors pose facial expression 
achieve goal ieee transactions pattern analysis machine intelligence vol 
december utilized distance measure representative set training facial images 
classifier assumes variation pose expression similar individual 
edwards linear discriminant analysis lda separate linearly variability identity variability 
showed subspace constructed orthogonal matrix matrix orthogonal vectors describing principal types variation models variation due change pose expression lightning 
expression recognition performance aam trained tested images basic emotional expressions shown subjects 
images chosen limited pose lightning variation 
achieved recognition rate basic neutral emotion categories percent 
edwards explain low recognition rate limitations utilized linear classifier 
known method behave case unknown subject 
table methods facial expression emotional classification achieve expression classification basic plus neutral emotion categories hong assumption persons look alike similar way showing expression 
fit labeled graph fig 
input facial image section 
best matching person personalized gallery available applying method elastic graph matching proposed 
personalized galleries people utilized gallery contained images images expression 
personalized gallery best matching person judgement category observed expression 
method tested images subjects 
achieved recognition rate percent case familiar subjects percent case unknown persons 
indicated hong availability personalized galleries individuals probably increase system performance 
time necessary performing full analysis incoming facial image seconds 
pantic rothkrantz automatic analysis facial expressions state art fig 

aps reprinted permission academic press academic press 
order perform emotional classification observed facial expression huang huang perform intermediate step calculating action parameters aps fig 

difference model feature parameters figs 
face examined facial expression person generates aps 
experimentally terms eigenvalues represent percent aps variations 
minimum distance classifier cluster principal action parameters training image samples clusters representing basic emotional expressions 
principal component distribution expression overlapped distribution expressions best matches selected 
highest score correlation determines final classification examined expression 
proposed method tested images shown subjects 
achieved correct recognition ratio percent 
known method behave case unknown subjects 
descriptions emotional expressions terms facial actions incomplete 
example expression lowered mouth corners raised eyebrows classified sadness 
fiducial grid manually positioned nodes fig 
lyons sample amplitude complex valued gabor transform coefficients combine data single vector call labeled graph vector lg vector 
ensemble lg vectors training set images subjected pca 
ensemble lg pca vectors training set analyzed lda order separate vectors clusters having different facial attributes 
lyons experimented binary classifiers presence absence particular facial expression 
built binary classifiers basic emotion category combined single facial expression classifier 
input lg vector classified projected discriminant vectors calculated independently trained binary classifier 
input image positively classified emotion categories normalized distances cluster centers deciding factor 
input sample classified member nearest cluster 
input image positively classified category classified neutral test method lyons set images different facial expressions displayed japanese females zhang see section 
entire set images divided segments discriminant vectors calculated segments generalization performance tested remaining segment results averaged distinct partitions 
generalization rate percent 
method tested image set partitioned segments corresponding 
generalization rate percent recognition expression novel subject 
extract facial movement parameters describe change face currently examined facial expression subject section 
recognize types expressions sadness surprise anger happiness bits represent values parameters movement identical discrete hopfield networks 
network consists neurons neurons added form neurons square initial value 
net nn trained data representing expressions shown subjects 
nn trained just data representing clearly shown expressions 
nns trained personnaz learning rule 
examined image output nn matched examples training nn euclidean distances calculated 
distances averaged expression 
difference minimal average second minimal average greater category examined expression decided 
output nn matched examples training nn order decide final category shown expression 
average recognition rate percent 
images training networks testing 
expression classification static images neural network methods second category surveyed methods automatic facial expression analysis static images applies neural network facial expression classification 
method proposed zhang methods belonging category perform facial expression classification single basic emotion category 
classification expression basic emotion categories hara kobayashi apply back propagation neural network 
units input layer correspond number brightness distribution data extracted input facial image section unit output layer ieee transactions pattern analysis machine intelligence vol 
december corresponds emotion category 
neural network trained images basic facial expressions shown subjects tested set facial expressions images shown subjects 
average recognition rate percent 
process takes ms 
emotional classification input facial image basic plus neutral emotion categories cottrell utilize back propagation neural network 
input network consists normalized projection pixel blocks principal components previously generated eigenspace section 
hidden layer nn contains nodes employs nonlinear sigmoid activation function 
output layer nn contains units corresponds emotion category 
cottrell images basic plus neutral expressions shown subjects 
trained network images subjects tested images th subject 
changing training testing set trained networks 
average correct recognition rate achieved percent 
zhang employ neural network facial expression classification basic plus neutral emotion categories 
input network consists geometric position facial points fig 
gabor wavelet coefficients sampled point 
neural network performs nonlinear reduction input dimensionality statistical decision category observed expression 
output unit gives estimation probability examined expression belonging associated category 
network trained resilient propagation 
set images different expressions displayed japanese females train test network 
database partitioned segments 
segments train network remaining segment test recognition performance 
process repeated segments results trained networks averaged 
achieved recognition rate percent 
performance network tested recognition expression novel subject 
zhao kearney utilize backpropagation neural network facial expression classification basic emotion categories 
images basic facial expressions selected emotion database assembled ekman friesen 
image distances manually measured 
difference distance measured examined image distance measured face person normalized 
measure mapped signaled intervals appropriate standard deviation corresponding average 
intervals formed input nn 
output nn represents associated emotion string represent happiness 
nn trained tested set data images percent recognition rate 
known method behave case unknown subject 
expression classification static images rule methods just surveyed methods automatic facial expression analysis static images applies rule approach expression classification 
method proposed pantic rothkrantz achieves automatic facial action coding input facial dual view steps 
processing system performs automatic detection facial features examined facial image section 
localized contours facial features model features fig 
extracted 
difference calculated currently detected model features features detected face person 
knowledge acquired facs production rules classify calculated model deformation appropriate aus classes total number classes 
performance system automatic facial action coding dual view images tested set dual views expressions separate facial actions shown twice human experts 
average recognition rate percent upper face aus percent lower face aus 
classification input facial dual view multiple emotion categories performed comparing au coded description shown facial expression au coded descriptions basic emotional expressions acquired linguistic descriptions ekman 
classification quantification resulting emotion labels assumption subexpression basic emotional expression influence scoring emotion category 
performance system tested set dual facial views representing basic various blended emotional expressions shown subjects 
correct recognition ratio percent reported 
dual views testing system recorded constant illumination subjects beard wear glasses 
expression classification image sequences template methods expression analysis facial image sequences applies template method expression classification 
facial action recognition method proposed cohn applies separate discriminant function analyzes facial regions eyebrows eyes mouth 
predictors facial points displacements section initial peak frames input image sequence 
separate group variance covariance matrices classification 
image sequences containing facial actions displayed subjects 
images recorded constant illumination fixed light sources subjects wear glasses 
data randomly divided training test sets image sequences 
discriminant functions facial actions eyebrow region discriminant functions facial actions eye region pantic rothkrantz automatic analysis facial expressions state art fig 

spatio temporal template surprise :10.1.1.40.359
discriminant functions facial actions nose mouth region 
accuracy classification percent eyebrow region percent eye region percent nose mouth region 
method proposed cohn deals image sequences containing facial actions row inaccurate facial data facial action intensity concept method possible 
essa pentland control theoretic method extract spatio temporal motion energy representation facial motion observed expression section :10.1.1.40.359
learning ideal motion views expression category generated spatio temporal templates fig 
different expressions facial actions smile raised eyebrows emotional expressions surprise sadness anger disgust 
template delimited averaging patterns motion generated subjects showing certain expression 
euclidean norm difference motion energy template observed image motion energy metric measuring similarity dissimilarity 
tested frontal view image sequences people showing distinct expressions correct recognition rate percent achieved 
kimura yachida fit potential net frame examined facial image sequence section fig 

pattern deformed net compared pattern extracted face frame sequence variation position net nodes processing 
kimura yachida built emotion space applying pca image sequences expressions anger happiness surprise shown single person gradually maximum 
eigenspace spanned principal components emotion space input image projected quantified emotional classification 
proposed method unsuccessfully tested image sequences unknown subjects 
small number training examples sequences insufficient diversity subjects person probably caused 
ohya match temporal sequence feature vector section models basic facial expressions left right hidden markov model 
hmm consists states relaxed contracted apex relaxing 
facilitate recognition single image sequence transition final state initial state added 
recognition multiple sequences expression images feasible transition final state initial states categories added 
transition probability output probability state obtained sample data baum welch algorithm 
initial probability estimated applying means clustering algorithm sample data squared sum vector components added extra component 
hmm trained image sequences shown male subjects 
method tested image sequences shown subjects 
known method behave case unknown 
ohya claim recognition performance define extent wang utilize points labeled graph weighted links represent face section fig 

twelve points expression recognition 
emotion categories anger happiness surprise average spline curves construct expression model 
curve describes relationship expression change displacement corresponding 
expression model defined image sequences displayed subjects 
category expression decided determining minimal distance actual trajectory trajectories defined models 
distance functions minimized method proposed brent 
degree expression change determined displacement consecutive frames 
method tested image sequences emotional expressions shown subjects young asian ethnicity 
images acquired constant illumination subjects beard wear glasses 
average recognition rate percent 
average process time frame 
expression classification image sequences rule methods just surveyed methods automatic facial expression analysis image sequences applies rulebased approach expression classification 
black yacoob utilize local parameterized models image motion represent rigid head motions nonrigid facial motions local facial areas fig 

motion parameters translation divergence derive predicates describe motion facial features 
predicate represented form rule left part rule comparison motion parameter certain threshold right part rule derived predicate 
thresholds dependent face size image set empirically sequences 
black yacoob give full list predicates number different facial actions method recognize known 
method facial expression emotional classification considers temporal consistency representation predicates 
ieee transactions pattern analysis machine intelligence vol 
december basic emotional expressions developed model represented set rules detecting expression 
rules applied predicates representation 
method tested image sequences containing expressions shown subjects ranged ethnicity age 
expressions displayed time 
achieved recognition rate percent 
lip biting mistakenly identified smile 
method deal blends emotional expressions 
example blend angry scared expressions recognized disgust 
reason lies rules classification 
discussion explored compared number different approaches facial expression detection classification static images image sequences 
investigation compared automatic expression information extraction facial motion analysis holistic spatial pattern analysis analysis facial features spatial arrangement :10.1.1.40.359
investigation compared facial expression classification holistic spatial analysis holistic spatiotemporal analysis gray level pattern analysis local spatial filters analytic feature spatial analysis :10.1.1.40.359
number surveyed systems large reader interested results performed comparison terms best performances 
deliberately didn attempt label surveyed systems better systems literature 
believe defined commonly single database testing images image sequences necessary prerequisite ranking performances proposed systems objective manner 
single testing data set established left reader decide ranking surveyed systems priorities properties surveyed systems tables 
detection face features currently existing systems facial expression analysis assume presence face scene ensured 
instances systems utilize camera setting ascertain correctness assumption 
surveyed systems process images acquired mounted camera systems deal automatic face detection arbitrary scene :10.1.1.40.359
addition instances strong assumptions problem facial expression analysis tractable table 
common assumptions images contain frontal facial view illumination constant light source fixed face facial hair glasses subjects young permanent wrinkles ethnicity 
real life situations assumed observed subject remain assumed methods 
fixed camera acquires images system capable dealing rigid head motions 
surveyed systems deal extent rigid head motions 
sake universality system able analyze facial expressions person independently age ethnicity outlook 
method proposed essa pentland deals facial images faces facial hair :10.1.1.40.359
researchers automated vision facial expression analysis suggest investigating robust detection face features despite changes viewing lightning conditions distractions glasses facial hair changes hair style 
interesting investigated ability human visual system filling missing parts observed face perceiving face part occluded hand 
facial expression classification general existing expression analyzers perform singular classification examined expression basic emotion categories proposed ekman friesen 
approach expression classification main limitations 
pure emotional expressions seldom elicited 
time people show blends emotional expressions 
classification expression single emotion category isn realistic 
automated facial expression analyzer realize quantified classification multiple emotion categories 
surveyed systems perform quantified facial expression classification multiple basic emotion categories 
second certain facial expressions displayed face classified basic emotion categories 
expression analyzer performs quantified expression classification multiple basic emotion categories probably capable interpreting encountered expression 
psychological discussion topic 
experimental proofs studies asian researchers reported asian subjects difficulties express basic expressions disgust fear 
defining interpretation categories facial expression classified key challenges design realistic facial expression analyzer 
lack psychological scrutiny topic problem harder 
way dealing problem build system learns expertise allows user define interpretation categories see 
system behavioral science investigations face perform expression recognition applied automated facs encoding 
words accomplish facial action coding input images quantification codes 
surveyed systems perform facial action coding input image image sequence table 
systems performs quantification facial action codes 
task particularly difficult accomplish number reasons 
merely different aus facs provides option score intensity level intensity scale low medium pantic rothkrantz automatic analysis facial expressions state art high 
second facial actions blink lips mouth encountered 
reasonable talk blink having higher intensity blink 
addition person maximal intensity displaying particular facial action 
aimed design system start generic facial action classification adapt particular individual perform quantification accomplished coding facial actions measuring activation intensity reasonable surveyed systems distinguish aus defined facs 
remains key challenge researchers automated facs encoding 
appealing investigated property human visual system assigning higher priority upper face features lower face features play important role facial expression interpretation 
analysis facial expressions intriguing problem humans solve quite apparent ease 
identified different related aspects problem face detection facial expression information extraction facial expression classification 
capability human visual system solving problems discussed 
serve point automatic vision system attempting achieve functionality 
problems facial expression classification studied due utility application domains human behavior interpretation hci 
surveyed systems frontal view images faces facial hair glasses unrealistic expect application domains 
proposed approaches automatic expression analysis perform facial expression classification basic emotion categories defined ekman friesen 
unrealistic certain facial expressions able displayed face classified basic emotion categories 
furthermore surveyed methods tested set images training 
hesitate belief systems turn basic property behavioral science research tool advanced hci 
discussed problems intriguing solved general case 
expect remain interesting researchers automated facial expression analysis time 
acknowledgments authors lyons providing high quality images dr kevin bowyer anonymous reviewers helping improve 
ieee press academic press granting permission reprint figures appearing 
facial motion perception faces emotional expression experimental psychology pp 

black yacoob recognizing facial expressions image sequences local parameterized models image motion lnt computer vision vol 
pp 

black yacoob tracking recognizing rigid non rigid facial motions local parametric models image motions proc 
int conf 
computer vision pp 

boyle anderson effects visibility dialogue cooperative problem solving task language speech vol 
pp 

bruce recognizing faces 
hove east sussex lawrence erlbaum assoc 
bruce human face tells human mind challenges robot human interface proc 
int workshop robot human comm pp 

buhmann lange von der malsburg distortion invariant object recognition matching hierarchically labelled graphs proc 
int joint conf 
neural networks pp 

campbell information falling retina reaches visual cortex stored memory seminar varia 
chen huang multimodal human emotion expression recognition proc 
int conf 
automatic face gesture recognition pp 

cohn lien kanade feature point tracking optical flow discriminates subtle differences facial expression proc 
int conf 
automatic face gesture recognition pp 

cootes taylor cooper graham active shape models training application computer vision image understanding vol 
pp 

cootes edwards taylor active appearance models proc 
european conf 
computer vision vol 
pp 

cottrell metcalfe emotion gender recognition holons advances neural information processing systems lippman ed pp 

decarlo metaxas stone anthropometric face model variational techniques proc 
siggraph pp 

de silva facial emotion recognition multimodal information proc 
information comm signal processing conf pp 

donato bartlett hager ekman sejnowski classifying facial actions ieee trans 
pattern analysis machine intelligence vol 
pp 
oct 
bartlett hager ekman sejnowski measuring facial expressions computer image analysis psychophysiology vol 
pp 

edwards cootes taylor face recognition active appearance models proc 
european conf 
computer vision vol 
pp 

girod analysing facial expressions virtual conferencing ieee trans computer graphics applications vol 
pp 

ekman friesen face 
new jersey prentice hall 
ekman friesen facial action coding system facs manual 
palo alto consulting psychologists press 
ekman emotion human face 
cambridge univ press 
ellis process underlying face recognition neuropsychology face perception facial expression ed 
pp 
new jersey lawrence erlbaum assoc 
essa pentland coding analysis interpretation recognition facial expressions ieee trans :10.1.1.40.359
pattern analysis machine intelligence vol 
pp 
july 
ekman facial expressions emotion review literature nonverbal behavior communication eds pp 

hillsdale nj lawrence erlbaum assoc 
ieee transactions pattern analysis machine intelligence vol 
december evolution facial action reflex social motive biological psychology vol 
pp 

hand discrimination classification 
john wiley sons 
hara kobayashi state art component development interactive communication humans advanced robotics vol 
pp 

holt huang netravali qian determining articulated motion perspective views pattern recognition vol 
pp 

hong neven von der malsburg online facial expression recognition personalized galleries proc 
int conf 
automatic face gesture recognition pp 

horn schunck determining optical flow artificial intelligence vol 
pp 

huang huang facial expression recognition model feature extraction action parameters classification visual comm 
image representation vol 
pp 

izard face emotion 
new york appleton century crofts 
izard facial expressions regulation emotions personality social psychology vol 
pp 

scherer acoustic profiles prototypical vocal expressions emotions proc 
int conf 
phonetic science vol 
pp 

sompolinsky associative recall memory errors physical review vol 
pp 

kass witkin terzopoulos snake active contour model proc 
int conf 
computer vision pp 

kato nakamura description synthesis facial expressions maps visual computing kunii ed pp 

tokyo springer verlag 
yamada emotion space interactive communication proc 
computer science conf pp 

kearney mckenzie machine interpretation emotion design memory expert system interpreting facial expressions terms signaled emotions janus cognitive science vol 
pp 

kimura yachida facial expression recognition degree estimation proc 
computer vision pattern recognition pp 

kobayashi hara facial interaction animated face robot human beings proc 
int conf 
systems man cybernetics pp 

kobayashi hara recognition basic facial expressions strength neural network proc 
int workshop robot human comm pp 

kobayashi hara recognition mixed facial expressions neural network proc 
int workshop robot human comm pp 

lam yan analytic holistic approach face recognition single frontal view ieee trans 
pattern analysis machine intelligence vol 
pp 
july 
lee kim hmm threshold model approach gesture recognition ieee trans 
pattern analysis machine intelligence vol 
pp 
oct 
li motion estimation model facial image coding ieee trans 
pattern analysis machine intelligence vol 
pp 

lien kanade cohn li automated facial expression recognition facs action units proc 
int conf 
automatic face gesture recognition pp 

lucas kanade iterative image registration technique application stereo vision proc 
joint conf 
artificial intelligence pp 

lyons akamatsu coding facial expressions gabor wavelets proc 
int conf 
automatic face gesture recognition pp 

lyons akamatsu automatic classification single facial images ieee trans 
pattern analysis machine intelligence vol 
pp 

mase recognition facial expression optical flow ieice trans vol 
pp 

nakamura matsui mathematical representation image generation human faces metamorphosis electronics comm 
japan vol 
pp 

lee tsuji recognition facial expression potential net proc 
asian conf 
computer vision pp 

communication words psychology today vol 
pp 

yamada modelling facial expression emotion recognition synthesis symbiosis human artifact anzai ogawa mori eds pp 
amsterdam elsevier science bv 
moses reynard blake determining facial expressions real time proc 
int conf 
automatic face pp 

creation new medium multimedia era proc 
ieee vol 
pp 

ohya recognition facial expressions hmm continuous output probabilities proc 
int workshop robot human comm pp 

ohya spotting segments displaying facial expression image sequences hmm proc 
int conf 
automatic face gesture recognition pp 

cottrell representing face images emotion classification proc 
conf 
advances neural information processing systems pp 

pantic rothkrantz expert system automatic analysis facial expression image vision computing vol 
pp 

pantic rothkrantz expert system multiple emotional classification facial expressions proc 
int conf 
tools artificial intelligence pp 

pavlovic sharma huang visual interpretation hand gestures human computer interaction review ieee trans 
pattern analysis machine intelligence vol 
pp 

pentland moghaddam starner view modular eigenspaces face recognition proc 
computer vision pattern recognition pp 

emotion speech recognition application call centers proc 
conf 
artificial neural networks eng 
picard offline online recognition emotion expression physiological data emotion agent architectures workshop notes int conf 
autonomous agents pp 

polzin waibel detecting emotions speech proc 
conf 
cooperative multimedia comm 
press teukolsky vetterling flannery numerical recipes cambridge univ press 
wilson neural network approach component versus holistic recognition facial expressions images spie intelligent robots computer vision algorithms techniques vol 
pp 

issues fuzzy linguistic modeling proc 
conf 
fuzzy systems pp 

riedmiller braun direct adaptive method faster backpropagation learning rprop algorithm proc 
int conf 
neural networks pp 

rosenblum yacoob davis human emotion recognition motion radial basis function network architecture proc 
ieee workshop motion non rigid articulated objects pp 

rowley baluja kanade neural network face detection ieee trans 
pattern analysis machine intelligence vol 
pp 
jan 
psychology facial expression russell fernandez eds 
cambridge cambridge univ press 
russell universal recognition emotion facial expression psychological bulletin vol 
pp 

ekman strong evidence universals facial expressions reply russell mistaken critique psychological bulletin vol 
pp 

pantic rothkrantz automatic analysis facial expressions state art samal minimum resolution human face detection identification spie human vision visual processing digital display ii vol 
pp 

samal iyengar automatic recognition analysis human faces facial expressions survey pattern recognition vol 
pp 

simoncelli distributed representation analysis visual motion phd thesis massachusetts inst 
technology 
neven fast robust system human detection tracking recognition proc 
int conf 
automatic face gesture recognition pp 

stephenson role visual communication social exchange britain social clinical psychology vol 
pp 

sung poggio example learning view human face detection ieee trans 
pattern analysis machine intelligence vol 
pp 
jan 
takeuchi nagao communicative facial displays new conversational modality proc 
acm interchi pp 

david akamatsu automatic detection human faces natural scene images skin color model invariant moments proc 
int conf 
automatic face gesture recognition pp 

terzopoulos waters analysis synthesis facial image sequences physical anatomical models ieee trans 
pattern analysis machine intelligence vol 
pp 
june 
thalmann thalmann indexed computer animation visualisation computer animation vol 
pp 

thalmann escher face virtual face proc 
ieee vol 
pp 

thalmann direct face face communication real virtual humans int information technology vol 
pp 

life communication agent emotion sensing character mic feeling session character muse proc 
conf 
multimedia computing systems pp 

turk pentland eigenfaces recognition cognitive neuroscience vol 
pp 

takagi yamaguchi recognition facial expressions conceptual fuzzy sets proc 
conf 
fuzzy systems vol 
pp 

haken applications decoding facial expression proc 
int conf 
automatic face gesture recognition pp 

vincent myers hutchinson image feature location multi resolution images hierarchy multi layer neural networks speech vision natural language pp 
chapman hall 
wang yachida expression recognition time sequential facial images expression change model proc 
int conf 
automatic face gesture recognition pp 

williams shah fast algorithm active contours curvature estimation computer vision image processing image understanding vol 
pp 

wilson bobick parametric hidden markov models gesture recognition ieee trans 
pattern analysis machine intelligence vol 
pp 
sept 
wiskott labelled graphs dynamic link matching face recognition scene analysis reihe physik vol 
frankfurt 
main verlag deutsch 
wu yachida face facial feature extraction color image proc 
int conf 
automatic face gesture recognition pp 

yacoob davis recognizing facial expressions spatio temporal analysis proc 
int conf 
pattern recognition vol 
pp 

yacoob davis computing spatio temporal representations human faces proc 
computer vision pattern recognition pp 

yamada visual information categorizing facial expressions emotions applied cognitive psychology vol 
pp 

yang waibel real time face tracker workshop applications computer vision pp 
pp 

shirai facial expressions recognition discrete hopfield neural networks proc 
int conf 
information processing vol 
pp 

yuille cohen hallinan feature extraction faces deformable templates proc 
computer vision pattern recognition pp 

zhang lyons schuster akamatsu comparison geometry gabor wavelets facial expression recognition multi layer perceptron proc 
int conf 
automatic face gesture recognition pp 

zhao kearney classifying facial emotions backpropagation neural networks fuzzy inputs proc 
conf 
neural information processing vol 
pp 

maja pantic received ms degree computer science cum laude department computer science faculty technical mathematics computer science delft university technology netherlands 
phd candidate computer science department computer science faculty information technology systems delft university technology netherlands 
research interests areas multimedia multimodal man machine interfaces artificial intelligence including knowledge systems distributive ai machine learning applications artificial intelligence intelligent multimodal user interfaces 
student member ieee 
leon rothkrantz received msc degree mathematics university utrecht netherlands phd degree mathematics university amsterdam netherlands msc degree psychology university leiden netherlands 
associate professor joined knowledge systems group faculty technical mathematics computer science delft university technology netherlands 
long range goal dr rothkrantz research design development human psychology anthropomorphic multimodal multimedia fourth generation man machine interface 
interests include applying computational technology analysis aspects human behavior 
