stochastic text generation jon oberlander chris brew human communication research centre division informatics university edinburgh edinburgh uk suggests statistical revolution left natural language generation nlg largely untouched identi es areas interacting constraints hard model traditional methods probabilistic models er 
interesting discourse phenomenon discussed show dealt handful existing statistical approaches nlg identify diculties evaluation functions 
argues maximum entropy framework ers better approach suggests applied problems generation heuristics help evaluator exploit features improve quality output language 
keywords natural language generation discourse structure statistical methods maximum entropy modelling 
natural language generation nlg research aims systems produce coherent natural language text underlying representation knowledge 
systems produce language single sentences complex discourses faithfully represents relevant knowledge ii way 
argued goals delity naturalness pull di erent directions certainly interesting language phenomena easier study generator point view common perspective interpretation understanding 
nlg proved largely exempt statistical revolution overrun eld computational linguistics 
rst discusses revolution left nlg untouched identi es areas probabilistic models er 
identifying areas may able prevent nlg research progressively bogged increasingly complex algorithms attempting juggle interacting constraints 
focuses interesting discourse phenomenon discussing dealt handful statistical approaches date identifying diculties evaluation functions 
maximum entropy framework ers better evaluation functions successfully take account interacting constraints 
sketches applied problems generation indicates heuristics generator trained exploit interesting features improve quality generation 
article submitted royal society oberlander brew 
understanding generation statistics generation missed statistical revolution 
see rst review advantages accrue statistics natural language understanding nlu consider advantages super cially relevant generation systems 
nlu speech recognition parsing semantic analysis require search large spaces possibilities 
argued fact unable unwilling codify necessary facts language means unable carry necessary disambiguation symbolic means 
point call statistics approximate knowledge lack choosing lucky correct alternative 
statistics guide search bridge knowledge gaps number striking useful properties 

extent correct analysis chosen downstream components freed obligation worrying non determinism 
simpli es system design possible applications demanding 

weaker version parse selection passes small set highly ranked analyses downstream components 
longer system design advantage handle non determinism volume computation required may markedly reduced 

possible machine learning techniques obtain useful statistical knowledge corpora 
especially useful knowledge question important performance little intrinsic interest resources needed provide hand encoding knowledge unavailable 

probability understood mechanism reasoning belief uncertainty viable option extend probabilistic systems extra probabilistic components need arises 
appropriate bring semantic pragmatic information bear done relatively straightforward manner 
importantly individual components understood probabilistic semantics may possible assign coherent probabilistic interpretation action entire system 
essence statistical methodology provides sound basis working dicult search problem bridging seriously problematic gap knowledge available system allowing applications possible 
turning natural language generation nd super cially different situation 
system usually knows exactly wants say generation matter constructing appropriate linguistic form pre speci ed content 
obvious gap knowledge system obvious need search 
generation systems highly modularised thompson argues distinction strategic tactical problems article submitted royal society stochastic text generation generation system deciding say say assumption generally ward puts normal case various decisions involved process generating sentence isolation 
ward 
assumption valid generation relatively easy task decompose small tractable subtasks 
mckeown uential text system subsequent carefully distinguished components algorithmic level architectures elaborates model retains pipelined sequence sub tasks cahill 
assumption isn justi ed meteer argued ward notes generators syntactic lexical options available generator able words syntactic constructions deal seriously interdependencies choices 
ward pp 
ward goes point addition new types knowledge language bring new subgoals naturalness goal 
ward 
examples cited ward include interaction prosody lexical choice cutler isard need 
generator attempting adopt particular literary style come having particular personality particular words phrases conversational strategies irrespective content conveyed 
satisfy goal delity system ensure states truths 
satisfy goal naturalness system try produce natural sounding text 
computer graphics counts natural realistic movable 
details care harder generate representation convincing 
multiple goals interacting choices look inherent property nlg task 
try ignore issues naturalness uency reduce number interacting constraints deal 
clear things nlg interesting furnishes opportunity study phenomena discourse structure style projection personality largely reach nlu 
concomitant fact may adequate formalisations knowledge principles need 
accept analysis acknowledge existence knowledge gaps nlg 

interacting constraints discourse diculties apparent complex real discourse gathered part ilex project analysed bateman 
example appears show interesting interaction delity goal inform listener set facts article submitted royal society oberlander brew naturalness goal produce conventional sounding uent text 
crucial point naturalness considerations merely say question interact say decisions jo say object 
lg 
roger morris teaching duncan college dundee wasn rst person develop idea combining precious metals rst 
rst german particular klaus 
morris followed quickly essentially technique exploring colour colour thing new time bold colours get 
note beats direct transition better turn history materials morris interesting thing morris free adjunct construction produced useful general strategy produce construction circumstances syntactic form construction fosters impression strong connection rst second parts semantics construction seriously constrain content expressed part 
context allows contribution play triple role 
needs bridge discontinuity topics 
topic concluded attributes credit invention assesses claims temporal priority goes details techniques materials 
mentions morris making salient techniques shortly mentioned strongly related morris immediately preceding discussion 

mentioning techniques materials sets context discussion lines 
subtle ective contribution generation systems able emulate 
generation system order kind contribution 
way build detailed computational model process content gives rise appropriate collateral material 
bateman sees problem terms content bears conjunctive relations crs immediate predecessor earlier segment contrasts types rhetorical structure analysis tries fold relations sticks deep nuclearity 
cr analysis capture keep surfacing behaviour 
getting operation right intellectual ort required 
bateman lot progress technical realisation approach entail considerable ort 
doubt sucient ingenuity particular example captured crs broader concern example required recruitment extra level discourse analysis example require level analysis 
article submitted royal society stochastic text generation bateman approach traditional way forward lies pole symbolic statistical axis 
pole simply generate material uncorrelated surrounding content language model choose highly rated utterance space possibilities 
technically easy misses regularities attractive discourse generation 

statistics bridge knowledge gaps nlg see consider langkilde knight collins miller detail 
knight nitrogen uses probabilistic model select analyses proposed non deterministic generator 
system architecture symbolic generator capable generating alternative answers word lattice produced symbolic generator statistical extractor capable unpacking evaluating alternative paths word lattice 
generator non deterministic input comes machine translation impoverished completely determine output may example lack information number case 
need language model particular properties architecture reported experiments simple gram models 
secondly collins miller give highly suggestive account process news items generated database rows 
motivation doing provide sound probabilistic model information extraction system 
system supposed reconstruct database rows textual input 
seen perspective language occurs noise contribute content slots need lled 
initially tempting suppose generation system just get away adopting collins miller model 
won wash classify interposed material news items noise purposes information extraction immaterial appears part text 
provided reliably distinguish noise slot llers catch slot llers leaving noise words uninterpreted 
information extraction domain lost modelling noise words really stochastic process inserts random words points 
note generation hypothesised process noise word insertion uenced operation rest generator 
inserted noise uncorrelated surrounding content 
reality constraints operate processes speaker chooses noise satis es textual stylistic collateral goals 
means collins miller process large classes texts extremely produced noise clearly identi able noise entirely inappropriate context 
article submitted royal society oberlander brew strength weakness 
hand get useful system need explicit model connection content collateral material 
hand bought setup erroneously insists connection aspects 
third option pursue generate noise stands heuristically appropriate relationship surrounding content 
instance schema continuity saying closely related generate lled contextually appropriate noise need require sampled unbiased estimate language way strings appear related preferred 
ensures chosen utterance follows smoothly immediate context evoking distant context want remind hearer 
simultaneously want sampled set strings appear related order second half free adjunct construction run smoothly continuation pattern impose strong constraints want measure continuity noticed mellish 
kind situation arises naturally nlg speakers typically trying satisfy multiple goals consequence utterances subject multiple possibly competing constraints 
order properly understand contribution text segment discourse usually need understand relationships participates 
relationship weak collins miller assumption segments modelled independent stretches noise may bad 
relationship segments gets stronger assumption realistic 
model means expressing fact pairs segments highly correlated 
knight gram model similar weaknesses provides basis extensions overcome diculty 

nitrogen langkilde knight point limitations bigrams means capturing dependencies discuss alternative approaches trigrams bigrams 
unfortunately produces small increase modelling power cost considerable worsening sparse data problem 
promising idea detail langkilde thesis proposal elaborate symbolic generator statistical extractor parse forests format penn treebank 
concrete proposal statistical extractor interpolate collins dependency model bigram model techniques proposed chelba jelinek 
concomitant changes symbolic generator allow article submitted royal society stochastic text generation produce treebank style trees strings 
extent long distance relationships suciently encoded bigrams trigrams treebank style trees nitrogen architecture want 
langkilde parse forest usual parse selection exempli ed collins parse forest produced base generator single input sentence lattice alternative texts 
problem studied arises input parser output speech recogniser 
worth making clear goals nitrogen successors adopt di er signi cantly mainstream goals nlg 
guarantee output correct goal produce output matter bad input output uent coherent possible 
langkilde 
attraction langkilde knight approach simplicity tractability primarily new existing technology parse selection string selection 
discourse level examples bateman probably elude model fall scope single parse tree 
fell scope nitrogen ort important rest nlg world 
wish address question achieve similar nitrogen done explicitly addressing larger goals nlg 
try extend nitrogen building formal probabilistic model process assume underly generation discourse 
model spell dependency related segments consequence relationship hidden variables encode choices adopted language user 
approach potential probabilistic perspective gives ready strategy managing trade soft constraints techniques available statistical literature especially bayesian networks heckerman :10.1.1.112.8434
applications bayesian networks method choice convenient computational knowledge engineering properties bayesian networks readily handle incomplete data allow learn causal relationships bayesian networks conjunction bayesian statistical techniques facilitate combination domain knowledge data 
heckerman :10.1.1.112.8434
standard example involves situation faced doctor chest clinic lauritzen spiegelhalter 
task appropriate inference symptom shortness breath observation chest ray risk factors smoking visits asia diseases cancer tuberculosis bronchitis crucially pattern dependencies variables known codi ed bayesian network 
crucial knowledge engineering advantage judgements dependencies easy elicit fairly reliable 
may additionally possible elicit rough judgements conditional probability tables experts infer parameters data 
article submitted royal society oberlander brew unfortunately comes discourse seen bateman example takes substantial insight get clear kinds hidden variables involved ambitious research programme 
alternative strategy pursue limited aims stronger probability immediate payo take view discourse focusing patterns word occurrence notably repetition paraphrase 
course known patterns useful cues various aspects document structure hearst katz 
bateman account roger morris example predicts speakers try evident particular segment segment relationships 
may patterns repetition paraphrase achieve goal 
may dicult give precise account hidden causes regularities established technology constructing probability distributions respect particular surface regularities della pietra 
maximum entropy modelling framework particular instantiation method learning conditional exponential models form jx berger della pietra rosenfeld 
section begins brief high level description framework move key question arises applying framework nlg modelling problem speci cation large class features considered search appropriate probability models 

maximum entropy modelling maximum entropy framework maximum entropy modelling learning technique constructs probabilistic models drawn highly general class log linear models 
details technique known need rehearsal 
purposes suces outline essential features 
assume availability quantity training data 
refer training data collection instances 
feature space speci cation user speci es large set features deemed relevant problem hand 
readers moment think features arbitrary predicates instances appear training 
way specify sets templates done ibm machine translation berger 
assume strategy ease exposition 
feature space de ned idea provide space features large cover regularities arise training test data 
stage economy tutorial material maximum entropy modelling textbooks jelinek manning sch 
single useful source material time www cs cmu edu maxent html berger 
treat case described feature functions binary valued indicator functions 
della pietra 
describe general version technique arbitrary non negative feature functions permissible associated computational cost 
don know severe extra cost stuck binary valued feature functions article submitted royal society stochastic text generation undesirable necessarily need choose right features feature selection dealt stages 
feature selection second part maximum entropy modelling process feature selection 
maximum entropy framework applies conditional exponential models form yjx binary valued indicator function picking measurable property training test instances 
feature selection task narrow large set potentially relevant features smaller set features model data hand 
associated model set features indexed set weights indexed feature selection proceeds initial set active features usually empty set greedily adding features bring model closer observed data 
greediness algorithm practical motivated primarily computational concerns 
ii parameter estimation third part maximum entropy modelling process parameter estimation 
consists search optimum values weights interleaved feature selection 
various computational approach practical instance space number features involved large 
features nlg course claimed unnecessary specify right features maximum entropy modelling statistical technique vulnerable ects training distorting ects limited training data 
choice feature templates determine success modelling technique section describe ideas features relevant nlg 
tested ideas welcome suggestions appropriate features endeavour 
heuristics nding features 
look properties believe characteristic uent coherent text look properties know characteristic mistakes current generation systems 
second heuristic easier apply ers clear prospects improvement 
features came rst heuristic maximum entropy technique user parts dicult see 
article submitted royal society oberlander brew grams nitrogen experience clearly suggests conventional gram features important 
langkilde knight argument excessive cost making move trigrams apply framework feature selection mechanism select higher order grams signi cant di erence likelihood training data 
ngrams represented style contextual features ibm statistical translation berger 
essence describe pattern occurrences target word order pattern hold 
extended grams presume training data part speech tagged tokenized named entity recognition system annotated free stipulate templates properties annotations 
word frequency especially generators aim achieve stylistic ects may matter frequent word construct predicates bins histogram word frequencies 
simply wanted achieve particular score reading ease scale may important class features 
sentence length sentence length important reason word frequency 
subsequent mention get rough handle nature text building predicates take account words rst subsequent mention text 
suspect feature uninteresting speculate may interact interesting ways features reasons mentioned katz noted probability repetition word corpus independent frequency language word introduced need longer keep track rare frequent 
type token ratio wealth personality psychological sociological data type token ratios 
instance generally perceived competence speaker correlates ratio personality variables implicated berry 
don know aggregate properties texts framework suggest measure distance occurrence current focus word proxy type token ratio 
turn second heuristic exploits know limitations current nlg systems 
sentence repetition conjunction linking uenced generator decision aggregate diagnosed repetition predicate subject verb 
rate pronoun claimed current algorithms overestimate levels pronoun poesio 
article submitted royal society stochastic text generation np length claimed algorithms referring expressions grice maxims generate nps fewer predicates people dale reiter oberlander 
inter class repetition distance resource wordnet measure distance occurrence class class cuts appropriate point hierarchy 
give handle bridging contexts 
certainly don expect able features listed 
weakness mentioned della pietra 
currently possible specify prior distributions space features considered 
facility available give somewhat delicate option designer models currently limited purely datadriven 

availability training data nlg advantage nlu 
output generator possibly post processing human informants training material data augmented annotated unannotated training material sources notably standard text corpora british national corpus penn treebank 
prospects consistently adhered nitrogen view generation problem base generator proposes space alternatives evaluation function probably statistics data scores alternative analyses best passed processing 
generation community feel architecture control statistical component 
example dale personal communication argues proximate reason accepting statistical aid nlu need achieve better coverage easily obtained direct method grammar writing tuning testing 
purposes happy point 
dale goes suggest nlg need statistical aid coverage allow pursue goals diculties knowledge engineering 
extent meet research goals building nlg systems function particular application domain evidence knowledge engineering demands nlg kept check cf 
grishman kittredge 
presumably practical reason emphasis generation systems particular sub languages ll highly constrained roles larger contexts 
possible role maximum entropy principle interpretation bayesian terms mean modellers inclined bayesian approach granted exibility accustomed eds re sure comment 
re wrong ll suppress article submitted royal society oberlander brew nlg interesting 
particularly want point possibility nlg create materials experiments human subjects 
studies topics personality projection textual style sullivan liebler experiments dicult naturally occurring texts runs risk confounding stylistic variation di erences subject matter 
lacking full computationally ective theory textual projection personality conventional nlg limited ability generate texts expressing extent subject matter di erent styles 
hybrid statistical approach go way meeting need encoding regularities evaluation function 
application alluded ward occurs spoken interaction important generate text syllable pattern capable carrying particular intonation contour generally stylistic decisions informed knowledge text spoken sound natural projecting appropriate image 
computational agent sales job better able manipulate speech rate vocabulary appropriate way 
human speaker may know wants achieve having introspective understanding needs change 
case nitrogen architecture psychologically appropriate 
applications need base generator ensure candidates submitted evaluation function faithful subject matter course base generator possibly 
allowed non deterministic imperfect matters dicult encode symbolically 
ideal scenario course base generator knows saying choose exactly candidate 
case evaluator results guaranteed 
dicult sure ideal achievable 
evidence human language production behavioural introspective inconclusive issue 
speakers museum curator talking roger morris clearly produce complex communicative behaviours analysed satisfying multiple goals extremely clever ways 
certain heads time 
bateman 
dynamics surfacing initial exploration working notes international workshop levels representation discourse edinburgh july pp 

berger della pietra della pietra 
maximum entropy approach natural language processing 
computational linguistics 
berry pennebaker mueller 
linguistic bases social perception 
personality social psychology bulletin 
liebler 
trait inferences sources validity zero acquaintance 
journal personality social psychology 

language attitudes impression formation 
handbook language social psychology eds giles robinson pp 

wiley 
article submitted royal society stochastic text generation cahill doran evans mellish paiva scott 
search architecture nlg systems 
proceedings seventh european natural language generation workshop toulouse france may 
chelba jelinek 
exploiting syntactic structure language modelling 
proceedings th annual meeting association computational linguistics th international conference computational linguistics montreal canada august pp 

collins 
generative lexicalised models statistical parsing 
proceedings th annual meeting acl jointly th conference eacl madrid 
collins miller 
semantic tagging probabilistic context free grammar proceedings th workshop large corpora montreal canada august 
cutler isard 
production prosody 
language production vol 
ed 
butterworth 
london academic press 
dale reiter 
computational interpretations gricean maxims generation referring expressions 
cognitive science 
della pietra della pietra la erty 
inducing features random elds 
ieee transactions pattern analysis machine intelligence 

test readability new york harper row 
grishman kittredge 
analyzing language restricted domains 
hillsdale nj lawrence erlbaum associates 
hearst 
texttiling segmenting text multi paragraph subtopic passages 
computational linguistics 
heckerman 
tutorial learning bayesian networks 
learning graphical models ed 
jordan pp 

cambridge mass mit press 
poesio 
long distance global focus proceedings th annual meeting association computational linguistics th international conference computational linguistics montreal canada august pp 

jelinek 
statistical methods speech recognition 
cambridge mass mit press 
katz 
distribution content words phrases text language modelling 
natural language engineering 
langkilde 
automatic sentence generation hybrid statistical model lexical collocations syntactic relations 
thesis proposal 
langkilde knight 
practical value grams generation 
proceedings th international natural language generation workshop lake canada august 
lauritzen spiegelhalter 
local computations probabilities graphical structures application expert systems 
royal statistical society 
mckeown 
text generation discourse strategies focus constraints generate natural language text cambridge cambridge university press 
manning sch 
foundations statistical natural language processing 
cambridge mass mit press 
mellish knott oberlander donnell 
experiments stochastic search text planning 
proceedings th international natural language generation workshop niagara lake canada august pp 
meteer 
linguistic resources text planning 
proceedings fifth international natural language generation workshop dawson pennsylvania 
article submitted royal society oberlander brew oberlander 
right thing expect unexpected 
computational linguistics 
sullivan ekman friesen scherer 
say say contributions speech content voice quality judgments 
journal personality social psychology 
rosenfeld 
maximum entropy approach adaptive statistical language modelling 
computer speech language 
thompson 
strategy tactics language production 
papers thirteenth regional meeting chicago linguistics society 
ward 
connectionist language generator 
norwood ablex 
article submitted royal society 
