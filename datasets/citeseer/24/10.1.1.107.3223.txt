journal machine learning research submitted published gaussian processes ordinal regression wei chu gatsby ucl ac uk zoubin ghahramani zoubin gatsby ucl ac uk gatsby computational neuroscience unit university college london london wc ar uk editor christopher williams probabilistic kernel approach ordinal regression gaussian processes 
threshold model generalizes probit function likelihood function ordinal variables 
inference techniques laplace approximation expectation propagation algorithm respectively derived hyperparameter learning model selection 
compare gaussian process approaches previous ordinal regression method support vector machines benchmark real world data sets including applications ordinal regression collaborative filtering gene expression analysis 
experimental results data sets verify usefulness approach 
keywords gaussian processes ordinal regression approximate bayesian inference collaborative filtering gene expression analysis feature selection 

practical applications supervised learning frequently involve situations exhibiting order different categories teacher rates students giving grades performance 
contrast metric regression problems grades usually discrete finite 
grades different class labels classification problems due existence ranking information 
example grade labels ordering learning task predicting variables ordinal scale setting bridging metric regression classification referred ranking learning ordinal regression 
literature ordinal regression domain machine learning 
kramer 
investigated regression tree learner mapping ordinal variables numeric values 
principled way devising appropriate mapping function 
frank hall converted ordinal regression problem nested binary classification problems encode ordering original ranks results standard binary classifiers organized prediction 
har peled 
proposed constraint classification approach ranking problems binary classifiers 
cohen 
considered general ranking problems form preference judgements 
herbrich 
applied principle structural risk minimization vapnik ordinal regression leading new distribution independent wei chu zoubin ghahramani 
chu ghahramani learning algorithm loss function pairs ranks 
shashua levin generalized formulation support vector machines ordinal regression numerical results shows significant improvement performance compared line algorithm proposed crammer singer 
statistics literature approaches generalized linear models mccullagh nelder 
cumulative model mccullagh known classical statistical approaches ordinal regression rely specific distributional assumption unobservable latent variables stochastic ordering input space 
johnson albert described bayesian inference parametric models ordinal data sampling techniques 
general framework semiparametric models extends generalized additive models hastie tibshirani incorporating nonparametric parts 
nonparametric components regression model fitted maximizing penalized log likelihood model selection carried aic 
gaussian processes hagan neal provided promising non parametric bayesian approach metric regression williams rasmussen classification problems williams barber 
important advantage gaussian process models gps non bayesian models explicit probabilistic formulation 
provides probabilistic predictions gives ability infer model parameters control kernel shape noise level 
gps different semiparametric approach ways 
additive models fahrmeir defined functions input dimension gps general non additive covariance functions second kernel trick allows infinite basis function expansions third gps perform bayesian inference space latent functions 
probabilistic kernel approach ordinal regression gaussian processes 
impose gaussian process prior distribution latent functions employ appropriate likelihood function ordinal variables regarded generalization probit function 
bayesian inference techniques applied implement model adaptation laplace approximation mackay expectation propagation minka respectively 
comparisons generalization performance support vector approach shashua levin benchmark real world data sets movie ranking gene expression analysis verify usefulness approach 
organized follows section describe bayesian framework gaussian processes ordinal regression section discuss bayesian techniques hyperparameter inference section predictive distribution probabilistic prediction section give extensive discussion techniques section report results numerical experiments benchmark real world data sets conclude section 
bayesian framework consider data set composed samples 
samples pair input vector xi corresponding target yi finite set ordered cat gaussian processes ordinal regression egories 
loss generality categories denoted consecutive integers keep known ordering information 
main idea assume unobservable latent function xi xi gaussian process ordinal variable yi dependent latent function xi modelling ranks intervals real line 
bayesian framework described details 
gaussian process prior latent functions xi usually assumed realizations random variables indexed input vectors zero mean gaussian process 
gaussian process fully specified giving covariance matrix finite set zero mean random variables xi 
covariance functions corresponding inputs xi xj defined mercer kernel functions wahba sch lkopf smola gaussian kernel defined cov xi xj xi xj exp andx denotes th element xi 
prior probability latent functions xi multivariate gaussian exp zf xn zf covariance matrix ij th element defined 
likelihood ordinal variables likelihood joint probability observing ordinal variables latent functions denoted denotes target set yi 
generally likelihood evaluated product likelihood function individual observation yi xi likelihood function yi xi intuitively defined xi yi xi br defined rand threshold variables defined bj positive padding variables 
role 
br divide real line 
mercer kernel functions polynomial kernels spline kernels covariance function 
chu ghahramani contiguous intervals intervals map real function value xi discrete variable yi enforcing ordinal constraints 
likelihood function ideally noise free cases 
presence noise inputs targets may explicitly assume latent functions contaminated gaussian noise zero mean unknown variance denote gaussian random variable mean variance henceforth 
ordinal likelihood function yi xi zi xi zi xi yi xi 
note binary classification special case ordinal regression case likelihood function probit function 
quantity ln yi xi usually referred loss function yi xi 
derivatives loss function respect xi needed approximate bayesian inference methods 
order derivative loss function written yi xi xi second order derivative yi xi xi zi zi zi zi zi zi zi zi zi zi zi 
zi graphs ordinal likelihood function derivatives loss function illustration 
note order derivative monotonically increasing function xi second order derivative positive value facts ln yi xi concave xi ln concave pointed pratt convexity loss function follows integral log concave function respect arguments log concave function remaining arguments lieb cor 

posterior probability bayes theorem posterior probability written yi xi prior probability defined likelihood function yi xi defined df 
bayesian framework described conditional model parameters including kernel parameters covariance function control kernel shape threshold parameters noise level likelihood function 
principle distribution gaussian assumed noise latent functions 
ordinal likelihood function gaussian processes ordinal regression ln df ln graph likelihood function ordinal regression problem second order derivatives loss function negative logarithm likelihood function noise variance thresholds andb 

parameters collected hyperparameter vector 
normalization factor exactly known evidence yardstick model selection 
section discuss techniques hyperparameter learning 

model adaptation full bayesian treatment hyperparameters integrated space 
monte carlo methods neal adopted approximate integral effectively 
prohibitively expensive practice 
alternatively consider model selection determining optimal setting 
optimal values hyperparameters simply inferred maximizing posterior probability 
prior distribution hyperparameters specified domain knowledge alternatively vague uninformative distribution 
evidence high dimensional integral df 
idea computing evidence approximate posterior distribution gaussian evidence calculated explicit formula mackay csat minka 
section describe bayesian techniques model adaptation laplace approximation expectation propagation respectively 
map approach laplace approximation evidence calculated analytically applying laplace approximation maximum posteriori map estimate gradient optimization methods infer optimal hyperparameters maximizing evidence 
map estimate latent functions referred map equivalent chu ghahramani minimizer negative logarithm yi xi yi xi ln yi xi known loss function 
note positive definite matrix diagonal matrix ii th entry yi xi 
convex programming problem unique xi solution 
laplace approximation refers carrying taylor expansion map point retaining terms second order mackay 
order derivative respect vanishes map written fmap map map map map denotes matrix map estimate 
equivalent approximating posterior distribution gaussian distribution centered map covariance matrix map map map 
laplace approximation zf defined evidence computed analytically follows zf exp df exp map map identity matrix 
gradients logarithm evidence respect hyperparameters derived analytically 
gradient optimization methods employed search maximizer evidence 
refer appendix detailed gradient formulae outline algorithm model adaptation 
expectation propagation variational methods expectation propagation algorithm ep approximate bayesian inference method minka regarded extension assumed density filter adf 
ep algorithm applied gaussian process classification variational methods model selection seeger kim ghahramani 
setting gaussian processes ep attempts approximate product distribution form ti xi ti xi si exp pi xi mi 
parameters si mi pi ti successively optimized minimizing kullback leibler divergence new yi xi ti old old ti 
exponential family minimization simply solved moment matching second order 
detailed updating scheme appendix 
newton raphson formula find solution simple cases 
gaussian processes ordinal regression equilibrium obtain approximate posterior distribution diagonal matrix ii th entry pi mn variational methods optimize hyperparameters maximizing lower bound logarithm evidence 
applying jensen inequality log log df log df logp df logp df logq df 
lower bound written explicit expression equilibrium gradients respect derived neglecting possible dependency 
detailed formulation appendix 
prediction described techniques map approach ep approach infer optimal model 
optimal hyperparameters inferred denoted take test case target yx unknown 
latent variable column vector containing zero mean random variables xi prior joint multivariate gaussian distribution xn conditional distribution gaussian denoted variance 
predictive distribution computed integral space written df 
posterior distribution approximated gaussian map approach ep approach refer section 
predictive distribution simplified gaussian variance map approach reach map map ep approach get 

predictive distribution ordinal targets yx yx yx df predictive ordinal scale decided arg max yx 

discussion chu ghahramani map approach mean predictive distribution depends map estimate map unique solving convex programming problem 
evidence maximization useful laplace approximation mode point map gives summary posterior distribution 
approach expectation propagation mean predictive distribution depends approximate mean posterior distribution 
true shape far gaussian centered mode ep approach great advantage laplace approximation 
ep algorithm guarantee convergence usually works practice 
gradient optimization method usually requests evidence evaluation tens different settings minimum 
inversion matrix required costs time number training samples 
csat opper proposed fast training algorithm gaussian processes set basis vectors determined line sparse representation 
lawrence 
proposed greedy selection criteria information theoretic principles sparse gaussian processes seeger 
tresp proposed bayesian committee machines divide conquer large datasets infinite mixtures gaussian processes rasmussen ghahramani promising technique 
algorithms applied directly settings ordinal regression speedup 
feature selection essential part modelling 
gaussian processes automatic relevance determination ard method proposed mackay neal embedded covariance function follows cov xi xj xi xj exp ard parameter 
gradients respect variables ln derived analytically model adaptation 
optimal value ard parameter indicates relevance th input feature target 
form feature selection results type feature weighting 
furthermore linear combination heterogeneous kernels positive coefficients valid covariance function 
lanckriet 
suggest learn kernel matrix semidefinite programming 
bayesian framework positive coefficients kernels treated hyperparameters optimized evidence criterion optimization 
note binary classification special case ordinal regression likelihood function probit function 
probit function logistic function likelihood function binary classification different origins 
due dichotomous nature classes discriminant functions constructed class compete softmax function determine likelihood 
logistic function special case softmax function comes general classification problems 

ard parameters control covariance length scale gaussian process input dimension 
gaussian processes ordinal regression metric regression warped gaussian processes assume nonlinear monotonic continuous warping function relating observed targets latent variables gaussian process 
warping function learned data thought pre processing transformation applied modelling gaussian process 
different common approach dealing preprocessing discretize target values different bins 
discrete values clearly ordinal applying ordinal regression discrete values natural choice 
interestingly number discretization bins increased ordinal regression model similar warped gaussian processes model 
particular varying thresholds ordinal regression model approximate continuous warping function 

numerical experiments start section simple synthetic dataset visualize behavior algorithms report experimental results sixteen benchmark datasets 
perform experiments collaborative filtering problem eachmovie data gleason score prediction gene microarray data related prostate cancer 
shashua levin generalized support vector formulation finding multiple thresholds define parallel discriminant hyperplanes ordinal scales reported performance support vector approach better line algorithm crammer singer 
problem size large margin ranking algorithm herbrich 
quadratic function training data size making algorithmic complexity 
experiments large datasets computationally difficult 
decide limit comparisons support vector approach svm shashua levin versions approach map approach laplace approximation map ep algorithm variational methods ep 
implementation routine bfgs byrd gradient optimization package started initial values hyperparameters infer optimal values criterion approximate evidence map variational lower bound ep respectively 
improved smo algorithm keerthi adapted implement svm approach refer chu keerthi detailed description extensive discussion fold cross validation determine optimal values model parameters kernel parameter regularization factor involved problem formulations 
initial search done coarse grid linearly spaced region log log log log uniform grid linearly spaced log log space 
utilized evaluation metrics quantify accuracy predictive ordinal scales yt respect true targets yt 
datasets publicly available www pt regression datasets html 

versions proposed approach implemented ansi source code accessible www gatsby ucl ac uk code tar 

numerical experiments initial values hyperparameters usually chosen gaussian kernel threshold suggest try starting points practice choose best model objective functional 

source code ansi available www gatsby ucl ac uk code tar 
lower noise higher noise svm approach svm approach chu ghahramani map approach map approach ep approach ep approach performance algorithms synthetic rank ordinal regression problem 
discriminant function values svm approach predictive mean values gaussian process approaches contour graphs indexed thresholds 
upper graphs case lower noise level lower graphs case higher noise level 
training samples graphs 
dots denote training samples rank crosses denote training samples rank circles denote training samples rank 
mean absolute error average deviation prediction true target yi yi treat ordinal scales consecutive integers mean zero error gives error incorrect prediction fraction incorrect predictions 
artificial data presents behavior algorithms gaussian kernel synthetic data ordinal scales 
support vector approach optimal thresholds determined smo algorithm fold cross validation decide optimal values kernel parameter regularization factor 
gaussian process algorithms model adaptation see section determine gaussian processes ordinal regression table datasets characteristics 
attributes state number numerical nominal attributes 
training instances instances test specify size training test partition 
partitions generated test results individual partitions accessed www gatsby ucl ac uk html 
datasets attributes numeric nominal training instances instances test diabetes wisconsin breast cancer set machine cpu auto mpg boston housing stocks domain abalone bank domains bank domains computer activity set ii computer activity california housing census domains census domains optimal values kernel parameter noise level thresholds automatically 
shows algorithms working reasonably task 
benchmark data collected benchmark datasets set table metric regression problems 
target values discretized ordinal quantities equal length binning 
bins divide range target values number intervals length 
resulting rank values ordered representing intervals original metric quantities 
dataset generated versions discretizing target values intervals respectively 
randomly partitioned dataset training test splits specified table 
partition repeated times independently 
gaussian kernel algorithms 
test results recorded table 
performance map ep approaches closely matching 
gaussian process algorithms yield better results support vector approach average value especially number training samples small 
experiment selected large metric regression datasets set ii table 
input vectors normalized zero mean unit variance coordinate wise 
target values datasets discretized ordinal quantities binning 
dataset small subset randomly selected training tested remaining samples specified table 
partition repeated times independently 
show advantage explicitly modelling ordinal nature targets employed standard gaussian process algorithm williams chu ghahramani table test results algorithms gaussian kernel 
targets benchmark datasets discretized equal length bins 
results averages trials standard deviation 
bold face indicate cases average value lowest results algorithms 
symbols indicate cases indicated entry significantly worse winning entry value threshold wilcoxon rank sum test decide statistical significance 
mean zero error mean absolute error data svm map ep svm map ep diabetes wisconsin machine auto mpg boston stocks abalone table test results algorithms gaussian kernel 
targets benchmark datasets discretized equal length bins 
results averages trials standard deviation 
bold face indicate cases average value lowest results algorithms 
symbols indicate cases indicated entry significantly worse winning entry value threshold wilcoxon rank sum test decide statistical significance 
mean zero error mean absolute error data svm map ep svm map ep diabetes wisconsin machine auto mpg boston stocks abalone rasmussen metric regression gpr tackle ordinal regression tasks ordinal targets naively treated continuous values predictions test cases rounded nearest ordinal scale 
gaussian kernel algorithms 
test results table ordinal regression algorithms clearly superior naive approach applying standard metric regression 
observed performance gaussian process algorithms significantly better support vector approach datasets 
verifies judgement previous experiment gaussian process algorithms yield better performance 
gpr type ii maximum likelihood model selection 
gaussian processes ordinal regression table test results algorithms gaussian kernel 
targets benchmark datasets discretized equal frequency bins 
results average trials standard deviation 
gpr denotes standard algorithm gaussian process metric regression treats ordinal scales continuous values 
nll denotes negative logarithm likelihood prediction 
bold face indicate cases average value lowest mean zero error algorithms 
symbols indicate cases indicated entry significantly worse winning entry value threshold wilcoxon rank sum test decide statistical significance 
mean zero error nll data gpr svm map ep map ep bank bank compact compact california census census support vector approach small datasets 
ep approach yields better results mean zero error map approach tasks detected statistically significant difference performance 
table report negative logarithm likelihood prediction nll 
performance map ep approaches closely matching statistically significant difference 
datasets training time map ep approaches substantially svm approach 
map ep approaches tune model parameters gradient descent usually required evidence evaluations tens different settings fold cross validation svm approach required evaluations different nodes grid fold 
larger data sets svm approach may advantage training time due sparseness property computation 
collaborative filtering collaborative filtering exploits correlations ratings population users 
goal predict person rating new items person past ratings similar items ratings people items including new item 
ratings ordered making collaborative filtering ordinal regression problem 
carried ordinal regression subset eachmovie data compaq 
rates user id number movies targets numbers zero star respectively 
selected users contributed ratings movies input 
compaq system research center ran eachmovie service months 
users entered total numeric ratings movies rated zero star 
chu ghahramani features 
ratings users movie input vector accordingly 
input matrix elements observed 
randomly selected subset size movies training tested remaining movies 
size random selection carried times independently 
pearson correlation coefficient popular correlation measure hofmann corresponds dot product normalized rating vectors 
instance applied movies define called scores indexes users indexes movies rating movie user movie specific mean standard deviation respectively 
correlation coefficient defined denotes summing users covariance kernel function experiments algorithms 
ratings observed input vectors consider ad hoc strategies deal missing values mean imputation weighted low rank approximation 
case unobserved values identified mean value means corresponding score zero 
second case applied em procedure described jaakkola fill missing data estimate 
input matrix observed elements weighted missing data weight zero 
low rank fixed 
test results cases different training data size 
mean imputation svm produced bit accurate results gaussian processes mean absolute error 
cases low rank approximation preprocessing performance algorithms highly competitive interestingly observed improvement mean absolute error algorithms 
serious treatment missing data interesting research topic 
gene expression analysis singh 
carried microarray expression analysis genes identify genes anticipate clinical behavior prostate cancer 
samples prostate tumor investigated 
sample gleason score ranging reflecting level differentiation prostate tumor 
predicting gleason score gene expression data typical ordinal regression problem 
samples score greater merged top level leading levels samples respectively 
randomly partitioned data folds training test repeated partitioning times independently 
ard linear kernel xi xj ix evaluate feature relevance 
ard parameters optimized evidence maximization 
optimal values ard parameters genes mean absolute error mean zero error gaussian processes ordinal regression mean imputation training data size training data size weighted low rank approximation training data size training data size test results algorithms subset eachmovie data trials 
grouped boxes represent results svm left map middle ep right respectively different training data size 
boxes lines lower quartile median upper quartile values 
whiskers lines extending box extreme data value iqr interquartile range box 
outliers data values ends whiskers displayed dots 
higher graphs results mean absolute error lower graphs mean zero error 
cases mean imputation left graphs cases weighted low rank approximation preprocessing right graphs 
mean zero error mean absolute error svm approach number selected genes svm approach number selected genes chu ghahramani map approach number selected genes map approach number selected genes ep approach number selected genes ep approach number selected genes test results algorithms linear kernel prostate cancer data selected genes 
axes indexed log scale 
boxes indicate mean values heights vertical boxes indicate standard deviations trials 
gaussian processes ordinal regression ranked irrelevant relevant 
removed irrelevant genes gradually rank list 
gene number reduced 
number selected genes linear kernel xi xj ix algorithms fair comparison 
presents test results algorithms different numbers selected genes 
observed great steady improvement subset genes selected ard technique 
best validation output achieved top ranked features 
case training samples bayesian approaches perform better svm ep approach generally better map approach difference statistically significant 

ordinal regression important supervised learning problem properties metric regression classification 
proposed simple novel nonparametric bayesian approach ordinal regression generalization probit likelihood function gaussian processes 
approximate inference procedures derived detail evidence evaluation model adaptation 
approach intrinsically incorporates ard feature selection provides probabilistic prediction 
existent fast algorithms gaussian processes adapted directly tackle relatively large datasets 
experiments benchmark real world data sets show generalization performance competitive better support vector methods 
acknowledgments main part carried institute pure applied mathematics ipam ucla 
david wild stimulating discussions 
david mackay valuable comments 
wei chu supported national institutes health national institute general medical sciences division number gm 
zoubin ghahramani partially supported cmu darpa calo project 
reviewers thoughtful comments gratefully appreciated 
appendix gradient formulae evidence maximization evidence maximization equivalent finding minimizer negative logarithm evidence written explicit expression follows ln yi fmap xi map map ln map 
usually collect ln ln ln ln set variables tune 
definition tunable variables helpful convert constrained optimization problem unconstrained optimization problem 
outline algorithm model adaptation described table 
chu ghahramani table outline algorithm model adaptation map approach laplace approximation 
initialization choose favorite gradient descent optimization package select starting point optimization package looping optimization package requests evidence gradient evaluation 
find map estimate solving convex programming problem 
evaluate negative logarithm evidence map 
calculate gradients respect 
feed evidence gradients optimization package exit return optimal optimization package derivatives ln respect variables derived follows ln ln trace map map map trace map map map ln ln yi fmap xi trace map map map ln yi fmap xi trace map map map ln yi fmap xi ln trace map map map 
note map estimate map yi xi details fmap define zi zi zi zi zi zi zi zi zi zi runs zi xi zi xi ii th entry diagonal matrix denoted ii defined ii 
detailed derivatives ii ii ii xi 

yi xi ii gaussian processes ordinal regression ii ii column vector th element 
ii ii ii xi column vector th element ii 
yi xi ii ii ii xi yi yi 
ii ii ii yi yi 

column vector th element defined ii yi yi 
appendix approximate posterior distribution ep expectation propagation algorithm attempts approximate gaussian distributions xi xi si exp mi 
updating scheme follows 
initial states individual mean mi individual inverse variance pi individual amplitude si posterior covariance diag pn posterior mean mn looping significant change mi pi si pi xi xi removed get leave posterior distribution having variance xi chu ghahramani aii mean xi hi pi hi mi ajj hj xi updated incorporating message yi xi zi yi xi xi df xi log zi log zi new new new new zi pnew exp 
note pnew time 
pnew pi skip sample updating update pi mi si posterior mean covariance follows anew new pi pnew new ai pi hi mi pi aii ai th column defined 
byproduct get approximate evidence ep solution written det si det exp ij aij pim appendix gradient formulae variational bound equilibrium variational bound analytically calculated follows xi hi aii ln yi xi df xi ln trace mt gaussian processes ordinal regression note directly obtained defined 
gradient respect variables ln ln ln ln ln log df trace ht trace trace ln xi hi aii yi yi xi hi aii yi yi ln yi yi mt ln yi xi df xi xi hi aii xi hi aii aii aii xi exp aii hi aii yi xi aii aii xi exp aii hi aii yi xi ln yi xi df xi xi hi aii aii aii xi hi aii aii aii xi hi aii ln yi xi df xi xi hi aii aii aii xi hi aii aii aii exp aii yi xi ii exp aii yi xi exp aii yi xi ii ii exp aii yi xi df xi ii df xi df xi df xi df xi df xi yi means summing samples targets satisfy yi dimensional integrals approximated gaussian quadrature calculated romberg integration appropriate accuracy 
justin thomas hofmann 
unifying collaborative content filtering 
proceedings th international conference machine learning pages 
lieb 
extensions brunn minkowski theorems including inequalities log concave functions application diffusion equation 
journal functional analysis 
byrd lu nocedal 
limited memory algorithm bound constrained optimization 
siam journal scientific statistical computing 
chu ghahramani chu keerthi 
new approaches support vector ordinal regression 
technical report yahoo 
research labs 
william cohen robert schapire yoram singer 
learning order things 
journal artificial intelligence research 
compaq 
eachmovie 
research compaq com src eachmovie 
crammer singer 
ranking 
dietterich becker ghahramani editors advances neural information processing systems pages cambridge ma 
mit press 
csat opper winther 
efficient approaches gaussian process classification 
sara solla todd leen klaus robert ller editors advances neural information processing systems pages 
csat opper 
sparse online gaussian processes 
neural computation mit press 
fahrmeir 
multivariate statistical modelling generalized linear models 
new york springer verlag nd edition 
eibe frank mark hall 
simple approach ordinal classification 
proceedings european conference machine learning pages 
har peled roth 
constraint classification new approach multiclass classification ranking 
thrun becker obermayer editors advances neural information processing systems pages 
hastie tibshirani 
generalized additive models 
chapman hall london 
herbrich graepel obermayer 
large margin rank boundaries ordinal regression 
advances large margin classifiers pages 
mit press 
johnson james albert 
ordinal data modeling statistics social science public policy 
springer verlag 
keerthi bhattacharyya murthy 
improvements platt smo algorithm svm classifier design 
neural computation march 
kim ghahramani 
em ep algorithm gaussian process classification 
proc 
workshop probabilistic graphical models classification ecml 
kramer widmer pfahringer 
prediction ordinal classes regression trees 
fundamenta informaticae 
gaussian processes ordinal regression gert lanckriet nello cristianini peter bartlett laurent el ghaoui michael jordan 
learning kernel matrix semidefinite programming 
journal machine learning research 
neil lawrence matthias seeger ralf herbrich 
fast sparse gaussian process methods informative vector machine 
becker thrun obermayer editors advances neural information processing systems pages 
mackay 
practical bayesian framework back propagation networks 
neural computation 
mackay 
bayesian methods backpropagation networks 
models neural networks iii pages 
mccullagh 
regression models ordinal data 
journal royal statistical society 
mccullagh nelder 
generalized linear models 
chapman hall london 
minka 
family algorithms approximate bayesian inference 
phd thesis massachusetts institute technology january 
neal 
bayesian learning neural networks 
lecture notes statistics 
springer 
neal 
monte carlo implementation gaussian process models bayesian regression classification 
technical report department statistics university toronto 
hagan 
curve fitting optimal design prediction discussion 
journal royal statistical society 
john pratt 
concavity log likelihood 
journal american statistical association 
carl edward rasmussen zoubin ghahramani 
infinite mixtures gaussian process experts 
dietterich becker ghahramani editors advances neural information processing systems pages 
sch lkopf smola 
learning kernels support vector machines regularization optimization 
adaptive computation machine learning 
mit press december 
seeger 
notes minka expectation propagation gaussian process classification 
technical report university edinburgh 
seeger 
bayesian gaussian process models pac bayesian generalisation error bounds sparse approximations 
phd thesis university edinburgh july 
chu ghahramani amnon shashua levin 
ranking large margin principle approaches 
thrun becker obermayer editors advances neural information processing systems pages 
mit press 
singh ross jackson manola ladd tamayo lander golub sellers 
gene expression correlates clinical prostate cancer behavior 
cancer cell 
www genome wi mit edu mpr prostate 
zoubin ghahramani carl rasmussen 
warped gaussian processes 
sebastian thrun lawrence saul bernhard sch lkopf editors advances neural information processing systems 
nathan tommi jaakkola 
weighted low rank approximations 
proceedings twentieth international conference machine learning 
volker tresp 
bayesian committee machine 
neural computation november 
gerhard 
generalized structured ordinal models 
biometrics june 
vapnik 
nature statistical learning theory 
new york springer verlag 
wahba 
spline models observational data volume cbms nsf regional conference series applied mathematics 
siam 
williams barber 
bayesian classification gaussian processes 
ieee transactions pattern analysis machine intelligence 
williams rasmussen 
gaussian processes regression 
touretzky mozer hasselmo editors advances neural information processing systems volume pages 
mit press 

