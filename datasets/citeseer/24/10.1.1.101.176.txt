adapting sample size particle filters kld sampling dieter fox department computer science engineering university washington seattle wa email fox cs washington edu years particle filters applied great success variety state estimation problems 
statistical approach increasing efficiency particle filters adapting size sample sets estimation process 
key idea kld sampling method bound approximation error introduced sample representation particle filter 
name kld sampling due fact measure approximation error kullback leibler distance 
adaptation approach chooses small number samples density focused small part state space chooses large number samples state uncertainty high 
implementation computation overhead approach small 
extensive experiments mobile robot localization test application show approach yields drastic improvements particle filters fixed sample set sizes previously introduced adaptation technique 
estimating state dynamic system noisy sensor measurements extremely important areas different speech recognition target tracking mobile robot navigation computer vision 
years particle filters applied great success variety state estimation problems including visual tracking speech recognition mobile robot localization map building people tracking fault detection :10.1.1.2.342:10.1.1.37.4334:10.1.1.12.9879
book provides excellent overview state art 
particle filters estimate posterior probability density state space dynamic system :10.1.1.117.1144:10.1.1.110.383:10.1.1.117.9046
key idea technique represent probability densities sets samples 
due representation particle filters combine efficiency ability represent wide range probability densities 
efficiency particle filters lies way place computational resources 
sampling proportion likelihood particle filters focus computational resources regions high likelihood approximations important 
complexity particle filters depends number samples estimation attempts efficient available samples :10.1.1.117.9046
far important source increasing efficiency particle filters rarely studied adapting number samples time 
sample sizes discussed context genetic algorithms interacting particle filters existing approaches particle filters fixed number samples entire state estimation process 
highly inefficient complexity probability densities vary drastically time 
previously adaptive approach particle filters applied :10.1.1.2.342
approach adjusts number samples likelihood observations important shortcomings show 
introduce novel approach adapting number samples time 
technique determines number samples statistical bounds sample approximation quality 
extensive experiments indicate approach yields significant improvements particle filters fixed sample set sizes previously introduced adaptation technique 
investigate utility adaptive particle filters context mobile robot localization problem estimating robot pose relative map environment 
localization problem occasionally referred fundamental problem providing mobile robot autonomous capabilities 
mobile robot localization problem comes different flavors 
simple localization problem position tracking 
initial robot pose known localization seeks identify small incremental errors robot odometry 
challenging global localization problem robot told initial pose determine scratch 
global localization problem difficult robot position estimate represented adequately unimodal probability densities :10.1.1.31.7646
approaches introduced solve global localization problem grid approaches topological approaches particle filters tracking :10.1.1.31.7646:10.1.1.2.342:10.1.1.35.8467
challenging problem mobile robot localization kidnapped robot problem localized robot position told 
problem differs global localization problem robot firmly believe time kidnapping 
kidnapped robot problem test robot ability recover autonomously catastrophic localization failures 
virtually approaches capable solving global localization problem modified solve kidnapped robot problem 
focus tracking global localization problem 
remainder organized follows section outline basics bayes filters discuss different representations posterior densities 
introduce particle filters application mobile robot localization 
section introduce novel technique adaptive particle filters 
experimental results section conclude section 
particle filters bayesian filtering robot localization section review basics bayes filters alternative representations probability densities beliefs underlying bayes filters 
introduce particle filters sample implementation bayes filters followed discussion application mobile robot localization 
bayes filters bayes filters address problem estimating state dynamical system sensor measurements 
key idea bayes filtering recursively estimate posterior probability density state space conditioned data collected far 
loss generality assume data consists alternating sequence time indexed observations zt control measurements ut describe dynamics system 
posterior time called belief bel xt defined bel xt xt zt ut zt ut 
bayes filters assumption dynamic system markov observations zt control measurements ut conditionally independent past measurements control readings knowledge state xt 
assumption posterior determined efficiently update rules new control measurement ut received state system predicted bel xt xt xt ut bel xt dxt observation zt state estimate corrected bel xt zt xt bel xt 
normalizing constant ensures belief entire state space sums 
term xt xt ut describes system dynamics state system changes control information ut 
zt xt perceptual model describes likelihood making observation zt current state xt 
note random variables high dimensional vectors 
belief immediately prediction observation called predictive belief bel xt 
belief bel initialized prior knowledge state system 
bayes filters concept provide probabilistic framework recursive state estimation 
implement bayes filters specify perceptual model zt xt dynamics xt xt ut representation belief bel xt 
belief representations properties different implementations bayes filters strongly differ way represent densities state xt 
discuss different belief representations grid topological kalman filter extended unscented fixed variable resolution kalman filter piecewise constant approximation arbitrary posteriors non linear dynamics observations optimal converges true posterior exponential state dimensions global localization discrete particle filter sample approximation arbitrary posteriors non linear dynamics observations optimal converges true posterior exponential state dimensions global localization state space arbitrary discrete posteriors dynamics observations dimensional graph global localization bayes filters second moment linear dynamics observations optimal linear gaussian polynomial state dimension position tracking multi hypothesis tracking ekf multi modal gaussian non linear dynamics observations optimal linear approx 
polynomial state dimension global localization second moment non linear dynamics observ 
optimal linear approx 
polynomial state dimension position tracking continuous fig 
properties common implementations bayes filters 
approaches markov assumption underlying bayes filters 
properties context mobile robot localization 
overview different algorithms 
kalman filters widely variant bayes filters 
approximate beliefs second moments mean covariance 
kalman filters optimal assumptions initial state uncertainty unimodal gaussian observation model system dynamics linear state gaussian noise 
non linearities typically approximated linearization current state resulting extended kalman filter ekf 
unscented kalman filter uf introduced 
approach deterministically generates samples sigma points taken gaussian state passes samples non linear dynamics followed gaussian approximation predicted samples 
unscented filter shown yield better approximations theory practice 
due linear approximations extended unscented kalman filters optimal 
despite restrictive assumptions kalman filters applied great success mobile robot localization yield efficient accurate position estimates highly non linear systems 
key advantages kalman filters efficiency 
complexity polynomial dimensionality state space observations 
due graceful increase complexity kalman filters applied simultaneous localization map building problem slam estimate full posteriors robot positions landmark positions typically consisting hundreds dimensions :10.1.1.6.5814
due assumption unimodal posteriors kalman filters applied robot localization solely aim tracking robot location 
designed globally localize robot scratch arbitrarily large environments 
limitations kalman filters context robot localization shown experimentally :10.1.1.116.6293
multi hypothesis approaches represent belief state mixtures gaussians 
hypothesis gaussian typically tracked extended kalman filter 
due ability represent multi modal beliefs approaches able solve global localization problem 
hypothesis tracked kalman filter methods rely assumptions underlying kalman filters apart unimodal posteriors 
practice multi hypothesis approaches shown robust violations assumptions 
far clear methods applied extremely non linear observations 
addition pure kalman filtering multi hypothesis approaches require sophisticated heuristics solve data association problem determine add delete hypotheses 
topological approaches symbolic graph structured representations environment 
state space robot consists set discrete locally distinctive locations corners crossings hallways 
advantage approaches lies efficiency fact represent arbitrary distributions discrete state space 
solve global localization problem 
additionally approaches may scale high dimensional state spaces complexity topological structure directly dependent dimensionality underlying state space 
key disadvantage lies coarseness representation due position estimates provide rough information robot location 
furthermore sensor information related symbolic representation environment adequate features available arbitrary environments 
grid metric approaches rely discrete piecewise constant representations belief :10.1.1.31.7646
indoor localization spatial resolution grids usually cm angular resolution usually degrees 
topological approaches methods represent arbitrary distributions discrete state space solve global localization problem contrast topological approaches metric approximations provide accurate position estimates combination high robustness sensor noise 
grid method applied successfully position estimation museum tour guide robots rhino minerva 
disadvantage grid approaches lies computational complexity requirement keep typically dimensional position probability grid memory update new observation 
efficient sensor models selective update schemes adaptive tree representations greatly increase efficiency methods making applicable online robot localization 
complexity methods grows exponentially number dimensions doubtful applied higherdimensional state spaces 
sample approaches represent beliefs sets samples particles :10.1.1.2.342:10.1.1.37.4334
key advantage particle filters ability represent arbitrary probability densities solve global localization problem 
furthermore topological particular grid implementations bayes filters robot localization referred markov localization 
particle filters shown converge true posterior non gaussian nonlinear dynamic systems 
compared grid approaches particle filters efficient focus resources particles regions state space high likelihood 
efficiency particle filters strongly depends number samples filtering process attempts efficient available samples :10.1.1.117.9046
worst case complexity methods grows exponentially dimensions state space clear particle filters applied arbitrary high dimensional estimation problems 
posterior focuses small lower dimensional regions state space particle filters focus samples regions making applicable high dimensional state spaces 
discuss details particle filters section 
mixed approaches independences structure state space break state lower dimensional sub spaces random variables 
structured representations known name dynamic bayesian networks 
individual sub spaces represented adequate representation continuous densities samples discrete values 
name rao blackwellised particle filters combination particle filters kalman filters yielded extremely robust efficient approaches higher dimensional state estimation including full posteriors robot positions maps 
particle filters particle filters variant bayes filters represent belief bel xt set st weighted samples distributed bel xt st 
state non negative numerical factors called importance weights sum 
basic form particle filter realizes recursive bayes filter sampling procedure referred sequential importance sampling resampling sisr see :10.1.1.110.383:10.1.1.110.383:10.1.1.56.1897
time update basic particle filter algorithm outlined table 
iteration algorithm receives sample set st representing previous belief robot control measurement ut observation zt 
steps generate samples representing posterior belief step determines sample draw previous set 
resampling step sample index drawn probability proportional sample weight sample index drawn corresponding sample control information ut predict state done sampling density xt xt ut represents system dynamics 
corresponds sample drawn predictive belief bel xt 
order generate samples posterior belief bel xt importance sampling applied bel xt target distribution resampling minimal variance implemented efficiently constant time sample procedure known name deterministic selection stochastic universal sampling :10.1.1.117.1144

inputs st control measurement ut observation zt 
representing belief bel xt 
st initialize 

generate samples resampling draw state previous belief 
sample index discrete distribution weights st sampling predict state 
sample xt xt ut conditioned ut 
zt compute importance weight 
update normalization factor 
st st insert sample sample set 

normalize importance weights 

return st table basic particle filter algorithm 
xt xt ut bel xt proposal distribution 
dividing distributions get zt importance weight sample see eq 
step keeps track normalization factor step inserts new sample sample set 
generating samples weights normalized sum steps 
shown procedure fact implements bayes filter approximate sample representation :10.1.1.110.383
furthermore sample posterior converges true posterior rate number samples goes infinity 
particle filters mobile robot localization illustrates particle filter algorithm dimensional robot localization example 
illustration purpose particle filter update broken separate parts robot motion steps observations steps 
robot map hallway know position 
shows initial belief uniformly distributed sample set approximates uniform distribution 
sample importance weight indicated equal heights bars 
assume robot detects door left 
likelihood observation shown upper graph 
likelihood incorporated sample set adjusting normalizing importance factor sample leads sample set shown lower part steps table 
samples states importance factors proportional 
robot moves right receiving control information particle filter algorithm draws samples current weighted sample set randomly predicts location robot motion information steps table 
resulting sample set shown 
notice sample set differs original majority sam dimensional illustration particle filters mobile robot localization 
ples centered locations 
concentration samples achieved resampling step subsequent motion 
robot senses second door leading probability shown upper graph 
weighting importance factors proportion probability obtain sample set 
robot motion includes resampling step probability mass consistent robot true location 
typical robot localization problems position robot represented twodimensional cartesian space robot heading direction 
measurements zt may straightforward number samples adapted estimation process problem rarely addressed far 
adaptive particle filters variable sample set sizes time complexity update particle filter algorithm linear number samples needed estimation 
attempts effective available samples allowing sample sets reasonable size 
method incorporates markov chain monte carlo mcmc steps improve quality sample posterior approximation 
approach auxiliary particle filters applies step lookahead minimize mismatch proposal target distribution minimizing variability importance weights turn determines efficiency importance sampler :10.1.1.117.9046
auxiliary particle filters applied robot localization 
similar line reasoning injection observation samples posterior advantageous :10.1.1.37.4334:10.1.1.18.8488
approach requires availability sensor model possible efficiently generate samples 
introduce approach increasing efficiency particle filters adapting number samples underlying state uncertainty 
localization example illustrates approach beneficial 
global localization robot highly uncertain large number samples needed accurately represent belief 
extreme robot knows small number samples suffices accurately track position 
fixed number samples choose large sample sets allow mobile robot address global localization position tracking problem 
approach contrast adapts number samples localization process choosing large sample sets global localization small sample sets position tracking 
introduce method adaptive particle filters discuss existing technique changing number samples filtering process :10.1.1.2.342
likelihood adaptation call approach likelihood adaptation determines number samples likelihood observations 
specifically approach generates samples sum non normalized likelihoods exceeds pre specified threshold 
sum equivalent normalization factor updated step particle filter algorithm see table 
likelihood adaptation applied dynamic bayesian networks mobile robot localization :10.1.1.2.342
intuition approach follows sample set tune sensor reading individual importance weight large sample set remains small 
typically case position tracking cf 

sensor reading carries lot surprise case robot globally uncertain lost track position individual sample weights small sample set large 
likelihood adaptation directly relates property variance importance sampler function mismatch proposal distribution target distribution 
unfortunately mismatch accurate indicator necessary number samples 
consider example ambiguous belief state consisting distinctive sample clusters shown fig 

due symmetry environment average likelihood sensor measurement observed situation approximately robot knew position unambiguously cf 

likelihood adaptation number samples situations 
obvious accurate approximation belief shown fig 
requires multiple samples needed represent belief fig 

likelihood approach shown superior particle filters fixed sample set sizes :10.1.1.2.342
previous discussion clear approach fully exploit potential adapting size sample sets 
kld sampling key idea approach adaptive particle filters stated follows iteration particle filter determine number samples probability error true posterior sample approximation 
kl distance derive bound approximation error assume true posterior discrete piecewise constant distribution discrete density tree multi dimensional histogram :10.1.1.32.3507
representation show determine number samples distance sample maximum likelihood estimate mle true posterior exceed pre specified threshold 
denote resulting approach kld sampling algorithm distance mle true distribution measured kullback leibler distance kl distance 
kl distance measure difference probability distributions log kl distance negative zero distributions identical 
metric symmetric obey triangle property 
despite fact accepted standard measure difference probability distributions densities 
follows determine number samples needed achieve high probability approximation arbitrary discrete probability distribution see 
show modify basic particle filter algorithm realizes adaptation approach 
see suppose samples drawn discrete distribution different bins 
vector 
xk denote number samples drawn bin 
distributed multinomial distribution 
pk specifies true probability bin 
maximum likelihood estimate samples furthermore likelihood ratio statistic testing xj identical pj get log pj xj log 
pj pj log pj log 
pj see likelihood ratio statistic times kl distance mle true distribution log nk 
shown likelihood ratio converges chi square distribution degrees freedom log 
pp denote probability kl distance true distribution sample mle equal assumption true distribution 
relationship probability number samples derived follows pp pp nk pp log follows convergence result stated 
quantiles chi square distribution 
choose equal combine get pp 
clear relationship number samples resulting approximation quality 
summarize choose number samples guarantee probability kl distance mle true distribution see 
order determine need compute quantiles chi square distribution 
approximation wilson transformation yields upper quantile standard normal distribution 
values typical values readily available standard statistical tables 
concludes derivation sample size needed approximate discrete distribution upper bound kl distance 
see required number samples proportional inverse error bound order linear number bins support 
assume bin multinomial distribution support probability certain threshold contains particle kl distance particle filters remains shown incorporate result particle filter algorithm 
problem know true posterior distribution estimate number samples needed approximation efficient estimation posterior main goal particle filter 
solution problem rely sample representation predictive belief estimate posterior samples belief generated step basic particle filter algorithm shown table 
furthermore shows necessary determine complete discrete distribution suffices determine number bins support 
know quantity generated samples predictive distribution estimate counting number bins support sampling 
update step kld sampling particle filter summarized table 
seen update number supported bins predictive distribution sample generated step 
determination done incrementally checking generated sample falls empty bin steps 
sample equation update number samples required current estimate step 
step additionally check minimum number samples generated min typically set 
concurrent increase number generated samples desired number samples works follows early stages sampling increases new sample virtually bins empty 
increase results increase number desired samples 
time bins non empty increases occasionally 
increases new sample reach sampling stopped condition step 
way determining number degrees freedom multinomial distribution common statistical tool 
approach key advantage approximation results efficient implementation depend threshold see paragraph 
implemented version algorithm complexity state space determine number samples 
complexity measured entropy distribution 
approach depend thresholding guaranteed approximation bounds implemented efficiently method described 
furthermore yield noticeably different results 

inputs st 
representing belief bel xt control measurement ut observation zt bounds bin size minimum number samples min 
st initialize 
generate samples 
resampling draw state previous belief 
sample index discrete distribution weights st sampling predict state 
sample 

xt xt ut ut zt compute importance weight update normalization factor 
st st insert sample sample set 
falls empty bin 
update number bins support 
non empty mark bin 
min 
update number desired samples 
update number generated samples 
min 
kl bound reached 

normalize importance weights 

return st table kld sampling algorithm 
implementation modified particle filter straightforward 
difference original algorithm keep track number supported bins number desired samples steps clarity omitted fact determined 
bins implemented fixed multidimensional grid compactly tree structures :10.1.1.10.8018:10.1.1.32.3507
note sampling process guaranteed terminate bin size maximum number bins limited limits maximum number desired samples 
summarize approach adapts number samples approximation error introduced sample representation 
uses predictive belief state estimate underlying posterior 
guaranteed approach diverge true unknown belief 
experiments show divergence occurs error bounds loose 
kld sampling easy implement determination sample set size done noticeable loss processing speed low dimensional robot localization context 
furthermore approach fig 
pioneer ii robot experiments 
map localization path followed robot data collection 
small circles mark different start points global localization experiments 
combination scheme improving approximation posterior 
experimental results evaluated kld sampling context indoor mobile robot localization data collected pioneer robot see 
data consists sequence sonar laser range finder scans odometry measurements annotated time stamps allow systematic real time evaluations 
beam sensor model compute likelihood sensor scans 
experiments compared kld sampling approach likelihood approach discussed section particle filters fixed sample set sizes 
experiments different parameters approaches 
fixed approach varied number samples likelihood approach varied threshold determine number samples approach varied bound kl distance 
experiments value fixed bin size cm cm deg 
limited maximum number samples approaches 
influence different parameter settings performance kld sampling discussed detail section 
approximation true posterior set experiments evaluated accurately different methods approximate true posterior density 
ground truth posteriors available generated sample sets particle filter fixed number samples far needed position estimation 
iteration test algorithms computed kl distance current sample sets corresponding sets histograms sets 
ignored time stamps experiments gave algorithm time needed process data 
fig 
plots average confidence intervals average number samples different algorithms parameter settings clarity omitted large error bars 
different data points kld sampling obtained varying error bound 
data point graph represents average global localization runs different start positions robot runs represents average approximately sample set comparisons different points kl distance kld adaptation likelihood adaptation fixed sampling number samples localization error cm kld adaptation likelihood adaptation fixed sampling number samples fig 
axis represents average sample set size different parameters approaches 
axis plots kl distance densities sample sets generated different approaches real time constraints considered experiment 
axis represents average localization error measured distance estimated positions positions 
shape due fact real time conditions increasing number samples results higher update times loss sensor data 
time 
expected samples different approaches better approximation 
curves clearly illustrate superior performance approach fixed approach requires samples converges kl distance approach converges level samples average 
improvement factor compared approximately samples needed likelihood approach 
graph shows approach guaranteed accurately track true belief 
due fact error bound computed relative current estimate belief true posterior 
approach diverge error bound loose see leftmost data point kld sampling graph 
experiments indicate approach approximations able accurately track true posterior far smaller sample sets average approaches 
real time performance due computational overhead determining number samples clear approach yields better results real time conditions 
test performance approach realistic conditions performed multiple global localization experiments real time considerations timestamps data sets 
previous experiment localization robot sonar sensors 
different average numbers samples kld sampling obtained varying bound 
minimum maximum numbers samples correspond bounds respectively 
update particle filter determined distance estimated robot position corresponding position results shown fig 

shape graphs nicely illustrates trade involved choosing number samples real time constraints choosing samples results poor approximation underlying posterior robot frequently fails localize 
position estimates extracted sample sets local averaging positions determined evaluating robot highly accurate laser range finder information 
number samples laser sonar time sec fig 
typical evolution number samples global localization run plotted time number samples shown log scale 
solid line shows number samples robot laser range finder dashed graph sonar sensor data 
hand choose samples update algorithm take seconds valuable sensor data discarded results accurate position estimates 
fig 
shows real time conditions kld sampling approach yields drastic improvements fixed sampling likelihood sampling 
smallest average localization error cm contrast minimal average error cm cm likelihood fixed approach respectively 
result due fact approach able determine best mix samples early stages localization samples position tracking 
due smaller sample sets approach needs significantly processing power approaches 
note average errors relatively high include large errors occurring localization run 
shows sample set sizes typical global localization run 
addition sizes sonar data previous experiments dashed line graph shows numbers samples resulting data collected robot laser range finder solid line 
expected higher accuracy laser rangefinder results faster drop number samples smaller sample sets final tracking phase experiment 
note parameters runs 
extensions illustrate global localization sonar laser range finders 
shown animations sample sets localization annotated number samples different points time 
runs start samples 
robot plotted position estimated sample sets 
timing animations proportional approximate update times particle filter real updates times faster 
tracking performance far showed kld sampling beneficial state uncertainty extremely high constantly decreases certain level 
experiment test kld sampling produce improvements tracking initial state system known 
data collected pioneer robot shown 
time robot laser range finder data 
run particle filter initialized gaussian distribution centered start location robot 
initial experiments showed context kld sampling significant advantage localization error cm adaptive fixed number samples number samples time sec adaptive fig 
localization error different average sample set sizes 
solid line shows error kld sampling dashed line illustrates results fixed sample set sizes 
number samples kld sampling runs 
grey shaded areas indicate time intervals sensor loss laser data available 
robot position robot position robot position robot position fig 
sample sets interval sensor loss 
normal tracking robot highly certain samples 
sensor loss interval robot uncertain samples represent belief 
receiving sensor data robot keep track multiple hypotheses samples 
position uncertainty low number samples reduced accordingly samples 
fixed sample sets 
result surprising uncertainty vary position tracking 
increase difficulty tracking task approach improve tracking ability 
difficulty increased adding random noise robot odometry measurements modeling temporary loss sensor data 
done randomly choosing intervals seconds laser scans deleted 
illustrates localization error different sample set sizes kld sampling fixed sample set sizes approach highly superior fixed sampling scheme 
approach tracking results average error cm achieved samples average fixed approach requires samples achieve comparable accuracy 
note relatively large errors due intervals sensor loss 
compare kld sampling likelihood approach method able adequately adapt situations sensor loss 
due fact integrating sensor data likelihoods 
number required samples number bins number required samples number bins fig 
number required samples versus number bins different settings 
fixed graphs show eq 
different values 
fixed varied 
illustrates approach performs better particle filters fixed sample sets 
plots number samples runs kld sampling 
grey shaded areas indicate time intervals laser data removed script 
graph shows approach automatically adjusts number samples availability sensor data 
samples normal tracking number samples automatically increases laser data available 
increased number samples needed keep track increased estimation uncertainty due sensor loss 
shows sequence sample sets intervals laser data 
clear distributions sensor loss interval require samples distributions normal tracking 
furthermore shows essential samples represent ambiguous situations occurring due increased uncertainty 
parameter settings section discusses influence different parameters performance 
approach independent parameters sensor motion models values changed kld sampling 
key parameters approach error bound probability bound bin size 
experiments far bound bin size fixed error bound changed 
number non empty bins number samples computed 
illustrates function different combinations 
graphs shown bins 
larger values individual graphs continue perfectly straight lines 
seen number required samples changes significantly different settings different values see eq 

reasonable fix adjust achieve performance 
shows kld sampling changes bin size 
results obtained context global localization experiment described section 
bin size fixed varied achieve different sample set sizes 
shows average number samples bin sizes ranging cm cm deg cm cm deg 
fixed values number samples increases smaller bin sizes 
difference prominent small values tolerated approximation error 
result surprising smaller bin sizes result larger values avg 
number samples error bound cm deg cm deg cm deg cm deg localization error cm cm deg cm deg cm deg cm deg number samples fig 
dependency bin size 
axis represents different values error bound 
axis plots resulting average number samples different bin sizes 
axis shows average number samples axis plots localization error achieved corresponding number samples bin size 
obviously bin sizes achieve comparable results 
non empty bins increasing number required samples 
shows localization error plotted versus average number samples different bin sizes 
number samples varied changing value 
experiment significant difference performances different bin sizes 
approximately samples average bin sizes achieved low level localization error 
difference lies value required achieve certain average number samples 
specifically quality achieved large bin sizes combination small approximation bound small bin sizes combination larger approximation bound 
result slightly surprising expected smaller bin sizes result better performance 
result encouraging shows performance robust changes parameters 
experience approach fix parameters reasonable values cm cm deg adjust remaining parameter get desired results 
research statistical approach adapting sample set sizes particle filters estimation process 
key idea kld sampling approach bound error introduced sample belief representation 
iteration approach generates samples number large guarantee kl distance maximum likelihood estimate underlying posterior exceed pre specified bound 
approach chooses small number samples density focused small subspace state space chooses large numbers samples samples cover major part state space 
kld sampling advantageous complexity posterior changes drastically time case example global robot localization position tracking sensor loss 
implementational computational overhead approach small 
extensive experiments context mobile robot localization show approach yields drastic improvements particle filters fixed sample sets previously introduced adaptation approach :10.1.1.2.342
experiments kld sampling yields better approximations samples required fixed approach samples required likelihood adaptation approach 
addition experiments kld sampling tested various indoor environments including museum wide open spaces 
environments results comparable ones 
far kld sampling tested robot localization 
conjecture applications particle filters benefit method 
general approach achieves maximal improvements lower dimensional state spaces state uncertainties vary time 
high dimensional state spaces bin counting implemented tree structures :10.1.1.10.8018:10.1.1.32.3507
case additional cost kld sampling higher tree lookup takes time logarithmic size state space constant time grid experiments 
furthermore approach combined method improving efficiency particle filters 
kld sampling opens directions research 
current implementation discrete distribution fixed bin size determine number samples 
assume performance filter improved changing discretization time coarse discretizations uncertainty high fine discretizations uncertainty low 
approach extended case certain parts state space highly accurate estimates needed parts crude approximation sufficient 
problem addressed locally adapting discretization desired approximation quality multi resolution tree structures combination stratified sampling :10.1.1.32.3507
result samples important parts state space samples parts 
acknowledgments research sponsored part national science foundation career number darpa mica sdr programme contract numbers 
author wishes jon wellner vladimir help deriving statistical background 
additional go wolfram burgard sebastian thrun valuable discussions 
castellanos 
feature multi hypothesis localization tracking mobile robots geometric constraints 
proc 
ieee international conference robotics automation 

hybrid high precision localization mail distributing mobile robot system mops 
proc 
ieee international conference robotics automation 
gordon clapp 
tutorial particle filters line non linear non gaussian bayesian tracking 
ieee transactions signal processing xx 
austin 
multiple gaussian hypotheses represent probability distributions mobile robot localization 
proc 
ieee international conference robotics automation 
baker 
reducing bias inefficiency selection algorithm 
proc 
second international conference genetic algorithms 
bar shalom 
li 
multitarget multisensor tracking principles techniques 
bar shalom 
burgard cremers fox hnel lakemeyer schulz steiner thrun 
experiences interactive museum tour guide robot 
artificial intelligence 
burgard fox cremers 
integrating global position estimation position tracking mobile robots dynamic markov localization approach 
proc 
ieee rsj international conference intelligent robots systems 
burgard fox schmidt :10.1.1.31.7646
estimating absolute position mobile robot position probability grids 
proc 
national conference artificial intelligence 
castellanos spmap probabilistic framework simultaneous localization map building 
ieee transactions robotics automation 

topological simultaneous localization mapping slam exact localization explicit localization 
ieee transactions robotics automation 
cover thomas 
elements information theory 
wiley series telecommunications 
wiley new york 
cox 
experiment guidance navigation autonomous robot vehicle 
ieee transactions robotics automation 
cox 
review statistical data association techniques motion correspondence 
international journal computer vision 
cox leonard 
modeling dynamic environment bayesian multiple hypothesis approach 
artificial intelligence 
de freitas 
rao blackwellised particle filtering fault diagnosis 
ieee aerospace 

maximally informative statistics localization mapping 
proc 
ieee international conference robotics automation 
del moral 
branching interacting particle systems approximations feynman kac formulae applications non linear filtering 
de number lecture notes mathematics 
springer verlag 
dellaert burgard fox thrun 
condensation algorithm robust vision mobile robot localization 
proc 
ieee computer society conference computer vision pattern recognition cvpr 
newman clark durrant whyte 
solution simultaneous localization map building slam problem 
ieee transactions robotics automation 
doucet de freitas murphy russell 
rao blackwellised particle filtering dynamic bayesian networks 
proc 
conference uncertainty artificial intelligence uai 
doucet de freitas gordon editors 
sequential monte carlo practice 
springer verlag new york 
doucet godsill andrieu 
sequential monte carlo sampling methods bayesian filtering 
statistics computing 
ritter fox palm 
soccer robot sporadic visual features 
proc 
international conference intelligent autonomous systems ias 
engelson mcdermott 
error correction mobile robot map learning 
proc 
ieee international conference robotics automation 
fox 
markov localization probabilistic framework mobile robot localization 
phd thesis dept computer science university bonn germany december 
fox 
kld sampling adaptive particle filters 
dietterich becker ghahramani editors advances neural information processing systems nips cambridge ma 
mit press 
fox burgard dellaert thrun :10.1.1.2.342
monte carlo localization efficient position estimation mobile robots 
proc 
national conference artificial intelligence 
fox burgard kruppa thrun 
probabilistic approach collaborative multi robot localization 
autonomous robots 
fox burgard thrun 
markov localization mobile robots dynamic environments 
journal artificial intelligence research jair 
fox thrun dellaert burgard 
particle filters mobile robot localization 
doucet 
gelb 
applied optimal estimation 
mit press 
ghahramani 
hidden markov models bayesian networks 
international journal pattern recognition artificial intelligence 
gilks 
moving target monte carlo inference dynamic bayesian models 
journal royal statistical society series 
godsill clapp 
improvement strategies monte carlo particle filters 
doucet 
gustafsson bergman jansson karlsson 

particle filters positioning navigation tracking 
ieee transactions signal processing 
gutmann burgard fox konolige :10.1.1.116.6293
experimental comparison localization methods 
proc 
ieee rsj international conference intelligent robots systems 
gutmann fox 
experimental comparison localization methods continued 
proc 
ieee rsj international conference intelligent robots systems 
gutmann nebel 
fast accurate robust self localization polygonal environments 
proc 
ieee rsj international conference intelligent robots systems 
hertzberg kirchner 
landmark autonomous navigation pipes 
proc 
euromicro workshop advanced mobile robots 
ieee computer society press 
isard blake 
condensation conditional density propagation visual tracking 
international journal computer vision 
kristensen 
active global localisation mobile robot multiple hypothesis tracking 
ieee transactions robotics automation october 
wijk austin andersson 
feature condensation mobile robot localization 
proc 
ieee international conference robotics automation 
johnson kotz balakrishnan 
continuous univariate distributions volume 
john wiley sons new york 
julier uhlmann 
new extension kalman filter nonlinear systems 
proc 
aerosense th international symposium aerospace defense sensing simulation controls 
kaelbling cassandra kurien 
acting uncertainty discrete bayesian models mobile robot navigation 
proc 
ieee rsj international conference intelligent robots systems 
kalman 
new approach linear filtering prediction problems 
trans 
asme journal basic engineering march 
kitagawa 
monte carlo filter smoother non gaussian nonlinear state space models 
journal computational graphical statistics 
koller 
learning approximation stochastic processes 
proc 
international conference machine learning 
koller lerner 
sampling factored dynamic systems 
doucet 
konolige chou 
markov localization correlation 
proc 
international joint conference artificial intelligence ijcai 
kuipers 
spatial semantic hierarchy 
artificial intelligence 
kuipers beeson 
bootstrap learning place recognition 
proc 
national conference artificial intelligence 
lenser veloso 
sensor resetting localization poorly modelled mobile robots 
proc 
ieee international conference robotics automation 
leonard durrant whyte 
mobile robot localization tracking geometric beacons 
ieee transactions robotics automation 
leonard feder 
computationally efficient method large scale concurrent mapping localization 
proc 
ninth international symposium robotics research 
liu chen 
sequential monte carlo methods dynamic systems 
journal american statistical association 
lu milios 
robot pose estimation unknown environments matching range scans 
journal intelligent robotic systems 
montemerlo thrun koller wegbreit 
fastslam factored solution simultaneous localization mapping problem 
proc 
national conference artificial intelligence 
montemerlo thrun whittaker 
conditional particle filters simultaneous mobile robot localization people tracking 
proc 
ieee international conference robotics automation 
moore schneider deng 
efficient locally weighted polynomial regression predictions 
proc 
international conference machine learning 
murphy russell 
rao blackwellised particle filtering dynamic bayesian networks 
doucet 
olson 
probabilistic self localization mobile robots 
ieee transactions robotics automation 
omohundro 
efficient function constraint classification learning 
lippmann moody touretzky editors advances neural information processing systems 
morgan kaufmann 
pelikan goldberg cant paz 
bayesian optimization algorithm population size time convergence 
proc 
genetic evolutionary computation conference gecco 
pitt shephard :10.1.1.117.9046
filtering simulation auxiliary particle filters 
journal american statistical association 
rice 
mathematical statistics data analysis 
duxbury press second edition 
bekey 
bayesian estimation kalman filtering unified framework mobile robot localization 
proc 
ieee international conference robotics automation 
schulz burgard fox cremers 
tracking multiple moving targets mobile robot particle filters statistical data association 
proc 
ieee international conference robotics automation 
simmons koenig 
probabilistic robot navigation partially observable environments 
proc 
international joint conference artificial intelligence ijcai 
smith gelfand 
bayesian statistics tears perspective 
american statistician 
smith self cheeseman 
estimating uncertain spatial relationships robotics 
cox wilfong editors autonomous robot vehicles 
springer verlag 
thrun beetz burgard cremers dellaert fox rosenberg roy schulte schulz 
probabilistic algorithms interactive museum tour guide robot minerva 
international journal robotics research 
thrun fox burgard dellaert 
robust monte carlo localization mobile robots 
artificial intelligence 
thrun langford fox 
monte carlo hidden markov models learning nonparametric models partially observable stochastic processes 
proc 
international conference machine learning 
verma langford simmons 
non parametric fault identification space rovers 
international symposium artificial intelligence robotics space 
andrieu doucet godsill 
particle methods bayesian modelling enhancement speech signals 
ieee transactions speech audio processing 
accepted publication 
vlassis kr se 
auxiliary particle filter robot localization high dimensional sensor observations 
proc 
ieee international conference robotics automation 
wan van der merwe 
unscented kalman filter nonlinear estimation 
proc 
symposium adaptive systems signal processing communications control 
welch bishop 
kalman filter 
notes acm sig graph tutorial kalman filter 
wolf burgard burkhardt 
robust vision localization mobile robots image retrieval system invariant features 
proc 
ieee international conference robotics automation 

