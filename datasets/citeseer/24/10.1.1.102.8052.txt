learning finite state controllers partially observable environments nicolas meuleau leonid peshkin kee kim leslie pack kaelbling computer science department box brown university providence ri nm ldp kek cs brown edu reactive memoryless policies sufficient completely observable markov decision processes mdps kind memory usually necessary optimal control partially observable mdp 
policies finite memory represented finite state automata 
extend baird moore vaps algorithm problem learning general finite state automata 
performs stochastic gradient descent algorithm shown converge locally optimal finitestate controller 
provide details algorithm consider question conditions stochastic gradient descent outperform exact gradient descent 
conclude empirical results comparing performance stochastic exact gradient descent showing ability algorithm extract useful information contained sequence past observations compensate lack observability time step 
learning optimal policy large partially observable environment recurrent problem application domains ai 
known technique scales increasing size difficulty problem 
situation due part fact planning partially observable environments difficult task learning plan easier 
partially observable markov decision process pomdp provides formal framework studying problems :10.1.1.107.9127:10.1.1.53.7233
difficulty planning partially observable environments illustrated fact optimal policy pomdp may complete previous history system sequence observations actions rewards time determine action perform 
need infinite memory want act optimally infinite horizon 
general way represent policies form call policy graphs 
policy representation form possibly infinite policy graph 
priori optimal solution pomdp may infinite policy graph 
evident computational limits may reduce search policies representable finite policy graphs 
existing algorithms learning plan pomdps rely similar assumption 
instance researchers try learn memoryless reactive policies mccallum learning algorithm uses finite horizon memory wiering schmidhuber hql learns finite sequences reactive policies implicit memory previous observations peshkin look optimal finite policies :10.1.1.14.5408
finite memory architectures correspond finite policy graphs particular structure case node transition choice action possible graph 
previous examples search finite space criterion optimality search finite graph value value optimal bayesian solution 
need explicitly continuous space belief functions cumbersome intractable process 
approach uses em find finite controller optimal finite horizon 
companion proposed solve problems large state space fixing size policy graph trying find best graph size 
may hope find graph size realizes compromise quality solution time required finding 
approach allows passing note remember finite number events general unconstrained finite policy graphs remember events arbitrarily far past 
belief state space performing computation discrete setting completely observable markov decision processes mdps 
algorithms provide evaluation quality solution produced relative optimal performance 
showed companion finding best finite policy graph size np hard 
classical optimization techniques branch search gradient descent accelerated previous knowledge structure problem hand optimal solution 
despite leverage techniques escape enumerating set states pomdp iteration state space pomdp 
applied problems large number states combinatorial problems state process vector state features number states exponential number features 
require complete initial knowledge parameters pomdp learn policy learning model environment 
direct model free learning policy possibly simulated interaction process classical technique planning large state spaces :10.1.1.32.7692
idea perform stochastic gradient descent sampling state transitions rewards experience 
sample possible reasonably probable trajectories algorithm may efficient exact method enumerates trajectory including impossible low probability ones 
principle basis successful application reinforcement learning rl real world problems 
propose model free algorithm learning general finite policy graphs size 
algorithm learn finite memory policies environments large number states 
performing stochastic gradient descent parameters policy graph ensured converge local optimum 
basically extension baird moore vaps algorithm learning simple reactive policies :10.1.1.14.5408
constitutes significant improvement original vaps restriction reactive policies severe handicap partially observable domains 
organized follows 
give quick optimality criterion bayesian approach expected discounted cumulative rewards expectation relative prior belief states singh showed stochastic reactive policies perform arbitrarily better deterministic ones 
proven best stochastic reactive policy arbitrarily worse optimal memory policy 
pomdps policy graphs 
second develop formalism baird moore vaps algorithm general framework finite policy graphs 
represents main contribution 
discuss conditions stochastic gradient descent outperform exact gradient descent possible problem known advance 
pole balancing problem show algorithm solve difficult real world problems limited observability state system 
pomdps finite policy graph pomdps partially observable markov decision process pomdp defined tuple finite set states finite set observations finite set actions underlying markov decision process mdp optimized way initial state aim maximize expected discounted cumulative reward discount factor 
optimal solution mapping remarkable property mdps exists optimal policy executes action state 
unfortunately policy partially observable framework residual uncertainty current state process 
pomdp policy rule specifying action perform time step function previous history complete sequence observation action pairs time 
particular kind policy called reactive policies rps condition choice action observation 
represented mappings probability distribution starting state policy reactive realizes expected cumulative reward classical bayesian approach allows determine policy maximizes value 
updating state distribution belief time step depending observations 
problem reformulated new mdp original states 
generally optimal solution reactive policy 
sophisticated behavior optimal balance exploration exploitation 
unfortunately bayesian calculation highly intractable searches continuous space beliefs considers possible sequence observations 
finite policy graphs policy graph pomdp graph nodes labeled actions arcs labeled observations arc emanating node possible observa tion 
system certain node executes action associated node 
implies state transition pomdp eventually new observation depends arrival state underlying mdp 
observation conditions transition policy graph destination node arc associated new observation 
interested stochastic policy graphs action choices node transitions probabilistic 
notation set nodes graph current node time probability choosing action node probability moving node node observation probability distribution initial node conditioned observation illustrates functioning policy graphs pomdps 
influence diagram illustrating functioning policy graphs pomdps 
dotted arrows represent dependencies take account represented formulations 
policy representation possibly infinite policy graph 
policy chooses different action possible previous history represented infinite tree branch possible history 
reactive policies correspond special kind finite policy graph nodes observations pomdp structure fixed 
finite memory architectures hql finite rp sequences finite external memory policies correspond finite policy graph special structural constraints 
finding optimal policy graph problem finding optimal policy graph size studied companion 
principle study exploit markov property association pomdps finite policy graphs 
ends proposition algorithms scale relatively respect size problem sensitive size policy graph 
methods enable solution problems states reasonable time approach fundamentally limited necessity enumerate complete set states pomdp time step 
fail solve problems exponentially states huge combinatorial problems met real world 
algorithms basically planning algorithms require complete accurate preliminary knowledge pomdp parameters learn policy line 
possible solution overcome curse dimensionality state space consists having direct reinforcement learning rl algorithm learn policy possibly simulated interaction process :10.1.1.32.7692
concentrate computation general definition finite state automata node depends previous node observation action case instance graphs representing external memory policies 
algorithm easily generalized framework 
interesting parts state space neglecting highly state transitions 
rest presents model free algorithm learning finite state controllers size 
simulated experience protocol learning direct interaction real environment process 
stochastic gradient descent general finite policy graphs baird moore vaps algorithm learns reactive policy trial interaction process optimized :10.1.1.14.5408
performing stochastic gradient descent general error measure tuned converge local optimum error measure probability 
formalism proposed encompasses kind error ranging classical residual markovian environments td td error uses sum rewards received trial :10.1.1.32.7692
origin name algorithm value td policy td search 
possible errors include sarsa advantage learning 
despite robustness type error vaps limited learns memoryless policies 
effective partially observable environments 
extend structure policy graph completely fixed advance case rps 
precisely algorithm learn general finite policy graph size possibly simple structural constraints 
develop formalism vaps general finite policy graphs 
presentation directly follows baird moore :10.1.1.14.5408
error functions assume problem goal achievement task exists absorbing goal state system reach fast possible 
assume goal state associated unique observation state produces system knows certainty reached goal 
write high level optimality criterion expectation trajectories set experience sequences termi nate time represents total error associated sequence total error additively separable instantaneous error function associated finite sequence prefix observation necessarily represents sequence truncated time denote set sequence prefixes length 
possibilities defining immediate error including squared bellman residual error sarsa error advantage learning see details 
definitions complete sense markovian environments 
pomdps approximate approach instance error sarsa learn rps pomdps 
algorithm finds local optimum error guarantees correspond optimal policy 
immediate error induces td search adapted non markovian environment 
notably error criteria optimality policy equation equation equal opposite signs 
rational try minimize equal stochastic gradient descent general framework represented parametric functions weights objective func tion re written note learning rp initial vaps algorithm completely determined omitted sequence weight partial derivative general easy calculate 
case 
difficulty differentiate conventions shown stochastic gradient descent error performed repeating trials interaction process 
experienced trial length provides sample estimate expectation equation 
course samples independent introduce bias sum different estimates 
trial weights kept constant approximate gradients error time accumulated 
weights updated trial sum immediate gradients 
incremental implementation algorithm obtained step update rules condition satisfied exist zero probability trajectories non zero contribution gradient 
represents experience prefix time step size parameter learning rate trace weight representation respectively 
complete policy update performed trial length trial 
note traces independent immediate error 
depend way policy graph parameters vary weight representation chosen parameters 
main novelty algorithm compared original vaps second trace analogous original trace summarizes node transition executed trial action choices 
examples look tables store parameters policy graphs weight denoted possible weight possible weight suppose immediate error performing td search 
contribution update weight time step sequence expressed number times action executed node time number times moved node node observation time 
despite simplicity look table representation drawbacks 
weights represent probabilities subject constraints 
matter fact guarantees probabilities belong sum apply update rule described 
classical solution problem involves projecting gradient simplex applying 
second drawback look table representation guarantee derivative may equal equation points policy graph space 
studying express gradient cases falls scope see 
experiments soft max function boltzmann law represent parameters graphs 
case weights values temperature parameter 
complicates calculation gradient slightly representation avoids problems look tables values take real values induced policy gives probability choice 
boltzmann law may strongly modify shape error function respect weights influences performance gradient algorithms vaps 
difficult say priori influence beneficial negative problem 
variants remarks straightforward extend algorithm handles simple constraints policy graph 
constraint graph represent rp algorithm equivalent baird moore original vaps 
consider example finite external memory architecture peshkin 
ways model architecture augment pomdp state observation action spaces rp leave pomdp unchanged complex policy graph simple rp graph contains nodes number external memory bits 
case probability changing content memory represented second case represented results coherent sense update rule uses completely similar way algorithm interpretation chosen 
possibility learn finite rp sequences hql defining new error function hq values algorithm 
case converge rp sequence locally optimal sense expected total reward 
second find local minimum error may correspond policy maximizes locally expected discounted reward 
question treat discounted problems goal state natural load unload problem locations agent starts unload location receives reward time returns place passing load location 
problem partially observable agent distinguish different locations load unload perceive loaded 
tion trial called maintainance tasks 
possibility dealing discounted maintainance task time step execute independent random drawing determine terminate trial 
set probability trial constant equal discount rewards received trial policy graph maximizes op policy graph usual sense equation 
trick allows td learning maintainance tasks adapted kinds immediate error baird moore argue vaps adapted discounted pomdps immediate error clear introducing bias estimates 
numerical simulations section results experiments 
aims comparing exact gradient algorithm stochastic gradient approach vaps 
second shows algorithm solve real world problem 
experiments immediate error initialized policy graph uniform distributions 
comparison exact gradient descent model free learning algorithm vaps may learn policy know parameters pomdp advance 
explained section useful problem perfectly known advance protocol simulated experience allows optimizing huge problems sparse structure sampling probable trajectories considering trajectories 
interesting look conditions vaps expected outperform exact algorithm 
noted exact gradient calculation optimal load unload gamma exact gradient vaps clock ticks learning curves vaps exact gradient descent load unload problem chosen optimal value algorithm re sults stochastic gradient averaged experiences 
sensitive size state space pomdp step computation complexity vaps influence size clear complexity updating weight independent bigger state space require induce longer experience trials 
practice easy build problem states observations vaps completely outperforms exact gradient terms real computing time 
rule vaps scales better exact gradient problems big state spaces 
surprising handling big state spaces precisely original motivation 
second important variable comparison discount factor general bigger helps exact stochastic gradient algorithms increases value function gradient steepest 
may contradictory effects algorithms 
case vaps trials ended probability bigger longer instructive trials 
hand exact gradient calculation requires solving equations cross product mdp cf 
section 
done successive approximation value iteration sensitive bigger iterations needed reach accuracy 
opposite tendencies algorithms increasing accelerate slow 
clarify point ran exact gradient algorithm vaps simple load unload problem locations 
number nodes graph fixed optimal number problem 
tried values ranging plotted learning curves produced optimal vaps load unload gamma exact gradient clock ticks algorithms 
learning curves represent evolution performance current policy expressed percentage optimal performance function real time spent learning expressed computer 
simple load unload provides illustration mechanism depicted 
exact gradient clearly outperforms vaps cf 

increases difference algorithm vanishes 
techniques roughly equivalent 
point vaps dominates exact gradient descent 
experiments execution time exact stochastic gradient descent increase increase dramatic case exact gradient 
stochastic gradient descent outperform exact gradient problems large state space large discount factor 
experiments pole balancing ran number experiments pole balancing problem 
famous problem known solved rp observation time step composed elements cart position speed pole angle angular speed measure difficulty task performance algorithm different settings completely observable relevant variables seen algorithms time step partially observable hidden 
setting ran different algorithms settings sarsa baird moore original vaps learning rp extension vaps allowing learn policy graphs varying number nodes graph 
sarsa original vaps expected succeed completely observable setting fail partially observable reactive policy performs task 
average performance sarsa pole balancing completely observable number trials rp learning curves obtained completely observable pole balancing problem 
rp stands original vaps algorithm proposed baird moore represents extension algorithms differ radically 
hand vaps immediate error equivalent td baird moore call pure policy search 
hand sarsa basically value search line td 
algorithm expected succeed settings provided sufficiently large policy graph algorithm get stuck local optimum 
nodes completely observable setting reactive policy actions case represented node policy graph 
partially observable framework nodes added allow algorithm memorize past observations 
experiments discount factor set increased gradually learning progressed 
learning rate optimized independently algorithm 
performance algorithm measured fixing policy executing trials measuring length trial terms control decisions averaging measures 
value intervals cart position pole position partitioned unequal parts smaller size partition center completely observable setting parts partially observable setting correspondingly 
making decisions rate hz meaning example actual physical time learning balance pole sequential ticks corresponds seconds balancing 
parameters cart pole balancing problem taken described supplementary www page 
presents learning curves obtained completely observable framework 
horizontal axis represents number trials corresponds num average performance pole balancing partially observable sarsa rp number trials learning curves obtained partially observable pole balancing problem 
rp stands original vaps algorithm proposed baird moore represent extension nodes respectively 
ber times dropped pole 
vertical axis represents performance algorithm measured explained 
see sarsa learns faster original vaps showing value search efficient policy search control problem extension vaps node policy graph learns slower original vaps 
phenomenon explained fact space nodes policy graphs bigger space rps 
presents results obtained partially observable framework 
results confirm expectation algorithms limited reactive policy fail 
contrast algorithm increases performance gradually showing able compensate lack observability 
nodes algorithm better performs 
striking see performance algorithm improve steps difficult predict learning 
limited time continue experiments iterations know performance continue increase system may balance infinitely long 
currently running experiment results shown forthcoming technical report 
significant current result learn structure policy graph extracts useful information contained string past observations compensate partially lack observability 
pole balancing widely accepted benchmark problem dynamic system control best knowledge learned partial information 
derived extension general algorithm enables learn policies memory 
basic principle algorithm perform stochastic gradient descent finite state controller parameters guarantees local optimality solution produced 
led experimental study approach compared classic non adaptive algorithms terms execution time learning speed 
showed algorithm solve difficult problem having access information usually required solve 
able find structure policy graph extracts useful information contained sequence past observations compensate lack observability time step 
believe constitutes significant achievement proves algorithm efficient realworld problems 
astrom 
optimal control markov decision processes incomplete state estimation 
math 
anl 
appl 
baird 
residual algorithms reinforcement learning function approximation 
machine learning proceedings twelfth international conference san francisco ca 
morgan kaufmann 
baird moore :10.1.1.14.5408
gradient descent general reinforcement learning 
advances neural information processing systems 
mit press cambridge ma 
bertsekas tsitsiklis 
neuro dynamic programming 
athena belmont ma 
dean hanks 
decision theoretic planning structural assumptions computational leverage 
journal ai research appear 
cassandra 
exact approximate algorithms partially observable markov decision processes 
phd thesis brown university 
cassandra kaelbling littman 
acting optimally partially observable stochastic domains 
proceedings twelfth national conference artificial intelligence 
hansen 
improved policy iteration algorithm partially observable mdps 
advances neural information processing systems 
mit press cambridge ma 
hansen 
finite memory control partially observable systems 
phd thesis department computer science university massachusetts amherst 
hansen 
solving pomdps searching policy space 
proceedings eighth conference uncertainty artificial intelligence pages madison wi 
hauskrecht 
planning control stochastic domains imperfect information 
phd thesis mit cambridge ma 

optimal control complex structured processes 
phd thesis university france 
howard 
dynamic programming markov processes 
mit press cambridge 
jaakkola singh jordan 
reinforcement learning algorithm partially observable markov problems 
advances neural information processing systems 
mit press cambridge ma 
kaelbling kim meuleau peshkin 
searching finite state pomdp controllers 
technical report cs brown university 
kaelbling littman cassandra 
planning acting partially observable stochastic domains 
artificial intelligence 
kaelbling littman moore 
reinforcement learning survey 
journal artificial intelligence research 
littman 
memoryless policies theoretical limitations practical results 
animals animats proceedings third international conference simulation adaptive behavior 
mit press cambridge ma 
mccallum 
overcoming incomplete perception utile distinction memory 
proceedings tenth international machine learning conference amherst ma 
mccallum 
reinforcement learning selective perception hidden state 
phd thesis university rochester rochester ny 
meuleau 
importance impossible trajectories vaps algorithm 
preparation 
meuleau kim kaelbling cassandra 
solving pomdps searching space finite policies 
proceedings fifteenth conference uncertainty artificial intelligence appear 
peshkin meuleau kaelbling 
learning policies external memory 
proceedings sixteenth international conference machine learning appear 
puterman 
markov decision processes discrete stochastic dynamic programming 
wiley new york ny 
lave 
markov decision processes probabilistic observation states 
management science 
singh jaakkola jordan 
learning state estimation partially observable markovian decision processes 
machine learning proceedings eleventh international conference 

smallwood sondik 
optimal control partially observable markov decision processes finite horizon 
operations research 
sondik 
optimal control partially observable markov decision processes infinite horizon discounted costs 
operations research 
sutton 
learning predict method temporal differences 
machine learning 
sutton barto 
reinforcement learning 
mit press cambridge ma 
wiering schmidhuber 
hq learning 
adaptive behavior 
williams 
theory connectionist systems 
technical report nu ccs northeastern university boston ma 
