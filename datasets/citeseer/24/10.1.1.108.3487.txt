automatically tuned linear algebra software clint whaley jack dongarra computer science department university knoxville tn mathematical sciences section oak ridge national laboratory oak ridge tn describes approach automatic generation optimization numerical software processors deep memory hierarchies pipelined functional units 
production software machines ranging desktop workstations embedded processors tedious time consuming process 
described help automating process 
concentrate orts widely linear algebra kernels called basic linear algebra subroutines blas 
particular general matrix multiply dgemm 
ofthe technology approach developed applied level blas general strategy impact basic linear algebra operations general may extended important kernel operations 
contents motivation atlas atlas approach 
building general matrix multiply chip multiply 
choosing correct looping structure 
cache blocking 
generation chip multiply 
instruction cache reuse 
floating point instruction ordering 
reducing loop overhead 
exposing parallelism 
finding correct misses 
putting 

compiler 
requirements performance 
ects poor compilers 
performance loss lack chip cache 
blas 
results results varying timing methods 
square matrix multiply 
lu timings 
threaded timings 
comparison downloading atlas blas compiler details chip multiply details list tables atlas performance comparison 
dgemm performance mflops sgi processor 
system summary 
cache large matrices 
cache small matrices 
theoretical observed peak mflops 
system atlas dgemm comparison platforms 
system atlas comparison platforms 
double precision lu timings various platforms 
asymptotic double precision lu performance 
threaded dgemm timings various platforms 
blas library version 
compiler version 
compiler ags 
chip multiply details systems 
list figures atlas vendor performance preview 
step matrix matrix multiply 
general matrix multiplication innermost matrix 
general matrix multiplication innermost matrix 
motivation today microprocessors peak execution rates ranging op op straightforward implementation fortran computations simple loops rarely results high performance 
realize peak rates execution simplest operations required tedious hand coded programming orts 
inception defacto standards blas means achieving portability ciency wide range kernel scienti computations 
blas heavily linear algebra computations solving dense systems equations way basic computing infrastructure applications 
blas basic linear algebra subprograms high quality building block routines performing basic vector matrix operations 
level blas vector vector operations level blas matrix vector operations level blas matrix matrix operations 
blas cient portable widely available commonly development high quality linear algebra software lapack example 
blas just standard speci cation semantics syntax operations 
set implementations written fortran attempt implementations promote ciency 
vendors provide optimized implementation blas speci machine architecture 
optimized blas libraries provided computer vendor independent software vendor 
general existing blas proven ective assisting portable cient software sequential vector shared memory high performance computers 
hand optimized blas expensive tedious produce particular architecture general created large market true platforms 
process generating optimized set blas new architecture slightly di erent machine version time consuming process 
programmer understand architecture memory hierarchy provide data optimum fashion functional units registers manipulated generate correct operands correct time best compiler optimization 
care taken optimize operations account parameters blocking factors loop unrolling depths software pipelining strategies loop ordering register allocations instruction scheduling 
computer vendors invested considerable resources producing optimized blas architectures 
cases near optimum performance achieved operations 
coverage level performance achieved uniform platforms 
example point wehave cient version matrix multiply pentium linux architecture 
goal develop methodology automatic generation highly cient basic linear algebra routines today microprocessors 
process processors chip cache reasonable compiler 
approach called automatically tuned linear algebra software atlas able match exceed performance vendor supplied version matrix multiply case 
complete timings section report timings various problem sizes multiple architectures 
preview complete coverage gure shows performance atlas versus vendor supplied dgemm available matrix multiply 
see section details results 
mflops dec alphastation dec alphastation double precision matrix matrix multiply various systems hp hp ibm power thin node ibm powerpc high node pentium mmx mhz pentium pro mhz pentium ii mhz pentium ii mhz system atlas vendor performance preview sgi sgi sgi ip sgi ip sun ii model sun ultra model sun ultra model sun ultra model vendor atlas atlas developed general methodology generation level blas describe approach carried preliminary results achieved 
moment operation supporting matrix multiply 
describe matrix multiply op op op matrix matrices size respectively 
general arrays large cache 
algorithm matrix multiply possible arrange operations performed data part cache dividing matrix blocks 
additional details see 
atlas approach approach isolated machine speci features operation routines deal performing optimized chip cache matrix multiply 
section code automatically created code generator uses timings determine correct blocking loop unrolling factors perform optimized chip multiply 
user may directly supply code generator detail desired user may explicitly indicate cache size blocking factor try details provided generator determine appropriate settings timings 
rest code change architectures handles looping blocking necessary build complete matrix matrix multiply onchip multiply 
building general matrix multiply chip multiply section describe code remains platforms routines necessary build general matrix matrix multiply xed size chip multiply 
section describes chip multiply code generator detail 
section know cient chip matrix matrix multiply form multiply xed size dimensions set system speci value 
available cleanup codes handle cases caused dimensions multiples blocking factor 
user calls rst decision problem large bene special techniques 
algorithm requires copying operand matrices problem small cost miscellaneous overheads function calls multiple layers looping optimized slower traditional loops 
size required costs dominate lower order terms varies machines switch point automatically determined installation time 
small problems standard loop multiply simple loop unrolling called 
code called algorithm unable allocate space blocking see details 
assuming matrix large presently algorithms performing general chip multiply 
algorithms correspond di erent orderings loops outer loop rows second loop columns order reversed 
dimension common loop currently innermost loop 
de ne input matrix looped outer loop outer outermost matrix input matrix inner innermost matrix 
algorithms try allocate space store output temporary panel outermost matrix entire inner matrix 
fails algorithms attempt allocate space hold panel minimum workspace required routines nb 
amount workspace allocated previously mentioned small case code called 
space copy entire innermost matrix see bene ts doing matrix copied time workspaces cache get complete reuse innermost matrix data copying limited outermost loop protecting inner loops unneeded cache thrashing space copy entire innermost matrix allocated innermost matrix entirely copied panel outermost matrix outermost matrix copy bdm times 
usable cache reduced copy panel innermost matrix take panel size cache true outermost panel copy seen rst time secondary loop 
regardless looping structure allocation procedure inner loop operation done inner loop routines shown gure 
step matrix matrix multiply operation actions performed order calculate nb block ci range dm nbe dn nbe 
zero section workspace store ab call workspace allocate nb nb allocated partial matrix false 
copy block major format partial matrix true 
allocate nb nb allocated call small case code return nb alpha nb block major format nb partial matrix nb block major format nb nb nb chip nb nb beta general matrix multiplication innermost matrix 
call chip multiply multiply block row panel block column panel dk 
chip multiply performing operation ab expected results multiplying row panel column panel 
perform block operation building inner loop loop orderings giving algorithms chip matrix multiplication 
figures give pseudo code algorithms 
simplify code showing cleanup code necessary cases dimensions evenly divide matrix copies shown coming case 
array access copy changes 
choosing correct looping structure call matrix multiply routine decide loop structure call matrix put outermost 
matrices di erent size cache reuse encouraged deciding looping structure criteria matrix completely cache put innermost matrix get cache reuse entire inner matrix matrix ts completely cache put largest panel allocate nb nb allocated partial matrix false 
copy block major format partial matrix true 
allocate nb nb allocated call small case code return nb alpha nb block major format nb partial matrix nb block major format nb nb nb chip nb nb beta general matrix multiplication innermost matrix cache outermost matrix get cache reuse panel outer matrix code explicit blocking instance size cache known code criteria presently selection 
matrix accessed row panels copy instance matrix matrix put copied ciently 
means workspace copy front accessed column wise putting innermost loop copying entire matrix placed outermost loop cost copying row panel lower order term 
matrices access patterns outermost matrix accessed columns 
cache blocking previously mentioned algorithms perform explicit cache blocking achieve substantial implicit caching certain cases 
innermost matrix ts completely cache algorithm get maximal cache reuse 
room cache panel matrix copy innermost panel don space copy innermost matrix front get extremely cache reuse reusing outermost matrix panel panels innermost matrix 
obvious opportunities cache reuse code presently take advantage 
rst case cache big hold entire panel case loop intermixed outer loops order achieve cache reuse 
algorithm blocks panels retained cache appropriate sections input matrix brought perform multiplication 
requires xn panels retained cache multiple writes option tends degrade performance 
run experiments test feasibility idea see performance gain 
interesting case multiple matrix panels cache entire innermost matrix 
case looping mechanism slightly complex 
algorithm copies panels outermost matrix cache reuses applying panels innermost matrix 
small large number panels outermost matrix previously discussed case 
restriction cache reuse grows idea promising implemented 
platforms surveyed timing section dec alphastation showed appreciable performance gains platform showed maximal speedup obviously problem sizes showed speedup platform gained speedup greater 
decided modest gain point justify added code complexity supporting explicit blocking 
reasons consider explicit instance deciding matrix innermost reasons support explicit calculation size algorithm easy add 
generation chip multiply previously mentioned chip matrix matrix multiply code change depending platform 
copy input matrices blocked form case required wehave chosen case chosen opposed instance ab generates largest ops cache misses ratio possible loops written unrolling 
machines hardware allowing smaller ratio addressed loop unrolling loops addressed permuting order loop technique 
multiply designed cache reuse brings input matrices completely cache reuses matrix looping rows columns input matrix 
code brings matrix loops columns arbitrary choice theoretical reason superior bringing looping rows common misconception cache reuse optimized input matrices matrices cache 
fact win tting matrices cache possible assuming cache write save cost pushing previously sections back cache 
ignoring cost maximal cache reuse case achieved ts cache room columns cache line column accessed time scenario having storage columns assures old column data cache ows making certain kept place obviously assumes cache replacement policy 
cache reuse account great amount performance win obviously factor 
chip matrix multiplication relevant factors instruction cache ow floating point instruction ordering loop overhead exposure possible parallelism number outstanding cache misses hardware handle execution blocked instruction cache reuse instructions cached important chip multiply instructions cache 
means able completely unroll loops instance 
floating point instruction ordering discuss oating point instruction ordering usually latency hiding 
modern architectures possess pipelined oating point units 
means results operation available cycles number stages oating point pipe typically 
remember onchip matrix multiply form individual statements naturally variant ofc 
architecture possess fused multiply add unit cause unnecessary execution stall 
operation register issued oating point unit add started result computation available cycles oating point pipe utilized 
solution remove dependence separating multiply add issuing unrelated instructions 
reordering operations done hardware order execution compiler generate code quite cient doing explicitly 
importantly platforms capability example gcc pentium case performance win large 
reducing loop overhead primary method reducing loop overhead loop unrolling 
desirable reduce loop overhead changing order instructions unroll loop dimension common unroll loop 
unrolling dimensions loops changes order instructions resulting memory access patterns 
exposing parallelism modern architectures multiple oating point units 
barriers achieving perfect parallel speedup oating point case 
rst hardware limitation hands oating point units need access memory perfect parallel speedup memory fetch usually need operate parallel 
second prerequisite compiler recognize opportunities parallelization amenable software control 
classical employed cases unrolling loops 
finding correct number cache misses operand register fetched memory 
operand cache fetched memory hierarchy possibly resulting large delays execution 
number cache misses issued simultaneously blocking execution varies architectures 
minimize memory costs maximal number cache misses issued cycle memory cache 
theory permute matrix multiply ensure true 
practice ne level control di cult ensure problems running instruction cache generation precision instruction sequence instance 
method control cache hit ratio classical loop unrolling 
putting obvious interacting ects di cult impossible predict priori best blocking factor loop unrolling approach code generator coupled timer routine takes initial information tries di erent strategies loop unrolling latency hiding chooses case demonstrated best performance 
timers structured operations large granularity leading fairly repeatable results non dedicated machines 
user may enter size cache program attempt calculate 
turn allows routine choose range blocking factors examine 
user may specify maximum number registers default dictate maximum amount ofm loop unrolling perform 
caused cache ow completely loop 
drastically reduces number cases search routine test 
search tries number possible blocking factors set amount ofm loop unrolling initial blocking factor chosen 
blocking factor range latency hiding factors presently tested 
latency factors produce speedup case latency hiding latency factor showing maximal performance tested timing 
initial blocking factor idea latency factor employ search routine loops loop possible number registers 
optimal unrolling try blocking factors latency factors choose best 
results stored les subsequent searches repeat experiments allowing searches build previously obtained data 
means search instance due machine failure previously run cases need re timed 
typical install takes hours precision 
cleanup code operations done square chip multiply xed dimension input matrices may multiple obvious need way handle remainder 
possible write cleanup code routine loops arbitrary dimension 
practice shows platforms results unacceptably large performance drops matrices dimensions multiples generating code possible cleanup cases di cult usable solution practice 
result routines take unacceptable amount compilation time user executable large 
key note majority time spent cleanup code case dimension equal generate roughly routines cleanup routines cases dimension remaining routines accept arbitrary known unroll inner loop critical reducing loop overhead 
routines generated general case correspond di ering values allowed 
routines dimension cient routines time spent negligible 
compiler 
ideal compiler capable performing optimization needed automatically 
compiler technology far mature perform optimizations automatically 
true blas widely marketed machines justify great expense compiler development 
adequate compilers widely marketed machines certain developed 
requirements performance approach wehave taken requirements necessary achieving performance 
cache oating point unit fetch operands cheaply level cache 
platform possesses adequate compiler 
requirements met poor performance may result 
systems attempted atlas installation unacceptable performance 
platforms reason performance loss summarized cray inadequate compiler intel inadequate compiler sgi nol cache accessible oating point unit sections describe drew give performance numbers user see magnitude performance loss 
ects poor compilers compiler play little role achieving performance code generator usually reserved compilers loop unrolling latency hiding 
compiler capable doing job register management oating point unit control 
code chip multiply compiler optimization low order cost 
examples role compiler play determining performance 
direct evidence comes experiments sgi cray 
nodes access dec alpha risc processors running mhz 
dec alphastation discussed report chip running mhz 
cray supplies compiler dec see large di erence performance 
despite having chip caches running roughly times clock rate timing numbers quite bit slower 
table shows atlas timing numbers platforms 
matrix order platform table atlas performance comparison intel unable get better op chip multiply general case 
access compiler platform state certain compiler fault 
cache obvious architectural peculiarities explain poor performance 
system supplied matrix multiply exceeds op know plenty achievable peak 
case got best performance loop unrolling happens platform 
anomalous result may due compiler inability handle increased register inherit outer loop unrolling 
option faced poor compiler try language 
hope provide option generate chip multiply 
legacy platforms er speed improvement coding performance loss lack chip cache system possess chip cache blocking perform help performance 
copy block format pure overhead 
machine access case sgi processor possesses chip cache accessible oating point unit 
oating point unit access chip cache level 
performance architecture extremely poor 
table shows performance atlas versus vendor supplied blas 
noted vendor supplied blas usually achieve greater performance performance upper lower apparently leading dimension timings poor performance degraded 
merely shows atlas compete platform system poor cases 
noted fortran blas available netlib run roughly op problem sizes timing 
matrix order system lib sys atl table dgemm performance mflops sgi processor blas point consider general method outlined section extended blas 
exception triangular solve level blas naturally expressed terms previously mentioned chip matrix multiply 
means system speci code generated support routines implies installation time increase additional blas supported 
support routines require development chip codes 
poor man blas may utilized order generate wider set level blas 
triangular solve written terms matrix multiply research needed see win compared directly generating onchip solve 
optimal performance demand mixture approaches 
level blas require di erent approach 
level luxury operations allows perform data copies concentrate optimization system speci code routines 
order operations done data feasible 
routines necessary generate code operation 
instance separate code generated transpose cases 
lead explosion routines generated timed implying extremely long installation times 
promising idea create general scheme tries mainly optimize memory fetch generally usable level perform complete generation timing sequence select routines special interest 

results section single double precision timings various platforms 
timings di erent blas timings ush cache call set leading dimensions arrays greater matrix timings section set leading dimension maximal size timed 
means performance numbers timing routine instance vendor supplied dgemm lower reported papers 
numbers general better estimate performance user see application 
devote brief section topic 
show timings square matrix multiply systems 
demonstrate performance shown timings translates actual applications give lu timings various systems 
platforms support show routines respond threading 
table shows con gurations various platforms wehave installed timed package 
appendix tables providing details 
table shows system blas timings 
note access hp optimal blas compare vector library describes optimized series 
tables show compiler version ags compiling chip matrix multiply 
abbr 
full clock data instr cache name name mhz cache kb cache kb kb mb dec alphastation mb dec alphastation kb mb hp hp hp hp unknown power ibm power thin node powerpc ibm powerpc high node mb pentium kb mmx pentium mmx kb pentium pro kb pii pentium ii kb pii pentium ii kb sgi ip sgi ip kb kip sgi ip mb kip sgi ip mb ms sun ii sun ultra model kb sun ultra model kb sun ultra model mb table system summary results varying timing methods numerous ways perform timings 
common method generate matrices call appropriate routine 
depending matrix cache sizes large di erence timings 
matrices signi cant portion matrices remain cache matrix generation memory costs main memory prevalent inthe timings 
small matrices signi cant portion matrices may remain cache timings truly misleading 
timers perform operation times row report best timing obtained 
result optimistic numbers 
obviously matrices level cache timings enjoy cache reuse just 
matrix cache may signi cant cache reuse 
instance chip multiply inner loop ts entirely level cache performance reported re ect cost bringing level cache 
timers set lda words matrices contiguous memory 
rules problems ill chosen leading dimension causes part cache instance 
insures maximal cache reuse 
unfortunately actual applications rarely case dgemm called leading dimension equal size matrix usually dgemm called submatrices larger array 
timings section memory corresponding size cache written read matrix generation matrices fetched main memory 
set leading dimension maximal size timed 
readily observed method gives lower bound performance commonly method gives upper bound 
just report upper bound 
reason upper bound achieved particular applications ones repeatedly memory space corrupting cache invocations problem size small cache large 
short users see timings indicative true performance 
appropriate timings important basing software decisions package 
case timing things cache causes non optimal code produced 
give reader feeling kinds di erences method timing cause provide examples 
tables method lda cache call 
method setting lda running problem times choosing best result 
note system blas timings clear speci implementation 
machines large caches table shows standard sizes time rest 
expect matrices get larger caching ects play role 
table shows thing smaller sizes problem severe 
pentium ii timings atlas access blas un der linux 
timing matrix order system method kip kip table cache large matrices timing matrix order system method pii pii kip kip table cache small matrices square matrix multiply table shows theoretical observed peaks matrix multiplication 
observed peak mean best repeatable timing produced platform problem size 
observed peak di ers best timings reported tables di erence usually due multiple blocking factor 
abbr 
clock theoretical dgemm mflops mflops name rate mhz peak vendor atlas vendor atlas unknown hp hp power powerpc mmx pii pii kip kip ms table theoretical observed peak mflops table shows times vendor supplied dgemm atlas dgemm platforms problems sizes ranging 
tables lib column indicates library timings sys system vendor supplied atl atlas matrix order system lib atl sys atl sys hp atl hp sys hp atl hp sys power atl power sys powerpc atl powerpc sys atl mmx atl atl pii atl pii atl atl sys atl sys kip atl kip sys kip atl kip sys ms atl ms sys atl sys atl sys atl sys table system atlas dgemm comparison platforms matrix order system lib atl sys atl sys hp atl hp sys hp atl hp sys power atl power sys powerpc atl powerpc sys mmx atl atl pii atl pii atl atl sys kip atl kip sgi kip atl kip sys ms atl ms sys atl sys atl sys atl sys table system atlas comparison platforms lu timings order demonstrate routines provide performance practice timed lapack lu factorization linking vendor supplied dgemm produced atlas 
blas fastest available usually ones supplied vendor 
blocking factor factorization determined timing blocking factors choosing performed best lu factorization matrix order 
previous timings caches ushed start algorithm 
table shows lu performance platforms 
lib column overloaded convey dgemm atlas system blocking factor chosen instance column indicates run atlas dgemm blocking factor 
matrix order system lib powerpc powerpc power power kip kip kip kip pii table double precision lu timings various platforms reader may notice lu times general timings 
rst reason sure include platforms vendor faster win marginal see adverse ects detail 
similar vein noted lu timings platform kip worse atlas vendor dgemm atlas faster large matrix multiplication 
due fact lu perform dgemm entire matrix uses rank update 
get matrix multiply size equal square matrix multiply order rst case atlas beat supplied blas run size lu 
subsequent dgemm calls smaller matrices 
see discrepancies dgemm lu timings 
order demonstrate matrix size primary reason di erence show performance large lu factorizations select platforms table matrix order system lib power power kip kip table asymptotic double precision lu performance threaded timings platforms surveyed multiple processors accessible threading 
implemented simple parallel matrix multiply pthreads 
table shows threaded timings platforms 
architectures sun provided vendor supplied threaded multiply report system numbers platforms 
powerpc produced incorrect results problem tracked 
table column lib overloaded provide number processors available 
indicates atlas run processors indicates system vendor supplied dgemm run processors 
matrix order system lib powerpc pii table threaded dgemm timings various platforms comparison orts produce optimal codes code generation 
closest parallel atlas seen ort 
deals code generator blas 
predates atlas years natural ask di erences packages atlas project begun 
atlas started needed optimized dgemm pentium running linux 
authors reported disappointing performance onthe linux intel platform longer case 
examined issue creating cient dgemm platform readily apparent require little ort portable 
answers question atlas begun rst place tell di erent 
main di erence complexity approach 
atlas puts system speci code square chip multiply 
uses chip code coerce problems format 
atlas counts level cache accessible oating point unit order able simplifying step writing chip multiply 
means need generate time routine new platform 
resulted code generator nishes relatively short time generally hours operations timed arti cially ated order ensure repeatability 
hand chose comprehensive approach directly optimizing individual operation 
means di erent code generated transpose combination instance 
results lengthy installation process usually matter days multiple cases routine generated timed 
approaches better 
approach probably yield better performance small problems may avoid unnecessary data copies machines cache 
methods code generation level blas pretty unchanged level 
cost increased generality seen longer installation time performance may sensitive factors poorly chosen leading dimensions atlas somewhat shielded factors data copy best way determine packages user time speci application 
user wishes compare raw performance reported publications mentioned timing method 
current timings reported timing method discussed section 
means performance numbers general include costs bringing operands cache 
section give reader idea ects 
downloading atlas alpha release atlas www netlib org utk projects atlas 
installation instructions provided supplied readme le 
currently code generator atlas works matrix multiplication isthe basic operation underlying blas 
extend generator level blas 
level blas consider level blas implement level blas solving triangular systems right hand sides terms matrix multiplication 
important operation matrix vector multiplication 
develop version atlas produce full set blas run variety architectures interest publicly available perform optimizations architectures problem shapes sizes alignments choice 
planning study architectures interest including development cost models prototyping hand written blas developing algorithmic generators appropriate architectures 
risc processors vector instructions require loop lengths match optimal vector lengths 
smps require load balancing avoiding false sharing cache lines di erent processors 
di erent ways thread management considered 
planning develop re ne algorithms exploit sparse blas 
sparse matrix vector multiplication essential kernel iterative algorithms large matrix problems 
optimizing performance requires number architecture matrix dependent transformations 
study extend atlas optimize vector multiplication optimizations may depend sparsity structure 
demonstrated ability produce highly optimized matrix multiply wide range architectures code generator probes searches system optimum set parameters 
avoids tedious task generating hand routines optimized speci architecture 
believe ideas expanded cover level blas level blas 
addition scope additional operations blas sparse matrix vector multiplication ffts 
anderson bai bischof demmel dongarra du greenbaum hammarling mckenney sorensen 
lapack users guide second edition 
siam philadelphia 
pages 
choi cleary azevedo demmel dhillon dongarra hammarling henry stanley walker whaley 
scala pack users guide 
siam philadelphia 
bilmes chin demmel 
optimizing matrix multiply portable high performance ansi coding methodology 
proceedings international conference supercomputing vienna austria july 
acm 
dongarra du du hammarling 
set level basic linear algebra subprograms 
acm trans 
math 
soft march 
dongarra du hammarling richard hanson 
extended set fortran basic linear algebra subroutines 
acm trans 
math 
soft march 
dongarra di 
ibm risc system linear algebra operations 
supercomputer 
ling van loan 
portable high performance level blas 
editor parallel processing scienti computing pages philadelphia 
siam 
toledo 
improving instruction level parallelism sparse matrix vector multiplication reordering blocking prefetching 
proceedings th siam conference parallel processing scienti computing 
siam 
blas compiler details section exists provide details regarding compilers blas timings 
table shows system blas timings 
tables show compiler version ags compiling chip matrix multiply 
system link version power powerpc hp revision hp revision standard execution environment fortran standard execution environment fortran kip standard execution environment fortran kip standard execution environment fortran ms lib sun performance library lib sun performance library lib sun performance library lib sun performance library table blas library version system compiler version cc digital unix compiler driver dec digital unix rev cc digital unix compiler driver dec digital unix rev power powerpc hp hp hp compiler hp gcc version mmx gcc version gcc version pii gcc version pii gcc version base compiler development environment base compiler development environment kip compiler development environment kip base compiler development environment ms cc workshop compilers oct cc workshop compilers oct cc workshop compilers oct cc workshop compilers oct table compiler version system compiler ags cc arch host tune host std assume aligned objects cc arch host tune host std assume aligned objects power pwr pwr powerpc ppc hp aa hp aa gcc frame pointer mmx gcc frame pointer gcc frame pointer pii gcc frame pointer pii gcc frame pointer cc mips cc mips opt targ platform ip targ processor alias typed blocking kip cc mips opt targ platform ip alias typed blocking kip cc mips opt targ platform ip alias typed blocking ms cc micro mem cc ultra xo mem cc ultra xo mem cc native xo mem table compiler ags dgemm system nb mu nu lat nb mu nu lat power powerpc hp hp mmx pii pii kip kip ms table chip multiply details systems chip multiply details table shows details loop blocking factors chip multiply various systems 
nb blocking factor mu unrolling loop nu unrolling loop lat latency factor 

