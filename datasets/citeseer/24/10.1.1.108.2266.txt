complexity solving markov decision problems michael littman thomas dean leslie pack kaelbling department computer science brown university waterman street providence ri usa email cs brown edu markov decision problems mdps provide foundations number problems interest ai researchers studying automated planning reinforcement learning 
summarize results regarding complexity solving mdps running time mdp solution algorithms 
argue mdps solved ciently theory study needed reveal practical algorithms solving large problems quickly 
encourage research sketch alternative methods analysis rely structure mdps 
markov decision process controlled stochastic process satisfying markov property costs assigned state transitions 
markov decision problem markov decision process performance criterion 
solution markov decision problem policy mapping states actions stochastically determines state transitions minimize cost performance criterion 
markov decision problems mdps provide theoretical foundations decision theoretic planning reinforcement learning sequential decision making tasks interest researchers practitioners arti cial intelligence operations research dean 
mdps employ dynamical models understood stochastic processes performance criteria established theory operations research economics combinatorial optimization social sciences puterman 
mdps exhibit special structure exploited expedite solution 
investment planning example initial state known certainty current price stock commodity result set reachable states prices viable invest ment strategies near term considerably restricted 
general notions time action reachability state space inherent characteristics mdps exploited produce cient algorithms solving 
important understand computational issues involved sources structure get idea prospects cient sequential parallel algorithms computing exact approximate solutions 
summarizes known unknown worth knowing computational complexity solving mdps 
brie mdp represented linear program lp solved polynomial time 
order polynomials large theoretically cient algorithms cient practice 
algorithms speci solving mdps known run worst case polynomial time 
algorithms analyses date little mdp speci structure results related areas monte carlo estimation markov chain theory suggest promising avenues research 
describing basic class problems 
markov decision problems purposes markov decision process state space action space state transition function instantaneous cost function 
state transition function de ned follows ij pr st st random variable denoting state action time cost ck de ned cost action state aj 
complexity results necessary assume encoded tables rational numbers 
maximum number bits required represent component ofp restrict attention discrete time processes nite 
markov decision process describes dynamics agent interacting stochastic environment 
initial state distribution states sequence actions markov decision process describes subsequent evolution system state possibly nite sequence times referred stages process 
focuses nite horizon case sequence stages nite 
policy mapping states actions 
policy independent current stage said stationary 
markov decision problem mdp markov decision process performance criterion 
performance criterion enables assign total cost state policy 
policy markov decision process initial state determine probability distribution sequences state action pairs called trajectories 
performance criterion assigns trajectory cost determined part instantaneous cost function probability weighted sum costs determine policy total cost state 
policy said dominate policy state total cost performing starting state equal total cost performing starting state state total cost performing strictly 
fundamental result theory mdps exists stationary policy dominates equal total cost policy bellman 
policy termed optimal policy total cost attaches state said optimal total cost state 
optimal solution markov decision problem policy total cost state optimal total cost 
problems interested optimal total cost function mapping states optimal total costs unique optimal policy need 
brie consider popular performance criteria expected cost target expected discounted cumulative cost average expected cost stage 
expected cost target criterion subset designated target cost assigned trajectory sum instantaneous costs state target set reached 
expected discounted cumulative cost criterion cost trajectory sum times instantaneous cost time rate indicates stage 
reasonable assumptions expected cost target expected discounted cumulative cost criteria give rise considered part input assumed encodable bits 
equivalent computational problems 
average expected cost stage criterion attractive require seemingly arbitrary discount rate speci cation set target states 
di cult criterion analyze 
focuses expected discounted cumulative cost criterion 
simplify notation suppose instantaneous costs dependent initial state action cij expected discounted cumulative cost respect state particular policy xed discount de ned system equations ji ij jj optimal total cost function de ned ji mine ji shown satisfy optimality equations ji min jj family equations due bellman basis practical algorithms solving mdps 
policy called optimal policy achieves optimal total cost function 
optimal total cost function follows arg min jj general complexity results known algorithm solve general mdps number arithmetic operations polynomial algorithm called strongly polynomial 
linear programming problem solved number arithmetic operations polynomial papadimitriou tsitsiklis analyzed computational complexity mdps 
showed cost criteria mentioned earlier problem complete 
means solvable polynomial time cient parallel algorithm available problems solvable ciently parallel outcome considered researchers eld 
linear programming problem complete result implies terms mdps lps equivalent fast parallel algorithm solving yield fast parallel algorithm solving 
known problems equivalent respect strong polynomiality clear strongly polynomial algorithm solving linear programs yield mdps inverse open 
papadimitriou tsitsiklis show mdps deterministic transition functions components optimal total cost functions ciently parallel cost criteria problem nc 
algorithms give strongly polynomial 
suggests stochastic nature mdps important consequences complexity mdps equally di cult solve 
algorithms analysis section describes basic algorithms solve mdps analyzes running times 
linear programming problem computing optimal total cost function nite horizon discounted mdp formulated linear program lp 
linear programming general technique appear take advantage special structure mdps 
reduction currently proof mdps solvable polynomial time 
primal linear program involves maximizing sum vj subject constraints vi vi variables solving optimal solution linear program determine optimal total cost function original mdp 
intuition state optimal total cost greater achieved rst action maximization insists choose greatest lower bound variables 
interest consider dual program involves minimizing sum subject constraints variables thought indicating amount policy ow state exits action interpretation constraints ow conservation constraints say total ow exiting state equal ow state plus ow entering state possible combinations states actions weighted probability 
objective minimize cost ow 
pis feasible solution dual ck interpreted total cost stationary stochastic policy chooses action state probability solution converted deterministic optimal policy follows arg max primal lp nm constraints variables dual constraints nm variables 
lps coe cients anumber bits polynomial algorithms solving rational lps take time polynomial number variables constraints number bits represent coe cients karmarkar 
mdps solved time polynomial drawback existing polynomial time algorithms run extremely slowly practice rarely 
popular practical methods solving linear programs variations dantzig simplex method 
simplex method works choosing subsets constraints satisfy equality solving resulting linear equations values variables 
algorithm proceeds iteratively swapping constraints selected subset continually improving value objective function 
swaps improve objective function optimal solution 
simplex methods di er choice pivot rule rule choosing constraints swap iteration 
simplex methods perform practice klee showed dantzig choices pivoting rule lead simplex algorithm take exponential number iterations problems 
pivoting rules suggested shown result exponential running times worst case 
shown result polynomial time implementation simplex 
note results may apply directly linear programming solve mdps set linear programs resulting mdps include counterexample linear programs 
open issue 
ways consider speeding solutions mdps nding improved methods solving lps solution methods speci mdps 
progress speeding linear programming algorithms subexponential simplex algorithm uses randomized pivoting rule kalai mdp speci algorithms hold promise cient solution 
address algorithms speci cally policy iteration value iteration sections 
policy iteration widely algorithms solving mdps iterative methods 
best known algorithms due howard known policy iteration 
policy iteration alternates value determination phase current policy evaluated policy improvement phase attempt improve current policy 
policy improvement performed mn arithmetic operations steps value determination steps solving system linear equations 
total running time polynomial number iterations required nd optimal optimal policy polynomial 
question addressed section 
basic policy iteration algorithm works follows 
deterministic stationary policy 

loop set 
determine ji solving set equations unknowns described equation 
exists jj ji set set 
repeat loop 
return step value determination phase step policy improvement phase 
distinct policies new policy dominates previous puterman obvious policy iteration terminates exponential number steps 
interested nding polynomial upper bound theory value determination probably done somewhat faster primarily requires inverting matrix done steps coppersmith winograd 
simple policy iteration requires exponential number iterations generate optimal solution family mdps illustrated condon 
showing upper bound exists number iterations exponential worst case 
direct analyses policy iteration scarce researchers examined variant policy iteration current policy improved state step 
detailed analogy constructed choice state update policy iteration choice pivot rule simplex 
shows feasible bases primal lp equation toone correspondence stationary deterministic policies 
simplex examples constructed sequential improvement policy iteration require exponentially iterations 
condon examine problem solving expected cost target mdps variations sequential improvement policy iteration algorithm 
version call simple policy iteration state labeled unique index iteration policy updated state smallest index policy improved 
show family counterexamples suggested particular starting policy takes exponential number iterations solve simple policy iteration 
counterexample constructed number gure 
states divided classes decision states labeled random states labeled absorbing state 
decision state actions available action heavy solid lines results transition decision state action dashed lines results transition random state random state choice action random transition probability reaching random state probability reaching decision state takes place 
actions decision state random state result transition absorbing state 
transition cost case decision state transitions zero cost 
initial policy decision state selects action takes decision state 
optimal policy 
policies highly similar condon show simple policy iteration steps policies arriving optimal policy 
example constructed expected cost target criterion mind holds discounted cumulative cost criterion regardless discount rate 
policy improved states parallel policy iteration longer direct simplex analogue 
open question lead exponential running time worst case resulting algorithm guaranteed converge polynomial time 
show popular version policy iteration strictly cient simple policy iteration algorithm mentioned 
policy iterations policy iteration 
ji total cost function associated ji total cost function byvalue iteration section starting ji initial total cost function 
puterman theorem shows ji dominates equal ji policy iteration converges slowly value iteration discounted nite horizon mdps 
combined result tseng described detail section bounds time needed value iteration nd optimal policy shows policy iteration takes polynomial time xed discount rate 
furthermore discount rate included part input rational number denominator written unary policy iteration takes polynomial time 
policy iteration pseudo polynomial time algorithm 
policy iteration runs polynomial time xed discount rate simple policy iteration take exponential time regardless discount rate 
novel observation stands contrast comment 
argues block pivoting simplex achieves goal parallel policy improvement policy iteration prefer commercial implementations simplex home grown implementations policy iteration 
argument misconception step policy improvement onn states equivalent iterations simple policy iteration 
fact policy improvement step states iterations simple policy iteration worst case 
policy iteration ruled preferred solution method mdps 
empirical study needed 
value iteration bellman devised successive approximation algorithm mdps called value iteration works computing optimal total cost function assuming rst stage nite horizon stage nite horizon 
total cost functions computed guaranteed converge limit optimal total cost function 
addition policy associated successive total cost functions converge optimal policy nite number iterations bertsekas practice convergence quite rapid 
basic value iteration algorithm described follows 
initialize 
set 
ji 

maximum number iterations ji jj ji min ji set 
arg min ji 
return maximum number iterations set advance determined automatically appropriate stopping rule 
bellman residual step de ned max ji ji examining bellman residual value iteration stopping gets threshold guarantee resulting policy optimal williams baird 
running time iteration mn method polynomial total number iterations required polynomial 
sketch analysis number iterations required convergence optimal policy detailed discussion tseng 

bound distance initial total cost function optimal total cost function 
maxi magnitude largest instantaneous cost 
total cost function policy components range 
choice initial total cost function components range di er optimal function state 

show iteration results improvement factor distance estimated optimal total cost functions 
standard contraction mapping result discounted mdps puterman 

give expression distance estimated optimal total cost functions iterations 
show gives bound number iterations required optimal policy 
iterations estimated total cost function di er optimal total cost function state 
solving result relating bellman residual total cost associated policy express maximum number iterations needed nd optimal policy log log 
argue value optimal policy fact optimal 
optimal total cost function expressed solution linear program rational components bits section 
standard result theory linear programming solution linear program written rational numbers component represented number bits polynomial size system schrijver 
means nd policy optimal policy optimal 

substitute value bound get bound number iterations needed exact answer 
substituting equation reveals running value iteration number iterations polynomial guarantees optimal policy 
analysis shows xed value iteration takes polynomial time 
useful constructing upper bound policy iteration see section 
known dependence quite large approaches dropped policy iteration show value iteration take long 
illustrates family mdps discovering optimal policy value iteration takes time proportional log 
consists states labeled 
state action causes deterministic transition state action causes deterministic transition state 
action instantaneous cost state cost time step 
action instantaneous cost value iteration requires number iterations proportional log generate optimal solution family mdps 
state zero cost absorbing state 
discounted nite horizon cost choosing action state total cost action smaller 
initialize value iteration zero total cost function estimate costs choices iteration 
value iteration continue choose suboptimal action iteration log log log worst case value iteration running time grows faster 
alternative methods analysis know mdps solved time polynomial unfortunately degree polynomial nontrivial methods guaranteed achieve polynomial time performance signi cant structure mdps 
furthermore multi commodity ow problem leighton existence linear programming solution preclude need cient algorithms means nding approximately optimal solutions 
section sketches directions pursued nd improved algorithms mdps 
depth empirical study existing mdp algorithms fruitful 
addition solution methods discussed earlier numerous elaborations hybrids proposed improve convergence speed running time 
puterman shin describe general method called modi ed policy iteration includes policy iteration value iteration special cases 
structure modi ed policy iteration essentially policy iteration value determination step replaced note costs speci ed log log bits 
approximation closely resembles value iteration xed policy 
bertsekas describes variations value policy iteration called asynchronous dynamic programming algorithms interleave improving policies estimating value policies 
methods resemble techniques reinforcement learning eld mdps solved performing cost update computations high probability trajectories 
promising approach literature involves heuristic dynamically choosing states update value iteration update improve estimated total cost function moore atkeson peng williams 
embarking study need compile suite benchmark mdps re ects interesting classes problems 
fast approximation algorithms useful trading solution accuracy time 
example approximation algorithms designed solving linear programs 
designed nding optimal solutions certain class linear programs includes primal linear program section plotkin 
particular scheme yield practical implementations useful solving linear programs exponentially constraints application approximate linear programming approaches mdps worth study 
probabilistic approximations desirable applications say nd optimal solution probability polynomial 
fully polynomial randomized approximation schemes generally designed problems computed exactly polynomial time dagum luby researchers developing iterative algorithms tight probabilistic performance bounds provide reliable estimates dagum 
optimal stopping rule monte carlo estimation 
identi ed properties graphs markov chains rapid mixing property markov chains jerrum sinclair approximating permanent may classify mdps easy hard problems 
related observation bertsekas context algorithm combines value iteration rule maintaining error bounds 
notes convergence algorithm controlled discount rate conjunction magnitude subdominant eigenvalue markov chain induced optimal policy unique 
value help characterize hard easy mdps 
done characterize mdps respect computational properties including experimental comparisons illustrate plenty easy problems mixed extraordinarily hard ones dean categorization schemes attempt relate measurable attributes mdps amount uncertainty actions type solution method works best kirman 
thing considered algorithms mentioned practice initial state known 
may possible nd nearoptimal solutions considering entire state space consider case relatively small takes stages reach log states initial state 
dean kaelbling kirman nicholson solve mdps algorithm exploits property provide error bounds performance 
barto bradtke singh rtdp real time dynamic programming algorithm exploits similar intuition nd optimal policy necessarily considering entire state space 
structure underlying dynamics allow aggregate states decompose problems weakly coupled subproblems simplifying computation 
aggregation long active topic research operations research optimal control schweitzer 
particular bertsekas casta non describe adaptive aggregation techniques important large structured state spaces kushner chen describe dantzig wolfe lp decomposition techniques solving large mdps 
researchers planning boutilier dean lin reinforcement learning kaelbling moore atkeson exploring aggregation decomposition techniques solving large mdps 
needed clear mathematical characterization classes mdps techniques guarantee approximations low order polynomial time 
preoccupation computational complexity ed 
theory mdps solved polynomial time size state space action space bits precision holds true called representations system dynamics states explicitly enumerated 
boutilier 
consider advantages structured state spaces representation dynamics log factor size state space 
cient algorithm mdps need run time bounded polynomial logarithm number number states considerably challenging endeavor 
summary focus primarily class mdps expected discounted cumulative cost performance criterion discount rate mdps solved linear programming number arithmetic operations polynomial number states number actions maximum number bits required encode instantaneous costs state transition probabilities rational numbers 
known strongly polynomial algorithm solving mdps 
general problem complete equivalent problem solving linear programs respect prospects exploiting parallelism 
best known practical algorithms solving mdps appear dependent discount rate value iteration policy iteration shown perform polynomial time xed value iteration take anumber iterations proportional log worst case 
addition version policy iteration policies improved state time shown require exponential number iterations regardless giving indication standard algorithm policy iteration strictly powerful variant 
note value iteration policy iteration signi cant structure underlying dynamical model 
fact linear programming formulation mdps solved polynomial time particularly 
existing algorithms solving lps provable polynomial time performance impractical mdps 
practical algorithms solving lps simplex method appear prone sort worst case behavior policy iteration value iteration 
suggest avenues attack mdps rst relax requirements performance second focus narrower classes mdps exploitable structure 
goal address problems representative types applications performance expectations practice order produce theoretical results interest practitioners 
nd current complexity results marginal practitioners 
call theoreticians practitioners generate set alternative questions answers inform practice challenge current theory 
acknowledgments justin boyan tony cassandra anne condon paul dagum michael jordan philip klein lu walter ludwig satinder singh john tsitsiklis marty puterman pointers helpful discussion 
barto bradtke singh 

learning act real time dynamic programming 
arti cial intelligence 
bellman 

dynamic programming 
princeton university press 
bertsekas 

dynamic programming deterministic stochastic models 
prentice hall englewood cli bertsekas casta non 

adaptive aggregation nite horizon dynamic programming 
ieee transactions automatic control 


new nite pivoting rules simplex method 
mathematics operations research 
boutilier dean hanks 

planning uncertainty structural assumptions computational leverage 
submitted second european workshop planning 
boutilier dearden goldszmidt 

exploiting structure policy construction 
proceedings international joint conference onarti cial intelligence 
coppersmith winograd 

matrix multiplication arithmetic progressions 
proceedings th annual acm symposium theory computing pages 
dagum karp luby ross 

optimal stopping rule monte carlo estimation 
submitted focs 
dagum luby 

approximating probabilistic inference bayesian belief networks nphard 
arti cial intelligence 
dantzig 

linear programming extensions 
princeton university press 
dantzig wolfe 

decomposition principle dynamic programs 
operations research 
dean kaelbling kirman nicholson 

planning deadlines stochastic domains 
proceedings aaai pages 
aaai 
dean kaelbling kirman nicholson 

planning time constraints stochastic domains 
appear arti cial intelligence 
dean kanazawa koller russell 

decision theoretic planning part submitted journal arti cial intelligence research 
dean lin 

decomposition techniques planning stochastic domains 
proceedings international joint conference onarti cial intelligence 


dynamic programming models applications 
prentice hall 

probabilistic production inventory problem 
management science 


finite state markovian decision processes 
cambridge university press new york 
howard 

dynamic programming markov processes 
mit press cambridge massachusetts 
jerrum sinclair 

conductance rapid mixing property markov chains approximation permanent resolved preliminary version 
acm symposium theoretical computing pages 
kaelbling 

hierarchical learning stochastic domains preliminary report 
proceedings tenth international conference machine learning page 
kalai 

subexponential randomized simplex algorithm 
proceedings th annual acm symposium theory computing pages 
karmarkar 

new polynomial time algorithm linear programming 
combinatorica 


polynomial algorithm linear programming 
soviet math dokl 
kirman 

predicting real time planner performance domain characterization 
phd thesis department computer science brown university 
klee 

simplex algorithm 
editor inequalities iii pages 
academic press new york 
kushner chen 

decomposition systems governed markov chains 
ieee transactions automatic control ac 
leighton plotkin stein tardos 

fast approximation algorithms multicommodity ow problems 
proceedings rd annual acm symposium theory computing pages 
condon 

complexity policy iteration algorithm stochastic games 
technical report cs tr computer sciences department university wisconsin madison 
appear orsa journal computing 
moore atkeson 

memorybased reinforcement learning cient computation prioritized sweeping 
advances neural information processing systems pages san mateo california 
morgan kaufmann 
moore atkeson 

algorithm variable resolution reinforcement learning multidimensional state spaces 
appear machine learning 
papadimitriou tsitsiklis 

complexity markov chain decision processes 
mathematics operations research 
peng williams 

cient learning planning dyna framework 
adaptive behavior 
plotkin shmoys tardos 

fast approximation algorithms fractional packing covering problems 
nd annual symposium foundations computer science pages 
puterman 

markov decision processes 
john wiley sons new york 
puterman shin 

modi ed policy iteration algorithms discounted markov decision processes 
management science 
schrijver 

theory linear integer programming 
wiley interscience 
schweitzer 

aggregation methods large markov chains 
editors mathematical computer performance reliability pages 
elsevier amsterdam holland 
tseng 

solving horizon stationary markov decision problems time proportional log 
operations research letters 
williams baird 

tight performance bounds greedy policies imperfect value functions 
technical report nu ccs northeastern university college computer science boston ma 
