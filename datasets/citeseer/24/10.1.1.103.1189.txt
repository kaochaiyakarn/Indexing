bernhard boser eecs department university california berkeley ca boser eecs berkeley edu training algorithm optimal margin classi ers training algorithm maximizes margin training patterns decision boundary 
technique applicable wide variety classi action functions including perceptrons polynomials radial basis functions 
effective number parameters adjusted automatically match complexity problem 
solution expressed linear combination supporting patterns 
subset training patterns closest decision boundary 
bounds generalization performance leave method vc dimension 
experimental results optical character recognition problems demonstrate generalization obtained compared learning algorithms 
generalization performance pattern classi ers achieved capacity classi cation function matched size training set 
classi ers large number adjustable parameters large capacity learn training set error exhibit poor generalization 
conversely classi er insu cient capacity able learn task 
optimal capacity classi er minimizes expected generalization error amount training data 
experimental evidence theoretical studies part performed boser bell laboratories 
university california berkeley 
isabelle guyon bell laboratories street th floor san francisco ca isabelle neural att com vladimir vapnik bell laboratories crawford corner road holmdel nj neural att com moo vap bh tls mac link generalization classi er error training examples complexity classi er 
methods structural risk minimization vap vary complexity classi cation function order optimize generalization 
describe training algorithm automatically tunes capacity classi cation function maximizing margin training examples class boundary km optionally removing atypical meaningless examples training data 
resulting classi cation function depends called supporting patterns vap 
training examples closest decision boundary usually small subset training data 
demonstrated maximizing margin amounts minimizing maximum loss opposed average quantity mean squared error 
desirable consequences 
resulting classi cation rule achieves separation training data possible 
outliers meaningless patterns identi ed algorithm eliminated easily supervision 
contrasts classi ers minimizing mean squared error quietly ignore atypical patterns 
advantage maximum margin classi ers sensitivity classi er limited computational accuracy minimal compared separations smaller margin 
analogy vap bound generalization performance obtained leave method 
maximum margin classi er ratio number linearly independent supporting patterns number training examples 
bound tighter bound capacity classi er family 
proposed algorithm operates large class decision functions linear parameters restricted linear dependences input components 
perceptrons ros polynomial neural networks hidden layer radial basis function rbf potential function classi ers abr bl md fall class 
pointed authors abr dh pg percep dual kernel representation implementing decision function 
optimal margin algorithm exploits duality improved ciency exibility 
dual space decision function expressed linear combination basis functions parametrized supporting patterns 
supporting patterns correspond class centers rbf classi ers chosen automatically maximum margin training procedure 
case polynomial classi ers perceptron representation involves number parameters 
problem overcome dual space representation classi cation rule weighted sum kernel function supporting pattern 
high order polynomial classi ers large training sets handled ciently proposed algorithm 
training algorithm described section 
section summarizes important properties optimal margin classi ers 
experimental results reported section 
maximum margin training algorithm maximum margin training algorithm nds decision function pattern vectors dimension belonging classes input training algorithm set examples xi labels yi xp yp yk xk class yk xk class training examples algorithm nds parameters decision function learning phase 
training classi cation unknown patterns predicted rule 
decision functions linear parameters restricted linear dependences functions expressed direct dual space 
direct space notation identical perceptron decision function ros nx wi equation prede ned functions wi adjustable parameters decision function 
polynomial classi ers special case perceptrons products components dual space decision functions form px kk xk coe cients parameters adjusted xk training patterns 
function prede ned kernel example potential function abr radial basis function bl md 
certain conditions ch symmetric kernels possess nite nite series expansions form particular kernel corresponds polynomial expansion 
provided expansion stated equation exists equations dual representations decision function wi px xk parameters wi called direct parameters referred dual parameters 
proposed training algorithm generalized portrait method described vap constructs separating hyperplanes maximum margin 
algorithm extended train classi ers linear parameters 
margin class boundary training patterns formulated direct space 
problem description transformed dual space means lagrangian 
resulting problem maximizing quadratic form constraints amenable cient numeric optimization algorithms lue 
maximizing margin direct space direct space decision function dimensional vectors bias 
de nes separating hyperplane space 
distance hyperplane pattern kwk 
assuming separation training set margin class boundary training patterns exists training patterns ful ll inequality xk kwk objective training algorithm nd parameter vector maximizes max kwk subject xk bound attained patterns satisfying xk maximum margin linear decision function 
gray levels encode absolute value decision function solid black corresponds 
numbers indicate supporting patterns 
patterns called supporting patterns decision boundary 
decision function maximum margin illustrated gure 
problem nding hyperplane space maximum margin minimax problem max kwk xk norm parameter vector equations xed pick nite number possible solutions di er scaling 
xing norm take care scaling problem product margin norm weight vector xed 
maximizing margin equivalent norm kwk 
problem nding maximum margin separating hyperplane stated reduces solving quadratic problem conditions xk maximum margin kw principle problem stated solved directly numerical techniques 
approach impractical dimensionality ofthe space large nite 
information gained supporting patterns 
training data linearly separable maximum margin may negative 
case kwk imposed 
maximizing margin equivalent maximizing kwk 
maximizing margin dual space problem transformed dual space means lagrangian lue kwk px subject xk factors called lagrange multipliers kuhn tucker coe cients satisfy conditions xk factor half included cosmetic reasons change solution 
optimization problem equivalent searching saddle point function 
saddle point minimum respect maximum respect 
solution necessary condition met px px kyk kyk patterns satisfy xk supporting patterns 
equation vector speci es hyperplane maximum margin linear combination supporting patterns patterns 
usually number supporting patterns smaller number patterns training set 
dependence lagrangian onthe weight vector removed substituting expansion equation transformations result lagrangian function parameters bias px subject square matrix size elements xk xl order unique solution exist positive de nite 
xed bias solution obtained maximizing conditions 
equations resulting decision function form yk kk xk supporting patterns appear sum nonzero weight 
choice bias gives rise variants algorithm 
considered 
bias xed priori subjected training 
corresponds generalized portrait technique described vap 

cost function optimized respect largest possible margin space vc 
cases solution standard nonlinear optimization algorithms quadratic forms linear constraints lue loo 
second approach gives largest possible margin 
guarantee solution exhibits best generalization performance 
strategy optimize margin respect described vap 
solves problem di erences pattern vectors obtain independent bias computed subsequently 
margin space maximized decision boundary halfway classes 
bias obtained applying arbitrary supporting patterns xa class xb class account xa andd xb 
xa xb px yk xa xk xb xk dimension problem equals size training set toavoid need solve dual problem exceedingly large dimensionality training data divided chunks processed iteratively vap 
maximum margin hypersurface constructed rst chunk new training set formed consisting supporting patterns solution patterns xk second chunk training set xk new classi er trained construct training set consisting supporting patterns examples rst chunks satisfy xk process repeated entire training set separated 
properties algorithm section highlight important aspects optimal margin training algorithm 
description split discussion qualities resulting classi er computational considerations 
classi cation performance advantages techniques illustrated section experimental results 
properties solution maximizing margin decision boundary training patterns equivalent maximizing quadratic form positive quadrant local minima solution unique full rank 
optimum kw px uniqueness solution consequence maximum margin cost function represents important advantage algorithms solution depends initial conditions parameters di cult control 
bene maximum margin objective insensitivity small changes parameters decision function linear function direct dual space probability misclassi cations due parameter variations components vectors minimized maximum margin 
robustness solution potentially generalization performance increased omitting supporting patterns solution 
equation indicates largest increase maximum margin occurs supporting patterns largest eliminated 
elimination performed automatically assistance supervisor 
feature gives rise important uses optimum margin algorithm database cleaning applications 
compares decision boundary maximum margin mean squared error mse cost functions 
mse decision function simply ignores outlier optimal margin classi ers sensitive patterns close linear decision boundary mse left maximum margin cost functions middle right presence outlier 
rightmost picture outlier removed 
numbers re ect ranking supporting patterns magnitude lagrange coe cient class individually 
decision boundary 
examples readily identi ed largest eliminated automatically supervision 
optimal margin classi ers give complete control handling outliers opposed quietly ignoring 
optimum margin algorithm performs automatic capacity tuning decision function achieve generalization 
estimate upper bound generalization error obtained leave method pattern xk removed training set 
classi er trained remaining patterns tested xk 
process repeated training patterns 
generalization error estimated ratio misclassi ed patterns maximum margin classi er cases arise xk supporting pattern decision boundary unchanged xk classi ed correctly 
supporting pattern cases possible 
pattern xk linearly dependent supporting patterns 
case classi ed correctly 

xk linearly independent supporting patterns 
case outcome uncertain 
worst case linearly independent supporting patterns misclassi ed omitted training data 
frequency errors obtained method direct relationship number adjustable parameters 
number linearly independent supporting patterns bounded min 
suggests number supporting patterns related ective capacity classi er usually smaller vc dimension vap 
polynomial classi ers example dimension space order polynomial 
practice thenumber supporting patterns smaller dimension space 
capacity tuning realized maximum margin algorithm essential get generalization high order polynomial classi ers 
computational considerations speed convergence important practical considerations classi cation algorithms 
bene dual space representation reduce number computations required example polynomial classi ers pointed 
dual space evaluation decision function requires evaluations kernel function xk forming weighted sum results 
number reduced appropriate search techniques yield negligible contributions 
typically training time separating surface database examples minutes workstation cient optimization algorithm 
experiments reported section database training examples took minutes cpu time separating surface 
optimization performed algorithm due powell described lue available public numerical libraries 
quadratic optimization problems form stated solved polynomial time ellipsoid method ny 
technique nds rst hyperspace guaranteed contain optimum volume space reduced iteratively constant fraction 
algorithm polynomial number free parameters encoding size accuracy problem solution 
practice algorithms guaranteed polynomial convergence cient 
alpha alpha alpha alpha alpha alpha alpha alpha alpha alpha alpha alpha alpha alpha alpha alpha supporting patterns database db class cleaning 
patterns ranked experimental results maximum margin training algorithm tested databases images handwritten digits 
rst database db consists clean images recorded subjects 
half data training half evaluate generalization performance 
comparative analysis performance various classi cation methods db gpp 
database db experiment consists images training testing recorded actual mail pieces 
results data reported publications see cbd 
resolution images databases pixels 
experiments margin maximized respect hypersurfaces class separate digits 
regardless di culty problem measured example supporting patterns algorithm similarity function preprocessing hypersurfaces experiment 
results obtained di erent choices corresponding linear hyperplanes polynomial classi ers basis functions summarized 
ect smoothing investigated simple form preprocessing 
linear hyperplane classi ers corresponding similarity function algorithm nds separation database db 
percentage errors test set 
result compares favorably hyperplane classi ers minimize mean squared error backpropagation pseudoinverse error test set 
database db linearly separable contains meaningless patterns 
shows supporting patterns large lagrange multipliers hyperplane class 
percentage test set db drops cleaning removing meaningless ambiguous patterns 
better performance achieved databases multilayer neural networks classi cation functions higher capacity linear subdividing planes 
tests polynomial classi ers order give error rates average number supporting patterns hypersurface 
average computed total number supporting patterns divided thenumber decision functions 
patterns support hypersurface counted total 
comparison dimension space listed 
db db error error linear results obtained db show strong decrease number supporting patterns linear third order polynomial classi cation function equivalently signi cant decrease error rate 
increase order polynomial little effect number supporting patterns performance dimension space increases exponentially 
error rate obtained forth order polynomial slightly better reported neural network sophisticated architecture cbd trained tested data 
experiment performance changes drastically rst second order polynomials 
may consequence fact maximum th order polynomial classi er equal dimension patterns th power larger gradual change vc dimension possible function chosen power series example exp equation parameter vary gradually 
small values equation approaches linear classi er vc dimension decision boundaries maximum margin classi ers second order polynomial decision rule left exponential rbf exp kx middle 
rightmost picture shows decision boundary layer neural network hidden units trained backpropagation 
equal dimension patterns plus 
experiments database db lead slightly better performance obtained second order polynomial classi er db hyperbolic tangent resulting classi er interpreted neural network hidden layer hidden units 
supporting patterns weights rst layer coe cients weights second linear layer 
number hidden units chosen training algorithm maximize margin classes substituting hyperbolic tangent exponential function lead better results experiments 
importance suitable preprocessing incorporate knowledge task hand pointed researchers 
optical character recognition introduce invariance scaling rotation distortions particularly important sld 
smoothing achieve insensitivity small distortions 
table lists error test set di erent amounts smoothing 
second order polynomial classi er database db forth order polynomial db 
smoothing kernel gaussian standard deviation db db error error smoothing performance improved considerably db 
db improvement signi cant optimum obtained smoothing db 
expected number training patterns db larger db versus 
higher performance gain expected selective hints smoothing small rotations scaling digits sld 
better performance similarity functions 
shows decision boundary obtained second order polynomial radial basis function rbf maximum margin exp kx 
decision boundary polynomial classi er closer classes 
consequence nonlinear transform space space polynomials realizes position dependent scaling distance 
radial basis functions exhibit problem 
decision boundary layer neural network trained backpropagation shown comparison 
maximizing margin class boundary training patterns alternative training methods optimizing cost functions mean squared error 
principle equivalent minimizing maximum loss number important features 
include automatic capacity tuning classi cation function extraction small number supporting patterns training data relevant classi cation uniqueness solution 
exploited cient learning algorithm classi ers linear parameters large capacity high order polynomial rbf classi ers 
key representation decision function dual space lower dimensionality feature space 
ciency performance algorithm demonstrated handwritten digit recognition problems 
achieved performance matches sophisticated classi ers task speci knowledge 
training algorithm polynomial number training patterns cases dimension solution space space exponential nite 
training time experiments hour workstation 
wish colleagues uc berkeley bell laboratories suggestions stimulating discussions 
comments bottou cortes sanders solla zakhor reviewers gratefully acknowledged 
especially indebted hochbaum investigating polynomial convergence property hein providing code constrained nonlinear optimization haussler warmuth help advice regarding performance bounds 
abr aizerman braverman 
theoretical foundations potential function method pattern recognition learning 
automation remote control 
bh baum haussler 
size net gives valid generalization 
neural computation 
bl broomhead lowe 
multivariate functional interpolation adaptive networks 
complex systems 
cbd yann le cun bernhard boser john denker henderson richard howard wayne hubbard larry jackel 
handwritten digit recognition propagation network 
david touretzky editor neural information processing systems volume pages 
morgan kaufmann publishers san mateo ca 
ch courant hilbert 
methods mathematical physics 
interscience new york 
dh duda hart 
pattern classi cation scene analysis 
wiley son 
geman bienenstock doursat 
neural networks bias variance dilemma 
neural computation 
gpp guyon personnaz dreyfus denker lecun 
comparing di erent neural network architectures classifying handwritten digits 
proc 
int 
joint conf 
neural networks int 
joint conference neural networks 
isabelle guyon vladimir vapnik bernhard boser leon bottou sara solla 
structural risk minimization character recognition 
david touretzky editor neural information processing systems volume 
morgan kaufmann publishers san mateo ca 
appear 
david haussler nick littlestone manfred warmuth 
predicting functions randomly drawn points 
proceedings th annual symposium foundations computer science pages 
ieee 
km 
learning algorithms optimal stability neural networks 
phys 
math 
gen 
loo editor 
numerical methods non linear optimization 
academic press london 
lue david luenberger 
linear nonlinear programming 
addison wesley 
mac mackay 
practical bayesian framework backprop networks 
david touretzky editor neural information processing systems volume 
morgan kaufmann publishers san mateo ca 
appear 
md moody darken 
fast learning networks locally tuned processing units 
neural computation 
matic guyon bottou denker vapnik 
computer aided cleaning large databases character recognition 
digest icpr 
icpr amsterdam august 
moo moody 
generalization weight decay architecture selection nonlinear learning systems 
david touretzky editor neural information processing systems volume 
morgan kaufmann publishers san mateo ca 
appear 
ny nemirovsky 
problem complexity method ciency optimization 
wiley new york 
omohundro 
cient function constraint classi cation learning 
lippmann editors nips san mateo ca 
ieee morgan kaufmann 
pg poggio girosi 
regularization algorithms learning equivalent multilayer networks 
science february 
poggio 
optimal nonlinear associative recall 
biol 
cybernetics vol 

ros rosenblatt 
principles neurodynamics 
spartan books new york 
sld simard lecun denker 
tangent prop formalism specifying selected invariances adaptive network 
david touretzky editor neural information processing systems volume 
morgan kaufmann publishers san mateo ca 
appear 
tls tishby levin solla 
consistent inference probabilities layered networks predictions generalization 
proceedings international joint conference neural networks washington dc 
vap vladimir vapnik 
estimation dependences empirical data 
springer verlag new york 
vc vapnik ya 
chervonenkis 
theory pattern recognition 
nauka moscow 
