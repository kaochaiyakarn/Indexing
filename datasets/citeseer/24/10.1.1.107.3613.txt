survey independent component analysis hyv rinen helsinki university technology finland common problem encountered disciplines statistics data analysis signal processing neural network research nding suitable representation multivariate data 
computational conceptual simplicity representation sought linear transformation original data 
known linear transformation methods include example principal component analysis factor analysis projection pursuit 
developed linear transformation method independent component analysis ica desired representation minimizes statistical dependence components representation 
representation capture essential structure data applications 
survey existing theory methods ica 
central problem neural network research statistics signal processing nding suitable representation data means suitable transformation 
important subsequent analysis data pattern recognition data compression de noising visualization data represented manner facilitates analysis 
trivial example consider speech recognition 
task clearly simpler speech represented audible sound sequence numbers 
shall concentrate problem representing continuous valued multidimensional variables 
denote dimensional random variable problem nd function dimensional transform sn de ned desirable properties 
note shall notation random variables realizations context distinction clear 
cases representation sought linear transform observed variables wx matrix determined 
linear transformations problem computationally conceptually simpler facilitates interpretation results 
treat linear transformations 
methods described extended non linear case 
extensions outside scope 
principles methods developed nd suitable linear transformation 
include principal component analysis factor analysis projection pursuit independent component analysis 
usually methods de ne principle tells transform optimal 
updates corrections comments sent hyv rinen hyvarinen hut 
neural computing surveys www icsi berkeley edu jagota ncs neural computing surveys www icsi berkeley edu jagota ncs optimality may de ned sense optimal dimension reduction statistical interestingness resulting components si simplicity transformation criteria including application oriented ones 
particular method nding linear transformation called independent component analysis ica gained wide spread attention 
name implies basic goal nd transformation components si statistically independent possible 
ica applied example blind source separation observed values correspond realization discrete time signal 
components si called source signals usually original uncorrupted signals noise sources 
sources statistically independent signals recovered linear mixtures xi nding transformation transformed signals independent possible ica 
promising application feature extraction si coe cient th feature observed data vector ica feature extraction motivated results neurosciences suggest similar principle redundancy reduction explains aspects early processing sensory data brain 
ica applications exploratory data analysis way closely related method projection pursuit 
review theory methods ica 
discuss relevant classical representation methods section 
section de ne ica show connections classical methods applications 
section di erent contrast objective functions ica reviewed 
corresponding algorithms section 
noisy version ica treated section 
section concludes 
reviews ica see :10.1.1.41.7785:10.1.1.10.7237
classical linear transformations principles developed statistics neural computing signal processing nd suitable linear representation random variable 
section discuss classical methods determining linear transformation 
methods discussed centered variables 
words mean random vector subtracted 
simplify discussion henceforth assumed variable centered means transformed efx original non centered variable 
second order methods popular methods nding linear transform eq 
second order methods 
means methods nd representation information contained covariance matrix data vector course mean initial centering 
second order techniques understood context classical assumption gaussianity 
variable normal gaussian distribution distribution completely determined second order information 
useless include information 
reason popularity second order methods computationally simple requiring classical matrix manipulations 
classical second order methods principal component analysis factor analysis see 
roughly characterize second order methods saying purpose nd faithful representation data sense reconstruction mean square error 
contrast higher order methods see section try nd meaningful representation 
course meaningfulness task dependent property higher order methods able nd meaningful representations wide variety applications 
neural computing surveys www icsi berkeley edu jagota ncs principal component analysis dimensional data cloud 
line shown direction rst principal component gives optimal mean square sense linear reduction dimension dimensions 
principal component analysis principal component analysis pca see widely signal processing statistics neural computing 
application areas called discrete karhunen lo transform hotelling transform 
basic idea pca nd components sn explain maximum amount variance possible linearly transformed components 
pca de ned intuitive way recursive formulation 
de ne direction rst principal component say arg max kwk ef wt dimension random data vector 
vectors column vectors 
rst principal component projection direction variance projection maximized 
having determined rst principal components th principal component determined principal component residual wk arg max kwk ef wt principal components si practice computation wi simply accomplished sample covariance matrix wi eigenvectors correspond largest eigenvalues basic goal pca reduce dimension data 
usually chooses proven representation pca optimal linear dimension reduction technique mean square sense 
reduction dimension important bene ts 
computational overhead subsequent processing stages reduced 
second noise may reduced data contained rst components may due noise 
third projection subspace low dimension example useful visualizing data 
note necessary principal components orthonormal basis subspace spanned principal components called pca subspace data compression denoising capabilities 
simple illustration pca fig 
rst principal dimensional data set shown 
neural computing surveys www icsi berkeley edu jagota ncs factor analysis method closely related pca factor analysis 
factor analysis generative model data postulated vector observed variables vector latent variables factors observed constant matrix vector noise dimension asx 
variables assumed gaussian 
addition usually assumed dimension factor analysis basically method reducing dimension data way similar pca 
main methods estimating factor analytic model 
rst method method principal factors 
name implies basically modi cation pca 
idea apply pca data away ect noise taken account 
simplest form assumes covariance matrix noise known 
nds factors performing pca modi ed covariance matrix covariance matrix vector simply vector principal components noise removed 
second popular method maximum likelihood estimation reduced nding principal components modi ed covariance matrix 
general case noise covariance matrix known di erent methods estimating described 
important di erence factor analysis pca di erence little formal de nitions methods 
equation de ne factors uniquely identi able rotation 
indeterminacy compared possibility arbitrary basis pca subspace subspace spanned rst principal components 
factor analysis conventional search rotation factors gives basis interesting properties 
classical criterion parsimony representation roughly means matrix signi cantly non zero entries 
principle rise techniques rotations 
rotation bene facilitating interpretation results relations factors observed variables simpler 
higher order methods higher order methods information distribution contained covariance matrix 
order meaningful distribution assumed gaussian information zero mean gaussian variables contained covariance matrix 
general families density functions representation problem degrees freedom 
sophisticated techniques may constructed non gaussian random variables 
transform de ned second order methods pca useful purposes optimal reduction dimension mean square sense needed 
pca neglects aspects non gaussian data clustering independence components non gaussian data illustrated fig 

shall review conventional methods higher order statistics projection pursuit redundancy reduction blind deconvolution 
projection pursuit projection pursuit technique developed statistics nding interesting projections multidimensional data 
projections optimal visualization clustering structure data purposes density estimation regression 
reduction dimension important objective especially aim visualization data 
basic projection pursuit try nd directions projection data direction interesting distribution displays structure 
argued huber jones sibson gaussian distribution interesting interesting directions show gaussian distribution 
neural computing surveys www icsi berkeley edu jagota ncs classical illustration problems variance methods pca 
data gure clearly divided clusters 
principal component direction maximum variance vertical 
projections principal component produce separation clusters 
contrast projection pursuit direction horizontal providing optimal separation clusters 
usefulness nding projections seen fig 
projection projection pursuit direction horizontal clearly shows clustered structure data 
projection rst principal component vertical hand fails show structure 
projection pursuit wants reduce dimension away interesting features data preserved 
contrast pca objective reduce dimension representation faithful possible mean square sense 
central theoretical problem projection pursuit de nition projection pursuit index de nes interestingness direction 
usually index measure non gaussianity 
natural choice di erential entropy 
di erential entropy random vector density isf de ned log dy consider zero mean variables di erent densities constrain covariance xed 
di erential entropy maximized respect gaussian density 
distribution entropy strictly smaller 
try nd projection pursuit directions minimizing respect constraining variance constant 
problem di erential entropy estimation entropy de nition requires estimation density ofw di cult theory practice 
measures non normality proposed 
weighted distances density multivariate gaussian density 
possibility cumulant approximations di erential entropy 
furthermore approximations negentropy maximum entropy principle introduced 
details section 
redundancy reduction barlow authors important characteristic sensory processing brain redundancy reduction 
aspect redundancy reduction input data represented components features independent possible 
representation useful processing stages 
theoretically values components activities neurons represented sum weight vectors neurons weighted activations 
leads linear encoding methods section 
neural computing surveys www icsi berkeley edu jagota ncs illustration blind deconvolution 
original signal depicted left 
right convolved version signal shown 
problem recover signal left observing signal right 
method performing redundancy reduction sparse coding 
idea represent data set neurons small number neurons activated time 
equivalently means neuron activated rarely 
data certain statistical properties sparse kind coding leads approximate redundancy reduction 
second method redundancy reduction predictability minimization 
observation random variables independent provide information predict variable 
blind deconvolution blind deconvolution di erent techniques discussed section sense simplest case dealing dimensional time signals time series multidimensional data blind deconvolution extended multidimensional case 
blind deconvolution important research topic vast literature 
shall describe special case problem closely connected 
blind deconvolution convolved version scalar signal observed knowing signal convolution kernel 
problem nd separating lter 
illustration fig 

equalizer assumed fir lter su cient length truncation ects ignored 
special case blind deconvolution especially interesting context case assumed values signal di erent points time statistically independent 
certain assumptions problem solved simply whitening signal 
solve problem full generality assume signal non gaussian higher order information 
techniques solving special case problem similar techniques higher order methods discussed section 
term blind equalization sense 
neural computing surveys www icsi berkeley edu jagota ncs statistical independence independent component analysis shall recall basic de nitions needed 
denote ym random variables joint density ym 
simplicity assume variables zero mean 
variables yi mutually independent density function factorized ym fm ym fi yi denotes marginal density 
distinguish form independence concepts independence example linear independence property called statistical independence 
independence distinguished means independence general stronger requirement 
yi independent efg yi yj efg yi yj functions 
clearly stricter condition condition 
important special case independence equivalent 
case ym joint gaussian distribution see 
due property independent component analysis interesting possible gaussian variables seen 
de nitions linear independent component analysis shall de ne problem independent components analysis ica 
shall consider linear case non linear forms ica exist 
literature di erent basic de nitions linear ica di erences de nitions usually emphasized 
probably due fact ica new research topic research concentrated simplest de nitions 
de nitions observed dimensional random vector denoted xm rst general de nition follows de nition general de nition ica random vector consists nding linear transform wx components si independent possible sense maximizing function sm measures independence 
de nition general sense assumptions data isin contrast de nitions 
course de nition quite vague de ne measure independence si 
de nition independence eq 
possible general nd linear transformation gives strictly independent components 
problem de ning measure independence treated section 
di erent approach taken estimation theoretically oriented de nition de nition noisy ica model ica random vector consists estimating generative model data latent variables components si vector sn assumed independent 
matrix constant mixing matrix dimensional random noise vector 
functions assumed measurable 
shall omit questions measurability 
neural computing surveys www icsi berkeley edu jagota ncs de nition reduces ica problem ordinary estimation latent variable model 
estimation problem simple great majority ica research concentrated simpli ed de nition de nition noise free ica model ica random vector consists estimating generative model data de nition 
noise vector omitted 
model introduced jutten seminal earlier jutten phd probably earliest explicit formulation ica see 
shall concentrate noise free ica model de nition 
choice partially justi ed fact research ica concentrated simple de nition 
estimation noise free model proved task di cult 
noise free model may considered tractable approximation realistic noisy model 
justi cation approximation methods simpler model certain kinds real data seen 
estimation noisy ica model treated section 
shown data follow generative model eq 
de nitions asymptotically equivalent certain measures independence de nition natural relation identi ability ica model identi ability noise free ica model treated 
imposing fundamental restrictions addition basic assumption statistical independence identi ability model assured 

independent components si possible exception component non gaussian 

number observed linear mixtures large number independent components 
matrix full column rank 
usually assumed centered practice restriction accomplished subtracting mean random vector interpreted stochastic processes simply random variables additional restrictions necessary 
minimum assume stochastic processes stationary strict sense 
restrictions ergodicity respect quantities estimated necessary 
assumptions ful lled example process time 
assumptions consider stochastic process random variable 
basic insigni cant indeterminacy model independent components columns estimated multiplicative constant constant multiplying independent component eq 
canceled dividing corresponding column mixing matrix constant 
mathematical convenience usually de nes independent components si unit variance 
independent components unique multiplicative sign may di erent component 
de nitions ica imply ordering independent components pca 
possible introduce order independent components 
way norms columns mixing matrix give contributions independent components variances xi 
ordering si descending norm corresponding neural computing surveys www icsi berkeley edu jagota ncs columns example gives ordering reminiscent pca 
second way non gaussianity independent components 
non gaussianity may measured example projection pursuit indexes section contrast functions introduced section 
ordering si non gaussianity gives ordering related projection pursuit 
rst restriction non gaussianity list necessary identi ability ica model 
gaussian random variables mere implies independence decorrelating representation give independent components 
components si gaussian possible identify non gaussian independent components corresponding columns mixing matrix 
hand second restriction completely necessary 
case mixing matrix identi able rigorous proofs exist knowledge realizations independent components identi able existing theory ica valid case second assumption 
case called ica overcomplete bases :10.1.1.134.6077:10.1.1.164.7690
rank restriction mixing matrix third restriction necessary form probably weakest possible 
regards identi ability noisy ica model restrictions guarantee partial identi ability noise assumed independent components si :10.1.1.30.4840
fact noisy ica model special case noise free ica model noise variables considered additional independent components 
particular mixing matrix identi able 
contrast realizations independent components si longer identi ed completely separated noise 
noise covariance matrix identi able :10.1.1.30.4840
shall assume assumptions announced valid shall treat noiseless ica model comments estimation noisy model 
conventional assumption dimension observed data equals number independent components simpli cation justi ed fact dimension observed vector reduced reduction dimension achieved existing methods pca 
relations classical methods ica closely related methods described section 
de nition ica considered method achieving redundancy reduction 
experimental evidence certain kinds sensory data conventional ica algorithms nd directions compatible existing neurophysiological data assumed re ect redundancy reduction 
see section 
noise free case estimation ica model means simply nding certain interesting projections give estimates independent components 
ica considered de nitions special case projection pursuit 
explained section conventional criteria nding interesting directions projections pursuit coincide essentially criteria estimating independent components 

close nity ica blind deconvolution precisely special case blind deconvolution original signal time 
due assumption values original signal independent di erent problem formally closely related problem independent component analysis 
ideas developed blind deconvolution directly applied ica vice versa 
blind deconvolution especially elegant powerful framework developed considered intellectual ancestor ica 
neural computing surveys www icsi berkeley edu jagota ncs ica noise model projection pursuit non gaussian data factor analysis gaussian data relations ica methods 
lines show close connections texts lines show assumptions needed connection 

comparing eq 
de nition de nition factor analysis eq 
connection factor analysis ica clear 
ica may considered non gaussian factor analysis 
main di erence usually ica reduction dimension considered secondary objective need case 
simple combination factor analysis ica obtained factor rotations 
nding factor subspace suitable rotation usually performed 
ica conceived rotation criterion depends higher order statistics factors structure matrix method roughly equivalent method advocated consists rst reducing dimension pca performing ica dimension reduction 

de nition relation principal component analysis evident 
methods formulate general objective function de ne interestingness linear representation maximize function 
second relation pca ica related factor analysis contradictory assumptions gaussianity non gaussianity respectively 
nity pca ica may important nity ica methods discussed 
pca ica de ne objective functions quite di erent ways 
pca uses second order statistics ica impossible second order statistics 
pca emphasizes dimension reduction ica may reduce dimension increase leave 
relation ica nonlinear versions pca criteria de ned quite strong seen section 
connections ica methods illustrated fig 

lines diagram indicate close connections assumptions lines 
assumptions data particular noise postulated data ica considered method exploratory data analysis projection pursuit 
de nition ica means simply nding interesting projections data measures interestingness essentially equivalent methods seen section 
hand assumes noisy data model de nition ica considered variation factor analysis non gaussian data 
possible approaches ica quite di erent stem clearly distinct classical methods 
ica de nition noise free ica data model approaches 
pca connection ica considered indirect perform factor analysis gaussian data 
pca neural computing surveys www icsi berkeley edu jagota ncs illustration blind source separation 
gure shows source signals independent components 
due external circumstances linear mixtures source signals fig 
depicted observed 
applications ica blind source separation classical application ica model blind source separation 
blind source separation observed values correspond realization dimensional discrete time signal 
independent components si called source signals usually original uncorrupted signals noise sources 
classical example blind source separation cocktail party problem 
assume people speaking simultaneously room cocktail party 
problem separate voices di erent speakers recordings microphones room 
principle corresponds ica data model xi recording th microphone si waveforms voices practical application noise reduction 
sources original uncorrupted source noise sources estimation uncorrupted source fact denoising operation 
simple arti cial illustration blind source separation figures 
illustration deterministic signals purposes illustration 
spectral properties signals ica framework results remain unchanged signals simply non gaussian white noise 
results applying ica blind separation electroencephalographic eeg meg data reported 
eeg data consisted recordings brain activity obtained electrodes attached scalp 
dimensional signal vector observed 
meg data obtained sophisticated measuring method giving rise dimensional signal vector 
ica algorithms succeeded separating certain source signals called artifacts noise sources corresponding brain activity 
canceling noise sources central unsolved problem eeg meg signal processing 
ica ers promising method 
similarly ica decomposition evoked eld potentials measured eeg meg application considerable interest neurosciences 
application brain imaging data time obtained functional magnetic resonance imaging fmri reported 
application area economic time series 
reported 
applications telecommunications published 
research ica done application source separation mind authors treating ica problem term ica speak simply blind source separation application taken illustrative example real application 
practice situation complicated described due time delays see section 
neural computing surveys www icsi berkeley edu jagota ncs linear mixtures fig 
source signals fig 
estimated multiplying factors 
gure shows estimates source signals 
bss 
clear distinction ica theoretical problem data model di erent applications blind source separation application solved various theoretical approaches including limited ica 
fact blind source separation problem solved methods di erent ica 
particular methods frequency information spectral properties prominent see :10.1.1.52.5680
methods time correlated signals course usual case blind source separation applications ica see 
frequency methods possible separate gaussian source signals 
feature extraction application ica feature extraction 
columns represent features si coe cient th feature observed data vector ica feature extraction motivated theory redundancy reduction see section 
essentially equivalent method sparse coding applied extraction low level features natural image data 
results show extracted features correspond closely observed primary visual cortex :10.1.1.134.6077
results robust replicated authors methods 
comparison ica features properties simple cells macaque primary visual cortex conducted authors match parameters especially video sequences images 
obtained features closely connected ered wavelet theory gabor analysis 
fact itwas shown derive completely adaptive version wavelet shrinkage estimation noisy ica model 
application features data compression pattern recognition important research topics 
blind deconvolution direct application ica methods blind deconvolution see section 
due fact values original signal independent di erent problem solved essentially formalism ica noted 
problem represented approximately eq 
realizations vectors containing subsequent observations signals di erent points time 
words sequence observations square matrix determined convolving lter 
formulation approximative exact formulation linear lters lead essentially algorithms convergence proofs 
blind separation convolved signals multi channel deconvolution represented combining approaches see example 
applications due close connection ica projection pursuit hand ica factor analysis possible ica applications projection pursuit factor analysis 
include exploratory data analysis areas economics psychology social sciences density estimation regression 
neural computing surveys www icsi berkeley edu jagota ncs objective contrast functions ica estimation data model independent component analysis usually performed formulating objective function minimizing maximizing 
function called contrast function authors reserve term certain class objective functions 
terms loss function cost function 
shall term contrast function loosely meaning function optimization enables estimation independent components 
shall restrict estimation noise free ica model noisy ica see section assume restrictions section su cient identi ability model imposed 
objective functions vs algorithms draw distinction formulation objective function algorithm optimize 
express equation ica method objective function optimization algorithm case explicitly formulated objective functions classical methods optimization optimizing objective function stochastic gradient methods newton methods cases algorithm estimation principle may di cult separate 
properties ica method depend elements right hand side 
particular statistical properties consistency asymptotic variance robustness ica method depend choice objective function algorithmic properties convergence speed memory requirements numerical stability depend optimization algorithm 
classes properties independent sense di erent optimization methods optimize single objective function single optimization method may optimize di erent objective functions 
cases distinction may clear 
section shall treat choice objective function 
optimization objective function treated section 
section shall compare objective functions mainly terms statistical properties 
multi unit contrast functions treat problem estimating independent components data model time 
likelihood network entropy possible formulate likelihood noise free ica model done estimate model maximum likelihood method 
denoting wm matrix log likelihood takes form tx mx log fi ln det wj fi density functions si assumed known realizations neural computing surveys www icsi berkeley edu jagota ncs related contrast function derived neural network viewpoint 
maximizing output entropy information ow neural network non linear outputs 
assume input neural network outputs form gi gi non linear scalar functions wi weight vectors neurons 
wants maximize entropy outputs gm mx gi chosen framework enables estimation ica model 
authors proved surprising result principle network entropy maximization infomax equivalent maximum likelihood estimation 
equivalence requires non linearities gi neural network chosen cumulative distribution functions corresponding densities fi fi 
advantage maximum likelihood approach regularity conditions asymptotically cient known result estimation theory 
drawbacks 
approach requires knowledge probability densities independent components 
estimated complicates method considerably :10.1.1.31.7467
second drawback maximum likelihood solution may sensitive outliers pdf independent components certain shapes see robustness outliers important property mutual information kullback leibler divergence theoretically satisfying contrast function multi unit case view mutual information 
concept di erential entropy de ned eq 
de nes mutual information scalar random variables yi follows ym yi denotes di erential entropy 
mutual information natural measure dependence random variables 
non negative zero variables statistically independent 
mutual information takes account dependence structure variables 
finding transform minimizes mutual information components si natural way estimating ica model 
approach gives time method performing ica general de nition section 
note properties mutual information invertible linear transformation wx ym yi log det wj mutual information motivated kullback leibler divergence de ned probability densities log dy kullback leibler divergence considered kind distance probability densities real distance measure symmetric 
yi independent joint probability density factorized de nition independence note possible theory consider robustness estimators case ica model noise 
distribution may contaminated contain outliers generated exactly noise free model 
realistic analysis robustness require noise model 
neural computing surveys www icsi berkeley edu jagota ncs eq 

measure independence yi kullback leibler divergence real density factorized density fm ym fi marginal densities yi 
fact quantity equals mutual information yi 
connection kullback leibler divergence shows close connection minimizing mutual information maximizing likelihood 
fact likelihood represented kullback leibler distance observed density factorized density assumed model 
methods minimizing kullback leibler distance observed density factorized density factorized densities asymptotically equivalent density accurately estimated part ml estimation method 
problem mutual information di cult estimate 
mentioned section de nition entropy needs estimate density 
problem severely restricted mutual information ica estimation 
authors approximations mutual information polynomial density expansions lead higher order cumulants de nition cumulants see appendix 
polynomial density expansions related taylor expansion 
give approximation probability density scalar random variable higher order cumulants 
example rst terms edgeworth expansion give scalar random variable zero mean unit variance density function standardized gaussian random variable cumulants random variable see appendix hi certain polynomial functions hermite polynomials 
expansions obtains example approximation mutual information mx yi yi yi yi yi constant yi constrained uncorrelated 
similar approximation derived earlier context projection pursuit 
cumulant approximations simplify mutual information considerably 
approximation valid far gaussian density function may produce poor results case 
sophisticated approximations mutual information constructed approximations di erential entropy introduced maximum entropy principle 
approximations cumulants replaced general measures see section section 
minimization approximation mutual information introduced 
non linear cross correlations seminal jutten jutten phd authors principle canceling non linear cross correlations obtain independent components 
non linear cross correlations form efg yi yj suitably chosen odd non linearities 
yi yj independent cross correlations zero assumption yi yj symmetric densities 
objective function formulated implicitly exact objective function may exist 
non linearities chosen pdf independent components guidelines 
non linear pca criteria related approach ered non linear pca criteria 
objective functions idea introducing non linearity known objective functions pca 
take example recursive de nition pca eq 

introducing non linearity formula obtains arg max kwk efg wt neural computing surveys www icsi berkeley edu jagota ncs modi ed principal component data non linearity suitably chosen function pdf independent components data preprocessed sphering see section optimizing non linear pca criteria enables estimation ica model 
de nitions pca may modi ed introducing non linearity 
connection non linear pca criteria bottom approach nonlinear cross correlations 
explicit contrast functions may formulated algorithms obtained directly existing pca algorithms introducing suitable non linearity 
higher order cumulant tensors principle ica estimation directly connected objective function framework decomposition higher order cumulant tensors de ned appendix 
solutions fourth order cumulant tensor properties relation estimation ica studied extensively 
related methods introduced 
fourth order cumulant tensor de ned linear operator space matrices ij cum xi xj xk xl subscript ij means th element matrix matrix 
linear operator eigenvalues correspond 
solving eigenvectors ica model estimated 
advantage approach requires knowledge probability densities independent components 
cumulants approximate mutual information shown approximation crude 
main drawback approach statistical properties estimators cumulants see section 
weighted covariance matrix interesting approach developed ica estimation performed ordinary eigenvalue decomposition quadratically weighted covariance matrix xx approach needs assumption independent components di erent distributions particular di erent 
realized approach fact special case cumulant tensor approach section 
unit contrast functions expression unit contrast function designate function optimization enables estimation single independent component 
estimating ica model try nd simply vector say linear combination equals independent components si 
procedure iterated nd independent components 
unit contrast functions motivated unit approach shows direct connection projection pursuit 
unit contrast functions discussed considered measures non gaussianity approach gives unifying framework techniques 
contrast functions algorithms interpreted di erent ways 
applications need estimate independent components 
finding 
ideal case unit contrast functions optimized globally independent components obtained order descending non gaussianity 
light basic principles projection pursuit means interesting independent components obtained rst 
reduces computational complexity method considerably input data high dimension 
neural computing surveys www icsi berkeley edu jagota ncs prior knowledge number independent components needed independent components estimated 
approach shows clearly connection neural networks 
construct neural network units learn neuron optimizes contrast function 
approach tends lead computationally simple solutions 
estimating independent component simple decorrelation nd di erent independent component independent components de nition uncorrelated 
maximizing unit contrast function constraint decorrelation respect independent components new independent component procedure iterated nd independent components 
symmetric parallel decorrelation see 
negentropy natural information theoretic unit contrast function negentropy 
eq 
tempted conclude independent components correspond directions di erential entropy minimized 
turns roughly case 
modi cation di erential entropy invariant scale transformations 
obtain linearly fact nely invariant version entropy de nes negentropy follows gaussian random vector covariance matrix negentropy negative normalized entropy non negative zero gaussian distribution 
usefulness de nition seen mutual information expressed negentropy giving yn yi log ii det covariance matrix ii diagonal elements 
yi uncorrelated third term obtain yn yi negentropy invariant linear transformations obvious nding maximum negentropy directions directions elements sum yi maximized equivalent nding representation mutual information minimized 
negentropy shows clearly connection ica projection pursuit 
di erential entropy projection pursuit index suggested amounts nding directions negentropy maximized 
unfortunately reservations respect mutual information valid 
estimation negentropy di cult contrast function remains mainly theoretical 
multi unit case negentropy approximated higher order cumulants example follows th order cumulant ofy 
random variable assumed zero mean unit variance 
validity approximations may limited 
argued cumulant approximations negentropy inaccurate cases sensitive 
new approximations introduced 
simplest case new approximations form efg efg neural computing surveys www icsi berkeley edu jagota ncs practically non quadratic function irrelevant constant gaussian variable zero mean unit variance standardized 
practical choice see 
approximations shown better cumulant ones respects 
approximations negentropy discussed interesting unit contrast functions right discussed 
higher order cumulants mathematically simplest unit contrast functions provided higher order cumulants kurtosis see appendix de nition 
denote observed data vector assumed follow ica data model 
search linear combination observations xi say kurtosis maximized minimized 
obviously optimization problem meaningful bounded assume ef 
unknown mixing matrix de ne data model obtains ef aa kzk recall efss known properties kurtosis see appendix give kurt kurt kurt mx kurt si constraint kzk function number local minima maxima 
argument clearer assume moment mixture independent component sj kurtosis negative kurtosis positive 
shown extremal points canonical base vectors ej vectors components zero component 
corresponding weight vectors ej rows inverse mixing matrix sign 
minimizing maximizing kurtosis eq 
constraint obtains independent components sj 
optimization modes combined single independent components correspond maxima modulus kurtosis 
kurtosis widely unit ica see example projection pursuit 
mathematical simplicity cumulants especially possibility proving global convergence results contributed largely popularity unit contrast functions ica projection pursuit related elds 
shown example kurtosis provides poor objective function estimation ica statistical properties resulting estimators considered 
note despite fact noise ica model independent components mixing matrix computed accurately independent components si random variables practice nite sample statistical properties estimators realizations analyzed just properties estimator 
analysis conducted see results show terms robustness asymptotic variance cumulant estimators tend far optimal intuitively main reasons 
firstly higher order cumulants measure mainly tails distribution largely una ected structure middle distribution 
secondly estimators higher order cumulants highly sensitive outliers 
value may depend observations tails distribution may outliers 
general contrast functions avoid problems encountered preceding objective functions new unit contrast functions ica developed 
contrast functions try combine positive properties preceding contrast functions statistically appealing properties contrast cumulants require prior knowledge densities independent components contrast basic maximum likelihood estimation allow simple algorithmic implementation contrast maximum likelihood approach consider robustness form optimality particular minimax optimality neighborhood assumed model space statistical models 
neural computing surveys www icsi berkeley edu jagota ncs simultaneous estimation densities simple analyze contrast non linear cross correlation non linear pca approaches 
generalized contrast functions introduced considered generalizations kurtosis ful ll requirements 
note intuitive interpretation contrast functions measures non normality 
family measures non normality constructed practically functions considering di erence expectation actual data expectation gaussian data 
words de ne contrast function measures non normality zero mean random variable non quadratic su ciently smooth function follows jg fg gj standardized gaussian random variable assumed normalized unit variance exponent typically 
subscripts denote expectation respect 
notation jg confused notation negentropy 
clearly jg considered generalization modulus kurtosis 
jg simply modulus kurtosis note quadratic jg trivially zero distributions 
plausible jg contrast function way kurtosis 
fact jg contrast function suitable sense locally shown 
fact jg coincides approximation negentropy 
nite sample statistical properties estimators optimizing general contrast function analyzed 
suitable choice statistical properties estimator asymptotic variance robustness considerably better properties cumulant estimators 
choices proposed log cosh exp suitable constants 
lack precise knowledge distributions independent components outliers functions approximate reasonably optimal contrast function cases 
experimentally itwas especially values constants give approximations 
reason corresponds log density super gaussian distribution closely related maximum likelihood estimation 
unifying view contrast functions possible give unifying view encompasses important contrast functions ica 
principles mutual information maximum likelihood essentially equivalent 
second discussed infomax principle equivalent maximum likelihood estimation 
nonlinear pca criteria shown equivalent maximum likelihood estimation 
hand discussed cumulant contrasts considered approximations mutual information 
seen multi unit contrast functions strictly equivalent closely related 
important reservation necessary valid densities fi likelihood su ciently approximations true densities independent components 
minimum bit information independent component sub super gaussian 
information available priori estimated data see :10.1.1.31.7467
situation quite di erent contrast functions cumulants general contrast functions section estimate directly independent components non gaussian distribution 
unit contrast functions avery similar situation 
negentropy approximated cumulants general contrast functions section shows considered contrast functions closely related 
neural computing surveys www icsi berkeley edu jagota ncs fact looking formulas likelihood mutual information sees considered sums unit contrast functions plus penalizing term prevents vectors wi converging directions 
called soft form decorrelation 
see contrast functions described single intuitive principle find nongaussian projections soft decorrelation sure di erent independent components 
choice contrast function essentially reduced simple choice estimating independent components parallel just estimating possibly 
corresponds approximately choosing symmetric hierachical decorrelation familiar pca learning 
important choice cumulant robust contrast functions functions robust contrast functions preferred applications 
algorithms ica choosing principles estimation ica discussed section needs practical method implementation 
usually means choosing objective function ica need decide optimize 
section shall discuss optimization problem 
statistical properties ica method depend objective functions 
statistical properties objective functions treated section 
section shall compare di erent algorithms mainly basis stability convergence speed memory requirements 
preprocessing data ica algorithms require preliminary sphering whitening data algorithms necessarily need sphering converge better sphered data 
recall data assumed centered zero mean 
sphering means observed variable eq 
linearly transformed variable qx covariance matrix equals unity transformation possible 
accomplished classical pca 
addition sphering pca may allow determine number independent components noise level low energy essentially concentrated subspace spanned rst principal components number independent components model 
methods exist estimating number signals independent components see 
reduction dimension partially justi es assumption section retained 
sphering qa orthogonal matrix bs gb bb recall assumed independent components si unit variance 
reduced problem nding arbitrary matrix model simpler problem nding orthogonal matrix eq 
solve independent components observed neural computing surveys www icsi berkeley edu jagota ncs worthwhile re ect sphering solve separation problem 
sphering de ned additional rotation sphering matrix uq sphering matrix orthogonal matrix 
nd correct sphering matrix equally separates independent components 
done rst nding sphering matrix determining appropriate orthogonal transformation suitable non quadratic criterion 
shall assume certain sections data sphered 
simplicity sphered data denoted transformed mixing matrix de nitions section 
algorithm needs preliminary sphering mentioned corresponding section 
mention sphering needed 
jutten algorithm pioneering inspired neural networks 
algorithm canceling non linear cross correlations see section 
non diagonal terms matrix updated wij yi yj odd non linear functions yi computed iteration diagonal terms wii set zero 
yi give convergence estimates independent components 
unfortunately algorithm converges severe restrictions see 
non linear decorrelation algorithms algorithms canceling non linear cross correlations introduced independently 
compared jutten algorithm algorithms reduce computational overhead avoiding matrix inversions improve stability 
example algorithm wx non linearities applied separately component vector identity matrix replaced positive de nite diagonal matrix 
algorithm called algorithm introduced yy yg principled way non linearities learning rules provided maximum likelihood infomax approach described subsection 
algorithms maximum likelihood infomax estimation important class algorithms consists maximization network entropy infomax conditions equivalent maximum likelihood approach see section 
usually algorithms stochastic gradient ascent objective function 
example algorithm derived tanh wx tanh function applied separately component vector wx 
tanh function derivative ofthe log density ofthe logistic distribution 
function works estimation super gaussian sparse independent components sub gaussian independent components functions see :10.1.1.31.7467
algorithm eq 
converges slowly noted researchers 
convergence may improved whitening data especially natural gradient 
neural computing surveys www icsi berkeley edu jagota ncs natural relative gradient method simpli es gradient method considerably better conditioned 
principle natural gradient geometrical structure parameter space related principle relative gradient uses lie group structure ica problem 
case basic ica principles amount right hand side obtain tanh wx 
modi cation algorithm need sphering 
interestingly algorithm special case non linear decorrelation algorithm closely related algorithm 
newton method maximizing likelihood introduced 
newton method converges fewer iterations drawback matrix inversion approximate needed iteration 
non linear pca algorithms nonlinear extensions known neural pca algorithms developed 
example non linear version hierarchical pca learning rule introduced wi yi yi ix yj wj suitable non linear scalar function 
symmetric versions learning rules extended non linear case manner 
connection algorithms non linear versions pca criteria see section proven 
general non linearities means learning rule uses higher order information learning 
learning rules may perform related higher order representation techniques projection pursuit blind deconvolution ica 
proven chosen non linearities learning rule perform ica data sphered whitened 
algorithms exactly maximizing nonlinear pca criteria introduced 
interesting simpli cation non linear pca algorithms algorithm 
feedback term learning rule replaced simpler giving learning rate step size sequence constant range function applied separately component vector wx data assumed sphered 
hierarchical version algorithm possible 
due simplicity algorithm properties analyzed detail 
neural unit learning rules principle stochastic gradient descent derive simple algorithms unit contrast functions explained 
consider rst whitened data 
example instantaneous gradient generalized contrast function respect normalization kwk account obtains hebbian learning rule normalize constant may de ned efg efg nonlinearity nonlinear function important point estimate multiplicative constant suitable manner 
fact estimate sign correctly shown 
unit neural computing surveys www icsi berkeley edu jagota ncs algorithms rst introduced kurtosis corresponds algorithms non whitened data introduced 
estimate independent components needs system units learns unit learning rule 
system contain feedback mechanisms units see 
special kind feedback developed solve problems non locality encountered learning rules 
neural adaptive algorithms relevant neural algorithms include exploratory projection pursuit algorithms applied ica 
due close connection ica projection pursuit surprising projection pursuit algorithms directly solve ica problem squares type algorithms non linear pca criteria algorithm adaptive algorithm related higher order cumulant tensors see section 
fastica algorithm adaptive algorithms stochastic gradient descent may problematic environment adaptation needed 
case practical situations 
convergence slow depends crucially choice learning rate sequence 
remedy problem batch block algorithms xed point iteration 
xed point algorithm named fastica introduced kurtosis fastica algorithm generalized general contrast functions 
sphered data unit fastica algorithm form efg gw weight vector normalized unit norm iteration function derivative function general contrast function eq 

expectations estimated practice sample averages su ciently large sample input data 
units fastica algorithm combined just case neural learning rules systems estimate independent components 
systems may estimate independent component hierarchical decorrelation de ation may estimate independent components parallel symmetric decorrelation 
versions need preliminary sphering introduced 
fastica algorithm neural parallel distributed adaptive 
data point immediately learning fastica uses sample averages computed larger samples data 
convergence speed xed point algorithms clearly superior neural algorithms 
speed factors range usually observed :10.1.1.56.2430
interesting point proven fastica symmetric decorrelation essentially equivalent newton method maximum likelihood estimation 
means fastica general algorithm optimize unit multi unit contrast functions 
tensor algorithms large amount research done algorithms utilizing fourth order cumulant tensor see section estimation ica 
typically batch algorithms non adaptive tensorial techniques decomposition generalization eigenvalue decomposition higher order tensors 
decomposition performed ordinary algorithms eigen value decomposition matrices requires matrices size matrices neural computing surveys www icsi berkeley edu jagota ncs large specialized lanczos type algorithms lower complexity developed 
algorithms perform ciently small dimensions 
large dimensions memory requirements may prohibitive coe cients th order tensor stored memory requires units memory 
algorithms tend quite complicated program requiring sophisticated matrix manipulations 
weighted covariance methods eigenvalue decomposition weighted covariance matrix data explained section allows computation ica estimates standard methods linear algebra matrices reasonable complexity 
data sphered 
method computationally highly cient unfortunately works severe restriction independent components di erent 
choice algorithm summarize choice ica algorithm basically choice adaptive batch mode block algorithms 
adaptive case algorithms obtained stochastic gradient methods 
case independent components estimated time popular algorithm category natural gradient ascent likelihood related contrast functions infomax unit case straightforward stochastic gradient methods give adaptive algorithms maximize negentropy approximations 
case computations batch mode cient algorithms available 
tensor methods cient small dimensions larger dimensions 
fastica algorithm xed point iteration cient batch algorithm maximize unit contrast functions multi unit contrast functions including likelihood 
noisy ica section shall treat estimation noisy ica model de nition section 
methods exist 
estimation noiseless model challenging task noise usually neglected order obtain tractable simple results 
may unrealistic cases assume data divided signals noise meaningful way 
practically methods noise explicitly account assume noise gaussian 
higher order cumulants example th th order cumulants una ected gaussian noise methods 
taken 
note cumulant methods second fourth order cumulants 
second order cumulants immune gaussian noise 
cumulant methods modi ed noisy case 
lack robustness may problematic noisy environment 
maximum likelihood estimation noisy ica model treated 
maximize joint likelihood mixing matrix realizations independent components 
principled method maximize marginal likelihood mixing matrix possibly noise covariance done :10.1.1.30.4840
idea approximating densities independent components gaussian mixture densities application em algorithm feasible 
simpler case discrete valued independent components treated 
problem em algorithm computational complexity grows exponentially dimension data 
neural computing surveys www icsi berkeley edu jagota ncs promising approach noisy ica bias removal techniques 
means ordinary noise free ica methods modi ed bias due noise removed reduced 
shown modify natural gradient ascent likelihood reduce bias 
new concept called gaussian moments introduced derive unit contrast functions obtain version fastica algorithm asymptotic bias consistent presence noise 
methods large dimensions 
surveyed contrast functions algorithms ica 
ica general concept wide range applications neural computing signal processing statistics 
ica gives representation transformation multidimensional data suited subsequent information processing 
components representation independent possible time non gaussian possible 
transformation may interesting right blind source separation 
discussion methods proposed ica shown basic choice ica method reduce questions 
choice estimating independent components time estimating subset possibly 
ica research concentrated rst option practice second option interesting due computational considerations 
second choice adaptive algorithms batch mode block algorithms 
research concentrated option applications option preferable computational reasons 
spite large amount research conducted basic problem authors area research means exhausted 
may estimation methods basic ica model developed probability new breakthroughs may large 
di erent extensions basic framework provide important directions research example 
estimation noisy ica model estimation model overcomplete bases independent components observed mixtures basic problems require research :10.1.1.134.6077:10.1.1.30.4840:10.1.1.164.7690

methods tailor characteristics practical application may important areas 
example cases useful able estimate model similar ica model components necessarily independent 
exact conditions enable estimation model case interesting formulate 
steps direction 

problem overlearning ica pointed 
avoiding detecting overlearning great importance practical applications 

come stochastic process sample random variable blind source separation accomplished methods time correlations 
integrating information ica methods may improve performance 
important extension ordinary ica contrast functions case introduced itwas proposed kolmogorov complexity gives meaningful extension mutual information 

ica blind separation stochastic processes may time delays 
case signals propagate slowly physical sources sensors distances sensors sources equal signals reach sensors time 
happens array processing sound signals 
related problem blind source separation 
due phenomena kind blind deconvolution blind source separation see 
neural computing surveys www icsi berkeley edu jagota ncs 
non linear ica important time intractable problem 
neural networks statistics non linear methods developed may possible apply ica :10.1.1.130.110
example self organizing map non linear ica sub gaussian independent components 
approach generalized generative topographic mapping 
identi ability non linear ica models needs research results appeared 

definition cumulants appendix de nitions cumulants 
consider scalar random variable zero mean say characteristic function denoted expanding logarithm characteristic function taylor series obtains log constants 
constants called cumulants distribution particular rst cumulants zero mean variables simple expressions efx efx particular interest fourth order cumulant called kurtosis expressed kurt efx efx kurtosis considered measure non gaussianity ofx 
gaussian random variable kurtosis zero typically positive distributions heavy tails peak zero negative densities lighter tails 
distributions positive resp 
negative kurtosis called super gaussian resp 
sub gaussian 
cumulants compared centered moments 
th moment ofx de ned 
simplicity assumed zero mean case centered moments moments mean equal non centered moments moments zero 
moments may obtained taylor expansion identical logarithm taken left side 
note rst moments equal rst cumulants 
longer case 
mathematical simplicity cumulant approach ica due certain linearity properties cumulants 
kurtosis formulated follows 
independent random variables holds kurt kurt kurt kurt kurt scalar 
cumulants random variables xm de ned similarly 
cross cumulant cum xi xi xik set indices ij de ned coe cient term ti ti tik taylor expansion logarithm characteristic function vector xm 
neural computing surveys www icsi berkeley edu jagota ncs nomenclature pca principal component analysis ica independent component analysis pdf probability density function variables constants general purpose index imaginary unit dimension observed data dimension transformed component vector time iteration index general purpose scalar random variables yi output th neuron neural network scalar constant learning rate constant sequence vectors printed boldface lowercase letters column vectors observed data dimensional random vector input vector neural network dimensional random vector transformed components si dimensional random noise vector dimensional constant vector wi weight vectors neural network indexed dimensional general purpose random vector output vector neural network matrices printed boldface capital letters constant mixing matrix ica model transformed mixing matrix covariance matrix weight matrix arti cial neural network rows general transformation matrix functions ef mathematical expectation probability density function fi marginal probability density functions characteristic function random variable scalar non linear function di erential entropy mutual information negentropy kullback leibler divergence jg generalized contrast function general transformation fir lter th order cumulant scalar random variable kurt kurtosis fourth order cumulant cum cumulant cross cumulant random variables notation change parameter proportional proportionality constant derivative function neural computing surveys www icsi berkeley edu jagota ncs amari cichocki yang 
new learning algorithm blind source separation 
advances neural information processing pages 
mit press cambridge ma 

amari 
neural learning structured parameter spaces natural riemannian gradient 
advances neural information processing pages 
mit press cambridge ma 

amari cichocki 
adaptive blind signal processing neural network approaches 
proceedings ieee 
atick 
entropy minimization design principle sensory perception 
international journal neural systems 
supp 

bar ness 
bootstrapping adaptive interference practical limitations 
globecom conf pages miami 

barlow 
possible principles underlying transformations sensory messages 
editor sensory communication pages 
mit press 
barlow 
single units sensation neuron doctrine perceptual psychology 
perception 
barlow 
unsupervised learning 
neural computation 
barlow 
computational goal neocortex koch davis editors large scale neuronal theories brain 
mit press cambridge ma 
barlow mitchison 
finding minimum entropy codes 
neural computation 
bartlett 
note multiplying factors various chi square approximations 
roy 
stat 
soc ser 
bell sejnowski 
information maximization approach blind separation blind deconvolution 
neural computation 
bell sejnowski 
learning higher order structure natural sound 
network 
bell sejnowski 
independent components natural scenes edge lters 
vision research 

cardoso 
maximum likelihood source separation expectationmaximization technique deterministic stochastic implementation 
proc 
pages 

cardoso moulines 
blind source separation technique second order statistics 
ieee trans 

bishop williams 
gtm generative topographic mapping 
neural computation 
ch 

blind separation wide band sources frequency domain 
proc 
icassp volume pages detroit michigan usa may 

cardoso 
source separation higher order moments 
proc 
icassp pages 
neural computing surveys www icsi berkeley edu jagota ncs 
cardoso 
eigen structure fourth order cumulant tensor application blind source separation problem 
proc 
icassp pages albuquerque nm usa 

cardoso 
super symmetric decomposition fourth order cumulant tensor 
blind identi cation sources sensors 
proc 
icassp pages 

cardoso 
iterative techniques blind source separation fourth order cumulants 
proc 
eusipco pages brussels belgium 

cardoso 
infomax maximum likelihood source separation 
ieee letters signal processing 

cardoso 
blind signal separation statistical principles 
proceedings ieee 
cardoso 
multidimensional independent component analysis 
proc 
icassp seattle wa 
cardoso 
entropic contrasts source separation 
haykin editor adaptive unsupervised learning 


cardoso comon 
independent component analysis survey algebraic methods 
proc 
iscas volume pages 

cardoso laheld 
equivariant adaptive source separation 
ieee trans 
signal processing 

cardoso 
blind beamforming non gaussian signals 
iee proceedings 
cichocki pope 
modi ed herault jutten algorithms blind separation sources 
digital signal processing 
cichocki douglas 
amari 
robust techniques independent component analysis noisy data 
neurocomputing 
cichocki unbehauen 
neural networks signal processing optimization 
wiley 
cichocki unbehauen 
robust neural networks line learning blind identi cation blind separation sources 
ieee trans 
circuits systems 
cichocki unbehauen 
new line adaptive algorithm blind separation source signals 
proc 
int 
symposium arti cial neural networks pages taiwan 
comon 
blind identi cation presence noise 
signal processing vi theories application proc 
eusipco pages 
elsevier 
comon 
independent component analysis new concept 
signal processing 
cook buja cabrera 
projection pursuit indexes orthonormal function expansions 
computational graphical statistics 
daugman 
entropy reduction decorrelation visual coding oriented neural receptive elds 
ieee trans 
biomedical engineering 
deco 
linear redundancy reduction learning 
neural networks 
neural computing surveys www icsi berkeley edu jagota ncs 
adaptive blind separation independent sources de ation approach 
signal processing 

adaptive blind separation convolutive mixtures 
proc 
icassp pages 
donoho 
minimum entropy deconvolution 
applied time series analysis ii pages 
academic press 
douglas cichocki amari 
bias removal technique blind source separation noisy measurements 
electronics letters 
field 
goal sensory coding 
neural computation 
friedman tukey 
projection pursuit algorithm exploratory data analysis 
ieee trans 
computers 
friedman 
exploratory projection pursuit 
american statistical association 
fyfe baddeley 
non linear data structure extraction simple hebbian networks 
biological cybernetics 
karhunen oja :10.1.1.56.2430
experimental comparison neural ica algorithms 
proc 
int 
conf 
arti cial neural networks icann pages sk vde sweden 
girolami fyfe 
extended exploratory projection pursuit network linear nonlinear anti hebbian connections applied cocktail party problem 
neural networks 
hampel 
robust statistics 
wiley 
harman 
modern factor analysis 
university chicago press nd edition 
hastie stuetzle 
principal curves 
journal american statistical association 
haykin editor 
blind deconvolution 
prentice hall 
haykin 
adaptive filter theory 
prentice hall international rd edition 
hecht nielsen 
replicator neural networks universal optimal source coding 
science 
huber 
robust statistics 
wiley 
huber 
projection pursuit 
annals statistics 
hyv rinen oja 
wavelets natural image statistics 
proc 
scandinavian conf 
image analysis finland 
hyv rinen 
purely local neural principal component independent component learning 
proc 
int 
conf 
arti cial neural networks pages bochum germany 
hyv rinen 
family xed point algorithms independent component analysis 
proc 
ieee int 
conf 
acoustics speech signal processing icassp pages munich germany 
neural computing surveys www icsi berkeley edu jagota ncs hyv rinen 
independent component analysis minimization mutual information 
technical report helsinki university technology laboratory computer information science 
hyv rinen 
unit contrast functions independent component analysis statistical analysis 
neural networks signal processing vii proc 
ieee workshop neural networks signal processing pages amelia island florida 
hyv rinen 
independent component analysis presence gaussian noise maximizing joint likelihood 
neurocomputing 
hyv rinen 
new approximations di erential entropy independent component analysis projection pursuit 
advances neural information processing systems pages 
mit press 
hyv rinen 
fast robust xed point algorithms independent component analysis 
ieee trans 
neural networks 
appear 
hyv rinen 
fast independent component analysis noisy data gaussian moments 
proc 
int 
symp 
circuits systems orlando florida 
appear 
hyv rinen 
xed point algorithm maximum likelihood estimation independent component analysis 
neural processing letters 
appear 
hyv rinen 
sparse code shrinkage denoising nongaussian data maximum likelihood estimation 
neural computation 
submitted 
hyv rinen oja 
fast algorithm estimating overcomplete ica bases image windows 
proc 
int 
joint conf 
neural networks washington 
hyv rinen hoyer 
independent subspace analysis shows emergence phase shift invariant features natural images 
proc 
int 
joint conf 
neural networks washington 
hyv rinen oja 
simple neuron models independent component analysis 
int 
journal neural systems 
hyv rinen oja 
fast xed point algorithm independent component analysis 
neural computation 
hyv rinen oja 
independent component analysis general nonlinear hebbian learning rules 
signal processing 
hyv rinen oja hoyer 
image feature extraction sparse coding independent component analysis 
proc 
int 
conf 
pattern recognition icpr pages brisbane australia 
hyv rinen pajunen 
nonlinear independent component analysis existence uniqueness results 
neural networks 
hyv rinen rel rio 
spikes bumps artefacts generated independent component analysis insu cient sample size 
proc 
int 
workshop independent component analysis signal separation ica pages france 
principal component analysis 
springer verlag 
jones sibson 
projection pursuit royal statistical society ser 

neural computing surveys www icsi berkeley edu jagota ncs jutten 
calcul traitement du signal analyse en ind 
phd thesis univ grenoble 
french 
jutten herault 
blind separation sources part adaptive algorithm neuromimetic architecture 
signal processing 
karhunen hyv rinen oja 
applications neural blind separation signal image processing 
proc 
ieee int 
conf 
acoustics speech signal processing icassp pages munich germany 
karhunen 
representation separation signals nonlinear pca type learning 
neural networks 
karhunen 
generalizations principal component analysis optimization problems neural networks 
neural networks 
karhunen oja wang 
class neural networks independent component analysis 
ieee trans 
neural networks 
karhunen pajunen 
blind source separation squares type adaptive algorithms 
proc 
ieee int 
conf 
acoustics speech signal processing icassp pages munich germany 
karhunen pajunen oja 
nonlinear pca criterion blind source separation relations approaches 
neurocomputing 
kendall 
multivariate analysis 
charles gri 
kendall stuart 
advanced theory statistics 
charles gri 
oja 
independent component analysis parallel nancial time series 
proc 
iconip volume pages tokyo japan 
kohonen 
self organizing maps 
springer verlag berlin heidelberg new york 
laheld jean fran ois cardoso 
adaptive source separation uniform performance 
proc 
eusipco pages edinburgh 
lambert 
multichannel blind deconvolution fir matrix algebra separation multipath mixtures 
phd thesis univ southern california 
de lathauwer de moor vandewalle 
technique higher order blind source separation 
proc 
iconip hong kong 
lawley 
test signi cance latent roots covariance correlation matrices 


lee girolami bell sejnowski 
unifying information theoretic framework independent component analysis 
international journal mathematical computer models 
appear 

lee girolami sejnowski 
independent component analysis extended infomax algorithm mixed sub gaussian super gaussian sources 
neural computation pages 


lee koehler 
blind source separation nonlinear mixing models 
neural networks signal processing vii pages 
neural computing surveys www icsi berkeley edu jagota ncs lewicki olshausen 
inferring sparse overcomplete image codes cient coding framework 
advances neural information processing proc 
nips pages 
mit press 
lewicki sejnowski 
overcomplete representations 
advances neural information processing proc 
nips pages 
mit press 
lindgren 
local convergence class blind separation algorithms 
ieee trans 
signal processing 
makeig bell 
jung 
sejnowski 
independent component analysis electroencephalographic data 
advances neural information processing systems pages 
mit press 
mallat 
theory multiresolution signal decomposition wavelet representation 
ieee trans 
pami 

extended anti hebbian adaptation unsupervised source extraction 
proc 
icassp pages atlanta georgia 
mckeown makeig brown 
jung kindermann bell sejnowski 
blind separation functional magnetic resonance imaging fmri data 
human brain mapping 
schuster 
separation mixture independent signals time delayed correlations 
phys 
rev lett 
moreau 
new self adaptive algorithms source separation contrast functions 
proc 
ieee signal processing workshop higher order statistics pages lake tahoe usa june 
moulines :10.1.1.30.4840
cardoso 
maximum likelihood blind separation deconvolution noisy signals mixture models 
proc 
ieee int 
conf 
acoustics speech signal processing icassp pages munich germany 

nadal parga 
non linear neurons low noise limit factorial code maximizes information transfer 
network 
mendel 
signal processing higher order spectra 
ieee signal processing magazine pages july 
oja 
simpli ed neuron model principal component analyzer 
mathematical biology 
oja 
neural networks principal components subspaces 
int 
neural systems 
oja 
nonlinear pca learning rule independent component analysis 
neurocomputing 
oja 
nonlinear pca criterion maximum likelihood independent component analysis 
proc 
int 
workshop independent component analysis signal separation ica pages france 
oja karhunen 
stochastic approximation eigenvectors eigenvalues expectation random matrix 
journal math 
analysis applications 
neural computing surveys www icsi berkeley edu jagota ncs oja ogawa 
learning nonlinear constrained hebbian networks 
kohonen editor arti cial neural networks proc 
icann pages espoo finland 
north holland amsterdam 
olshausen field 
emergence simple cell receptive eld properties learning sparse code natural images 
nature 
olshausen field 
natural image statistics cient coding 
network may 
olshausen field 
sparse coding overcomplete basis set strategy employed 
vision research 
pajunen 
blind source separation algorithmic information theory 
neurocomputing 
pajunen hyv rinen karhunen 
nonlinear blind source separation self organizing maps 
proc 
int 
conf 
neural information processing pages hong kong 
pajunen karhunen 
maximum likelihood approach nonlinear blind source separation 
proceedings int 
conf 
arti cial neural networks icann pages lausanne switzerland 
papoulis 
probability random variables stochastic processes 
mcgraw hill rd edition 
pearlmutter parra 
context sensitive generalization ica 
proc 
iconip pages hong kong 

pham jutten 
separation mixture independent sources maximum likelihood approach 
proc 
eusipco pages 

performance blind source separation cdma downlink 
proc 
int 
workshop independent component analysis signal separation ica pages france 
sato 
method self recovering equalization multilevel amplitude modulation system 
ieee trans 
communications 
schervish 
theory statistics 
springer 
schmidhuber 
semilinear predictability minimization produces wellknown feature detectors 
neural computation 
weinstein 
new criteria blind deconvolution phase systems channels 
ieee trans 
information theory 
weinstein 
super exponential methods blind deconvolution 
ieee trans 
information theory 

blind separation sources part iii stability analysis 
signal processing 
sun 
practical aspects exploratory projection pursuit 
siam sci 
comput 
taleb jutten 
nonlinear source separation mixtures 
proc 
european symposium arti cial neural networks pages bruges belgium 
neural computing surveys www icsi berkeley edu jagota ncs 
nguyen thi jutten 
blind source separation convolutive mixtures 
signal processing 
tong 
liu soon 
huang 
indeterminacy identi ability blind identi cation 
ieee trans 
circuits systems 
tong soo liu huang 
new blind identi cation algorithm 
proc 
iscas new orleans usa 
torkkola 
blind separation delayed sources information maximization 
proc 
icassp pages atlanta georgia 
van hateren ruderman 
independent component analysis natural image sequences yields spatiotemporal lters similar simple cells primary visual cortex 
proc 
royal society ser 

van hateren van der schaaf 
independent component lters natural images compared simple cells primary visual cortex 
proc 
royal society ser 

rio 
extraction ocular artifacts eeg independent component analysis 

clin 
neurophysiol 
rio ki inen hari oja 
independent component analysis identi cation artifacts recordings 
advances neural information processing proc 
nips pages cambridge ma 
mit press 
rio rel oja 
independent component analysis wave decomposition auditory evoked elds 
proc 
int 
conf 
arti cial neural networks icann pages sk vde sweden 
rio rel ki oja 
independent component analysis decomposition auditory somatosensory evoked elds 
proc 
int 
workshop independent component analysis signal separation ica pages france 
wang 
coherent signal subspace processing detection estimation angles arrival multiple wide band sources 
ieee trans 
assp 

wang karhunen 
uni ed neural algorithm robust pca mca 
int 
neural systems 
wax kailath 
detection signals information theoretic criteria 
ieee trans 
assp 
weinstein feder oppenheim 
multi channel signal separation decorrelation 
ieee trans 
sap 
wiggins 
minimum entropy deconvolution 

yellin weinstein 
criteria multichannel signal separation 
ieee trans 
signal processing 
yellin weinstein 
multichannel signal separation methods analysis 
ieee trans 
signal processing 
