journal machine learning research submitted published word sequence kernels nicola nicola xrce xerox com eric gaussier eric gaussier xrce xerox com cyril goutte cyril goutte xrce xerox com jean michel renders jean michel renders xrce xerox com xerox research centre europe de france editors kandola thomas hofmann tomaso poggio john shawe taylor address problem categorising documents kernel methods support vector machines 
joachims ample experimental evidence svm standard word frequencies features yield state art performance number benchmark problems 
lodhi 
proposed string kernels novel way computing document similarity matching non consecutive subsequences characters 
article propose technique sequences words characters 
approach advantages particular efficient computationally ties closely standard linguistic pre processing techniques 
extensions sequence kernels dealing symbol dependent match dependent decay factors empirical evaluations extensions reuters datasets 
keywords kernel machines text categorisation linguistic processing string kernels sequence kernels 
application machine learning techniques classification documents rich challenging research area related tasks routing filtering cross lingual information retrieval 
joachims researchers yang liu shown support vector machines svm perform favourably compared competing techniques document categorisation kernel machines popular choice document processing 
reported works documents represented standard vector space aka bag word model salton mcgill word frequencies various added conjunction general purpose kernels linear polynomial rbf 
watkins lodhi 
proposed string kernels significant departures vector space model 
string kernels features word frequencies implicit expansion thereof extent possible ordered subsequences characters represented document 
addition lodhi 
proposed recursive dynamic programming formulation allowing practical computation similarity sequences arbitrary symbols dot product implicit feature space ordered non consecutive subsequences symbols 
allows perform kernel calculation performing explicit feature space expansion formulation extremely computationally demanding applicable current processing power large document collections approximation lodhi 
document propose extend idea sequence kernels process documents sequences words 
greatly expands number symbols consider symbols words characters reduces average number symbols document 
dynamic programming formulation computing sequence matching depends sequence length yields significant improvement computing efficiency 
training svm dataset documents reuters nicola eric gaussier cyril goutte jean michel renders 
gaussier goutte renders corpus feasible approximation 
addition matching sequences words allows symbols expected linguistically meaningful 
leads extensions word sequence kernels implement kind inverse document frequency idf weighting allowing decay factors 
words may equivalent context show implement soft word matching conjunction word sequence kernel 
allows kernel multilingual context application possible string kernel formulation 
section offer brief self contained kernel methods proceed presentation sequence kernels section 
section formulates verifies hypothesis helps explain string kernels perform processing entities characters essentially meaningless linguistic point view 
section presents extension sequence kernels word sequences test empirically section 
section extensions word sequence kernels soft matching cross lingual document similarity 

kernel methods kernel methods research field rapid expansion 
theoretical statistical learning theory vapnik reproducing kernel hilbert spaces wahba kernel machines popular svm boser cortes vapnik 
section briefly introduce kernel classifiers sequence kernels 
additional information reader may refer general introductions cristianini shawe taylor sch lkopf smola herbrich 
kernel classifiers binary classifier function input space set binary labels 
supervised learning algorithm function assigns labeled training set binary classifier 
computational reasons order limit overfitting learning algorithms typically consider subset classifiers hypothesis space 
common case vector space simplest binary classifiers linear discriminant sign denotes standard dot product 
learning linear classifier amounts finding values maximise measure performance 
learning algorithms linear classifiers include fisher linear discriminant perceptron svm 
linear classifiers naturally fail boundary classes linear 
possible leverage conceptual simplicity projecting data different feature space hope achieve linear separation classes 
denoting projection feature space generalised linear classifier sign separating hyperplane defined feature space corresponding classifier input space generally non linear 
note setting need vector space long crucial observation rest article dealing case associative monoid sequences fixed alphabet vector space 
case svm optimal weights expressed linear combination data yi xi weight th training example input label yi 
accordingly combining svm classifier written dual form sign yi xi word sequence kernels notice projection appears context dot product 
addition optimal weights solution high dimensional quadratic programming problem expressed terms dot products projected training data explicit mapping kernel function long calculated efficiently need explicitly map data feature space kernel trick 
especially useful computational cost calculating overwhelming high dimensional simple expression 
fact mercer theorem see cristianini shawe taylor section states positive semi definite symmetric function arguments corresponds mapping space valid kernel 
means principle kernel nature corresponding mapping known 
article addresses application kernel methods known problem document categorisation 
far kernel area standard bag words representation word frequencies features vector representations proved largely successful investigate representations intuitively closer intrinsic nature documents sequences symbols 
representation form vector space appropriate kernels allow leverage discriminative power kernel algorithms 
see kernels going consider allow test basic factors order locality multi word terms context document categorisation 
sequence kernels assumption popular bag words representation relative position tokens little importance information retrieval ir tasks 
documents just represented word frequencies possibly additional weighting normalisation loss information regarding word positions compensated powerful algorithms working vector space 
methods inject positional information phrases multi word units perez strzalkowski local occurrence statistics wong underlying techniques vector space representation 
string kernels sequence kernels significant departures vector space representation domain 
string kernels haussler watkins lodhi similarity measures documents seen sequences symbols possible characters alphabet 
general similarity assessed number possibly non contiguous matching subsequences shared sequences 
non contiguous occurrences penalized number gaps contain 
finite alphabet sequence si 
subset indices indicate subsequence si si sin note necessarily form contiguous subsequence example sequence cart write length spanned 
sequence kernel strings defined kn decay factor penalise non contiguous subsequences sum refers possible subsequences length equation defines valid kernel amounts performing inner product feature space feature ordered subsequence value intuitively means match possible subsequences symbols subsequences consecutive occurrence discounted length 
gaussier goutte renders direct computation terms nested sum impractical small values addition spirit kernel trick discussed section wish calculate directly perform explicit expansion feature space 
done recursive formulation proposed lodhi 
leads efficient dynamic programming implementation 
derivation efficient recursive formulation appendix illustrative example 
results equations min min sx ty sx iff sx tx sx sx sx kn min kn sx kn time required compute kernel formulation 
situations may convenient perform linear combination sequence kernels different values recursive formulation turns computing kernel values subsequences lengths significantly costly computing kernel 
order keep kernel values comparable different values independent length strings may advisable consider normalised version generalisation cosine normalisation widespread ir corresponds mapping norm 

noisy stemming hypothesis text categorisation string kernels operating character level shown yield performance comparable kernels traditional bag words representation lodhi 
result somewhat surprising considering string kernels low level information 
possible explanation effectiveness string kernel supported observation performance improves subsequences relevant categorisation may correspond noisy versions word stems implicitly implementing form morphological normalisation 
furthermore gaps sequence allowed penalized string kernels pick stems consecutive words illustrated 
known help ir applications salton mcgill 
order test noisy stemming hypothesis experiment conducted 
svm string kernel trained small subset reuters corpus labels topic 
sample features subsequences fixed length randomly chosen 
order avoid vast majority irrelevant low weight features restricted possibly non contiguous subsequences appearing support vectors 
sampled feature impact categorisation decision compared measure impact categorisation decision measured absolute 
available www com resources reuters 
subset built documents acq label documents acq 
grams word sequence kernels pri grams sh oi nt en ture noisy stemming hypothesis 
grams may able pick parts words stems top pick parts stems consecutive words bottom 
weight wu linear decision wu jy order assess feature extent feature approximates word stem simplifying assumption considering stems initial characters word 
order verify feature may span consecutive stems measure devised consider pairs consecutive tokens 
tm 
tm tm tokenisation string tokenisation segmentation process provides indices symbol token 
set matches subsequence pair consecutive tokens starting tk tk respectively tk tk tk matching pair constraint tk tk considered automatically satisfied 
addition forced different order avoid double counting single word matches empty symbol added string allow word matching token 
subsequence respect defined avg min tk tk words pair consecutive tokens containing match pair indices selected matches token matches second component compact close start corresponding token possible 
respect training corpus defined micro average documents corpus 
experiment performed sample features length sample features cases 
noisy stemming hypothesis expect relatively features high weight low 
outcome experiment table confirm 
features high weight high features low weight fraction features high 
similar results respectively 
non empty set matches calculated finding compact match argmin tk tk interesting check minimal subsequence matches tokens opposed single token match 
purpose define compact match token 
subsequence sequence avg gaussier goutte renders wu low high low high wu low high low high table contingency tables weight 
left trigrams weights split separate lowest highest values split separate lowest highest values 
right weights split separate lowest highest values split separate lowest highest values 
values tables left right suggests highly significant departure independence cases 
density density high weight high stem high weight high stem density density high weight low stem high weight low stem density high weight features 
feature samples estimate distribution features high weights interesting features 
shows mass distribution close indicating matching occurs word 
effect clearer larger high weight features length concentrated near 
reasonable considering longer subsequences harder match single word 
hand significant amount high weight high features length clearly single stem features 
addition features high weight low tendency spread stems right column vs left column 
suggests multiple word matching really occur beneficial forming discriminant high weight features 
agreement results svm traditional kernels joachims showing polynomial kernels consistently outperform linear kernel 

word sequence kernels word sequence kernels validity noisy stemming hypothesis suggests sequence kernels operating word possibly word stem level prove effective operating character level 
section describe impact applying sequence kernels word level extend original kernel formulation take account additional information 
theory direct application sequence kernel defined alphabet individual words raises significant issues 
feature space documents implicitly mapped dimension ordered tuples symbols alphabet going characters words order magnitude increases hundreds tens thousands number dimensions feature space increases accordingly 
average length symbols documents decreases order magnitude 
algorithm computing sequence kernels depends sequence length alphabet size yields significant improvement computing efficiency word sequence kernels computed datasets string kernels approximated 
combination effects increase alphabet size decrease document average length causes documents extremely sparse implicit representations feature space 
turn means kernel similarity pair distinct documents tend small respect self similarity documents especially larger values words gram matrix tends nearly diagonal meaning examples nearly orthogonal 
order overcome problem convenient replace fixed length sequence kernel combination sequence kernels subsequence lengths fixed straightforward formulation parameter linear combination kn ki considering dynamic programming technique implementation computing sequence kernel subsequences length requires marginal increase time compared computation subsequences length 
notice kernel values different subsequence lengths normalised independently combined 
way possible control relative weight different subsequence lengths directly means parameter 
parameter possibly parameters general linear combination sequence kernels different orders optimized cross validation alternatively kernel alignment cristianini :10.1.1.23.6757
original form sequence kernel relies mere occurrence counts term frequencies lacks feature weighting schemes deemed important ir community 
want extensions standard sequence kernel 
uses different values assign different weights gaps allows established weighting schemes inverse document frequency idf 
discriminate symbols say part speech 
second extension consists adopting different decay factors gaps symbol matches generalises previous extension sense informative symbols treated differently depending matches gaps 
symbol dependent decay factors original formulation sequence kernel uses unique decay factor symbols 
case sequences containing symbols significantly better discriminating power sequences containing symbols 
example sequence nouns informative sequence preposition determiner noun 
way leverage non uniform discriminative power consists assigning different decay factors distinct symbols 
gaussier goutte renders introduce distinct 
induces new embedding value feature gas injection sequence gas assist plastic injection gas assist plastic injection corresponding weighted sequence kernel kw defined follows kwn jn recursive formulation analogous original sequence kernel derived 
define functions kw kw store intermediate results see appendix kw kw sx kw general recursion equations section modifications replace equation equations 
different values way incorporating prior knowledge sequence kernel 
case word sequence kernels instance symbols words grouped part speech 
doing possible instance penalize heavily occurrences extraneous nouns gas assist plastic injection occurrences extraneous adverbs gas injection 
rationale sequence semantically closer gas injection plastic gas main substance injected sequence 
extension useful case word sequence kernels order integrate information inverse document frequency terms 
broadly formula idf account term weighting schemes log nc number documents collection number documents containing term formula yields value log terms occurring collection non zero weight need normalised decay factor log nc log notice tension different types information integrated 
hand words pertaining certain parts speech tend non discriminative inserted sequences example assigned high time words tend appear different documents leading low idf weighting scheme 
second extension propose solves problem 
independent decay factors gaps matches previous section introduced variable decay factors different symbols mentioned inclusion part speech information inverse document frequency background knowledge added extension 
gaps containing highly relevant symbols nouns word sequence kernels strongly penalized small highly relevant matching symbols rewarded large 
requirement met previous formulation 
way obtain desired behaviour introduce separate sets decay factors gaps matches 
coordinate feature vector sequence defined sk discount factor symbol occurs gap discount factor symbol occurs matching symbol 
definition instance discount gaps part speech weight matching symbols inverse document frequency information 
value feature gas injection sequence gas injection gas injection way consider unimportant appears gap close uninformative appears match close 
define weighted sequence kernel sequences evaluation define sk tl ui sl tr sx 
general recursion equations section modifications replace equation replace equation sx tx sx replace equation replace equation 
formulation shows considering separate sets decay factors matching symbols gaps impact computational complexity kernel 
remainder refer extension corresponding equation word sequence kernels symbol dependent matching scores decay factors 
experiments word sequence kernels generalise previously introduced methods respects individual terms matched 
amounts performing usual cosine measure document vectors kernel normalised 
symbol dependent matching scores decay factors adopted idf weighting schemes enforced gaps symbols subsequence penalised sequence kernel similar polynomial kernel unordered tuples terms replaced ordered tuples suggests way test importance order multi word terms gaussier goutte renders contiguous sequences matched sequence kernel amounts computing similarity documents number grams precisely number gram grams grams share varying constraint contiguity sequences relaxed suggests way test locality multi word terms fact words sequence appear relatively close important factor 
see word sequence kernels define paradigm possible test importance different basic factors document categorisation task 
assessing validity factors performance word sequence kernels goal experiments designed 
precisely want answer questions 
knowledge order matched terms sequence occur improve quality similarity measure 
words word sequence kernel perform better corresponding polynomial kernel 

knowledge distance matched terms sequence occur improve quality similarity measure 
word sequence kernel perform better 
similarly version gram version compare flexible word sequence kernel contiguity constraint partially removed 

relative importance individual words combination words indexing features 
words parameter affect performance 

results improve combination words considered 
performance affected value 

useful symbol dependent matching scores decay factors 
related point questioning word sequence kernel sensitive idf weighting schemes 

word sequence kernel compare kernels text categorisation 
order answer questions performed series experiments standard benchmark text categorisation systems reuters corpus 
adopted called modapte split leads training set documents test set 
computationally far tractable sequence kernels operating character level note standard string kernels directly computed collection need approximated mentioned lodhi 
word sequence kernels somewhat resource demanding 
decided limit attention frequent categories 
characteristics composition training test set relative categories summarised table 
documents preprocessed performing resolving ambiguities means part speech tagger 
xerox tools effect 
maps different morphological variants word singular plural forms nouns past tenses verbs feature lexeme 
underlying assumption inflected form bring extra useful information compared normalised form 
notice words ambiguous saw determining correct lemma example saw noun saw verb see verb non trivial 
reason part speech pos tagger tagger model able choose disambiguate pos category word context 
preprocessing number features average document length stopwords removed average length drops 

order kernel computations second sequences length ultra sparc ii processor 
word sequence kernels category training set neg pos ratio test set earn acq money grain crude trade interest ship wheat corn table number positive examples frequent categories reuters corpus modapte split 
experiments conducted svm light joachims package version appropriately adapted sequence kernels 
training sets categories unbalanced favour negative examples 
known problem svm algorithms implicitly tends maximise accuracy leading learning overly conservative classifiers 
svm light provides parameter weight relative importance positive negative examples training set 
heuristic rule experiments run parameter set integer value closest ratio negative positive examples training set 
evidence setting optimises measures usually adopted ir reasonable choice reduce impact lack balance training set 
performance evaluation experimental results standard ir performance measures 
test categorisation results calculate true positives tp number documents model correctly identifies positives false positives fp number documents model falsely identifies positives false negatives fn number documents model fails identify positives 
counts calculate performance measures precision ratio true positives retrieved documents tp tp fp recall ratio true positives positives tp tp fn score harmonic mean precision recall pr note measures depend threshold applied decision function order decide document relevant 
order provide threshold independent measure compute break point performance obtained threshold definition break point precision recall score equal 
remainder section experimental results centered setting weight terms size double terms order 
see setting general yield optimal performance 
note study impact factors order locality multi term length control way single word terms interact multi word terms 
context small value gives importance longer terms making impact visible 

available svmlight joachims org 
adaptation mainly consisted extending processing data structures different traditional vector modifying functions enhancing computing time performance precomputing gram matrix categories 
gaussier goutte renders micro average macro average bp bp stopwords removed stopwords removed table effect removing stopwords precision recall score break point bp 
results obtained 
performance measures calculated category 
order synthetic measure performance categories micro averaged macro averaged performance 
macro averaging consists simply averaging results obtained category micro averaging averages individual decisions document category 
effect micro averaging dominated performance large categories macro averaging gives equal influence categories 
experiments methods give consistent results 
microaveraging dominated large categories relatively easy learn effect different parameter settings usually visible macro averaged performance 
stopword removal stopword removal common practice document categorisation filtering 
context kernels favourable side effect reducing average sequence length correspondingly reducing kernel computation time 
preliminary experiment verified stopword removal beneficial case word sequence kernels 
results table show performance improves significantly performance measures stopword removal 
accordingly subsequent experiments performed sequences stopwords removed 
order word sequence kernels length implicit features ordered combinations words 
quadratic kernel bag word representation uses kind features take word order account 
word order allows differentiate documents containing words different orders potentially yielding higher precision 
hand order information considered recall may increase documents considered similar 
aim experiment evaluate impact order word combinations accuracy similarity measure 
effect compared performance word sequence kernel defined equation polynomial kernel degree 
cases svm margin parameter set default avg 
order ensure effect order measured preprocessing kernels stopword removal 
addition order ensure single terms multiple terms relative influence similarity computed normalised version quadratic kernel normalisation equations kp rows table display results obtained idf polynomial kernels fixed word sequence kernel 
word sequence kernel performs slightly worse normalised polynomial small consistent margin indicating account word order help categorisation task 
results obtained standard quadratic kernel largely inferior kernels 
idf applied un normalised quadratic kernel dominated products frequencies common words tend poor discriminators 
word sequence kernels micro average macro average bp bp kp polynomial var kp idf polynomial idf table impact word order account precision recall score break point bp 
compare word sequence kernels normalised polynomial standard polynomial kernel idf 
micro average macro average bp bp var var var var table impact locality precision recall score break point bp 
compare performance obtained symbol dependant decay factors gaps allowed gap allowed idf dependent 
results word sequence kernels 
rows table display results obtained idf eq 
polynomial kernels corresponding variable eq 
word sequence kernel 
word order account word sequence kernel yields better precision offset large gain recall observed standard polynomial kernel performs best 
results suggest word order account benefit categorisation performance 
locality decay factor allows control extent gaps allowed matching subsequences 
gaps effect sequence similarity gap subsequence drive corresponding feature value similarity cf 
eqs 

aim second experiment check importance account distance matched words occur matches local small may occur entire document large 
note symbol dependent decay factors corresponds pure gram model similar polynomial kernels additional order constraint 
fixed shows setting little effect performance suggesting locality overly important categorisation task 
table displays similar results decay factors different settings results suggest locality strong impact categorisation task 
mean setting irrelevant 
symbol dependent decay factors gaps matches line table yields best performance small consistent edge alternatives 
micro averaged performance impact precision recall score break point gaussier goutte renders macro averaged performance impact precision recall score break point effect locality varying decay factor performance 
experiments 
decay factor number gaps matching subsequence little impact performance 
micro average macro average bp bp var var table impact precision recall score break point bp 
compare performance obtained symbol dependent decay factors weight multi word matches weight single word matches 
influence parameter gives relative weight multi word terms compared single words 
experimental results focus corresponds giving double weight matches words compared single word matches 
earlier allows emphasize effect multi word matches necessarily optimal 
results ir suggest usually helps give weight single terms respect multi word terms gaussier 
aim third experiment check impact performance 
shows increases influence multi word matches decreases precision decreases 
matches words certain indicator similarity matches single words 
hand forcing similarity consider multi word matches small may fail identify similar related documents share single words lower recall 
shows recall increases greatly larger 
compensates loss precision score increases 
effect observed symbol dependent illustrated table 
despite clear consistent impact precision recall fixed variable cases break point relatively insensitive parameter 
note larger values favour score categorisation task result consistent findings ir gaussier 
micro averaged performance impact word sequence kernels precision recall score break point macro averaged performance impact precision recall score break point effect varying relative weight multi terms performance 
experiments 
micro averaged performance impact precision recall score break point macro averaged performance impact precision recall score break point effect varying subsequence length performance 
experiments 
influence previous experiments considered single words word pairs 
performed experiments assess impact performance 
results displayed 
longer multi terms taken account precision improves 
corresponding loss recall compensates increase score breakeven point decrease 
gaussier goutte renders influence symbol dependent matching scores decay factors experiments show symbol dependent matching symbols positive influence performance 
mentioned section symbol dependent allow idf weighting scheme 
fact scheme improves results comes surprise importance idf term weighting known ir literature 
observe word sequence kernels sensitive term weighting quadratic kernels 
table shows performance kernel improves idf weighting scheme fourth line particular macro average improvement remains marginal 
contrary idf yields significant improvement quadratic kernel micro macro average break point increases micro average macro average table lines 
comparison word sequence kernels kernels best performance obtained word sequence kernels length line table comparable best performance obtained polynomial kernel degree line table 
addition results comparable state art results collection svm various bag word kernels 
experimental results show word sequence kernels perform better string kernels complete set documents string kernels approximated lodhi 
compared polynomial rbf kernels word sequence kernels disadvantage computationally demanding 
seen word sequence kernels allowed test importance different factors text categorisation approach possible 
lastly word sequence kernels rely parameters fixed experiments theory optimised specific collection 
investigate tuning 
experimental results summary summary experiments show 
word order account table little effect performance 
idf applied precision slightly increased recall slightly reduced 
locality se table virtually impact 
improvement obtained penalising heavily non contiguous word combinations obtained skipping terms low document frequency 
giving relative importance word combinations similarity measure increases precision expenses recall table 
similarly considering combination words indexing terms increases precision expenses recall 
weighting matches idf improves performance 
results obtained word sequence kernels comparable state art results collection 

new directions word sequence kernels previous section defined illustrated word sequence kernels extension symbol dependent decay factors 
introduce additional natural extensions word sequence 
categories joachims reported micro averaged break points linear quadratic rbf kernel 
word sequence kernels kernels 
extensions address linguistically motivated problems estimating similarity documents containing different words similar meanings synonyms documents different languages cross lingual 
soft matching standard definition word sequence kernels sections exact symbol matches contribute similarity 
shortcoming approach synonyms words related meanings considered similar 
address problem considering soft matching symbols matching equivalent identical symbols ensuring matching equivalent symbols contributes similarity exact match 
ir techniques implement kind soft matching 
example generalised vector space model gvsm cf 
wong sheridan uses document term matrix estimate term term similarity basis occurrences terms documents 
similarly context sequence kernels define soft matching extension invoking similarity matrix implicit feature space setting recover original formulation 
equation expresses valid kernel long similarity matrix positive definite 
note indexed feature space dimensions ordered subsequences length order simplify processing able calculate kernel value feature space expansion may convenient express similarity subsequence level product similarities symbol level uv case fold tensor product symbol similarity matrix axy positive definite soft matching word sequence kernel may calculated recursively equations replacing equations single equation sx ty sx axy modifying equation kn sx kn axt soft matching combined distinct decay factors gaps matches recursion formulas adapted accordingly 
vk sl tp jn cross lingual document similarity soft matching different symbols introduced possible define similarity sequences different alphabets 
context word sequence kernels means words may different languages 
kernel soft directly applied symbol similarity matrix encoding weighted translation words languages derived bilingual dictionaries corpora 
vector models developed monolingual ir extended multi lingual case parallel corpus 
see word sequence kernel relates models previously proposed cross language ir generalised vector space model gvsm latent semantic indexing lsi 
gaussier goutte renders alphabets corresponding dictionaries languages consideration 
document parallel collection pair aligned documents designate document clarity 
decompose matrix language specific parts 
generalised vector space model gvsm cross language ir amounts substitute original documents projection dual space dimensions induced clusters documents 
case cluster contains document standard cosine measure gets cos language language 
assuming raw frequencies term frequencies document representation expand numerator way ut uv corresponds term occurring position similarly 
lastly considering square matrix uv obtain symmetric expression corresponds soft matching word sequence kernel eq 
uv 
complete cosine similarity obtained standard length normalisation 
cross lingual lsi cross language latent semantic indexing model dumais 
similar monolingual setting deerwester 
singular value decomposition combined document term matrix assuming select dimensions highest singular values similarity calculated projecting documents matrix containing columns accordingly cosine similarity measure cross language lsi cos uk uk documents appropriately zero padded dimensional vectors 
way similar derived word sequence kernel gvsm equation derive word sequence kernel cross language lsi provided raw frequencies term frequencies 
case similarity coefficients uk uk uv 
complete equivalence cosine measure relies length normalisation 
note cross language ir gvsm lsi need parallel corpus case word sequence kernels computed bilingual lexicon derived comparable corpus example 

kernel methods allow efficient learning algorithms cases document representation necessarily vector 
example sequence kernels string kernel syllable kernel word sequence kernel represent competitive alternatives traditional bag words representation 
word sequence kernels tried analyse sequence kernels operating character level higher level information successful 
formulated noisy stemming hypothesis sequence kernels perform implicitly select relevant word stems indexing units 
showed hypothesis supported empirical observations 
observations suggest subsequences discriminant essentially distributed word stem contribute effectiveness categorisation indexing pairs consecutive words 
focussed word sequence kernels documents considered sequences words 
advantage string kernel semantically meaningful indexing units 
addition word sequence kernels significantly computationally demanding 
lend number natural extensions 
described symbol dependent decay factors independent decay factors gaps matches context word sequence kernels 
allows linguistically motivated background knowledge traditional weighting schemes 
showed independent idf decay factors yields better performance fixed 
flexibility word sequence kernels allowed test number hypotheses regarding impact word order locality multi term matches categorisation performance 
results suggest order locality essentially effect performance length multi term matches weight similarity measure clear influence precision recall score 
envisioned possibility soft matching symbols order take account similarity different related words 
opens possibility word sequence kernels context multi lingual document processing apply kernel methods documents different languages 
word sequence kernels particular case convolution kernels contributes investigation document representations structured bag words model context ir document categorisation 

sponsored european commission ist programme contract ist 
extremely grateful partners eu kernel methods images text project discussing topics 
particular wish john shawe taylor comments suggestions possible extensions word sequence kernel craig saunders help string kernel experiments 
acknowledge insightful comments anonymous reviewers 
appendix recursive formulation sequence kernels appendix gives intuitive description basic sequence kernel derivation efficient recursive formulation dynamic programming 
recall basic notation definitions finite alphabet sequence alphabet si 

subset indices indicate subsequence si si sin write value length window spanned 
basic sequence kernel works feature space possible subsequences length value associated feature defined decay factor penalise non contiguous subsequences 
extensively explained exploited section decay factor applied gaps gaussier goutte renders matching symbols 
instance value gaps taken account computing value feature 
value gap symbol contributes dividing feature value gap 
sequence kernel strings defined inner product kn intuitively means match possible subsequences symbols occurrence discounted size window spans 
consider example alphabet elementary sequences weights subsequences features sequences aat cag aca cat act atg ctt att instance value feature aat sequence occurrences aat 
spans window width third fourth symbols second spans window width third fifth symbols 
similarity score feature sequences non null value cat 
direct computation terms nested sum impractical small values addition spirit kernel trick discussed section wish calculate directly perform explicit expansion feature space 
done recursive formulation proposed lodhi 
leads efficient dynamic programming implementation 
recursive formulation reasoning 
suppose know value kernel strings need compute value kernel sx 
notice 
subsequences common common sx 
addition consider new matching subsequences occur symbol prefix occur possibly non contiguously 
point consider example 
distinct sequences length 
occur occurrences contain gaps occurrence spans principle window different length 
focus instance occurrence marked ifi set indices occurrence length window spanned occurrence ia 
occurrence give raise new matches length subsequence sx due presence occurrence marked occurrences right matches contribute kernel lengths ia ia set indices jl jl indices relevant occurrences indicated 
note factor contribution matching rest word sequence kernels contributions different occurrences common subsequences strings contribution gaps kernel 
similar inputs occurrences subsequences length strings 
rewrite kernel sx kn sx kn ln kn ln notice part second term innermost sums looks quite similar definition kernel sequences length contribution decays ln 
defining equation rewritten kn sx kn refers symbols intuitively counts matching subsequences symbols discounting length window span kn discounts distance symbol subsequence complete sequence 
example aa ga ac gc ag gg gt ca ta cc tc cg tg ct tt gaussier goutte renders act act act acta illustration recursion calculate similarity 
value feature ct sequence occurrences ct start second symbol symbols away values calculated recursively sx ji jn additional insight example useful 
sequences binary matrix defined pi mij value sx acta counts number matches sequences length length appropriately discounted element match sequences 
example matches taken account term act second appropriately discounted additional distance sequence sx caused final addition matches appropriately discounted taken account term act second third 
notice contribution term act null symbols act matches symbol intuitively algorithm store intermediate result total discounted mass matches length ready turned matches length symbol word sequence kernels match symbols mass propagated recursive formulation equation 
terms matrix means values left right rows discounting additional new step 
position value computed corresponds symbol match value accrued masses sequences length stored immediately previous column rows current position 
summarise recursive formulation inclusive base steps min kn min sx kn sx kn time required compute kernel formulation 
seen observing outermost recursion increasing values subsequence lengths length additional symbol sum prefix position considered required 
notice complexity reduced storing intermediate values sum 
define additional function sx intuitively sx stores sum discounted masses matches subsequences length column just considered matrix previous row 
easy see sx ty sx sx tx sx 
expressed function sx sx having introduced new element single sum updating sufficient sum values time complexity reduced 
boser guyon vapnik 
training algorithm optimal margin classifiers 
fifth annual workshop computational learning theory pages 
gaussier goutte renders cortes vapnik 
support vector networks 
machine learning 
cristianini shawe taylor 
support vector machines 
cambridge university press cambridge uk 
cristianini shawe taylor elisseeff kandola 
kernel target alignment 
technical report neurocolt 
www neurocolt com tech reps ps 
deerwester dumais furnas landauer harshman 
indexing latent semantic analysis 
journal american society information science 
dumais landauer littman 
automatic cross linguistic information retrieval latent semantic indexing 
proceedings acm sigir conference research information retrieval sigir 
gaussier grefenstette hull roux 
recherche information en traitement automatique des 
traitement automatique des 
hermes 
haussler 
convolution kernels discrete structures 
technical report ucsc crl department computer science university california santa cruz santa cruz ca 
herbrich 
learning kernel classifiers theory algorithms 
mit press cambridge mass 
joachims 
text categorization support vector machines learning relevant features 
proceedings european conference machine learning ecml number lecture notes computer science pages 
springer verlag 
joachims 
making large scale svm learning practical 
bernhard sch lkopf chris burges alex smola editors advances kernel methods support vector learning 
mit press 
lodhi cristianini shawe taylor watkins 
text classication string kernel 
advances neural information processing systems 
mit press 
lodhi saunders shawe taylor cristianini watkins 
text classification string kernels 
journal machine learning research 
perez strzalkowski 
natural language information retrieval progress report 
information processing management 
salton mcgill 
modern information retrieval 
mcgraw hill new york 
sch lkopf smola 
learning kernels 
mit press cambridge mass 
sheridan 

experiments multilingual information retrieval spider system 
proceedings th annual international acm sigir conference research development information retrieval sigir pages 
vapnik 
nature statistical learning theory 
springer new york 
wahba 
spline models observational data 
number nsf regional conf 
ser 
appl 
math 
siam 
watkins 
dynamic alignment kernels 
technical report csd tr department computer science royal holloway university london 
wong wong 
generalized vector space model information retrieval 
proceedings acm sigir conference research information retrieval si gir pages 
yang liu 
re examination text categorization methods 
proceedings nd acm sigir conference research development information retrieval pages 

