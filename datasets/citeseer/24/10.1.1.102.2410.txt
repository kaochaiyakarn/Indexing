online large margin training dependency parsers ryan mcdonald crammer fernando pereira department computer information science university pennsylvania philadelphia pa crammer pereira cis upenn edu effective training algorithm linearly scored dependency parsers implements online multi class training crammer singer crammer top efficient parsing techniques dependency trees eisner 
trained parsers achieve competitive dependency accuracy english czech language specific enhancements 
research training parsers annotated data part focused models training algorithms phrase structure parsing 
best phrase structure parsing models represent joint probability sentence having structure collins charniak 
generative parsing models convenient training consists computing probability estimates counts parsing events training set 
generative models complicated poorly justified independence assumptions estimations expect better performance discriminatively trained models shown tasks document classification joachims shallow parsing sha pereira 
ratnaparkhi conditional maximum entropy model ratnaparkhi trained maximize conditional likelihood training data performed nearly generative models vintage scores parsing decisions isolation may suffer label bias problem lafferty 
discriminatively trained parsers score entire trees sentence investigated riezler clark curran collins roark taskar 
reason discriminative training requires repeatedly training corpus current model determine parameter updates improve training criterion 
cost quite high simple context free models parsing complexity prohibitive lexicalized grammars parsing complexity 
dependency trees alternative syntactic representation long history hudson 
dependency trees capture important aspects functional relationships words shown useful applications including relation extraction sorensen paraphrase acquisition shinyama machine translation ding palmer 
parsed time eisner 
dependency parsing potential sweet spot deserves investigation 
focus projective dependency trees word parent arguments dependencies non crossing respect word order see 
cases crossing dependencies may occur case czech haji 
edges dependency tree may typed instance indicate grammatical function 
focus simpler non typed root john hit ball bat example dependency tree 
case algorithms easily extendible typed structures 
dependency parsing relevant research 
eisner gave generative model cubic parsing algorithm edge factorization trees 
yamada matsumoto trained support vector machines svm parsing decisions shift reduce dependency parser 
ratnaparkhi parser classifiers trained individual decisions quality parse 
scholz developed history learning model 
parser uses hybrid bottom topdown linear time heuristic parser ability label edges semantic types 
accuracy parser lower yamada matsumoto 
new approach training dependency parsers online large margin learning algorithms crammer singer crammer 

svm parser yamada matsumoto ratnaparkhi parser parsers trained maximize accuracy tree 
approach related collins roark taskar 
phrase structure parsing 
collins roark linear parsing model trained averaged perceptron algorithm 
parse features sufficient history parsing algorithm prune heuristically possible parses 
taskar 
formulate parsing problem large margin structured classification setting taskar limited parsing sentences words due computation time 
approaches represent steps discriminatively trained parsers able display benefits discriminative training seen extraction shallow parsing 
simplicity method efficient accurate demonstrate experimentally english czech treebank data 
system description definitions background follows generic sentence denoted possibly subscripted ith word denoted xi 
generic dependency tree denoted dependency tree sentence write indicate directed edge word xi word xj tree xi parent xj 
xt yt denotes training data 
follow edge factorization method eisner define score dependency tree sum score edges tree high dimensional binary feature representation edge xi xj 
example dependency tree feature value xi hit xj ball 
general real valued feature may binary features simplicity 
feature weights weight vector parameters learned training 
training algorithms iterative 
denote weight vector th training iteration 
define dt set possible dependency trees input sentence set dependency trees dt highest scores weight vector ties resolved arbitrary fixed rule 
basic questions answered models form find dependency tree highest score sentence learn appropriate weight vector training data feature representation 
sections address questions 
parsing algorithm feature representation edges weight vector seek dependency tree algorithm eisner needs keep indices stage 
trees maximize score function 
primary difficulty sentence length exponentially possible dependency trees 
slightly modified version lexicalized cky chart parsing algorithm possible generate represent sentences forest size takes time create 
eisner observation head chart item left right periphery possible parse 
idea parse left right dependents word independently combine stage 
removes need additional head indices algorithm requires additional binary variables specify direction item gathering left dependents gathering right dependents item complete available gather dependents 
shows algorithm schematically 
normal cky parsing larger elements created bottom pairs smaller elements 
eisner showed algorithm sufficient searching space dependency parses slight modification finding highest scoring tree sentence edge factorization assumption 
eisner satta give cubic algorithm lexicalized phrase structures 
works limited class languages tree spines regular 
furthermore large grammar constant typically thousands treebank parsers 
online learning gives pseudo code generic online learning setting 
single training instance considered iteration parameters updated applying algorithm specific update rule instance consideration 
algorithm returns averaged weight vector auxiliary weight vector maintained accumulates training data xt yt 



update instance xt yt 


generic online learning algorithm 
values iteration returned weight vector average weight vectors training 
averaging shown help reduce overfitting collins 
mira crammer singer developed natural method large margin multi class classification extended taskar 
structured classification min dt real valued loss tree relative correct tree define loss dependency tree number words incorrect parent 
largest loss dependency tree length sentence 
informally update looks create margin correct dependency tree incorrect dependency tree large loss incorrect tree 
errors tree farther away score score correct tree 
order avoid blow norm weight vector minimize subject constraints enforce desired margin correct incorrect trees constraints may unsatisfiable case relax slack variables svm training 
margin relaxed algorithm mira crammer singer crammer employs optimization directly online framework 
update mira attempts keep norm change parameter vector small possible subject correctly classifying instance consideration margin large loss incorrect classifications 
formalized substituting update line generic online algorithm min xt yt xt yt dt xt standard quadratic programming problem easily solved hildreth algorithm censor zenios 
crammer singer crammer 
provide analysis online generalization error convergence properties mira 
equation calculated respect weight vector optimization apply mira dependency parsing simply see parsing multi class classification problem dependency tree possible classes sentence 
interpretation fails computationally general sentence exponentially possible dependency trees exponentially margin constraints 
circumvent problem assumption constraints matter large margin optimization involving incorrect trees highest scores 
resulting optimization mira see line min xt yt xt yt xt reducing number constraints constant tested various values development data set small values sufficient achieve close best performance justifying assumption 
fact grew began observe slight degradation performance indicating overfitting training data 
experiments 
eisner algorithm modified find best trees adding additional log factor runtime huang chiang 
common approach factor structure output space yield polynomial set local constraints taskar taskar 
factorization dependency trees min yt yt trivial show constraints satisfied 
implemented model required training time larger best formulation typically improve performance 
furthermore best formulation flexible respect loss function assume loss function factored sum terms dependency 
feature set need suitable feature representation dependency 
basic features model outlined table features conjoined direction attachment distance words attached 
features represent system backoff specific features words partof speech tags sparse features just partof speech tags 
features added entire words gram prefix word longer characters 
just features parent child node pairs tree high accuracy attachment decisions outside context words occurred 
solve problem added types features seen table 
features type look words occur child parent 
features take form pos trigram pos parent child word words linearly parent child 
feature particularly helpful nouns identifying parent basic uni gram features word pos word pos word pos word pos basic big ram features word pos word pos pos word pos word word pos word pos pos word pos word word word pos pos pos features pos pos pos surrounding word pos features pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos pos table features system 
word word parent node dependency tree 
word word child node 
pos pos parent node 
pos pos child node 
pos pos right parent sentence 
pos pos left parent 
pos pos right child 
pos pos left child 
pos pos word parent child nodes 
typically rule situations noun attached noun verb uncommon phenomenon 
second type feature provides local context attachment words parent child pair 
feature took form pos gram pos parent child word parent word child 
system back features various trigrams local context pos tags removed 
adding features resulted large improvement performance brought system state art accuracy 
system summary performance see section approach dependency parsing described advantages 
system general contains language specific enhancements 
fact results report english czech identical features obviously trained different data 
online learning algorithms intuitive easy implement 
efficient parsing algorithm eisner allows system search entire space dependency trees parsing thousands sentences minutes crucial discriminative training 
compare speed model standard lexicalized phrase structure parser section show significant improvement parsing times testing data 
major limiting factor system restriction features single dependency attachments 
determining depen dent word useful know previous attachment decisions incorporate features 
fairly straightforward modify parsing algorithm store previous attachments 
modification result asymptotic increase parsing complexity 
experiments tested methods experimentally english penn treebank marcus czech prague dependency treebank haji 
experiments run dual bit amd opteron ghz processor 
create dependency structures penn treebank extraction rules yamada matsumoto approximation lexicalization rules collins 
split data parts sections training section development section evaluation 
currently system features 
instance uses tiny fraction features making sparse vector calculations possible 
system assumes pos tags input uses tagger ratnaparkhi provide tags development evaluation sets 
table shows performance systems compared 
svm parsing model yamada matsumoto memory learner scholz mira system described 
implemented averaged perceptron system collins online learning algorithm comparison 
table compares pure dependency parsers english czech accuracy root complete accuracy root complete avg 
perceptron mira table dependency parsing results english czech 
accuracy number words correctly identified parent tree 
root number trees root word correctly identified 
czech measure sentence may multiple roots 
complete number sentences entire dependency tree correct 
exploit phrase structure 
ensured gold standard dependencies systems compared identical 
table shows model described performs better previous comparable systems including yamada matsumoto 
method potential advantage svm batch training takes account constraints training instances optimization online training considers constraints instance time 
fundamentally limited approximate search algorithm 
contrast system searches entire space dependency trees benefits greatly 
difference amplified looking percentage trees correctly identify root word 
models search entire space suffer bad approximations early search identify correct root approximate algorithms prone error propagation attachment decisions top tree 
comparing online learning models seen mira outperforms averaged perceptron method 
difference statistically significant mcnemar test head selection accuracy 
czech experiments dependency trees annotated prague treebank predefined training development evaluation sections data 
number sentences data set nearly twice english treebank leading large number features 
instance uses just handful features 
pos tags automatically generated tags data set 
language specific model changes need data specific changes 
particular method collins 
simplify part speech tags rich tags czech led large rarely seen set pos features 
model mira performs czech slightly outperforming averaged perceptron 
unfortunately know parsing systems tested data set 
czech parser collins 
run different data set dependency parsers evaluated english 
learning model czech training data somewhat problematic contains crossing dependencies parsed eisner algorithm 
trick rearrange words training set trees nested 
allows training algorithm obtain reasonably low error training set 
improve performance slightly accuracy 
lexicalized phrase structure parsers known dependency trees extracted lexicalized phrase structure parsers collins charniak typically accurate produced pure dependency parsers yamada matsumoto 
compared system bikel re implementation collins parser bikel collins trained head rules system 
ways extract dependencies lexicalized phrase structure 
automatically generated dependencies explicit lexicalization trees call system 
second take just phrase structure output parser run automatic head rules extract dependencies call sys english accuracy root complete complexity time collins auto collins rules mira normal mira collins table results comparing system collins parser 
complexity represents computational complexity parser time cpu time parse sec 
penn treebank 
tem collins rules 
table shows results comparing system mira normal collins parser english 
systems implemented java run machine 
interestingly dependencies automatically produced collins parser worse extracted statically head rules 
arguably displays english dependency parsing dependencies automatically extracted treebank phrase structure trees 
system falls better automatically generated dependency trees worse head rule extracted trees 
dependencies returned system better learnt collins parser argue model learning parse dependencies accurately 
phrase structure parsers built maximize accuracy phrase structure lexicalization just additional source information 
surprising dependencies output collins parser accurate system trained built maximize accuracy dependency trees 
complexity run time system huge improvement collins parser 
final system table takes output collins rules adds feature mira normal indicates edge collins parser believed dependency exists call system mira collins 
known discriminative training trick suggestions generative system influence decisions 
system essentially considered corrector collins parser represents significant improvement 
added complexity model requires output collins parser 
accuracy train time table evaluation best mira approximation 
best mira approximation question asked justifiable best mira approximation 
table indicates accuracy testing time took train models english data set 
parsing algorithm proportional log empirically training times scale linearly peak performance achieved early slight degradation 
reason phenomenon model overfitting ensuring trees separated correct tree proportional loss 
summary described successful new method training dependency parsers 
simple linear parsing models trained margin sensitive online training algorithms achieving state art performance relatively modest training times need pruning heuristics 
evaluated system english czech data display state theart performance language specific enhancements 
furthermore model augmented include features lexicalized phrase structure parsing decisions increase dependency accuracy parsers 
plan extending parser ways 
add labels dependencies represent grammatical roles 
labels important parser output tasks information extraction machine translation 
second looking model extensions allow dependencies occur languages czech german dutch 
acknowledgments jan haji answering queries prague treebank providing yamada matsumoto head rules english allowed direct comparison systems 
supported nsf itr 
bikel 

intricacies collins parsing model 
computational linguistics 
censor zenios 

parallel optimization theory algorithms applications 
oxford university press 
charniak 

maximum entropy inspired parser 
proc 
naacl 
clark curran 

parsing wsj ccg log linear models 
proc 
acl 
collins roark 

incremental parsing perceptron algorithm 
proc 
acl 
collins haji ramshaw tillmann 

statistical parser czech 
proc 
acl 
collins 

head driven statistical models natural language parsing 
ph thesis university pennsylvania 
collins 

discriminative training methods hidden markov models theory experiments perceptron algorithms 
proc 
emnlp 
crammer singer 

algorithmic implementation multiclass kernel vector machines 
jmlr 
crammer singer 

ultraconservative online algorithms multiclass problems 
jmlr 
crammer dekel shalev shwartz singer 

online passive aggressive algorithms 
proc 
nips 
sorensen 

dependency tree kernels relation extraction 
proc 
acl 
ding palmer 

machine translation probabilistic synchronous dependency insertion grammars 
proc 
acl 
eisner satta 

efficient parsing bilexical context free grammars head automaton grammars 
proc 
acl 
eisner 

new probabilistic models dependency parsing exploration 
proc 
coling 
haji 

building syntactically annotated corpus prague dependency treebank 
issues valency meaning 
huang chiang 

better best parsing 
technical report ms cis university pennsylvania 
richard hudson 

word grammar 
blackwell 
joachims 

learning classify text support vector machines 
kluwer 
lafferty mccallum pereira 

conditional random fields probabilistic models segmenting labeling sequence data 
proc 
icml 
marcus santorini marcinkiewicz 

building large annotated corpus english penn treebank 
computational linguistics 
scholz 

deterministic dependency parsing english text 
proc 
coling 
ratnaparkhi 

maximum entropy model part speech tagging 
proc 
emnlp 
ratnaparkhi 

learning parse natural language maximum entropy models 
machine learning 
riezler king kaplan crouch maxwell johnson 

parsing wall street journal lexical functional grammar discriminative estimation techniques 
proc 
acl 
sha pereira 

shallow parsing conditional random fields 
proc 
hlt naacl 
shinyama sekine sudo grishman 

automatic paraphrase acquisition news articles 
proc 
hlt 
taskar guestrin koller 

max margin markov networks 
proc 
nips 
taskar klein collins koller manning 

max margin parsing 
proc 
emnlp 
yamada matsumoto 

statistical dependency analysis support vector machines 
proc 
iwpt 
