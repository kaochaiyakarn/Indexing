neural computation vol pp 
incremental active learning optimal generalization sugiyama ogawa department computer science graduate school information science engineering tokyo institute technology 
ku tokyo japan 
og cs titech ac jp ogawa www cs titech ac jp problem designing input signals optimal generalization called active learning 
give stage sampling scheme reducing bias variance scheme propose active learning methods 
multi point search method applicable arbitrary models 
effectiveness method shown computer simulations 
optimal sampling method trigonometric polynomial models 
method precisely specifies optimal sampling locations 
keywords active learning generalization capability projection learning incremental projection learning trigonometric polynomial space 
incremental active learning optimal generalization supervised learning obtaining underlying rule sampled information 
depending type sampling supervised learning classified different categories 
case information unilaterally environment 
example time series prediction sample points fixed regular intervals learners change interval 
case learners design input signals sample corresponding output signals 
example possible design input signals scientific experiments learning sensorimotor maps robot arms 
learning performed efficiently actively design input signals 
problem designing input signals optimal generalization called active learning cohn ghahramani jordan vijayakumar ogawa 
referred optimal experiments kiefer fedorov cohn query construction 
reinforcement learning kaelbling extensively studied field machine learning regarded form active learning 
mathematical statistics active learning criterion called optimal design thoroughly studied kiefer kiefer fedorov 
optimal design minimizes determinant dispersion matrix estimator 
advantages optimal design invariant affine transformations input space kiefer 
kiefer showed optimal design agrees minimax design noise variance magnitude domain 
minimax design aimed finding sample points xj minimizing maximum noise variance argmin max xj en en andf ensemble average noise learning result learning target function respectively 
order find optimal design sample points learning criterion prescribing mapping training examples learning result determined 
general approach optimal design best linear unbiased estimation adopted learning criterion aimed minimizing mean noise variance domain constraint 
implies criterion optimal design inconsistent criterion best linear unbiased estimation causing crucial problem acquiring optimal generalization capability 
framework bayesian statistics mackay derived criterion selecting informative training data specifying parameters neural networks 
cohn cohn ghahramani jordan gave active learning criterion minimizing variance estimator 
proposed active learning method multi layer perceptrons asymptotic approximation estimating generalization error 
essentially criteria derived papers incremental active learning optimal generalization equivalent optimal design shown fedorov 
generally intractable calculate value criteria difficult find optimal solution 
mackay proposed fixed set points estimating value criterion 
cohn ghahramani jordan recommended monte carlo sampling estimating value criterion cohn gradient method finding local optimum criterion 
introduced parametric family density function generating sampling locations 
approaches active learning criteria aimed minimizing variance estimator 
shown geman bienenstock doursat generalization error consists bias variance 
implies methods assume bias zero small neglected 
yue showed upper bound generalization error derived active learning criterion minimizing upper bound 
criterion includes unknown controlling parameter trade bias variance optimal solution obtained 
cohn resampling methods bootstrapping efron tibshirani cross validation stone estimating bias proposed active learning method reducing bias 
experiments showed bias approach outperforms variance approach 
functional analytic point view vijayakumar ogawa gave necessary sufficient condition sampling locations provide optimal generalization capability absence noise 
condition bias explicitly evaluated utilizing knowledge distribution learning target functions 
vijayakumar sugiyama ogawa extended condition noisy case dividing sampling scheme stages 
stage reducing bias second stage reducing variance small bias attained stage maintained 
propose active learning methods presence noise 
multi point search method applicable arbitrary models 
effectiveness method shown computer simulations 
optimal sampling method trigonometric polynomial models 
method precisely specifies optimal sampling locations 
methods idea stage sampling scheme proposed vijayakumar sugiyama ogawa 
difference priori knowledge distribution target functions required 
organized follows 
section supervised learning problem formulated 
section describes general learning process requirements acquiring optimal generalization capability 
section devoted giving basic sampling strategy 
strategy section gives multi point search method section gives optimal sampling method trigonometric polynomial models 
computer simulations performed section demonstrating effectiveness proposed methods 
incremental active learning optimal generalization formulation supervised learning problem section supervised learning problem formulated functional analytic point view see ogawa 
consider supervised learning problem obtaining optimal approximation target function ofl variables set training examples 
training examples input signals xj subset dimensional euclidean space corresponding output signals yj unitary space xj yj yj xj nj yj degraded zero mean additive noise nj 
letn dimensional vectors th elements nj yj respectively 
called sample value vector space belongs called sample value space 
target function assumed belong reproducing kernel hilbert space bergman saitoh wahba 
unknown estimated model selection methods akaike sugiyama ogawa 
reproducing kernel bivariate function defined satisfies conditions 
fixed function function holds denotes inner product 
note reproducing kernel unique exists 
theory hilbert space arguments developed regarding function point space 
value function point discussed general framework hilbert space 
hilbert space reproducing kernel possible deal value function point 
function defined value sample point xj expressed xj xj 
reason called sampling function 
am operator mapping dimensional vector th element xj xm incremental active learning optimal generalization sample value function space space sampling operator target am function learning result fm learning operator xm xm supervised learning inverse problem 
denotes transpose vector 
call am sampling operator 
note am linear operator concerned non linear function 
am expressed am th vector called standard basis stands neumann product relationship expressed 
fm learning result obtained training examples denote mapping fm xm fm xm called learning operator 
supervised learning problem reformulated inverse problem obtaining xm providing best approximation fm certain learning criterion fig 
fixed hilbert space fixed hilbert space product operator defined 
incremental active learning optimal generalization ii learning process learning criterion determined data gather chosen incremental active learning training examples gathered learning procedure carried incremental learning learning result evaluated unsatisfactory satisfactory general process supervised learning 
section show general process supervised learning describe requirements optimal generalization 
requirements optimal generalization supervised learning generally processed illustrated fig 
learning criterion determined accordance purpose learning 
data gather decided sample values gathered decided locations 
gathered training examples ii learning procedure carried obtained learning result evaluated 
learning result satisfactory learning process completed 
training examples added improve learning result satisfactory 
training examples sampled added process 
practical situations number training examples finite 
acquiring optimal generalization capability learning process requirements met non asymptotic sense 
incremental active learning optimal generalization criterion active learning consistent purpose learning 
active learning method precisely specifies optimal sample points 
incremental learning method provides exactly generalization capability obtained batch learning training examples 
strictly speaking optimal requirement meanings 
globally optimal set training examples optimal sugiyama ogawa 
greedy optimal training example sample optimal step mackay cohn :10.1.1.33.830
focus greedy case devise incremental active learning method meeting requirements 
rest section review projection learning method incremental projection learning meets requirement 
projection learning mentioned section function approximation performed basis learning criterion 
purpose learning minimize generalization error learning result fm measured proposition holds 
proposition holds jg en fm 
jg en fm 
second terms eq called bias variance fm respectively 
restrict discussion case learning operator xm eq linear 
follows eqs learning result fm decomposed fm xmn 
case follows eq mean fm noise belongs denotes range operator 
ps orthogonal projection operator subspace order minimize bias fm agree orthogonal projection pr 
albert operator equation ps incremental active learning optimal generalization solution denotes adjoint operator am 
bigger provides better approximation adopt largest 
reason called approximation space 
order reduce generalization error variance fm minimized 
learning method called projection learning definition projection learning ogawa operator xm called projection learning operator xm minimizes functional constraint jp xm en xmn pr 
moore penrose generalized inverse am 
proposition holds 
proposition ogawa general form projection learning operator expressed xm ma mu ym im ym arbitrary operator qm en um ama qm vm mu mam 
substituting eqs eq jg pr en xmn 
eq implies projection learning reduces bias fm certain level minimizes variance fm 
various methods calculating projection learning operator xm projection learning result fm matrix operation 
show simplest methods valid finite dimensional hilbert spaces operator called moore penrose generalized inverse operator satisfies conditions albert ben israel 
ax ax xa xa 
note moore penrose generalized inverse unique denoted incremental active learning optimal generalization dimension denoted finite functions expressed form ak coefficients 
consider dimensional parameter space functions expressed 
regard parameter space sampling function expressed xj xj xj denotes complex conjugate transpose 
sampling operator am matrix element am jk xj 
am called design matrix efron tibshirani 
projection learning operator obtained eq matrix projection learning result fm obtained eq dimensional vector fm 
learning result function fm fm bk 
orthonormal basis ak practice calculation moore penrose generalized inverse unstable 
overcome recommend tikhonov regularization tikhonov arsenin ama small constant say shown learning results obtained projection learning invariant inner product sample value space yamashita ogawa 
loss generality euclidean inner product adopted sample value space 
noise covariance matrix qm form qm im projection learning operator ogawa xm 
implies projection learning agrees usual mean squares learning aimed minimizing training error fm xj yj 
incremental active learning optimal generalization incremental projection learning consider case new training example xm ym added learning result fm obtained xj yj 
follows eq learning result fm obtained xj yj batch manner expressed fm xm 
order devise exact incremental learning method meeting requirement shown section calculate fm eq fm xm ym 
noise characteristics xm ym qm en nm en nm nm denotes complex conjugate nm 
dimensional vector scalar 
am notation defined 
vectors sm am qm tm msm 
scalars xm tm sm ym fm xm tm 
functions pn am mam mtm 
eq xm eq agrees shown sugiyama ogawa additional training examples rejected effect learning results 
focus training examples 
exact incremental learning method called incremental projection learning ipl follows 
proposition incremental projection learning sugiyama ogawa defined eq zero posterior projection learning result fm obtained prior results fm am andy follows 
fm fm 
xm incremental active learning optimal generalization fm fm 
note fm obtained proposition exactly agrees learning result ob tained batch projection learning eq xj yj xm eq equivalent see eqs 
condition linearly independent approximation space wider 
contrast linearly dependent approximation space 
consider case noise covariance matrix qm positive definite diagonal qm diag ym fm xm am 
vanishes 
vm replaced ipl reduced simpler form 
proposition sugiyama ogawa qm eq posterior projection learning result fm obtained prior results fm follows 
fm fm xm 
fm fm 
compared proposition replaced required proposition 
fm obtained proposition exactly agrees learning result obtained batch projection learning xj yj qm im operator said positive definite tf 
incremental active learning optimal generalization eq yields fm fm mam mam 
eqs imply value required ipl qm form eq 
basic sampling strategy section devoted giving sampling strategy basis devising active learning methods sections 
jb jv changes bias variance fm addition training example xm ym respectively proposition holds 
jb pr pr jv en xm en xmn 
proposition sugiyama ogawa additional training example xm ym relations hold 
jb 
jb 
proposition states additional training example reduces maintains bias increases maintains variance 
contrast additional training example maintains bias reduces maintains variance 
consider case dimension hilbert space finite total number training examples sample larger equal dimension case follows eq bias learning results zero am 
reason comply stage sampling scheme shown fig 
start 
stage training examples added reduce bias reaches zero 
dimension stage ends training example added times attained 
stage training examples added reduce variance number added training examples note incremental active learning optimal generalization start find sample point xm minimizing jv constraint am find sample point xm minimizing jv stage sampling scheme 
stage stage incremental active learning optimal generalization bias fm variance interpretation assumptions statistical active learning methods method 
function best approximation statistical active learning methods assume 
belongs mean fm noise agrees contrast method assumes difference explicitly evaluated stage 
additional training examples maintain bias see proposition 
bias remains zero stage 
mentioned section criterion active learning consistent purpose learning active learning criterion aimed minimizing generalization error 
active learning problems stages follows 
stage find sample point minimizing jv constraint 
stage find sample point minimizing jv constraint 
note additional training examples stage satisfy 
means stage constraint taken account 
condition stage easily verified practice recommend criterion 
pn am 
small constant say statistical active learning methods devised far bias estimator assumed zero mackay cohn :10.1.1.33.830
interpretation assumption illustrated fig 
function best approximation incremental active learning optimal generalization assumption zero bias equivalent 
belongs mean fm noise agrees contrast condition assumed framework difference explicitly evaluated stage 
stage sampling scheme shown fig propose active learning methods sections 
multi point search active learning section propose active learning method multi point search 
derivation multi point search method theorem plays central role 
theorem jv defined eq expressed follows 
jv xm jv 

proofs theorems appendix theorem implies jv calculated ym 
evaluate quality additional training examples sampling locations 
noted eq conceptually similar criteria shown fedorov mackay cohn 
difference noise assumed methods correlated noise treated method noise covariance matrix available 
noise uncorrelated theorem reduced simpler form 
theorem qm eq expressed follows 
jv xm jv 

incremental active learning optimal generalization compared theorem required theorem 
theorem follows 
qm im corollary qm eq jv expressed follows 
jv mam 
xm jv mam mam 
corollary implies qm eq value required minimization jv 
theorems corollary algorithm multi point search active learning method described fig 
strictly algorithm shown fig meet requirement mentioned section 
experimentally shown computer simulations section multi point search method specifies better sampling location 
dimension input large candidates may required finding better sampling location 
measures employ gradient method finding local maximum initial value xm updated certain convergence criterion holds xm xm jv xm small positive constant jv xm gradient jv xm 
optimal sampling trigonometric polynomial space previous section gave active learning method general finite dimensional hilbert spaces 
section focus trigonometric polynomial space devise effective active learning method 
method strictly meets requirement 
model trigonometric polynomial space defined follows 
incremental active learning optimal generalization am generate locations argmin jv sample ym carry ipl ym generate locations argmin jv sample ym carry ipl ym candidates candidates algorithm multi point search active learning method 
definition trigonometric polynomial space non negative integer dl 
function space called trigonometric polynomial space order nl spanned exp inl nl nl nl defined dl inner product defined 
note function space spanned exp equal function space spanned cos sin dimension trigonometric polynomial space order nl nl 
reproducing kernel trigonometric polynomial space order nl expressed kl incremental active learning optimal generalization profile reproducing kernel trigonometric polynomial space order kl sin nl sin nl profile eq illustrated fig 
theorem 
theorem suppose noise covariance matrix qm 
nl qm im arbitrary constant denotes maximum integer equal xm nl ql ql mod ifl mod nl nr incremental active learning optimal generalization interval interval sample point optimal sample points trigonometric polynomial space order 
number training examples 
xm minimizes jv constraint xj eq 
successively determined theorem states locations fixed regular intervals 
fig illustrates set sample points determined theorem 
base point fixed dark region sample points fixed regular intervals 
eq set sampling functions forms orthonormal basis sampling scheme derived greedy optimal scheme scheme fact globally optimal time mod see sugiyama ogawa 
computer simulations section effectiveness proposed active learning methods demonstrated computer simulations 
incremental active learning optimal generalization illustrative simulation consider learning trigonometric polynomial space order 
target function sin cos sin cos 
noise covariance matrix qm im 
shall compare performance sampling schemes 
optimal sampling training examples sampled theorem multi point search training examples sampled multi method shown fig 
number candidates randomly generated domain 
experiment design eq cohn adopted active learning criterion 
value criterion evaluated monte carlo sampling points 
sampling location determined multi candidates 
locations randomly created domain minimizing criterion selected 
passive learning training examples randomly supplied domain 
learning results obtained sampling schemes training examples shown fig 
solid dashed lines show target function learning results respectively 
denotes training example 
fig show learning results obtained sampling schemes generalization errors measured eq respectively 
results show sampling schemes give percent reductions generalization error respectively compared sampling scheme 
generalization capability acquired sampling schemes close obtained sampling scheme 
means sampling schemes works quite small number candidates sampling schemes gives optimal generalization capability see section 
changes variance addition training examples shown fig 
horizontal axis denotes number training examples vertical axis denotes variance measured eq 
solid line shows sampling scheme 
dashed dash dotted dotted lines denote means trials sampling schemes respectively 
sampling schemes hold dimension see section 
sampling schemes attained trials simulation 
incremental active learning optimal generalization jg jg jg jg results learning simulation trigonometric polynomial space order noise covariance matrix 
solid line shows target function 
dashed lines show learning results obtained optimal sampling method theorem multi point search method fig experimental design method passive learning respectively 
denotes training example 
generalization error jg measured eq 
incremental active learning optimal generalization variance optimal sampling multi point search experiment design passive learning number training examples relation number training examples noise variance trigonometric polynomial space order noise covariance matrix qm im 
horizontal axis denotes number training examples vertical axis denotes noise variance measured eq 
vertical axis regarded generalization error number training examples larger equal 
follows eq vertical axis fig regarded generalization error 
graph shows variance sampling schemes increases phenomenon agreement proposition 
case sampling schemes give lower variance sampling schemes 
noted sampling scheme gives higher variance sampling scheme passive learning 
may caused fact bias zero criterion optimal experiment design longer valid 
variances sampling schemes decrease shown proposition 
sampling schemes suppress variance efficiently sampling scheme 
simulation shows sampling schemes outperform sampling scheme especially number training examples small sampling scheme 
multi point search method small number candidates shown 
learning sensorimotor map joint robot arm subsection apply multi point search active learning method proposed section real world problem 
consider learning sensorimotor maps joint robot arm shown fig 
sensorimotor map th joint incremental active learning optimal generalization joint joint joint robot arm 
mapping joint angle angular velocity angular acceleration torque applied th joint 
function spaces hk belong follows vijayakumar 
cos cos sin sin sin sin cos sin cos cos sin sin sin cos sin cos spanned inner product hk defined 
shall perform learning simulation sensorimotor map eqs incremental active learning optimal generalization variance multi point search passive learning number training examples results learning sensorimotor map horizontal axis denotes number training examples vertical axis denotes noise variance measured eq 
solid dotted lines show means trials multi point search method passive learning respectively 
vertical axis regarded generalization error number training examples larger equal 
orthonormal basis follows 
cos sin sin sin cos sin cos 
reproducing kernel cos sin 
suppose training examples degraded additive noise 
consider sampling schemes 
multi point search training examples sampled multi method shown fig 
number candidates randomly generated domain 
passive learning training examples randomly supplied domain 
incremental active learning optimal generalization fig shows simulation results 
horizontal axis denotes number training examples vertical axis denotes variance measured eq 
solid dashed lines show mean variances trials sampling schemes respectively 
sampling scheme holds dimension equal 
sampling scheme attained trials simulation 
follows eq vertical axis fig regarded generalization error 
graph shows multi point search method candidates provides better generalization capability passive learning 
performance multi point search method simulation excellent previous 
reason may number candidates small contrast size domain 
gave basic sampling strategy called stage sampling scheme reducing bias variance scheme proposed active learning methods 
multi point search method applicable arbitrary models optimal sampling method trigonometric polynomial space 
effectiveness proposed methods demonstrated computer simulations 
usual active learning methods devised far methods assume model learning target belongs available 
model unknown estimated model selection methods 
evaluate robustness methods combined model selection important 
appendix proof theorem tr stand trace operator 
follows eqs en xmn xm tr 
tr tr tr mx tr ma tr pr tr tr pr 
incremental active learning optimal generalization eqs yield jv tr tr pr tr tr pr 
shall prove case 
follows eq tr pr tr pr 
sugiyama ogawa expressed xm holds ogawa eqs yield xm 
pn am 
follows eqs tr tr tr xm 
xm substituting eqs eq eq 
shall prove case 
follows eq sugiyama ogawa holds 
substituting eqs eq eq 
incremental active learning optimal generalization proof theorem qm eq holds ogawa projection learning operator expressed eqs yield xm mq 
en xmn tr mq tr tr 
follows eqs jv tr tr 
shall prove case 
follows sugiyama ogawa xm xm holds eq 
pn am 
follows eqs tr tr tr xm xm 
substituting eq eq eq 
shall prove case 
follows sugiyama ogawa eqs yield eq 

incremental active learning optimal generalization proof theorem follows eq jv 
jv minimized 
focus case 
suppose sample points xj successively determined eq 
fixed integer consider sample points xj xj 
mod andt mod 
follows eqs xj xj xj tt tt denotes kronecker delta 
shall prove case 
follows eqs yields mam pr mam pr 
fact follows eqs equality holds jv eq 
eq implies eq jv minimized 
incremental active learning optimal generalization shall prove case 

follows eqs mam ih ih denote identity operator orthogonal projection operator respectively 
holds mam ih 
follows eqs function jv 
eq replaced follows eq 
focus domain 
derivative respect dg dt continuous minimized 
implies jv minimized attained eq 
akaike 

new look statistical model identification 
ieee transactions automatic control ac 
albert 

regression moore penrose pseudoinverse 
new york london academic press 
incremental active learning optimal generalization 

theory reproducing kernels 
transactions american mathematical society 
ben israel 

generalized inverses theory applications 
newyork sons 
bergman 

kernel function conformal mapping 
providence rhode island american mathematical society 
cohn 

neural network exploration optimal experiment design 
cowan tesauro alspector eds advances neural information processing systems pp 

morgan kaufmann 
cohn 

neural network exploration optimal experiment design 
neural networks 
cohn 

minimizing statistical bias queries 
mozer jordan petsche eds advances neural information processing systems pp 

mit press 
cohn ghahramani jordan 

active learning statistical models 
journal artificial intelligence research 
efron tibshirani 

bootstrap 
new york chapman hall 
fedorov 

theory optimal experiments 
new york academic press 


active learning multilayer perceptrons 
touretzky mozer hasselmo eds advances neural information processing systems pp 

cambridge mit press 
geman bienenstock doursat 

neural networks bias variance dilemma 
neural computation 
kaelbling 
ed 

special issue reinforcement learning 
machine learning 
kiefer 

optimal experimental designs 
journal royal statistics society series 
kiefer 

equivalence extremum problems 
annals mathematical statistics 
mackay 

information objective functions active data selection 
neural computation 
incremental active learning optimal generalization ogawa 

projection filter regularization ill conditioned problem 
proceedings spie inverse problems optics 
ogawa 

inverse problem neural networks 
proceedings ieice nd workshop circuits systems pp 

japan 
japanese ogawa 

neural network learning generalization learning 
proceedings international conference intelligent information processing system pp 

beijing china 
saitoh 

theory reproducing kernels applications 
research notes mathematics series 
uk longman scientific technical 
saitoh 

integral transform reproducing kernels applications 
research notes mathematics series 
uk longman 


norm ideals completely continuous operators 
berlin springer verlag 


query construction entropy generalization neural network models 
physical review 
stone 

cross choice assessment statistical predictions 
journal royal statistical society series 
sugiyama ogawa 

exact incremental projection learning presence noise 
proceedings th scandinavian conference image analysis pp 

greenland 
sugiyama ogawa 

incremental projection learning optimal generalization 
technical report tr department computer science tokyo institute technology japan 
available www cs titech ac jp tr tr html sugiyama ogawa 

properties incremental projection learning 
technical report tr department computer science tokyo institute technology japan 
available www cs titech ac jp tr tr html sugiyama ogawa 

functional analytic approach model selection subspace information criterion 
proceedings workshop information induction sciences ibis pp 

japan 
complete version available www cs titech ac jp tr tr html subspace information criterion model selection 
technical report tr department computer science tokyo institute technology japan 
incremental active learning optimal generalization sugiyama ogawa 

training data selection optimal generalization trigonometric polynomial networks 
solla leen 
ller eds advances neural information processing systems pp 

mit press 


modern mathematical statistics 
tokyo 
japanese tikhonov arsenin 

solutions ill posed problems 
washington dc winston 
vijayakumar 

computational theory incremental active learning optimal generalization 
ph thesis department computer science tokyo institute technology japan 
vijayakumar ogawa 

improving generalization ability active learning 
ieice transactions information systems 
vijayakumar sugiyama ogawa 

training data selection optimal generalization noise variance reduction neural networks 
eds neural nets pp 

springer verlag 
wahba 

spline model observational data 
philadelphia pennsylvania society industrial applied mathematics 
yamashita ogawa 

optimum image restoration topological invariance 
transactions ieice ii ii 
japanese yue 

robust designs fitting linear models 
statistica sinica 
