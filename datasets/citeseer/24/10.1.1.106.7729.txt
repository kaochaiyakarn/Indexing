ieee transactions information theory vol 
february analysis sum product decoding low density parity check codes gaussian approximation sae young chung member ieee thomas richardson urbanke density evolution algorithm computing capacity low density parity check ldpc codes messagepassing decoding 
memoryless binary input continuous output additive white gaussian noise awgn channels sum product decoders gaussian approximation message densities density evolution simplify analysis decoding algorithm 
convert infinite dimensional problem iteratively calculating message densities needed find exact threshold dimensional problem updating means gaussian densities 
simplification allows calculate threshold quickly understand behavior decoder better easier design irregular ldpc codes awgn channels 
various regular ldpc codes examined thresholds estimated db exact value 
rates codes designed gaussian approximation perform db best performing codes far density evolution maximum variable degree ih 
show gaussian approximation visualize sum product decoding algorithm 
show optimization degree distributions understood done graphically visualization 
index terms density evolution fixed points gaussian approximation low density parity check ldpc codes stability algorithm threshold 
channels iterative decoders interest parity check ldpc codes discovered gallager rediscovered spielman mackay exhibit threshold phenomenon block length tends infinity arbitrarily small bit error probability achieved noise level smaller certain threshold 
noise level threshold hand probability bit error larger positive constant 
gallager observed phenomenon binary symmetrical channels regular ldpc manuscript received january revised july 
supported part lucent technologies 
material part international symposium information theory sorrento italy june 

chung laboratory information decision systems massachusetts institute technology cambridge ma usa 
ma usa mail sae young chung com 
richardson bell labs lucent technologies murray hill nj usa 
technologies nj usa mail richardson com 
urbanke bell labs lucent technologies murray hill nj usa 
communications theory lab epfl dsc ch lausanne switzerland mail urbanke epfl ch 
communicated frey guest editor 
publisher item identifier 
ieee codes explicit construction regular graphs 
luby generalized idea randomly constructed irregular ldpc codes showed irregular codes perform better regular ones showed threshold phenomenon occurs codes 
observation generalized richardson urbanke large range binary input channels including binary erasure binary symmetric laplace awgn channels various decoding algorithms including belief propagation sum product algorithm collectively called message passing algorithms 
richardson proved general concentration result showing decoder performance random graphs converges expected value length code increases generalizing result luby 
difficult determine expected performance ensemble finite size expected behavior limit infinitely long codes determined corresponding cycle free graph 
defined threshold indicated random ensemble irregular codes specified degree distributions developed algorithm called density evolution iteratively calculating message densities enabling determination thresholds 
result constructed ldpc codes clearly beat powerful turbo codes awgn channels 
improved threshold rate ldpc code awgn channel db shannon limit simulation results db shannon limit bit error rate block length calculating thresholds optimizing degree distributions density evolution computationally intensive tasks difficult channels binary erasure channels 
density evolution dimensional possible analysis construct capacity achieving codes 
interesting channels including awgn channels density evolution complicated analyzed 
simple method estimate threshold irregular ldpc codes memoryless binary input continuous output awgn channels sum product decoding 
method approximating message densities gaussians regular ldpc codes gaussian mixtures irregular ldpc codes 
show sacrifice accuracy dimensional quantity mean gaussian density act faithful surrogate message density infinite dimensional vector 
ieee transactions information theory vol 
february method easier analyze computationally faster density evolution useful tool understanding behavior decoder optimizing irregular codes 
example show determine rate convergence error probability alternation fast slow decoding stages noted 
method find irregular codes linear programming 
algorithm optimizes degree distributions orders magnitude faster optimization methods full density evolution 
turbo codes dimensional gaussian approximation extrinsic information transfer chart exit chart brink considered parallel concatenated codes 
exit chart methods designed rate serial concatenated code repetition code state rate code db shannon limit 
similar gaussian approximation method signal noise ratio snr turbo codes appeared time 
see issue 
similar approximation method snr 
simple formula updating message densities turbo decoding monte carlo simulations analyze approximate trajectories gaussian messages turbo decoding papers 
similar mean dimensional quantities mutual information snr approximate message densities 
showed gaussian approximations reasonably turbo decoding argued position turbo waterfall region estimated visualizing trajectories 
related works sum product algorithm analyzed graphs cycles messages jointly gaussian 
gaussian density completely characterized mean vector covariance matrix analysis sum product algorithm tractable 
main purpose works analyze decoding works graphs cycles 
surprisingly means converge correct posterior means certain conditions satisfied cycles graph 
consider regular binary ldpc code denotes number neighbors variable node denotes number neighbors check node 
sum product algorithm message passing general variable nodes check nodes exchange messages iteratively 
check node gets messages neighbors processes messages sends resulting messages back neighbors 
similarly variable node receives messages neighbors processes messages sends messages back neighbors 
output message variable check node function incoming messages node incoming message edge output message sent 
restriction essential sum product algorithm produce correct marginal posteriori probabilities cycle free graphs 
step procedure repeated times 
iterations variable node decodes associated bit information obtained depth subgraph neigh 
local tree assumption girth graph large subgraph forms tree repeated nodes subgraph analyze decoding algorithm straightforwardly incoming messages node independent 
furthermore general concentration theorem generalizes results assured randomly constructed codes inputs decoder performance close decoder performance local tree assumption high probability block length code long 
base analysis local tree assumption 
convenient log likelihood ratios messages output message variable node bit value node denotes information available node iteration obtained edges carrying likewise define output message check node bit value variable node gets message check node denotes information available check node iteration obtained edges carrying message variable node check node 
sum product decoding equal sum incoming incoming neighbors variable node check node gets message observed llr output bit associated variable node 
density sum random variables easily calculated convolution densities efficiently done fourier domain 
message update rule check nodes obtained duality variable check nodes resulting fourier transform relationship 
get tanh rule incoming neighbors check node message sent remaining neighbor 
figs 
show message flows variable node check node respectively normal realizations variable check nodes chung analysis sum product decoding low density parity check codes fig 

message flow variable node 
fig 

message flow check node 
come repetition parity check respectively 
representation half edges represent variables full edges represent states 
sum product decoding constraints computation nodes states communication links constraint nodes 
fig 
shows decoding check node done dual graph repetition code fourier transforms special case general idea dualizing sum product algorithm dual graph 
apply fourier transform technique need take logarithms side convert product sum signs respectively 
define sign 
sum sign performed sum magnitude ordinary sum case density evolution step done numerically fourier domain 
generalized irregular codes variable number degrees allowed 
generating functions degree distributions variable check nodes respectively fractions edges belonging degree variable check nodes respectively 
expression nominal rate code 
detailed density evolution irregular codes refer readers 
stage computation called density evolution key step calculating thresholds message passing decoding 
loss generality assume codeword sent 
fix channel parameter noise power run algorithm iteratively density tends point mass infinity equivalently probability error tends zero converges density finite probability error probability negative 
threshold defined maximum noise level probability error tends zero number iterations tends infinity 
ii 
gaussian approximation regular ldpc codes llr message channel gaussian mean variance variance channel noise 
independent identically distributed gaussian resulting sum gaussian sum independent gaussian random variables 
inputs gaussian central limit theorem sum look gaussian independent random variables added noted 
normalize output zero mean unit variance variable finite variance normalized output converges gaussian distribution tends infinity central limit theorem 
due interaction variable check nodes easy rigorously show close distributions gaussian actual density evolution 
simulations wiberg observed message distributions awgn channels resemble gaussians regular code 
empirical results density evolution show regular graph output output approximated gaussian densities tends gaussian especially mean near zero 
despite difference approximation point mass contributes half probability mass probability error 
limit infinitely long codes 
ieee transactions information theory vol 
february method develop section works regular ldpc codes 
point regular ldpc codes assume variables gaussian 
gaussian completely specified mean variance need keep means variances iterations 
important condition called symmetry condition preserved density evolution messages expressed density llr message 
enforcing condition approximate gaussian densities iteration greatly improve accuracy approximation 
gaussian mean variance condition reduces means need keep mean 
snr gaussian mean approximate density evolution turbo codes 
define snr gaussian mean variance assume symmetry condition approximate messages 
snr equivalent mean 
simulations due lack tools analyzing density evolution turbo codes authors unable calculate thresholds turbo codes precision digits 
concentrating regular irregular ldpc codes find analytic expressions approximate density evolution calculate approximate threshold values arbitrary precision 
empirical results show limited digits due precision double precision numbers 
mean intuitive physically motivated especially irregular codes 
needs steps show different snrs mixed irregular codes done naturally mean show section 
denote means respectively 
simply mean denotes th iteration 
omit index mean note initial message check node updated mean th iteration calculated expectations side omitted index simplified product complete iteration begins variable nodes ends check nodes 
variables gaussian respectively gaussian density mean variance note expectation depends mean gaussian mean variance define function useful convenient analyses 
definition easy check continuous monotonically decreasing obtain update rule initial value lemma gives upper lower bounds lemma obtain proof expanding follows function 
get similarly get chung analysis sum product decoding low density parity check codes fig 

approximate exact thresholds regular ldpc codes 
bounds tight analyzing behavior decoder optimizing degree distributions sections 
possible calculate thresholds optimize degree distributions exact values note approximations speed calculations sacrifice accuracy 
small say approximation accurate better range values optimized minimize maximum absolute error 
note curves fit better purposes 
large say average upper lower bounds approximation table shows approximate thresholds calculated compared exact thresholds density evolution 
error db compared exact values 
fig 
shows differences approximate exact thresholds various regular ldpc codes snr defined distance shannon limit decibels 
note accuracy codes improved large explained inputs added variable node output gaussian 
algorithms developed calculate exact thresholds table table approximate exact threshold values various regular ldpc codes binary input awgn channel sum product decoding iii 
gaussian approximation irregular ldpc codes preceding analysis easily extended irregular ldpc codes 
consider ensemble random codes degree distributions node get messages neighbors messages random mixture different densities neighbors different degrees 
denote mixtures variable nodes check nodes respectively 
assume individual output variable check node gaussian section empirical evidence 
mean output message degree variable node th iteration ieee transactions information theory vol 
february mean mean st iteration gaussian mixture general concentrated degree 
variance output density section 
th iteration incoming message check node gaussian mixture density mean gaussian output node 
get check node calculate mean output 
mean gaussian output message check node degree th iteration probability gaussian mixture 
output message variable node characterized mean sum means incoming densities need keep mean gaussian mixture check nodes variable nodes 
linearly combining means degree check nodes weights weget define rewrite initial value note start 
iteration definition threshold infimum converges threshold terms noise power equal monotonically decreasing conclude monotonically increasing finite induction conclude converge lemma converge iff proof assume converge finite assume continuous exists choose smallest conclude alternative expression define equivalent initial value easy see iff satisfied iff convergence condition useful optimizing 
linear take example demonstrate message densities approximated gaussian mixtures 
rate code designed density evolution threshold 
threshold calculated gaussian approximation 
fig 
shows message densities variable check nodes density evolution gaussian approximation code 
noise power set db threshold case 
surprisingly close 
fig 
shows corresponding densities check variable nodes 
case exact density spike modeled gaussian approximation 
densities look different fact densities direction close suggests approximation working 
fact exact density fig 
chung analysis sum product decoding low density parity check codes fig 

approximate exact densities check node input 
fig 

approximate exact densities variable node input 
different gaussian output variable node gaussian inputs added variable node 
furthermore probability error small exact density fig 
gaussian observed 
note mismatch exact approximate densities fig 
exists regular cases shape exact density fig 
due nature check node computations rule irregular 
iv 
stability section find condition convergence tends infinity large ieee transactions information theory vol 
february known stability condition 
derive rate convergence probability error result 
prove lemma gives behavior large equivalently small probability error 
lemma nonzero proof large simplified second nonzero respectively 
define get sides get lemma 
lemma get get result 
converges infinity regardless tends infinity large 
constant large 
case get theorem summarizes results 
theorem converge infinity tends infinity large 
converge infinity regardless initial value 
note condition similar stability condition replaced jensen inequality conclude equality iff approximate stability condition looser original implies stable degree distributions gaussian approximation may stable density evolution 
stability density evolution guarantees stability gaussian approximation 
case close concentrated consecutive degrees case degree distributions far 
large express constant 
probability error th iteration fraction degree variable nodes 
large get approximation probability error 
lemma large approximated positive constants depend degree distributions 
proof large nonzero term dominant 
constants depend degree distributions 
approximately positive constant depends degree distributions approximation function result follows 
implies small error probability decrease factor iteration matches empirical observations density evolution 
chung analysis sum product decoding low density parity check codes fig 

ph top bottom 
optimization degree distributions assuming optimize maximizing rate code 
constraints normalization inequality constraint 
linear program solved efficiently 
alternatively maximize ignoring term 
shown theorem optimized way form theorem concentrated degree distribution minimizes subject constant 
proof dual linear program problem max subject suppose choose integer claim feasible solutions primal dual problems equal cost optimal solutions primal dual problems weak duality theorem check constraints dual problem satisfied observe conclude finite induction 
note solution maximizes term subject rate constraint 
experiments show concentrated form different optimized degree distributions linear programming noticeable performance degradation cases examined far 
concentrated optimization done similarly optimization assuming optimize maximizing rate constraints normalization inequality constraint 
done linear programming 
fig 
shows degree distribution noise level equal gaussian approximation threshold code 
case noise power optimization may performed maximizing rate ieee transactions information theory vol 
february fig 

function magnified 
code subject normalization constraint inequality constraints figs 
show curve combined produce curve degree distribution 
curve barely touches point 
hits point curve hits fixed point error probability decreased 
note slope curve near different stability condition theorem described shown concentrated may want visualize optimization done 
fig 
shows noise level equal gaussian approximation threshold code 
case noise power optimization may performed maximizing rate code subject normalization constraint inequality constraints figs 
show curves corresponding combined produce curve 
curve barely touches point 
hits point curve hits fixed point error probability decreased 
figs 
show thresholds snr codes rates optimized density evolution gaussian approximation maximum variable degree respectively 
show difference exact approximate thresholds small db codes designed gaussian approximation 
shows optimization gaussian approximation especially low rate high 
example rates higher difference db db vi 
fixed points seen gaussian approximation useful calculating thresholds optimizing degree distributions 
clear accurate approximation just looking single number threshold 
section show accurately gaussian approximation predict locations potential fixed points density evolution show aspects approximation thresholds 
observe figs 
slow fast phases width gap curve horizontal axis determines iteration speed 
explains slow fast phases sum product decoding noted 
fig 
shows decrease probability error function probability error code density evolution gaussian approximation noise power set db threshold case 
interesting observe estimated chung analysis sum product decoding low density parity check codes fig 

ih top bottom 
fig 

function magnified 
fixed points density evolution including zero probability error gaussian approximation 
increase noise power probability error fixed point gaussian approximation density evolution 
noise power increased fixed point density evolution 
fig 
shows decrease probability error function probability error code rate density evolution gaussian approximation noise power set db threshold case 
case number potential fixed points estimated correctly gaussian approximation curves ieee transactions information theory vol 
february fig 

performance various codes designed evaluated ga ga ga de de ga de de ga denotes gaussian approximation de denotes density evolution 
fig 

performance various codes designed evaluated ga ga ga de de ga de de 
quite close 
case gaussian approximation calculates thresholds accurately tracks exact probability error reasonably 
note empirical observations maximum variable degree increases number fixed points tends increase 
gaussian approximation es number fixed points exactly maximum variable degree large useful case 
maximum variable degree small gaussian approximation works reasonably 
contradict previous observations regular codes accuracy gaussian approximation codes improves chung analysis sum product decoding low density parity check codes fig 

probability error versus decrease probability error density evolution gaussian approximation 
fig 

probability error versus decrease probability error density evolution gaussian approximation 
increases 
suggest maximum variable degree code increases gaussian approximation accurate irregularity code increases fraction high degree variable nodes increases 
vii 
ldpc codes sum product decoding message distributions approximated gaussians gaussian mixtures memoryless binary input continuous output ieee transactions information theory vol 
february awgn channels 
gaussian approximation simplified density evolution dimensional problem simple analyzable expressions describing approximate density evolution 
huge reduction dimension problem sacrifice accuracy find thresholds faster optimize degree distributions faster easier 
enables analyze behavior density evolution especially near zero probability error possible find approximate rate convergence probability error near zero 
gaussian approximation tool visualize mean densities updated density evolution explains fast slow phases density evolution 
helps get hints optimize degree distributions 
graphically demonstrated optimization degree distributions visualized 
online demonstration optimization degree distributions gaussian approximation results available truth mit edu 
acknowledgment authors express appreciation prof forney jr helpful comments 
anonymous reviewers helpful comments suggestions 
gallager low density parity check codes ire trans 
inform 
theory vol 
pp 
jan 
gallager low density parity check codes 
cambridge ma mit press 
sipser spielman expander codes ieee trans 
inform 
theory vol 
pp 
nov 
mackay neal near shannon limit performance low density parity check codes electron 
lett vol 
pp 
aug 
mackay error correcting codes sparse matrices ieee trans 
inform 
theory vol 
pp 
mar 
luby mitzenmacher shokrollahi spielman analysis low density codes improved designs irregular graphs proc 
th annu 
acm symp 
theory computing pp 

richardson urbanke capacity low density codes message passing decoding ieee trans 
inform 
theory vol 
pp 
feb 
berrou glavieux near shannon limit error correcting coding decoding turbo codes proc 
ieee int 
conf 
commun geneva switzerland may pp 


chung forney jr richardson urbanke design low density parity check codes db shannon limit ieee commun 
lett published 

chung construction capacity approaching coding schemes ph dissertation mass inst 
technol cambridge ma 
available truth mit edu 
luby mitzenmacher shokrollahi spielman practical loss resilient codes proc 
th annu 
acm symp 
theory computing pp 


chung urbanke richardson gaussian approximation sum product decoding low density parity check codes proc 
int 
symp 
information theory sorrento italy june 
richardson shokrollahi urbanke design capacity approaching low density parity check codes ieee trans 
inform 
theory vol 
pp 
feb 
brink iterative decoding trajectories parallel concatenated codes rd ieee itg conf 
source channel coding munich germany jan 
rate half code approaching shannon limit db electron 
lett vol 
pp 
july 
el gamal jr analyzing turbo decoder gaussian approximation proc 
int 
symp 
information theory sorrento italy june 
analyzing turbo decoder gaussian approximation ieee trans 
inform 
theory vol 
pp 
feb 
low complexity turbo codes proc 
int 
symp 
turbo codes france 
frey turbo factor analysis proc 
adv 
inform 
processing syst vol 
dec submitted publication 
van roy analysis turbo decoding gaussian priors proc 
adv 
neural inform 
processing syst vol 
dec 
weiss freeman correctness belief propagation gaussian graphical models arbitrary topology proc 
adv 
neural inform 
processing syst vol 
dec 
forney jr codes graphs normal realizations ieee trans 
inform 
theory vol 
pp 
feb 
hartmann rudolph optimum symbol symbol decoding rule linear codes ieee trans 
inform 
theory vol 
pp 
sept 
replication decoding ieee trans 
inform 
theory vol 
pp 
may 
hagenauer offer iterative decoding binary block convolutional codes ieee trans 
inform 
theory vol 
pp 
mar 
wiberg codes decoding general graphs ph dissertation univ link ping link ping sweden 
jacobs principles communication engineering 
new york wiley 
table integrals series products 
new york academic 
bertsimas tsitsiklis linear optimization 
belmont ma athena scientific 
jin mceliece irregular repeat accumulate codes proc 
nd int 
symp 
turbo codes related topics france sept pp 

van roy analysis belief propagation turbo decoding graph gaussian densities ieee trans 
inform 
theory vol 
pp 
feb 
