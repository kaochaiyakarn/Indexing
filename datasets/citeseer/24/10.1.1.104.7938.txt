learning decision trees dynamic data streams jo gama fep university porto rua de porto portugal pt pedro university porto rua de porto portugal pt presents system induction forest functional trees data streams able detect concept drift 
ultra fast forest trees incremental algorithm works online processing example constant time performing single scan training examples 
uses analytical techniques choose splitting criteria information gain estimate merit possible splitting test 
multi class problems algorithm builds binary tree possible pair classes leading forest trees 
decision nodes leaves contain naive bayes classifiers playing different roles induction process 
naive bayes leaves classify test examples 
naive bayes inner nodes play different roles 
multivariate splitting tests chosen splitting criteria detect changes class distribution examples traverse node 
change class distribution detected sub tree rooted node pruned 
naive bayes classifiers leaves classify test examples splitting tests outcome naive bayes naive bayes classifiers decision nodes detect changes distribution examples directly obtained sufficient statistics required compute splitting criteria additional computations 
aspect main advantage context high speed data streams 
methodology tested artificial real world data sets 
experimental results show performance comparison batch decision tree learner high capacity detect drift distribution examples 
key words data streams incremental decision trees concept drift category journal universal computer science vol 
submitted accepted appeared ucs sources produce data continuously 
examples include telephone record calls customer click streams large sets web pages multimedia data sets retail chain transactions 
sources called data streams 
data streams training examples come time usually time 
authors desirable properties learning data streams incrementality online learning constant time process example single scan training set take drift account 
situations highly unprovable assumption examples generated random stationary probability distribution 
complex systems large time periods expect changes distribution examples 
natural approach incremental tasks adaptive learning algorithms incremental learning algorithms take account concept drift 
algorithm generates forest functional trees data streams 
main contributions include fast method choose cut point splitting tests multivariate splitting tests functional leaves classify test cases ability detect concept drift 
aspects integrated sense sufficient statistics needed splitting criteria statistics functional leaves multivariate splitting tests drift detection method 
organized follows 
section related areas incremental decision tree induction concept drift detection 
section main issues algorithm 
system implemented evaluated set benchmark problems 
preliminary results section 
section resume main contributions point 
related gama learning decision trees dynamic data streams section analyze related dimensions 
dimension related methods dealing concept drift 
dimension related induction decision trees data streams 
literature machine learning methods deal time changing concepts 
basic methods temporal windows window fixes training set learning algorithm weighting examples ages examples shrinking importance oldest examples 
basic methods combined 
weighting time window forgetting systems incremental learning 
method dynamically choose set old examples learn new concept faces difficulties 
select examples learner algorithm keep old data disturbing learning process older data different probability distribution new concept 
larger set examples allows better generalization concept drift happened examples arrived 
systems weighting examples partial memory select examples probably new context 
repeated examples assigned weight 
older examples threshold forgotten newer ones learn new concept model 
drift concept occurs older examples gama learning decision trees dynamic data streams irrelevant 
apply time window training examples learn new concept description examples 
time window improved adapting size 
widmer klinkenberg methods choose time window dynamically adjusting size heuristics track learning process 
methods select time window include examples current target concept 
klinkenberg presents method automatically select time window size order minimize generalization error 
kubat widmer describe system adapts drift continuous domains 
klinkenberg shows application methods handling concept drift adaptive time window training data selecting representative training examples weighting training examples 
systems automatically adjust window size example selection example weighting minimize estimated generalization error 
concept drift context data streams appears example :10.1.1.14.4071
wang train ensembles batch learners sequential chunks data error estimates test data time evolving environment 
hulten domingos proposed method scale learning algorithms large databases :10.1.1.119.3124
system vfdt fast decision tree algorithm data streams described nominal attributes 
main innovation vfdt hoeffding bound decide leaf expanded decision node 
vfdt extended ability detect changes underlying distribution examples 
system mining decision trees time changing data streams 
works keeping model consistent sliding window examples 
new example arrives increments counts corresponding new example decrements counts oldest example window forgotten 
node tree maintains sufficient statistics 
periodically splitting test recomputed 
new test chosen starts growing alternate sub tree 
old replaced new accurate 
ultra fast forest trees algorithm supervised classification learning generates forest binary trees 
algorithm incremental processing example constant time works online 
designed continuous data 
uses analytical techniques choose splitting criteria information gain estimate merit possible splitting test 
multi class problems algorithm builds binary tree possible pair classes leading forest trees 
training phase algorithm maintains short term gama learning decision trees dynamic data streams memory 
data stream limited number examples maintained data structure supports constant time insertion deletion 
test installed leaf transformed decision node descendant leaves 
sufficient statistics leaf initialized examples short term memory fall leaf 
shown results large medium size problems 
incorporate system ability support concept drift detection 
detect concept drift maintain inner node naive bayes classifier trained examples traverse node 
statistical theory guarantees stationary distribution examples online error naive bayes decrease distribution function examples changes online error naive bayes node increase 
case decide test installed node appropriate actual distribution examples 
occurs sub tree rooted node pruned 
algorithm forgets sufficient statistics learns new concept examples new concept 
drift detection method check stability distribution function examples decision node 
sections provide detailed information relevant aspects system 
algorithm details splitting criteria starts single leaf 
splitting test installed leaf leaf decision node descendant leaves generated 
splitting test possible outcomes conducting different leaf 
value true associated branch value false 
splitting tests numerical attribute form 
analytical method split point selection 
choose numerical attributes promising 
sufficient statistics required mean variance class numerical attribute 
major advantage approaches exhaustive method necessary statistics computed fly 
desirable property treatment huge data streams guarantees constant time processing example 
analytical method uses modified form quadratic discriminant analysis include different variances classes analysis assumes distribution values attribute follows normal distribution classes 
exp normal density function reader note class problem decomposed class problems 
gama learning decision trees dynamic data streams sample mean variance class 
class mean variance normal density function estimated sample set examples node 
quadratic discriminant splits axis intervals possible roots equation denotes estimated probability example belongs class split root closer sample means classes 
root 
splitting test candidate numeric attribute form di 
choose best splitting test candidate list heuristic method 
information gain choose splitting point candidates best splitting test 
compute information gain need construct contingency table distribution class number examples lesser greater di di di class class information kept tree sufficient compute exact number examples entry contingency table 
doing require maintain information examples leaf 
assumption normality compute probability observing value greater di 
probabilities distribution examples class leaf populate contingency table 
splitting test maximum information gain chosen 
method requires maintain mean standard deviation class attribute 
quantities easily maintained incrementally 
authors extension vfdt deal continuous attributes 
btree store continuous complexity nlog 
complexity method proposed 
denote algorithm ultra fast 
merit splitting evaluated decide expansion tree 
problem discussed section 
leaf decision node expand tree test di installed leaf leaf decision node new descendant leaves 
expand leaf conditions satisfied 
requires information gain selected splitting test positive 
gain expanding leaf expanding 
second condition exist statistical support favor best splitting test asserted hoeffding bound vfdt 
new nodes created short term memory 
described section 
gama learning decision trees dynamic data streams short term memory short term memory maintains limited number examples 
examples update statistics new leaves created 
examples short term memory traverse tree 
reach new leaves update sufficient statistics tree 
data structure algorithm supports constant time insertion elements sequence constant time removal elements sequence 
functional leaves classify unlabeled example example traverses tree root leaf 
follows path established decision node splitting test appropriate attribute value 
leaf reached classifies example 
classification method naive bayes classifier 
naive bayes classifiers tree leaves enter overhead training phase 
leaf maintain sufficient statistics compute information gain 
necessary statistics compute conditional probabilities xi class assuming attribute values follow class normal distribution 
number attributes denotes standard normal density function values attribute belong class 
assuming attributes independent class bayes rule classify example class maximizes posteriori conditional probability ci log pr ci log xi sik 
simple motivation option 
changes leaf decision node sufficient number examples support change 
usually hundreds thousands examples required 
classify test example majority class strategy information class distributions look attribute values 
uses small part available information crude approximation distribution examples 
hand naive bayes takes account prior distribution classes conditional probabilities attribute values class 
way better exploitation information available leaf 
forest trees splitting criterion applies class problems 
real world problems multi class 
original batch learning scenario problem solved decision node means cluster algorithm group classes super classes 
obviously cluster method applied context learning data streams 
gama learning decision trees dynamic data streams propose methodology round robin classification 
round robin classification technique decomposes multi class problem binary problems pair classes defines class problem 
author shows advantages method solve class problems 
algorithm builds binary tree possible pair classes 
example class problem algorithm grows forest binary trees pair 
general case classes algorithm grows forest binary trees 
new example received tree growing phase tree receive example class attached classes tree label 
example train trees tree get examples 
short term memory common trees forest 
leaf particular tree decision node examples corresponding tree initialize new leaves 
fusion classifiers doing classification test example algorithm sends example trees forest 
example traverse tree root leaf classification registered 
tree forest prediction 
prediction takes form probability class distribution 
account classes tree discriminates probabilities aggregated sum rule 
probable class classify example 
note examples forced classified erroneously binary base classifiers classifier label examples belonging classes trained 
functional inner nodes evaluating splitting criteria merit best attributes closed difference gains satisfy hoeffding bound 
aspect pointed 
vfdt authors propose user defined constant decide split hoeffding bound satisfied 
tie evaluation merit tests single attributes system starts trying complex splitting tests 
shown sufficient statistics splitting criteria directly construct naive bayes classifier 
idea functional inner nodes install splitting tests predictions naive bayes classifier build node 
suppose observe leaf difference gain best attributes satisfies hoeffding bound 
tie gama learning decision trees dynamic data streams new training example falls leaf classified naive bayes derived sufficient statistics 
predictions populate contingency table cell nij contains number examples class naive bayes predict class evaluation evaluate addiction evaluation original attributes information gain contingency table obtained naive bayes predictions 
evaluation corresponds consider new attribute naive bayes predictions 
implicit attribute best attribute terms information gain difference respect second best satisfies hoeffding bound leaf decision node outcomes naive bayes predictions 
uses naive bayes classifiers leaves 
considering splitting tests naive bayes consider advantage splitting versus splitting 
example predictions naive bayes accurate corresponding gain high 
cases don need expand leaf avoiding structure overfitting 
tie expand leaf gain naive bayes predictions user defined threshold 
experiments described threshold set 
naive bayes classifiers attributes predictions 
aspect negative presence irrelevant attributes 
consider splitting tests naive bayes classifiers tie 
aspect select informative attributes naive bayes 
concept drift detection algorithm maintains node decision trees naive bayes classifier 
classifiers constructed sufficient statistics needed evaluate splitting criteria node leaf 
leaf node naive bayes classifier classify examples traverse node 
basic idea drift detection method control online error rate 
distribution examples traverse node stationary error rate naive bayes decreases 
change distribution examples naive bayes error increase 
system detect increase naive bayes error node indication change distribution examples suggest splitting test installed node longer appropriate 
cases subtree rooted node pruned node leaf 
sufficient statistics leaf initialized examples new context short term memory 
designate context set contiguous examples distribution stationary assuming data stream set contexts 
goal method detect sequence examples data stream change context 
gama learning decision trees dynamic data streams new training example available cross corresponding binary decision trees root node till leaf 
node naive bayes installed node classifies example 
example correctly incorrectly classified 
set examples error random variable bernoulli trials 
binomial distribution gives general form probability random variable represents number errors sample examples 
estimator true error classification function pi number examples number examples misclassified measured actual context 
estimate error variance 
standard deviation binomial pi pi si number examples observed context 
sufficient large values example size binomial distribution closely approximated normal distribution mean variance 
considering probability distribution unchanged context static confidence interval examples approximately pi si 
parameter depends confidence level 
experiments confidence level drift set 
drift detection method manages registers training learning algorithm pmin smin 
time new example processed values updated pi si lower pmin smin 
warning level define optimal size context window 
context window contain old examples new context minimal number examples old context 
suppose sequence examples traverse node example correspondent pi si 
warning level reached pi si pmin smin 
drift level reached pi si pmin smin 
suppose sequence examples naive bayes error increases reaching warning level example kw andthe drift level example kd 
indicator change distribution examples 
new context declared starting example kw pruned leaf 
sufficient statistics leaf initialized examples short term memory time stamp greater kw 
itis possible observe increase error reaching warning level followed decrease 
assume situations corresponds false alarm changing context 
method learning forgetting ensure way continuously keep model better adapted context 
method uses information available learning algorithm require additional computational resources 
advantage method continuously monitors online error naive bayes 
detect changes class distribution examples time 
decision nodes contain naive bayes detect changes examples traverse node correspond detect gama learning decision trees dynamic data streams shifts different regions instance space 
nodes near root able detect abrupt changes distribution examples deeper nodes detect smoothed changes 
main characteristics due splitting criteria 
statistics required splitting criteria computed incrementally 
directly derive naive bayes classifiers sufficient statistics 
naive bayes classifiers leaves classify test examples inner decision nodes detect drift splitting tests 
known naive bayes low variance classifier 
property relevant mainly naive bayes acts splitting test drift detection 
experimental stationary data experimental done waveform led balance datasets available uci repository 
waveform problems classes 
problem defined numerical attributes 
second contains attributes 
known optimal bayes error 
led problem binary attributes irrelevant classes 
optimal bayes error 
balance problem attributes classes 
choice datasets motivated existence dataset generators uci repository simulate streams data 
problems generate training sets varying number examples starting till 
test set contains examples 
generates model training set seeing example 
generated model classifies test examples 
algorithm parameters values nmin buffer size 
algorithms ran ghz mb ram linux mandrake 
comparative purposes state art decision tree learning 
batch algorithm requires data fit memory 
data successively re decision nodes order choose splitting test 
decision node continuous attributes sorted operation complexity log 
conducted set experiments comparing 
algorithms learn training dataset generated models evaluated test set 
detailed results table 
orders magnitude faster generating simpler terms number decision nodes models similar performance 
advantage multivariate splitting tests evident waveform datasets 
differences performance appears columns version default table 
datasets observed improvement gama learning decision trees dynamic data streams error rate training time tree size exs drift version default balance dataset attributes waveform dataset attributes waveform dataset attributes led dataset attributes table learning curves datasets study 
results versions disabling drift drift disabling naive bayes splitting tests version enabling naive bayes drift detection default 
led dataset figures tree size refer mean trees 

datasets generated stationary distribution 
signals false alarms drift detection 
appear root node deeper nodes tree 
impact performance reduced null 
non stationary data illustrative purposes evaluate sea concepts previously evaluate ability detect concept drift 
table presents average error rate runs setting ability drift detection 
results different significance level 
clear indicate benefits drift detection dataset 
results 
electricity market dataset collected australian nsw elec gama learning decision trees dynamic data streams sea dataset electricity market dataset lower upper bound drift drift test set bound data year mean day variance week table error rates drift detection problems 
market 
market prices fixed affected demand supply market 
prices market set minutes 
class label identifies change price related moving average hours 
goal problem predict price increase decrease 
original dataset design experiments 
experiments test set day examples test set week examples 
problem detect lower bound upper bound error batch decision tree learner 
upper bound ad hoc heuristics choose training set 
heuristic training data heuristic year training examples 
predicting day error rate training set restricting training set year error decrease 
compute lower bound perform exhaustive search best training set produces lower error rate day training set 
results appear table 
experiments drift detection exhibit performance similar lower bound exhaustive search 
indication quality results 
advantage drift detection method ability automatically choose set training examples 
real world dataset know context changing 
presents incremental learning algorithm appropriate processing high speed numerical data streams 
main contributions ability multivariate splitting tests ability adapt decision model concept drift 
impact performance system extends range applications dynamic environments 
system process new examples arrive performing single scan training data 
method choose cut point splitting tests quadratic discriminant analysis 
complexity version cart implemented 
examples 
sufficient statistics required analytical method computed incremental way guaranteeing constant time process example 
analytical method restricted class problems 
forest binary trees solve problems classes 
contributions short term memory initialize new leaves functional leaves classify test cases 
important aspect ability detect changes distribution examples 
detect concept drift maintain inner node naive bayes classifier trained examples cross node 
distribution examples stationary online error naive bayes decrease 
distribution changes naive bayes online error increase 
case test installed node appropriate actual distribution examples 
occurs subtree rooted node pruned 
pruning corresponds forget older examples 
empirical evaluation stationary data shows competitive state art batch decision tree learning computational resources 
main factors justifies performance system 
powerful classification strategies tree leaves 
ability multivariate splits 
experimental results non stationary data suggest system exhibit fast reaction changes concept learn 
performance system indicates adaptation decision model actual distribution examples 
stress naive bayes classifiers leaves classify test examples naive bayes splitting tests naive bayes classifiers decision nodes detect changes distribution examples directly obtained sufficient statistics required compute splitting criteria additional computations 
aspect main advantage context high speed data streams 
acknowledgments developed context projects posi eia 
gama learning decision trees dynamic data streams 
blake keogh merz 
uci repository machine learning 

domingos hulten 
mining high speed data streams 
ramakrishnan stolfo editors proceedings international conference knowledge discovery data mining pages 
acm press 

duda hart stork 
pattern classification 
sons 

rnkranz 
round robin classification 
journal machine learning research 
gama learning decision trees dynamic data streams 
gama 
functional trees 
machine learning 

gama rocha 
accurate decision trees mining high speed data streams 
domingos faloutsos editors th acm sigkdd int 
conference 
acm press 

hulten domingos 
catching data research issues 
inproc 
workshop research issues data mining knowledge discovery 

hulten spencer domingos 
mining time changing data streams 
provost editor proceedings seventh international conference knowledge discovery data mining 
acm press 

gentleman 
language data analysis graphics 
journal computational graphical statistics 

kittler 
combining classifiers theoretical framework 
pattern analysis applications 

klinkenberg 
learning drifting concepts example selection vs example weighting 
intelligent data analysis 

klinkenberg joachims 
detecting concept drift support vector machines 
langley editor proceedings icml th international conference machine learning pages 
morgan kaufmann publishers san francisco 

klinkenberg renz 
adaptive information filtering learning presence concept drifts 
learning text categorization pages 
aaai press 

kubat widmer 
adapting drift continuous domain 
proceedings th european conference machine learning pages 
spinger verlag 


loh 
shih 
split selection methods classification trees 
statistica sinica 

michalski 
selecting examples partial memory learning 
machine learning 

mitchell 
machine learning 
mcgraw hill 

quinlan 
programs machine learning 
morgan kaufmann publishers 

street kim 
streaming ensemble algorithm large scale classification 
proc 
th acm sigkdd pages 
acm press 

vicente caticha 
statistical mechanics online learning drifting concepts variational approach 
machine learning 

wang fan yu han 
mining concept drifting data streams ensemble classifiers 
domingos faloutsos editors th acm sigkdd int 
conference 
acm press 

widmer kubat 
learning presence concept drift hidden contexts 
machine learning 
