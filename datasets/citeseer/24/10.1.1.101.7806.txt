journal information fusion diversity creation methods survey categorisation gavin brown jeremy wyatt rachel harris xin yao university birmingham school computer science park road birmingham tt uk www cs bham ac uk brown cs bham ac uk ensemble approaches classification regression attracted great deal interest years 
methods shown theoretically empirically outperform single predictors wide range tasks 
elements required accurate prediction ensemble recognised error diversity 
exact meaning concept clear literature particularly classification tasks 
review varied attempts provide formal explanation error diversity including heuristic qualitative explanations literature 
completeness discussion include classification literature excerpts mature regression literature believe provide insights 
proceed survey various techniques creating diverse ensembles categorise forming preliminary taxonomy diversity creation methods 
part taxonomy introduce idea implicit explicit diversity creation methods dimensions may applied 
propose new directions may prove fruitful understanding classification error diversity 
creating diverse sets classifiers keys success multiple classifier systems 
easy understanding classifier error diversity incomplete 
measures diversity numerical taxonomy literature complete grounded framework useful guide myriad techniques create error diversity 
ensemble approaches classification usually non linear combination methods majority voting regression problems naturally tackled linearly weighted ensembles 
type ensembles clearer framework explaining role diversity voting methods 
particular ambiguity decomposition bias variance covariance decomposition provide solid quantification diversity linearly weighted ensembles connecting back objective error criterion mean squared error 
believe provide insights diversity classification problems 
significant contribution survey categorisation ad hoc methods diversity creation classification context 
fully acknowledge embark journal information fusion venture debate result categorisation field 
mind note useful taxonomy achieve things firstly allow spot gaps literature allowing explore new techniques secondly allow measure distance existing techniques afford form metric group techniques families perform different application domains 
believe study achieves allowing identify locations space possible ensemble techniques exploited 
suggest studies akin friedman isle framework importance sampling ensembles give insights challenge 
section provide explanations ensembles 
leads section description existing formal accounts diversity case regression linearly weighted ensemble 
part explicitly show link bias ambiguity decompositions 
attempts characterise diversity classification problems dealt section 
section describe proposed taxonomy diversity creation methods 
discussions important issues ensemble learning section 
research directions section 
ensemble better single learner 
section concept error diversity 
review existing explanations ensembles diverse errors perform completeness discussion include literature regression classification case 
process clarify subtle point overlooked quantifying classifier ensemble diversity inherent predictor outputs 
proceed review literature attempts create diversity forms 
comment structure field section propose novel way categorise ad hoc diversity creation techniques 
regression context major study combining regression estimators perrone fact time independently hashem developed results 
study machine learning literature topic covered research communities years example financial forecasting bates granger clemen 
consequence understanding diversity quite mature show 
illustrative scenario consider single neural network approximating sine wave network supplied limited set data points train inputs chosen randomly uniform small amount gaussian noise added outputs 
consider single testing data point find value sin 
true answer know network may possibly value 
way errors follow distribution dependent random training data sample received random initialisation weights 
mean distribution expectation value network trained particular dataset drawn random variable particular weight initialisation drawn random variable expectation operator respect random variables 
noted convenience omitted input vector normally 
journal information fusion addition observations respect single pattern results easily generalise full space integrating joint input density 
illustrates typical error distribution target value shown 
crosses marked estimates sin hypothetical network estimates differ network started different initial weights time 
typical error distribution unbiased estimator approximating sin viewpoint immediately see parallels ensemble system 
member ensemble realisation random variable defined distribution possible training datasets weight 
ensemble output average set realisations diversity promoting mechanisms encourage sample mean closer approximation 
large ensemble large sample space consequently large sample expect approximation mean 
smaller ensemble expect sample mean may upward downward biased 
order correct methods bagging construct networks different training datasets allowing sample representative portion space 
illustration assumes expected value estimator equal true target value unbiased estimator 
case may situation 
typical error distribution biased estimator approximating sin estimator upward biased expected value high target case sample times distribution able estimate target accurately simple average combination simple average converge add networks 
need non uniformly weight outputs giving higher weights networks predicting lower values 
course purely hypothetical journal information fusion scenario look closely single data point manually set weights combination serve illustrate chance error reduced combination predictors 
intuition formalised ambiguity decomposition 
ambiguity decomposition krogh vedelsby proved single data point quadratic error ensemble estimator guaranteed equal average quadratic error component estimators wi fi wi fi convex combination wi component estimators details original proof omitted authors space considerations 
fact shown simply manipulations bias variance decomposition reflecting strong relationship decompositions 
alternative version wi fi wi fi wifi wi fi fi wi fi wi fi wi fi encouraging result ensemble research providing simple expression effect due error correlation ensemble 
section see number researchers exploited result various ways influence design ensemble learners 
significance ambiguity decomposition shows set predictors error convex combined ensemble equal average error individuals 
course individuals may fact lower error average lower ensemble particular pattern 
criterion identifying best individual pick random 
way looking significance ambiguity decomposition tells combination predictors better average patterns method selected predictors random 
decomposition terms 
wi fi weighted average error individuals 
second wi fi ambiguity term measuring amount variability ensemble member answers pattern 
positive subtractive term meaning ensemble guaranteed lower error average journal information fusion individual error 
larger ambiguity term larger ensemble error reduction 
variability individuals rises value term 
shows diversity need get right balance diversity ambiguity term individual accuracy average error term order achieve lowest ensemble error 
ambiguity decomposition holds convex combinations property ensemble trained single dataset 
bias variance decomposition take account distribution possible training sets possible weight 
interested course expected error data points distributions 
bias variance covariance decomposition takes exactly account 
bias variance covariance concept ensemble diversity regression estimators understood examine bias variance decomposition ensemble 
bias variance decomposition quadratic loss states generalisation error estimator broken components bias variance 
usually opposition attempts reduce bias component cause increase variance vice versa 
decomposition follows mse var bias expected value target point noise 
estimator convex combined ensemble variance component breaks bias variance covariance decomposition 
clearly understand define concepts 
bias averaged bias ensemble members bias second var averaged variance ensemble members var third covar averaged covariance ensemble members covar fi fi fi fi fi fj fj gives bias variance covariance decomposition mean square error fi bias var covar journal information fusion see mean square error ensemble networks depends critically amount error correlation networks quantified covariance term 
ideally decrease covariance causing increases bias variance terms 
worth noting bias variance constrained positive valued covariance term negative 
connection ambiguity covariance show exact link decompositions just described 
know expressed terms ambiguity decomposition 
know property quadratic loss function convex combination predictors specific particular target value 
expected value target data case re express terms average quadratic error ambiguity term 
substitute right hand side equation left hand side equation 
fi fi bias var covar interesting understand portions bias variance covariance decomposition correspond ambiguity term portions average individual error term 
manipulations details see show fi fi fi bias fi var var covar term interaction sides fi term sides combine subtracting ambiguity equation average mse equation interaction terms cancel get original bias variance covariance decomposition back rhs equation 
term mean 
examine little fi fi ei fi ei fi journal information fusion ei fi ei fi var deviations ei fi average variance estimators plus average squared deviation expectations individuals expectation ensemble 
fact interaction exists illustrates simply maximise ambiguity affecting parts error effect interaction quantifies diversity trade regression ensembles 
section tried give intuition diversity ensemble members averaged idea 
noted framework decompositions told merely quantify diversity achieve types ensemble 
addition said ensemble schemes involving types combination voting non linear schemes 
address section 
classification context shown regression context rigorously define differences individual predictor outputs contribute ensemble accuracy 
classification context neat theory 
subtle point overlooked 
difficulty quantifying classification error diversity intrinsic ensembles tackling classification problems 
possible reformulate classification problem regression choosing approximate class posterior probabilities allows theory discussed apply progressing area notably tumer ghosh roli 
regression context discussed previous section question clearly phrased quantify diversity predictors output real valued numbers combined convex combination 
case tumer ghosh study question just real valued numbers probabilities 
harder question appears restricted predictors output discrete class labels decision trees neighbour classifiers 
case outputs intrinsic concept covariance undefined 
non implies change combination function popular majority voting individual votes 
harder question phrased quantify diversity predictors output non ordinal values combined majority vote 
account simply clear analogue bias variance covariance decomposition zero loss function 
number highly restricted theoretical results assumptions probably strong hold practice 
describe known tumer ghosh combining posterior probability estimates ordinal values turn considering harder question non ordinal outputs 
ordinal outputs tumer ghosh provided theoretical framework analysing simple averaging combination rule predictor outputs estimates posterior probabilities class 
dimensional feature vector solid curves show true posterior probabilities classes respectively 
dotted curves show estimates journal information fusion class class tumer ghosh framework analysing classifier error posterior probabilities predictors 
solid vertical line indicates optimal decision boundary boundary minimise error posterior probabilities overlap 
overlap dark shaded area termed bayes error irreducible quantity 
dotted vertical line indicates boundary placed predictor certain distance optimal 
light shaded area indicates added error predictor addition bayes error 
individual predictor approximates posterior probability class pi true posterior probability class estimation error 
assume estimation errors different classes independent identically distributed random variables zero mean variance consider predictor expected added error distinguishing classes expected size light shaded area variance stated derivative true posterior probability class decision boundary placed ensemble predictors tumer ghosh show expected added error ensemble estimator ens add number classifiers 
expected added error individual classifiers assumed error 
correlation coefficient see details measuring correlation errors approximating posterior probabilities journal information fusion direct measure diversity zero classifiers ensemble statistically independent posterior probability estimates 
add error ensemble times smaller error individuals 
perfect correlation error ensemble just equal average error individuals 
achieve simple expression assume errors different classifiers variance possibly critical assumption posterior probabilities monotonic decision boundary 
extended roli allowing assumptions lifted important allows non uniformly weighted combinations 
demonstrates understanding particular diversity formulation outputting posterior probabilities progressing 
sticking point ensemble research non ordinal output case illustrate 
non ordinal outputs ensemble research hansen salamon seen seminal diversity neural network classification ensembles 
stated necessary sufficient condition majority voting ensemble classifiers accurate individual members classifiers accurate diverse 
accurate classifier error rate better random guessing new input 
classifiers diverse different errors new data points 
binomial theorem explain assume networks arrive correct classification pattern probability statistically independent errors 
gives probability majority voted ensemble error ensemble incorrect expressed probability ensemble error single testing data point 
example ensemble predictors combined majority vote members wrong order ensemble wrong 
area binomial distribution probability ensemble incorrect 
stressed applies predictors statistically independent assumption strong hold practice represent theoretical unachievable ideal 
breiman presents upper bound error random forests ensembles decision trees constructed particular randomization technique 
bound generalisation error integrated possible training sets ensemble generalization error strength ensemble expressed expected size margin ensemble achieves correct labelling averaged pairwise correlation oracle fact tumer ghosh rely fundamentally manipulations bias decomposition interesting see link explicit 
results plurality voting turns far complex 
journal information fusion outputs 
bound tight kuncheva comments regarded piece missing general theory diversity 
comment nature problem section 
point onwards referring classification error diversity assumed referring difficult task quantifying non ordinal output diversity 
description problem clearly stated deriving expression classification diversity easy regression 
ideal situation parallel regression case squared error ensemble re expressed terms squared errors individuals term quantifies correlation 
expression similarly decomposes classification error rate error rates individuals term quantifies diversity 
state art number empirical investigations gone deriving heuristic expressions may approximate unknown diversity term 
heuristic metric classification error diversity 
number authors tried qualitatively define classification error diversity 
sharkey suggested scheme ensemble pattern errors described terms level diversity 
noted concentrated neural networks ideas equally applicable type classifier 
levels proposed describing incidence effect coincident errors members ensemble degree target function covered ensemble 
coincident error occurs input ensemble member gives incorrect answer 
function coverage indication test input yields correct answer individuals ensemble 
level coincident errors target function covered 
majority vote produces correct answer 
level coincident errors majority correct function completely covered 
ensemble size greater hold 
level majority vote yield right answer members ensemble cover function correct answer input 
level function covered members ensemble 
sharkey acknowledges ensembles exhibiting level diversity upwardly mobile possible ensemble labelled having level diversity contain subset members displaying level diversity test set level ensemble contain subsets members displaying level level diversity test set removal certain members result change diversity level better 
problem heuristic gives indication typical error behaviour described assigned diversity level respect test data tell generate diversity 
metric denoting error diversity sharkey assigns level describes worst case observed happen basis ensemble performance example test data 
example ensemble perform consistently level diversity test dataset fail single pattern mean ensemble demoted level diversity 
table shows journal information fusion maximum number classifiers allowed error single test pattern order diversity level hold 
level maximum networks error odd table maximum allowable networks error sharkey diversity levels hold single test pattern 
heuristic sufficient illustrative purposes intuitively seen particular multi classifier system exhibit different diversity levels different subsets dataset 
levels may act better indication ensemble pattern errors coupled proportions test data performs described levels 
distribution error diversity levels observed testing produced describe performance ensemble 
example test set patterns plausible diversity levels observed show table 
ensemble data examples produce level diversity second ensemble examples produce level indicating second ensemble lower error 
level level level level ensemble ensemble table distribution diversity levels dataset number patterns exhibiting level diversity hypothetical ensembles 
sharkey ensembles assigned level diversity despite fact second ensemble performs better shown adding proportions different levels diversity occur 
suggested improvement give indication members ensemble responsible proportions different levels error 
carney cunningham suggested entropy measure allow calculation individual contribution diversity 
cunningham proposed measure classification ambiguity 
ambiguity ith classifier averaged patterns ai ai xn journal information fusion ai xn output individual disagrees ensemble output 
ensemble ambiguity ai xn vast majority empirical evidence examining classifier diversity due kuncheva 
studies explored measures diversity numerical taxonomy literature 
kuncheva emphasizes existence styles measuring diversity pairwise non pairwise 
pairwise measures calculate average particular distance metric possible pairings classifiers ensemble 
distance metric determines characteristics diversity measure 
non pairwise measures idea entropy calculate correlation ensemble member averaged output 
myriad metrics studied statistic consider 
take classifiers fi fj 
large set testing patterns exhibit certain coincident errors probability error coincidence calculated 
referred oracle outputs 
table illustrates assigning label type coincidence 
fi correct fj wrong fi correct fi wrong table probabilities coincident errors classifier fi fj 
noted definition 
statistic classifier classifier qi ad bc ad bc statistic ensemble calculated average value possible pairings ensemble members 
addition statistic metrics examined 
extensive experimental conducted find measure diversity correlate majority vote accuracy 
summary kuncheva states rough tendency confirmed prominent links appeared diversity ensemble accuracy 
diversity poor predictor ensemble accuracy alternative measure diversity advocated dietterich kappa statistic 
coincidence matrix kappa defined ac bd dietterich produced kappa error plots adaboost bagging 
possible pair classifiers plotted space average error pair 
showed distinctive clouds points indicated definite trade individual accuracy journal information fusion measure 
comparing clouds points adaboost versus bagging verified adaboost produces diverse ensembles classifiers 
spite seemingly intuitive definitions diversity proved definite link ensemble error 
amorphous concept diversity elusive 
reviewed state field regard explanations term diversity errors means lead improved ensemble performance 
community puts great stead concept classification error diversity ill defined concept 
lack definition diversity stopped researchers attempting achieve 
effective performing ensemble 
taxonomy methods creating diversity determined previous section regression classification contexts correlation individual predictor outputs definite effect ensemble error classification formalised literature 
section attempt move possible way understand different methods researchers create ensemble exhibiting error diversity 
constructing ensemble may choose take information diversity account may may may explicitly try optimise metric diversity building ensemble 
distinction types explicit implicit diversity methods 
technique bagging implicit method randomly samples training patterns produce different sets network point measurement taken ensure diversity emerge 
boosting explicit method directly manipulates training data distributions ensure form diversity base set classifiers obviously guaranteed right form diversity 
learning function approximator follows trajectory hypothesis space 
individuals ensemble occupy different points hypothesis space 
implicit methods rely randomness generate diverse trajectories hypothesis space explicit methods deterministically choose different paths space 
addition high level dichotomy possible dimensions ensuring diversity ensemble 
sharkey proposed neural network ensemble exhibit diversity influencing things initial weights training data architecture networks training algorithm 
means providing ensemble member different training set different architecture 
sensible way group literature difficult group ensemble techniques 
add penalty error function changing sharkey factors 
came categories believe majority ensemble diversity techniques placed 
starting point hypothesis space methods branch vary starting points hypothesis space influencing space converge 
set accessible hypotheses methods vary set hypotheses accessible ensemble 
certain hypotheses may accessible inaccessible particular training subset function approximator architecture techniques vary training data architecture employed different ensemble members 
journal information fusion traversal hypothesis space alter way traverse space leading different networks converge different hypotheses 
course noted hypothesis space search space necessarily thing 
neural networks search hypothesis space directly search space space possible weight values turn causes different network behaviour 
decision trees hypothesis space directly searched construct trees 
starting point hypothesis space starting network differing random initial weights increase probability continuing different trajectory networks 
common way generating ensemble generally accepted effective method achieving diversity authors default benchmark methods 
discuss implicit instances axis weights generated randomly discuss explicit diversity networks directly placed different parts hypothesis space 
sharkey investigated relationship initialisation output weight vectors solutions converged backpropagation 
systematically varied initial output weight vectors neural networks circle radius trained fuzzy xor task fixed set training data 
resulting networks differed number cycles took converge solution converged 
trained neural networks statistically independent generalisation performance displayed similar patterns generalisation despite having derived different initial weight vectors 
networks converged similar local optima spite starting different parts space 
varying initial weights neural networks important deterministic training method backpropagation effective stand method generating error diversity ensemble neural networks 
observations supported number studies 
partridge conducted experiments large patterns synthetic data sets concludes network type training set structure number hidden units random initialization weights effective method generating diversity 
munro doyle synthetic dataset medical diagnosis datasets compare fold cross validation bagging random weight initializations random weights method comes place 
methods implicit diversity methods manipulating starting point hypothesis space 
explicit methods weights occur literature topic small 
maclin shavlik approach initializing neural network weights uses competitive learning create networks initialised far origin weight space potentially increasing set reachable local minima show significantly improved performance standard method initialization real world datasets 
relevant technique fast committee learning trains single neural network snapshots state weights number instances training 
snapshots different ensemble members 
performance separately trained nets offers advantage reduced training time necessary train network 
modification method explicitly choosing stopping points metric 
journal information fusion set accessible hypotheses argued ways manipulate set hypotheses accessible learner firstly alter training data receives secondly alter architecture learner 
discuss create error diversity 
manipulation training data methods attempt produce diverse complementary networks supplying learner slightly different training set 
probably widely investigated method ensemble training 
regard bold square represents training set ensemble 
different ensemble members different parts set hopefully learn different things task 
methods divide training pattern supplying member features different subset rows patterns 
methods divide feature supplying member patterns set consists different subset columns features 
termed resampling methods provide overlapping non overlapping subsets rows columns different learners 
alternative pre process features way get different representation example log scaling features 
viewed diagram different plane moving space possible features 
data techniques transform features termed distortion methods 
training patterns features space possible features space possible training sets ensemble duin tax find combining results type classifier different feature sets far effective combining results different classifiers feature set 
conclude combination independent information different feature sets useful different approaches classifiers data 
known resampling method probably fold cross validation 
dividing dataset randomly disjoint pattern subsets new overlapping training sets created ensemble member leaving subsets training remainder 
bagging algorithm example randomly selecting patterns replacement original set patterns 
journal information fusion sharkey uses distortion method re represent features engine fault diagnosis task 
classifiers provided original data learn third provided data passed untrained neural network 
essentially applies random transformation features sharkey shows ensemble technique outperform ensemble classifiers non transformed data 
intrator raviv report simply adding gaussian noise input data help 
create bootstrap resample bagging add small amount noise input vector 
ensembles trained weight regularisation get estimate generalisation error 
process repeated different noise variance determine optimal level 
test data show significant improvements synthetic medical datasets 
far discussed input patterns resampled distorted breiman proposed adding noise outputs training data 
technique showed significant improvements bagging natural artificial datasets comparing adaboost improvements 
covered number papers training data create diversity far implicit diversity methods 
turn considering explicit methods deterministically change data supplied network 
popular adaboost algorithm explicitly alters distribution training data fed member 
distribution recalculated round account errors immediately previous network 
presents variant adaboost calculates distribution respect networks trained far 
way data received successive network explicitly designed errors diverse compensate 
cunningham metric classification diversity defined equation select subsets features learner 
build ensemble adding predictors successively metric estimate diversity ensemble far 
feature subset train predictor determined hill climbing strategy individual error estimated ensemble diversity 
predictor rejected causes reduction diversity pre defined threshold increase ensemble error 
case new feature subset generated predictor trained 
decorate algorithm melville mooney utilises metric decide accept reject predictors added ensemble 
predictors generated training original data plus diversity set artificially generated new examples 
input vectors set passed current ensemble see decision 
pattern diversity set output re labelled opposite ensemble predicted 
new predictor trained set high disagreement ensemble increasing diversity hopefully decreasing ensemble error 
ensemble error reduced new diversity set produced new predictor trained 
algorithm terminates desired ensemble size specified number iterations reached 
tumer input decimation ensembles seeks reduce correlations individual estimators different subsets input features 
feature selection achieved calculating correlation feature individually class training predictors specialists particular classes groups classes 
showed significant benefits real artificial datasets 
liao moody demonstrate informationtheoretic technique feature selection input variables grouped mutual information 
statistically similar variables assigned group journal information fusion member input set formed input variables extracted different groups 
experiments noisy nonstationary economic forecasting problem show outperforms bagging random selection features 
authors domain knowledge divide features predictors 
example sharkey pressure temperature information indicate properties engine wan combines information fossil record sunspot time series data predict sunspot fluctuations 
methods discussed manipulate input data 
dietterich bakiri manipulate output targets error correcting output coding 
output class problem represented binary string chosen orthogonal close possible representation classes 
input pattern predictor trained reproduce appropriate binary string 
testing string produced predictor exactly match string representing classes hamming distance measured class closest chosen 
kong dietterich investigate technique works :10.1.1.57.5909
find bagging ecoc reduces variance ensemble addition correct bias component 
important point note result loss bias variance decomposition utilised assumes bayes rate zero zero noise 
manipulation architectures number investigations different types neural network different types learners general ensembles small 
want diverse errors ensemble sense different types function approximator may produce 
partridge concludes variation numbers hidden nodes initial weights useful method creating diversity neural network ensembles due methodological similarities supervised learning algorithms 
number hidden nodes varied single dataset limited experiment indicates done 
partridge mlps radial basis functions ensemble investigate effect network type diversity finding productive route varying hidden nodes 
consider may case problem choosing compatible network topologies place ensemble simply hard human 
opitz shavlik algorithm evolutionary algorithm optimise network topologies composing ensemble 
trains standard backpropagation selects groups networks error diversity measurement diversity 
proposed algorithm constructively builds ensemble monitoring diversity process 
simultaneously designs ensemble architecture training individual nns whilst directly encouraging error diversity 
determines automatically number nns ensemble number hidden nodes individual nns 
uses incremental training negative correlation learning training individual nns 
entirely possible ensemble consisting networks different architectures emerge incrementally constructed ensembles 
experiments done hybrid ensembles 
wang combined decision trees neural networks 
neural networks decision trees decision tree system performed better ratio 
langdon combines decision trees neural networks ensemble uses genetic programming journal information fusion evolve suitable combination rule 
woods combines neural networks nearest neighbour classifiers decision trees quadratic bayes classifiers single ensemble uses estimates local accuracy feature space choose classifier respond new input pattern 
studies hybrid ensembles indicate produce estimators differing accuracies different regions space sensible systems represent problem search space radically different ways may show different strengths different patterns generalisation 
specialisation implies hybrid ensembles selection single estimator fusion outputs estimators may effective 
dynamic selection method woods easily applied ensemble containing networks differing numbers hidden nodes having algorithm ensure network architectures established appropriately 
hypothesis space traversal particular search space defined architecture network training data provided occupy point space give particular hypothesis 
choose traverse space possible hypotheses determines type ensemble 
discuss penalty methods error function network warped penalty term encourage emergence diverse hypotheses secondly mention evolutionary search methods engage population search landscape enforce diversity population 
penalty methods authors benefit penalty term error function neural network ensemble 
noted regularization term sense tikhonov regularization previous ensemble research shown regularization learners smoothing error landscape prevent overfitting detrimental 
fact desire overfitting individual learners emerge reduces bias component error leaving variance component reduced ensemble combination 
penalty term error network ei fi weighting parameter penalty term parameter controls trade terms ensemble network training plain backpropagation increases emphasis placed minimising penalty term chosen 
rosen penalty term pi indicator function specifying decorrelation networks pi penalty function pi fi fj product ith jth network biases 
indicator function specifies networks decorrelated 
penalise network correlated previous trained network journal information fusion indicator function 
negative correlation nc learning extended rosen training networks parallel 
nc regularisation term pi fi fj average output ensemble networks previous timestep 
nc seen number empirical successes consistently outperforming simple ensemble system 
previous formalised certain aspects nc algorithm showing applied learning machine minimise mean square error function defining upper bound parameters 
shows revised version nc assuming gradient descent method learning 

final number predictors required 

take training set xn dn 

training pattern calculate fi xn network perform single update weight network ei fi xn dn fi xn ei fi fi xn dn fi xn new testing pattern ensemble output fi negative correlation learning algorithm nc particularly important technique considering diversity issue 
regression ensembles shown nc directly controls covariance term covariance trade 
directly adjusts amount diversity ensemble 
journal information fusion classification non ordinal outputs described apply 
interesting step forward field see equivalent technique classification directly controlling classification diversity term 
mckay root quartic negative correlation learning alternative penalty term 
term pi fi nc technique applied genetic programming system shown outperform standard nc larger ensembles explained exactly case 
standard mean squared error function presents certain error landscape ensemble system 
nc add penalty terms providing learner ensemble different landscape 
shown minimisation error landscapes reduce error unmodified landscape 
considered explicit diversity methods directly encouraging convergence different parts hypothesis space 
evolutionary methods term diversity evolutionary computation literature context maintaining diversity population individuals evolving 
different meaning concept diversity discussed article 
evolutionary algorithms wish maintain diversity order ensure explored large area search space focussed search unprofitable area 
decide search continued long typically take best performing individual far ignoring individuals population 
optimal diversity context optimises explore exploit trade best points search space quickly efficiently 
contrast ensemble diversity methods create diversity intention population ensemble members combined 
evolutionary diversity methods concern creating population complementary way just ensuring maximum amount hypothesis space explored order find best single individual 
spite differences researchers interesting parallels 
yao liu evolves population neural networks fitness sharing techniques encourage diversity combines entire population ensemble just picking best individual 
khare yao extend concept classification data kullback leibler entropy diversity metric optimise evolutionary search 
discussion categorise multiple classifier systems 
concluded categorisation diversity literature 
fully accept taxonomy categorising field broad axes journal information fusion believe clear statement problem tackling clear representation state field may help progress research 
problem describe summarise field concisely difficult 
categorisation fact result prototypes including entirely satisfactory 
ho divides space classifier combination techniques coverage optimisation decision optimisation diversity creation methods described come branch coverage optimisation 
sharkey proposed categorisation scheme multi network architectures 
architecture categorised competitive cooperative top bottom 
competitive architecture fashion select single network give final answer cooperative architecture fuse outputs networks 
architecture top combination mechanism network outputs 
mere fact equally valid ways categorise field pays tribute wide multiple classifier systems means multiple ways divide space 
proposed decomposition field note interesting family algorithms exploit ambiguity decomposition 
krogh vedelsby showed ensemble error broken terms dependent correlations networks 
number techniques explicitly measured ambiguity guide constructing ensemble utilised aspect ensemble construction 
krogh vedelsby active learning scheme method query committee selecting patterns train large ambiguity 
showed significant improvements passive learning approximating square wave function 
estimate ambiguity optimise ensemble combination weights showed cases optimal set network weight zero essentially removing ensemble 
previous showed negative correlation learning uses ambiguity term penalty error function network 
means optimise ensemble performance tuning emphasis diversity error function strength parameter 
opitz selected feature subsets ensemble members train genetic algorithm ga ambiguity fitness function showed gains bagging adaboost classification datasets uci repository 
precursor opitz shavlik algorithm fitness function optimise network topologies composing ensemble 
interestingly ga approaches strength parameter vary emphasis diversity 
difference nc nc incorporates ambiguity backpropagation weight updates trains standard backpropagation selects networks error diversity 
summary ambiguity utilised ways pattern selection feature selection optimising topologies networks ensemble optimising combination function optimising network weights 
quantify classification diversity 
stated earlier diversity problem really quantifying correlation classifier outputs 
natural analogue bias variance covariance decomposition 
step understanding question taken considering bias variance covariance decomposition comes falls neatly bias variance decomposition ensemble error 
classification data point correct incorrect zero loss function usual quadratic loss function journal information fusion regression context 
number authors attempted define bias variance decomposition zero loss functions assumptions shortcomings 
domingos james propose general definitions include original quadratic loss function special case 
leads naturally ask question exist analogue bias variance covariance decomposition applies zero loss functions 
formulation covariance term major stepping stone understanding role classification error diversity 
optimal classification error diversity understood terms trade zero loss functions 
article reviewed existing qualitative quantitative explanations error diversity affects error ensemble 
clear specification problem covered diversity regression classification contexts 
described prominent theoretical results regression ensembles ambiguity decomposition bias variance covariance decomposition 
demonstrated believe formal link making explicit relate 
described current state art formalising concept diversity classification ensembles illustrating clearly problem quantifying form correlation non ordinal predictor outputs 
process suggested modification existing heuristic measure classification error diversity accounts variability test data 
believe allows fine grained judgement ensemble performing 
addition suggested directions take understanding formal grounding diversity studies bias variance covariance decomposition generalised bias variance decomposition zero loss 
subject current research 
main contribution article thorough survey categorisation literature ensemble techniques choose encourage diversity 
techniques choose explicitly enforce diversity metric implicitly encourage diversity methods 
grouped techniques factors initialise learners hypothesis space space accessible hypotheses space traversed learning algorithm 
note taxonomy bound subject heated debate healthy field believe categorisation help identify gaps exciting rapidly expanding field 
kuncheva whitaker measures diversity classifier ensembles machine learning 
krogh vedelsby neural network ensembles cross validation active learning nips 
ueda nakano generalization error ensemble estimators proceedings international conference neural networks pp 

journal information fusion friedman popescu importance sampling learning ensembles tech 
rep stanford university september 
url www stat stanford edu ftp isle pdf perrone improving regression estimation averaging methods variance reduction extensions general convex measure optimization ph thesis brown university institute brain neural systems 
hashem optimal linear combinations neural networks ph thesis school industrial engineering university purdue 
bates granger combination forecasts operations research quarterly 
granger combining forecasts years journal forecasting 
clemen combining forecast review annotated bibliography international journal forecasting 
breiman bagging predictors machine learning 
geman bienenstock doursat neural networks bias variance dilemma neural computation 
brown diversity neural network ensembles ph thesis school computer science university birmingham 
url www cs bham ac uk research tumer ghosh error correlation error reduction ensemble classifiers connection science 
roli linear combiners classifier fusion theoretical experimental results proc 
int 
workshop multiple classifier systems lncs springer guildford surrey pp 

url link springer de link service series tocs htm roli analysis linear order statistics combiners fusion imbalanced classifiers proc 
int 
workshop multiple classifier systems lncs springer italy pp 

url link springer de link service series tocs htm tumer ghosh analysis decision boundaries linearly combined neural classifiers pattern recognition 
tumer ghosh theoretical foundations linear order statistics combiners neural pattern classifiers tech 
rep tr computer vision research center university texas austin 
hansen salamon neural network ensembles ieee transactions pattern analysis machine intelligence 
journal information fusion breiman random forests random features tech 
rep university california berkley dept statistics 
kuncheva elusive diversity classifier ensembles conference pattern recognition image analysis available lncs volume pp 

sharkey sharkey combining diverse neural networks knowledge engineering review 
cunningham carney diversity versus quality classification ensembles feature selection lncs european conference machine learning vol 
springer berlin pp 

cunningham diversity preparing ensembles classifiers different feature subsets minimize generalization error lecture notes computer science 
kuncheva whitaker duin limits majority vote accuracy classifier fusion pattern analysis applications 
kuncheva whitaker duin independence combining proceedings th international conference pattern recognition barcelona spain pp 

kuncheva generating classifier outputs fixed accuracy diversity pattern recognition letters 
kuncheva whitaker measures diversity classifier ensembles limits classifiers iee workshop intelligent sensor processing iee birmingham uk 
kuncheva relationships combination methods measures diversity combining classifiers information fusion 
kuncheva duin bagging boosting nearest mean classifier effects sample size diversity accuracy proc 
int 
workshop multiple classifier systems lncs springer italy pp 

url link springer de link service series tocs htm dietterich pruning adaptive boosting proc 
th international conference machine learning morgan kaufmann pp 

url citeseer nj nec com pruning html freund schapire experiments new boosting algorithm proceedings th international conference machine learning morgan kaufmann pp 

sharkey multi net systems springer verlag ch 
combining artificial neural nets ensemble modular multi net systems pp 

journal information fusion liu negative correlation learning evolutionary neural network ensembles ph thesis university college university new south wales australian defence force academy canberra australia 
rosen ensemble learning decorrelated neural networks connection science special issue combining artificial neural networks ensemble approaches 
opitz maclin popular ensemble methods empirical study journal artificial intelligence research 
sharkey sharkey searching weight space backpropagation solution types current trends connectionism proceedings swedish conference connectionism 
partridge yates engineering multiversion neural net systems neural computation 
yates partridge methodological diversity improve neural network generalization neural computing applications 
url citeseer nj nec com partridge html munro doyle improving committee diagnosis resampling techniques advances neural information processing systems 
maclin shavlik combining predictions multiple classifiers competitive learning initialize neural networks proceedings th international joint conference artificial intelligence montreal canada pp 

fast committee learning preliminary results electronics letters 
sharkey sharkey test select approach ensemble combination proc 
int 
workshop multiple classifier systems lncs springer italy pp 

duin tax experiments classifier combining rules proc 
int 
workshop multiple classifier systems lncs springer italy pp 

url link springer de link service series tocs htm sharkey sharkey diversity selection ensembles artificial neural nets neural networks applications pp 

sharkey sharkey diverse neural net solutions fault diagnosis problem neural computing applications 
raviv intrator bootstrapping noise effective regularisation technique connection science 
journal information fusion breiman randomizing outputs increase prediction accuracy technical report statistics department university california may 
url www boosting org papers bre pdf boosting averaged weight vectors proc 
int 
workshop multiple classifier systems lncs springer guildford surrey pp 

melville mooney constructing diverse classifier ensembles artificial training examples proceedings eighteenth international joint conference artificial intelligence mexico pp 

tumer input decimation ensembles decorrelation dimensionality reduction proc 
int 
workshop multiple classifier systems lncs springer cambridge uk pp 

url link springer de link service series tocs htm tumer dimensionality reduction classifier ensembles tech 
rep nasa arc ic nasa ames labs 
liao moody constructing heterogeneous committees input feature grouping advances neural information processing systems 
haykin neural networks comprehensive foundation macmillan new york 
wan combining fossil sunspot data committee predictions international conference neural networks icnn 
url citeseer ist psu edu html dietterich bakiri error correcting output codes general method improving multiclass inductive learning programs dean mckeown eds proceedings ninth aaai national conference artificial intelligence aaai press menlo park ca pp 

kong dietterich error correcting output coding corrects bias variance proceedings th international conference machine learning morgan kaufmann pp :10.1.1.57.5909

partridge network generalization differences quantified neural networks 
url citeseer nj nec com partridge network html opitz shavlik generating accurate diverse members neural network ensemble nips 
islam yao murase constructive algorithm training cooperative neural network ensembles ieee transactions neural networks 
wang jones partridge diversity neural networks decision trees building multiple classifier systems proc 
int 
workshop multiple classifier systems lncs springer italy pp 

url link springer de link service series tocs htm journal information fusion langdon barrett buxton combining decision trees neural networks drug discovery genetic programming proceedings th european conference ireland pp 

woods kegelmeyer bowyer combination multiple local accuracy estimates ieee transactions pattern analysis machine intelligence 
tikhonov arsenin solutions ill posed problems winston sons washington 
krogh learning ensembles overfitting useful 
modelling conditional probabilities network committees overfitting useful neural network world 
liu yao negatively correlated neural networks produce best ensembles australian journal intelligent information processing systems 
liu yao ensemble learning negative correlation neural networks 
brown wyatt ambiguity decomposition neural network ensemble learning methods fawcett mishra eds th international conference machine learning icml washington dc usa 
mckay measures genetic programming japan workshop intelligent evolutionary systems pp 

mckay analyzing ensemble learning proceedings conference artificial neural networks expert systems new zealand pp 

koza genetic programming programming computers means natural selection mit press 

ahn 
cho neural networks evolved fitness sharing technique proceedings congress evolutionary computation seoul korea pp 

darwen yao speciation automatic categorical modularization ieee trans 
evolutionary computation 
khare yao artificial speciation neural network ensembles ed proc 
uk workshop computational intelligence university birmingham uk pp 

yao liu making population information evolutionary artificial neural networks ieee transactions systems man cybernetics part cybernetics vol 
ieee press pp 

ho data complexity analysis classifier combination proc 
int 
workshop multiple classifier systems lncs springer cambridge uk pp 

url link springer de link service series tocs htm journal information fusion sharkey types multinet system proc 
int 
workshop multiple classifier systems lncs springer italy pp 

url link springer de link service series tocs htm opitz feature selection ensembles proceedings th national conference artificial intelligence aaai pp 

kohavi wolpert bias plus variance decomposition zero loss functions saitta ed machine learning proceedings thirteenth international conference morgan kaufmann pp 

breiman bias variance arcing classifiers tech 
rep statistics department university california berkeley 
friedman bias variance loss curse dimensionality tech 
rep stanford university 
domingos unified bias variance decomposition applications proc 
th international conf 
machine learning morgan kaufmann san francisco ca pp 

url citeseer nj nec com domingos unified html james variance bias general loss functions machine learning 
