spectral learning kamvar dan klein christopher manning computer science dept computer science dept stanford university stanford university stanford university stanford ca stanford ca stanford ca cs stanford edu klein cs stanford edu manning cs stanford edu interested reader model simple easily implemented spectral learning algorithm applies equally supervisory information pairwise link constraints labeled examples 
unsupervised case performs consistently spectral clustering algorithms 
supervised case approach achieves high accuracy categorization thousands documents dozen labeled training documents newsgroups data set 
furthermore classification accuracy increases addition unlabeled documents demonstrating effective unlabeled data 
normalized affinity matrices symmetric stochastic obtain probabilistic interpretation method certain guarantees performance 
spectral algorithms information contained eigenvectors data affinity item item similarity matrix detect structure 
approach proven effective tasks including information retrieval deerwester web search page kleinberg image segmentation shi word class detection brew schulte im walde data clustering ng 
spectral algorithms useful unsupervised learning clustering little done developing spectral algorithms supervised learning classification 
consider adaptation spectral clustering methods classification 
method combining item similarities supervision information produce markov transition process data items 
call markov process interested reader model appeal special case text clustering classification 
algorithm incorporates supervisory information available form pairwise constraints labeled data 
empirically algorithm achieves high accuracy supplied small amounts labeled data section small numbers pairwise constraints section 
propose markov chain model similar spirit random surfer model page 
description motivated context text categorization model depends notions pairwise data similarity completely general 
model collection documents possibly unknown topic 
reader begins document interest continues read successive documents 
chooses document read tries read document topic prefer documents similar current document 
mapping similarities transition probabilities chosen describe specific choice section 
transition probabilities define markov chain documents collection 
exist distinct topic areas document set generally clusters data markov chain composed subsets high intra set transition probabilities low inter set transition probabilities 
refer subsets cliques 
cliques corresponds topic text clustering problem 
course natural clusters data need perfectly compatible document labels said supervision information 
section supervision override similarity transition 
example disallow transition documents known regardless pairwise similarity 
spectral clustering algorithms section discuss process turning affinity matrix pairwise document similarities normalized markov transition process eigenvectors detect blocks near blocks correspond clusters data 
note important difference way models random surfer model left eigenvector transition matrix indicates relative amount time process spends data item 
hand interested right eigenvectors transition matrix straightforwardly relate near block structure transition matrix 
learning clustering 
treating row point cluster clusters means sensible clustering algorithm 

assign original point xi cluster row assigned cluster classification 
represent data point row xi 
classify rows points reasonable classifier trained labeled points 

assign data point class assigned 
spectral learning algorithm 
algorithm normalization divisive symmetric divisive ad lsa sl normalized additive dmax table normalizations spectral methods 
calculating transition matrix order fully specify data data markov transition matrix map document similarities transition probabilities 
affinity matrix documents elements aij similarities documents documents points xi distance function common definition aij free scale parameter 
lsa deerwester row normalized term document matrix defined cosine similarity matrix salton 
may map document similarities transition probabilities ways 
define meila shi diagonal matrix elements corresponds transitioning probability proportional relative similarity values 
alternatively define fiedler chung dmax maximum transition probabilities sensitive absolute similarity values 
example document similar interested reader may keep reading document repeatedly move document 
normalizations plausible chose slight empirical performance benefits data 
meila shi shown probability transition matrix markov chain strong cliques piecewise constant eigenvectors suggest clustering finding approximately equal segments top eigenvectors 
algorithm uses general method spectral learning means news news lymphoma soybean table comparison spectral learning 
details differ algorithm shown 
algorithm similar algorithm ng call authors 
fact difference type normalization 
differences algorithm meila shi normalization different additionally docs row normalize step 
table describes different types normalizations mentions algorithms 
noted data sets distant outliers additive normalization lead poor performance 
additive normalization outliers clique 
clusters represent outliers true clusters 
dataset distant outliers divisive normalization lead better performance 
parameter selection importance parameter selection overlooked presentation standard spectral clustering methods 
different values results spectral clustering vastly different 
ng parameter chosen value gives distorted clusters 
text experiments data term document matrix similarity function gave pairwise cosine similarities entry aij set zero top nearest neighbors reverse 
thresholding affinity matrix manner useful spectral methods empirically better zeros affinity matrix pairs items class 
experiments chose may learn optimal manner ng learn optimal scale factor empirical results compared spectral learning algorithm means data sets newsgroups collection approximately postings usenet newsgroups 
newsgroups newsgroups sci crypt talk politics mideast soc religion christian 
lymphoma gene expression profiles normal malignant lymphocyte samples 
classes diffuse large cell lymphoma samples non samples alizadeh 
www ai mit edu newsgroups total documents 
documents stripped headers stopwords converted lowercase 
numbers discarded 
words occur documents removed 
learning soybean soybean data set uci repository 
classes 
results shown table 
numbers reported adjusted rand index values hubert arabie clusters output algorithms 
rand index frequently evaluating clusters pairs placed different clusters partitionings 
adjusted rand index ranges key property expected value random clustering 
result spectral methods generally perform better means consistent results ng ai brew schulte im walde 
cases poor performance means reflects inability cope noise dimensions especially case text data highly non spherical clusters case composite negative cluster lymphoma 
spectral learning outperforms means soybean low dimensional multi class data set 
spectral classification previous section described clustering data set creating markov chain similarities data items analyzing dominant eigenvectors resulting markov matrix 
section show classify data set making changes 
modify markov chain class labels known override underlying similarities 
second classification algorithm spectral space clustering algorithm 
modifying interested reader model model described section modified incorporate labeled data simple manner 
interested reader happens labeled document probability choose labeled document category high probability choose labeled document different category low zero 
transition probabilities unlabeled documents proportional similarity current source document current document labeled 
wc wish create markov matrix reflects modified model 
propose doing manner normalization introduced section 
similarity functions maximum pairwise similarity value minimum similarity 
say points class maximally similar points different classes minimally similar arc instances features different classes 
nominal hamming distance 
sets means numbers low 
partially illusory due zeroed expectation adjusted rand index partially real consequence sparse text data 
better means results text typically require kind aggressive dimensionality reduction usually lsa spectral method careful feature selection 

define affinity matrix previous algorithms 

pair points class assign values aj 
likewise pair points different classes assign values aij aji 
gives symmetric markov matrix describing interested reader process uses supervisory information data similarities 
strength model lies fact incorporates unlabeled data majority classification models deal strictly labeled data 
benefit additive normalization affinities adjusted labeled pairs higher equal mutual transition probability unlabeled pairs 
necessarily case normalization schemes 
spectral classification algorithm natural classes occur data markov chain described cliques 
furthermore cliques stronger number labeled documents increases 
model wish categorize documents assigning appropriate clique markov chain 
spectral clustering methods section adapted classification replacing final steps clustering spectral space steps shown classify spectral space 
key differences spectral classifier clustering algorithm transition matrix incorporates labeling information wc classifier spectral space clustering method 
novel algorithm able classify documents similarity transition probabilities known subsets model incorporates labeled unlabeled data improve addition labeled data addition unlabeled data 
observe empirically section 
empirical results cliques suggested section markov chain described cliques subsets nodes graph internally fast mixing mutually fast mixing 
shows thresholded sparsity pattern affinity matrices data set labeled data added 
left matrix affinity matrix labeled data 
underlying similarities show block behavior weakly 
extent unlabeled data gives block affinity matrix clusters naturally exist data basis spectral clustering 
subsequent matrices increasing fractions data labeled 
effect adding labeled data sharpen coerce natural clusters desired classes 
labels added blocks clearer cliques stronger limit labeled data interested reader jump document topic 
learning categorization accuracy newsgroups task number unlabeled labeled points increases 
labeled documents number unlabeled training set 
training set news groups fraction labeled 
cases test set run consisted documents newsgroups labels known training run 
categorization accuracy newsgroups task amount unlabeled data fraction labeled data increases 
accuracies validate utility spectral classification performed experiments newsgroups data set 
built batch classifiers 
standard multinomial naive bayes nb classifier standard smoothing 
second spectral classifier described single nearest neighbor classifier perform final classification spectral space 
affinity matrix thresholded cosine similarity documents 
note thresholding important weakens effect outliers 
furthermore saves space computation time resulting affinity matrix sparse 
split data labeled training set unlabeled test set 
classification spectral classifier processed training test set evaluated test set 
shows effect sample documents newsgroups data labeled training set increasing number unlabeled documents test set 
accuracy nb classifier course constant sampling variation discards unlabeled data 
spectral classifier accurate nb classifier sufficiently additional unlabeled document take similar documents put similarities appropriate row column 
entries 
documents incorporate 
shows effect supplying increasingly large fractions newsgroups data set labeled training instances remainder data set test instances 
spectral classifier outperforms naive bayes substantially little labeled data 
figures show graphs news groups data set 
spectral classification performs especially data labeled 
noticed data set naive bayes outperforms spectral classifier strongly supervised case labeled data 
strength spectral learning lies incorporating unlabeled data strongly supervised case value 
spectral space investigate behavior spectral classifier wc performed experiment 
took newsgroups data labeled various fractions classes 
plotted document position resulting dimensional spectral space space rows matrix defined spectral learning algorithm 
shows dimensions plots 
labels data tightly cluster 
add labels circled points things happen 
labeled points move close labeled points away different labeled points generally outside hubs markov process 
second pull unlabeled points radially 
effective pull classes apart classes naturally strong clusters unlabeled data 
related yu shi spectral grouping method call grouping bias allows top level bias labeled data 
formulate problem constrained optimization problem optimal partition sought subject constraint normalized cut values nodes preassigned class 
main drawback algorithm yu shi constrains labeled data points 
algorithm benefits zeros sparsity pattern introduced differently labeled pairs 
furthermore noted multi class case labeled sets tend embody differently labeled pairs labeled pairs 
drawback information differently labeled points trivial partition points cluster satisfy constraints points labeled 
fact data labeled partition grouping bias algorithm trivial partition 
shows spectral classifier outperforms grouping bias algorithm newsgroups data set 
fact grouping bias started performing slightly worse large fraction data labeled 
learning classes newsgroups data set spectral space increasing mounts labeled data 
classes sci crypt pluses talk politics dots soc religion christian squares 
labeled points circled 
bottom graphs show sparsity patterns associated affinity matrices 
constrained spectral clustering 
links items known class 

links items different classes 
interest constrained clustering constrained clustering allows exploratory data wagstaff cardie klein analysis prior knowledge class clustering types pairwise constraints 
classifier section easily modi learning classification accuracy vs fraction pairs constrained spectral classifier vs grouping bias rand index spectral constrained clustering vs fraction pairs constrained shown constrained 
newsgroups data 
fied kind prior knowledge reducing labeling information pairwise information affinity modification 
specifically affinity matrix defined follows 
define affinity matrix 

pair linked points assign val equivalent imposing constraints step klein 
step linked points similar pair points data set linked points dissimilar pair points data set 
spectral constrained clustering algorithm proceeds just spectral clustering algorithms difference uses modified normalized affinity matrix section 
shows plot accuracy vs number constraints newsgroups data set spectral constrained clustering 
accuracy measured constrained rand index klein cardie 
accuracy increases number constraints showing spectral constrained clustering effectively constraints better cluster data 
probabilistic model associated spectral learning algorithm able unsupervised semi supervised fully supervised learning problems 
show algorithm able cluster able effectively utilize prior knowledge pairwise constraints labeled examples 
acknowledgments research supported part national science foundation research collaboration ntt communication science laboratories telegraph telephone csli stanford university research project concept bases lexical acquisition intelligently reasoning meaning 
alizadeh alizadeh 
distinct types diffuse large lymphoma identified gene expression profiling 
nature 
brew schulte im walde brew schulte im walde 
spectral clustering german verbs 
proceedings 
chung chung 
spectral graph theory 
ams providence ri 
scott deerwester susan dumais thomas landauer george furnas richard harshman 
indexing latent semantic analysis 
journal american society information science 
fiedler fiedler 
property eigenvectors nonnegative symmetric matrices application graph theory 
mathematical journal 
hubert arabic hubert arabic 
comparing partitions 
journal classification 
klein etal dan klein kamvar christopher manning 
instance level constraints space level constraints making prior knowledge data clustering 
nineteenth international conference machine learning 
kleinberg kleinberg 
sources environment 
proceedings th acm siam symposium discrete algorithms 
shi marina meila jianbo shi 
learning segmentation random walks 
advances neural information processing systems pages 
shi marina meila jianbo shi 
random walks view spectral segmentation 
aj statistics ais 
ng andrew ng michael jordan yair weiss 
spectral clustering analysis algorithm 
proceedings advances neural information processing systems nips 
page lawrence page sergey brin rajeev motwani terry winograd 
pagerank citation ranking bringing order web 
technical report stanford digital library technologies project 
salton salton 
automatic text processing analysis retrieval information computer 
addison wesley 
cardie wagstaff claire cardie 
clustering instance level constraints 
seventeenth international conference machine learning pages 
yu shi stella yu jianbo shi 
grouping bias 
technical report cmu ri tr robotics institute carnegie mellon university pittsburgh pa july 
learning 
