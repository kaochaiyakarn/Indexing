appears adv 
lang 
comp 
par 
proc banerjee gelernter nicolau padua ed pro le information assist advanced compiler optimization scheduling william chen scott mahlke nancy warter richard hank roger wen mei hwu compilers superscalar vliw processors expose su cient instruction level parallelism order achieve high performance 
compiletime code transformations expose instruction level parallelism typically take account constraints imposed execution scenarios program 
additional opportunities increase parallelism frequent execution scenario expense frequent execution sequences 
pro le information identi es important execution sequences program 
major categories pro le information studied control ow memory dependence 
pro transformations incorporated impact compiler 
transformations include global optimization acyclic global scheduling software pipelining 
ectiveness pro le techniques evaluated range superscalar vliw processors 
compile time code transformations code optimization scheduling typically take account constraints imposed possible execution scenarios program 
usually achieved static analysis methods live variable analysis reaching de nitions de ne chains data dependence analysis 
static analysis methods distinguish frequent infrequent execution scenarios 
conservative approach ensures correctness transformations may overlook opportunities optimize frequent scenarios constraints infrequent scenarios 
problem addressed pro le information estimates frequency scenario assist compiler code transformations 
pro le information assist compile time code transformations received increasing attention research product development communities 
appears adv 
lang 
comp 
par 
proc banerjee gelernter nicolau padua ed area handling conditional branches pipelined processors shown pro branch prediction compile time performs best hardware schemes 
area global code scheduling trace scheduling popular global microcode compaction technique 
trace scheduling ective compiler able identify frequently executed sequences basic blocks ow graph 
shown pro ling ective way 
instruction placement code optimization arranges basic blocks ow graph particular linear order maximize sequential locality reduce number executed branch instructions 
shown pro ling ective way guide instruction placement 
pro le information help register allocator identify frequently accessed variables 
function inline expansion eliminates overhead function calls enlarges scope global code optimizations 
pro le information compiler identify frequently invoked calls determine best expansion sequence 
area classic code optimization eliminating constraints imposed frequent execution paths infrequent paths control ow pro ling shown improve performance classic global loop optimizations 
methods obtain pro le information improve performance compile time code transformations expose instruction level parallelism ilp 
objective improve performance superscalar vliw processors helping compiler expose exploit ilp 
particular presents methods take advantage major categories pro le information control ow memory dependence 
control ow pro ling instrumentation provides compiler relative frequency alternative execution paths 
presents methods allow code optimizations acyclic global scheduling software pipelining focus frequent program execution paths 
systematically eliminating constraints imposed infrequent execution paths program performance improved better parallelization scheduling frequent paths 
memory dependence pro ling instrumentation summarizes frequency address matching con icts memory 
information allows compiler optimize reorder pair memory presence inconclusive memory dependence analysis 
run time instrumentation indicates memory address compiler perform code reordering optimization assuming con ict 
performing code reordering compiler generates repair code correct execution state con ict occur 
special hardware support detect occurrence con icts invoke repair code run time 
demonstrate ectiveness control ow pro ling memory dependence modi ed optimizer acyclic global code scheduler software impact compiler take advantage pro le information 
describes modi ed optimization scheduling methods explains usefulness information quanti es bene pro le information code transformations 
appears adv 
lang 
comp 
par 
proc banerjee gelernter nicolau padua ed control flow pro ling compilers usually represent control ow function ow graph 
nodes ow graph represent basic blocks function arcs represent possible control ow transfers basic blocks 
control ow pro ling ow graph augmented weights 
weight associated node represent number times basic block executed 
weight associated arc represent number times control transfer taken 
ow graph augmented node arc weights referred ow graph 
section discusses advantages control ow pro le information global optimization global scheduling superblock structure 
superblock optimization scheduling ective exposing exploiting ilp general purpose applications 
control ow pro le information software pipelining 
software pipelining ective technique scheduling loops numerical applications vliw superscalar processors 
superblock ective structure utilize control ow pro le information compiler optimization scheduling superblock 
superblock sequence instructions control may enter top may leave exit points 
equivalently superblock linear sequence basic blocks control may enter rst basic block 
superblocks occur functions naturally superblocks typically size natural superblocks small 
compiler form larger superblocks utilizing weighted ow graph 
superblock formation consists steps 
traces identi ed function 
trace set basic blocks execute sequence 
traces selected identifying seed node weighted ow graph growing trace forward backward preceding succeeding nodes predecessor successor predecessor successor placed trace 
basic block member exactly trace 
example trace selection applied weighted ow graph shown 
example traces identi ed code segment consisting basic blocks 
second create superblocks traces control entry points middle trace eliminated 
side entrances eliminated duplicating set basic blocks trace 
set union blocks side entry points blocks trace control may subsequently transfer 
control transfers side trace moved corresponding duplicated basic block 
process converting traces superblocks referred tail duplication 
example tail duplication shown 
trace consisting basic blocks contains control entry points basic block tail duplication replicates basic block basic block adjusts control transfers basic blocks basic block removing side entrances trace superblock 
superblocks optimization scheduling superscalar vliw processors discussed sections 
appears adv 
lang 
comp 
par 
proc banerjee gelernter nicolau padua ed example superblock formation trace selection tail duplication 
superblock optimizations order utilize parallel hardware superscalar vliw processors su cient ilp exposed code optimizer 
large number compiler optimizations applied superblocks increase ilp 
superblock ilp optimizations divided categories superblock enlarging optimizations superblock optimizations 
purpose superblock enlarging optimizations increase size frequently executed superblocks scheduler larger number instructions manipulate 
scheduler nd independent instructions schedule cycle superblock instructions choose 
important feature superblock enlarging optimizations frequently executed parts program enlarged 
selective enlarging strategy keeps code expansion control 
superblock enlarging optimizations currently utilized branch target expansion loop peeling loop unrolling 
branch target expansion appends copy target superblock superblock ends taken control transfer 
superblock loop peeling loop unrolling replicate body superblock loops 
superblock loop superblock ends control transfer 
superblock loops unrolled peeled weight superblock expected number times superblock loop iterate 
purpose superblock dependence removing optimizations eliminate data dependences instructions frequently executed superblocks 
optimizations directly increase amount ilp available code scheduler 
side ect appears adv 
lang 
comp 
par 
proc banerjee gelernter nicolau padua ed optimizations require additional instructions inserted superblock entry exit points 
applying optimizations frequently executed superblocks code expansion regulated 
superblock dependence removing optimizations currently utilized register renaming induction variable expansion accumulator variable expansion operation migration operation combining tree height reduction 
register renaming common technique remove anti output dependences instructions 
induction variable expansion accumulator variable expansion remove ow anti output dependences de nitions induction accumulator variables unrolled superblock loop bodies 
temporary induction accumulator variables created remove dependences 
operation migration moves instruction important superblock important superblock instruction result directly home superblock 
operation combining eliminates ow dependences pairs instructions constant source operand 
tree height reduction exposes ilp computation arithmetic expressions 
ectiveness superblock optimizations superscalar vliw processors experimentally evaluated section 
superblock scheduling ciently schedule code superscalar vliw processors necessary schedule instructions basic blocks 
instructions moved conditional branches 
superblocks provide ective framework schedule instructions important paths execution 
superblock scheduling consists steps dependence graph construction followed list scheduling 
dependence graph contains types dependences ow anti output control 
control dependences initially inserted instruction preceding branches superblock 
scheduling model underlying processor support speculative execution control dependences removed allow code motion exibility 
list scheduling instruction latencies resource constraints applied superblock determine nal schedule 
code scheduler moves instructions superblock branches achieve cient schedule 
non control branch subroutine call instruction moved branch br provided copy placed target br location writes live br taken 
equivalently dest live br move br copy placed target br 
non control instruction moved branch br provided restrictions met restriction dest isnotin live br 
restriction cause exception may terminate program execution 
rst restriction eliminated su cient compiler renaming support 
second restriction di cult eliminate 
conventional processors memory load memory store integer divide oating point instructions may moved branches superblock compiler prove instruction cause exception 
architectural support form non trapping versions instructions normally trap utilized remove restriction 
scheduler moves possible appears adv 
lang 
comp 
par 
proc banerjee gelernter nicolau padua ed benchmark size lines benchmark description cccp gnu preprocessor cmp compare les compress compress les eqn format math formulas tro eqntott boolean equation minimization espresso truth table minimization grep string search lex lexical analyzer generator tbl format tables tro wc word count xlisp lisp interpreter yacc parser generator table benchmarks 
trapping instruction branch superblock converts instruction counterpart 
instructions depend possible trapping instruction moved branch 
exception condition exists non trapping instruction garbage value written destination instruction 
programs trapped scheduled conventional techniques garbage value ect correctness program results instructions moved branch branch taken 
programs code programs rely traps normal operation errors caused trap may cause exception trapping instruction may cause incorrect result 
superblock scheduling non trapping support referred general code percolation 
part promising techniques allow code scheduling exibility general percolation ignoring exceptions investigated 
experimental results superblock optimization scheduling implemented impact compiler 
impact compiler prototype optimizing compiler designed generate cient code superscalar vliw processors 
study ectiveness superblock optimization superblock scheduling execution driven simulation performed range superscalar vliw processors 
benchmarks study described table 
benchmark pro led variety inputs obtain fair control ow pro le information 
input di erent pro ling perform simulation experiments reported 
basic processor study risc processor instruction set similar mips 
processor assumed register interlocking deterministic instruction latencies table 
processor contains integer registers oating point registers 
furthermore architectural support code scheduling general percolation assumed 
performance superblock optimizations scheduling compared superscalar vliw processors issue rates 
issue rate maximum number appears adv 
lang 
comp 
par 
proc banerjee gelernter nicolau padua ed speedup instruction class latency instruction class latency integer alu fp alu integer multiply fp conversion integer divide fp multiply branch fp divide memory load memory store superblock optimization superblock formation traditional optimizations aa aa aa table instruction latencies 
aa aaa aa aa aa aa aa aaa aa aaa aa aa aa aa aa aaa aaaaa aa aaa aaa aa aaaaa aa aaaa aa aa aaa aaaa aaaa aa aa aaa aa aa aaa aa aaaa aaaaa aaa aaa aaaaa aa aaaa aa aa aaa aa aa aa aa aa aaaaa aa aa aa aaa aa aa aaaa aa aa aa aa aa aa aa aaa aa aa aa aa aaaa cccp cmp compress eqn eqntott espresso grep lex li tbl wc yacc performance improvement resulting superblock optimization superblock scheduling 
instructions processor fetch issue cycle 
limitation placed combination instructions issued cycle 
machine con guration program execution time assuming cache hit rate reported speedup relative program execution time base machine con guration 
base machine con guration issue processor traditional optimization support basic block 
traditional optimizations include conventional local global loop optimizations utilize control ow pro le information 
furthermore optimizations utilize control ow pro le information included traditional function inline expansion register allocation instruction placement branch prediction 
performance improvement due superblock formation scheduling improvement due superblock optimization shown 
superblock formation optimization applied addition traditional optimizations 
gure seen superblock techniques substantially increase performance superscalar vliw processors 
traditional optimization scheduling support little performance improvement base processor achieved 
superblock formation signi cant performance improvements observed benchmarks due increased number instructions scheduler examines time 
example issue processor superblock formation scheduling provide average performance gain traditional compiler optimization scheduling 
superblock optimization applied addition superblock formation provides larger performance improvements 
issue processor average improvement traditional compiler optimization appears adv 
lang 
comp 
par 
proc banerjee gelernter nicolau padua ed scheduling observed 
importance superblock optimization scheduling apparent rate processors 
processors compiler expose large amount ilp fully utilize existing resources 
traditional compiler support little performance gain observed increasing issue rate 
superblock optimization scheduling increased level ilp exposed take better advantage higher issue rates 
assisting software pipelining software pipelining compile time scheduling technique exploits vliw superscalar processors overlapping execution loop iterations 
overlapping loops conditional constructs constructs potential large amount code expansion 
time conditional constructs overlap number execution paths double 
software pipeline schedule overlaps copies conditional construct di erent iteration loop code expansion 
existing techniques code expansion 
technique modulo scheduling conversion incur code expansion 
method requires architectural support predicated execution 
alternative modulo scheduling technique uses hierarchical reduction 
technique modi ed conditional construct di erent iterations overlap 
code expansion comes duplicating code scheduled conditional construct overlapping di erent conditional constructs loop body 
loop body conditional constructs may code expansion due overlapping conditional constructs 
technique relatively low code expansion require special architectural support improve performance hierarchical reduction restriction conversion 
control ow pro ling information improve performance modied hierarchical reduction modulo scheduling technique 
general software pipelining method scheduling numeric loops loops dependence cycles fully overlapped 
dependence cycles loops partially overlapped 
numeric codes branches detect boundary exception conditions branch infrequently executed 
cases infrequently executed path typically takes longer execute 
pro le information remove longer execution path pipelined loop 
section review modulo scheduling hierarchical reduction discuss technique modied reduce code expansion 
show pro ling improve performance modi ed hierarchical reduction technique compare results conversion 
modulo scheduling modi ed hierarchical reduction modulo scheduling interval loop iterations initiated iteration interval ii xed iteration loop 
algorithm determines lower bound ii tries nd schedule ii 
algorithm rst tries schedule operations involved dependence cycles list schedules remaining operations 
appears adv 
lang 
comp 
par 
proc banerjee gelernter nicolau padua ed schedule ii incremented process repeated schedule ii reaches prede ned limit 
minimum ii determined resource dependence cycle constraints 
focus loops dependence cycles 
dependence cycles modulo scheduling consists basic steps 
data dependence graph constructed 
dependence cycles graph intra iteration dependencies 
second lower bound ii determined heavily utilized resource loop 
iteration uses resource cycles copies resource minimum ii due resource constraints rii rii max set resources 
step list schedule loop body modulo resource reservation table 
modulo resource reservation table row cycle iteration interval column function unit 
operation scheduled time necessary functional unit free row mod ii modulo resource reservation table 
order modulo schedule loops conditional branches control dependences converted data dependences 
architectural support predicated execution conversion convert control dependences data dependences 
hardware support explicitly convert control dependences data dependences hierarchical reduction implicitly converts control dependences encapsulating operations control construct statement pseudo operation pseudo op 
form pseudo op paths control statement list scheduled including conditional branch 
resource pattern pseudo op union resource pattern path 
data dependences operation pseudo op operation outside pseudo op migrated pseudo op associated delays adjusted pseudo op list schedule 
nested control statements control statements hierarchically converted pseudo op 
conditional statements converted pseudo op new data dependence graph modulo scheduled 
avoid exponential code expansion restriction added modulo scheduling requires pseudo op scheduled ii 
table summarizes characteristics modulo scheduling modi ed hierarchical reduction technique conversion 
pro le optimization additional constraint hierarchical reduction requires pseudo op scheduled ii minimum ii long longest path conditional construct 
degrade performance hierarchical reduction compared conversion 
stated earlier numerical programs predictable branches frequently executed path longer frequently executed path 
pro le information remove infrequently executed path 
shows example pro le optimization hierarchical reduction assuming issue vliw processor limitation placed combination operations issued 
operations conditional branch jump operation 
appears adv 
lang 
comp 
par 
proc banerjee gelernter nicolau padua ed feature hierarchical reduction conversion resource constraints union resources sum resources paths paths conditional construct conditional construct additional constraints conditional construct scheduled ii code expansion due duplicate code scheduled conditional constructs overlap di erent conditional constructs hardware support predicated execution table characteristics modi ed hierarchical reduction conversion 
operations scheduled vliw instructions 
basic pro le optimization algorithm follows 
generate weighted graph shown 
remove basic blocks infrequently executed paths path 
second form pseudo node corresponding frequently executed path generate data dependency graph remaining loop operations 
shows data dependence graph dependence distance zero arcs labeled type dependence ow operation latencies 
third modulo schedule operations dependence graph modulo resource reservation table shown 
note operation available cycle functional units available delayed cycle 
fourth duplicate operations scheduled frequently executed paths schedule corresponding infrequently executed operations 
correct jump operation infrequently executed basic block jump instruction instruction frequently executed path 
shown instruction operation rst instruction loop 
resulting loop kernel shown 
kernel execution shown numbers indicate iteration operation 
branch taken operation iteration executed operation 
similar technique conversion 
conversion pro le optimization preserves zero code expansion predicates eliminate need copying operations infrequently executed paths 
experimental results versions modulo scheduling implemented impact compiler modi ed hierarchical reduction technique conversion 
evaluate ectiveness control ow pro ling improve performance techniques execution driven simulation performed range superscalar vliw software pipeline prologue kernel epilogue 
prologue epilogue ll empty software pipeline respectively 
kernel steady state loop execution 
appears adv 
lang 
comp 
par 
proc banerjee gelernter nicolau padua ed modulo resource reservation table cycle fu fu mod ii 

start time modulo scheduling modi ed hierarchical reduction pro le optimization weighted control ow graph data dependence graph modulo schedule kernel schedule kernel execution taken 
processors 
benchmarks study loops selected perfect suite 
loops cross iteration dependences conditional construct 
pro ling optimization applied branches path taken time 
loops assumed execute large number times steady state kernel execution measured modulo scheduled loops 
basic processor study risc processor instruction set similar intel 
intel instruction latencies 
processor nite number registers ideal cache 
performance modulo scheduling techniques modi ed hierarchical reduction conversion pro ling optimization compared superscalar vliw processors issue rates limitation placed combination instructions issued cycle 
conversion architectural support predicated execution assumed 
machine con guration loop execution time reported speedup relative execution time base machine con guration 
base machine con guration issue processor induction variable reversal applied loops modulo scheduling appears adv 
lang 
comp 
par 
proc banerjee gelernter nicolau padua ed speedup profiling profiling aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa hierarchical reduction conversion performance improvement modulo scheduling pro le optimization 
execution time improvement cmp eqn espresso profiling profiling grep qsort wc xlisp yacc performance comparison memory dependence pro ling 
traditional local global optimization support basic block 
base machine support predicated execution 
bene control ow pro le information improve performance modi ed hierarchical reduction technique shown 
speedups calculated harmonic mean 
pro ling improves performance hierarchical reduction scheme approximately issue machine issue machine issue machine 
pro ling optimization conversion average performs approximately better hierarchical reduction issue machine approximately better issue machine 
pro ling performance hierarchical reduction reduced approximately issue issue machines 
note hierarchical reduction pro ling performs better conversion pro ling 
interesting note ect pro le optimization technique 
hierarchical reduction limiting scheduling conditional construct ii limits number iterations overlapped 
constrains speedup technique issue rate increases 
frequently executed path longest path loops pro ling information reduces size pseudo node scheduled ii allows overlap 
hierarchical reduction improvement due pro ling increases issue rate increases 
conversion resource constraints due fetching operations paths condition construct limits performance 
pro ling reduces resource constraints 
particularly bene ts lower issue rates incur resource con icts 
section shown pro ling improve performance modulo scheduling hierarchical reduction conversion 
hierarchical reduction appears adv 
lang 
comp 
par 
proc banerjee gelernter nicolau padua ed pro ling performance low code expansion need additional hardware support 
code expansion best performance architectural support predicated execution pro ling 
memory dependence pro ling shown section superblock optimizations substantially increase performance superscalar vliw processors 
dependences pairs memory remain superblocks 
dependences restrict ability scheduler move loads upward past stores 
loads occur critical paths program loss code reordering opportunities limit ectiveness compiletime code scheduling 
practical limitations current memory techniques dependence arcs memory added conservatively 
independent pairs memory marked dependent dependence analyzer conclusively determine di erent addresses 
dependent memory may address occasionally execution 
dependences removed memory compilation aggressive code reordering done 
memory dependence pro ling uses run time information estimate pairs memory access location 
cases dependence analyzer indicates dependence memory pro le information reports rarely address 
cases reordered dependence repair code provided compiler added maintain correct program execution 
special hardware support called memory con ict bu er mcb reduce overhead detecting reordered dependent memory may cause incorrect program execution invoking repair code 
memory store precedes memory load may access location load dependence analysis indicates dependence conclusively determine pair accesses address store referred ambiguous store respect load 
pair called ambiguous store load pair 
load moved store load preload 
situation preload ambiguous store address referred con ict pair 
scheduling model section instructions preloaded data moved ambiguous store 
con ict occurs computations involving preload destination register retried 
memory dependence pro ling execution repair code ambiguous store load pair con icts adds runtime overhead 
overhead due executing repair code greater bene reordering pair decrease program performance occur 
important reorder pair memory repair code predicted infrequently invoked 
information obtained memory dependence pro ling appears adv 
lang 
comp 
par 
proc banerjee gelernter nicolau padua ed compiler decide bene cial reorder ambiguous store load pair 
simple way implement memory dependence pro ling compare address ambiguous store subsequent load addresses 
require storage execution time pro ler 
solve problem program instrumented measure con icts ambiguous store load pairs scheduler reorder dependence 
current implementation memory dependence pro ling done follows 
code scheduled memory dependence arcs removed ambiguous store load pairs 
second probes inserted compare addresses referenced reordered pairs 
repair code inserted reordered pairs program run correctly pro ling 
program executed 
repair code invocation count reordered ambiguous store load pair collected mapped back superblock data structure 
compiler utilizes con ict frequency con ict threshold value store load pair reordering decisions 
starting original code sequence dependence arcs ambiguous store load pairs code generation done follows 
dependence arc removed ambiguous store load pair meeting con ict threshold criteria 
second code scheduler reorders store load pairs may move instructions dependent ambiguous stores 
third repair code generated check instruction part mcb support described detail placed immediately rst ambiguous store preload moved 
instruction responsible invoking repair code con icts occur 
code scheduler allowed schedule ambiguous stores corresponding check instructions cycle 
fourth virtual register renaming performed preserve source operands repair code overwritten instructions moved stores 
important bene memory dependence pro ling minimizes negative impact added repair code instruction cache performance 
pro le information invocation frequency correction code kept low reduce cache interference 
furthermore placing repair code function compiler reduce wasted fetches code instruction cache 
overview memory con ict bu er section contains overview mcb design 
reader referred technical report details implementation considerations mcb 
mcb hardware supports code reordering detecting con icts invoking repair code supplied compiler restore correctness program execution 
major components mcb hardware consist set address registers store preload addresses compare units match store addresses preload addresses con ict vector having number bits equal number general purpose registers keep track occurrence con icts 
preload executed virtual address saved address register 
store instruction executed virtual address compared valid preload addresses address register le 
match occurs address register bit con ict vector corresponding preloaded register set 
signals need reload register memory appears adv 
lang 
comp 
par 
proc banerjee gelernter nicolau padua ed re execute instructions depend 
con ict bits examined new conditional branch opcode called check instruction 
check instruction executed con ict bit speci ed instruction examined 
con ict bit set processor branches repair code 
repair code re executes preload instructions depend 
branch instruction repair code brings execution back instruction immediately check 
normal execution resumes point 
experimental results section illustrate usefulness memory dependence pro ling non numeric programs cmp eqn espresso grep qsort wc xlisp yacc scheduled reordering ambiguous store load pairs 
program ects reordering pairs memory dependence pro le information studied 
base architecture study issue processor instruction set similar mips 
limitation placed combination instructions issued cycle 
processor assumed register interlocking deterministic instruction latencies table 
processor contains integer registers oating point registers 
superblock optimization scheduling general percolation support assumed 
mcb support provided ambiguous store load pairs reordered 
shows performance comparison issue machine mcb support base architecture 
lighter bar shows percentage performance gain base architecture achieved reordering ambiguous store load pairs utilizing memory dependence pro ling information 
darker bar presents results memory dependence pro ling information 
information gathered memory dependence pro ling scheduling mcb support obtains average performance increase base architecture 
pro ling scheduling mcb support lead performance loss due high invocation frequency repair code 
memory dependence pro ling improves performance program tested 
directions pro ling powerful tool assist compiler making optimization decisions 
developed system exposing exploiting ilp utilizing pro le information 
potential pro ling assist compilation demonstrated series experiments 
control ow pro le information form superblocks assist superblock optimizations perform global code scheduling aid software pipelining 
additional ilp obtained memory dependence pro ling con ict detection hardware reorder dependent memory 
optimizations compilation process bene pro le information 
examples include compiler assisted data prefetching data locality optimizations 
possible bene ts pro ling optimizations discussed 
appears adv 
lang 
comp 
par 
proc banerjee gelernter nicolau padua ed traditional problems associated compiler assisted data prefetching increased instruction count memory bandwidth data cache pollution 
due cache mapping con icts di cult task determine compile time accesses need prefetched prefetch 
prefetching data cache unnecessarily adds prefetch instruction associated address calculation instructions code wastes memory bandwidth 
prefetching data early displace useful data cache 
memory access pro ler gather information data reuse patterns estimate need prefetched 
data locality optimizations include data layout loop distribution 
pro le information frequency di erent data referencing patterns cache data structures arranged memory minimize con icts 
loop distribution separate accesses data structures con ict cache 
cost functions frequency cache block replacement due con icting mapping data structures data reuse pattern aid deciding perform loop distribution 
currently understanding memory access pro ling limited 
investigating advantages information gathered memory access pro ling 
issues worth investigating type information obtainable program execution collect cheaply map back compiler additional optimizations bene information 
hope incorporate ideas impact compiler system 
aho sethi ullman compilers principles techniques tools 
reading ma addison wesley 
mcfarling hennessy reducing cost branches proceedings th international symposium computer architecture pp 
june 
hwu conte chang comparing software hardware schemes reducing cost branches proceedings th international symposium computer architecture pp 
may 
fisher trace scheduling technique global microcode compaction ieee transactions computers vol 
pp 
july 
ellis bulldog compiler vliw architectures 
cambridge ma mit press 
chang hwu trace selection compiling large application programs microcode proceedings st international workshop microprogramming microarchitecture pp 
november 
hwu chang achieving high instruction cache performance optimizing compiler proceedings th international symposium computer architecture pp 
may 
appears adv 
lang 
comp 
par 
proc banerjee gelernter nicolau padua ed pettis hansen pro le guided code positioning proceedings acm sigplan conference programming language design implementation pp 
june 
wall global register allocation link time proceedings sig plan symposium compiler construction pp 
june 
hwu chang inline function expansion compiling realistic programs proceedings acm sigplan conference programming language design implementation pp 
june 
chang mahlke hwu pro le information assist classic code optimizations software practice experience vol 
pp 
december 
chang mahlke chen warter hwu impact architectural framework multiple instruction issue processors proceedings th international symposium computer architecture pp 
may 
kuck kuhn padua wolfe dependence graphs compiler optimizations proceedings th acm symposium principles programming languages pp 
january 
mahlke chen gyllenhaal hwu chang compiler code transformations superscalar high performance systems proceeding supercomputing nov 
nakatani ebcioglu combining compilation technique vliw architectures proceedings nd international workshop microprogramming microarchitecture pp 
september 
kuck structure computers computations 
new york ny john wiley sons 
nix donnell rodman vliw architecture trace scheduling compiler proceedings nd international conference architectural support programming languages operating systems pp 
april 
kane mips risc architecture 
englewood cli nj prentice hall 
aiken nicolau optimal loop parallelization proceedings acm sigplan conference programming language design implementation pp 
june 
ebcioglu compilation technique software pipelining loops conditional jumps micro pp 
december 
appears adv 
lang 
comp 
par 
proc banerjee gelernter nicolau padua ed lam software pipelining ective scheduling technique vliw machines proceedings acm sigplan conference programming language design implementation pp 
june 
su wang new global software pipelining algorithm micro pp 
november 
rau scheduling techniques easily schedulable horizontal architecture high performance scienti computing proceedings th annual workshop microprogramming microarchitecture pp 
october 
allen kennedy porter eld warren conversion control dependence data dependence proceedings th acm symposium principles programming languages pp 
january 
rau yen yen departmental supercomputer ieee computer pp 
january 
control data dependence program transformations 
phd thesis department computer science university illinois urbana il 
warter hwu pro le information assist modulo scheduling tech 
rep center reliable high performance computing university illinois urbana il may 
berry chen kuck lo pang sameh chin schneider fox messina walker lue seidl johnson swanson martin perfect club benchmarks ective performance evaluation supercomputers tech 
rep csrd center supercomputing research development university illinois urbana il may 
intel bit microprocessor 
santa clara ca 
warter hwu bene predicated execution software pipelining proceedings rd hawaii international conference system sci ences january 
nicolau run time disambiguation coping statically unpredictable dependencies ieee transactions computers vol 
pp 
may 
chen mahlke hwu assisting compile time code reordering memory con ict bu er tech 
rep center reliable high performance computing university illinois urbana il may 
