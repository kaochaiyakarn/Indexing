generalized hebbian algorithm incremental singular value decomposition natural language processing department computer information science link ping university link ping sweden ida liu se algorithm generalized hebbian algorithm described allows singular value decomposition dataset learned single observation pairs serially 
algorithm minimal memory requirements interesting natural language domain large datasets datasets quickly intractable 
technique demonstrated task learning word letter bigram pairs text 
dimensionality reduction techniques great relevance field natural language processing 
persistent problem language processing specificity language sparsity data 
techniques depend sufficiency examples order model human language zipfian nature frequency behaviour language means approach diminishing returns corpus size 
short large number ways say thing matter large corpus cover things reasonably said 
language rich task performed example difficult establish documents discussing topic 
likewise matter data system seen training invariably see new run time domain complexity 
approach au natural language processing encounter problem levels creating need techniques compensate 
imagine set data stored matrix 
techniques eigen decomposition allow matrix transformed set orthogonal vectors associated strength eigenvalue 
transformation allows data contained matrix compressed discarding significant vectors dimensions matrix approximated fewer numbers 
meant dimensionality reduction 
technique guaranteed return closest squared error approximation possible number numbers golub 
certain domains technique greater significance 
effectively forcing data bottleneck requiring describe impoverished construct set 
allow critical underlying features reveal 
language example features semantic constructs 
improve data case detail noise richness relevant task 
singular value decomposition svd near relative eigen decomposition appropriate domains input asymmetrical 
best known application singular value decomposition natural language processing latent semantic analysis deerwester 
latent semantic analysis lsa allows passages text compared reduced dimensionality semantic space words contain 
technique successfully applied information retrieval language particularly problematic text searches relevant documents different vocabulary chosen search terms document example user searches eigen decomposition fails retrieve documents factor analysis 
lsa applied language modelling bellegarda incorporate long span semantic dependencies 
research done optimising eigen decomposition algorithms extent optimised depends area application 
natural language problems involve sparse matrices words natural language great majority appear example document 
domains matrices sparse lend techniques golub kahan golub jacobi approaches 
techniques described berry appropriate natural language domain 
optimisation important way increase applicability eigen singular value decomposition 
designing algorithms accommodate different requirements 
example drawback jacobi approaches calculate singular triplets singular vector pairs associated values simultaneously may practical situation top required 
consider methods mentioned far assume entire matrix available start 
situations data may continue available 
berry describe number techniques including new data existing decomposition 
techniques apply situation svd performed collection data new data available 
techniques expensive approximations degrade quality time 
useful context updating existing batch decomposition second batch data applicable case data serially example context learning system 
furthermore limits size matrix feasibly processed batch decomposition techniques 
especially relevant natural language processing large corpora common 
random indexing kanerva provides principled simple efficient alternative svd dimensionality reduction large corpora 
describes approach singular value decomposition generalized hebbian algorithm sanger 
gha calculates eigen decomposition matrix single observations serially 
algorithm differs gha produces eigen decomposition symmetrical data algorithm produces singular value decomposition asymmetrical data 
allows singular vectors learned paired inputs serially memory required store singular vector pairs 
relevant situations size dataset conventional batch approaches infeasible 
interest context adaptivity potential adapt changing input 
learning update operation cheap computationally 
assuming stable vector length update operation takes exactly long previous increase corpus size speed update 
matrix dimensions may increase processing 
algorithm produces singular vector pairs time starting significant means useful data available quickly standard techniques produce entire decomposition simultaneously 
learning technique differs normally considered incremental technique algorithm converges singular value decomposition dataset point having best solution possible data seen far 
method potentially appropriate situations dataset large unbounded smaller bounded datasets may efficiently processed methods 
furthermore approach limited cases final matrix expressible linear sum outer products data vectors 
note particular latent semantic analysis usually implemented example lsa takes log final sums cell dumais :10.1.1.50.8278
lsa depend singular value decomposition webb webb discuss eigen decomposition perform lsa demonstrate lsa generalized hebbian algorithm unmodified form 
sanger sanger presents similar involve detailed comparison approach 
section describes algorithm 
section describes implementation practical terms 
section illustrates word gram letter gram tasks examples section concludes 
algorithm section introduces generalized hebbian algorithm shows technique adapted rectangular matrix form singular value decomposition 
eigen decomposition requires input square matrix say cell value row column row column kind data described matrix correlation data particular space data space 
example wish describe particular word appears particular word 
data symmetrical relations items space word appears word exactly word appears word singular value decomposition rectangular input matrices handled 
ordered word bigrams example imagine matrix rows correspond word bigram columns second 
number times word appears word means number times word appears word rows columns different spaces rows space words bigrams columns space second words 
singular value decomposition rect angular data matrix matrices orthogonal left right singular vectors columns respectively diagonal matrix corresponding singular values 
matrices seen matched set orthogonal basis vectors corresponding spaces singular values specify effective magnitude vector pair 
convention matrices sorted diagonal monotonically decreasing property svd preserving largest columns provides squared error rank approximation original matrix singular value decomposition intimately related eigenvalue decomposition singular vectors data matrix simply eigenvectors respectively singular values square roots corresponding eigenvalues 
generalised hebbian algorithm oja karhunen oja karhunen demonstrated incremental solution finding eigenvector data arriving form serial data items vectors sanger sanger generalized finding eigenvectors generalized hebbian algorithm 
algorithm converges exact eigen decomposition data probability 
essence algorithms simple hebbian learning rule un un aj aj un th eigenvector see equation learning rate aj th column training matrix timestep 
modification required order extend multiple eigenvectors un needs shadow lower ranked um projection input aj order assure orthogonality ordered ranking resulting eigenvectors 
sanger final formulation sanger cij cij yi xj yi yk cij individual element current eigenvector xj input vector yi activation say ci xj dot product input vector ith eigenvector 
learning rate 
summarise formula updates current eigenvector adding input vector multiplied activation minus projection input vector eigenvectors far including current eigenvector multiplied activation 
including current eigenvector projection subtraction step effect keeping eigenvectors normalised 
note sanger includes explicit learning rate 
formula varied slightly including current eigenvector projection subtraction step 
absence influence vector allowed grow long 
effect introducing implicit learning rate vector begins grow long settles right direction learning impact vector long 
weng 
weng demonstrate efficacy approach 
vector form assuming eigenvector currently trained expanding implicit learning rate ci ci cj cj delta notation describe update readability 
subtracted element responsible removing training update projection previous singular vectors ensuring 
assume moment calculating eigenvector 
training update vector added eigenvector simply described follows making steps readable extension paired data simplification cx upper case entire data matrix 
number training items 
simplification valid case stabilised simplification case valid time 
extension paired data initially appears problem 
mentioned earlier singular vectors rectangular matrix eigenvectors matrix multiplied transpose eigenvectors transpose matrix multiplied 
running gha non symmetrical matrix 
paired data achievable standard gha follows ca mm mm cb left right singular vectors 
able feed algorithm rows matrices mm need entire training corpus available simultaneously square hoped avoid 
impossible gha singular value decomposition serially paired input way transformation 
equation gives bx ax ax bx singular value left right data vectors 
valid case left right singular vectors settled accurate time data vectors outer product sum inserting allows reduced follows cbm mm cbm cb ca cb ca element reinserted gha 
summarise gha dotted input eigenvector multiplied result input vector form training update adding input vector eigenvector length proportional extent reflects current direction eigenvector formulation dots right input vector right singular vector multiplies left input vector quantity adding left singular vector vice versa 
way sides cross train 
final modification gha extended cover multiple vector pairs 
original gha beneath comparison 
ci ci cj cj equations introduced approximations accurate direction singular vectors settles 
approximations interfere accuracy final result interfere rate convergence 
constant dropped 
relevance purely respect calculation singular value 
recall weng eigenvalue calculable average magnitude training update formulation singular value divided dropping achieves implicitly singular value average length training update 
section discusses practical aspects implementation 
section illustrates usage english language word letter bigram data test domains 
implementation framework algorithm outlined room implementation decisions 
naive implementation summarised follows datum train singular vector pair projection singular vector pair datum subtracted datum datum train second singular vector pair vector pairs ensuing data items processed similarly 
main problem approach follows 
training process singular vectors close values initialised far away values settle 
second singular vector pair trained datum minus projection singular vector pair order prevent second singular vector pair 
pair far away eventual direction second chance move direction eventually take 
fact vectors whilst remaining orthogonal move strongest direction 
pair eventually takes right direction difficulty recovering start receive data little projection meaning learn slowly 
problem addressed waiting singular vector pair relatively stable train 
stable mean vector changing little direction suggest close target 
measures stability include average variation position endpoint normalised vector number training iterations simply length unnormalised vector long vector reinforced training data settled dominant feature 
termination criteria include target number singular vector pairs reached vector increasing length slowly 
application task relating linguistic bigrams mentioned earlier example task appropriate singular value decomposition data paired data 
consider word bigrams example 
word space non symmetrical relationship second word space spaces necessarily dimensionality conceivably words corpus appear word slot appear start sentence second word slot appear 
matrix containing word counts unique word forms row unique second word forms column square symmetrical matrix value row column value row column coincidence 
significance performing dimensionality reduction word bigrams thought follows 
language clearly adheres extent rule system rich individual instances form surface manifestation 
rules govern words follow words rule system complex longer range word bigrams hope illustrate rule system governs surface form word bigrams hope possible discern word bigrams nature rules 
performing dimensionality reduction word bigram data force rules describe impoverished form collection instances form training corpus 
hope resulting simplified description generalisable system applies instances encountered training time 
practical level outcome applications automatic language acquisition 
example result applicable language modelling 
learning algorithm appropriate large dimensions realistic corpus language corpus chosen demonstration margaret mitchell gone wind contains unique words total fully realized correlation matrix example byte floats consume gigabytes case natural language processing considered particularly large corpus 
results word bigram task section 
letter bigrams provide useful contrasting illustration context input dimensionality allows result easily visualised 
practical applications include automatic handwriting recognition estimate likelihood particular letter useful information 
fact letters western alphabets usefulness incremental approach dimensionality reduction techniques general obvious domain 
extending space letter trigrams grams change requirements 
section discusses results letter bigram task 
word bigram task gone wind algorithm word bigrams 
word mapped vector containing zeros slot corresponding unique word index assigned word 
effect making input algorithm normalised vector making word vectors orthogonal 
singular vector pair reaching combined euclidean magnitude criterion train vector pair reasoning singular vectors start grow long settle approximate right direction data starts reinforce length forms reasonable heuristic deciding settled training vector pair 
chosen ad hoc observation behaviour algorithm training 
data words representative top singular vectors say directions singular vectors point 
table shows words highest scores top vector pairs 
says vector pair normalised left hand vector projected vector word words vectors dot product 
normalised right hand vector projection word table shows left side dominated prepositions right side far important word contains pronouns 
fact singular vector pair effectively right hand side points far direction word reflects status common word english language 
result saying allowed feature describe word english bigrams feature describing words appearing words behaving similarly best choose 
common words english prominent feature 
table top words st singular vector pair vector eigenvalue table puts top left common verbs right indicating pronoun verb pattern second dominant feature corpus 
table top words nd singular vector pair vector eigenvalue letter bigram task running algorithm letter bigrams illustrates different properties 
letters english alphabet meaningful examine entire singular vector pair 
shows third singular vector pair derived running algorithm letter bigrams 
axis gives projection vector letter singular vector 
left singular vector left right right say letter bigram left second right 
singular vector pairs dominated letter frequency effects third interesting clearly shows method identified vowels 
means third useful feature determining likelihood letter letter letter vowel 
letter vowel letter vowels dominate negative right singular vector 
features introduce subcases particular vowel follow particular vowel result suggests dominant case happen 
interestingly letter appears negative right singular vector suggesting part follow vowel english 
items near zero strongly represented singular vector pair tells little 
incremental approach approximating singular value decomposition correlation matrix 
wv incremental approach means singular value decomposition option situations data takes form single serially observations unknown matrix 
method particularly appropriate natural language contexts datasets large processed traditional methods situations dataset unbounded example systems learn 
approach produces preliminary estimations top vectors meaning information available early training process 
avoiding matrix multiplication data high dimensionality processed 
results preliminary experiments discussed task modelling word letter bigrams 
include evaluation larger corpora 
author webb contribution graduate school language technology financial support 
bellegarda 

exploiting latent semantic information statistical language modeling 
proceedings ieee 
michael berry susan dumais gavin brien 

linear algebra intelligent information retrieval 
siam review 
berry 

large scale sparse singular value computations 
international journal supercomputer applications 
ml wp scott deerwester susan dumais thomas landauer george furnas richard harshman 

indexing latent semantic analysis 
journal american society information science 
dumais 

enhancing performance latent semantic indexing 
tm technical report bellcore 
golub 

handbook series linear algebra 
singular value decomposition squares solutions 
numerical mathematics 
webb 

generalized hebbian algorithm latent semantic analysis 
proceedings 
kanerva holst 

random indexing text samples latent semantic analysis 
proceedings nd annual conference cognitive science society 
oja karhunen 

stochastic approximation eigenvectors eigenvalues expectation random matrix 
math 
analysis application 
terence sanger 

optimal unsupervised learning single layer linear feedforward neural network 
neural networks 
terence sanger 

iterative algorithms computing singular value decomposition input output samples 
nips 
weng zhang hwang 

candid covariance free incremental principal component analysis 
ieee transactions pattern analysis machine intelligence 

