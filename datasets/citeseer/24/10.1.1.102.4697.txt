sparse solutions systems equations sparse modeling signals images alfred bruckstein david donoho michael elad july full rank matrix ir generates underdetermined system linear equations ax having infinitely solutions 
suppose seek sparsest solution fewest nonzero entries unique 

optimization sparsity combinatorial nature efficient methods finding sparsest solution 
questions answered positively constructively years exposing wide variety surprising phenomena particular existence easily verifiable conditions optimally sparse solutions concrete effective computational methods 
theoretical results inspire bold perspective important practical problems signal image processing 
known signal image processing problems cast demanding solutions undetermined systems equations 
problems previously intractable 
considerable evidence problems sparse solutions 
advances finding sparse solutions underdetermined systems research signal image processing problems striking effect 
review theoretical results sparse solutions linear systems empirical results sparse modeling signals images applications inverse problems compression image processing 
lies intersection signal processing applied mathematics arose initially wavelets harmonic analysis research communities 
aim introduce key notions applications connected sparsity targeting newcomers interested mathematical aspects area applications 
keywords underdetermined linear system equations redundant overcomplete sparse represen tation sparse coding basis pursuit matching pursuit mutual coherence sparse land dictionary learning inverse problems denoising compression 
research supported part israel science foundation isf united states israel binational science foundation bsf part applied materials research fund 
computer science department technion israel institute technology haifa israel 
statistics department stanford ca usa 
corresponding author computer science department technion israel institute technology haifa israel email elad cs technion ac il 
central achievement classical linear algebra thorough examination problem solving systems linear equations 
results definite timeless profound give subject completely settled appearance 
surprisingly understood arena elementary problem explored depth see problem surprising answers inspires numerous practical developments 
sparse solutions linear systems equations 
consider full rank matrix ir define underdetermined linear system equations ax system infinitely solutions desires narrow choice defined solution additional criteria needed 
familiar way 
introduce function evaluate desirability solution smaller values preferred 
define general optimization problem pj pj min subject ax 
selecting strictly convex function guarantees unique solution 
readers familiar approach case squared euclidean norm problem say results choice fact unique solution called minimum norm solution explicitly aa squared norm course measure energy consider measures sparsity 
simple intuitive measure sparsity vector simply involves number nonzero entries vector sparse nonzeros possible entries convenient introduce norm sparse 
xi consider problem obtained general prescription pj choice explicitly min subject ax 
sparsity optimization looks superficially minimum norm problem notational similarity masks startling differences 
solution unique readily available standard tools computational linear algebra 
probably considered possible goal time time years initially pose conceptual challenges inhibited widespread study application 
rooted discrete discontinuous nature norm standard convex analysis ideas underpin analysis apply 
basic questions immune immediate insight uniqueness solution claimed 
conditions 
candidate solution available perform simple test verify solution global minimizer 
instances special matrices left hand sides ways answer questions apparent general classes problem instances insights initially 
conceptual issues uniqueness verification solutions easily overwhelmed apparent difficulty solving 
classical problem combinatorial search sweeps exhaustively possible sparse subsets generating corresponding subsystems denotes matrix having columns chosen columns indices checking solved 
complexity exhaustive search exponential proven general np hard 
mandatory crucial set questions arise efficiently solved means 
approximate solutions accepted 
accurate 
kind approximations 
fact believe progress kind possible 
hint 
denote norm xi consider problem obtained setting 
problem intermediate 
convex optimization problem convex problems sense closest 
see matrices incoherent columns sufficiently sparse solution solution unique equal solution 
convex solution obtained standard optimization tools fact linear programming 
surprisingly class simple greedy algorithms find sparsest solution 
today pure applied mathematicians pursuing results concerning sparse solutions un systems linear equations 
results achieved far range identifying conditions unique solution conditions solution solution pursuit algorithm 
extensions range widely studying restrictive notions sparsity impact noise behavior approximate solutions properties problem instances defined ensembles random matrices 
hope introduce reader provide appropriate pointers growing literature 
signal processing perspective know finding sparse solutions underdetermined linear systems better behaved practically relevant notion supposed just years ago 
parallel development insight developing signal image processing media types imagery video acoustic sparsely rep resented transform domain methods fact important tasks dealing media fruitfully viewed finding sparse solutions underdetermined systems linear equations 
readers familiar media encoding standard jpeg successor jpeg 
standards notion transform encoding 
data vector representing raw pixel samples transformed represented new coordinate system resulting coordinates processed produce encoded bitstream 
jpeg relies discrete cosine transform dct variant fourier transform jpeg relies discrete wavelet transform dwt 
transforms viewed analytically rotations coordinate axes standard euclidean basis new basis 
sense change coordinates way 
sparsity provides answer 
dct media content property transform coefficients quite large ones small 
treating coefficients zeros approximating early ones quantized representations yields approximate coefficient sequence efficiently stored bits 
approximate coefficient sequence inverse transformed yield approximate representation original media content 
dwt media content slightly different property relatively large coefficients necessarily ones 
approximating dwt setting zero small coefficients quantizing large ones yields sequence efficiently stored inverse transformed provide approximate representation original media content 
success dwt image coding directly tied ability image content 
types image content jpeg outperforms jpeg fewer bits needed accuracy approximation 
underlying reason dwt media sparse dct representation short sparsity representation key widely techniques transform image com pression 
transform sparsity driving factor important signal image processing problems including image denoising image deblurring :10.1.1.125.3682:10.1.1.124.5288:10.1.1.161.9236:10.1.1.29.5390
repeatedly shown better representation technique leads sparsity basis practically better solution problems 
example certain media types musical signals strong harmonic content sinusoids best compression noise removal deblurring media types images strong edges wavelets better choice sinusoids 
realistic media may superposition types conceptually requiring sinusoids wavelets 
idea leads notion joining sinusoids wavelets combined representation 
mathematically lands situation similar previous section 
basis sinusoids matrix basis wavelets matrix concatenation matrix 
problem finding sparse representation signal vector system exactly similar previous section 
system equations unknowns know underdetermined look sparsity second important reason superiority jpeg improved bit plane quantization strategy 
principle find desired solution 
connection formal follows 
denote vector signal image values represented matrix columns elements different bases representation 
problem offers literally sparsest representation signal content 
measuring sparsity norm providing simple easily grasped notion sparsity notion sparsity available really right notion empirical 
vector real data rarely representable vector coefficients containing strict zeros 
weaker notion sparsity built notion approximately representing vector small number nonzeros quantified weak norms measure tradeoff number nonzeros error reconstruction 
denoting number entries exceeding measures sparsity defined sup interesting range giving powerful sparsity constraint 
weak norm popular measure sparsity mathematical analysis community models cartoon images sparse representations measured weak 
equivalent usual norms defined xi familiar objects weak norms range measuring sparsity interest 
natural discussion media sparsity attempt solve problem form 
pp min subject ax example 
unfortunately choice leads nonconvex optimization problem difficult solve general 
point discussion norm section brought bear 
norm naturally related norms measures sparsity fact norm limit norms sense lim lim xk 
presents behavior scalar weight function core norm computation various values showing goes zero measure count non zeros behavior various values tends zero approaches indicator function 
note norms choice gives convex functional choice yields concave functional 
mentioned solving attacked solving appropriate heuristic greedy algorithm lesson applies want solve pp better solving applying appropriate heuristic greedy algorithm 
keywords sparse sparsity sparse representations sparse approximations sparse decom positions increasingly popular institute scientific information june issue essential science indicators formally identified new research front involving key papers discuss 
emerging research front brevity referred sparse land identified topic sparse modeling fit models terms sparsely model important natural data types 
hope give sampling new area spanning range theory applications 
sparsest solution ax return basic problem core discussion 
underdetermined linear system equations ax full rank matrix ir pose questions uniqueness sparsest solution claimed 
candidate solution tested verify global optimality 
solution reliably efficiently practice 
performance guarantees various approximate practical solvers 
section addresses questions extensions 
uniqueness uniqueness spark key property crucial study uniqueness spark matrix term coined defined 
start definition definition spark matrix smallest number columns linearly dependent 
recall rank matrix defined largest number columns linearly independent 
clearly resemblance definitions noticeable 
spark matrix far difficult obtain compared rank calls combinatorial search possible subsets columns importance property matrices study uniqueness sparse solutions 
interestingly property previously appeared literature termed kruskal rank context studying uniqueness tensor decomposition 
spark related established notions matroid theory formally precisely girth linear matroid defined length shortest cycle matroid 
consider definition arithmetic underlying matrix product performed fields real complex numbers ring integers mod quantity arises coding theory allows compute minimum distance code 
resemblance concepts striking instructive 
spark gives simple criterion uniqueness sparse solutions 
definition vectors null space matrix ax satisfy spark vectors combine linearly columns give zero vector spark columns necessary definition 
spark obtain result theorem uniqueness spark system linear equations ax solution obeying spark solution necessarily sparsest possible 
proof consider alternative solution satisfies linear system ay implies null space 
definition spark spark 
left term inequality simply states number non zeros difference vector exceed sum number non zeros vectors separately 
solution satisfying spark conclude alternative solution necessarily spark non zeros claimed 
result elementary quite surprising bearing mind highly cated optimization task combinatorial flavor 
general combinatorial optimization problems considering proposed solution hopes check local optimality simple modi fication gives better result 
find simply checking solution sparsity comparing spark lets check global optimality 
clearly value spark informative large values spark evidently useful 
large spark 
definition spark range spark 
example comprises random independent identically distributed entries say gaussian probability spark implying columns linearly dependent 
case uniqueness ensured solution fewer non zero entries 
uniqueness mutual coherence spark difficult evaluate solving 
simpler ways guarantee uniqueness interest 
simple way exploits mutual coherence matrix defined follows definition mutual coherence matrix largest absolute normalized inner product different columns denoting th column ak mutual coherence max aj ak aj 
mutual coherence way characterize dependence columns matrix unitary matrix columns pairwise orthogonal mutual coherence zero 
general matrices columns rows necessarily strictly positive desire smallest possible value get close possible behavior exhibited unitary matrices 
reported considered structured matrices ir unitary matrices 
mutual coherence dictionaries satisfies lower bound achievable certain pairs orthogonal bases identity fourier identity hadamard 
considering random orthogonal matrices size shown tend incoherent implying typically proportional log nm 
shown full rank matrices size mutual coherence bounded equality obtained family matrices named frames 
family matrices spark highest value possible 
mention quantum infor mation theory constructing error correcting codes collection orthogonal bases minimal coherence obtaining similar bounds mutual coherence orthogonal bases 
mutual coherence relatively easy compute allows lower bound spark hard compute 
lemma matrix ir relationship holds spark 
proof modify matrix normalizing columns unit norm obtaining 
operation preserves spark mutual coherence 
entries resulting gram matrix satisfy properties gk gk consider arbitrary minor size built choosing subgroup columns computing sub gram matrix 
disk theorem minor diagonally dominant gi gi sub matrix positive definite columns linearly independent 
condition implies positive definiteness minor spark 
analog theorem 
theorem uniqueness mutual coherence system linear equations ax solution obeying solution necessarily sparsest possible 
compare theorems 
parallel form different assumptions 
general theorem uses spark sharp far powerful theorem uses coherence lower bound spark 
coherence smaller cardinality bound theorem larger 
spark easily large theorem gives bound large 
partial answers questions posed start section 
seen sufficiently sparse solution guaranteed unique sufficiently sparse solutions 
consequently sufficiently sparse solution necessarily global optimizer 
results show searching sparse solution lead posed question interesting properties 
turn discuss practical methods obtaining solutions 
pursuit algorithms practice straightforward approach solving hopeless discuss methods hope working specific conditions 
greedy algorithms suppose matrix spark optimization problem value val scalar multiple column matrix identify column applying tests column procedure requires order mn flops may considered reasonable 
suppose spark optimization problem known value val 
linear combination columns generalizing previous solution think enumerate subsets columns test 
enumeration takes nk flops prohibitively slow settings 
greedy strategy abandons exhaustive search favor series locally optimal single term updates 
starting iteratively constructs term approximant maintaining set active columns initially empty stage expanding set additional column 
column chosen stage maximally reduces residual error approximating currently active columns 
constructing approximant including new column residual error evaluated falls specified threshold algorithm terminates 
exhibit presents formal description strategy associated notation 
procedure known literature signal processing name orthogonal matching pursuit omp known earlier names fields see 
delivered approximation nonzeros method requires mn flops general dramatically better exhaustive search requires nm flops 
single term time strategy efficient exhaustive search works 
strategy fail badly explicit examples see simple term representation possible approach yields term dense representation 
general said single term time strategies approximation error reduced possible starting approximation single term time constraint 
property earned type algorithm name greedy algorithm approximation theory 
variants algorithm available offering improvements accuracy complexity :10.1.1.55.1254
family greedy algorithms known extensively fact algorithms re invented various fields 
setting task approximate solution minx subject ax parameters matrix vector error threshold 
initialization initialize set initial solution 
initial residual ax initial solution support support 
main iteration increment perform steps sweep compute errors optimal choice rk aj 
update support find minimizer update 
update provisional solution compute minimizer ax subject support update residual compute ax stopping rule 
apply iteration 
output proposed solution obtained iterations 
exhibit omp greedy algorithm approximating solution 
statistical modeling greedy stepwise squares called forward stepwise regression widely practiced 
signal processing setting goes name matching pursuit mp orthogonal matching pursuit omp :10.1.1.55.1254
approximation theorists refer algorithms greedy algorithms ga consider variants pure pga orthogonal relaxed weak greedy algorithm 
convex relaxation techniques second way render tractable regularize highly discontinuous norm replacing continuous smooth approximation 
examples regularizations include replacing norms smooth functions log 
example method uses fixed seeks local minimum norm iteratively reweighted squares 
practical strategy little known circumstances successful numerical local minimum approximation global minimum 
strategy replace norm norm natural sense best convex approximant optimization tools available shelf solving :10.1.1.135.1907
turning regularizations pp care taken respect normalization columns norm indifferent magnitude non zero entries norms tend penalize higher magnitudes bias solution choosing put non zero entries locations multiply large norm columns order avoid bias columns scaled appropriately 
norm new objective min wx subject ax 
matrix diagonal positive definite matrix introduces described pre com weights 
natural choice entry matrix case ai 
assuming zero columns norms strictly positive problem defined 
case columns normalized named basis pursuit bp :10.1.1.135.1907
name general setup equation 
problem cast linear programming lp problem solved modern interior point methods simplex methods techniques homotopy methods :10.1.1.135.1907
algorithms far sophisticated greedy algorithms mentioned earlier obtain global solution defined optimization problem 
possible understand working great detail apparently case greedy algorithms 
readily available carefully programmed solvers accurate solution approximating solution greedy algorithms considered common perception high accuracy solution computationally heavy task 
emerging alternative approximately solving introduced independent papers leading similar algorithms may called iterated shrinkage methods 
iteratively multiplication adjoint simple operation sets zero small entries shrinkage operation 
methods compete greedy methods ity efficiency 
line infancy required establish effectiveness algorithms compared greedy ones 
family techniques section 
pursuit algorithms performance far provided answers 
turn discuss performance guarantees described pursuit algorithms 
assume linear system ax sparse solution non zeros 
fur thermore assume spark 
matching pursuit basis pursuit succeed recovering sparsest solution 
clearly success expected matrices conflict known np hardness problem general case 
equation sufficiently sparse solution success algorithms addressing original objective guaranteed 
results corresponds matching pursuit algorithm omp described exhibit basis pursuit solving place 
greedy algorithm solves sufficiently sparse cases theorem equivalence orthogonal greedy algorithm system linear equations ax ir full rank solution exists obeying run threshold parameter guaranteed find exactly 
proof suppose loss generality sparsest solution linear system non zero entries vector decreasing order values xj aj 
ax 
step algorithm set computed errors sweep step min zj aj aj aj 
step choose entries vector require columns outside true support ai 
substitution equation requirement translates xt xt 
ai order consider worst case scenario construct lower bound left hand side upper bound right hand side pose requirement 
left hand side xt xt xt exploited definition mutual coherence equation descending ordering values xj aj 
similarly right hand side term bounded xt ai xt ai xt 
bounds plugged inequality obtain xt leads ai xt exactly condition sparsity required theorem 
condition guarantees success stage algorithm implying chosen element correct support sparsest decomposition 
done step update solution residual residual written value orthogonal ak due squares computation 
repeating process assume loss generality entries rearranged decreasing order values xj aj simple permutation columns set steps obtain condition guarantees algorithm finds index true support solution 
due orthogonality rk necessarily get highest computed errors chosen 
repeating reasoning holds true iterations algorithm selects values correct set indices index chosen 
iterations residual zero algorithm stops ensuring success algorithm recovering correct solution theorem claims 
basis pursuit solves sufficiently sparse cases turn consider basis pursuit replacement optimization problem 
theorem equivalence basis pursuit system linear equations ax ir full rank solution exists obeying solution unique solution unique solution 
note assumptions theorem concerning basis pursuit theorem concerning matching pursuit 
mean algorithms expected perform similarly 
empirical evidence section showing methods behave differently 
proof define set alternative solutions wy wx 
set contains possible solutions different larger support satisfy linear system equations ay weighted perspective 
set non empty implies alternative solution basis pursuit find desired view theorem fact necessarily unique sparsest possible solution alternative solutions necessarily denser 
requirement omitted definition defining rewrite shifted version cs wx ae 
strategy proof enlarge set show enlarged set empty 
prove basis pursuit succeeds recovering start requirement wx 
assuming loss generality simple column permutation non zeros located vector requirement written wx ej xj xj ej inequality fact relax condition demand ej ej ej xj xj ej 
inequality written compactly adding subtracting term ej denoting indicate sum entries vector 
leads substituting definition cs get 
cs ae 
turn handle requirement ae replacing relaxed requirement expands set 
multiplication yields condition ae change set equation re written aw 
left multiplication inverse leaves condition 
inner multiplication inverse cancel 
term aw desirable entry matrix normalized inner product definition mutual coherence 
main diagonal matrix contains ones 
equation rewritten adding removing follows aw 
entry wise absolute value sides relax requirement obtain aw aw 
term stands rank matrix filled ones 
step definition mutual coherence fact bounds normalized inner products columns returning set write 
defining written differently cf obtained set cf unbounded cf cf 
order study behavior restrict quest normalized vectors 
new set denoted cr cr 
condition relation fact 
order vector satisfy requirement needs concentrate energy entries 
requirements fj restrict entries exactly fj maximal allowed values 
returning condition get requirement 
means set necessarily empty implying basis pursuit leads desired solution theorem claims 
proof amounts showing solutions incoherent underdetermined system sparse moving line segment solutions causes increase norm move away sparse solution 
historically theorem omp result theorem provided kind existence proof interesting said solving underdetermined systems equations conditions 
fact form assumptions lead form result algorithms tantalizing deeper meaning 
keep reading 
variations far focused narrowly viewpoint efficiently showed interesting said finding sparse solutions underdetermined systems 
broaden viewpoint expanding connections broad rapidly growing literature 
leads interesting variations results discussed far 
uncertainty principles sparsity phenomena exposed far noticed concrete setting case con catenation orthogonal matrices identity matrix fourier matrix 
setting fact system ax underdetermined means concretely ways representing signal superposition spikes columns identity matrix sinusoids columns fourier matrix 
sparse solution system representation said signal superposition sinusoids spikes 
uniqueness sparse solution ability minimization find surprising noticed 
proof sharing key ideas proofs interpreted time kind uncertainty principle 
reader doubt knows classical uncertainty principle says signal tightly concentrated time frequency places lower bound product spread time spread frequency 
uniqueness sparse representation time frequency systems interpreted saying signal sparsely represented time frequency 
viewpoint helpful understanding preceding discussion briefly develop 
uncertainty principle sparse representations suppose non zero vector ir signal say 
represented linear combination columns linear combination columns 
clearly uniquely defined 
particularly important case simply identity matrix matrix fourier transform 
time domain representation frequency domain representation 
certain pairs bases interesting phenomenon occurs sparse sparse 
fact inequality uncertainty principle 
mutual coherence bases small sparse 
example identity fourier matrix follows signal fewer nonzeros time frequency domains 
heisenberg classical uncertainty principle discrete setting says view probability distributions absolute value entries normalizing product variances const 
contrast gives lower bound sum nonzeros 
uncertainty principle interpretation developed greater length 
uncertainty uniqueness connection uniqueness problem 
consider problem finding solution ax light uncertainty principle 
suppose solutions underlying linear system sparse 
see sparse 
necessarily difference null space partition sub vectors entries entries respectively 

vector nonzero nonzero nonsingular 
invoke 
uncertainty principle 
words distinct solutions linear system sparse 
fact general matrix obtained rule product proof uniqueness result 
inequality posed terms spark recast terms mutual coherence due lemma 
interestingly lower bound general case 
general case bound nearly factor weaker uses special structure 
returning case dictionary formed concatenating ortho bases direct consequence inequality uniqueness result flavor discussed section solution fewer non zeros solution denser 
notice specific case special structure yields stronger uniqueness result theorem 
equivalence pursuit algorithms uniqueness sufficiently sparse solution natural ask specific algorithms perform 
result similar theorem obtained showing ensures basis pursuit finds proper sparsest solution 
result kind general case addressed 
result better general result theorem factor 
similar result holds greedy algorithms ortho basis case unaware publication 
intermediate ortho basis case general dictionary case case con orthogonal bases 
surprisingly concatenations ortho bases coherence small concatenate specially chosen get coherence large dictionaries coherence 
important result emmanuel knill theory quantum error correcting codes special called nice error bases coherence important reasons unrelated interests 
mentioned uniqueness equivalence theorems generalized concatenations ortho bases 
exact approximate solution general motivation exact constraint ax relaxed approximate equality measured quadratic penalty function ax relaxation allows define quasi solution case exact solution exists ii exploit ideas optimization theory iii measure quality candidate solution 
rationale previous sections may reconsider tolerate slight dis ax define error tolerant version error tolerance min subject ax 
norm evaluating error ax replaced options weighted norm 
problem discrepancy size permitted proposed representation ax signal applied problem instance error tolerant problem give results sparse arising 
typical general problem instance solution nonzeros 
hand real world problems see solution nonzeros solution seen far fewer 
alternative natural interpretation problem noise removal 
consider sufficiently sparse vector assume ax nuisance vector finite energy roughly speaking aims find roughly thing noiseless data ax 
papers study problem briefly discuss known 
results ways parallel noiseless case notions uniqueness equivalence longer apply replaced notion stability 
stability sparsest solution theorem stability consider instance problem defined triplet 
suppose sparse vector ir satisfies sparsity constraint gives representation error tolerance ax 
solution obey 
result parallels theorem reduces case 
result similar flavor proposing simple constructive test near optimality solution appears 
pursuit algorithms impractical solve general case direct attack sensible goal 
pursuit algorithms discussed adapted allow error tolerances perform 
referring options devising algorithms greedy approach regularization functional variants methods may investigated 
consider example greedy algorithm described exhibit omp 
choosing stopping rule algorithm accumulates non zero elements solution vector constraint ax satisfied 
similarly relaxing norm get variant known literature basis pursuit denoising min wx subject ax diagonal positive definite weight matrix :10.1.1.135.1907
written standard problem linear optimization quadratic linear inequality constraints 
problems studied specialists optimization practical methods solving practically modern convex optimization theory applicable particularly advances solving large systems interior point related methods :10.1.1.135.1907
review literature discuss simple approach suitable readers background convex optimization 
appropriate lagrange multiplier solution precisely solution uncon strained optimization problem min lagrange multiplier function 
wx ax general methods discussed way get reliable solution little programming effort set problem type optimizer solve 
large scale applications general purpose optimizers slow improved special purpose techniques 
mention 
iteratively reweighted squares simple strategy attack iteratively reweighted squares irls algorithm 
setting diag may view norm adaptively weighted version squared norm 
current approximate solution xk set xk diag xk attempt solve mk min wx ax quadratic optimization problem solvable standard linear algebra 
obtain ap proximate solution xk say diagonal matrix xk constructed entries xk diagonal new iteration 
algorithm formally described exhibit 
task find approximately solves minx wx ax 
iterative thresholding initialization initialize set initial approximation 
initial weight matrix main iteration increment apply steps regularized squares approximately solve linear system wx iteratively conjugate gradient iterations may suffice producing result xk 
weight update update diagonal weight matrix xk xk xk 
stopping rule xk xk smaller predetermined threshold 
apply iteration 
output desired result xk 
exhibit irls strategy approximately solving 
irls loses appeal facing large scale problems today applications see 
alternative iterative family approximate solution techniques developed coined iterated shrinkage algorithms methods easy implement sense intuitive apply 
square unitary matrix problem simple non iterative closed form solution say follows 
apply vector obtaining preliminary solution favorable cases exhibits large entries rising small noise entries sticking weeds 
second apply soft thresholding setting zero entries threshold shrinking entries zero 
done formally defining scalar function sign inducing vector function shrink element wise application 
equation known shrinkage operation clearly tends shrink magnitude entries setting small ones zero :10.1.1.125.3682:10.1.1.124.5288:10.1.1.161.9236:10.1.1.29.5390
general case unitary matrix apply idea iteratively 
step generalization proposed initially bruce tseng block coordinate relaxation algorithm considers concatenations unitary matrices iteratively updating solution part time shrinkage 
handling general case somewhat involved seen 
exhibit spells details 
algorithm particularly useful large scale problems defined explicitly matrix operator know apply rapidly 
note large problems omp really practical requires direct manipulation columns task find approximately solves minx wx ax 
initialization initialize set initial solution 
initial residual ax compute normalize replacing aw main iteration increment apply steps back projection compute multiply entry wise 
shrinkage compute es shrink threshold 
line search choose minimize real valued function es objective function 
update solution compute es update residual compute ax stopping rule smaller predetermined threshold 
apply iteration 
output result exhibit iterated shrinkage algorithm solving 
stepwise algorithms lars homotopy certain heuristic methods inspired true solvers currently attracting serious interest entirely greedy viewed principles true solver 
include lars polytope faces pursuit 
practice extremely example viewpoint phase transition discussed section 
possible solve algorithm reminiscent omp 
suppose matrix normalized columns ai 
starting support set proceed stepwise 
th step find index current zero corresponding column highest correlation current residual columns max ax sk ai 
label entry ik form new support set sk sk ik 
obtain nonzeros positions sk 
far sounds omp 
solve squares 
choose nonzeros residual ax column ai sk ax ai const sk 
omp demand residual uncorrelated 
lars algorithm efron 
demonstrated analog theorem lars incoherence sufficient sparsity lars takes steps stops having produced unique sparsest solution 
lars sense equally omp bp setting 
fact true 
modify lars algorithm stage new term enter old term leave support seeking maintain sk ax ai const max ax sk aj 
variation lars lasso algorithm known homotopy algorithm osborne 
new algorithm solves continuing residual zero gives solution minimization problem 
shown incoherence sufficient sparsity lars lasso homotopy takes steps stops having produced unique sparsest solution 
results step lars 
short small modification omp produces stepwise algorithm exactly solves 
helps explain similarity coherence results methods 
great deal algorithmic progress review revision 
mention examples 
cand romberg developed fast approximate solver projections convex sets 
stephen boyd coworkers way speed standard interior point methods solution sparse run quickly 
performance pursuit algorithms pursuit methods approximately solve 
quote theorems corresponds omp 
stability solution guaranteed sufficiently sparse solution exists 
similar flavor stability claimed theorem weaker ways sparsity requirement strict tolerated error level larger 
theorem stability consider instance problem defined triplet 
suppose vector ir satisfies sparsity constraint gives representation error tolerance ax 
solution obey 
addition stability results successful recovery support interest 
reported offer results requires strict realistic con ditions 
similarly considering signal denoising procedure interest expected performance 
reported uses information theoretic point view pro vide analysis extreme weak strong noise cases 
turning omp theorem taken establishes stability correct recovery support 
notice hold true magnitude smallest non zero entry ideal solution sufficiently large compared noise level 
results flavor derived 
theorem performance omp consider omp algorithm applied problem instance triplet 
suppose vector ir satisfies sparsity constraint xmin xmin smallest non zero entry absolute value 
assume gives representation error tolerance ax 
result produced omp obey 
furthermore omp guaranteed recover solution correct support 
various ways results far weaker 
show adopting sparsity goal lead sensible results stable additive noise 
coherence arguments analysis far largely coherence arguments presents simple limited portrait ability concrete algorithms find sparse solutions near solutions 
briefly point interesting challenging research territory lies coherence 
start simple simulations 
empirical evidence consider random matrix size entries independently drawn random gaussian distribution zero mean unit variance 
spark matrix probability implying solution system ax entries necessarily sparsest possible solution 
randomly generating sufficiently sparse vectors choosing non zero locations uniformly support random values generate vectors way know sparsest solution ax shall able compare algorithmic results 
graph shows success rate omp bp recovering true sparsest solution 
cardinality repetitions conducted results averaged 
value mutual coherence experiment cardinalities lower pursuit methods guaranteed succeed 
see pursuit algorithms succeed recovery sparsest solution far coverage theorems 
see greedy algorithm omp performing somewhat better bp 
graph similar showing success rate omp bp approximated case solution 
generate random vector pre specified cardinality non zeros 
normalized ax 
compute ax random vector predetermined norm 
original vector feasible solution close optimal due sparsity 
approximate solution irls line search determining desired misfit obtained stability process tested 
seen methods stability results set lower expectation bounds stability hold 
probability success basis pursuit matching pursuit cardinality solution probability success pursuit algorithms recovery sparsest solution linear system ax results shown function cardinality desired solution 
formal machinery suites ensembles simulations show coherence tell story 
working matrices having specific properties problems exhibiting structured sparsity find coherence gives weak guarantees compared happens 
introduce notion formally bring information properties matrix sparsest solution picture 
problem suite defined ensemble matrices shape collection solution vectors obeying sparsity condition 
examples matrix ensembles form problem suites incoherent ensemble consisting matrices normalized columns 
gaussian ensemble age consisting entries drawn gaussian iid distribution 
partial fourier ensemble consisting matrices rows drawn random replacement fourier matrix 
time frequency dictionary singleton ensemble consisting just fourier matrix 
collections solution vectors form problem suites sparse collection consisting vectors ir nonzero entries 
bernoulli gaussian ensemble bg consisting random vectors ir sites ros chosen random tossing coin probability nonzero sites having values chosen standard gaussian distribution 
far discussed incoherent problem suite consisting incoherent matrix ensemble sparse collection 
previous subsection implicitly considering gaussian problem suite age bg 
suites represent extremes kind 
incoherent suite natural study worst case behavior algorithms gaussian suite natural study typical probability success basis pursuit matching pursuit cardinality solution probability success pursuit algorithms stable recovery sparsest solution linear system ax presence noise 
behavior 
subsection simulations show worst case behavior incoherent suite different typical behavior gaussian suite 
fact distinction worst case typical behavior known time 
shown empirically time frequency dictionary typical sparse sequence dramatically stronger uncertainty principle true uncertainty principle guarantees number nonzeros combined time frequency analysis exceed fact typical number closer simulations ones reported section show equivalence representations typical surprisingly weak levels sparsity fact time frequency dictionary coherence theory able guarantee equivalence fewer nonzeros solution equivalence typical fewer nonzeros 
phase transitions typical behavior simulation studies typical case behavior algorithms exhibit surprising regularities 
consider problem suite age define variables interesting case 
simulations section explored case reveals relatively rapid drop probability successful recovery bp omp increased 
phenomena observed variety sparsity seeking algorithms 
typical example 
panel taken depicts unit square interesting behavior shaded attribute displays simulation results probability solutions equivalent 
just relatively rapid transition probability near probability near zero increases 
panel taken depicts behavior iterative thresholding algorithm 
shaded attribute displays fraction truly nonzero coefficients recovered rapid transition nearly success nearly success 
panel phase transition behavior minimization 
shaded attribute fraction cases minimization successfully finds sparsest solution range black white 
curve displays function defined theorem 
curve closely follows rapid change shaded attribute 
panel phase transition behavior stagewise orthogonal matching pursuit 
shaded attribute fraction cases successfully finds sparsest solution 
red curve displays function defined 
curve closely follows rapid change shaded attribute 
panels coordinates ratio number equations number unknowns ratio number nonzeros number equations 
experiments explore grid values problem size 
underlying matrix ensemble gaussian 
problem size increases transition typicality success typicality failure increasingly sharp large limit perfectly sharp 
rigorous result explains meaning curve panel theorem fix pair 
problem size set mn kn 
draw problem instance ax random mn matrix gaussian ensemble vector sparse collection kn 
function property increases probability random problem instance problems solution tends zero tends 
words large really phases phase plane success phase failure phase 
curve interesting form small theorem 
log informally setting gaussian random matrices threshold log bit larger threshold failure phase bit smaller success phase 
words number gaussian measurements sparse vector exceeds log vector highly recovered minimization 
techniques underlying proofs combine exact identities convex integral geometry asymptotic analysis 
curve panel concerning iterative thresholding scheme similar large interpretation rigorous result establishing existence curve st omp separating phases rigorous result showing compute curve 
empirical matter existence phase transitions established algorithms including particularly omp 
fact showed omp exhibit phase transition performance competitive bp 
knowledge moment theoretical calculation giving curve omp matches empirically observed behavior 
part problem may case bp phase transition omp sensitive distribution nonzero elements sparsest solution 
empirical phase transition different lower omp coefficients random signs random gaussian 
particular apparent advantage omp bp seen disappears replace gaussian distributed nonzeros random terms 
years wide range rigorous mathematical analysis published address large setting just discussed 
covers sparsity seeking algorithms variety assumptions matrix ensemble sparse solution 
difficult summarize short space limit examples 
cand romberg tao great impact announcing prove typicality equivalence sparsity control parameter large log matrix drawn partial fourier ensemble 
log factor unnecessarily small 
donoho effectively showed matrices gaussian ensemble equivalence hold sparsity control parameter unspecified function 
cand tao considered random gaussian matrices able show sufficient equivalence certain explicit function 
qualitative results opened way asking precise quantitative behavior 
gilbert studied running omp problem suite consisting gaussian matrix ensemble sparse coefficients solution showed sparsest solution high probability provided log 
empirical evidence suggests true state affairs phase transition omp function omp important result somewhat weaker expect case 
general researchers making progress asymptotic studies exploit ideas geometric func tional analysis techniques random matrix theory 
useful results literature include bound singular values random gaussian matrix allow easily control maximal minimal singular values sub matrices principle studying equivalence studying omp 
fundamental ideas include results widths octahedron quotient subspace theorem volume bounds reflecting properties norms restricted random subspaces lie heart equivalence 
effective geometric functional analysis tools giving shortest simplest proofs sufficient equivalence 
presently tools allow precisely pin location actual phase transitions 
flexible widely applicable 
authors able geometric functional analysis techniques establish equivalence matrices random entries 
tools get rigorous results stability solutions showing stability obtains point literature growing rapidly difficult justice field achievements results 
mention particularly elegant result cand tao develops tool going coherence coherence focus attention current research sparse representation 
definition matrix said restricted isometry property rip sub matrix ai formed combining columns nonzero singular values bounded 
cand tao shown rip implies identical solutions sparse vectors stably approximates sparsest near solution ax reasonable stability coefficient 
restricted isometry property useful established probabilistic methods 
high probability matrix gaussian iid entries property log small positive constant 
shown results singular values gaussian matrices 
analysis matrices example entries challenging 
rip approach gives qualitative bound geometric functional analysis techniques qualitatively correct useful apparently smaller actual behavior phase transitions 
sparsest solution ax summary previous sections discussed wealth results study underdetermined linear system ax quest sparsest solution 
questions solvability problems uniqueness sparsest solutions extensions approximate solutions addressed past years 
remains done list open questions research directions special structure structure exploited order obtain stronger unique ness equivalence claims 
case concatenations unitary matrices 
required structured matrices ones columns re wavelet bases gabor bases 
particular interest exploitation multi scale structure underlying dictionaries 
explicitly done possible 
mentioned quite surprisingly study performance greedy algorithms concatenation unitary matrices missing 
case studied thoroughly basis pursuit seen similar analysis omp greedy techniques 
need fast algorithms basis pursuit accurate approximate versions compete favorably greedy methods 
ambitious goal attempt unify methods show common grounds 
respect progress iterated shrinkage methods promising research direction 
progress reported section iterated shrinkage lars lasso fast general solvers indicates great deal ongoing expect progress near 
average performance existing results limited scope due asymptotic nature limiting assumptions structured matrix random content 
derivation stronger results refer specific matrices bypass mutual coherence attempted 
mutual coherence definition stems worst case point view avoided 
uniqueness equivalence claims shown far general hold true uniformly signal dependent representation dependent theorems derived yielding stronger guarantees uniqueness equivalence 
mutual coherence suggests way bounding spark testing pairs atoms similar better treatment may possible considering behavior triplets sets atoms 
turn practical applications 
sparsity seeking methods signal processing see problem finding sparse representations signal vectors meaningful definition contrary expectation computationally tractable 
suggest sparsity driven signal processing valid research agenda potentially useful practical tool 
develop idea detail 
priors transforms signals consider family signals ir discussion concrete assume signal pixel image patch representing natural typical image content 
discussion applies obvious changes signal types sound signals seismic data medical signals financial information 
image patches scattered ir populate uniformly 
example know spatially smooth patches frequently seen images highly non smooth image content rare 
talk typical behavior suggest bayesian approach signal processing 
approach researcher model probability density function pdf images specific prior distribution derive bayesian algorithms specific assumption 
example consider denoising problem observe noisy version true underlying image bayesian assume probability density noise independent probability model 
reconstruction maximum posteriori solve max 
approach tried successfully concrete problems wide range interesting results finding prior distributions signals active topic research signal image processing communities 
familiar starting point takes gaussian prior signal example exp ly discrete laplacian 
gaussian priors frequently course intimately connected beautiful classical topics wiener filtering 
gaussian processes beautiful analytic properties fail match empirical facts 
edges fundamental components image content stationary gaussian processes fail exhibit properly 
repeatedly non gaussian priors give better results bayesian framework example obtained replacing norm norm exponent prior density second order laplacian operator pair order difference operators horizontal dh vertical dv direction exp 
approach uses signal transforms 
matrix associated discrete orthogonal wavelet transform matrix columns orthogonal basis functions specific wavelet transform say daubechies nearly symmetric vanishing moments 
consider prior exp 
bayesian methods priors outperform traditional gaussian priors image denoising 
orthodox bayesian approaches 
insist making distinction practical approach bayes quickly derive candidate algorithms approach believing assumptions strictly true correctness assumptions unique reason algorithms 
results discussed earlier contradict provide different explanation important bayesian algorithms successful certain situations sparsity 
bayesians right wrong reasons 
specific non gaussian priors just mentioned give rise interesting signal processing algo rithms map framework 
writing problem terms map estimation equation gives prior equation min prior equation min 
step bayesian framework interpret optimization criteria algorithm generators 
recognize generates variant total variation denoising second generates instance wavelet denoising successful algorithms hundreds application papers print 
frequent success algorithms causes difficulties bayesian interpretation 
corre sponding prior distributions demonstrably agreement image statistics cases algorithms successful 
true cases algorithms dramatically successful underlying transform sparsity image content case total variation means spatial gradients nearly zero pixels case wavelet denoising means wavelet coefficients nearly zero 
successful case algorithm involves transform takes signal renders sparse 
orthodox bayesian cases seek better prior 
careful empirical modeling wavelet coefficients images edges shown cases prior model exp ty improved :10.1.1.29.5390
general form exp ty studied values significantly smaller give better fit image libraries 
surprisingly actual algorithm results better model qualitatively different minimization 
furthermore performance algorithms underlying noiseless object truly sparse wavelet coefficients comparable 
observations suggest driving factor content modeling sparsity 
transform maps content sparse vector matters great deal 
precise amplitude distribution nonzeros transform domain may detail matters little comparison 
combined representation continuing line thinking ask sparsity important fundamental best way achieve 
traditional transform techniques achieve success image content may best 
traditional transform techniques apply linear transform signal content place prior resulting transformed content 
signal content mixture different types phenomena harmonics transients acoustic data edges textures image data 
component mixture may modelled adapted fashion 
harmonic sub signal modelled superposition sinusoids transient modelled superposition spikes 
considerations sparsity play differently setting 
restrict single representation say sinusoids expect mixture sinusoids transients sparsely representable sinusoids 
fact uncertainty principle earlier expressly prevents 
achieve sparsity combine representations 
suppose bases corresponding matrices having columns elements basis 
model signal interest superposition elements basis vectors give coefficients allowing represent 
leads important distinction 
harmonic analysis operation transforming signal domain transform domain called analysis 
operation returning transform domain signal domain called synthesis 
trying synthesize signal combined transform domain synthesis coefficients provide combined representation signal 
setting sparsity means combined coefficient vector sparse 
viewpoint forced uncertainty principle says sparse analysis coefficients expected general sufficiently sparse synthesis coefficients uniquely recovered settings 
generative model simple way sparsely synthesize signals 
start matrix columns elementary atoms model generate random vector having entries nonzero 
choose positions nonzeros uniformly random values nonzeros chosen laplace distribution having density function exp 
example combined time frequency dictionary small fraction entries nonzero 
way generate random sparse combination spikes sinusoids 
example concatenation fourier matrix matrix orthonormal wavelet transform say daubechies wavelets 
get random combination harmonic signals transients 
approach provides flexible class probabilistic models 
varying dictionary sparsity control parameter amplitude parameter get different signal types markedly different characteristics 
course real signals expected deviate model 
instance expected contain noise 
model incorporate effect adding noise vector uniformly distributed sphere radius 
final generated noisy signal ax call class models informally sparse land signals 
natural variations possible number nonzeros random example sample geometric poisson distribution 
cases large variation little difference 
gaussian white noise power cases large thing provided calibrate parameters concerned specific choice laplace density nonzeros 
argued earlier specific phenomena concerned getting distribution right doesn matter surprising may 
allow sparsity parameter position dependent example control sparsity blocks 
wavelet transform sense allow coarse scales dense fine scales get increasingly sparse spatial scale shrinks 
possibilities important empirical exposition simpler model having features 
processing sparsely generated signals practice signal processing sparse land suppose signal generated model parameters model known 
numerous signal processing tasks interest discuss see sparsity seeking representations enter 
possible core applications analysis determine underlying vector generated 
process may called atomic decomposition leads infer individual atoms generated clearly true underlying representation obeys ax vectors generating similarly approximations suppose solve min assumptions solution subject ax 
problem necessarily underlying sparse fewer nonzeros 
earlier results show small solution away 
compression nominally requires description numbers 
solve resulting solution affords approximation ax scalars approximation error 
increasing obtain stronger compression larger approximation error way obtain rate distortion curve compression mechanism 
denoising suppose observe noisy version noise known obey 
solve resulting solution nonzeros earlier results show small away 
inverse problems generally suppose observe noisy indirect measurement hy linear operator generates blurring masking kind degradation noise 
solve min subject expect identify directly sparse components underlying signal obtain approximation ax compressed sensing signals sparsely generated obtain reconstruc tions reduced numbers measurements compressing sensing process traditionally sensed data 
fact random matrix gaussian entries suppose possible directly measure py entries attempt recovery solving min subject pax obtain sparse representation synthesizing approximate reconstruction ax 
morphological component analysis mca suppose observed signal super position different sub signals sparsely generated model sparsely generated model 
separate sources 
source separation problems fundamental processing acoustic signals example separation speech impulsive noise independent component analysis ica algorithms 
turning signal model solve min subject resulting solution generate plausible solution ax ax separation problem 
fact successful trials idea acoustic image processing :10.1.1.5.1195
appealing image processing application relies mca inpainting missing pixels image filled sparse representation existing pixels 
mca necessary piece wise smooth cartoon texture contents image separated part recovery process 
see details 
wide range applications including encryption watermarking scrambling target detection envisioned 
applications ideas call solution variants 
intentionally described proposed applications conditional mood general known tractable defined 
earlier discussion shows problem right conditions sensible approximately solved practical algorithms 
word caution required serious application check dictionary sparsity level suitable application existing results new results needed similarly algorithms discussed new algorithms need designed 
general defined problem suitability application verified 
stage reader may skeptical applications inspired solving really figures worked large scale applications 
presents compressed sensing dynamic mri real time acquisition heart motion michael lustig workers stanford mri lab 
obtain successful recon struction moving imagery beating heart raw pseudo random samples space factor sampling solve system equations times un equations 
sparsity desired solution wavelet fourier domain exploited minimization 
presents image separation results obtained jean luc starck workers image barbara decomposed piece wise smooth cartoon texture mca described :10.1.1.5.1195
dictionary combining representations curvelets compressed sensing dynamic acquisition heart motion 
dynamic image created setting classical sampling theorem allows reconstruction rates higher frames second 
attempts reconstruct faster frame rates classical linear tools minimization fail badly exhibiting temporal blurring artifacts see panel 
penalized reconstruction sparse transform coefficients dynamic sequence reconstructed higher rate frames second significantly reduced image artifacts see panel 
top images show heart time frames bottom ones presents time series cross section heart 
information 
representing cartoon part local overlapped dct texture 
second row taken presents inpainting results missing values text recovered separation 
see successful application driven goal approximately solving 
quest dictionary fundamental ingredient definition sparse land signals deployment applications dictionary wisely choose perform signals mind 
line considered choosing pre constructed dictionaries un decimated wavelets steerable wavelets curvelets :10.1.1.3.5374
generally suitable stylized cartoon image content assumed piece wise smooth smooth boundaries 
papers provide detailed theoretical analysis establishing sparsity original image cartoon part texture part image missing data inpainting result top row mca image separation texture cartoon :10.1.1.5.1195
bottom row image inpainting filling missing pixels text image 
representation coefficients content 
tunable selection basis frame generated control particular parameter discrete continuous wavelet packets parameter time frequency subdivision parameter spatial partition 
third option build training database signal instances similar anticipated application build empirically learned dictionary generating atoms come underlying empirical data theoretical model dictionary application fixed redundant dictionary 
explore third option detail 
assume training database yi thought generated fixed unknown model 
training database allow identify generating model specifically dictionary 
difficult problem studied initially field olshausen motivated analogy atoms dictionary population simple cells visual cortex :10.1.1.134.6077
thought learning dictionary empirically model evolutionary processes led existing collection simple cells able find rough empirical match properties learned dictionary known properties population simple cells 
extended methodology algorithm various forms 
describe related training mechanism leaning 
assume model deviation known aim estimation consider optimization problem min xi xi subject yi axi 
solve obtain dictionary gives sparse approximation elements training set 
roles penalty constraints equation reversed choose constrain sparsity obtain best fit sparsity assume known min xi yi axi subject xi 
problems properly posed 
meaningful solutions 
obvious scaling permutation columns 
fix scale ordering unclear meaningful answer general 
uniqueness property underlying problem implying dictionary exists explains sparsely set training vectors 
surprisingly case answer problem shown 
suppose exists dictionary spark sufficiently diverse database examples representable spark atoms 
dictionary permits equally sparse representation elements training database derivable simply rescaling permutation columns 
readers may prefer think terms matrix factorizations 
concatenate database vectors column wise forming matrix similarly corresponding sparse representa tions matrix size dictionary satisfies ax 
problem discovering underlying dictionary problem discovering factorization matrix ax indicated shapes sparse columns 
matrix factorization viewpoint connects problem related problems nonnegative matrix factorization sparse nonnegative matrix factorization :10.1.1.9.3590
clearly general practical algorithm solving problem reasons general practical algorithm solving 
just lack general guarantees reason try heuristic methods see specific cases 
view problem posed equation nested minimization problem inner mini mization number nonzeros representation vectors xi fixed outer minimization strategy alternating minimization natural th step dictionary th step solve instances database entry yi dictionary 
gives matrix solve squares arg min ax yx 
may re scale columns obtained dictionary 
increment satisfied convergence criterion repeat loop 
block coordinate descent algorithm proposed termed method directions mod 
improved update rule dictionary proposed atoms columns handled sequentially 
leads svd algorithm developed demonstrated 
keeping columns fixed apart th aj column updated coefficients multiply term minimized see rewritten ax ajx ajx aj xt 
description aj xt referring term stands th row expression target update ej simplify notation omit iteration number ajx known pre computed error matrix 
optimal aj xt minimizing equation furnished svd rank approximation typically yields dense vector xt order minimize term fixing cardinalities representations subset columns ej taken correspond signals example set th atom columns non zero 
sub matrix rank approximation svd applied updating atom aj coefficients deploy sparse representations 
dual update leads substantial speedup convergence training algorithm 
interestingly process considered case constraining representation coefficients binary posed problem reduces clustering task 
furthermore case training algorithms simplify known means algorithm 
iteration means computes means different subsets svd algorithm performs svd different sub matrices name svd assumed number columns 
exhibit describes mod svd algorithms detail 
applications image processing sparse representation viewpoint discussed far merely viewpoint 
theoretical results merely tell sparse modeling favorable cases mathematically founded enterprise practically useful computational tools 
way tell sparse modeling works real world apply see performs 
section review selected results applying viewpoint image compression image denoising 
due space imitations unable discuss interesting examples including problems array processing inpainting images image decomposition cartoon texture :10.1.1.5.1195
applications rely adapted dictionary svd algorithm 
successful applications demonstrated dictionaries curvelets see examples 
task train dictionary sparsely represent data yi approximating solution problem posed equation 
initialization initialize initialize dictionary build ir random entries randomly chosen examples 
normalization normalize columns 
main iteration increment apply sparse coding stage pursuit algorithm approximate solution xi arg min yi subject 
obtaining sparse representations xi form matrix 
dictionary update stage options mod update dictionary formula arg min ax yx svd procedure update columns dictionary obtain repeat 
define group examples atom aj 
compute residual matrix ej ajx xj th rows matrix 
restrict ej choosing columns corresponding obtain er apply svd decomposition er vt update dictionary atom aj representations 
stopping rule smaller preselected threshold 
apply iteration 
output desired result 
exhibit mod svd dictionary learning algorithms 
compression facial images application image compression fundamental today digital imagery exploit daily basis digital cameras satellite internet downloads 
sparse representation lies successful applications image compression jpeg jpeg compression standards exploit fact natural images sparse representations fourier wavelet domains respectively 
course sparsity develop effective content transmission system particular efficient coding sparse vectors needed order obtain bit streams 
images commonplace hear highly targeted applications imaging biometric identification fingerprint searching cardiac imaging 
specialized application raises issue application specific compression 
generic representation fourier wavelet transform employs dictionary specific image content encountered application 
section address compression facial images considering application passport photograph storage digital id system 
methodology algorithms gather passport photo database facial images size pixels train test compression algorithms described shortly 
consider types compression algorithms fixed transform algorithms dct jpeg dwt jpeg content adaptive algorithms learned dictionaries principal component analysis pca svd 
adapted methods dictionaries learned disjoint image patches size pixels extracted training images database 
location image obtains different dictionary content manifested examples 
pca technique models patch realization multivariate gaussian distribution learned dictionary simply set usual principal axes empirical covariance matrix 
implementation methods available default parameters 
file sizes include headers principle omitted obtaining better compression 
option impact 
svd technique models patch approximately sparse linear combination atoms learned image database 
pca course classical relatively inexpensive compute svd technique time consuming 
svd training matlab requires hours 
training compression decompression facial image takes second stored dictionaries encoder decoder 
details experiments concentrate showing main results 
experiments results method evaluated different compression ratios corresponding bytes respectively 
rendering transform coefficients byte streams done crudely leaving open possibility improvements 
standard evaluate performance psnr defined snr log original image pixels compressed decompressed ones 
figures show results bytes bytes image respectively testing images test set opposed training set learning dictionaries 
seen svd method far better image quality peak signal noise ratio psnr 
block artifacts seen results due block coding employed improvement introduced selective smoothing block edges 
denoising images application images contain noise may arise due sensor imperfection poor illumination com munication errors 
removing noise great benefit applications wide variety techniques proposed ideas disparate partial differential equations local polynomial spline fitting filtering hidden markov models shrinkage transform coefficients 
extensive comparison leading methods :10.1.1.3.5374
original jpeg db jpeg db pca db svd db original jpeg db jpeg db pca db svd db original jpeg db jpeg db pca db svd db face image compression bytes image comparison results jpeg jpeg principal component analysis pca sparse coding svd dictionary training 
values result show peak signal noise ratio psnr 
original jpeg db jpeg db pca db svd db original jpeg db jpeg db pca db svd db original jpeg db jpeg db pca db svd db face image compression bytes image comparison results jpeg jpeg principal component analysis pca sparse coding svd dictionary training 
values result show peak signal noise ratio psnr 
sparse representation applicable image denoising years researchers developed applied novel transforms represent images sparsely traditional transforms harmonic analysis primary application area image denoising 
transforms steerable wavelets curvelets related direction sensitive transforms ability sparsely represent edges fourier wavelet methods 
shrinkage transform coefficients followed reconstruction reduction image noise observed edges approximately preserved :10.1.1.3.5374:10.1.1.24.4098
methodology algorithms denoising methods described take different approach training dictionary image content directly 
option standard library clean images corel library images develop standard dictionary adapted general images 
ambitious goal develop dictionary adapted problem hand learning dictionary noisy image 
presumably yields sparser representations effective denoising strategy 
fact papers apply svd algorithm shown exhibit image patches carved noisy image 
curse dimensionality learning structure data rapidly intractable dimension feature vector increases 
svd algorithm relatively small image patches cited papers patches 
cited papers apply sparse representation patch extracted image pixel average results patches containing pixel 
experiments results results reported reproduced best seen 
shows dictionaries obtained global group natural scene images adapted image barbara 
dictionaries atoms 
denoising results demonstrated methods 
results reported psnr original image prior additive noise denoising result 
candidate dictionaries globally trained svd dictionary general images svd dictionary trained noisy barbara image directly 
concluding remarks summary fields signal image processing offer unusually fertile playground applied distance mathematical idea application product may small 
discussed concept sparse representations signals images 
sparse representation poorly defined problem computationally impractical goal general pointed mathematical results showing certain conditions obtain results positive nature guaranteeing uniqueness stability computational practicality 
inspired positive results explored potential applications sparse representation real signal processing settings showed certain denoising compression tasks content adapted sparse representation provides state art solutions 
open questions list research directions original image noisy image db denoised image global trained dictionary db denoised image adaptive dictionary db denoising comparisons additive noise standard deviation unprocessed snr db 
results globally trained adapted svd dictionaries patches show improvement db respectively 
sparsity redundancy form model images 
cal empirical claim goes expected try see desirable 
required carefully map connections signal model studied ones markov random field mrf pca example regularization 
general purpose compression algorithm sparsity redundancy 
watermarking 
encryption 
classification 
applications addressed show strength sparsity redundancy concepts representation 
model perfect undermines ability improve perfor mance applications 
model extensions better match true data desired 
example defining statistical dependencies representation coefficients necessary 
synthesize signals sparsity redundancy model 
direct approach randomly generating sparse vector entries lead natural images dictionary quality 
modifications model necessary enable synthesis 
training dictionary limited small signal dimensions 
limitation 
multi scale concept natural context 
uniqueness stability learned dictionaries established 
empirically training generates denoising effect careful documentation effect theoretical understanding needed 
redundancy dictionary chosen wisely 
critical value performance deteriorates 
current applications tend address question empirically better understanding role redundancy required 
dictionary training algorithms algorithm guaranteed appropriate conditions say involving coherence 
example desired dictionary known small mutual coherence process initialized arbitrary matrix sufficiently small mutual coherence guaranteed path desired dictionary 
stanford joel caltech extensive editorial suggestions fraction accommodated 
michael lustig stanford jean luc starck cea providing figures illustrating 
elad bruckstein svd non negative variant dictionary design proceedings spie conference wavelets vol 
july 
elad bruckstein uniqueness overcomplete dictionaries practical way retrieve journal linear algebra applications july 
elad bruckstein 
svd algorithm designing overcomplete dictionaries sparse representation ieee trans 
signal processing november 
barron cohen devore adaptive approximation learning greedy algorithms submitted annals statistics 
dias bayesian wavelet image deconvolution gem algorithm exploiting class heavy tailed priors ieee trans 
image processing april 
bj rner las sturmfels white ziegler oriented matroids encyclopedia mathematics vol 
cambridge university press 

starck elad morphological diversity source separation ieee signal processing letters july 

starck sz cmb reconstruction generalized morphological component analysis astronomical data analysis ada marseille france september 
simoncelli image compression joint statistical characterization wavelet domain ieee trans 
image processing 
elad face image compression svd algorithm submitted ieee trans 
image processing 
calderbank shor quantum error correcting codes exist phys 
rev august 
cand donoho recovering edges ill posed inverse problems optimality curvelet frames annals statistics 
cand donoho new tight frames curvelets optimal representations objects piecewise singularities comm 
pure appl 
math 
cand romberg practical signal recovery random projections wavelet xi proc 
spie conf 

cand romberg tao robust uncertainty principles exact signal reconstruction highly incomplete frequency information ieee trans 
information theory 
cand romberg tao quantitative robust uncertainty principles optimally sparse decompositions appear foundations computational mathematics 
cand romberg tao stable signal recovery incomplete inaccurate measurements appear communications pure applied mathematics 
cand tao decoding linear programming ieee trans 
information theory december 
chang yu vetterli adaptive wavelet thresholding image denoising compression ieee trans 
image processing 
chang yu vetterli wavelet thresholding multiple noisy image copies ieee trans 
image processing 
chang yu vetterli spatially adaptive wavelet thresholding context modeling image denoising ieee trans 
image processing 
chandrasekaran baron baraniuk sparse representation multidimensional functions containing smooth discontinuities ieee symposium information theory chicago il 
chen billings luo orthogonal squares methods application non linear system identification international journal control 
chen donoho saunders atomic decomposition basis pursuit siam journal scientific computing :10.1.1.135.1907
chen donoho saunders atomic decomposition basis pursuit siam review 
cohen devore xu nonlinear approximation space bv ir american journal mathematics june 
coifman donoho translation invariant denoising 
wavelets statistics lecture notes statistics 
coifman meyer quake wickerhauser signal processing compression wavelet packets 
progress wavelet analysis applications toulouse pp 

coifman wickerhauser adapted waveform analysis tool modeling feature extraction denoising optical engineering july 
optimality backward greedy algorithm subset selection problem siam journal matrix analysis applications 
daniel wood fitting equations data computer analysis data nd edition john wiley sons 
daubechies de mol iterative thresholding algorithm linear inverse problems sparsity constraint communications pure applied mathematics 
davis mallat adaptive greedy approximations journal constructive approximation 
davis mallat zhang adaptive time frequency decompositions optical engineering 
devore jawerth lucier image compression wavelet transform coding ieee trans 
information theory 
devore remarks greedy algorithms advances computational mathematics 
vetterli rotation invariant texture characterization retrieval steerable wavelet domain hidden markov models ieee trans 
multimedia december 
vetterli finite ridgelet transform image representation ieee trans 
image processing 
vetterli framing pyramids ieee trans 
signal processing 
vetterli transform efficient directional multiresolution image representation ieee trans 
image image processing 
donoho compressed sensing ieee trans 
information theory april 
donoho de noising soft thresholding ieee trans 
information theory 
donoho large underdetermined systems linear equations minimal norm solution sparsest solution communications pure applied mathematics june 
donoho large underdetermined systems linear equations minimal norm near solution approximates sparsest near solution communications pure applied mathematics july 
donoho elad optimally sparse representation general non orthogonal dictionaries minimization proc 
national academy sciences 
donoho elad stability basis pursuit presence noise signal processing march 
donoho elad stable recovery sparse overcomplete representations presence noise ieee trans 
information theory 
donoho huo uncertainty principles ideal atomic decomposition ieee trans 
information theory 
donoho johnstone ideal denoising orthonormal basis chosen library bases comptes rendus del des sciences series 
donoho johnstone ideal spatial adaptation wavelet shrinkage biometrika 
donoho johnstone kerkyacharian picard wavelet shrinkage journal royal statistical society series methodological 
donoho johnstone minimax estimation wavelet shrinkage annals statistics 
donoho starck uncertainty principles signal recovery siam journal applied mathematics june 
donoho non negative matrix factorization give correct decomposition parts 
advances neural information processing proc 
nips mit press 
donoho tanner sparse nonnegative solutions underdetermined linear equations linear programming pro ceedings national academy sciences july 
donoho tanner randomly projected simplices high dimensions proceedings national academy sciences july 
donoho extensions compressed sensing signal processing march 
donoho 
starck sparse solution underdetermined linear equations stagewise orthogonal matching pursuit submitted ieee trans 
signal processing 
efron hastie johnstone tibshirani angle regression annals statistics 
elad simple shrinkage relevant redundant representations appear ieee trans 
information theory 
elad sparse representations sparsest possible eurasip journal applied signal processing 
elad image denoising learned dictionaries sparse representation international conference computer vision pattern recognition new york june 
elad image denoising sparse redundant representations learned dictionaries ieee trans 
image processing december 
elad bruckstein generalized uncertainty principle sparse representation pairs bases ieee trans 
information theory 
elad image denoising shrinkage redundant representations ieee conference computer vision pattern recognition cvpr ny june 
elad class optimization methods linear squares non quadratic regularization appear applied computational harmonic analysis 
elad 
starck donoho simultaneous cartoon texture image inpainting morphological component analysis mca journal applied comp 
harmonic analysis 
multi frame compression theory design eurasip signal processing 
radha transform image de noising cycle spinning proceedings asilomar conference signals systems computers pp 
november 
radha translation invariant transform application image denoising ieee trans 
image processing november 

starck sparse representation image deconvolution iterative thresholding astronomical data analysis ada marseille france september 
nemirovsky sparse representation pairs bases ieee trans 
information theory june 
figueiredo nowak em algorithm wavelet image restoration ieee trans 
image processing 
figueiredo nowak bound optimization approach wavelet image deconvolution ieee international conference image processing icip genoa italy september 
figueiredo dias nowak majorization minimization algorithms wavelet image tion appear ieee trans 
image processing 
fletcher rangan goyal ramchandran analysis denoising sparse approximation random frame asymptotics ieee int 
symp 
inform 
theory 
fletcher rangan goyal ramchandran denoising sparse approximation error bounds rate distortion theory eurasip journal applied signal processing 
fuchs sparse representations arbitrary redundant bases ieee trans 
information theory 
fuchs recovery exact sparse representations presence bounded noise ieee trans 
information theory 
gersho gray vector quantization signal compression kluwer academic publishers dordrecht lands 
gilbert muthukrishnan strauss approximation functions redundant dictionaries coherence th ann 
acm siam symposium discrete algorithms 
golub van loan matrix computations johns hopkins studies mathematical sciences third edition 
rao sparse signal reconstruction limited data re weighted norm mini mization algorithm ieee trans 
signal processing 
simple test check optimality sparse signal approximation signal processing march 
nielsen sparse decompositions unions bases ieee trans 
information theory 
exponential convergence matching pursuits quasi incoherent dictionaries ieee trans 
information theory 
nonlinear approximation image recovery adaptive sparse reconstructions iterated denoising part theory ieee trans 
image processing 
nonlinear approximation image recovery adaptive sparse reconstructions iterated denoising part ii adaptive algorithms ieee trans 
image processing 
hastie tibshirani friedman elements statistical learning 
new york springer 
horn johnson matrix analysis new york cambridge university press 
hoyer non negative matrix factorization sparseness constraints journal machine learning research 
huo sparse image representation combined transforms phd thesis stanford 
hyv rinen karhunen oja 
independent component analysis wiley interscience 
jain fundamentals digital image processing englewood cliffs nj prentice hall 
jansen noise reduction wavelet thresholding springer verlag new york 
construction nearest points norms journal approximation theory 
widths certain finite dimensional sets classes smooth functions 
akad 
nauk sssr ser 
mat pp 

elad cross modality localization sparsity appear ieee trans 
signal processing 

kim koh lustig boyd method large scale regularized squares problems applications signal processing statistics submitted 
kreutz delgado murray rao lee sejnowski dictionary learning algorithms sparse representation neural computation 
kruskal way arrays rank uniqueness trilinear decompositions application arithmetic complexity statistics linear algebra applications 
lang guo noise reduction undecimated discrete wavelet transform ieee signal processing letters 
lee seung learning parts objects non negative matrix factorization nature 
bimbot learning unions orthonormal bases thresholded singular value decomposition icassp ieee conf 
acoustics speech signal proc 
lewicki olshausen probabilistic framework adaptation comparison image codes journal optical society america optics image science vision 
lewicki sejnowski learning overcomplete representations neural computation 
li cichocki 
amari analysis sparse representation blind source separation neural computation 
liu sidiropoulos cramer rao lower bounds low rank decomposition multidimensional arrays ieee trans 
signal processing 
lustig donoho pauly sparse mri application compressed sensing rapid imaging submitted magnetic resonance medicine 
lustig santos donoho pauly sparse high frame rate dynamic mri exploiting spatio temporal sparsity proceedings th annual meeting seattle 
elad sapiro sparse representation color image restoration submitted ieee trans 
image processing 
willsky optimal sparse representations general overcomplete bases ieee international conference acoustics speech signal processing icassp may montreal canada 
willsky sparse signal reconstruction perspective source localization sensor arrays ieee trans 
signal processing 
mallat wavelet tour signal processing academic press 
mallat sparse geometric image representation ieee trans 
image processing 
mallat le pennec bandelet image approximation compression siam journal multiscale modeling simulation 
mallat zhang matching pursuits time frequency dictionaries ieee trans 
signal processing 
mendelson uniform uncertainty principle bernoulli ensembles appear 
mendelson reconstruction processes comptes rendus mathe 
meyer fast adaptive wavelet packet image compression ieee trans 
image processing 
meyer coifman multilayered image representation application image compression ieee trans 
image processing 
meyer coifman tool directional image analysis image compression applied compu tational harmonic analysis 
moulin liu analysis multiresolution image denoising schemes generalized gaussian complexity priors ieee trans 
information theory 
natarajan sparse approximate solutions linear systems siam journal computing 
olshausen field natural image statistics efficient coding network computation neural systems 
olshausen field emergence simple cell receptive field properties learning sparse code natural images nature 
olshausen field sparse coding overcomplete basis set strategy employed 
vision research 
osborne new approach variable selection squares problems ima numerical analysis 
orthogonal matching pursuit recursive function approximation ap plications wavelet decomposition seventh asilomar conference signals systems computers 
peterson jr error correcting codes nd edition mit press cambridge mass 
volume convex bodies banach space geometry cambridge university press 
plumbley geometry homotopy sparse representations proceedings workshop signal processing adaptive sparse structured representations november rennes france 
plumbley polar polytopes recovery sparse representations preprint arxiv cs oct 
plumbley recovery sparse representations polytope faces pursuit proceedings th international conference independent component analysis blind source separation ica charleston sc usa march lncs pp 
springer verlag berlin 
portilla wainwright simoncelli image denoising scale mixtures gaussians wavelet domain ieee trans :10.1.1.3.5374
image processing 
michael elad image sequence denoising sparse redundant representations submitted ieee trans :10.1.1.3.5374
image processing 
rao palmer kreutz delgado subset selection noise diversity measure minimization 
ieee trans 
signal processing 
rao kreutz delgado affine scaling methodology best basis selection ieee trans 
signal processing 
geometric approach error correcting codes reconstruction signals technical report department mathematics university california davis 
rudin osher fatemi nonlinear total variation noise removal algorithms physica bruce tseng block coordinate relaxation methods nonparametric signal denoising wavelet dictionaries journal computational graphical statistics 
simoncelli adelson noise removal bayesian wavelet coring proceedings international conference image processing switzerland 
september 
simoncelli freeman heeger shiftable multi scale transforms ieee trans 
information theory 

starck cand donoho curvelet transform image denoising ieee trans 
image processing 

starck elad donoho redundant multiscale transforms application morphological component separation advances imaging electron physics 

starck elad donoho image decomposition combination sparse representations variational approach 
ieee trans 
image processing 

starck murtagh undecimated wavelet decomposition reconstruction ieee transactions image processing 
heath frames applications coding communication applied compu tational harmonic analysis 
condition number random matrices complexity pp 

spaces large distance random matrices amer 
math pp 


jpeg image compression fundamentals standards practice kluwer aca publishers norwell ma usa 
greedy algorithms term approximation journal approximation theory 
weak greedy algorithms advances computational mathematics 
greed algorithmic results sparse approximation ieee trans 
information theory october 
just relax convex programming methods subset selection sparse approximation ieee trans 
information theory march 
gilbert signal recovery random measurements orthogonal matching pursuit submitted publication april 
gilbert muthukrishnan strauss improved sparse approximation quasi incoherent ieee international conference image processing barcelona september 
properties linear dependence american journal mathematics july 
sparse solution underdetermined linear systems algorithms applications phd thesis stanford 
donoho breakdown equivalence minimal norm solution sparsest solution signal processing pp 
march 
noise sensitivity sparse signal representations reconstruction error bounds inverse problem 
ieee trans 
signal processing 
pearlmutter blind source separation sparse decomposition signal dictionary neural compu tation 

