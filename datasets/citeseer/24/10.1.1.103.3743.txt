genetic programming kernel learning evolving subsets selection christian marc schoenauer mich le sebag marco information systems institute universit de lausanne ch switzerland 
christian marco unil ch tao inria cnrs umr lri bat 
universit paris sud orsay cedex france 
marc schoenauer michele sebag lri fr 
support vector machines svms established machine learning ml algorithms 
rely fact linear learning formalized posed optimization problem ii non linear learning brought linear learning kernel trick mapping initial search space high dimensional feature space 
kernel designed ml expert governs efficiency svms approach 
new approach automatic design kernels genetic programming called evolutionary kernel machine ekm 
ekm combines founded fitness function inspired margin criterion evolution framework ensuring computational scalability approach 
empirical validation standard ml benchmark demonstrates ekm competitive state art svms tuned hyper parameters 
kernel methods including called support vector machines svms established learning approaches strong theoretical foundations successful practical applications 
svms rely main advances statistical learning 
linear supervised machine learning task set posed quadratic optimization problem 
second setting extended non linear learning kernel trick manually designed change representation mapping initial space called feature space linear hypotheses characterized terms scalar product feature space kernel 
hypotheses correspond non linear hypotheses initial space 
specific kernels proposed literature designing kernel suited application domain dataset far remains art science 
proposes system evolutionary kernel machine ekm automatic design data specific kernels 
ekm applies genetic programming gp construct symmetric functions kernels optimizes fitness function inspired margin criterion 
kernels assessed nearest neighbor classification process 
order cope computational complexity cooperative evolution prototype subset selection gp kernel design done hand host parasite competitive evolution fitness case subset selection hand 
organized follows 
section introduces formal background notations kernel methods 
sections respectively describe gp representation fitness function proposed ekm 
scalability issues addressed evolutionary framework introduced section 
results benchmark problems section 
related works discussed section concluding section 
formal background notations supervised machine learning takes input dataset xi yi xi yi examples xi yi respectively stand description label th example 
goal construct hypothesis mapping minimal generalization error 
vectorial domains ir considered binary classification problems considered rest section 
due space limitations reader referred comprehensive presentation svms 
simplest linear separable case hyper plane maximizing geometrical margin distance closest examples constructed 
label associated example sign xi xi denotes scalar product xi 
denotes mapping instance space feature space kernel defined ir conditions kernel trick non linear classifiers con structed linear case characterized ik xi svms kernel trick revisit learning methods involving distance measure 
kernel nearest neighbor kernel nn algorithm revisit nearest neighbors nn considered 
distance dissimilarity function defined instance space set labelled examples 
xn yn instance classified nn algorithm determines examples closest ii outputs majority class examples 
kernel nn proceeds nn distance dk defined kernel section 
standard kernels ir include gaussian polynomial kernels noted addition multiplication compositions kernels kernels standard svm machinery find optimal value hyper parameters finite set 
quite opposite functional symbolic optimization tackled best knowledge genetic programming 
genetic programming kernels evolutionary kernel machine applies gp determine symmetric functions ir ir best suited dataset hand 
shown table main difference compared standard symbolic regression terminals symmetric expressions xi xix xjx enforcing symmetry kernels 
initialization gp individuals done ramped half half procedure 
selection probability terminals ai mi ii si respectively ci divided resp 
dimension initial instance space ir 
kernel functions built table satisfy mercer condition required svm optimization 
kernels assessed kernel nn classification rule fact necessarily positive limitation 
quite contrary ekm kernels achieve feature selection typically terminals associated non informative features disappear evolution 
ekm feature selection examined 
fitness measure kernel assessed kernel nn classification rule dissimilarity dk defined dk prototype set ep 
training example assuming ep ordered increasing dissimilarity dk xi dk xi 
denotes minimum rank prototype examples class min yi denotes minimum rank prototype examples belonging class min yi 
respectively exp table 
gp primitives involved kernel functions ir name args 
description add addition values fadd 
add addition values fadd 
add addition values fadd 
sub subtraction 
mul multiplication values fmul 
mul multiplication values fmul 
mul multiplication values fmul div 
protected division max maximum value fmax max 
min minimum value fmin min 
exp exponential value exp 
pow square power ai 
add th components xi mi 
multiply th components xix si 
maximum th components max xi 
ii 
minimum th components min xi 
ci 

crossed multiplication addition th th components xix xjx 
dot scalar product 
euc euclidean distance 
ephemeral random constants generated uniformly 
noted quality kernel nn classification assessed 
higher confident classification respect perturbations ep dk measures margin respect kernel nn 
accordingly prototype set ep 
fitness case subset es 
ym fitness function associated defined computation linear complexity number prototypes number fitness cases 
standard setting ep es coincide training set 
quadratic complexity fitness computation respect number training examples incompatible scalability approach 
parameters gp kernels population size size prototype subset individuals size fitness case subset individuals number offsprings prototype species number offsprings fitness case species fraction prototype subset individuals replaced mutation fraction fitness case subset individual replaced mutation 

initial prototype subset stratified uniform sample size 
initial fitness case subset stratified uniform sample size 
gp initial population gp kernels 

loop 
apply selection variation operators gp kernel population con gp 
compute fitness 
prototype subset case subset denote best fitness generate offsprings replacing fraction prototypes uniform stratified sampling assess offsprings set best offspring generate offsprings replacing fraction fitness case uniform stratified sampling assess offsprings set best offspring 

output selected 
minimizing nn error rate training set associated prototype subset 
fig 
evolutionary kernel machine evolution framework tractability evolution ekm scalability obtained directions reducing number prototypes classification ii reducing size fitness case subset considered generation 
precisely evolutionary framework involving species considered detailed 
species includes gp kernels 
second species includes prototype subset fixed size subsets training set subject cooperative evolution gp kernels 
third species includes fitness case subset fixed size subsets training set subject competitive host parasite evolution gp kernels 
prototype species evolved find prototypes maximize fitness gp kernels 
fitness case species evolved find hard challenging examples minimize kernels fitness 
course danger fitness case subset ultimately capture noisy examples observed boosting framework see section 
table 
uci data sets experimentations 
data set size features classes application domain bcw breast cancer benign malignant 
bupa liver disorders disorders disorder 
bos boston housing median value 
cmc method choice short term longterm 
ion ionosphere radar signal structure detected structure detected 
pid pima indians diabetes tested negative tested positive diabetes 
prototype selection species initialized stratified uniform sampling replacement class distribution sample dataset examples distinct 
species evolved evolution strategy generation offsprings generated uniform stratified replacement fraction parent subset assessed best kernel current kernel population 
parent subset replaced best offspring 
generation kernels assessed current prototype fitness case individuals 
experimental validation section reports experimental validation ekm standard set benchmark problems detailed table 
system implemented open beagle framework evolutionary computation 
experimental setting parameters ekm reported table 
average evolution time benchmark problem hour amd athlon 
problem ekm evaluated standard fold cross validation methodology 
data set partitioned stratified subsets training set subsets best hypothesis learned training set evaluated remaining subset test set 
accuracy averaged folds test set ranges subsets dataset fold ekm launched times best beagle gel ca table 
tableau evolutions parameters 
parameter description parameter values gp kernel functions evolution parameters primitives see table 
gp population size population individuals criterion evolution ends generations 
replacement strategy genetic operations applied generational scheme 
selection lexicographic parsimony pressure tournaments selection participants 
crossover classical subtree crossover prob 

standard mutation crossover random individual prob 

swap node mutation exchange primitive arity prob 

shrink mutation replace branch children remove branch mutated children subtrees prob 

prototype subset selection parameters prototype subset size examples prototype subset 
number offsprings offsprings generation 
mutation rate prototype examples replaced mutation 
fitness case subset selection parameters fitness case subset size examples fitness case subset 
number offsprings offsprings generation 
mutation rate selection examples replaced mutation 
hypotheses accuracy training set assessed test set reported accuracy average folds best hypotheses test set 
total ekm launched times problem 
ekm compared state art algorithms including nearest neighbor svms gaussian kernels similarly assessed fold cross validation 
nn underlying distance euclidean scaling normalization option considered parameter varied best setting kept 
gaussian svms torch implementation error cost parameter varied parameter set best setting similarly retained 
results table shows results obtained ekm compared nn gaussian svm optimal parameters algorithms 
size best gp kernel column shows bloat occurred lexicographic parsimony pressure 
algorithm shown best table 
comparative fold results nn gaussian svm ekm uci data sets optimal settings scaling nn svm 
reported test error averaged folds 
fold tested ekm solutions runs best training error assessed test set error averaged 
test error rates bold denotes statistically best results tails paired student test 
nn svm ekm data best conf 
train test best train test train best half mean set scaling error error error error error test error size bcw bos cmc ion pid performing half tested datasets frequent ties paired student test 
typically problems gaussian svm perform optimal cost error parameter high suggesting noise level datasets high 
fitness case subset selection embedded ekm favor selection noisy examples challenging gp kernels 
progressive selection mechanism account kernels gp population better filter noisy examples outliers considered research 
nn outperforms svm ekm bos problem noise level appears low 
optimal value number nearest neighbors optimal cost error suggesting error rate low 
fact error rate close explained target concept complex examples lie close frontier 
bcw differences algorithm statistically different test error rate suggesting problem easy 
ekm best performing pid problems tied nn data set 
result demonstrates kernel dissimilarity improves euclidean distance rescaling 
furthermore noted ekm classifies test examples examples prototype set nn uses training set examples problem pid problem 
known free lunch theorem applies machine learning learning method expected universally competent 
experimental validation demonstrates gp evolution kernels improve prominent learning algorithms circumstances 
related works pertinent ekm genetic kernel support vector machine gk svm 
gk svm similarly uses gp svm approach main differences compared ekm 
hand gk svm focuses feature construction gp optimize mapping kernel 
hand fitness function gk svm suffers quadratic complexity number training examples 
accordingly datasets considered experimentations small examples 
larger dataset authors acknowledge approach improve standard svm chosen parameters 
related similarly uses gp feature construction order classify time series 
set features gp trees evolved ga fitness function accuracy svm classifier 
works related evolutionary optimization svms see focus parametric optimization tuning hyper parameters involved kernels cost error 
related proposed weinberger optimizing mahalanobis distance nn margin criterion inspired ekm 
restricted linear changes representation optimization problem tackled semi definite programming 
lastly ekm inspired dynamic subset selection proposed ross developed address scalability issues ec machine learning 
evolutionary kernel machine proposed aims improve nearest neighbor classification combining original aspects 
ekm implicitly addresses feature construction problem designing new representation application domain better suited dataset hand 
contrast ekm takes advantage kernel trick gp optimize kernel function 
secondly ekm proposes evolution framework ensure scalability approach control computational complexity fitness computation 
empirical validation demonstrates new approach competitive founded learning algorithms svm nn tuned hyper parameters 
limitation approach observed known boosting algorithm competitive evolution kernels examples tends favor noisy validation examples 
perspective research exploit evolution archive estimate probability example noisy achieve sensitivity analysis 
perspective incorporate ensemble learning typically bagging boosting ekm 
diversity solutions constructed population optimization enables ensemble learning free 
acknowledgments supported postdoctoral fellowships ercim europe qu bec 

shawe taylor cristianini kernel methods pattern analysis 
cambridge university press cambridge uk 
koza genetic programming programming computers means natural selection 
mit press cambridge ma usa 
gilad bachrach tishby margin feature selection theory algorithms 
proc 
st international conference machine learning icml acm press 
duda hart stork pattern classification 
second edn 
john wiley sons new york ny usa 
yu ji zhang kernel nearest neighbor algorithm 
neural processing letters 
cristianini shawe taylor support vector machines kernel learning methods 
cambridge university press 
potter de jong cooperative coevolution architecture evolving subcomponents 
evolutionary computation 
hillis evolving parasites improve simulated evolution optimization procedure 
physica 
freund shapire experiments new boosting algorithm 
proc 
th 
international conference machine learning icml morgan kaufmann 
newman blake merz uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
genericity evolutionary computation software tools principles case study 
international journal artificial intelligence tools 
bengio mari torch modular machine learning software library 
technical report idiap rr idiap 
madden genetic kernel support vector machine description evaluation 
artificial intelligence review 
hill davis perkins ma porter theiler genetic algorithms support vector machines time series classification 
proc 
fifth conference applications science neural networks fuzzy systems evolutionary computations 

igel evolutionary tuning multiple svm parameters 
neurocomputing 
weinberger john lawrence distance metric learning large margin nearest neighbor classification 
neural information processing systems nips 

ross dynamic training subset selection supervised learning genetic programming 
parallel problem solving nature iii ppsn 
volume lncs springer verlag 
song training genetic programming half patterns example anomaly detection 
ieee transactions evolutionary computation 
