department computer science university college london combining graph laplacians semi supervised learning andreas mark herbster massimiliano pontil pontil herbster cs ucl ac uk combining graph laplacians semi supervised learning andreas mark herbster massimiliano pontil department computer science university college london gower street london wc bt england uk herbster pontil cs ucl ac uk foundational problem semi supervised learning construction graph underlying data 
propose method optimally combines number differently constructed graphs 
graphs associate basic graph kernel 
compute optimal combined kernel 
kernel solves extended regularization problem requires joint minimization data set graph kernels 
encouraging results different ocr tasks optimal combined kernel computed graphs constructed variety distance functions nearest neighbors 
semi supervised learning received significant attention machine learning years see example :10.1.1.19.3957:10.1.1.57.7612:10.1.1.14.544
defining insight semi supervised methods unlabeled data may improve performance learners supervised task 
key semi supervised learning methods builds assumption data situated low dimensional manifold ambient space data manifold approximated weighted discrete graph vertices identified empirical labeled unlabeled data 
graph construction consists stages selection distance function application distance function determine graph edges weights thereof 
example consider distance functions images euclidean distance euclidean distance combined image transformations related tangent distance determine edge set graph nearest neighbors 
common choice weight edges decreasing function distance surplus unlabeled data may improve quality empirical approximation manifold graph leading improved performances practical experience methods indicates performance significantly depends graph constructed 
model selection problem consider selection distance function parameters graph building process described 
diversity methods proposed graph construction advocate selecting single graph propose combining number graphs 
solution implements method regularization builds 
combination distance functions edge set specifications distance dataset lead specific graph 
graphs may associated kernel 
apply regularization select best convex combination kernels minimizing function trade fit data norm 
unique regularization minimization single kernel space space corresponding convex combinations kernels 
data may conserved training reduced cross validation 
section illustrates algorithm simple example 
different distance matrices images digits depicted euclidean distance distance invariant small centered 
image rotations distance invariant rotations clearly distance problematic similar nines 
performance graph regularization learning algorithm discussed section distances reported plot expected performance lower case third distance 
constructed follows 
section discuss regularization may applied single graphs 
review regularization context reproducing kernel hilbert spaces section section specialize discussion hilbert spaces functions defined graph 
review normalized laplacian graph kernel pseudoinverse graph laplacian 
section detail algorithm learning optimal convex combination laplacian kernels 
section experiments usps dataset algorithm trained different classes laplacian kernels 
background graph regularization section review graph regularization perspective reproducing kernel hilbert spaces :10.1.1.57.7612
reproducing kernel hilbert spaces set kernel function 
say reproducing kernel hilbert space rkhs ii functions reproducing kernel holds inner product particular ii tells implying property symmetric positive semi definite set inputs matrix notation regularization rkhs learns function basis available input output examples solving variational problem loss function positive parameter 
solution problem form real vector coefficients see example denotes transposition 
replacing vector right hand side equation equation optimizing respect practical situations convenient compute solving dual problem conjugate loss function defined function see example discussion 
choice loss leads different learning methods prominent square loss regularization support vector machines see example 
function graph regularization vertices undirected graph vertices adjacency matrix edge connecting zero graph laplacian matrix defined degree vertex 
identify linear space real valued functions defined graph introduce semi inner product induced semi norm semi norm 
constant vector verified noting recall connected components eigenvectors zero eigenvalues 
eigenvectors piece wise constant connected components graph 
particular connected constant vector system eigenvalues vectors eigenvalues non decreasing order subspace define linear zero eigenvalue orthogonal eigenvectors eigenvector zero eigenvalue 
framework wish learn function basis set labeled vertices 
loss generality assume vertices labeled corresponding labels 
prescribe loss compute function function solving optimization problem note similar approach essentially obtained minimal norm interpolant labeled vertices 
functional balances error labeled points smoothness term measuring complexity graph 
note term contains information labeled unlabeled vertices graph laplacian 
ideas discuss naturally extend weighted graphs 
method special case problem 
restriction norm 
see pseudoinverse laplacian semi norm reproducing kernel example proof 
means holds reproducing kernel property th setting column 
see note analysis naturally extends case replaced positive semidefinite matrix 
particular experiments normalized laplacian matrix typically problem solved optimizing 
particular square loss regularization minimal norm interpolation requires solving squared linear system equations respectively 
contrary representer theorem express approach advantageous computed line typically advantage approach multiple problems may solved laplacian kernel 
coefficients obtained solving problem 
example square loss regularization computation parameter vector involves solving linear system equations learning convex combination laplacian kernels describe framework learning multiple graph laplacians 
assume graphs having vertices corresponding kernels laplacians spaces hilbert norms propose learn optimal convex combination graph kernels solve optimization problem defined set kernel problem motivated observing optimal convex combination kernels smaller right hand side individual kernel motivating expectation improved performance 
furthermore approach identify relevant kernels 
problem special case problem jointly minimizing functional convex hull kernels prescribed set problem discussed detail see case finite con sidered :10.1.1.13.6524
practical experience method indicates enhance performance learning algorithm computationally efficient solve 
solving problem important require kernels satisfy normalization condition example trace frobenius norm see discussion :10.1.1.13.6524
initialization choose 
compute 
find terminate 
compute 
set solution problem exist algorithm compute optimal convex combination kernels set dual problem formulation discussed see equation inner minimum rewrite problem solution saddle point problem 
problem simpler solve original problem objective function linear see discussion 
algorithms point computing saddle adapt algorithm alternately optimizes reproducibility algorithm reported 
note computed minimizer problem 
particular square loss regularization requires solving equation 
variational problem expresses optimal convex combination kernels experiments section experiments optical character recognition 
observed 
optimal convex combination kernels computed algorithm competitive best base kernels 
second observing weights convex combination distinguish strong weak candidate kernels 
proceed discussing details experimental design interleaved results 
usps dataset images handwritten digits pixel values ranging 
results pairwise classification tasks varying difficulty odd vs digit classification 
pairwise classification training set consisted images digit usps training set number labeled points chosen equal numbers digit 
odd vs digit classification training set consisted images digit usps training set number labeled points equal numbers digit 
performance averaged random selections number labeled points 
graphs constructed combining different graph construction methods experiment nearest neighbors corresponding laplacians computed associated kernels 
chose loss squared loss 
kernels obtained different types function available www stat class stanford edu data html table misclassification error percentage top standard deviation bottom best convex combination different handwritten digit recognition tasks different distance metrics transformations 
see text description 
euclidean transformation tangent distance task labels vs vs vs vs vs labels odd vs graphs vary widely necessary renormalize 
chose normalize kernel training process frobenius norm submatrix corresponding labeled data 
observed similar results obtained normalizing trace submatrix 
regularization parameter set algorithms 
convex minimization starting kernel algorithm average kernels maximum number iterations table shows results obtained graph construction methods 
method euclidean distance images euclidean distance 
second method transformation distance images smallest euclidean distance pair transformed images determined applying number affine transformations thickness transformation see information 
optimal distances approximated matlab constrained minimization function 
third method tangent distance described order approximation transformations 
columns table euclidean distance columns image transformation distance columns tangent distance 
columns methods jointly compared 
results indicate combining different types kernels algorithm tends select effective ones case tangent distance kernels lesser degree transformation distance kernels matlab optimization routine 
methods performance convex combination comparable best kernels 
reports weight individual kernel learned algorithm labels pairwise tasks labels odd vs 
exception easy vs task large weights associated graphs kernels built tangent distance 
effectiveness algorithm selecting graphs kernels better demonstrated euclidean transformation kernels combined low quality kernel 
low quality kernel induced considering distances invariant rotation range image easily small distance image images image obtained rotating degrees set shows distance matrix set labeled unlabeled data eu transformation low quality distance respectively 
best error different values method error learned convex combination total learned weights method shown plot 
clear solution algorithm dominated kernels influenced ones low performance 
result error convex combination comparable euclidean transformation methods 
vs vs vs vs vs odd kernel weights euclidean transformation middle tangent 
see text information 
euclidean transformation low quality distance error error error convex combination error similarity matrices corresponding learned coefficients convex combination vs task 
see text description 
final experiment see demonstrates unlabeled data improves performance method 
euclidean transformation tang 
dist euclidean transformation tang 
dist misclassification error vs number training points odd vs classification 
number labeled points left right 
method computing optimal kernel framework regularization graphs 
method consists convex optimization problem efficiently solved algorithm 
tested optical character recognition tasks method exhibits competitive performance able select graph structures 
focus sample extensions algorithm continuous optimization versions 
particular may consider continuous family graphs corresponding different weight matrix study graph kernel combinations class 
micchelli pontil 
learning convex combinations continuously parameterized basic kernels 
proc 
th conf 
learning theory 

theory reproducing kernels 
trans 
amer 
math 
soc 
bach lanckriet jordan 
multiple kernels learning conic duality smo algorithm 
proc 
int 
conf 
machine learning 
belkin niyogi 
regularization semi supervised learning large graphs 
proc 
th conf 
learning theory colt 
belkin niyogi 
semi supervised learning riemannian manifolds 
mach 
learn blum chawla 
learning labeled unlabeled data graph proc 
th international conf 
learning theory chung 
spectral graph theory 
regional conference series mathematics vol 

hastie simard 
models metrics handwritten character recognition 
statistical science 
herbster pontil wainer 
online learning graphs 
proc 
nd int 
conf 
machine learning appear 
joachims 
transductive learning spectral graph partitioning 
proc 
international conference machine learning icml 
kondor lafferty 
diffusion kernels graphs discrete input spaces 
proc 
th int 
conf 
machine learning 
lanckriet cristianini bartlett el ghaoui jordan :10.1.1.13.6524
learning kernel matrix semidefinite programming 
machine learning research 
lin zhang 
component selection smoothing smoothing spline analysis variance models 
institute statistics mimeo series ncsu january 
micchelli pontil 
learning kernel function regularization 
preprint 
ong smola williamson 

advances neural information processing systems becker eds mit press cambridge ma 
smola kondor 
kernels regularization graphs 
proc 
th conf 
learning theory colt 
vapnik 
statistical learning theory 
wiley new york 
wahba 
splines models observational data 
series applied mathematics vol 
siam 
zhang 
dual formulation regularized linear systems convex risks 
machine learning pp 

zhu ghahramani lafferty 
semi supervised learning gaussian fields harmonic functions 
proc 
th int 
conf 
machine learning 
zhu kandola ghahramani lafferty 
nonparametric transforms graph kernels semi supervised learning 
advances neural information processing systems 
