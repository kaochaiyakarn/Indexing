delft university technology delft center systems control interactive collaborative information systems collaborative decision making technical report learning coordination dynamic multiagent systems delft center systems control delft university technology cd delft netherlands phone secretary fax url www tudelft nl bu de babu ska october learning coordination dynamic multiagent systems bu de babu ska october research financially supported ministry economic affairs netherlands project interactive collaborative information systems 

contents list figures list tables vii list symbols ix list abbreviations xiii multiagent systems adaptive learning 
theoretical framework 
communication 
agent roles 
credit assignment 
taxonomy 
agent related dimensions 
environment related dimensions 
examples 
conventional control 
model adaptive control 
team model 
multiagent reinforcement learning 
single agent case 
formal model 
learning goal 
markov property 
value functions optimality 
exploration issue 
solution techniques 
multiagent case 
formal model 
solution concepts 
learning goal 
single agent techniques ma rl 
fully cooperative multiagent teams 
iii general multiagent systems 
stateless problems 
multiple state problems 
adaptive techniques 
parametric adaptation 
structural adaptation 
realistic rl 
concluding remarks 
multiagent learning methods evolutionary techniques 
heuristic approaches 
concluding remarks 
coordination 
taxonomy 
coordination frameworks 
learning coordination 
learning agents 
value coordination 
social conventions 
roles 
coordination graphs 
concluding remarks 
applications domains 
robotic teams 
distributed control 
logistics 
information systems 
concluding remarks 
research agenda 
bibliography iv list figures schematic view dmas 
conventional control scheme 
model adaptive control scheme 
reinforcement learning model 
example steam operator hierarchy 
decision making roles social conventions 
example coordination graph 
decision making roles coordination graphs 
list tables correspondences com mtdp communicative dmas elements correspondences mdp dmas elements 
correspondences markov game dmas elements 
correspondences alliance dmas elements 
correspondences load balancing model dmas elements correspondences social conventions emergence framework dmas elements 
correspondences steam communicative dmas elements vii list symbols common notations generic placeholder list argument joint variable collected agents vector transpose vector concatenation tuple joint collection strategies policies deterministic variant originally stochastic function parameterized function value alternate variable optimal value estimated value probability conditional probability probability distribution argument set operators vector norm set cardinality expectation ef expectation conditioned function ef expectation conditioned function initial condition product sets discrete difference discrete th order difference dynamic agents multiagent systems discrete time index set agents ix agent variable agent index agent index number agents system environment state space environment state variable environment dynamics agent internal state space agent internal state variable agent internal state space adaptation mapping agent internal dynamics parameters vector parameterized agent internal dynamics parameter space parameterized agent internal dynamics adaptation mapping agent internal dynamics agent observation space agent observation variable agent observation distribution agent action space agent action variable agent policy joint action space joint action variable joint agent policy parameters vector parameterized agent policy parameter space parameterized agent policy agent policy adaptation mapping communication communication channel state space state space environment excluding communication channel communication channel state variable state variable environment excluding communication channel communication channel dynamics dynamics environment excluding communication channel set agent messages rcv message received agent snd message sent agent joint message sent agents agent space environment observations agent environment observation variable agent environment observation distribution rcv agent message distribution agent domain level action space agent domain level action agent domain level policy snd agent message sending policy joint domain level action space joint domain level action agent domain level observations handling dynamics rcv agent messages handling dynamics roles set agent roles space ordinary agent actions agent role agent ordinary action agent role choice policy agent ordinary action choice policy role constraints function reinforcement learning reward function reward variable state value function action value function eligibility trace discount factor learning rate recency factor exploration probability agent strategy joint strategy reduced strategy profile excluding agent xi list abbreviations mas multiagent system dmas dynamic multiagent system mas adaptive learning multiagent system rl reinforcement learning ma rl multiagent reinforcement learning mdp markov decision process mmdp multiagent markov decision process com mtdp communicative markov team decision problem interactive collaborative information systems sarsa state action reward state action wolf phc win learn fast policy hill climbing alliance learning alliance steam shell teamwork xiii chapter multiagent systems adaptive learning agent informally entity perceive environment sensors act actuators russell norvig 
definition broad 
includes robotic agents perceive environment cameras distance sensors act motors grippers 
includes process controllers sensors measure outputs process case environment act command signals 
includes humans perceiving world senses acting motor verbal skills 
multiagent system mas informally collection agents interact vlassis 
examples mas involving types agents mentioned order teams robots distributed networks controllers social groups humans 
formalize notions foreword section 
focus motivating mas learning mas 
specifically interested synthetic mas robotic teams controller networks exemplified 
mas useful modeling analysis design systems control distributed autonomous decision makers 
mas arise naturally viable solution representing considered problem 
instance natural way looking distributed systems robotic teams controller networks 
centralized perspective systems typically help ignores interactions autonomous decision makers interactions help damage point rendering impossible activity system 
mas provide alternative perspective systems originally represented ways 
instance resource allocation seen centralized decision making problem single central scheduler assigns resources users 
seen set autonomous proactive users compete gain access resources 
perspective provide valuable insight problem possible solutions 
mas offer potential advantages centralized systems stone veloso speed system activity due parallel computation 
robustness reliability capabilities agents overlap 
system resists failures agents having agents take activity faulty ones 
scalability flexibility 
principle mas inherently modular adding removing agents system easy 
way system adapt changing task fly needing shutdown redesigned 
ease design development maintenance 
follows inherent modularity mas 
noted point mas panacea 
potential benefits described carefully weighed simplicity centralized solution considering characteristics task 
instance task parallel distributed control possible benefit mas solution stone veloso 
learning informally acquisition incorporation knowledge skills agent leading improvement agent performance 
learning necessary mas times environment mas large complex open time varying sen weiss 
properties imply designing agent behaviour takes consideration possible circumstances agents encounter difficult impossible undertaking 
second properties openness variation time imply behaviour designed quickly obsolete environment changes 
behavioural improvement view focus survey 
complementary view pursues insight multiagent learning opposed traditional isolated machine learning possible 
insight lead novel machine learning techniques algorithms sen weiss 
adaptation necessary top learning 
reason stems fact due difficulties dealing open time varying environments multiagent learning algorithms designed unchanging environments 
typically involve fixed learning structures updated set rules involving fixed scheduled parameters 
call algorithms static learning 
allowing learning parameters structures static algorithms adapt learning processes agents able regain ability handling open time varying environments 
note adaptive learning radically different process learning 
viewed kind meta learning special case learning learn 
review organized way 
rest chapter dedicated formalizing discussion 
introduce consistent framework focusing dynamic nature learning agents adding important elements interagent communication 
taxonomy mas formalized framework 
close chapter presenting examples single agent multiagent control showing fit framework 
chapter deals application reinforcement learning rl techniques mas 
introduce single agent rl framework solutions continue presenting extension multiagent case 
analyze number adaptive rl techniques close chapter detailed presentation research opportunities 
chapter briefly presents methods multiagent learning evolutionary techniques heuristic approaches 

theoretical framework chapter deals issue coordination mas focusing relationship learning 
review prominent coordination techniques encountered literature focusing discussion learned variants techniques 
outline promising research opportunities multiagent coordination 
chapter reviews relevant application domains mas perspective interactive collaborative information systems large scale traffic incident scenario 
chapter concludes review 
summarizes main multiagent learning coordination techniques analyzed preceding chapters identified research opportunities concise set research questions 
theoretical framework defining concepts dynamic agent dynamic mas 
goal isolate elements belonging agents mind reasoning learning processes data rely purpose analyzing 
emphasize dynamic nature elements 
definition general encompass models possible variety learning coordination algorithms addressed review 
concepts tightly interrelated agents embedded multiagent world turn influenced decisions agents 
definition dynamic agent tuple internal state space agent 
observation space agent 
action space available agent 
agent transition function describing agent evolves result observations environment 
decision probability distribution agent describing behaviour 
agent initial state 
definition dynamic multiagent system dmas tuple set dynamic agents number 
environment state space 
aui joint action space environment transition probability distribution describing environment evolves result agents actions 
yi observation probability distributions describing state environment translated agent observations 
initial environment state 
denote agent set simply index notation 
indicate element belongs agent subscript index si 
interpretation agent complete control state space si dynamics pi behaviour hi 
part agent mind 
observation distribution 
strictly speaking part environment 
observation space yi set available actions ui agent interface environment 
considered sense part agent sensory body endowed change 
observation function known agent agents sees results applying function state environment 
observation functions part environment distinct agent agent sees world eyes 
environment state observation internal state action spaces may continuous discrete case infinite finite 
spectra observations yi agents may experience sets actions ui available agents may depend state environment 
robotic agent moves narrow straight underground tunnel observe position sun move left right 
prevent notation cluttered definition capture dependence 
discrete time virtually learning coordination algorithms iteration basis 
denote current value discrete time variable readability context clearly indicates exposition refers current time step may omit time variable 
denote environment state value agent action ui 
joint agent action defined 
un denote probability agent observes yi yi time step environment state xk yi xk xk yi 
definition allows incomplete uncertain observations 
observations deterministic observation probability distribution changes function yi 
agent viewed controller environment observation may interpreted input control feedback 
denote fact agent changes internal state si result observing yi state si si pi si yi 
denote probability agent action ui step updated internal state si observation yi ui si yi hi si yi ui 
refer hi policy agent 
model assumes stochastic policies general case 
policy deterministic changes function hi si yi ui 
agent interpreted controller environment action may interpreted command output 
sight appears strange value updated agent state si action choice 
reason essential agent take account adapt environment state soon observed 
thing happens agent receives observation update internal state si pi si yi 
notation si conventional understood updated value value intuitively exist 
denote probability environment changing state xk result application joint action uk state xk time step xk uk xk xk uk xk 
definition allows stochastic environment evolution 
environment evolution deterministic transition probability distribution changes function 
theoretical framework notations description interaction agents environment algorithm 
deterministic functions keep notation simple 
assignments equal signs statements operational declarative 
system components clock evolve discrete time axis 
algorithm dmas evolution loop yi xk observe state si pi si yi evolve ui hi si yi take action xk xk uk uk 
un environment evolves loop presents schematic view dynamic multiagent system 
operator delays discrete signal time step 
yn un schematic view dmas 
evolution dmas world interpreted way 
environment agent states components global state xk xk 
sn agents choose actions decision making functions dictate ui hi si yi 
world changes state described global transition function xk xk uk cycle repeats 
observation function environment internal transition functions incorporated global transition function general interpretation provide explicit separation agents mind environment interested 
define concepts rational agent learning agent introduced framework 
definition rational agent dynamic agent continuously strives optimize performance measure expressed terms observations 
performance measure encodes represents goal agent 
performance measure expressed terms observations way agent access environment state 
goal agent times originally expressed terms environment state agent rely observations estimate state 
definition learning agent rational agent internal state dynamics attempt improve ability optimizing aforementioned performance measure 
meaning internal state dynamics agent context learning internal state si incorporates agent learned knowledge dynamics pi include learning processes agent agent index 
processes strive improve knowledge agent decision making abilities 
historical data stored agent internal state agent dynamics 
internal state structure prior knowledge stored 
things dynamics pi perform predictions help action selection 
result predictions saved internal state read agent policy 
models employed multiagent learning algorithms explicitly account internal state agents consider agent learning processes separate environment evolution processes give access certain learning data structures 
model integrates learning structures generic concept internal agent state allows agents environment flow evolution 
dynamic nature learning agent clearly stands 
agent dynamic necessarily learning 
definition learning multiagent system dmas agents learning agent 
allowing agent alter elements control internal state space transition function policy obtain adaptive agent 
definition adaptive agent tuple meaning definition 
internal state adaptation mapping agent 
mapping specifies internal state space agent adapts step result agent internal state observation sk sk sk yk 
sk adapting internal state space agent initial state space 
transition adaptation mapping agent 
mapping specifies transition function agent adapts step result agent internal state observation pk pk sk yk 
pk sk sk adapting transition function agent initial transition function 

theoretical framework policy adaptation mapping agent 
mapping specifies decision distribution probability agent adapts step result agent internal state hk hk sk yk 
hk sk adapting decision probability distribution agent initial distribution 
adaptive agent enhancement dynamic agent 
adaptive agent learning able time step dynamics improve knowledge alter structure knowledge way learning algorithm policy 
call agent adaptive learning agent 
definition adaptive learning multiagent system mas dmas agents adaptive learning agent 
algorithm describes interaction agents environment mas 
deterministic functions assume agents adaptive keep notation simple 
prior doing agent adapts learns presence incremented time indices adaptive elements 
explained incremented indices interpreted updated elements 
algorithm mas evolution loop yi xk observe state si si si si yi pi pi pi si yi adapt hi hi hi si yi si pi si yi learn ui hi si yi decide xk xk uk uk 
un environment evolves loop note require agents adaptive learning dmas adaptive learning agents 
allows greater flexibility 
forbid adaptation agents 
important reasons adaptation agent keeping pace timevarying environment fk happens dmas closed interacts outside world objects inserted removed spatial domain temperature drifts affect parameters process controlled agents 
distinguish special interesting cases adaptation 
occurs agent adapts parameters learning policy function second adapts state space 
definition parametric adaptation process adaptation occurring adaptive learning agent 
uses parametric representations learning policy functions pk hk parameter vectors initial values 
adapts parameter vectors functions sk yk sk yk 
definition structural adaptation process adaptation occurring adaptive learning agent state space adaptation function adapt internal state space transition policy adaptation mappings allow transition policy functions accommodate alterations structure internal state space 
definitions limit adaptation behaviour agent 
instance agent parametric adaptation may structural adaptation viceversa 
restrictive type structural adaptation accommodate alterations caused changes dimensionality constraints state space 
restrictive type parametric adaptation adaptation state space forbidden 
broader type adaptation parametric adaptation special case imagined learning policy functions directly adapted state space may may adapted 
important note adaptive learning seen static learning process augmented static agent state space parametric adaptation augmented state space incorporates parameter spaces augmented state variable sk sk structural adaptation augmented state space incorporates structural parameters adapting state space sk number dimensions component ranges discretization grain 
communication introduce special case dmas deal communication 
stating environment mean environment excluding communication channel 
definition communicative multiagent system interference dmas state space environment state space communication channel 

ii yi mi space environment observations mi message space agent yi yi yi ye yi yi mi 
variable called received message 
iii exist distributions xe rcv xc mi xe xc yi ye yi yi yi iff ye xe xe ye xc rcv xc 
distribution environment observation distribution rcv message distribution 
iv ui mi space domain level environment actions mi message space agent ui ui ui ue ui ui mi 
variable called sent message 

theoretical framework exist distribution si function si mi ui ue ui ui si hi si ui iff ue si si ue si 
distribution domain level environment policy function message sending policy 
vi defining au ue 
ue ami snd 
snd exist distributions 
mn iff 
distribution environment dynamics communication channel transmission dynamics 
assume explicit communication channel xc embedded world state read message distribution rcv time step produce message iii 
message distinct part agent observation usable agent internal processes pi ii 
similarly time step agent opportunity create send message usual action applied environment 
message distinct part agent output set messages sent received identical iv 
definition ensures interferences communication channel environment sending transmission evolutions environment communication channel independent vi environment observation distribution message distribution independent iii 
characterization communication interference 
removing requirements allow environment interfere communication analysis difficult distinction communicative general dmas blurred 
note allow message transmission noisy general case rcv stochastic 
complex definition intended clearly expose way communication emerges natural dmas framework taken 
advantage assumption interference separating agents dynamics parts handles messages si mi si handling observations pe si si distinguish independent communication process running dmas described algorithm 
algorithm assumes noise free communication deterministic functions channel dynamics 
algorithm communication interference free communicative dmas loop rcv xc read channel input message si si process input message si create send output message xc xc 
channel processes messages loop important observation communication instantaneous 
delay time step placing message communication channel agents 
model interactions pass environment 
processes algorithm algorithm take place simultaneously different clocks communication process faster environmental interaction process limiting case communication considered instantaneous 
agent described communication mechanism adaptive 
extension intuitively straightforward involves endowing agent possibility alter communication policy 
part observation space available set messages remain constant 
agent roles informally role restriction imposed space available domain level actions agent role 
explicit representation roles leads special case dmas 
extend definition plain dmas introduce conflicts communication roles dmas mechanisms 
definition inspired jung 

definition multiagent system roles tuple dmas set available roles 
action space agent form ui roles available agent agent policy agent decomposed parts space ordinary actions available role choice policy hr si yi indicating agent chooses roles internal state observation 
time varying ordinary action choice policy ho si yi ci ur ci role constraints function ur role chosen step hr si yi interpretation definition roles agents choose employ imposed 
rational agents choose roles benefits performance 
time step agent decides maintain current role change decides ordinary action set indicated role constraint function ci 
roles typically cooperative multiagent systems 
set roles characteristic multiagent system agent uses subset set 
interpretation roles constraints imposed action space particular agent 
typically agents information roles agents constraints imposed roles agents actions 
full control role choice policy role constraints adaptive agent principle adapt elements 
due fact agents may information elements may rely information making decisions adaptation role choice constraints area done cautiously 
credit assignment problem 
taxonomy credit assignment problem problem determining dmas activities responsible detected change performance agent 
problem aspects structural temporal credit assignment 
structural credit assignment problem problem agent determining components dmas responsible detected change performance 
structural credit assignment performed levels inter agent agent tries determine size contribution performance change relative contribution intra agent agent subsequently distributes partial contribution internal components responsible decision making sen weiss 
temporal credit assignment problem arises dmas environment dynamic fact times effects action prolong far 
robotic agent goes right junction passing straight corridor leads destination turns robot determine decision going right earlier junction enabled reach destination 
formally temporal credit assignment problem problem determining past actions agent responsible detected change performance 
taxonomy multiagent systems framework introduced section describe taxonomy dimensions multiagent systems 
select dimensions importance researchers agree bear relevance review 
taxonomy categories impose restrictions respect adaptivity agents restrictions explicitly mentioned 
agent related dimensions degree heterogeneity homogeneous dynamic multiagent system dmas si sj yi yj ui uj pi pj hi hj 
note require observation distributions initial states agents identical 
allows local agent perspectives different initial knowledge 
homogeneous agents identical gain different knowledge lifetime 
heterogeneous dynamic multiagent system dmas set identities satisfied pair homogeneous heterogeneous agents adaptive 
additional requirement homogeneous agents internal state transition policy adaptation mappings si sj pi pj hi hj compatibility agent goals common goal mas agents strive maximize common performance measure 
mas typically referred multiagent team 
spectrum goal agent completely opposite goals agents 
fully competitive setting 
intermediate situations exist goals agents completely opposite come conflict 
times common goal may lead agents partially conflicting secondary goals way solution example possession shared resource 
inter agent cooperation cooperative multiagent system agents willing help agents obtain goals 
self interested agents hand act achieving goals 
dimension strongly related goals compatibility 
multiagent teams cooperative fully competitive agents self interested 
grey area cooperation optional 
agent helping agents achieving goals goals may cost agent temporary losses performance may 
long run cooperation beneficial rational agent incentive cooperate 
simple situation agents share common goal temporary performance time step affected negatively cooperation called fully cooperative 
degree control decentralization actions agents dictated single central authority say control centralized 
case multiagent system degenerates single agent 
control schemes control engineering centralized see section 
opposite spectrum agents arbiters actions 
decentralized distributed control 
decentralized control mean agent coordinate actions agents means agent free choose coordinate 
sits hierarchical control 
hierarchical control scheme agents retain part autonomy 
agents higher levels hierarchy impose goals agents residing lower levels 
lower level agents bound try realizing goals 
higher level agents may judge quality actions subordinates offer feedback respect 
necessary single top level root agent may exist 
refer hierarchical decentralized control generic term distributed control 
reactivity versus deliberation reactive agent tuple policy elements meaning definition 
says reactive agent internal state takes actions basis observation 
agent deliberative literature typically requires uses internal dynamics search space available actions predict effects choose action basis predictions 
requirement strictly necessary attaining performance learning agent 
instance learning agents act reflex fashion basis function see section update 
taxonomy function basis feedback environment 
argue function encodes experience agent effect predictions results actions learning agent sense deliberative 
wee see clear definition deliberative agent difficult give 
consequently distinction reactive deliberative agents clear cut 
intermediate example deliberative agent uses reflexes perform certain simple tasks 
environment related dimensions degree communication communicative multiagent system defined section definition 
say dmas interference free communication exist sets xc mi distributions rcv functions sition definition possible 
dropping requirement interference explained section distinction communicative non communicative multiagent systems blurs 
case look interaction communication channel dynamics environment dynamics ways 
see interference environment affecting channel 
point view perform separation communication channel environment look communication process transmitting information indirectly affecting environment 
concept stigmergy 
example stigmergy multiple robots performing cleaning task robots passing area cleaned 
robot infer robot passed area earlier explicit communication place 
communicating non communicating agents adaptive 
special characteristics adaptation process communicating agent described section 
scope agent perspective agents dmas common perspective common observation distribution 
immediately implies equality agents observation spaces yi yj common observation space 
global perspective offers agents quantity information environment 
doesn mean environment state translated observations identically agents 
statement holds global perspective agents homogeneous 
case notions global common perspective coincide 
general common perspective special case global perspective 
quantities information offered agents differ say agents local perspective 
degree measurability dmas environment completely measurable simply measurable corresponding observation yi uniquely identify state yi 
satisfied environment partially measurable 
definitions inspired pynadath tambe 
researchers identified interesting level measurability observations agent uniquely identify environment state collective observation agents environment collectively completely measurable collectively measurable 
formally dmas environment collectively measurable corresponding observations 
yn uniquely identify state 
yn 
level measurability useful presence communication 
stress virtually mas literature identifies measurability observability speaks property calling observability 
entirely accurate shall see measurability special case broader notion observability 
treated separately carries weight literature 
degree observability dmas environment completely observable possible sequence environment states current state determined finite time observations 
formally dmas environment completely observable simply observable possible sequences states 
xk 
agent exists finite li xk uniquely determined sequence yi 
yi li xk yi 
yi li 
satisfied environment partially observable 
similarly observations agent uniquely identify environment state finite time collective observation agents environment collectively completely observable collectively observable 
definitions inspired control engineering notion observability 
observability dimension orthogonal measurability dimension generalization sequences observations li zero observability reduces measurability 
instance environment measurable necessity observable 
observable measurable 
similarly environment collectively measurable collectively observable converse necessarily true 
episodic vs continuing tasks task mas episodic interaction agents environment breaks naturally subsequences terminating set terminal states final time step 
subsequences episode trial episode termination mas reset initial state state drawn distribution initial states 
interaction agents environment break naturally identifiable episodes goes indefinitely mas task continuing 
definitions borrowed sutton barto 
fact identified fourth level 
system receives feedback environment control terminology control open loop environment measurable 
level little setting refer 

examples example episodic task navigating maze agents exit maze task ends 
times goal mas episodic task reach subset desirable terminal states minimum time 
example continuing task process control 
taxonomy dimensions interested 
homogeneous heterogeneous agents 
agent teams 
cooperative necessarily fully cooperative 
distributed agent control 
reactive deliberative agents 
levels inter agent communication 
global local perspectives especially 
complete partial measurability especially 
complete partial observability 
episodic continuing tasks 
examples section frameworks control engineering multiagent systems literature analyzing fit dmas mas models introduced section 
addition illustrating generality framework analysis provide helpful insight characteristics 
conventional control scheme simplest feedback control scheme control engineering linear time invariant single input single output plant controlled linear time invariant controller 
evolution plant described discrete state space model xk yk np discrete transfer matrix discrete input matrix cp np discrete output matrix np size plant state space 
signal plant state command input plant output 
controller sk cyk uk ref nc nc nc cc nc similar meanings time controller dc discrete direct feedthrough matrix controller state ref signal 
basic control goal plant output follow signal closely possible 
ref controller dc cc cs cy plant px pu conventional control scheme 
seen immediately scheme bears resemblance dmas representation 
notable difference reduction single agent controller 
fact control scheme equivalent single agent deterministic dmas 
clearly notations indicate corresponds agent observation state agent action environment state 
correspondence linear relations functional elements dmas xk uk xk sk yk cyk sk ref cp interesting difference observed dmas explicit element similar signal control goal 
opens way interpretations goal internal agent 
agent knows wants achieve pursues goal arbiter actions 
goal component agent observation input 
imposed external authority human agent level higher hierarchical control scheme 
scheme isolates plant 
dmas plant corresponds environment places agents environment noisy transmission channels command feedback signals 
modeled part environment 
allow controller time varying sk ksk kyk uk cc ksk dc ky ref identify adaptive agent pk sk yk ksk kyk hk sk cc ksk dc ky ref model adaptive control 
examples model adaptive system adaptive control scheme expresses control goal terms model 
signal passed model generate desired response plant 
control law parameterized parameters adapted adjustment mechanism way difference actual desired output plant minimized 
ref model controller parameters adjustment mechanism controller plant model adaptive control scheme 
assume general form plant parameterized controller xk fp xk uk yk hp xk sk fc sk ref yk uk hc sk ref ref command input 
signal generates desired plant output model 
similarly done section establish correspondence single agent deterministic mas xk uk fp xk uk xk hp xk sk yk fc sk ref yk sk hc sk ref controller adaptive agent parametric adaptation 
goal representation ref framework internal agent 
adjustment mechanism adapts controller parameters uk ym yk uk ym yk corresponds adaptation mappings definition sk yk uk ym yk sk yk uk ym yk model plant dmas interpretation part agent 
state vector interpreted part extended state signal similarly dynamics seen part extended agent dynamics 
output function model integrated adaptation mappings compute communicative multiagent team model example multiagent teamwork model developed pynadath tambe purpose analyzing tradeoff optimality complexity teamwork models theories 
choose illustrate model reasons general representative multiagent literature second sources inspiration defining communicative dmas model 
model subsumes markov decision processes team markov games deterministic rewards easily extensible handle stochastic rewards conflicting goals situations 
frameworks shall see important multiagent learning 
name model communicative markov team decision problem com mtdp 
team agents com mtdp tuple ua ma ya ba set world states 
ua aui set combined actions ui set actions agent ma ami set combined messages mi set messages agent state transition probability distribution xk xk uk 
ya set observations ui set observations agent joint observation distribution xk uk 
observation distribution agent yi yi yi xk uk 
ba abi set combined belief states bi set possible belief states agent common reward function team representing joint preference states cost actions communication ma ua reward function sum rewards ua representing utility domain actions ii ma representing cost communication 
agent decides actions take messages send basis belief state bi bi 
domain actions dictated domain level policy bi ui 
similarly sent messages dictated communication policy bi mi 
belief state updated stages time step agent observes world pre communication second time receives joint message sent team post communication 
updates performed called state estimators pre communication pre 
examples bi yi bi post communication post bi ma bi respectively 
step agent executes action sends message indicated domain level communication policies respectively world evolves consequence 
proceed show com mtdp described communicative dmas definition enumerate differences frameworks 
correspondences com mtdp communicative dmas elements summarized table 
com mtdp communicative dmas world state space environment state space xe domain level action space ui domain level action space set messages mi transition distribution set messages mi environment transition distribution set observations yi set environment observations observation distributions environment observation distributions belief state space bi included agent internal state space si pre communication estimators pre included observations agent dynamics pe messages handling agent dynamics reward function implicit env 
observation distributions domain level policy domain level policy communication policy hc message sending policy channel state space xc channel dynamics message distributions rcv post communication estimators post table correspondences com mtdp communicative dmas elements correspondences immediate 
differences exist dmas model assume authority providing reward signal exists environment agent maintain kind goal representation internal state si judge quality actions basis effect environment monitored observations explicit reward function assumed part agent observation function view com mtdp agent internal state represent beliefs world state consequently agent dynamics update beliefs 
dmas view broader agent internal state includes information relevant decision making process agent possibly including priori knowledge learned knowledge prediction results 
consequently agent dynamics include things learning prediction processes 
com mtdp explicit representation communication channel dynamics message process 
describe transmission noise 
contrast com mtdp communicative dmas communication added top preexisting structures arises naturally basic dmas framework 
communicative dmas general model com mtdp offering additional expressiveness form agent internal dynamics explicit communication dynamics 
chapter multiagent reinforcement learning reinforcement learning rl problem faced agent learn behaviour trial error interactions dynamic environment kaelbling sutton barto 
rl field born junction research threads various disciplines important trial error learning animal psychology dynamic programming approach optimal control control engineering sutton barto 
reinforcement learning assumes interaction model agent environment terms world environment interchangeably rl context discrete time step agent observes current state environment chooses action 
result environment transitions new state agent receives scalar reward signal see 
signal measure quality agent actions determined environment placement evaluation process environment characteristic rl 
action agent environment state reward reinforcement learning model 
reward consider instance mouse agent maze needs find exit 
actions mouse movements state consists position changes result mouse movements 
mouse receives zero reward inside maze positive reward cheese finds exit 
added incentive find exit quickly provided mouse giving small negative reward penalty rl terminology zero time step spends inside maze 
state multiagent reinforcement learning simple example problem illustrates fundamental characteristics reinforcement learning trial error search delayed reinforcement sutton barto 
mouse receive indication movement best junction maze case supervised learning 
informed relative quality movement reward signal needs find moves best trial error search 
assume mouse crucial decision maze junction close start point 
decision enabled agent reach maze exit certain time positive reward obtained significant part consequence decision agent interval elapsed 
agent infer decisions trajectory contributed obtaining final reward weight delayed reinforcement 
clear learning techniques model attractive regardless agents involved 
agents reinforcement learners task designer reduces approximation specifying reinforcement signal accurately represents goals set multiagent system easy task 
course agent involved environment longer static reactive machine represented 
due strong appeal rl research multiagent learning focuses field 
chapter constitutes main body part review dealing learning multiagent systems 
survey multiagent learning methods chapter 
chapter organized way rl problem formally introduced single agent case solution concepts techniques 
proceed formal representations solution concepts multiagent reinforcement learning multiagent reinforcement learning ma rl problem explored literature selection methods researchers solve problem conclude remarks research opportunities 
single agent reinforcement learning multiagent reinforcement learning algorithms build single agent methods 
thoroughly introduce theory techniques employed methods 
formal model independent learning agent case reinforcement learning task typically formalized markov decision process 
definition finite markov decision process mdp tuple discrete finite state space discrete finite action space reward probability distribution state transition probability distribution 
general type mdp possibly infinite continuous state action spaces 
exist methods handling types problems theoretical guarantees valid approaches dealing finite case typically carry methods 
remainder presentation consider finite mdps 

single agent case behaviour rl agent mdp described possibly stochastic policy mapping states actions 
policy changed time reinforcement learning algorithm 
formal model rl iteration described follows agent observes current state xk environment chooses action uk 
result environment transitions new state xk probability xk uk xk agent receives reward rk probability xk uk xk rk 
mdp fits dmas model definition reduction agent denoted agent assumption full measurability 
mdp represents learning tasks dmas elements internal agent missing 
furthermore model assume authority providing reward signal exists environment rl general case agent maintain kind goal representation internal state judge quality actions basis effect environment 
explicit reward function assumed part agent observation function 
environment fully measurable observation agent form xk rk correspondences summarized table 
mdp dmas state space environment state space action space action space transition distribution transition distribution reward distribution implicit observation distribution policy policy table correspondences mdp dmas elements rl policy time varying described influence learning algorithm 
model policy time varying function time varying internal state changed learning processes 
discussion rl explicitly address internal agent state chapter simply stating state mean environment state 
discussing internal agent state declare explicitly 
learning goal rl agent rational agent uses optimality measure expressed terms rewards 
common ways expressing optimality measure kaelbling finite horizon expected return rk rk measure episodic tasks 
multiagent reinforcement learning ii infinite horizon expected discounted return rk 
rk measure suitable episodic continuing tasks 
variable discount factor may interpreted ways probability agent survives step measure far sighted agent considering rewards mathematical trick bound infinite sum 
iii infinite horizon expected average return rk lim rk 
measure appropriate episodic continuing tasks seen limiting case discounted expected return discount factor approaches kaelbling 
expected discounted return review popular handle continuing episodic tasks including episodic tasks repeated agent life 
markov property important assumption theoretical results reinforcement learning rely markov property 
property refers state signal satisfied signal contains step information relevant agent decision making process 
state signal course compact possible 
formally markov property expressed identity sutton barto xk rk xk uk xk uk 
xk rk xk uk 
sufficient know state action determine state reward 
call task satisfies property markov task 
value functions bellman equations rl algorithms rely estimating agent state take action state 
estimates embodied state value function action value function respectively sutton barto 
definition value state policy expected return starting eh rk xk 

single agent case expected discounted return eh rk xk 
definition value action state policy expected return starting eh rk xk uk 
expected discounted return eh xk uk 
rk action values called values 
call function table results chapter rely complete tabular representation function indexed states actions 
fundamental property value functions satisfy recursive equations policy assuming deterministic rewards prevent notation cluttered definition optimality mdps 
optimal state value function function associating state maximal attainable value state max 
ii optimal action value function function associating state action pair maximal attainable value action state max qh 
iii optimal policy policy attaining optimal state value function optimal action value function 
mdp deterministic optimal policy arg max 
multiagent reinforcement learning may fact optimal policies attain state action value functions optimal ones 
clearly optimal value functions satisfy recursive relations 
obtained expressions bellman optimality equations max max equations cornerstone rl solution techniques 
exploration issue 
equation suggests agent choose step action action value function yields maximum value 
called greedy action selection agent chooses greedy action say exploiting 
greedy action course best idea agent knew optimal value function 
learning agent estimate value function 
action underestimated better action highest value current value estimate 
step learning agent weigh direct benefits choosing greedy action possible benefit choosing action find fact better 
course action called exploration agent follows say exploring 
maze example assume mouse agent chosen junction go left exit close right path reachable left path 
mouse reaches chances estimated value left path greater right path 
time mouse junction estimates prefer going left greedy action 
mouse chooses explore goes right find underestimated path gets goal faster 
reinforcement delayed rely heuristics approximations balancing exploration exploitation 
thrun classified exploration strategies categories directed undirected 
exploration directed takes account measure expected gain information obtained exploratory moves attempts maximize measure 
measure exploration undirected 
undirected exploration efficiency exploration tremendous consequences learning performance rl researchers simple stochastic strategies 
common detailed discussion see singh greedy exploration 
strategy chooses step greedy action probability random action probability 
exploration probability typically initialized relatively high value decays learning progresses 

single agent case ii softmax boltzmann exploration 
basic mechanism greedy exploration exploring strategy select random action 
ranks actions values chooses stochastically 
specifically probability selecting action exploring state eq 
temperature parameter controls randomness action selection 
high values actual value actions little influence action selected 
low values softmax strategy approaches greedy action selection 
option optimism face uncertainty action value function initialized overestimated values disappointment experienced tried actions drives agent explore new ones sutton barto 
initial values sufficiently high example special case initial values exactly computable see sen time agent explore entire state action space 
desirable better initialize action value function moderate values agent doesn waste time exploring 
directed exploration thrun separates directed exploration strategies classes frequency exploration remembers states state action pairs visited 
illustrative variant counts number times state visited chooses state action move agent adjacent state thrun 
goal improve agent knowledge states presumably information 
ii recency exploration relies elapsed time action tried 
exploration biased actions weren tried long time order improve knowledge agent effects 
iii error exploration uses second order information estimate uncertainty estimated action values 
exploration biased actions higher potential 
method kaelbling interval estimation heuristic kaelbling 
method computes upper bound confidence interval instance interval actions chooses action highest upper bound 
de jong attempted combine advantages recency error methods exploration buckets method 
method actions associated exploration bucket 
step quantity related error reward predicted action time chosen added bucket 
buckets chosen actions emptied 
error character method obvious recency character fact biases accumulate exploration buckets 
multiagent reinforcement learning solution techniques model techniques model methods assume model environment reward transition functions available compute optimal course action model 
easy indirectly computing optimal value function greedy action selection 
method called value iteration algorithm 
algorithm value iteration require transition model reward model input threshold discount factor output optimal policy repeat max arg breaking ties randomly algorithm turns bellman equation assignment 
convergence optimal value function proven certain conditions 
policy iteration operates directly policy agent algorithm 
works iteratively executing interdependent processes policy evaluation value function current policy computed policy improvement value function compute new policy 
shown policy evaluation converges true value policy policy improvement yields better policy policy optimal stopping condition algorithm 
equivalent value policy iteration algorithms state value function exist see sutton barto 
practical solution methods rely action values focus action value function techniques directly derived dynamic programming optimal control bear name 
model free techniques cases difficult impossible obtain accurate models world required model techniques 
state space problems large processing states practical 
problems solved model free methods 
monte carlo class model free methods evaluates policy executing large numbers episodes policy averaging obtained returns sutton barto 
returns obtained direct experience environment model longer needed 
computation focused interesting areas algorithm policy iteration require transition model reward model input threshold discount factor output optimal policy 
single agent case random action repeat repeat policy evaluation max true policy improvement arg breaking ties randomly false state space useful policies evolve 
instance mouse example starts near exit large maze interested computing accurate values states far back starting position 
advantages monte carlo methods obvious disadvantage require large numbers episodes may costly obtain 
continuing tasks defined 
temporal difference methods able perform useful updates step taken world 
combine ideas dynamic programming monte carlo methods 
actual reward obtained experience world monte carlo methods temporal difference performs updates reward step long experience sequences 
update rule moves current value estimate target formed combining observed reward current estimated value state value iteration actual experience model obtain state reward xk uk xk uk rk xk uk xk uk 
agent chose uk state xk time step result world changed state xk rewarding agent rk 
action uk chosen agent time step 
interesting things worth noting update rule estimate xk uk moved way target rk xk uk value iteration 
moved fraction distance 
multiagent reinforcement learning fraction called step size learning rate 
typically varies learning process 
estimate xk uk updated estimate xk uk 
technique called bootstrapping 
agent uses action uk determine value state 
estimates converge value policy 
update rule said reason policy 
policy algorithm resulting combination policy derived current action value estimates called sarsa structure experience instance update rule state action reward state action 
certain conditions shown sarsa converges optimal policy 
conditions notably include exploratory policy decaying exploration limit policy greedy learning rate series sums infinity squares sum finite value 
disadvantage sarsa agent know actual optimal action values policy converged optimal policy convergence conditions require policy exploratory non optimal 
turns temporal difference methods estimate optimal value function sub optimal policy methods characterized policy 
modification required update rule straightforward xk uk xk uk rk max xk xk uk 
algorithm resulting combination policy derived current action value estimates called learning algorithm watkins dayan 
algorithm learning input learning rate discount factor observe initial state loop policy derived greedy apply observe loop shown learning converges optimal policy relaxed conditions sarsa watkins dayan condition imposed policy state action pairs continue updated 
initialization values arbitrary finite initial values satisfy convergence properties 
immediate advantage optimistic initial values section 
multiagent context refer original variant learning basic plain learning 

single agent case learning derivations widely reinforcement learning algorithms 
analyze fit dmas framework 
table accompanying exposition explained basic mdp components fit dmas 
need investigate deeper place elements specific learning table update rule 
dmas interpretation table part agent knowledge internal state 
table corresponds part agent internal state vector agent omit agent index 
denote flat vector rm 
denote value step qk 
order bring forth agent dynamics temporal difference update rules introduce selector function ji 
position element corresponding multiplication produces scalar equal value pair 
multiplied scalar produces vector positions corresponding pair scalar placed 
rewrite sarsa update rule qk qk xk uk rk xk uk qk xk uk qk qk xk uk xk uk qk xk uk xk uk qk xk uk rk xk uk xk uk xk uk xk uk qk xk uk rk identity matrix 
learning dynamics linear agent needs remember previous state environment xk previous action uk 
storing internal state 
state agent composed vector previous environment state chosen action sk qk xk uk index shift appears notation convention dmas model knowledge qk basis agent decides time step updated observation yk denoted index see section detailed discussion aspect 
linear agent internal state nonlinearity appears elements serve purpose short term memory 
remembering observation agent form yk xk rk written considering explained index shift qk sk yk qk sk yk 
equation describes learning dynamics agent form part agent internal dynamics sk sk yk 
interpretation learning dynamics similar learning dynamics nonlinear involve messy notation 
give just rewritten version learning update rule multiagent reinforcement learning qk qk xk uk rk max xk qk xk uk qk 
learning update rule propagates information step back trajectory agent state space 
maze example means mouse reaches cheese time information large obtained reward propagated visited state nearest cheese say denoted mouse put back position maze idea way cheese reaches state near moves point information propagated back step 
clearly quite inefficient 
issue instantiation temporal credit assignment problem 
efficient way propagating value information 
agent marks trajectory called eligibility trace passes updates step value entire set values path proportionally eligibility values 
trace decays exponentially factor called recency factor 
resulting algorithm called algorithm 
algorithm accumulating trace input learning rate discount factor recency factor observe initial state loop arg modify exploratory move indicated exploration strategy arg reset trace apply observe compute temporal difference mark trace decay trace loop accumulating refers fact trace reset incremented line 
line resets trace exploratory action taken step causality agent path interrupted 
credit assigned actions preceding exploratory move influence cut moment 
version avoids issue peng williams 
model free method related complex heuristic nature learning classifier system 
introduce surveyed chapter 

single agent case learning classifier system parallel message passing rule systems designed permit nontrivial modifications knowledge performs task booker 
population classifier rules represents knowledge classifier system 
classifiers consists condition part message part 
condition part basic variant learning classifier system bit string bit take values set meaning don care 
behaviour classifier system subsystems performance credit assignment rule discovery 
messages list circulates information system 
iteration environmental state processed input interface produce set binary messages placed messages list 
messages list matched condition input strings classifier population 
performance module probabilistically chooses set classifiers activation matching degree messages strength 
strength measures value classifier system 
classifiers chosen activation generate corresponding messages placed new messages list replaces old 
messages list processed output interface produce actions learning classifier system 
messages may remain list processing fed back classifiers directly passing environment endowing system memory 
credit assignment system bucket brigade version works way classifier bids fraction strength purpose activation 
activated pays bid classifiers active previous time step receives bids classifiers activated time step 
direct reward environment distributed classifiers activated prior 
rule discovery system responsible generating new classifiers enhance performance system typically genetic algorithms 
credit assignment system similar temporal difference update rule 
classifier strengths similar action values classifiers encoding relations states processed input interface obtain messages actions obtained messages output interface 
bids play role value increments rewards updates received environment 
example clarify 
simple classifier system introduced dorigo bersini messages identified overt actions classifier exists state action combination exactly classifier active time step bucket brigade strength redistribution reduces ck mk ck mk rk ck mk ck mk denote respectively condition message parts time indices point corresponding active classifiers 
relation clearly bears strong resemblance 
fact dorigo bersini argued certain modifications simple classifier system equivalent learning 
general version learning classifier system important advantages temporal difference learning 
presence don care symbols offers generalization abilities 
system natively incorporates memory direct message passing iteration memory helps situations learning task satisfy markov property 
learning classifier systems natively perform structural adaptation rule discovery mechanism 
due complexity mathematical formulation multiagent reinforcement learning learning classifier systems provide theoretical convergence optimality guarantees 
mixing model model free methods sutton introduced dyna architecture dynamic programming 
architecture uses experience efficient way model free methods 
conventional learning rule perform model free updates dyna constructs model environment experience uses perform model updates interactions real world 
dyna combines learning value iteration ideas resulting experiences real world necessary reach behaviour 
computational cost dyna greater incurred model free methods 
computation cheap interaction real world expensive 
model updates dyna performed randomly chosen state action pairs 
improvement obtained state action pairs queued decreasing order impact update may predecessors 
iteration state action pairs popped top queue updated 
sweeps performed value iteration state space algorithm called prioritized sweeping moore atkeson 
direct policy search important class methods solving rl problems direct search policy space 
methods dynamic programming machinery introduced relying finding appropriate policy directly 
typically done gradient ascent genetic algorithms 
main advantage course greater generality 
disadvantage policy search takes time classical methods 
gradient methods introduced converge situations dynamic programming shown diverge due violation convergence assumptions baird baird moore 
hand gradient methods shown converge local optima necessarily give policies 
locality problem alleviated global optimization techniques genetic algorithms see section brief description complexity method theoretical results difficult 
genetic algorithms computationally intensive gradient methods 
multiagent case rl field mature understood theoretical results proven practical applications 
due relaxed assumptions learning task rl attractive solution multiagent learning problem 
extension single agent rl multiagent reinforcement learning ma rl trivial 
main difficulty comes fact viewpoint agent mas environment longer markovian 
agents part environment agent dynamic system changes behaviour learns explained section exemplified sarsa learning 
multiagent case section 
sufficient know state environment reason principle evolve 
stated differently difficulty results agent action depend state environment action taken actions agents performed time 
violation markov assumption destroys theoretical convergence guarantees single agent rl 
order achieve results agent needs reason explicitly behaviour agents learns action choices mechanisms led choices 
unfortunately easy thing take rest chapter difficulties arise explored solutions 
markov assumption violation problem ma rl 
important problem exponential explosion state space number agents 
renders simple tabular representations value functions completely impractical simplest problems 
credit assignment problem difficult mas single agent 
follows interdependence agents actions 
benefits agents learning mas 
come mainly knowledge sharing 
instance agents learn perform similar tasks share experience speed learning tan 
new agent arrives older skilled agents may serve teachers clouse 
teaching desirable newcomer learn watching imitating skilled agents perform tasks price boutilier 
behaviour multiple rational agents interaction extensively studied game theory 
consequence research ma rl game theoretic notions 
fact influence game theory extends ma rl general multiagent learning coordination methods 
section formal model ma rl task detailing stateless version bears relevance game theoretic techniques 
review controversy literature regarding ma rl learning goal 
sections review solutions ma rl task explored literature 
start direct application single agent rl continue special cases multiagent setting fully cooperative followed stateless mas techniques dealing general ma rl case 
formal model notions introduced denoted names literature 
case give alternatives parentheses chosen notion name 
concept markov game stands basis ma rl task models 
prior defining markov game define stateless version strategic game 
definition strategic game matrix game tuple ui set agents number ui discrete sets actions available agent yielding joint action set aui reward functions agents 
agents take actions indicated strategies ui receive rewards basis joint action 
un ri 
strategies stochastic called randomized deterministic called pure 
denote joint strategy multiagent reinforcement learning agents 
denotes space probability distributions set argument 
reward function written dimensional matrix discrete actions indices reward values content alias matrix game 
refer rewards strategic games game theoretic term payoff 
problems multiagent learning coordination modeled repeated strategic games 
strategic games played repeatedly agents 
agents incrementally learn solve strategic game 
definition markov game stochastic game tuple ui set agents number 
discrete set states 
ui discrete sets actions available agent yielding joint action set aui 
state transition probability distribution 
reward probability distributions agents 
time step agent observes state xk takes action ui indicated policy hi ui 
result joint agent action uk 
un world changes state xk probability xk xk uk xk uk xk agent receives reward ri probability ri xk uk xk xk uk xk ri 
similarly policies stochastic deterministic called pure 
stationary policy change action selection probabilities time 
denote joint policy agents 
hn 
markov game extension strategic game multiple states stochastic rewards 
state markov game different strategic game stochastic rewards played agents fact multiagent reinforcement learning algorithms deal stochastic games separately solving strategic games arise state stochastic game 
similarly policy extension strategy multiple states conversely strategy reduction policy single state 
markov game extension mdp conversely mdp markov game 
markov game special case dmas definition 
similar argument section holds dmas environment fully measurable rewards included agents observations 
correspondences markov game dmas elements summarized table 
game theoretic term play denote evolution agent markov game 
agents homogeneous section learning algorithm call learning process place markov game self play 
homogeneity equivalent identical learning algorithms agents common perspective due complete measurability assumed markov game 
typically defined literature context repeated strategic games definition includes special cases 

multiagent case markov game dmas state space environment state space action space ui action space ui transition distribution transition distribution reward distribution implicit observation distribution policy hi policy hi table correspondences markov game dmas elements particularly interested special cases markov games agents common goal reward functions agents act 
deterministic reward functions literature extension stochastic rewards trivial 
definition multiagent markov decision process mmdp fully cooperative game collaborative multiagent mdp team markov game markov game ui agents share reward function 
definition fully competitive game zero sum game players markov game 
name multiagent markov decision process justified follows agents considered centralized decision maker control centralized see section mmdp reduces mdp action space joint action space mmdp 
significant part multiagent learning coordination focuses type markov game 
kok 
slightly different version multiagent markov decision process definition multiagent markov decision process stochastic game ui global reward function agents version mmdp attempt maximize global return step definition equivalent definition 
fully competitive game called zero sum rewards agents sum 
discussing fully competitive games agent games general perspective agent refer agent game theoretic term opponent agent perspective assume player 
solution concepts currently exist consensus literature learning goal ma rl agents 
summarize controversy developed issue section 
simply building blocks defining various learning goals proposed researchers reviewed chapter 
building blocks game theoretic equilibria 
basic concept standing basis ma rl solution concepts similarly rl value function 
state action value functions defined similarly multiagent reinforcement learning single agent case account policies action value function actions agents 
give definitions directly terms expected discounted return 
definition value state agent joint policy expected return agent environment starts agents follow xk 
eh ri definition value joint action state agent joint policy expected return environment starts agents take follow xk uk 
eh ri noted section multiagent learning algorithms solving strategic games state markov game 
give formalization terms stateless games 
state values joint policies expected returns simply values joint strategies expected rewards concept state loses meaning ri 
extension multi state case formally straightforward simply solve joint action state values strategy values joint strategy agents known strategy profile 
denote joint strategy agents agent reduced strategy profile 


say best response agent reduced strategy profile ui 
widely game theoretic solution concept multiagent learning nash equilibrium 
definition nash equilibrium joint strategy best response fully competitive games nash equilibrium takes special meaning strategy evaluated respect opponent strategy combination yields value 
agent behave maximizes payoff worst case arg max min 
principle called minimax littman 
unknown nash equilibria computed polynomial time 
strategies forming nash equilibrium uncorrelated sense probability distributions mutually independent 
removing independence requirement general class equilibria called correlated equilibria obtained 
set correlated equilibria 
multiagent case convex contains set nash equilibria 
advantage convex set correlated equilibria computable polynomial time linear programming 
general definition markov games symmetric agents favoured 
allow agents leaders possess information agents followers act new notion equilibrium arises 
restrict definition agents simplicity ba sar 
definition pair strategies equilibrium leader follower equilibrium unique follower responses players strategic game exists unique mapping satisfying 
agent leader agent follower mapping describes follower reacts actions leader 
condition ensures rational follower agent obey mapping equilibrium condition 
type solution concept regret 
regret measures difference maximum total reward achieved fixed deterministic policy actual reward obtained agent ri max hi xl hi xl ri assumed deterministic rewards keep notation simple 
agent performance associated low negative regret 
learning goal ma rl algorithms set goal learning agent convergence game theoretic equilibrium nash equilibrium littman hu wellman greenwald hall 
explanation nash equilibrium natural long term consequence best response play rational agents 
shoham 
put forth tough criticism focus game theoretic equilibria general nash equilibrium special 
argued nash equilibrium presents important problems general strategic game multiple nash equilibria 
leads awkward convergence guarantees requiring agents coordinate selection equilibria external mechanism 
meaning desirability nash equilibrium defined terms stateless games doubtful full markov game setting delayed reward important role 
multiagent reinforcement learning important note objections extend equilibrium notions 
problem agents consistently selecting equilibrium multiple equilibria exist state markov game recurring theme ma rl known equilibrium selection problem 
shoham 
interpreted unjustified focus nash equilibria symptom missing clearly defined problem statement ma rl field 
attempt problem definition done bowling veloso definition rationality convergence criteria 
learning algorithm rational agents converge stationary policies learning agent converges policy best response stationary policies 
learning algorithm convergent agents learning algorithms set learning agent necessarily converge stationary policy 
authors interpretation desirable learning algorithms rational convergent 
powers shoham argued criteria reasons require learner agents converge stationary policies non stationary policies interesting designer multiagent system 
rationality convergence required hold limit offering guarantees reasonable performance finite time 
authors argued properties defined terms policy agent actual performance measure reward address effect going directly cause 
authors defined new criteria learning goal ma rl algorithms 
terms repeated games learning agent arbitrarily high probability finite time achieve targeted optimality learning algorithms agents set average reward arbitrarily close best response value 
compatibility self play average reward arbitrarily close value best nash equilibrium 
safety agents learning algorithms average reward arbitrarily close minimax value worst case value 
criteria completely drop convergence requirements learning goal 
shortcoming impose requirements average reward 
means moment time performance agent may arbitrarily poor 
order avoid shortcoming bowling introduced requirement regret agent converge stationary policy regret negative zero 
convergence deemed desirable characteristic learning algorithm leads stability markov game evolution stability allows accuracy technical statement condition slightly complicated see powers shoham exact form 

single agent techniques ma rl estimating value functions 
delayed reward involved practical point view ability properly estimating value functions critical rl agent 
converging achieving rewards mutually exclusive goals 
tradeoff properties may exist 
order obtain high rewards agent needs predict value function accurately stability required agent learning process convergence sacrificing reward 
case debate issue multiagent reinforcement learning goal open definitive answers 
application single agent techniques multiagent reinforcement learning easiest way dealing consequences presence agents rl task course disregard 
results reported approach certain problems ranging simple simulations real complex tasks 
sen 
lies spectrum 
study agents learning learned complementary policies pushing block dimensional surface 
authors specific type problem goal state horizontal coordinate start state 
able quantify position information vertical stripes provide instantaneous reinforcement agents basis horizontal distance block goal position 
particularity simplified rl problem ways number dimensions state space halved second problem delayed reward eliminated 
specificity learning problem allow conclude approach extend general settings 
issue representative single agent learning simple multiagent simulations 
large part body complex settings comes field multirobot systems 
matari results multirobot foraging task robots learned collect pucks scattered world bring designated home region 
state action space agents highly abstracted needed learn mapping small number high level conditions small number high level behaviours homing 
reinforcement signal composed complex way separate goals instantaneous parts called progress estimators intruder avoidance estimator rewarded agent maintaining appropriate distance teammates 
representative simulated robotic soccer application fuzzy learning classifier system 
state information abstracted fuzzy linguistic variables 
example agent knows zero teammates vicinity closest close right farthest far left zero close far left right linguistic values 
information repeated agents opponent team 
similar fuzzy values defined ball internal agent state containing instance stamina 
view dmas model part internal agent state contains learned knowledge 
complex tools heterogeneous reinforcement progress estimators high level behaviours 
crites barto applied reinforcement learning task elevator scheduling 
multiagent reinforcement learning model task discrete event system continuous time 
reinforcement signal global yielding common goal task different rl controllers allocated elevator system dmas 
complexities arise partial observability environment parts state hidden destinations passengers waiting floor 
researchers returns defined continuous time integrals neural networks represent value function 
results obtained simulations outperformed commercial scheduling algorithms 
characteristic realistic multiagent applications empirical heuristic machinery order complex considered problems learnable 
machinery difficult analysis techniques usefulness settings 
fully cooperative multiagent teams theoretical model describes fully cooperative setting multiagent markov decision process definition 
mmdp optimal solution principle treating multiagent system single agent noted section estimating optimal joint action values learning 
agents table form 
un learned 
approach yields optimal policy simple greedy action choice 
problem course multiagent system agents certain degree autonomy choosing actions mas decentralized complete autonomy 
solution duplicate value function learning algorithm agent 
possible long actions measurable agents 
algorithm relying solution called team friend literature littman 
difficulty coming distributed nature decision making process 
world state joint actions yielding best value 
classical greedy action choice break ties randomly 
agent breaks tie randomly different agents may break tie different ways 
lead agents select different joint actions execute part actual resulting joint action suboptimal 
special case equilibrium selection problem 
fact littman offered team convergence guarantees value function functions maintained agents converge optimal guarantee complemented convergence policies optimal joint actions achieving termed coordination equilibria unique state 
summary team requires assumptions markov game fully cooperative mmdp 
ii actions measurable agents 
iii optimal joint actions unique state world lauer riedmiller gave algorithm call distributed learning removes assumption ii 
agent maintains table indexed action qi ui uses modified temporal difference update rule qi xk ui max qi xk ui rk max ui qi xk 

general multiagent systems update rule moves estimate qi xk ui way new estimate rk ui qi xk contrast moves fraction way update leads increase value estimate 
authors proved update rule assumptions reward function positive values initialized values agents learn maxima joint values qi ui max un uj uj ui ui 
requiring reward function positive strange restrict generality algorithm 
rl agents act basis relative differences value estimates basis absolute values estimates agent compares values actions takes action highest value 
agents maintain explicit estimates hi optimal policy 
modifying greedy policy modifications allowed lead improvements values hi xk max qi xk ui changed hi xk ui ui ui authors showed joint policy 
hn attains greedy value respect joint table 
ui action taken agent time step distributed learning policy agents need follow exploratory policies action need taken hi learning take place 
greediness joint policy respect joint table authors show convergence distributed learning follows basic learning 
general multiagent systems full cooperation assumption cases restrictive 
true especially agents cooperating teams agents may encounter situations immediate interests agents conflict 
example shared resource high level goals agents need point resource expensive multiplied agent compete obtain resource 
theoretical model describing type tasks markov game definition 
algorithms handling cases typically assumptions actions measurable agents 
ii rewards measurable agents 
algorithms additional assumptions 
state assumptions explicitly case 
multiagent reinforcement learning times agents learn value functions basis joint action stochastic policies 
motivated existence markov games targeted solutions expressed deterministic policies 
approaches dealing general multiagent systems come field game theory 
stateless problems exist ma rl approaches stemming game theory handle repeated strategic games definition 
possess concept state lose main characteristics rl delayed reward 
approaches relevant discussion ma rl infinitesimal gradient ascent forms basis general method review section 
algorithms section strengthen assumption ii iii reward functions agents common knowledge formally common knowledge means agents know fact agents know agents know fact indefinitely 
add extra assumption iv agents play repeated strategic game world states 
approaches designed games agents 
littman stone focused agent repeated games introduced aggressive leader strategies attempt induce follower behaviour opponent 
strategies agent tries create asymmetric situation originally symmetric game see section 
authors motivation best responses usually considered game theoretic approaches multiagent learning essentially follower strategies important benefits achieved employing aggressive leader strategies implicitly induce cooperation opponent 
strategies called bully godfather rely respectively threats 
look game perspective agent 
bully deterministic strategy choosing action arg max arg max 
bully assumes opponent play best response strategy plays maximize reward assumption 
godfather strategy chooses joint action yields agents minimax security level payoffs 
executes part joint action 
opponent doesn execute corresponding component godfather takes action reduce payoff opponent security level basically telling play half matter obtain security level 
formally joint action chosen godfather arg min max 
general multiagent systems authors tested leader strategies repeated games showing better best response strategies paired follower opponents 
conitzer sandholm targeted rationality convergence learning goals defined bowling veloso section 
algorithm authors called adapt everybody stationary move equilibrium agent switches equilibrium best response strategies basis experience 
acts beliefs hypotheses agents stationary ii agents play part precomputed equilibrium 
beliefs updated basis analysis changes empirical distributions agents actions consecutive epochs learning iterations 
method shown asymptotically learn best response stationary agents converge self play repeated games 
addition defining targeted optimality compatibility safety section powers shoham gave algorithm authors provably meets requirements 
algorithm thoroughly tested stateless games broad set reinforcement learning techniques fared asymptotic performance 
important result forthcoming investigation adaptive ma rl falls area direct policy search 
singh 
focused gradient ascent incremental policy update agents actions repeated strategic games 
games stochastic strategy agent modeled real number interval 
say strategy agent represented agent chooses action probability second action probability 
similarly second agent stochastic strategy described parameter 
gradient ascent update rule vi expected payoff agent agents play strategies gradient ascent step size 
main result proven authors assuming infinitesimal step size lim payoffs agents converge limit nash payoffs 
multiple state problems order practical interest ma rl algorithm handle problems non empty state space delayed reward full blown markov games 
algorithms typically extra requirement assumptions ii listed section agents learning algorithm self play situation exists 
ma rl algorithms markov games derived learning share common skeleton 
skeleton algorithm agent identified agent dots stand values corresponding argument 
algorithm clearly similar basic learning algorithm 
important differences exist 
analyze turn 
multiagent reinforcement learning algorithm generic multiagent learning agent input learning rate discount factor qi hi ui ui ui ui observe initial state loop hi 
qn draw ui hi ui apply ui suitable exploration observe actions uj rewards rj state qi qi ri 
qn qi qj qj rj 
qn qj loop line updates policy agent state hi strategy computed tables agents 
usual greedy policy agent maintains uses explicit stochastic policy computed basis agents tables 
line temporal difference update state value update target value greedy action state basic learning 
value evaluation function takes account tables agents 
steps described need tables agents mas 
agent needs maintain tables agents table 
done modeling step lines 
necessity measurable rewards ii self play assumptions obvious line rewards agents needed perform update agents learning algorithm update reflects reality 
line deserves comments 
agent acts stochastic policy 
action agent applied environment simultaneously agents actions 
agent follows strategy state computed solve takes exploratory actions 
important note exploration necessary general case policies agents stochastic 
stops strategies returned solve assign zero probability weight actions 
suitable exploratory policy hand may assign zero probability weight action limit 
times solve searches certain type game theoretic equilibrium joint strategy strategic game state markov game eval gives value solution 
functions cases closely interrelated may regard single subroutine returning strategy game state value state strategy 
cases separate strategy state value solutions agents form consistent joint solution subroutine thought running identically parallel agents returning agent part solution 
equilibrium selection problem arises solution unique 
general multiagent systems context necessity solve eval returning complementary parts consistent solution agents 
multiagent learning algorithms understood instantiations skeleton algorithm 
game fully competitive definition minimax principle applied littman 
case agent player chooses strategy maximizes benefit assumption agent opponent act minimize benefit 
value state update assumed worst case value 
perspective player assuming deterministic action selection opponent eval max min solve arg max min note due fact agent needs store single table denoted simply modeling opponent table necessary 
minimax optimization problem solved linear program 
algorithm converges minimax values fully competitive game resulting policy attains learned values regardless opponent policy 
author experimented algorithm simulated soccer game agents dimensional gridworld see section 
minimax algorithm extended agents assuming agents act learner minimizing joint action 
situation formalized littman 
integrated minimax team section single algorithm called friend foe learning convergence conditions algorithm specific classes markov games converge littman 
name algorithm reflects fact agents designated friends maximization consequently applied action terms value computation minimax consequently applied action terms 
give actual intricate formulae computing value policy 
team learning friend foe learning instantiations generic multiagent learning algorithm 
team uses plain maximization eval function deterministic action choice achieving maximum value solve 
game fully cooperative tables agents identical modeling required 
nash learning algorithm works general class markov games hu wellman 
name suggests algorithm computes nash equilibrium state markov game uses temporal difference policy updates 
qn ne qn 
qn nei 
qn expression nei represents strategy component nash equilibrium corresponding agent slightly abused notation keep formula simple value vi fact conditioned joint policy component state nash equilibrium strategy 
multiagent reinforcement learning estimated value vi state agent joint stochastic policy summation agent tables weighted probability corresponding joint actions assigned joint policy state hn un un 
hn un qi 
un 
nash learning agent needs model tables 
algorithm proven converge agents consistently nash equilibria compute eval states game 
nash learning suffers equilibrium selection problem issues described section 
replacing nash equilibrium correlated equilibrium obtain correlated equilibrium learning greenwald hall 
give modified formulae identical respects type equilibrium 
advantage correlated equilibrium computable linear programming see section 
authors experimented types equilibria correlations agent policies expressed linear programming objective function utilitarian maximizing sum agents rewards egalitarian maximizing minimum agents rewards republican maximizing maximum agents rewards independently maximizing maximum agent reward 
experiments run small gridworld grid soccer game 
authors circumvented need modeling tables agents equilibrium selection problem centralizing learning process 
course feasible solution method applied realistic multiagent system equilibrium selection problem remains intact 
similarly equilibrium see section updates obtain asymmetric multiagent rl nen 
algorithm addresses situations leader agents able enforce action selections follower agents 
author experimented simple grid world obtaining marginally better results asymmetric case respect symmetric 
agents identical algorithms necessary follow complementary parts asymmetric learning process sense assumption self play necessary 
variant multiagent learning give self play assumption socalled extended optimal response algorithm hayashi 
algorithm designed games agents 
describe agent perspective 
goal algorithm reach nash equilibrium second agent learning exploit best response exhibits stationary behaviour extended optimal response 
agent uses heuristic solve function solve arg max tuning parameter estimate second agent policy distance function approximates increase return second agent achieve changing policy policy max bh 

adaptive techniques values joint policies computed 
actual temporal difference update agent sarsa update eval simply value taken joint action 
part maximization argument accounts best response part agent behaviour second part drives system nash equilibrium indirectly reducing opponent desire deviate policy 
extended optimal algorithm empirically tested simple stateless games matching pennies battle 
second agent learning policies learned agent kept oscillating slightly converging due heterogeneous nature criterion 
adaptive techniques adaptive multiagent reinforcement learning thread represented researchers investigated possibilities 
approaches fall heading parametric adaptation definition structural adaptation definition 
parametric adaptation win learn fast policy hill climbing wolf phc algorithm performs parametric adaptation learning process bowling veloso 
approach combines basic learning policy gradient ascent idea outlined section singh 
precisely simple agent action setting step size parameter allowed take values adapt coarse sense basis analysis agent behaviour win lose win lose equilibrium strategies independently selected agents 
win lose infinitesimal step size assumption lim strategies agents shown converge nash equilibrium enhancing result singh 
proved convergence average payoffs nash payoffs 
intuition agent change policy cautiously winning opponent take strong measures remedy situation boldly losing improve chances escaping losing situation 
encourage convergence effect extend self play just agents uses uses learning rule 
goals multiagent reinforcement learning researchers designing algorithm rationality convergence properties described section 
practical version algorithm authors supporting problems multiple states number agents discrete action spaces finite size 
practical version algorithm 
uses heuristic average performance criterion assess agent winning losing line 
note algorithm depend directly knowledge interaction agents 
policy update line simplified form actual update ensures validity probability distribution 
algorithm wolf phc agent input learning rate discount factor step sizes win lose state visits counter observe initial state loop average policy draw apply suitable exploration observe increment visits counter update average policy win adapt step size lose arg max policy loop algorithm empirically tested simple stateless games matching pennies rock scissors games small state spaces gridworld gridworld soccer game littman 
convergence achieved cases extremely large numbers learning iterations matching pennies order gridworld gridworld soccer 
averaging nature heuristic adaptation criterion gives algorithm certain amount inertia may part reason slow convergence 
banerjee peng propose instantaneous performance criterion expressed terms second order difference agent strategy 
criterion adaptation rule agent win lose 

adaptive techniques validity criterion proven practical algorithm demonstrated simple games dimensional block pushing task continuous state 
results encouraging algorithm converged simple tasks significantly faster wolf phc factors various problems 
continuous state task comparison favourable terms convergence speed quality learned policy times better terms average path length goal position 
structural adaptation existing structural adaptation multiagent rl focuses adaptation action dimensions state action space ventura kok 
closely related empirical study concept awareness cooperative multirobot teams 
awareness defined perception robots locations actions 
ma rl setting location replaced generic term state 
structural adaptation ma rl takes awareness concept step discriminating awareness needs agents basis state 
relate need awareness degree coupling agents 
region state space reward function agent depends significantly action choice ui 
ui 
un ui uj uj say region agent loosely coupled agents 
situation agent learn achieve performance aware agents 
hand satisfied region meaning reward function agent depends significantly components joint action say agent tightly coupled corresponding agents region conclude needs aware agents order perform 
important questions course identify regions space agent loosely coupled agents regions determine agents tightly coupled 
literature takes heuristic approaches answering questions 
ventura tree structure node contains values 
initially values discriminated basis agent action 
exists node state node contains vector values discriminated agent action 
take example node state containing table dimension qi ui 
node children set fringe nodes corresponding agents action variable 
inside fringe node detailed table stored discriminating values action agent corresponding node example fringe node agent child node state contain dimensional table form qi ui uj 
number learning steps elapses fringe children node examined determine lead greatest increase returns values discriminated action variable node 
node expanded multiagent reinforcement learning action branch node regular node child action 
say fringe node agent chosen expansion spawn set uj nodes corresponding fixed uj containing dimensional table form qi ui ui varies ui 
child receives set fringe nodes precise action variables expanded fringe node agent different process continues maximum expansion depth reached increase returns gained expanding nodes drops specified threshold 
algorithm tested stateless game players extended version matching pennies achieved performance close algorithm considered complete joint action start storing number values 
kok 
introduced structured approach uses model able explicitly represent awareness needs agents 
model coordination graph described detail section 
brief coordination graph agents nodes relationships agents arcs 
agent needs aware agents directly connected 
variable elimination algorithm choose coordinated joint action graph structure value functions agents known 
authors method learning structure coordination graph statistical analysis expected returns hypothetical relationships 
method empirically evaluated simple example problem domain able reach performance team learning longer time needed discovering value rules 
issues realistic rl ma rl rl general applied real life tasks realistic simulations number issues arise sake brevity haven discussed 
briefly remarks relevant issues follows 
large continuous state action spaces state action spaces agents large continuous dimensions tabular representations value functions 
function approximation techniques supervised learning help situations 
linear approximators tile coding nonlinear approximators neural networks sutton barto 
problem theoretical convergence guarantees longer hold function approximators 
class methods retain convergence properties function approximators direct policy search gradient descent 
methods typically slow get stuck local optima baird baird moore 
partial measurability state environment completely measurable markov assumption violated 
depending severity violation may may safely ignored learning agents 
needs accounted literature uses 
concluding remarks classes solutions 
maintain probabilistic distributions beliefs state underlying markov task update beliefs observations bayesian framework 
incurs high computational costs 
second class solutions endows agent ability remember sequences states hope sequences retain sufficient information satisfy markov property whitehead lin 
prior knowledge realistic problems tabula rasa model free rl effective exploration huge stateaction space delayed reinforcement uncertainty slow learning impossible 
accurate models task available prior knowledge help agents learn incorporating bias learning solution 
done ways initialization 
value function initialized contain prior knowledge 
trivial task especially generalizing function approximators 
local reinforcement signals help alleviating structural temporal credit assignment problem 
probably natural way biasing reinforcement learning contradicting recommendations sutton barto state reinforcement signal encode goal agent path goal 
matari progress estimators akin error signals process control tell agent right path 
heterogeneous reinforcement signals encode information multiple agent goals 
teaching 
human temporarily take control agent guide goal relinquishing control allowing agent fine tune behaviour 
speed learning significantly longest phase model free rl agents having knowledge whatsoever goal roam state space searching 
shaping 
start learning simple tasks progressively increase difficulty 
problem decomposition followed layered learning stone veloso 
elementary behaviours learned separately integrated higher level policy solve original task 
reflexes provided agent inception 
building blocks complex learned behaviour matari 
concluding remarks field multiagent reinforcement learning reached maturity 
extending firm understood results single agent rl multiagent case currently progress 
clear sign controversy existent field regarding suitable ma rl goal section 
multiagent reinforcement learning sign gap theoretical results practical applications ma rl 
theoretically justified methods strongly influenced game theory restrictive assumptions learning task order prove results assumptions hold practical situations 
methods assume agents know reward functions rarely case 
methods assume agents learning algorithm removes heterogeneous open multiagent systems discussion 
problem inherited single agent case requirement explicit tabular representation state action space 
problem aggravated multiagent context exponential explosion space number agents 
implies learning algorithms selective information words local possible 
empirical evaluations algorithms typically performed simple problems stateless small state spaces 
hints algorithms scale realistic problem sizes rarely exception banerjee peng 
hand practical applications direct extensions basic single agent algorithms 
extensions placed top complex ad hoc machinery designed simplify difficult learning problem 
features extracted environment state value function computed terms features 
local instantaneous reward functions guide agents state space matari 
machinery analysis methods difficult 
important reason times machinery takes account agents 
agents learn indirect obscure way 
ma rl algorithms designed stateless games 
multi state problems biased stateless game theoretic perspective 
solving gametheoretic equilibria stage wise fashion sits core theoretically justified ma rl methods 
meaning suitability stage wise game theoretic equilibria doubtful context finite infinite horizon tasks delayed reward 
issue rl framework general rl goal focused optimality disregarding desirable properties agents behaviour robustness changes environment approximately monotonic increase performance learning process 
rl algorithms offer asymptotic convergence guarantees saying transient performance agent 
set relevant research questions arises considerations stemming corresponding set possible research directions 
suitable generic ma rl goal 
incorporate performance requirements optimality 

stage wise game theoretic methods suitable multiple states finite infinite horizon delayed reward tasks 

unify theoretically justified practically applied ma rl methods 
question implies complementary research directions attempting 
concluding remarks relax assumptions theoretically justified methods second attempting prove soundness practical methods 

rl agent determine minimal information basis learn effectively 
research direction related question arises context partially measurable learning tasks 
state world observable control engineering sense directly measurable see section control engineering observers estimate components state 
observers conditioned availability knowledge model environment 
opportunities adaptive learning area parametric adaptation immediate opportunity represented basic parameters multiagent single agent rl learning exploration rates 
learning performance sensitive parameters 
exploration especially important quality learning current approaches simple exploration techniques 
initial values decay schedules learning exploration rates typically obtained painstaking trial error process 
automated procedure online adaptation parameters significant value 
basic parameters methods specific parameters adaptation lead improvements performance 
win learn fast algorithm step sizes win lose give coarse adaptation way adaptively shifting step size extremes investigated 
shown section structural adaptation focuses action dimensions state action space learner 
fruitful research direction pursued 
components environment state irrelevant learner small part state space 
state components referring agent relevant regions state space learner closely interacts agent 
related research question 
significant reductions computational requirements learning time obtained cases learner considers relevant parts state 
aspect typically disregarded literature ma rl state world regarded monolithic mandatory agents consider 
techniques encountered action space adaptation second order analysis returns applied state space adaptation 
begun investigating possibility bu 
idea starting point develop techniques allow agent discover simple learning techniques closer single agent rl suffice solve problem switch multiagent learning necessary 
just structural full blown adaptation important step bridging gap mentioned section 
chapter multiagent learning methods reinforcement learning technique applied learning multiagent systems number approaches worth considering 
review focusing learning tool direct improvement agent behaviour world opposed knowledge agents 
chapter intended comprehensive presentation non rl methods multiagent systems goal introduce illustrative approaches 
presenting evolutionary approaches multiagent learning 
review heuristic learning techniques close chapter set brief concluding remarks 
evolutionary techniques genetic algorithms genetic programming popular evolutionary computation techniques 
genetic algorithm population heuristic search method uses mechanisms borrowed biological evolution mutation recombination 
genetic algorithm maintains randomly initialized population candidate solutions represented strings values binary values 
strings called chromosomes values called genes 
fitness function gives quality solution represented chromosome real number 
iteration genetic algorithm performs operations 
chromosomes evaluated fitness function 

selection function applied population probabilistically choosing set chromosomes reproduce 
selection biased chromosomes higher fitness completely remove probability weight poor chromosomes maintain diversity population avoid local optima 

selected chromosomes reproduce mutation bit flips chromosome mutate crossover chromosomes recombine crossover give birth new chromosomes 
new chromosomes added population 
algorithm stops number iterations quality best solution exceeds threshold 
multiagent learning methods genetic programming class genetic algorithms evolves programs 
programs parse trees composed building blocks terminals nonterminals 
terminals constants variables functions arguments 
non terminals functions require arguments 
conventional genetic programming non terminals accept return single value fixed data type float 
evolutionary computation searches directly space agent behaviours closely related direct policy search ma rl 
difference problem formalized rl 
important thing note learning offline evolved generation performance agent task tested number trials yielding fitness value agent 
selection function applied new population agents born process continues 
performance evaluation step somewhat similar learning value estimates monte carlo rl 
best individual survives generations individual significant evolutionary view genetic algorithms populations individuals 
learning takes place lifetime agent generations hopefully improving agents 
haynes 
applied enhanced variant genetic programming called strongly typed genetic programming predator prey domain see section 
strongly typed genetic programming allows variables type definition generic functions generic data types 
restrictions imposes incur smaller search space basic genetic programming 
order evolve predator program authors set terminals boolean action north east south west stand terminal self current predator agent prey agent 
set functions construct 
distance comparison function 
function computing resulting cell position action cell 
function computing manhattan distance cells argument manhattan distance sum horizontal vertical offsets cells 
functions terminals predator program evolved strongly typed genetic programming 
program predators 
resulting best program instructed predators converge prey efficient manner previous predator algorithms literature 
programs evolved strongly typed genetic programming humanly understandable evolved basic genetic programming 
able generalize behaviour different world sizes genetic programming solutions 
higher quality strongly typed genetic programming solutions probably due part smaller search space size 
haynes sen attempted improve solution competitive coevolution predators prey 
coevolution evolutionary techniques applied simultaneously 
evolutionary techniques agent behaviours evaluation behaviours performance achieve interaction 
learning offline explained 
setting haynes sen predator prey program developed competitive coevolution hope arms race render quality predator behaviour progressively better prey skilled evading 
results surprising prey quickly developed simple strategy chose random direction steadily moved straight line direction world toroidal prey able indefinitely 
result casts shadow doubt relevance prey behaviours typically pursuit domain test cases multiagent learning cooperation algorithms 
cooperative coevolution investigated de jong dispersion game 
thought special case strategic game definition agents set actions un prefer select different actions 
dispersion games helpful modeling task allocation problems tasks equal priorities goal agents distribute evenly possible set tasks 
context actions agents task choices action corresponds task 
version authors dispersion game number actions agents ideal situation agents select action 
represented fully cooperative reward function ui uj 
un authors evolved populations behaviours coevolution agent 
behaviour agent represented vector preferences actions 
preference agent selecting action uo 
vector chromosome individual population 
agent selects action softmax rule uo 
fitness agents computed reward function way 
chromosome agent population paired fittest chromosomes agent populations 
strategies represented chromosomes agents select actions 
agents chose action task task assigned randomly 
fitness chromosome task successfully assigned agent corresponding strategy 
trials performed fitness values averaged obtain better estimate agents performance 
fitness mas computed selecting strategies best fitness agent populations putting executing 
fitness mas percentage tasks successfully assigned 
authors experimented idea collective intelligence 
collective intelligence addresses problem designing utility functions agents easily learnable learned give performance multiagent system 
multiagent learning methods called wonderful life utility computed subtracting collective utility system agent place collective utility agent removed 
coevolution setting utility measured fitness system 
wonderful life utility agent difference fitness mas agent place fitness mas agent removed 
authors wonderful life utility led enhanced performance agents 
process significantly sensitive learning parameters population size number trials ran estimating fitness value 
heuristic approaches approaches discuss section oriented resource management task allocation see section 
typically decision making algorithm relies number parameters 
parameters learned observation learning rules similar spirit temporal difference update methods rl framework 
learning alliance alliance framework deals task allocation heterogeneous multiagent systems parker 
agent actions alliance structured levels 
agent say agent perform set mid level functions ui 
performing function ui set agent working high level task ti ui ti ui set tasks required system mission 
lowest level agents take functions ui ui working task ti ui give working task ti ui completed 
efficiency agent performing function ui evaluated performance metric completion time applied task ti ui 
executing low level actions agents driven respectively motivations impatience functions giving 
rate impatient depends control parameters fast ui rate impatience agent concerning function ui agent working ti ui 
slow ui rate impatience agent concerning function ui agent working ti ui 
ui time agent maintain function ui agent 
agent adjust parameters heuristic learning mechanism set mi monitors monitor function able perform mi ui 
monitor mi function ui observes agents performing task ti ui records performance 
system execution divided phases learning phase termed active learning authors problem solving phase phase termed adaptive learning authors adaptive learning framework name justified give reason 
learning phase agents maximally patient minimally take attended tasks give tasks began undertaking 
purpose phase allow agents familiarize performance team 
problem solving phase agents impatient 
heuristic approaches control strategies achieve performance keep updating monitors contents reflect current performance agents 
give actual heuristic control strategies updates see parker 
examine alliance fits dmas framework definition 
information contained monitors mi part agent internal state si updated basis agent observations agents yi dynamics pi 
internal state si contains impatience levels agent parameters alteration levels 
parameters altered basis information contained monitors state influences state dynamics pi 
agent decides take give tasks policy function hi applied impatience components state 
see view term adaptive learning apply monitor parameter updates occurring problem solving phase 
fact processes simply learning online opposed learning phase considered sense line performance system relevant 
relationship alliance dmas summarized table 
note control parameters fast ui slow ui ui part internal state agent constant coefficients internal dynamics pi 
alliance dmas set agents set agents tasks ti functions ui low level actions action space ui set monitors mi part internal state space si impatience levels part internal state si monitors impatience levels updating internal dynamics pi low level action execution rules policy hi table correspondences alliance dmas elements schaerf 
approach load balancing problem perspective closely related rl 
instructive study formal model load balancing multiagent system definition multiagent multi resource stochastic system tuple set agents number 
set resources number 
probabilistic job submission function gives probability idle agent submits new job time step agent idle time step jobs submitted completed prior time step probabilistic job size function probability job submitted agent time step size probabilistic resource capacity function probability resource capacity time step resource selection rule instructing agents resource choose submit new job 
multiagent learning methods notation slightly altered accommodate notational conventions 
goal system minimize processing time unitary job size averaged jobs ii standard deviation quantity 
small average job processing time yields efficiency small standard deviation yields fairness jobs processed significantly faster slower 
discuss selection rule detail 
rule dictates agents choose resources submit jobs 
fixed selection rules authors study learning selection rule term adaptive 
rule uses efficiency estimators counters completed jobs maintained agent resource vectors initialized 
spirit rl information learning local 
comes form experiences completed job 
experience contains order resource job submitted start completion times size job 
experience agent say agent updates efficiency estimators job counters ei ei ei bi bi bi positive constant 
probability agent selecting resource roulette wheel hi hi hi hi ei ei bi positive constant 
relation seen temporal difference update old estimate ei moved fraction distance target investigate learning process formal model definition fits dmas framework 
set resources represented explicitly dmas part environment state resources part environment state capacity function part environment dynamics experience tuples agent observations yi 
efficiency estimators ei counters bi components internal state si update formulae part agent dynamics pi 
job submission size functions interpreted part agent policy hi resource choices sizes submitted jobs part stochastic 
concluding remarks agent actions dependence time captured dmas dependence agent policy internal state si 
job submission size functions completely specify policy agent 
correspondences summarized table 
alliance dmas set resources environment state space resource choices job sizes submitted jobs action space ui experience tuples agent observations yi resource capacity functions part environment dynamics efficiency estimators ei counters bi agent internal state si update rules agent dynamics pi resource selection strategy hi agent policy hi table correspondences load balancing model dmas elements adaptive parameter 
question illustrates difficulty distinguishing adaptive learning static non adaptive learning situations 
trivially corrects internal state obtain learning rate choose consider parametric adaptation 
distinction course slightly arbitrary difficult complex learning algorithms 
number approaches literature regard multiagent learning highly interactive proactive process considered ma rl methods described 
plaza multiagent approach case reasoning probably best described distributed learning 
case base divided agents disjoint partially overlapping subsets 
receiving new problem solve agent attempts solve problem assesses quality solution confidence measure 
solution deemed considered final solution 
hand agent finds solution reliable proceeds ask agents opinion problem 
querying process continues termination condition satisfied majority realized 
situation arise real life instance group organizations desire maintain privacy case bases wish collaborate finding solutions 
note mas seen tool facilitate machine learning technique decentralizing learning process 
view opposite encountered far methods chapter learning tool mas improve decision making process 
concluding remarks approaches chapter take different view learning introduced chapter learning process improves decision making abilities agent lifetime 
evolutionary approaches generations short lived individuals perform learning 
approaches interaction agents environment multiagent learning methods needs simulated efficiently 
fitness evaluation requires large number trials executed 
executing real world question 
unclear agents adapt online fashion changes environment behaviour learned evolutionary computation loaded agents 
approaches section looked mas tool distributed learning 
research focus perspective appears little relevance multiagent control 
heuristic complex nature learning methods reviewed section difficult analyze judge adaptation parameters structure useful 
chapter coordination problem coordination arises multiagent systems due distributed nature control exercised agents 
coordination defined vlassis process individual decisions agents result decisions group 
problem stringent cooperative multiagent systems appears agents self interested 
example coordination cooperative setting team agents controlling traffic lights busy city 
properly synchronize decisions switching colour traffic signals traffic jams probably result likelihood accidents increase 
example network controllers plant locally control decisions harmful plant leading dangerous regime installations people risk 
agents self interested need coordinate certain situations police car ambulance driving different emergencies coordinate entrance meet intersection crash emergencies serviced 
formally coordination defined context dynamic multiagent system definition follows 
definition coordination dmas problem consistent selection agents time step joint action uk 
un jeopardize performance agent definition necessarily vague coordination technique long agents opportunity reach respective goals eliminated incorrect action selections 
definition imply goals agents achievable says remain achievable joint action selection 
say easy agents reach goals action selection coordination techniques better 
important thing note tight interconnection learning coordination 
effective learning requires coordination agent actions point agents learn useful things 
conversely coordination assisted ways learning 
aspect mentioned chapters disguise names consistent joint action selection equilibrium selection 
chapter coordination treats detail reviewing frameworks techniques achieving coordination 
order investigate learning help coordination discussed techniques differentiate designed learned coordination focus exposition 
chapter structured way 
classify coordination techniques dimensions review coordination frameworks encountered literature 
discuss possible alternatives coordination multiagent system including mentioned learning coordination social conventions roles coordination graphs 
close concluding remarks research opportunities 
taxonomy degree information sharing coordination techniques differ significantly dimension information sharing information shared agents information comes shared 
spectrum lie coordination methods require agents explicitly share information 
agents methods rely learning way develop coordination skills boutilier kok 
agents exchange rich information communication tambe 
sit techniques relying prior domain knowledge 
distinction categories clear cut 
instance roles rely exclusively prior knowledge require communication extent order properly assigned spaan 
offline design vs learned coordination coordination methods may designed offline hardwired agents learned agents interactions lifetime 
approach typically simpler second flexible beneficial open complex multiagent systems walker wooldridge 
spectrum lie methods view coordination offline phase planning 
view coordination phase followed execution phase formed plans pursued 
type offline coordination handle activity multiagent systems focus 
accurate model problem available second plans initially satisfactory suitability degrade environment evolves 
discuss approaches 
taxonomy dimensions interested 
degrees information sharing 
offline designed learned coordination especially 
coordination frameworks 
coordination frameworks markov game definition stateless version strategic game definition popular formal models coordination problem non communicative setting 
coordination problem setting interpreted consistent selection agents part joint action 
joint action deemed agents times game theoretic equilibrium nash equilibrium 
coordination problem markov game times interpreted equilibrium selection problem introduced section 
note game theoretic equilibria currently accepted goal multiagent learning problem section 
discussed strategic markov games proceed different formal model non communicative context walker wooldridge describe emergence social conventions 
model interesting explicitly acknowledges existence internal agent state albeit state simple memory 
model consists elements set agents common set strategies term strategy meaning described chapter strategies represents possible convention agents may agree 
fact entire policies constraints policies dictating robotic agents yield right way robot coming right see section 
modeling learning process suffices identify primitive actions 
set possible interactions interaction occurs agents meet compare strategies 
fact special structure observation functions pairing agents time step allowing observe action 
common observation space agents structure observation yi uj identifies agent agent interacted strategy observed 
memory si agent memory consists finite sequence past interactions agent participated 
memory space si length memory 
strategy update function hi agent memory chooses strategy agent follow ui hi si 
goal agents eventually settle strategy 
update fact policy meta policy switches strategies 
update internal memory explicitly model forms internal agent dynamics pi 
similarly memory agent internal state 
notations intentionally altered bring forth similarities 
similarities summarized table 
note agents homogeneous 
see model primitive form learning learning process simple transfer observations internal state 
coordination social conventions model dmas set strategies conventions action space ui interactions memory space si length observation space yi internal state space si memory update function pi internal dynamics pi strategy update function hi policy hi table correspondences social conventions emergence framework dmas elements coordination frameworks communication illustrate shell teamwork steam model tambe 
steam model relies typically class vast domain knowledge shared agents 
knowledge specified operational form inside agent operator hierarchy 
operator building block agents behaviour 
portion example operator hierarchy helicopter attack domain taken tambe 
execute mission engage wait battle position employ weapons mask observe scout forward example steam operator hierarchy 
wait scouting operators types individual operators require single agent executed team operators requiring team subteam execution 
team operators enclosed square brackets involving subteams marked asterisk 
core notion steam team joint intention 
joint intention action mental state team agents members team jointly committed action mutually believe committed 
informally joint commitment ensures action taken terminated jointly involved agents 
order execute team operator steam corresponding team subteam instantiates team joint intention execute 
steam agent private state includes information application individual operators team state including beliefs team subteams participates team members team leader available communication channels 
goal steam agent team achieve top level team operator execute mission 
team operator unachievable team special team operator called repair invoked attempts reassign agents team failed operator achievable 
role communication steam ensure correct application team operators communicating beliefs 
lead large number messages exchanged 
order limit communication overhead steam implements selective 
coordination frameworks communication mechanism 
precisely message send agent heuristically balances cost communication estimated cost consideration utility achieving goal team operator possibility team knows information contained message 
agent sends message cost exceeds communication cost 
easy precisely fit highly complex expressive framework general dmas model 
approximate correspondence table 
steam communicative dmas individual operator domain level policy team operator domain level message sending policies private team state agent internal state si selective communication message sending policy snd table correspondences steam communicative dmas elements communicative multiagent team decision problem com mtdp framework introduced goal analyzing optimality complexity tradeoff multiagent coordination pynadath tambe 
introduced analyzed framework example section 
illustrated representative approaches modeling coordination multiagent systems 
mention interesting ideas going details 
stone veloso introduced periodical team synchronization domains time critical environments agents act autonomously low communication periodically synchronize free communication setting 
periods free communication agents establish called room agreements consisting role formations environmental triggers switching formations 
agents move low communication periods apply previously set agreements allowed limited amount communication 
combination line line coordination 
authors applied approach robotic soccer free communication period mid game break 
relevant example robotic search rescue team opportunity establishing room agreements starting mission possibly periods intensive activity mission 
interesting view coordination mediated specialized entities existing environment called coordination artifacts ricci 
artifact consists usage interface operable agents domain level actions set operating instructions describing interface specification coordination behaviour implemented artifact 
think semaphore system intersection green light request buttons pedestrians order form image artifact 
type coordination falls online category offers compromise information sharing dimension way 
agents longer need share extensive domain knowledge integrated coordination artifact need know read operating instructions 
coordination learning coordination popular model learning coordination markov game definition 
noted section coordination problem context times interpreted equilibrium selection problem 
learning coordination typically studied homogeneous non communicative multiagent systems endowed full measurability 
times assumptions measurable actions known reward functions necessary learning algorithms 
characteristics game theoretic approaches multiagent learning 
multiagent learning approach classify learning coordination require agents reason explicitly coordination 
means agents acknowledge fact action choices agents necessarily part desired joint action reasoning explicitly terms agent actions learning mechanisms agents strive minimize negative impact issue performance 
includes learning coordination mechanisms roles social conventions 
discuss section treat separately detail sections 
learning agents obvious way learn coordination learn agents behave predict actions knowledge choose appropriate response actions 
knowledge agent behaviour model agent 
approach called opponent modeling literature game theoretic context exploit agents model known 
useful cooperative settings agents access limited information limited communication 
refer learning agents broader term agent modeling 
typically modeled agents assumed reactive reactive agent models maintained updated learning agent 
note order exploit learned models agent know effect agent actions performance 
stochastic game means agent say agent know learn model reward function fictitious play simplest agent modeling algorithms 
designed repeated stochastic games 
fictitious play agent models agent counter function nj uj value nj uj counts times agent observed action uj 
game repetition agent estimates mixed strategy uj nj uj nj uj uj 
uj plays best response estimated reduced strategy profile reduced strategy profile consists estimated strategies agents see section 
convergence results fictitious play exist restrictive settings fully competitive games notable 
interesting property fictitious play nash equilibria absorbing point agents play nash equilibrium continue playing equilibrium subsequent repetitions game 
important shortcomings fictitious play restriction repeated games requirement actions agents measurable 
boutilier attempted 
learning coordination soften restrictions 
author argued coordination problem multiagent markov decision process mmdp definition decomposed coordination problems state solved locally terms expected returns states 
visit state mmdp play embedded repeated game corresponding state 
important advantage approach allows adding coordination overhead states coordination problems arise optimal joint action uniquely determined 
method learn coordination setting bayesian extension fictitious play 
agent models agent prior distribution strategies 
uses prior predict actions agent updates bayes rule observations 
dirichlet distribution prior 
dirichlet distributions mmdp states conveniently expressed counter function similar fictitious play nj uj value nj uj counts times agent executed action uj state meaning quotes evident immediately 
actions measurable nj easily updated adding unitary increment counter observed action 
case non measurable actions agents bayes rule infer distribution agents actions 
time step nj xk uj increased fractional increment equal probability agent performed action uj nj exactly counter function meaning values similar counters fictitious play 
infer posterior probability agent executed uj time step agent uses uj xk ui xk xk xk uj ui uj xk xk xk ui agent observed transition xk xk executed ui quantity uj xk belongs estimated policy agent xk represented dirichlet prior computed similarly uj xk hj xk uj nj xk uj 
nj xk uj probabilities computed integration transition function xk xk uk xk uk xk 
assumes model environment known 
author mention model world learned similar fashion agent model 
hu wellman studied recursive agent models domain auction market 
purposes modeling policy non learning agent assumed deterministic affine linear state hj sj bj aj bj constant matrices appropriate dimensions 
linear relation expresses relation quantities goods sj held agent bidding prices uj 
note implies internal state modeled agent needs observed estimated turn 
modeling level agent defined recursively follows coordination level learning agent models agents looking history data agents actions similarly fictitious play 
level learning agent models policies agents assuming policies fixed form 
level learning agent models agents level learning agents 
important incorrect assumptions agents harmful level algorithm making fewest assumptions agents performs better higher level algorithms modeling level agents overestimated 
value coordination coordination techniques ask question agents coordinate proceed seek answer typically optimal coordinated joint policy agents 
coordination certain fact especially case learning coordination rounds required agents learn coordinate important issue arises 
possible harmful alternate suboptimal safer path preferable 
new question agents strive coordinate equivalently value coordination boutilier introduced extension value iteration state underlying problem explicitly considers state coordination mechanism 
actual nature coordination mechanism principle unimportant 
state variable intuitively thought having values coordinated uncoordinated 
discrimination state dimension enables usage dynamic programming engine reason benefits attempting coordinate 
specifically certain state underlying markov game requires coordination value coordination mechanism state underlying state uncoordinated agent may choose pursue path uncoordinated state due costs 
boutilier placed value coordination fully bayesian framework 
state agent expanded belief state containing beliefs model environment models agents 
tradeoff exploration improving model exploitation resolved bayesian inference 
expected return current state weighed possible increase knowledge brought exploratory action 
component termed expected value information 
value component high agent biased learning world agents risk losing immediate reward capable coordinate 
value information low agent prefer safer choices 
providing methods computing value coordination approaches high computational costs 
problem accentuated second method exact inference intractable small problems approximate methods impractical significantly larger problems 
social conventions 
social conventions social convention recipe places constraints behaviour agents 
social conventions functions strike balance individual freedom agents hand goal multiagent system ii simplify agents decision making process walker wooldridge 
coordination perspective interested function 
agents faced choosing set equally joint actions tie social conventions focus choice fewer joint actions 
ideally doing left single joint action tie eliminated coordination arises 
typical example social convention everyday life righthand right way rule applied drivers arriving unmarked intersection 
note convention tie free streets meeting intersection occupied tie arises broken mechanism righthand right way 
social conventions rational order appeal rational agents 
social convention common knowledge agents better 
coordination technique social conventions designed offline learned online fashion interactions agents 
designed social conventions action sets agents discrete common knowledge simple social convention called lexicographic ordering applied boutilier 
convention requires set agents ordered action sets agents ordered orderings common knowledge 
orderings agents uniquely sort joint actions agent action agent 
coordination achieved agent selects joint action sorted list executes part joint action 
learned social conventions typically talking conventions emergence researchers simple learning rules update agents choice social conventions 
term emergence context just name learning 
walker wooldridge falls line 
model researchers describe conventions emergence introduced section 
model assumes set possible conventions common knowledge time step agents paired allowed interact 
interaction agents chance observe memorize current convention choice 
memory agent finite length trigger changes convention choice agent policy hi 
researchers investigated set policies majority rules instances allowing agents communicate memory contents 
turned tradeoff speed convergence social convention number convention changes incurred process 
number switches measure cost learning process switch conventions incur cost agent real life 
coordination matari casted social conventions learning problem reinforcement learning framework interpreting social rules actions providing reinforcement actions 
power rl available learning social conventions 
author physical domain robotic agents forage food pucks limited area return pucks home base 
agents learn social behaviours social convention yielding motion conflicts 
reward agents progress individual goal repeating behaviour observed agents observational reinforcement 
form imitation reinforcement obtained agents vicarious reinforcement 
heuristic solution structural credit assignment problem rewarding agent progress multiagent system 
weighted combination rewards obtained sources individual observational vicarious agents successfully learned yielding social rule 
due small number rl states interpreted environmental cues designed fashion actions obtained results insightful 
framing learning social conventions rl methodology powerful 
roles role constraint imposed action space agent 
roles formalized dmas framework definition 
practice roles reduce number actions agents consider decision 
roles similar social conventions 
difference constraints imposed roles applied action space prior computing set actions eligible execution current state say set equilibria stochastic game constraints imposed social conventions applied set computed 
roles greater potential simplifying decision making process agents social conventions 
reduce size input time consuming actual decision making process agent 
process illustrated 
action space designed roles role constraints constrained action set core decision making eligible actions social conventions constraints decision making roles social conventions 
chosen action literature dealing offline designed roles typically associates elements multiagent system endowed roles set roles containing roles 
priority ordering preference ordering roles 
ur 
encodes importance roles task performed 

roles utility potential function value ur measures environment state fit agent fulfil role set formations strategies 
formation associated different priority ordering 
set rules switching formations 
literature uses names element alternatives parentheses spaan stone veloso 
rules switching formations typically simple 
robotic soccer formation team possession ball ball opposing team spaan 
larger set environmental cues may determine formations switching stone veloso 
assume roles utility function common knowledge action sets discrete roles assignment state performed algorithm spaan 
algorithm role assignment agent require roles utility function input current state role ur current formation ur 
assign role ur agent arg max ur agents know utilities performing roles exchange communication line 
learned roles roles assignment learned literature consider priority orderings roles 
presence learning priority orderings superfluous provided learning successful priority orderings implicit learned role choices 
rules formations rules switching formations 
left elements set roles utility function 
yields types role learning learning roles utility function 
ii learning utility function 
line exists research showing certain conditions teams learning agents tendency evolve behavioural diversity balch 
imply specialization multiagent teams roles beneficial cases effectively learned agents 
second line prasad 
applied utility learning multiagent cooperative problem solving domain 
agents learn roles assume different situations coordination utility probability cost upc framework described 
activity system divided learning phase problem solving phase 
phase agents learn appropriate situation role assignments applied actual problem solving 
regard problem perspective agent perspective omit agent index 
states mapped smaller number situations agent fulfill set roles maintains tables utilities probabilities costs potentials role situation 
utility agent estimate final state expected value selects role situation probability represents uncertainty agent final state reached chooses role cost represents expected computational cost reaching final state choosing potential role estimates usefulness role discovering pertinent global information constraints 
measure strongly related value coordination high biases agent discovering new information problem risking short term performance process 
objective function evaluate role choice utility probability cost potential values 
function corresponds role utility function introduced 
learning function choose roles roulette wheel selection mechanism 
probability choosing role situation ur ur ur ur ur ur ur ur ur 
agent learns estimates monte carlo fashion cost disregarded objective function particular prasad 
problem solving instance learning phase agent updates estimates situation role pairs encountered en route final solution state uf uf utility final state final state success failure conflicts agents followed information exchange encountered path solution 
constant learning rate 
problem solving phase agent uses greedy role choice learned values arg max ur 
approach resembles reinforcement learning ways 
learning selection rule thought exploratory policy related softmax fact greedy policy objective function 
information joint utility roles selection encoded targets updates 
value estimates take account role choice action agent 
greedy action selection method probably suffers threats convergence similar affect basic learning multiagent contexts especially couplings agents significant typically case task allocation 
coordination graphs 
coordination graphs realistic situations agent consider entire joint action coordinate agents environment states 
current environment state small set agents influences outcomes agent actions 
agent may interested certain components environment state vector 
coordination graphs model represent situations assumptions hold guestrin 
typically placed top underlying task model 
coordination graph agents nodes arcs express coordination relationships agents agents connected arc need directly coordinate actions 
example 
knowing environment state agent knows needs coordinate agent needs coordinate 
prior conditioning state 
conditioning state 
example coordination graph 
conditioning current environment state coordination graph reduce henceforth example borrowed kok 

functions qi local expected returns functions specify coordination dependencies agents local function involving agents 
local expected returns components global function mmdp state mmdp reward system sum agents rewards global function sum local functions course omit state variable local functions conditioned current state optimal action choice maximizes variable elimination 
proceeds follows step agent chosen elimination 
agent collects local functions involved communication neighbours computes optimal strategy conditioned action choices rest agents 
strategy longer depends chosen agent eliminated graph 
process continues iteratively agent remains action trivially chosen maximize return 
reverse order elimination agents choose actions conditional strategies actions chosen predecessors 
coordination optimal joint action way depend order agents eliminated 
efficiency algorithm 
example chosen elimination agent collects local functions involved maximizes sum max max max 
conditional strategy agent corresponds inner maximization term max 
agent computes communicates new function essentially committing execute action maximizes function chosen actions 
relation written max max 
depends agent computes 
relation written max max 
elimination agent reduces max max straightforward maximization 
chooses arg communicates choice agents 
agent binding computes simple maximization commu 
backward pass continues agent moment optimal joint action determined 
selective dependence state actions agent returns central coordination graphs approach conveniently represented called value rules kok 
rules condition returns agent environment state actions agents 
instance rule describing agent close wall say lying vertical coordinate moves north bump look move north vertical coordinate component environment state return agent obtained summing returns value rules condition parts satisfied 
variable elimination algorithm described ensure consistent selection optimal joint action state game 
function similar social conventions 
means manner similar 
coordination graphs problem simplified combining coordination graphs roles kok 
roles assigned agents algorithm 
agents assigned roles conditioning coordination graph role restrictions follows environment state 
alternatively say role term added condition part value rules agent 
process represented schematically 
initial set reduced set value rules value rules state conditioning role conditioning reduced set value rules variable elimination decision making roles coordination graphs 
designed coordination graphs chosen action coordination relationships agents known priori coordination graph designed offline wired agents 
approach taken guestrin 
kok 

variable elimination algorithm requires communication agents 
resulting joint action depend order agents eliminated additional assumptions variable elimination run parallel agents kok 
assumptions exists lexicographic ordering actions ordering common knowledge agents 
ordering break ties consistent manner backward pass variable elimination 
value rules agent common knowledge agents connected agent coordination graph 
agent observes components state appear condition part value rules agents connected coordination graph 
learned coordination graphs kok 
method learning coordination relationships available priori 
prior knowledge available initial coordination graph arcs agents independent learners 
coordination relationships guessed case situations coordination graph initialized capture extended learning 
agents create hypothetical arcs coordination graph store statistics expected returns hypothetical arcs 
periodically agents examine stored statistics analyzing difference obtained returns hypothetical arc place 
difference statistically significant benefit considering corresponding coordination relationship inserted coordination graph 
coordination concluding remarks reviewed chapter representative frameworks relevant techniques coordinating activity multiagent system 
approaches look coordination distinct process separate learning added top 
coordination issues arise presence absence learning 
believe activities tightly interrelated manifestations effort rational agents goals 
believe looking unified perspective helpful 
framework coordination graphs important step direction 
coordination graphs tool coordination representation multiagent learning task kok vlassis 
hope dmas framework complementary step 
viewing learning coordination part agent dynamics facilitate interpretation analysis processes interconnection 
important research opportunities stem agent modeling area 
approaches common shortcoming consider modeled agent static reactive system 
recursive modeling looks considers agents turn model agents 
static agent assumption violated settings 
immediate common example fails modeled agent learning agent 
furthermore agent modeling approaches black box models notable exception recursive modeling simple linear policy assumed hu wellman see section 
ignores cooperative situations rough models known agents 
solution problem model able represent agent dynamics 
solution second problem grey box modeling model assumed partly known 
possibilities come mind structure model known grey box modeling filtering approaches 
known modeled agent black box modeling dynamics captured neural network 
opportunities learning coordination mechanisms add overhead decision making terms processing communication 
overhead significant techniques share little information agents learning coordination see boutilier 
situations desirable agents ask coordinate ii value coordination iii coordinate 
specifically agents selective situations apply coordination mechanisms 
flexible way doing online discovery coordination needs 
falls term structural adaptation seen example section kok 
direction bu 
just steps existence form general criteria answering question iii investigated 
chapter multiagent application domains review chapter specific application domains multiagent system mas literature test beds learning coordination approaches 
select promising domains validate research multiagent learning coordination 
identify specific challenges posed domains investigate fits large scale traffic incident scenario interactive collaborative information systems project 
task needs performed mas influences position taxonomy dimensions introduced section 
influences course environment related dimensions degree communication available degree environment observable agents local global perspectives 
objective task translates goals agents influences degree cooperation agents 
task imply design considerations restrict choices rest dimensions agent heterogeneity reactivity vs deliberation point consider aspects 
interested cooperative agent teams focus cooperative tasks 
bias review partially observable tasks allow approximation assumption complete observability 
impose restrictions dimensions mention restrictions implied reviewed domains 
robotic teams robotic teams identified multi robot systems literature probably widely mas application domain 
agent case robot task instantiated spatial domain having dimensions 
multi robot domain real simulated 
interesting advantage simulated version difficulty task control designer 
advantages lower cost greater speed experiments 
simplest abstraction multi robot domain dimensional rectangular gridworld 
especially popular multiagent reinforcement learning literature hu wellman bowling veloso 
varieties extensions exist applications domains toroidal world advancing edge returns agent opposite side kok continuous coordinates sen circular arbitrarily shaped world especially useful multiagent localization mapping 
follows list relevant multi robot tasks studied literature 
terms agent robot interchangeably 
literature typically uses twodimensional tasks describe dimensional terminology 
extensions dimensions possible tasks 
navigation goal navigation task agent find way starting position final goal position avoiding harmful interference agents hu wellman bowling veloso 
goal position may fixed may change dynamically 
importance navigation task obvious task appears implicitly explicitly basic building block multi robot tasks 
area sweeping task involves navigating multi robot environment observation exploitation purposes 
kinds related area sweeping tasks exist retrieval retrieving objects environment matari 
coverage navigating surface environment possible instance purpose harvesting resources cleaning balch arkin 
exploration observing surface environment possible 
difference coverage agents need reach point environment bring sensor range 
note retrieval implicitly involves initial objects grouped permanent objects scattered exploration component 
coverage exploration continuous variants agents need minimize time elapsed consecutive passes observations point environment 
multi target observation multi target observation task extension exploration task goal agents maintain sensor range group targets motion 
object transportation object transportation task name implies requires relocation set objects final positions configuration 
times method moving object pushing 
task especially useful studying coordination mass 
robotic teams properties objects may exceed transportation capabilities agent requiring agents coordinate order bring objective matari 
robotic soccer robotic soccer popular test bed multiagent learning coordination approaches simulated real variants stone veloso 
reason combines tasks possibly adding things limited communication leading necessity selectivity exchanging messages 
involves instance object retrieval transportation intercepting ball leading goal multi target observation keeping adversary team observation advanced version coverage task strategic dynamic placement players field 
lately robotic soccer competitions complemented rescue competition objective team robots salvage victims simulated disaster areas 
attack pursuit attack pursuit tasks stem research military field involve destruction capture static moving enemy targets robotic teams 
type task involves special case foraging objects attacked discovery 
offensive tasks may conceivably include phase stealth scouting multi target observation main component 
example task helicopter attack domain tambe 
pursuit domain predator agents gridworld capture moving prey agent converging kok 
domain popular type gridworld task 
team predators contains predators typically single prey agent exists 
challenges posed robotic team domains include real domains typically include partial observability due limited number sensors limited range uncertain observations uncertain effect robot actions due noisy sensors imperfect actuators limited communication time constraints decision making process limited resources available robot control program 
disaster scenario large scale traffic incident main application multi robot systems robotic search rescue teams 
robotic airborne scouts may observe scene disaster rescue robots may advance dangerous area incident occurred 
fact rescue robots relief reach incident area long time instance mont blanc tunnel fire raised temperature tunnel rescue teams unable get inside hours 
skills useful robotic search rescue assistance teams applications domains navigation skills presumably needed incident area due agglomeration vehicles debris 
exploration foraging transportation skills retrieve victims return safe area medical assistance 
multi target observation airborne scouts monitor optimize activity robotic rescue teams 
soccer attack teams immediately applicable rescue task bear strong resemblance pose similar challenges task 
results research fields modifications applicable robotic search rescue teams 
distributed control distributed control approach separates control task lower level subtasks realized controllers distinct significant degree autonomy interconnected network 
plant controlled typically large complex 
control structure may completely distributed controllers interact directly plant include hierarchy 
case controllers upper levels hierarchy look coarser grain control task communicate coarse decisions lower level controllers carry 
multiagent context controllers agents plant environment see chapter 
view multiagent applications described chapter effect distributed control systems 
instance case object transportation robotic teams plant agents world containing objects transport control goal agents moving objects desired final positions configuration 
distributed control typically places agents fixed locations topology system 
representative relevant example distributed control traffic control system 
process case flow traffic highways ring roads streets control goal roughly speaking maintain traffic flow fast smooth safe possible 
observations agents include traffic density average vehicle speed time day control outputs include traffic signals dynamic route information panels 
agents may placed intersections allocated highway sectors 
examples distributed control applications power networks communication networks flexible manufacturing systems 
challenges posed distributed control include hard real time constraints control decisions need efficient reliable coordination mechanisms coherent joint control decisions taken need obtaining rough process models order achieve desired control performance 
find traffic control scenario critical aspects efficient traffic control system prevent possible traffic incidents avoiding dangerous situations traffic jams high vehicle velocities bad weather 

logistics incident avoided traffic control reroute flow traffic incident area size incident kept control access relief teams impeded 
logistics logistics process controlling flow usage resources system activities system properly supplied needed resources 
applied logistics agents manage resources 
differentiate closely related types logistics multiagent context nature controlled resources 
actual resource management resources passive tools agents 
examples load balancing schaerf scheduling crites barto routing boyan littman 
workflow management resources agents 
decompose task mas smaller subtasks synchronize subtasks parker 
decomposition subtasks workflow management reduces simpler problem called task allocation 
typically schedules resource allocation computed enforced central authority 
mas offer robust alternative distributing resource management process agents 
approaches cited learning scheduling agents require complete information problem 
challenges posed logistics domain multiagent systems include highly dimensional search spaces constraints need efficient multiagent coordination mechanism partial agent solutions form coherent combined mas solution incomplete information available agents order develop solution 
mas control partially completely flow resources ambulances fire trucks incident area scenario 
workflow management sequence synchronize activity 
automated information systems information system collects processes transmits disseminates data 
agent automated information system agents controlling stages 
view automated information system special case distributed control system controlled process flow data information system components 
essential characteristic information system extensive communication 
applications automated information systems include purely informational systems database access electronic commerce 
interested physical applications information systems smart sensor networks lewis 
applications domains sensing equipment smart sensor network collects data performs functions initial stages data processing decision making order activate alarms self diagnosis communication coordination nodes 
functions common goal sensor network lewis 
sensor network seen common goal cooperative communicative multiagent system 
challenges posed information systems include singh huhns coping open unpredictable nature environment adding removing components part normal functioning system handling unusually high degree agent heterogeneity 
smart sensor networks traffic incident scenario monitor traffic infrastructure notably sensitive areas tunnels 
smart sensor network possibly working tandem traffic control system perform functions collecting supplying data systems traffic control system 
line reaction incident alarm relief services medical fire assistance take immediate countermeasures activating smoke ventilation shafts 
collaborating traffic control system quickly direct unaffected vehicles incident area 
concluding remarks reviewed chapter relevant applications multiagent learning coordination context traffic incident scenario multirobot teams distributed control logistics automated information systems 
applications useful 
auctions resource management application games provide valuable insight 
examples relevant coordinated cooperative teams focus 
unified view applications integrated single distributed system managing monitoring disaster relief effort 
traffic control data supplied smart sensor network monitor regulate traffic 
incident occurs components collaborate rerouting traffic area simultaneously alerting relief services 
robotic human search rescue teams dispatched area function autonomously robotic team skills 
tasks serviced areas assigned automated resource workflow management system 

concluding remarks keeping realistic target integrated components system 
relevant applications foreseen point excluded 
immediate research avenue encountered literature extensions gridworld dimensions 
extension useful simulating robotic agents autonomous underwater vehicles unmanned air vehicles ground robots uneven surfaces 
chapter reviewed frameworks methods learning coordination multiagent systems 
chapter state main topics identify set useful research questions directions 
reinforcement learning prevalent learning technique mas 
field multiagent rl reached maturity 
important problems lack widely accepted problem statement gap theory practical applications 
theoretical methods strongly influenced game theoretical stateless solutions meaning unclear multiple state delayed reward ma rl setting 
theoretical methods strong restrictive assumptions difficult justify practice 
hand practical applications heuristic nature difficult analyze 
statement applies reviewed multiagent learning techniques 
coordination essential part learning coordination learned agents 
reviewed coordination techniques look coordination stage wise activity similarly ma rl methods 
consequence agent modeling methods consider agents stateless reactive entities 
opinion order understand multiagent learning relationship coordination essential dynamic nature agents environment taken account 
involves considering rich state spaces state evolution time environment agents 
believe dynamic nature effects placing multiple dynamic agents dynamic environment necessarily daunting difficulty overcome prevailing view literature 
looking problem perspective provide insight new solutions agents fact part dynamic multiagent system trying circumvent 
issue stated succinctly research question dynamic agent best fact part dynamic multiagent system 
suggested research directions deriving question 
dynamic black grey box agent models neural networks kalman particle filters section 
similarly observers estimate state partially measurable observable environment section 
research avenue context control system theoretic methods analyzing properties learning coordination techniques stability robustness changes environment 
relevant approaches area adaptive distributed control large scale systems 
way mas context explored mas learning literature teaching imitation 
investigation suitability stage wise learning coordination methods dmas related research question 
research question arises controversy literature respect ma rl goal 
addition aspects mentioned literature reviewed section believe encountered learning goals biased optimality considering aspects agent behaviour 
results times terms asymptotic average performance concern agent transient short term performance 
desirable properties agent behaviour optimality include robustness disturbances changes environment 
approximate monotonicity agent performance 
consider approximate monotonicity exploration issue agent may accept temporary losses performance order gain knowledge problem 
stress temporary losses bounded 
literature concerned aspect 
satisfying lower bounds performance 
brief suitable multiagent learning goal 
include performance requirements optimality 
mentioned section important difficulty ma rl explosion state space number agents 
observed agents take consideration entire state world actions agents 
concluded learning focused interesting parts state space local possible 
addition reducing computational requirements selective focus speed learning due smaller size search space 
similarly coordination restricted regions state space necessary section 
aspects question exist general conditions determine learning agent perform focusing restricted region state action space 
exist 
conditions exist give different answers different moments time agent learns world evolves 
implies agent adapt focus time bu 
focusing restricted regions state space imply cases agent ignores part sensory input 
conditions analyzed entire task span way determine agent perform limited sensory input determine minimal set sensors agent endowed order perform task 
related research question 
research agenda agent effectively incomplete uncertain information 
times focusing restricted regions state space lead incompleteness uncertainty information considered agent 
incompleteness uncertainty natively system due limited noisy sensors 
especially interested issue context modeling agent wishes construct models environment agents 
literature typically takes approaches uncertainty ignored model output considered true value estimated variable bayesian reasoning framework model outputs distribution possible variable values 
ignoring uncertainty dangerous thing 
hand bayesian reasoning highly computationally intensive 
intermediate methods investigated models addition estimates output confidence levels estimates 
methods online construction models conveying limited imperfect prior knowledge agents imperfect models 
need framework determining adaptation static learning necessary problem general conditions exist learning problem determine adaptation learning processes agents necessary 
exist 
adaptive learning restated static learning see section answers question determine problem learning necessary static agent behaviour suffice 
research agenda conclude enumerating research questions identified order relevance 
general conditions exist learning problem determine adaptation learning processes agents necessary 
exist 

suitable multiagent learning goal 
include performance requirements optimality 

dynamic agent best fact part dynamic multiagent system 

exist general conditions determine learning agent perform focusing restricted region state action space 
exist 

agent effectively incomplete uncertain information 
bibliography ba sar 

equilibrium theory decision making multiple probabilistic models 
ieee transactions automatic control 
baird 

residual algorithms reinforcement learning function approximation 
proceedings twelfth international conference machine learning icml pages tahoe city 
baird moore 

gradient descent general reinforcement learning 
advances neural information processing systems nips pages denver 
balch 

hierarchic social entropy information theoretic measure robot group diversity 
autonomous robots 
balch arkin 

communication reactive multiagent robotic systems 
autonomous robots 
banerjee peng 

adaptive policy gradient multiagent learning 
proceedings nd international joint conference autonomous agents multiagent systems aamas pages melbourne australia 


learning fuzzy classifier systems multi agent coordination 
information sciences 
booker 

classifier systems learn internal world models 
machine learning 
boutilier 

planning learning coordination multiagent decision processes 
proceedings sixth conference theoretical aspects rationality knowledge tark pages de netherlands 
boutilier 

sequential optimality coordination multiagent systems 
proceedings th international joint conference artificial intelligence ijcai pages stockholm sweden 
bowling 

convergence regret multiagent learning 
advances neural information processing systems nips pages vancouver canada 
bowling veloso 

multiagent learning variable learning rate 
artificial intelligence 
bibliography boyan littman 

packet routing dynamically changing networks reinforcement learning approach 
advances neural information processing systems neural information processing systems nips pages denver colorado usa 
bu de babu ska 

multiagent reinforcement learning adaptive state focus 
proceedings th belgian dutch conference artificial intelligence pages brussels belgium 
boutilier 

coordination multiagent reinforcement learning bayesian approach 
proceedings nd international joint conference autonomous agents multiagent systems aamas pages melbourne australia 
clouse 

learning automated training agent 
working notes workshop agents learn agents twelfth international conference machine learning icml tahoe city 
conitzer sandholm 

general multiagent learning algorithm converges self play learns best response stationary opponents 
proceedings twentieth international conference machine learning icml pages washington 
crites barto 

elevator group control multiple reinforcement learning agents 
machine learning 
de jong 

accumulative exploration method reinforcement learning 
notes workshop multiagent learning th national conference artificial intelligence aaai providence rhode island 
dorigo bersini 

comparison learning classifier systems 
animals animats 
proceedings rd international conference simulation adaptive behavior sab pages brighton united kingdom 
ventura 

dynamic joint action perception learning agents 
proceedings international conference machine learning applications pages los angeles 
greenwald hall 

correlated learning 
proceedings twentieth international conference machine learning icml pages washington 
guestrin lagoudakis parr 

coordinated reinforcement learning 
proceedings nineteenth international conference machine learning icml pages sydney australia 
haynes sen 

evolving behavioral strategies predators prey 
wei sen editors adaptation learning multi agent systems pages 
springer verlag 
haynes wainwright sen 

strongly typed genetic programming evolving cooperation strategies 
proceedings th international conference genetic algorithms icga pages pittsburgh 
bibliography hu wellman 

learning agents dynamic multiagent system 
journal cognitive systems research 
hu wellman 

nash learning general sum stochastic games 
journal machine learning research 
jung nair tambe marsella 

computational models multiagent coordination analysis extending distributed pomdp models 
proceedings nd international workshop formal approaches agent systems pages 
kaelbling littman moore 

reinforcement learning survey 
journal artificial intelligence research 
kok spaan vlassis 

non communicative multi robot coordination dynamic environment 
robotics autonomous systems 
kok bakker vlassis 

utile coordination learning interdependencies cooperative agents 
proceedings ieee symposium computational intelligence games cig pages colchester united kingdom 
kok vlassis 

sparse cooperative learning 
proceedings international conference machine learning icml pages banff canada 
nen 

asymmetric multiagent reinforcement learning 
proceedings ieee wic international conference intelligent agent technology iat pages halifax canada 
lauer riedmiller 

algorithm distributed reinforcement learning cooperative multi agent systems 
proceedings seventeenth international conference machine learning icml pages stanford university 
lewis 

smart environments technologies protocols applications chapter wireless sensor networks 
john wiley new york 
littman 

friend foe learning general sum games 
proceedings eighteenth international conference machine learning icml pages williams college 
littman 

value function reinforcement learning markov games 
journal cognitive systems research 
littman stone 

implicit negotiation repeated games 
proceedings th international workshop agent theories architectures languages atal pages seattle 
matari 

learning multi robot systems 
wei sen editors adaptation learning multi agent systems pages 
springer verlag 
matari 

learning social behavior 
robotics autonomous systems 
bibliography moore atkeson 

prioritized sweeping reinforcement learning data time 
machine learning 
parker 

alliance task oriented multi robot learning behavior systems 
advanced robotics 
special issue selected papers international conference intelligent robots systems 
peng williams 

incremental multi step learning 
machine learning 
plaza 

cooperative multiagent learning 
alonso kudenko kazakov editors adaptive agents multi agent systems pages 
springer 
powers shoham 

new criteria new algorithm learning multiagent systems 
advances neural information processing systems nips pages vancouver canada 
prasad lesser lander 

learning organizational oles negotiated search multiagent system 
international journal human computer studies 
price boutilier 

accelerating reinforcement learning implicit imitation 
journal artificial intelligence research 
pynadath tambe 

communicative multiagent team decision problem analyzing teamwork theories models 
journal artificial intelligence research 
ricci omicini 

environment coordination coordination artifacts 
weyns parunak michel editors environments multiagent systems pages 
springer verlag 
revised selected invited papers st workshop environments multiagent systems mas new york 
russell norvig 

artificial intelligence modern approach 
prentice hall englewood cliffs nd edition 
schaerf shoham tennenholtz 

adaptive load balancing study multi agent learning 
journal artificial intelligence research 
sen hale 

learning coordinate sharing information 
proceedings th national conference artificial intelligence aaai pages seattle 
sen weiss 

learning multiagent systems 
weiss editor multiagent systems modern approach distributed artificial intelligence chapter pages 
mit press 
shoham powers 

multi agent reinforcement learning critical survey 
technical report computer science dept stanford university california 
bibliography singh huhns 

challenges machine learning cooperative information systems 
wei editor distributed artificial intelligence meets machine learning learning multi agent environments pages 
springer 
selected papers ecai workshop budapest hungary icmas workshop kyoto japan 
singh jaakkola littman ri 

convergence results single step policy reinforcement learning algorithms 
machine learning 
singh kearns mansour 

nash convergence gradient dynamics general sum games 
proceedings th conference uncertainty artificial intelligence uai pages san francisco 
spaan vlassis groen 

high level coordination agents multiagent markov decision processes roles 
workshop cooperative robotics ieee rsj international conference intelligent robots systems iros pages lausanne switzerland 
stone veloso 

layered approach learning client behaviors robocup soccer server 
applied artificial intelligence 
stone veloso 

task decomposition dynamic role assignment low bandwidth communication real time strategic teamwork 
artificial intelligence 
stone veloso 

multiagent systems survey machine learning perspective 
autonomous robots 
hayashi 

multiagent reinforcement learning algorithm extended optimal response 
proceedings st international joint conference autonomous agents multiagent systems aamas pages bologna italy 
sutton 

integrated modeling control reinforcement learning dynamic programming 
advances neural information processing systems neural information processing systems conference nips pages denver colorado usa 
sutton barto 

reinforcement learning 
mit press cambridge 
de jong 

evolutionary multi agent systems 
proceedings th international conference parallel problem solving nature ppsn viii pages birmingham united kingdom 
tambe 

flexible teamwork 
journal artificial intelligence research 
tan 

multi agent reinforcement learning independent vs cooperative agents 
proceedings tenth international conference machine learning icml pages amherst 
bibliography thrun 

role exploration learning control 
white editors handbook intelligent control neural fuzzy adaptive approaches 
van nostrand reinhold 


robot awareness cooperative mobile robot learning 
autonomous robots 
vlassis 

concise multiagent systems distributed ai 
technical report university amsterdam netherlands 
url www science uva nl vlassis pdf 
walker wooldridge 

understanding emergence conventions multi agent systems 
proceedings st international conference multi agent systems icmas pages san francisco 
watkins dayan 

technical note learning 
machine learning 
whitehead lin 

reinforcement learning non markov decision processes 
artificial intelligence 

