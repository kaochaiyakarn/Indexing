volume number pages hierarchical text categorization fuzzy relational thesaurus jae dong yang sun lee bang text categorization classi cation assign text document appropriate category prede ned set categories 
new approach text categorization means fuzzy relational thesaurus frt 
frt category system stores maintains adaptive local dictionary category 
goal approach develop reliable text categorization method certain subject domain expand initial frt automatically added terms obtaining incrementally de ned knowledge base domain 
implemented categorization algorithm compared hierarchical classi ers 
experimental results shown algorithm outperforms rivals document corpora investigated 
keywords text mining knowledge base management multi level categorization hierarchical text categorization ams subject classi cation 
advent data warehouses importance text mining increasing decade 
signi cant sub eld text mining text document categorization aims automatic classi cation electronic documents 
text categorization classi cation assign document appropriate category ies called topic prede ned set categories 
traditionally document categorization performed manually 
number documents increases task longer amenable manual categorization requiring vast amount time cost 
lead numerous researches automatic document classi cation 
originally research categorization addressed binary problem document relevant category 
real world situation great variety di erent sources categories usually poses mainly done rst author visiting supported national university korea 
funded hungarian scienti research fund 
hungarian ministry education 

yang bang multi class classi cation problem document belongs exactly category selected prede ned set :10.1.1.42.7488:10.1.1.109.2516:10.1.1.54.6608:10.1.1.11.6124
general case multi label problem document classi ed category 
binary multi class problems investigated extensively multi label problems received little attention 
assign documents categories text categorization methods usually employ dictionaries consisting words extracted training documents 
assignment frequencies occurrence dictionary words document 
conventional methods employ large global dictionary local dictionaries category pooled local dictionary method adopts combined dictionary :10.1.1.109.2516:10.1.1.39.3129:10.1.1.11.6124
uses local dictionaries category incrementally expanded global dictionary training 
number topics larger multi class face problem complexity may incur rapid increase time storage compromise categorized subject domain 
common way manage complexity text exception 
internet directories large online databases organized hierarchies see yahoo ibm patent database real world applications pose problems multilevel category classi cation sorting mails les folder hierarchies structured search browsing text categorization topic hierarchies called taxonomies particular label classi cation problem 
document belonging topic taxonomy belongs parent topics topic path 
consequence categories document subsequently determined level going downward taxonomy 
feature saves time considerably time select best category 
having selected topic certain level hierarchy considered prospective categories level 
level taxonomy children node search method described reduces number considered categories 
describe multilevel text categorizer fuzzy relational thesaurus frt 
thesaurus information retrieval system irs considered knowledge base represents conceptual model certain subject domain 
fuzzy thesauri concepts usually organized hierarchy connected di erent kinds weighted relations 
approach frt serves implementation topic hierarchies 
call concepts frt topics categories emphasizing link concept hierarchy frt hierarchy taxonomy 
frt stores maintains local dictionaries consisting descriptive terms category 
terms words ngrams sequence words 
local dictionaries simply descriptors incrementally built training frt categorization 
exemplify classi cation problem terminology approach consider general hierarchy considered acyclic digraph restrict somewhat de nition see section 
www yahoo com www ibm com patents hierarchical text categorization fuzzy relational thesaurus subject domain electronic appliances 
part taxonomy domain depicted 
consider document classi ed topic laser printer 
topic path document printer laser printer 
descriptive terms copier occurrences fax ram printer scanner modem 
document classi ed descriptive terms categories 
initially size descriptors small typically contains topic name determined category may incorrect 
add typical terms categories selected training documents descriptors order improve ectiveness classi er 
details described section 
root level electronic appliances top level topics level office communication appliances oca computer devices cpd computer components cc level level copier fax pda phone monitor printer scanner mouse keyboard main memory cpu multimedia kit communication kit mono copier color copier pda phone telephone mobile phone dot printer laser printer ink jet printer mobile printer ram rom router modem lan card fig 

part taxonomy electronic appliances 
subject domain name denoted light gray toplevel topics darker gray boxes 
yang bang aim twofold primarily develop hierarchical text categorization algorithm hierarchical structured frt performs 
secondly want expand frt created manually semi automatically trained descriptors automatic way enhance descriptive power knowledge base 
obviously terms descriptors suitable expand frt task necessitates ltering descriptor elements order keep knowledge base consistent 
er implementation option user maintaining frt domain expert supervise modify terms added permanently frt re ecting view subject domain best 
concentrate categorization task 
organized follows 
section reviews related works text categorization 
section main part frt application categorization described details 
section shows experimental results follows section 
related works numerous statistical classi cation machine learning techniques applied text categorization 
include nearest neighbor classi ers knn regression models voted classi cation bayesian classi ers decision trees support vector machines svm information theoretic approaches distributional clustering neural networks :10.1.1.109.2516:10.1.1.11.6124
comparative studies see 
usually techniques compared standardized collection documents reuters corpus 
version knn svm voted classi cation provide best results achieving percentage break points results may somewhat vary di erent authors 
greatest break point attained weiss 
method uses decision trees induced means adaptive resampling algorithm pooled local dictionaries 
determine category document weiss approach applies voting multiple decision trees 
hand hierarchical text categorization emerged topic text mining 
result koller sahami hierarchical clustering :10.1.1.21.988
authors focused reduction local dictionaries called feature set aimed minimizing number terms discriminate categories :10.1.1.21.988
bayesian classi er allowed dependencies features 
results experimented small subsets reuters collection see section tables shows hierarchical classi ers outperform ones number features small 
approach criticized show improvement larger dictionaries domains established large dictionary sizes perform best :10.1.1.14.5443:10.1.1.11.6124:10.1.1.14.1043
alternative approaches hierarchical text categorization combined works feature subset selection 
feature subset selection improved classication accuracy reduced measurement cost storage computational overhead hierarchical text categorization fuzzy relational thesaurus nding best subset features :10.1.1.32.5484
examples taper employs taxonomy classi es text statistical pattern recognition techniques 
nds feature subset fisher discriminant 
simpli ed assumption koller sahami authors na bayesian classi er combined feature subset grams 
mccallum na classi er :10.1.1.14.5443
adopted established statistical technique called shrinkage improve parameter estimates class probabilities taxonomy 
simple fast solution proposed tfidf classi er tf idf weighting see applied hierarchical classi cation :10.1.1.32.5484
applied greedy algorithm level hierarchy resulted log time documents 
referred results hierarchical classi er showed superior performance ones 
straightforward comparison methods di culty due di erent text corpora applied 
return problem section 
improved training documents removal function words stemming term indexing creation expansion sets classifier correct misclassified performance check measure acceptable category frt descriptors penalize misleading terms topic descriptors fig 

training algorithm 
insert terms descriptors expansion sets yang bang 
proposed method core idea frt categorization training algorithm adds new terms descriptors certain weights modi es weight terms necessary 
start small frt manually created domain expert see describes topic hierarchy documents categorized 
frt contain topic hierarchy subgraph detailed possible empty descriptors may categories 
initial descriptors typically just terms 
brie describe training procedure 
primary descriptors topics frt categorize document 
procedure fails due small size descriptors insert new terms descriptor correct category expansion set document 
expansion set frequency ordered set terms document belonging category 
size controlled threshold parameters 
frt determines incorrect topic terms descriptor generated choice penalized weakening weights 
training algorithm executed iterative way ends performance improved signi cantly 
see block diagram overview details subsection training algorithm 
test documents classi er works pass omitting feedback cycle 
rest section organized follows 
subsection describes vector space model notation terminology 
subsection focuses descriptors expansion sets 
subsection presents core algorithm document classi cation frt nally subsection includes training frt detail 

de nitions 
taxonomy xed nite set categories organized topic hierarchy 
refer hierarchy disjoint acyclic digraphs connected root 
root represent category take categorization 
acyclic digraph describes topics top level category 
parent parents subgraph due disjointness condition 
see example multiple 
document classi ed leaf category hierarchy 
assume parent category owns documents child categories document belongs topic path containing nodes representing categories leaf root 
allow multiple topics deeper hierarchy strong relations especially taxonomy describes relative small subject domain example 
example topic cassette mp player belongs categories mp player cassette 
topic assigned level depth taxonomy de ned hierarchical text categorization fuzzy relational thesaurus recursively function level level root min parent ofc level depth taxonomy de ned level deepest category top level frt elements topics take part categorization top level frt elements categorization categories topic path topic path depth max level root frt top level topics fig 

taxonomy vs frt hierarchy gray area indicates subset frt hierarchy part categorization 
topic hierarchy just described ers straightforward way connect categorization purpose fuzzy relational thesauri having hierarchically organized structure 
relations frt elements weighted real number interval alternatively relations fuzzy 
adapt frt described details text categorization disregarding various type relationships uniquely broader narrower concept application topic relationship 
mentioned serves implementation topic hierarchy 
cases 
frt created solely implementation basis categorization purpose graph identical taxonomy 
existing frt categorization require frt acyclic digraph contain taxonomy isomorphic level invariant subgraph see 
case subgraph frt takes part categorization 
descriptors nodes cleared augmented training documents belong 
due structural identity taxonomy frt element ofthe frt hierarchy category 
yang bang cassette cassette audio cassette mp player mp player mp cd player fig 

example multiple electronic appliances taxonomy 
category cassette mp player linked mp player cassette 

vector space model set text documents dan arbitrary element ofd 
general documents pre classi ed categories case leaf categories 
di erentiate training test documents training documents inductively construct classi er 
test documents test performance classi er 
test documents participate construction classi er way 
texts directly interpreted classi er 
indexing procedure maps text compact representation content needs uniformly applied documents training test 
choose words meaningful units representing text grams increases dramatically storage need model reported sophisticated representation simple words increase ectiveness signi cantly :10.1.1.161.6020
research works vector space model document dj represented term weights dj jt jj set terms occurs ones training documents wkj represents relevance kth term characterization document indexing documents function words articles prepositions conjunctions removed stemming grouping words share morphological root performed term set called universal dictionary 
certain settings method occurrence vector char hierarchical text categorization fuzzy relational thesaurus document dj occur dj ho jt jji determines number occurrence kth term document dj 
numerous possible weighting schemes literature determine values term weights wkj 
best sophisticated method entropy weighting apply popular tf idf weighting de nes wkj proportion number occurrence kth term document inverse proportion number documents collection terms occurs nk wkj log nk term vectors normalized training 
document dj classi ed leaf category belongs parent categories belongs categories root topic path 
formally topic dj fc cq ci level cj jg determines set topics dj belongs topic path deepest highest 
note cq top level category subgraph dj classi ed disregard root 
multiple allowed happen topic path contains categories intermediate level 

descriptors expansion sets frt classi cation works matching vectors representing documents categories 
represent categories analogously documents 
descriptor term weights descr ci jt ji ci weights set training 
weight initial descriptors domain expert 
weights initialized 
descriptor category interpreted prototype document belonging 
usually initial number descriptive terms zero su cient cient classi er 
order ll descriptor training create expansion sets phase 
training categorization fails terms expansion sets added descriptors increase classi cation 
ciency create training document topic topic expansion set ed frequency ordered set terms document characterizing category term assigned cumulated value yang bang sum appropriate term weights nearest neighbors belonging category note may greater number documents documents considered 
selection mechanism balance number considered documents topic contains large number training documents 
dj contains elements descending order cumulated term weights dj nearest neighbors calculated wij pi wij wi wij value assigned ith term jtj 
set contains index nearest neighbors document dj category jlj dj pin jtj im pin pim jtj mg distance documents calculated cosine measure 
note concept expansion set di erent descriptor 
assigned category document assigned category 
second ordered set terms document set terms characterizing entire topic 
topic descriptors augmented training expansion sets 

classi cation means frt classifying document means frt term vector representing compared topic descriptors 
vector matched set descriptors result classi er selects normally unique category 
classi cation method works downward topic hierarchy level level 
determines best top level categories 
children categories considered selected 
considered categories siblings linked winner category previous level 
greedy type algorithm ends leaf category 
mccallum criticized greedy topic selection method requires high accuracy internal nonleaf nodes :10.1.1.14.5443
experiments see section 
algorithm performs works plan consider algorithms node reject document send back upward hierarchy re classi cation 
assume select categories arbitrary stage classi cation document dj ck 
calculate similarity term vector dj topic descriptors descr descr ck select category gives highest similarity measure 
carried experiences similarity measures 
simplest binary comparison calculates number terms hierarchical text categorization fuzzy relational thesaurus mutually occurs descriptor document dj descr ci jt sign wkj sign signum function 

second unnormalized cosine similarity measure calculates value sum products document descriptor term weights dj descr ci jt wkj 
third similarity measure takes account occurrence vector dj descr ci jt wkj occur dj depends dj operand set change 
measure strongly emphasizes multiple occurring terms document 
matching vectors document category descriptor weights control selection best category minimum conformity parameter minconf greedy selection algorithm continues dj descr minconf satis ed arbitrary similarity measure best category level 
example 
show simpli ed example category selection method similarity measures top level taxonomy introduced 
document descr cpd descr oca descr cc obtain cpd oca cc computer components cc selected 
get cpd oca cc computer peripheral devices cpd selected 
occur obtain cpd oca cc ce communication appliances oca chosen 
explanatory example weight values real setting shows selection similarity method crucial concerning ectiveness classi cation 
yang bang 
updating knowledge base training algorithm order improve ectiveness classi cation apply supervised iterative learning check correctness selected categories training documents necessary modify term weights category descriptors 
modify descriptor term weights cases 
case frt unable nd correct category document alleviate type error raise weight terms descr characterizes best link terms expansion set see section 
take pre select level set set contains rst terms weight equal higher adjustable parameters 
descriptor term weights corresponding selected terms set pi de ned 
expansion set updated removing pre parameters rede ned training cycle 
example 
rst training cycle level set expansion set assigned sample document category laser printer consists terms max print cm dpi 
second training cycle page comm postscript plain stemmed words 

case category determined frt incorrect 
handle type error modifying certain descriptor term weights category term weights having nonzero value term vector document multiplied factor adjustable parameter 
value controls strength penalization 
size descriptors grows terms nonzero weights 
order avoid proliferation descriptor term weights zero certain threshold 
training cycle repeated maximal iteration nished performance classi er improve signi cantly 
measure ectiveness classi ers training documents 
setting maximum variance value typically training actual drops best wheref best best achieved far training 

implementation experimental results 
document collections standard document corpus particularly multi class multilabel text classi cation test decided initial phase project collect web documents domain electronic appliances ea 
performed document collection available online www mft hu publications zip 
hierarchical text categorization fuzzy relational thesaurus tests corpora see 
collected documents 
jt terms size global dictionary stemming removal function words 
frt created semi automatic thesaurus construction software 
depth taxonomy depth cea 
collected documents classi ed top level topics audio computer computer components computer peripheral device house hold appliances ce communications appliances 
number topics subsequent levels 
documents classi ed lowest level categories average category 
category document 
documents distributed evenly subgraphs top level categories computer components topic documents 
applied fold cross validation approach divide document corpus training test documents 
table 
description hier data set reuters :10.1.1.21.988
data set size major topics minor topics training testing grain business corn wheat money ects dlr interest crude oil nat gas ship total compare algorithm hierarchical classi ers tested ectiveness corpora tv closed data courtesy chuang subsets reuters database :10.1.1.32.5484
case topic hierarchy ready case taxonomies introduced :10.1.1.21.988
reuters collection created benchmark database hierarchical classi cation taxonomies deals subsets categories entire collection organized simple hierarchies depicted tables 
get koller sahami original setting training testing documents experiments documents speci ed categories modapte split :10.1.1.21.988
case document attached leaf category total number documents sum training testing documents 

performance evaluation measure microaveraged recall precision test ectiveness classi er reuters collection may freely downloaded www com resources reuters 
yang bang table 
description hier data set reuters :10.1.1.21.988
data set size major topics minor topics training testing business acq earn 
oil business total substituted bonds acq category bonds removed reuters 

results results achieved ea collection shown table 
applied similarity measures de ned subsection provide close similar results 
simplicity carried experiments simple cosine similarity measure 
applied fold cross validation approach values separate training test documents 
obviously results improve considerably value increases 
average number documents category low categories document certain settings categories training documents 
re ected individual test results lowest measure averaged test results highest 
parameters xed minconf 
tested training classi cation ciency terms required time experiences performed ghz mb ram pc dependency size global dictionary jt modi ed size global dictionary removing frequent terms collection :10.1.1.161.6020
total number terms reduced terms occurring collection removed 
generally size global dictionary disregarding terms satisfy dj oij jt dj integer threshold parameter typically range ith term total occurrence th part cumulated total occurrences terms 
obviously number terms uences average size document term weight vector zeros stored speed classi cation 
table shows required time fold cross validation approach function size global dictionary 
conclude size global dictionary ect signi cantly performance kept reasonable level 
table shows results obtained tv closed data set 
data set consists categories training test documents collection 
leaf categories average number oij hierarchical text categorization fuzzy relational thesaurus training documents category 
main reasons high ectiveness small size topic hierarchy better distribution training documents 
time requirement training testing training loops test sec regardless size global dictionary 
minconf parameter set remain unchanged 
table 
categorization results ea document set 
refers appropriate value fold cross validation approach 
measures averaged tests case precision recall values range worst best result 
training set test set table 
size global dictionary vs elapsed time includes training runs tests average document vector size average test results case fold cross validation approach parameters xed 
size average number elapsed aver 
dictionary term document time training cycles table gives results achieved hier hier taxonomies reuters corpus 
indicated corresponding results despite fact experiments achieved di erent settings get original document setting authors documents pre classi ed categories accuracy measure ectiveness method criticized authors due insensitivity page :10.1.1.21.988:10.1.1.109.2516
results due small number categories large number yang bang documents 
minconf parameter set remain unchanged 
table 
results tv closed data set :10.1.1.32.5484
training test method depth depth chuang frt table :10.1.1.32.5484
results hier hier subsets reuters collection 
taxonomy best result frt accuracy hier hier :10.1.1.21.988
works proposed new method text categorization support classi cation task 
showed ectiveness algorithm di erent document corpora topic hierarchies di erent sizes 
main advantage algorithm builds classi er gradually supervised iterative learning method feedback intermediate experiments method training 
intend extend experiments algorithm larger document corpora having documents near 
received february 
aas text categorisation survey 
nr norwegian computing center 
apte damerau weiss automated learning decision rules text categorization :10.1.1.39.3129
acm trans 
information systems 
baker mccallum distributional clustering words text classication 
proc 
th annual internat 
acm sigir conference research development information retrieval sigir melbourne australia pp 

chakrabarti dom agrawal raghavan scalable feature selection classi cation signature generation organizing large text databases hierarchical topic taxonomies 
vldb journal 
hierarchical text categorization fuzzy relational thesaurus choi park yang lee object approach domain speci thesauri semiautomatic thesaurus construction browsing 
technical report tr dept computer science national university 
cs ac kr publication html 
chuang yang fast algorithm hierarchical text classi cation :10.1.1.32.5484
proc 
nd internat 
conference data warehousing knowledge discovery london greenwich uk pp 

dagan karov roth mistake driven learning text categorization 
proc 
second conference empirical methods natural language processing cardie weischedel eds association computational linguistics somerset nj pp 

dumais improving retrieval information external sources 
behaviour research methods instruments computers 
dumais platt heckerman sahami inductive learning algorithms representations text categorization :10.1.1.161.6020
proc 
th acm internat 
conference information knowledge management cikm bethesda md pp 

fisher knowledge acquisition incremental conceptual clustering 
machine learning 
joachims text categorization support vector machines learning relevant features :10.1.1.11.6124
technical report university dortmund dept informatics dortmund germany 
koller sahami hierarchically classifying documents words :10.1.1.21.988
international conference machine learning volume san mateo ca morgan kaufmann 
korfhage information storage retrieval 
wiley newyork 
larsen yager fuzzy relational thesaurus classi problem solving information retrieval expert systems 
ieee trans 
systems man cybernetics 
lewis ringuette comparison learning algorithms text classi cation 
proc 
third annual symposium document analysis information retrieval pp 

mccallum rosenfeld mitchell ng improving text classi cation shrinkage hierarchy classes :10.1.1.14.5443
proceedings icml 
www cs cmu edu mccallum papers hier icml ps gz 
mitchell machine learning 
mcgraw hill newyork 
miyamoto fuzzy sets information retrieval cluster analysis 
number theory decision library system theory knowledge engineering problem solving 
kluwer dordrecht 
mladenic grobelnik feature selection classi cation text hierarchy 
working notes learning web conference automated learning discovery 
nigam mccallum thrun mitchell learning classify text labeled unlabeled documents 
proc 
th national conference arti cal intelligence aaai 
fuzzy set theoretical approach document retrieval 
information processing management 
ruspini bonissone eds handbook fuzzy computation 
oxford university press institute physics publishing bristol philadelphia 
yang bang salton automatic text processing transformation analysis retrieval information computer 
wesley reading ma 
salton mcgill modern information retrieval 
mcgraw hill new york 
sebastiani machine learning automated text categorization 
acm computing surveys 
van rijsbergen information retrieval 
second edition 
butterworths london 
www dcs gla ac uk keith 
weiss apte damerau johnson oles goetz maximizing text mining performance 
ieee intelligent systems 
wiener pedersen weigend neural network approach spotting 
proc 
th annual symposium document analysis information retrieval pages 
yang evaluation statistical approaches text categorization :10.1.1.109.2516
information retrieval 
citeseer nj nec com yang evaluation html 
dr department telecommunications media informatics budapest university technology economics budapest intelligent integrated systems japanese hungarian laboratory budapest 
hungary 
mail bme hu prof dr jae dong yang sun lee bang department computer science national university korea 
mails cs ac kr 
