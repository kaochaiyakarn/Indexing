planning acting partially observable stochastic domains leslie pack kaelbling computer science department brown university box providence ri usa michael littman duke university anthony cassandra mcc bring techniques operations research bear problem choosing optimal actions partially observable stochastic domains 
introducing theory markov decision processes mdps partially observable mdps pomdps 
outline novel algorithm solving pomdps line show cases nite memory controller extracted solution pomdp 
conclude discussion approach relates previous complexity nding exact solutions pomdps possibilities nding approximate solutions 
consider problem robot navigating large ce building 
robot move hallway intersection intersection local observations world 
actions completely reliable 
intends move stays goes far intends turn overshoots 
similar problems observation 
corridor looks corner junction looks junction 
error plagued robot navigate map corridors 
supported part nsf iri iri 
supported part 
get number supported part bellcore 
preprint submitted elsevier preprint december general robot remember history actions observations information knowledge underlying dynamics world map information maintain estimate location 
engineering applications follow approach methods kalman lter maintain running estimate robot spatial uncertainty expressed ellipsoid normal distribution cartesian space 
approach robot 
uncertainty may discrete certain north east corner fourth seventh admits chance fth oor 
uncertain estimate location robot decide actions take 
cases besu cient ignore uncertainty take actions appropriate location 
cases better robot take actions purpose gathering information searching landmark reading signs wall 
general take actions ful ll purposes simultaneously 
bring techniques operations research bear problem choosing optimal actions partially observable stochastic domains 
problems described modeled partially observable markov decision processes pomdps 
course interested problems robot navigation 
similar problems come factory process control oil exploration transportation logistics variety complex real world situations 
essentially planning problem complete correct model world dynamics reward structure nd optimal way behave 
arti cial intelligence ai literature deterministic version problem addressed adding knowledge preconditions traditional planning systems :10.1.1.57.3126
interested stochastic domains depart traditional ai planning model 
plans sequences actions may rarely execute expected take mappings situations actions specify agent behavior matter may happen 
cases may want full policy methods developing partial policies conditional plans completely observable domains subject current interest 
weakness methods described require states world represented compositional representations nets probabilistic operator descriptions 
served substrate devel opment complex cient representations :10.1.1.116.1003
section describes relation approach prior research detail 
important facet pomdp approach distinction drawn actions taken change state world actions taken gain information 
important general action types ect 
stopping ask questions may delay robot arrival goal spend extra energy moving forward may give robot information dead resulting crash 
pomdp perspective optimal performance involves akin value information calculation complex agent chooses actions amount information provide amount reward produce change state world 
intended contributions 
rst recapitulate operations research literature describe connection closely related ai 
second describe novel algorithmic approach solving pomdps exactly theory markov decision processes mdps pomdps 
outline novel algorithm solving pomdps line show cases nite memory controller extracted solution pomdp 
conclude brief discussion related approximation methods 
markov decision processes markov decision processes serve basis solving complex partially observable problems ultimately interested 
mdp model agent interacting synchronously world 
shown agent takes input state world generates output actions ect state world 
mdp framework assumed may great deal uncertainty ects agent actions uncertainty agent current state complete perfect perceptual abilities 
markov decision processes described depth variety texts just brie cover necessary background 
states world agent actions fig 

mdp models synchronous interaction agent world 
basic framework markov decision process described tuple hs ri nite set states world nite set actions 
state transition function giving world state agent action probability distribution world states write probability state agent starts state takes action reward function giving expected immediate reward gained agent action state write expected reward action state 
model state expected reward depend previous state action taken condition additional previous states transition probabilities expected rewards remain 
known markov property state reward time dependent state time action time fact mdps nite state action spaces 
algorithms describe section apply nite case context pomdps consider class mdps uncountably nite state spaces 
acting optimally agents act away maximize measure long run reward received 
framework nite horizon optimality agent act order maximize expected sum reward gets steps maximize reward received step model somewhat inconvenient rare appropriate known exactly 
prefer consider nite lifetime agent 
straightforward nite horizon discounted model sum rewards nite lifetime agent discount geometrically discount factor agent act optimize model rewards received earlier lifetime value agent nite lifetime considered discount factor ensures sum nite 
sum expected amount received decision terminate run step probability larger discount factor closer ect rewards current decision making 
discussions nite horizon optimality discount factor value equivalent simple nite horizon case described 
policy description behavior agent 
consider kinds policies stationary non stationary 
stationary policy situation action mapping speci es state action taken 
choice action depends state independent time step 
non stationary policy sequence situation action mappings indexed time 
policy choose action th step function current state nite horizon model optimal policy typically stationary way agent chooses actions step life generally going di erent way chooses long life ahead 
discounted model agent constant expected amount time remaining reason change action strategies stationary optimal policy 
policy evaluate long run value agent expects gain executing 
nite horizon case expected sum reward gained starting state executing nonstationary policy steps 
clearly step value just expected reward action speci ed nal element policy 
de ne inductively step value state executing non stationary policy immediate reward plus discounted expected value remaining steps 
evaluate consider possible rt resulting states likelihood occurrence step value policy 
nite horizon discounted case write expected discounted sum reward starting state executing policy recursively de ned value function policy unique simultaneous solution set linear equations equation state know compute value function policy 
need go opposite way compute greedy policy value function 
really sense nite horizon discounted case derive policy nite horizon wewould need sequence value functions 
value function greedy policy respect value function de ned argmax policy obtained step action maximizes expected immediate reward plus expected discounted value state measured optimal nite horizon policy agent step easy maximize nal reward 
argmax optimal policy th step de ned terms optimal step value function written simplicity asv argmax derived 
vt nite horizon discounted case initial state want execute policy maximizes 
howard showed exists stationary policy optimal starting state 
value function policy equations written de ned set max loop loop vt vt maxa loop vt table value iteration algorithm nite state space mdps 
unique solution 
optimal policy just greedy policy respect way understand nite horizon value function approach increasing discounted nite horizon 
horizon approaches nity approaches guaranteed occur discount factor tends wash details exactly happens agent life 
computing optimal policy methods nding optimal policies mdps 
section explore value iteration serve basis nding policies partially observable case 
value iteration proceeds computing sequence discounted optimal value functions shown table superscript omitted shall henceforth considering optimal value func tions 
auxiliary function step value starting state action continuing optimal step non stationary policy 
algorithm terminates maximum difference successive value functions known bellman error magnitude shown exists polynomial jsj jaj magnitude largest value greedy policy respect equal optimal nite horizon policy calculating bound advance running value iteration long result regarding bellman error magnitude order terminate near optimal policy 
jv value greedy policy respect di er state 
max jv case vt long near tighter bounds may span semi norm value function 
partial observability mdps compute optimal policy act simply executing current state happens agent longer able determine state currently complete reliability 
naive approach agent map observation directly action remembering past 
hallway navigation example amounts performing action location looks hardly promising approach 
somewhat better results obtained adding randomness agent behavior policy mapping observations probability distributions actions 
randomness ectively allows agent choose di erent actions di erent locations appearance increasing probability choose action practice deterministic observation action mappings prone getting trapped deterministic loops 
order behave truly ectively partially observable world necessary memory previous actions observations aid disambiguation states world 
pomdp framework provides systematic method doing just 
pomdp framework partially observable markov decision process described tuple hs oi describe markov decision process nite set observations agent experience world 
observation function gives action resulting state probability distribution possible observations write probability making observation agent landed state 
observation se world agent action fig 

pomdp agent decomposed state estimator se policy 
pomdp mdp agent unable observe current state 
observation action resulting state 
agent goal remains maximize expected discounted reward 
problem structure decompose problem controlling pomdp parts shown 
agent observations generates actions 
keeps internal belief state summarizes previous experience 
component labeled se state estimator responsible updating belief state action current observation previous belief state 
component labeled policy responsible generating actions time function agent belief state state world 
exactly belief state 
choice probable state world past experience 
plausible basis action cases su cient general 
order act ectively agent take account degree uncertainty 
lost confused appropriate take sensing actions asking directions reading map searching landmark 
pomdp framework actions explicitly distinguished informational properties described observation function 
choice belief states probability distributions states possible formulate equivalent model observation depends previous state addition resulting state complicates exposition adds expressive power model converted pomdp model described cost expanding state space 
fig 

simple pomdp illustrate belief state evolution 
world 
distributions encode agent subjective probability state world provide basis acting uncertainty 
furthermore comprise su cient statistic past history initial belief state agent agent current belief state properly computed additional data past actions observations supply information current state world 
means process belief states markov additional data past help increase agent expected reward 
illustrate evolution belief state simple example depicted algorithm computing belief states provided section 
states example goal state indicated star 
possible observations agent state goal state 
possible actions east west 
actions succeed probability fail movement opposite direction 
movement possible particular direction agent remains location 
assume agent initially equally non goal states 
initial belief state position belief vector corresponds state number 
agent takes action east observe goal new belief state takes action east observe goal probability mass right state notice long agent observe goal state non zero belief non goal states actions non zero probability failing 
computing belief states belief state probability distribution denote probability assigned world state belief state axioms probability require 
state estimator compute new belief state old belief state action observation new degree belief state obtained basic probability theory follows pr jo pr ojs pr ja pr oja pr ojs pr ja pr pr oja pr oja denominator pr oja treated normalizing factor independent ofs causes sum 
state estimation function se output new belief state state estimation component pomdp controller constructed quite simply model 
finding optimal policy policy component pomdp agent map current belief state action 
belief state su cient statistic optimal policy solution continuous space belief mdp 
de ned follows set belief states comprise state space set actions remains transition function de ned pr ja pr ja pr oja pr jb se reward function belief states constructed original reward function world states reward function may strange agent appears rewarded merely believing states 
state estimator constructed correct observation transition model world belief state represents true occupation probabilities states reward function represents true expected reward agent 
belief mdp optimal policy coupled correct state estimator give rise optimal behavior discounted sense original pomdp 
remaining problem solve mdp 
di cult solve continuous space mdps general case shall see section optimal value function belief mdp special properties exploited simplify problem 
value functions pomdps case discrete mdps compute optimal value function directly determine optimal policy 
section concentrates nding approximation optimal value function 
approach problem value iteration construct iteration optimal step discounted value function belief space 
policy trees agent step remaining take single action 
steps go take action observation take action depending previous observation 
general agent non stationary step policy represented policy tree shown 
tree depth speci es complete step policy 
top node determines rst action taken 
depending resulting observation arc followed node level determines action 
complete recipe steps conditional behavior 
expected discounted value gained executing policy tree 
depends true state world agent starts 
policy trees essentially equivalent decision trees decision theory represent sequential decision policy decision trees machine learning compactly represent single stage decision rule 
fig 

step policy tree 
steps go steps go steps go step go simplest case step policy tree single action 
value executing action state action speci ed top node policy tree generally ifp step policy tree vp expected value pr js pr oi voi oi voi step policy subtree associated observation top level step policy tree expected value computed rst expectation possible states considering value states 
value depends policy subtree executed depends observation 
take expectation respect possible observations value executing associated subtree starting state agent know exact state world able determine value executing policy tree belief state just expectation world states executing state vp vp useful exposition express compactly 
hv value executing policy tree possible belief state 
construct optimal step policy generally expected step discounted value bs fig 

optimal step value function upper surface value functions associated step policy trees 
necessary execute di erent policy trees di erent initial belief states 
nite set step policy trees 
max optimal step value starting belief state value executing best policy tree belief state 
de nition value function leads important geometric insights form 
policy tree induces value function linear upper surface collection functions 
piecewise linear convex 
illustrates property 
consider world states 
belief state consists vector non negative numbers hb sum 
constraint single number su cient describe belief state 
value function associated policy tree linear function shown gure line 
value functions policy trees similarly represented 
maximum pi point belief space giving upper surface drawn gure bold line 
world states belief state determined values simplex constraint requires individual values non negative sum 
belief space seen triangle space vertices 
value function associated single policy tree plane space optimal value function bowl shape composed planar facets atypical example shown possible bowl side degenerate single plane 
general pattern repeats higher dimensions di cult contemplate harder draw 
convexity optimal value function intuitive sense think value belief states 
states middle belief space high entropy agent uncertain real underlying state world 
belief states agent select fig 

value function dimensions 
actions appropriately tends gain long term reward 
belief states near corners simplex agent take actions appropriate current state world gain reward 
connection notion value information agent incur cost move low entropy state worthwhile value information di erence value states exceeds cost gaining information 
piecewise linear convex value function step policy trees itwas derived straightforward determine optimal policy execution th step 
optimal value function projected back belief space yielding partition polyhedral regions 
region single policy tree maximal entire region 
optimal action belief state region action root node policy tree furthermore entire policy tree executed point conditioning choice actions directly observations updating belief state necessarily cient way represent complex policy 
shows projection optimal value function policy partition dimensional example introduced intervals illustrated single policy tree executed maximize expected reward 
expected step discounted value fig 

optimal step policy determined projecting optimal value function back belief space 
pa expected step discounted value bs fig 

policy trees may totally dominated ignored 
value functions sets vectors possible principle possible policy tree represent optimal strategy point belief space contribute computation optimal value function 
luckily rarely case 
generally policy trees value functions totally dominated tied value functions associated policy trees 
shows situation value function associated policy completely dominated equal value function policy situation value function policy somewhat complicated completely dominated single value function completely dominated taken 
set policy trees possible de ne unique minimal subset represents value function 
call parsimonious representation value function say policy tree useful component parsimonious representation value function 
vector set vectors de ne region assume policy trees value function identical 
pb pc pd belief space dominates fb bg relatively easy linear program nd point exists determine region empty :10.1.1.53.7233
simplest pruning strategy described monahan test remove dominant 
cient pruning method proposed white described detail littman cassandra :10.1.1.53.7233
subtle technical details described 
step value iteration value function pomdp computed value iteration basic structure discrete mdp case 
new problem compute parsimonious representation parsimonious representation 
simplest algorithms solving problem attributed monahan works constructing large representation pruning 
stand set policy trees tree need store top level action vector values idea algorithm set useful step policy trees construct superset useful step policy trees 
step policy tree composed root node associated action subtrees step policy trees 
propose restrict choice subtrees step policy trees useful 
belief state choice policy subtree useful subtree state reason include non useful policy subtree 
time complexity single iteration algorithm divided parts generation pruning 
jj elements jaj di erent ways choose action possible lists length may set vt form subtrees 
value functions policy trees computed ciently subtrees 
pruning requires linear program element starting set policy trees add asymptotic complexity 
keeps parsimonious representations value functions step algorithm may necessary 
small go step generating size exponential sections brie outline existing algorithms attempt cient monahan 
witness algorithm complexity analysis 
witness algorithm improve complexity value iteration algorithm avoid generating generate elements directly 
able reach computation time iteration polynomial jsj jaj 
cheng smallwood sondik try avoid generating constructing vt directly :10.1.1.56.7115
algorithms worst case running times exponential problem parameters 
fact existence algorithm runs time polynomial jsj jaj settle long standing complexity theoretic question np rp pursue slightly di erent approach 
computing directly compute action set step policy trees action root 
compute union sets actions pruning described previous section 
witness algorithm method computing time polynomial jsj jaj jv jq speci cally run time polynomial size inputs outputs important intermediate result 
possible exponentially larger rarely case practice sense witness algorithm superior previous algorithms solving pomdps 
experiments indicate witness algorithm faster practice wide range problem sizes 
primary complexity theoretic di erence witness algorithm runs polynomial time number policy trees example problems cause algorithms construct directly run time exponential number policy trees means restrict problems jq polynomial resulting running time polynomial 
worth noting possible create families pomdps cheng algorithm solve polynomial time take witness exponential time solve problems small exponentially larger action de nition state estimator se step value function algorithm zhang inspired witness algorithm asymptotic complexity appears current fastest algorithm empirically problem 
fh ig loop foreach witness vt prune get vt vt table outer loop witness algorithm 
vt express qa recall value action belief state continuing optimally steps formally pr oja vt belief state resulting action observing belief state se 
value best action vt maxa qa 
arguments similar previous section show functions piecewise linear convex represented collections policy trees 
qa collection policy trees specify qa de ne unique minimal useful set policy trees function 
note policy trees needed represent function vt subset policy trees needed represent qa functions vt qa maximizing actions policy trees maximizing pooled sets policy trees 
code table outlines approach solving pomdps 
basic structure remains value iteration 
iteration algorithm representation optimal step value function 
value iteration loop separate functions action represented parsimonious sets policy trees returned calls witness value function previous iteration 
union sets forms representation optimal value function 
may extraneous policy trees combined set pruned yield useful set step policy trees witness inner loop basic structure witness algorithm follows 
nd minimal set policy trees representing consider functions time 
set policy trees initialized single policy tree action root best arbitrary new ok step policy trees fig 

constructing new policy 
belief state easy 
iteration ask belief state true value computed step lookahead di erent estimated value computed set 
call belief state witness sense testify fact set perfect representation qa 
note qa qa approximation underestimate true value function 
witness identi ed nd policy tree action root yield best value belief state 
construct tree nd observation step policy tree executed observation executing action happens agent belief state se execute step policy tree maximizes po 
tree built subtrees observation add new policy tree improve approximation 
process continues prove witness points exist current function perfect 
identifying witness nd witness points able construct evaluate alternative policy trees 
step policy tree observation step policy tree de ne new step policy tree agrees action subtrees observation new illustrates relationship new 
state witness theorem true function di ers approximate function pnew belief state new improvement policy trees far witness 
conversely trees improved replacing single subtree witness points 
proof theorem included appendix 
checking witness condition witness theorem requires search ano condition holds guarantee quadruple exists 
nite hope small checking combinations time consuming 
combination need search belief states test condition 
linear programming 
combination compute policy tree new described 
belief state policy tree pnew gives advantage policy tree new starting nd maximizes advantage policy trees algorithm far 
linear program table solves exactly problem 
variable minimum amount improvement ofp new policy tree set constraints restrict bound di erence set simplex constraints force formed belief state 
seeks maximize advantage new constraints linear accomplished linear programming 
total size linear program variable component belief state representing advantage plus constraint policy tree constraint state constraint ensure belief state sums 
linear program nds biggest advantage positive new improvement ps 
witness point 
single step value iteration complete value iteration step starts agenda containing single policy tree empty 
takes policy tree top agenda uses new linear program table determine improvement policy trees witness point discovered best policy tree point calculated added policy trees di er current policy tree single subtree added linear programming packages variables implicit non negativity constraints constraints needed 
inputs ua new variables maximize improvement constraints ua simplex constraints table linear program nd witness points 
agenda 
witness points discovered policy tree removed agenda 
agenda empty algorithm terminates 
know witness points discovered adds tree set useful policy trees jv jj trees added agenda addition tree initial agenda 
linear program solved jsj variables jsj jq constraints 
linear programs removes policy agenda happens jv jjq times witness point discovered happens jq times 
facts imply running time single pass value iteration witness algorithm bounded polynomial size state space jsj size action space jaj number policy trees representation previous iteration value function jv number observations number policy trees representation current iteration functions jq 
note assume number bits precision specifying model polynomial quantities polynomial running time linear programming expressed function input precision 
alternative approaches paragraph cheng sondik incremental pruning 
short discussion relative ciencies 
nite horizon sure right tony new insights cite thesis 
previous section showed optimal step value function piecewise linear convex 
necessarily true discounted value function remains convex may facets 
optimal nite horizon discounted value function approximated arbitrarily closely nite horizon value function su ciently long horizon 
optimal nite horizon discounted value function approximated value iteration series step discounted value functions computed iteration stopped di erence successive results small yielding arbitrarily piecewise linear convex approximation desired value function 
approximate value function extract stationary policy approximately optimal 
understanding policies section introduce simple example illustrate properties pomdp policies 
examples explored earlier :10.1.1.53.7233
tiger problem imagine agent standing front closed doors 
doors tiger large reward 
agent opens door tiger large penalty received presumably form amount bodily injury 
opening doors agent listen order gain information location tiger 
unfortunately listening free addition entirely accurate 
chance agent hear tiger left hand door tiger really right hand door vice versa 
refer state world tiger left right ass actions left right listen 
reward opening correct door penalty choosing door tiger 
cost listening 
possible observations hear tiger left tl hear tiger left listen right fig 

tiger example optimal policy right tr 
immediately agent opens door receives reward penalty problem resets randomly relocating tiger doors 
transition observation models described detail follows 
listen action change state world 
left right actions cause transition world state probability state probability essentially resetting problem 
world state listen action results observation tl probability observation tr probability conversely world state matter state world left right actions result observation probability 
finite horizon policies optimal undiscounted nite horizon policies tiger problem striking richness structure 
policy time step agent gets single decision 
agent believes high probability tiger left best action open right door believes tiger right best action open left door 
agent highly uncertain tiger location 
best thing listen 
guessing incorrectly incur penalty guessing correctly yield reward 
agent belief bias way guess wrong guesses right expected reward opening door 
listening value greater value opening door random 
shows optimal step policy 
policy trees shown node node belief interval policy tree dominates inside node action root policy tree 
move case agent act time steps 
optimal non stationary step policy begins situation action mapping shown 
situation action mapping surprising property chooses act listen 

agent open doors step tiger belief interval speci ed terms sl sr 
listen listen listen listen listen fig 

tiger example optimal policy listen listen listen listen listen tl tr tr tl tl tr left listen right fig 

optimal non stationary policy illustrating belief state transformations tot randomly placed doors agent belief state reset 
opening door agent left information tiger location action remaining 
just saw step go best thing listen 
agent opens door listen step 
better strategy listen order informed decision step 
interesting property step policy multiple policy trees action 
implies value function linear linear regions 
belief states single region similar transformed se resulting belief states lie belief region de ned policy 
words single belief state particular region action observation transformed belief state lies region policy 
relationship shown 
optimal policy consists solely policy trees listen action roots 
agent starts uniform belief state listening change belief state expected value opening door greater listening 
argument parallels case 
argument listening rst steps longer applies optimal situation action mappings choose open belief states 
shows structure emerges optimal non stationary policy 
notice nodes tr tl tl tr listen tl tr left listen listen listen right listen listen listen listen listen tl tr tl tr tr listen listen listen listen listen listen tl tr tr tr tl tl tl tl tr tr tr tl tr left listen right tl tr tr tl tl tl tr tl tr tl tl tr fig 

optimal non stationary policy incoming arcs happens belief state optimal action resulting observation generates new belief state lies regions de ned unused nodes 
graph interpreted compact representation useful policy trees level 
forest policy trees transformed directed acyclic graph collapsing nodes stand policy tree 
nite horizon policies include discount factor decrease value rewards structure nite horizon pomdp value function changes slightly 
horizon increases rewards received nal steps decreasing uence policy earlier time steps value function begins converge 
discounted pomdp problems optimal mapping large looks optimal situation action mapping 
shows portion optimal non stationary policy discounted nite horizon version tiger problem large values notice structure graph exactly time 
vectors nodes de ne value function di er decimal place 
structure rst appears time step remains constant 
precision algorithm calculate policy tl tr left listen listen listen listen listen listen listen right left listen listen listen listen listen listen listen right left listen listen listen listen listen listen listen right fig 

portion optimal non stationary policy large longer discern di erence vectors values succeeding intervals 
point approximately optimal value function nite horizon discounted problem 
pomdp property optimal nite horizon value function nite number linear segments 
associated optimal policy nite description called nitely transient 
pomdps optimal nitely transient policies solved nite time value iteration 
pomdps optimal policies nitely transient nite horizon value function nite number segments problems sets grow iteration 
best hope solve pomdps approximately 
known way value iteration approach described solving pomdps nitely transient optimal policies nite time conjecture 
nite time algorithm described solving pomdps nitely transient optimal policies nite horizon version policy iteration described sondik 
plan graphs drawback pomdp approach maintain belief state select optimal action step underlying state space large computation expensive 
cases possible encode policy graph select actions explicit representation belief state refer graphs plan graphs 
recall algorithm nearly converged nite horizon policy tiger problem 
situation action mappings level structure non stationary policy stationary edges level succeeding level 
rearrangement edges shown result redrawn plan graph 
left listen listen listen listen listen listen listen right left listen listen listen listen listen listen listen right left listen listen listen listen listen listen listen right fig 

rearranging edges form stationary policy tr listen left tr tl listen tl tr tl tr listen tr tl listen tl tl tr listen tr tl right tr listen tl tr tl listen fig 

plan graph tiger example tr left listen tl tl tr tr listen tl tl tr right tr listen fig 

trimmed plan graph tiger example nodes graph visited door opened belief state reset 
agent starts state complete uncertainty belief state lies region non reachable nodes 
results simpler version plan graph shown 
plan graph simple interpretation keep listening heard tiger twice side 
nodes represent partition belief space belief states particular region map single node level plan graph representation require agent maintain line representation belief state current node su cient tl tr tr listen left listen listen listen tl right tl tr tl tr tl listen listen tr tl tl tl tr tl tr fig 

trimmed plan graph tiger example listening reliability representation current belief 
order execute plan graph initial belief state choose starting node 
agent need maintain pointer current node graph 
step takes action speci ed current node receives observation follows arc associated observation new node 
process continues inde nitely 
plan graph essentially nite state controller 
uses minimal possible amount act optimally partially observable environment 
surprising pleasing result possible start discrete problem reformulate terms continuous belief space map continuous solution discrete controller 
furthermore extraction controller done automatically successive equal value functions 
important note priori bound size optimal plan graph terms size problem 
tiger problem instance probability getting correct information listen action reduced optimal plan graph shown larger agent hear tiger place times su ciently con dent act 
observation reliability decreases increasing amount memory required 
related section examine assumptions pomdp model relate earlier planning ai 
consider models nite state action spaces static underlying dynamics assumptions consistent majority area 
comparison focusses issues imperfect knowledge uncertainty initial state transition model observation model objective planning representation tr listen tl tr tl listen tl listen tr tr domains plan structures 
closely related kushmerick hanks weld buridan system draper hanks weld buridan system 
imperfect knowledge plans generated standard mdp algorithms classical strips partial order planning algorithms assume underlying state process known certainty plan execution 
mdp framework agent informed current state time takes action 
classical planners snlp ucpop current state calculated trivially known initial state knowledge deterministic operators :10.1.1.18.4442:10.1.1.57.3126
assumption perfect knowledge valid domains 
research epistemic logic relaxes assumption making possible reason known time :10.1.1.16.7898
unfortunately epistemic logics representation automatic planning systems richness representation provide cient reasoning di cult 
step building working planning system reasons knowledge relax generality logic schemes 
approach cnlp uses valued propositions addition true false value unknown represents state truth proposition known :10.1.1.30.7370
operators refer propositions unknown value preconditions value ects 
representation imperfect knowledge appropriate designer system knows advance aspects state known unknown 
insu cient multiple agents reasoning knowledge 
formulating knowledge predicate values known unknown impossible reason gradations knowledge 
example agent fairly certain knows combination lock willing try unlock seeking precise knowledge 
reasoning levels knowledge quite common natural pomdp framework 
long agent state knowledge expressed probability distribution possible states world pomdp perspective applies 
initial state classical planning systems snlp ucpop cnlp require starting state known planning phase 
exception plan system creates separate plan possible initial state aim making plans easy merge form single plan 
conditional planners typically aspects initial state unknown 
aspects important planning process tested execution 
pomdp framework starting state required known precisely represented probability distribution possible states 
buridan buridan probability distributions states internal representation uncertainty deal initial state uncertainty way 
transition model classical planning systems operators deterministic ects 
plans constructed brittle apply speci starting state require trajectory states go exactly expected 
domains easily modeled deterministic actions action di erent results applied exactly state 
extensions classical planning cassandra considered operators nondeterministic ects :10.1.1.30.7370:10.1.1.30.7370
operator set possible states occur 
drawback approach gives information relative likelihood possible outcomes 
systems plan possible contingency ensure resulting plan guaranteed lead goal state 
approach modeling nondeterministic actions de ne probability distribution possible states 
possible reason resulting states possible assess plan reach goal guaranteed 
type action model mdps pomdps aswell buridan buridan 
representations compute probability distributions states 
observation model starting state known actions deterministic need get feedback environment executing plan 
starting state unknown actions nondeterministic ects ective plans built exploiting feedback observations environment concerning identity state 
observations reveal precise identity current state planning model called completely observable 
mdp model planning systems cnlp cassandra assume complete observability 
systems observation model solve completely unobservable problems 
classical planning systems typically observation model fact initial state known operators deterministic means thought solving completely observable problems 
completely observable completely unobservable models particularly clean unrealistic 
pomdp buridan frameworks model partially observable environments observations provide information underlying state guarantee known certainty 
model provides great deal expressiveness completely observable completely unobservable models viewed special cases quite di cult solve 
interesting powerful model allows systems reason actions gather knowledge important decision making 
objective job planner nd plan satis es particular objective objective goal achievement arrive state set problem speci goal states 
probabilistic information available concerning initial state transitions general objective reaching goal state su cient probability see example buridan buridan 
popular alternative goal attainment maximizing total expected discounted reward total reward criterion 
objective action results immediate reward function current state 
exponentially discounted sum rewards execution plan nite nite horizon constitutes value plan 
objective extensively mdps pomdps including 
authors example koenig pointed completely observable problem stated goal achievement reward functions constructed policy maximizes reward maximize probability attainment original problem 
shows total reward criterion general goal achievement completely observable domains 
holds nite horizon partially observable domains 
interestingly complicated transformation holds opposite direction total expected discounted reward problem completely observable nite horizon transformed goal achievement problem similar size 
roughly transformation simulates discount factor introducing absorbing state small probability entered step 
rewards simulated normalizing reward values zero probability absorption equal amount normalized reward 
counterintuitive goal attainment problems reward type problems computationally equivalent 
qualitative di erence kinds problems typically addressed pomdp models addressed planning models 
quite frequently pomdps model situations agent expected go behaving inde nitely simply goal achieved 
inter representability results goal probability problems discounted optimality problems hard technical sense difference 
fact pomdp models probably addressed average reward context 
discounted optimal policy truly nite duration setting convenient approximation similar situation action mapping nite horizon policy receding horizon control 
littman catalogs alternatives total reward criterion idea objective value plan summary immediate rewards duration run 
koenig simmons examine risk sensitive planning showed planners total reward criterion optimize risk sensitive behavior 
haddawy looked broad family decision theoretic objectives possible specify trade partially satisfying goals quickly satisfying completely 
bacchus boutilier grove show richer objectives evaluations sequences actions converted total reward problems 
objectives considered planning systems aside simple goals achievement include goals maintenance goals prevention types goals typically represented immediate rewards 
representation problems propositional representations planning anumber advantages state space representations associated mdps pomdps 
main advantage comes compactness combined operator schemata represent individual actions single operator propositional representations exponentially concise fully expanded state transition matrix mdp 
algorithms manipulating compact factored pomdps begun appear promising area research :10.1.1.116.1003
evidence algorithms result improved planning time signi cantly representation state space 
plan structures planning systems di er structure plans produce 
important planner able express optimal plan exists domain 
brie review popular plan structures domains su cient expressing optimal behavior 
traditional plans simple sequences actions 
su cient initial state known actions deterministic 
slightly elaborate structure partially ordered plan generated example snlp ucpop parallel plan :10.1.1.53.7233
type plan actions left unordered orderings equivalent performance metric 
actions stochastic partially ordered plans buridan contingent plans ective 
simplest kind contingent branching plan tree structure generated cnlp 
plan actions di erent possible outcomes observed ow execution plan conditioned outcome 
branching plans su cient representing optimal plans nite horizon domains 
directed acyclic graphs dags represent class plans potentially succinctly separate branches share structure 
buridan uses representation contingent plans allows structure sharing di erent type dag structured plans 
pomdps nds dag structured plans nite horizon problems 
nite horizon problems necessary introduce loops plan representation 
loops useful long nite horizon pomdps representational succinctness 
simple loop plan representation de plan labeled directed graph 
node graph labeled action labeled outgoing edge possible outcome action 
possible generate type plan graph pomdps :10.1.1.53.7233
completely observable problems high branching factor convenient representation policy maps current state situation action 
action choice speci ed possible initial states policies called universal plans :10.1.1.49.6261
representation appropriate pomdps underlying state fully observable 
pomdp policies viewed universal plans belief space 
interesting note nite horizon pomdps nite state plan su cient 
simple state examples constructed optimal behavior requires counting simple stack machine reason believe general pushdown automata turing machines necessary represent optimal plans general 
argues limit plan program 
techniques proposed searching program controllers pomdps restrict attention simpler nite horizon case small set nite horizon problems optimal nite state plans 
extensions pomdp model provides rm foundation planning uncertainty action observation 
gives uniform treatment action gain information action change world 
derived domain continuous belief spaces elegant nite state controllers may constructed algorithms witness algorithm 
experimental results suggest witness algorithm impractical problems modest size jsj 
current explores function approximation methods representing value functions simulation order concentrate approximations frequently visited parts belief space 
results encouraging allowed get solution state observation instance hallway navigation problem similar described 
optimistic hope extend techniques get solutions large problems 
area addressed acquisition world model 
approach extend techniques learning hidden markov mod els learn pomdp models :10.1.1.131.2084
apply algorithms type described learned models 
approach combine learning model computation policy 
approach potential signi cant advantage able learn model complex support optimal behavior making irrelevant distinctions idea pursued chrisman mccallum :10.1.1.54.2637:10.1.1.54.132:10.1.1.56.7115
appendix theorem non empty set useful policy trees complete set useful policy trees 
tree belief state pnew new step policy tree agrees action subtrees observation new note assuming trees equal value function 
proof 
direction easy identify policy tree missing direction rephrased belief state ap new new larger value start picking choose highest value useful 
argmax vp ua policy tree highest value policy tree highest value construction 
di er subtree done serve new theorem 
di er subtree identify policy tree act pnew 
choose observation vo satisfying inequality get contradiction vp oi voi oi voi de ne new identical place subtree put 
addition pnew pnew pnew oi voi pnew oi voi policy trees new observation belief state satisfy conditions theorem 
astrom 
optimal control markov decision processes incomplete state estimation 
journal mathematical analysis applications 
fahiem bacchus craig boutilier adam grove 
rewarding behaviors 
proceedings thirteenth national conference arti cial intelligence pages 
aaai press mit press 
dimitri bertsekas :10.1.1.116.1003
dynamic programming optimal control 
athena scienti belmont massachusetts 
volumes 
avrim blum furst :10.1.1.53.7233
fast planning planning graph analysis 
proceedings th international joint conference onarti cial intelligence ijcai pages august 
jim blythe 
planning external events 
proceedings tenth conference uncertainty arti cial intelligence pages 
craig boutilier david poole :10.1.1.116.1003
computing optimal policies partially observable decision processes compact representations 
proceedings thirteenth national conference onarti cial intelligence pages 
aaai press mit press 
anthony cassandra leslie pack kaelbling michael littman :10.1.1.53.7233
acting optimally partially observable stochastic domains 
proceedings twelfth national conference arti cial intelligence pages seattle wa 
anthony rocco cassandra :10.1.1.53.7233
exact approximate algorithms partially observable markov decision problems 
phd thesis department computer science brown university may 
te cheng :10.1.1.56.7115
algorithms partially observable markov decision processes 
phd thesis university british columbia british columbia canada 
chrisman :10.1.1.56.7115
reinforcement learning perceptual aliasing perceptual distinctions approach 
proceedings tenth national conference onarti cial intelligence pages san jose california 
aaai press 
anne condon 
complexity stochastic games 
information computation february 
thomas dean leslie pack kaelbling jak kirman ann nicholson 
planning time constraints stochastic domains 
arti cial intelligence 
denise draper steve hanks dan weld 
probabilistic planning information gathering contingent execution 
technical report university seattle wa december 
mark drummond john bresina 
anytime synthetic projection maximizing probability goal satisfaction 
proceedings eighth national conference arti cial intelligence pages 
morgan kaufmann 
emmanuel fernandez aristotle steven marcus 
average cost optimality equation structure optimal policies partially observable markov processes 
annals operations research 
peter haddawy steve hanks 
utility models goal directed decisiontheoretic planners 
technical report department computer science engineering university june 
eric hansen 
cost ective sensing plan execution 
proceedings twelfth national conference arti cial intelligence pages 
aaai press mit press 
ronald howard 
dynamic programming markov processes 
mit press cambridge massachusetts 
ronald howard 
information value theory 
ieee transactions systems science cybernetics ssc august 
kalman 
new approach linear ltering prediction problems 
transactions american society mechanical engineers journal basic engineering march 
sven koenig 
optimal probabilistic decision theoretic planning markovian decision theory 
technical report ucb csd berkeley may 
sven koenig reid simmons 
risk sensitive planning probabilistic decision graphs 
proceedings th international conference principles knowledge representation reasoning pages 
john koza 
genetic programming programming computers means natural selection 
mit press 
nicholas kushmerick steve hanks daniel weld 
algorithm probabilistic planning 
arti cial intelligence september 
hong lin thomas dean 
generating optimal policies high level plans conditional branches loops 
proceedings third european workshop planning pages 
michael littman 
memoryless policies theoretical limitations practical results 
dave cli philip husbands jean meyer stewart wilson editors animals animats proceedings third international conference simulation adaptive behavior cambridge ma 
mit press 
michael littman anthony cassandra leslie pack kaelbling 
learning policies partially observable environments scaling 
armand prieditis stuart russell editors proceedings twelfth international conference machine learning pages san francisco ca 
morgan kaufmann 
michael littman anthony cassandra leslie pack kaelbling 
cient dynamic programming updates partially observable markov decision processes 
technical report cs brown university providence ri 
michael littman 
algorithms sequential decision making 
phd thesis department computer science brown university february 
technical report cs 
william lovejoy :10.1.1.18.4442
survey algorithmic methods partially observable markov decision processes 
annals operations research 

method planning uncertain incomplete information 
proceedings th conference uncertainty arti cial intelligence pages 
morgan kaufmann publishers july 
david mcallester david rosenblitt :10.1.1.18.4442
systematic nonlinear planning 
proceedings th national conference onarti cial intelligence pages 
andrew mccallum 
overcoming incomplete perception utile distinction memory 
proceedings tenth international conference machine learning pages amherst massachusetts 
morgan kaufmann 
andrew mccallum 
instance utile distinctions reinforcement learning hidden state 
proceedings twelfth international conference machine learning pages san francisco ca 
morgan kaufmann 
george monahan 
survey partially observable markov decision processes theory models algorithms 
management science january 
robert moore :10.1.1.57.3126
formal theory knowledge action 
jerry hobbs robert moore editors formal theories commonsense world pages 
ablex publishing norwood new jersey 
morgenstern 
knowledge preconditions actions plans 
proceedings th international joint conference onarti cial intelligence pages 
penberthy weld :10.1.1.57.3126
ucpop sound complete partial order planner adl 
proceedings third international conference principles knowledge representation reasoning pages 
mark peot david smith :10.1.1.30.7370
conditional nonlinear planning 
proceedings international conference arti cial intelligence planning systems pages 

feasible computational approach nite horizon partially observed markov decision problems 
technical report georgia institute technology atlanta ga january 
louise pryor gregg collins :10.1.1.30.7370
planning contingencies decision approach 
journal arti cial intelligence research 
martin puterman 
markov decision processes discrete stochastic dynamic programming 
john wiley sons new york ny 
lawrence rabiner 
tutorial hidden markov models selected applications speech recognition 
proceedings ieee february 
akira ichikawa 
optimal control partially observable markov decision processes nite horizon 
journal operations research society japan march 
scherl levesque :10.1.1.49.6261
frame problem actions 
proceedings th national conference onarti cial intelligence pages 
zhao jurgen schmidhuber 
incremental self improvement life time multi agent reinforcement learning 
pattie maes maja mataric jean meyer jordan pollack stewart wilson editors animals animats proceedings fourth international conference simulation adaptive behavior pages 
mit press 
marcel schoppers :10.1.1.49.6261
universal plans reactive robots unpredictable environments 
proceedings international joint conference onarti cial intelligence pages 
alexander schrijver 
theory linear integer programming 
wiley interscience new york ny 
satinder pal singh tommi jaakkola michael jordan 
model free reinforcement learning non markovian decision problems 
proceedings eleventh international conference machine learning pages san francisco california 
morgan kaufmann 
richard smallwood edward sondik 
optimal control partially observable markov processes nite horizon 
operations research 
edward sondik 
optimal control partially observable markov processes 
phd thesis stanford university 
edward sondik 
optimal control partially observable markov processes nite horizon discounted costs 
operations research 
andreas stolcke stephen omohundro 
hidden markov model induction bayesian model merging 
stephen jose hanson jack cowan lee giles editors advances neural information processing systems pages san mateo california 
morgan kaufmann 
jonathan stuart russell 
control strategies stochastic planner 
proceedings th national conference onarti cial intelligence pages 
paul tseng 
solving horizon stationary markov decision problems time proportional log 
operations research letters 
white harrington 
application jensen inequality adaptive suboptimal design 
journal optimization theory applications 
chelsea white iii 
partially observed markov decision processes survey 
annals operations research 
ronald williams baird iii 
tight performance bounds greedy policies imperfect value functions 
technical report nu ccs northeastern university college computer science boston ma november 
nevin zhang liu 
planning stochastic domains problem characteristics approximation 
technical report cs department computer science hong kong university science technology 
uri zwick mike paterson 
complexity mean payo games graphs 
theoretical computer science 

