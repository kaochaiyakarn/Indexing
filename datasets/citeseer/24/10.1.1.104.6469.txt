splash programs characterization methodological considerations steven cameron woo evan pal singh anoop gupta computer systems laboratory department computer science stanford university princeton university stanford ca princeton nj splash suite parallel applications released facilitate study centralized distributed space multiprocessors 
context goals 
quantitatively characterize splash programs terms fundamental properties architectural interactions important understand 
properties study include computational load balance communication computation ratio traffic needs important working set sizes issues related spatial locality properties scale problem size number processors 
related goal methodological assist people programs architectural evaluations prune space application machine parameters informed meaningful way 
example characterizing working sets applications describe operating points terms cache size problem size representative realistic situations re redundant 
splash example hope convey importance understanding interplay problem size number processors working sets designing experiments interpreting results 
appear proceedings nd annual international symposium computer architecture june architectural studies parallel programs workloads quantitative evaluation ideas tradeoffs 
shared address space multiprocessing early research typically small workloads consisting simple programs 
different programs different problem sizes making comparisons studies difficult 
studies stanford parallel applications shared memory splash swg suite parallel programs written cache coherent shared address space machines 
splash provided degree consistency comparability studies suite applications limitations 
particular consists small number programs provide broad coverage scientific engineering computing 
splash programs implemented optimal interaction modern memory system characteristics long cache lines high latencies physically distributed memory machines scale relatively small number processors 
limitations increasing splash architectural studies suite expanded modified include new programs improved versions original splash programs 
resulting splash suite contains programs represent wider range computations scientific engineering graphics domains ii better algorithms implementations iii architecturally aware 
related goals 
characterize splash programs terms basic properties architectural interactions important understand 
help people programs system evaluation choose parameters prune experimental space informed meaningful ways 
goal clearly useful provides data behavior new parallel programs allows compare results previous studies second ways important 
architectural evaluations faced huge space application machine parameters substantially impact results study 
performing complete sensitivity analysis space prohibitive 
addition architectural studies software simulation typically slow scale problem machine configurations really evaluate 
points parameter space scaled original lead execution characteristics representative reality blind sensitivity sweeps may appropriate anyway 
reasons important understand relevant characteristics programs architectural evaluation characteristics change problem machine parameters 
goal avoid unrealistic combinations parameters choose representative points realistic ones prune rest space possible 
provide necessary quantitative characterization qualitative understanding splash programs 
identify specific methodological guidelines emerge characterization 
doing hope help people prune parameter spaces contribute adoption sound experimental methodology programs architectural evaluation 
section discusses particular program characteristics measure motivations choosing 
describes approach gathering presenting results 
section provide brief description splash programs concentrating features relevant explaining effects observed 
sections characterize programs dimensions discussed section 
concluding remarks section 
characteristics approach axes characterization characterize programs axes consider important understanding shared address space programs viewpoint choosing experimental parameters 
speedup load balancing ii working sets iii communication computation ratios traffic needs iv issues related spatial locality 
discuss characteristics change important application parameters number processors understanding important programs appropriately 
concurrency load balancing characteristics program indicate processors effectively utilized program assuming perfect memory system communication architecture 
indicates program certain data set appropriate evaluating communication architecture machine scale 
example program speed may appropriate evaluating large scale machine 
working sets program den indicate temporal locality 
identified knees curve cache rate versus cache size 
important working set fits cache tremendous impact local memory bandwidth communication needs 
crucial understand sizes scaling important working sets application machine parameters chosen ways represent realistic situations 
shall see knowledge working sets help prune cache size dimension parameter space 
communication computation ratio indicates potential impact communication latency performance potential bandwidth needs application 
actual performance impact bandwidth needs harder predict depend characteristics burstiness communication latency hidden 
goal characterizing ratio scales guide simulation studies making unrepresentative bandwidth provisions relative bandwidth needs 
addition inherent communication application characterize total communication traffic local traffic set architectural parameters 
spatial locality program tremendous impact memory communication behavior 
addition uniprocessor tradeoffs long cache lines prefetching fragmentation transfer time cache coherent multiprocessors potential drawback false sharing causes communication expensive 
need understand spatial locality false sharing programs scale 
important program characteristics examine quantitatively patterns data sharing communication contention 
useful understanding program second performance critical important issues viewpoint choosing application memory system parameters 
approach characterization experimental environment perform characterization study execution driven simulation tango lite generator gol drive multiprocessor cache memory system simulator 
simulator tracks cache misses various types extension classification dsr developed handle effects finite cache capacity 
simulate cache coherent shared address space sor physically distributed memory processor node 
processor single level cache kept coherent directory illinois protocol dirty shared valid exclusive invalid states pap 
processors assumed send replacement hints home nodes shared copies data replaced caches list sharing nodes maintained home contains nodes require invalidations invalidating action occurs 
instructions simulated multiprocessor complete single cycle 
performance memory system assumed perfect pram model fow memory complete single cycle regardless cache hits local remote misses 
reasons 
non deterministic programs difficult compare data rates bus traffic architectural parameters varied execution path program may change 
second focus study absolute performance architecturally relevant characteristics programs 
affected interleaving instructions different processors timing model deterministic particularly nondeterministic programs timing model necessarily better perspective 
fact believe effect timing model characteristics measure small applications including nondeterministic ones 
data distributed processing nodes guidelines stated splash application 
cases measurements just parallel processes created 
exceptions cases application practice run iterations time steps simulate 
cases start measurements initialization cold start 
programs compiled cc compiler version silicon graphics indy machines optimization level 
data sets scaling default input data sets specified programs splash suite 
applications larger data sets provided automatically generated programs 
data sets means large compared practical data sets run real machines 
intended small simulate reasonable time large interest problem domains practice 
data set size number processors tremendous impact results characterization experiments due space constraints quantitative data default problem configuration fixed number processors 
fix number processors characterizations communication computation ratio discuss effects scaling number processors qualitatively 
inherent versus practical characteristics question arises study characterize inherent properties applications characterize properties arise realistic machine parameters 
example best way measure inherent communication infinite caches line size single word best way measure inherent working sets word cache line fully associative caches 
memory system parameters realistic timing parameters change observed characteristics substantially 
ideally inherent properties obtained realistic machine parameters space constraints prevent doing 
researchers splash may true multiprogrammed workloads exercise operating system intensively workloads real time component changes instructions executed processor due nondeterminism may affect memory system behavior substantially 
impact timing model memory system behavior ignored easily cases 
suite realistic memory system parameters choose focus trying approach inherent properties avoid artifacts 
example default line size characterizations vary measuring spatial locality bytes leads away inherent properties 
hand default cache associativity way realistic large relatively free cache mapping artifacts 
data researchers may want view data ways created online database characterization results 
database menu driven interface interactive graphing tool allows results different combinations machine experiment parameters viewed 
tool database accessible world wide web www flash stanford edu 
splash application suite splash suite consists mixture complete applications computational kernels 
currently complete applications kernels represent variety computations scientific engineering graphics computing 
original splash codes removed poor formulation medium large scale parallel machines mp longer maintainable pthor improved 
briefly describe applications kernels 
complete descriptions available ing splash report 
descriptions refers number processors 
barnes barnes application simulates interaction system bodies galaxies particles example dimensions number time steps barnes hut hierarchical body method 
differs version splash respects allows multiple particles leaf cell hos ii implements cell data structures differently better data locality 
splash application represents computational domain octree leaves containing information body internal nodes representing space cells 
time spent partial traversals octree traversal body compute forces individual bodies 
communication patterns dependent particle distribution quite unstructured 
attempt intelligent distribution body data main memory difficult page granularity important performance 
cholesky blocked sparse cholesky factorization kernel factors sparse matrix product lower triangular matrix transpose 
similar structure partitioning lu factorization kernel see lu major differences operates sparse matrices larger communication computation ratio comparable problem sizes ii globally synchronized steps 
fft fft kernel complex version radix fft algorithm described bai optimized minimize interprocessor communication 
data set consists complex data points transformed complex data points referred roots unity 
sets data orga matrices partitioned processor signed contiguous set rows allocated local memory 
communication occurs matrix transpose steps require interprocessor communication 
processor transposes contiguous submatrix processor transposes submatrix locally 
transposes blocked exploit cache line reuse 
avoid memory submatrices communicated staggered fashion processor transposing submatrix processor processor see details 
fmm barnes fmm application simulates system bodies number timesteps 
simulates interactions dimensions different hierarchical body method called adaptive fast multipole method gre 
barnes major data structures body tree cells multiple particles leaf cell 
fmm differs barnes respects tree traversed body single upward downward pass timestep computes interactions cells propagates effects bodies ii accuracy controlled cells body cell interacts accurately interaction modeled 
communication patterns quite unstructured attempt intelligent distribution particle data main memory 
lu lu kernel factors dense matrix product lower triangular upper triangular matrix 
dense matrix divided array blocks nb exploit temporal locality submatrix elements 
reduce communication block ownership assigned scatter decomposition blocks updated processors 
block size large keep cache rate low small maintain load balance 
fairly small block sizes strike balance practice 
elements block allocated contiguously improve spatial locality benefits blocks allocated locally processors 
see details 
ocean ocean application studies large scale ocean movements eddy boundary currents improved version ocean program splash 
major differences partitions grids square groups columns improve communication computation ratio ii grids conceptually represented arrays allocated contiguously locally nodes iii uses red black gauss seidel multigrid equation solver bra sor solver 
see details 
radiosity application computes equilibrium distribution light scene iterative hierarchical diffuse radiosity method hsa 
scene initially modeled number large input polygons 
light transport interactions computed polygons polygons hierarchically subdivided patches necessary improve accuracy 
step algorithm iterates current interaction lists patches subdivides patches recursively modifies interaction lists necessary 
step patch combined upward pass quadtrees patches determine radiosity converged 
main data structures represent patches interactions interaction lists quadtree structures bsp tree facilitates efficient visibility computation pairs polygons 
structure computation access patterns data structures highly irregular 
parallelism managed distributed task queues processor task stealing load balancing 
attempt intelligent data distribution 
see sgl details 
radix integer radix sort kernel method described blm 
algorithm iterative performing iteration radix digit keys 
iteration processor passes assigned keys generates local histogram 
local histograms accumulated global histogram 
processor uses global histogram permute keys new array iteration 
permutation step requires communication 
permutation inherently sender determined keys communicated writes reads 
see hhs details 
raytrace application renders dimensional scene ray tracing 
hierarchical uniform grid similar octree represent scene early ray termination antialiasing implemented antialiasing study 
ray code problem size total instr total flops total reads traced pixel image plane reflects unpredictable ways objects strikes 
contact generates multiple rays recursion results ray tree pixel 
image plane partitioned processors contiguous blocks pixel groups distributed task queues task stealing 
major data structures represent rays ray trees hierarchical uniform grid task queues primitives describe scene 
data access patterns highly unpredictable application 
see sgl information 
volrend application renders dimensional volume ray casting technique 
volume represented cube voxels volume elements octree data structure traverse volume quickly 
program renders frames changing viewpoints early ray termination adaptive pixel sampling implemented adaptive pixel sampling study 
ray shot pixel frame rays reflect 
rays sampled linear paths interpolation compute color corresponding pixel 
partitioning task queues similar raytrace 
main data structures voxels octree pixels 
data accesses input dependent irregular attempt intelligent data distribution 
see nil details 
water application improved version water program splash swg 
application evaluates forces potentials occur time system water molecules 
forces potentials computed algorithm name predictor corrector method integrate motion water molecules time 
main difference splash program locking strategy updates accelerations improved 
process updates local copy particle accelerations computes accumulates shared copy 
water spatial application solves problem water uses efficient algorithm 
imposes uniform grid cells problem domain uses algorithm efficient water large numbers total writes shared reads shared writes barriers locks pauses barnes particles cholesky tk fft points fmm particles lu matrix blocks ocean ocean radiosity room ae en bf radix integers radix raytrace car volrend head water molecules water sp molecules table 
breakdown instructions executed default problem sizes processor machine 
instructions executed broken total floating point operations processors applications significant floating point computation reads writes 
number synchronization operations broken number barriers encountered processor total number locks pauses flag synchronizations encountered processors 
molecules 
advantage grid cells processors cell need look neighboring cells find molecules cutoff radius molecules box owns 
movement molecules cells causes cell lists updated resulting communication 
table provides basic characterization applications processor execution 
examine characteristics previously discussed splash suite 
concurrency load balance discussed section concurrency load balance program change problem size number processors important understanding application data set appropriate study involving machine number processors 
example program limited computational load balance small speedup input may appropriate evaluating large scale machine 
study computational load balance scales number processors measuring speedups pram architectural model 
measured manner deviations ideal speedup attributable load imbalance serialization due critical sections overheads redundant computation parallelism management 
shows pram speedups splash programs processors 
programs speed default data sets 
exceptions lu cholesky radiosity radix 
illustrate load imbalance shows time spent waiting synchronization points processor executions application 
indicates minimum maximum average fraction time processes spent synchronization points locks barriers pauses 
note cholesky lu radiosity average synchronization time exceeds execution time 
reasons sub linear speedups applications sizes input data sets inherent nature applications 
lu cholesky default speedup ideal barnes fft fmm raytrace lu cholesky speedups splash applications default input data sets shown table perfect memory system 
poor scalability lu cholesky radiosity due large part small problem sizes 
poor scalability radix due prefix computation phase completely parallelizable 
data sets result considerable load imbalance processors despite block oriented decompositions 
larger data sets reduce imbalance providing blocks processor step factorization 
lu number blocks processor kth step total blocks see section explanation parameters 
larger input data sets studying larger machines 
radiosity sublinear speedup due small data set 
imbalance difficult analyze larger data set currently available appropriate form 
radix poor speedup processors due parallel prefix computation phase completely parallelized 
time spent prefix computation time spent phases fraction total unbalanced phase decreases quickly number keys sorted increas logp pct execution time spent synchronization barnes cholesky fft fmm lu ocean number processors synchronization characteristics splash suite processors 
graph shows breakdown minimum maximum average execution time spent synchronization processors locks barriers pauses user defined synchronization radiosity 
radiosity radix raytrace maximum average minimum volrend water water sp speedup ideal volrend water spatial ocean water radix radiosity number processors es 
applications evaluate larger machines long larger data sets chosen 
default input data sets programs splash scale suitable studying processor systems 
useful studies involving larger numbers processors 
characteristics examine sizes scaling important working sets applications 
study working sets communication computation ratio section examine inherent communication artifactual communication local traffic understanding working sets pick parameters 
working sets temporal locality temporal locality program effectively cache organization exploits determined examining processor rate changes function cache size 
relationship rate cache size linear contains points inflection knees cache sizes working set program fits cache den 
shown parallel applications hierarchy working sets corresponding different knee rate versus cache size curve 
working sets important performance fitting cache lowers rate 
methodological importance depending data distributed main memory capacity misses resulting fitting important working set cache may satisfied locally increase local data traffic may cause inter node communication 
methodologically important understand sizes application important working sets scale application parameters number processors cache ability hold changes line size associativity 
help determine working sets expected fit fit cache practice 
turn helps achieve methodological goals avoiding unrealistic situations selecting realistic ones properly 
understanding particularly important scaling problem sizes ease simulation cache sizes reduced problems chosen represent realistic situations full scale problems running full scale caches 
knees working set curve defined knees particularly separated relatively flat regions valuable opportunity prune cache size dimension experimental space 
knowledge size scaling important working set indicate unrealistic working set fit cache realistic problem machine parameters ii unrealistic working set fit cache iii situations fitting fitting realistic 
knowledge indicates regions rate versus cache size curves representative practice 
helps prune space ways 
ignore unrealistic regions 
second curve representative region relatively flat care respect cache rate single operating point cache size chosen region rest pruned 
inherent working sets application best characterized simulating fully associative caches different sizes word cache lines 
cache sizes varied fine granularity precisely identify sizes knees occur 
interest realism section byte line size examine cache sizes powers 
cache size needed hold working set depends cache associativity results finite way way 
supply fully associative rate information comparison 
ignore impact line size cache size required hold working sets space reasons 
programs exhibit spatial locality required cache size change 
rate rate rate barnes ws lu ws raytrace ws ws ws ws ws cholesky ws ws ocean ws ws volrend ws ws depicts rate function cache size splash suite 
results shown power cache sizes kb mb 
mb chosen realistic size second level caches today large comfortably accommodate important working sets applications 
methodological danger ignoring caches smaller kb may important working set smaller default problem sizes grows rapidly larger problem sizes may longer fit cache 
true applications 
examine results default way set associative caches bold lines 
see applications rate completely completely stabilized mb caches 
important working sets problem sizes applications entire footprint data processor smaller mb 
fact realistic problem sizes yield speedups see section sizes growth rates working sets shall discuss infer having important working sets fit cache important operating point consider applications 
mb caches operating points rest characterizations 
question realistic operating point practice important working set fit modern secondary cache 
case choose cache size represent operating point 
examine question 
cache size kb cache size kb cache size kb cache size kb way way way full rates versus cache size associativity 
rate data assumes processors byte line sizes 
note applications difference rate way way associativity larger difference way way associativity 
ws ws refer observed working sets described table 
fft ws ws radiosity ws water ws ws fmm ws ws radix ws water sp ws ws applications large rates kb caches decrease dramatically mb 
applications barnes cholesky fft fmm lu radiosity volrend water water spatial important working set encountered quite early cases kb 
working set grows quickly problem size number processors situations practice larger problems machines fit real second level cache 
see true examine constitution growth rates working sets summarized table 
see important working sets applications listed grow increasing numbers processors grow slowly data set size 
expected fit realistic second level caches practice sense simulate cache sizes smaller working sets 
example lu cholesky factorization applications important working set single block sized fit cache 
working sets programs larger may fit cache practice usually amount processor entire partition data set turn important performance 
applications realistic operating point practice important working set fit cache ocean raytrace radix lesser extent fft 
ocean streams partition different grids different phases computation incur substantial capacity conflict misses problem sizes increase 
raytrace unstructured reflections rays result large working sets curves defined knees flat regions asymptote 
radix streams different sets keys regular irregular strides types phases accesses small histogram heavily 
results working set sharply defined scaling expressions assume fully associative caches line size difficult analyze artifacts associativity line size 
difficult analytic scaling models predict working sets exactly finite associativity caches 
users advised base results working set growth rates provide guides determine working sets fall problem machine sizes choose 
may may fit cache 
fft processor typically captures important working set cache may may capture partition data set 
important working set splash fft proportional row matrix 
fit cache row wise ffts blocked working set fit 
applications examine cache size accommodate working sets described 
ocean fft choose cache size holds working set described table expect fit cache practice second 
radix raytrace working sets sharply defined choose range cache sizes kb mb 
compromise simply choosing reasonable cache size yields relatively high capacity rate 
cache size kb works cases applications shall results subsequent sections mb kb caches 
shows rate potentially desirable cache sizes examine change associativity 
cases increasing cache associativity way direct mapped way improves rates greatly increasing way way changes 
direct mapped caches change power cache size needed hold working set compared fully associative caches way set associative caches problem sizes 
general impact associativity working set size unpredictable 
summarize results discussion section clearly show understanding relationship working sets cache sizes important experimental methodology ii requires substantial understanding important working sets application hierarchy depend problem machine parameters 
comm comp ratio traffic section examine communication computation ratio traffic characteristics applications default cache parameters way set associative byte lines representative cache sizes identified default data set size previous section mb cases kb cas growth rate growth rate working fits working fits code working set set 
cache 
working set set 
cache 
barnes tree data body partition ds ds cholesky block fixed partition ds ds fft row matrix ds partition ds ds fmm expansion terms fixed partition ds ds lu block fixed partition ds ds ocean ds partition ds ds radiosity bsp tree log polygons unstructured unstructured radix histogram radix partition ds ds raytrace unstructured unstructured unstructured unstructured volrend octree part ray ds partition ds approx ds water private data fixed partition ds ds water sp private data fixed partition ds ds table 
important working sets growth rates splash suite 
ds represents data set size represents number processors represent large small constants respectively 
es 
approach useful characterizing traffic realistic cache parameters necessarily useful characterizing inherent communication algorithm traffic includes artifacts cache parameters 
fact true sharing traffic data traffic due true sharing misses approximation inherent communication 
true sharing defined independent finite capacity finite associativity false sharing effects see section 
difference true sharing traffic inherent communication true sharing traffic includes unnecessary traffic occurs spatial locality entire byte communicated line 
discuss section applications generally spatial locality byte lines true sharing traffic approximation inherent communication 
metrics evaluate different types traffic needed programs 
programs perform large amounts floating point computation communication traffic floating point operation flop number flops influenced compiler technology number instructions 
programs perform integer computation report bytes instruction executed 
break traffic major categories remote data traffic caused data transferred nodes satisfy processor requests ii remote overhead traffic associated remote data request messages invalidations acknowledgments replacement hints headers remote data transfers iii local data amount data transmitted processor requests local memory 
remote data broken subcategories remote shared remote cold remote capacity remote writeback 
subcategories decomposition remote traffic excluding cache type remote shared consists traffic due remote true false sharing 
shows traffic broken categories true sharing traffic processor runs default input data set sizes mb caches 
cases headers data packets overhead packets assumed bytes long 
traffic processors examine traffic fixed number processors focusing attention second bar right application 
integer applications radix remote traffic bottom sections bar bytes instruction 
processors executing mips translates mb sec traffic processor 
absence contention effects processor network bandwidths shared memory multiprocessors today 
radix remote traffic processors executing mips approaches mb sec processor 
quite high traffic fact bursty evaluation studies radix model memory contention network bandwidth limitations provide accurate results 
shows overhead traffic moderate byte cache lines study impact larger cache line sizes section 
amount local traffic usually small mb caches hold important working sets keep capacity misses low decomposition misses type shown 
floating point intensive applications remote traffic typically quite small mb caches 
cholesky processors executing mflops required bandwidth mb sec processor unreasonable networks true sharing misses generate remote traffic processor requests locally allocated cache block written back modified remote processor 
true sharing traffic shown consists local remote traffic due true sharing misses 
multiprocessors today 
saw section cholesky dominated load imbalance problem size reduces bandwidth requirements 
exception fft remote bandwidth requirement close mb traffic bytes flop procs traffic bytes flop procs traffic bytes instr remote shared remote cold remote capacity remote writeback remote overhead local data true shared data breakdown traffic generated bytes instruction bytes flop 
results shown processors 
graphs assume mb way associative byte line caches 
overhead packets data headers assumed bytes long 
barnes cholesky fmm lu fft ocean procs radiosity raytrace water water sp volrend radix code growth rate comm comp ratio barnes approximately ds cholesky ds input dependent fft fmm approximately ds lu ds ocean ds radiosity unpredictable radix raytrace unpredictable volrend unpredictable water ds water sp ds table 
growth rates communication computation ratio splash suite 
ds represents data set size represents number processors 
sec node 
communication fft bursty radix studies fft careful modeling network memory system bandwidth contention 
scaling number processors data set size look effects traffic changing number processors keeping problem size fixed 
typically communication computation ratio increases number processors due finer grained decomposition problem 
example applications perform localized communication dimensional physical domain amount computation proportional area partition amount communication proportional perimeter 
table shows communication computation ratio changes data set size number processors applications growth rates modeled analytically 
ds table represents data set size number processors 
fft radix high communication computation ratios growth rates small particularly large values look results mb caches 
true false sharing traffic remote sharing traffic increase capacity related traffic may decrease processor accesses data data may fit cache 
example working set grows ds may fit cache small may fit large capacity effect reduces local traffic applications increases reduce remote traffic due capacity misses nonlocal data see remote capacity traffic raytrace particularly volrend working set close mb default data set processors completely unimportant fit mb cache smaller 
significant change capacity related traffic depends importance working set scales impact increasing data set size usually just opposite increasing sharing traffic decreases capacity related traffic local remote may increase 
example shows effect different data set sizes ocean 
change traffic total communication remote traffic depends different components scale 
examine representative cases important working set fit cache traffic kb caches applications discussed section 
results traffic bytes flop remote shared remote capacity remote overhead true shared data breakdown traffic ocean bytes floating point operation problem sizes 
graph assumes mb way associative byte line caches 
shown 
total traffic including local course larger mb caches 
increased capacity related traffic may local ocean fft cause communication raytrace 
important model contention working set fit cache 
results importance understanding interplay problem size number processors working set sizes application architectural studies 
spatial locality false sharing set characteristics examine related multiword cache lines spatial locality false sharing 
programs spatial locality perform long cache lines due prefetching effects 
poor spatial locality better shorter cache lines avoid fetching unnecessary traffic bytes flop procs remote cold local data ocean ocean remote writeback traffic bytes instr breakdown traffic generated processors kb way associative byte line caches 
overhead packets data headers assumed bytes long 
traffic show bytes flop fft ocean bytes instruction 
true shared fft ocean radix raytrace local data rem 
overhead rem 
writeback rem 
capacity rem 
cold rem 
shared data undergoing capacity misses due fragmentation 
parallel machines long cache lines detrimental units coherence assume program may exhibit false sharing 
perfect spatial locality implies false sharing program quite spatial locality processor stream processor writes element contiguous array having locality suffer greatly false sharing processor may write intervening elements array time 
section characterize behavior splash applications function cache line size looking rates translate latency traffic translates bandwidth 
explain results respect data structures access patterns applications classify applications regard discuss behavior changes data set size number processors 
methodologically characterization shows interaction line size depends parameters aware dependence performing architectural studies 
characterization tells applications predict effects line size ones perform sensitivity analysis dimension 
application perfect spatial locality fold increase line size reduce rate fold keeping total data traffic constant 
case false sharing traffic decrease relative impact header overhead decreases line size increases 
understand application falls short perfect interaction obtain insights program behavior misses incurred application divided broad components cold misses ii capacity replacement misses iii true sharing misses iv false sharing misses 
classification scheme dubois dsr extended account effects finite cache capacity 
classification true sharing lifetime line cache processor accesses value written different processor true sharing processor line ii execution previous true sharing line replaced incurred sharing true false replaced classified sharing false sharing line modified time processor cache execution time processor cache processor newly defined values 
misses cold misses line processor cache capacity misses 
rate bytes breakdown rate versus cache line size 
way associative caches capacities fixed mb line sizes varied bytes 
definition captures true communication inherent application independent cache size recognizes benefits long cache lines capturing required communication 
cache size methodology results applications mb way set associative caches fit important working sets applications kb caches 
encounter interesting methodological question cache size needed hold working set may depend line size spatial locality perfect 
particularly applications poor spatial locality important ensure line size changes cache sizes chosen represent operating point regard fitting working set 
shows breakdown rate applications line size varied bytes mb caches kb caches relevant applications 
volrend applications start incur capacity misses mb caches line size changes 
operating point shift side important knee working set curve line size changes 
true kb caches 
focus results mb caches shown 
case important working sets fit cache sharing cold misses prominent 
shows impact long cache lines varies greatly applications 
applications lu halve ratio doubling line size range study radiosity don improve 
fmm improve early worse 
examine applications different categories interaction data structures access patterns respect long cache lines 
process raise important points architects keep mind performing evaluations depend line size effects 
summarize observations section 
class applications consists data access patterns regular data structures organized access patterns stride contiguously allocated data structures 
include lu cholesky fft ocean 
lu bars figures contain new category called upgrades 
writes find memory block cache shared state send upgrade request ownership 
put upgrades white top bars readers easily ignore visually necessary discuss rates compute traffic 
upgrade false true capacity cold barnes fmm lu ocean radiosity raytrace volrend water water sp cholesky fft radix rate bytes false true capacity cold upgrade fft ocean radix raytrace breakdown rate versus cache line size kb way associative caches applications important working 
cholesky blocked matrix codes data structures block contiguous address space 
fft maintains stride row wise fft computations uses blocked submatrix transpose ensure cache line utilization 
ocean processors stream grid partitions allocated contiguously locally 
result lu rate drops linearly increasing line size 
cholesky cold true sharing misses false sharing misses fall linearly 
fft cold true sharing misses fall linearly bytes explained shortly 
ocean access patterns spatial locality rate mb caches fall linearly increasing line size 
reason best spatial locality obtained processor partition 
majority cause misses mb cache problem size sharing misses observed 
expect see larger influence long cache lines processor local partition fit cache shown 
sharing misses occur ocean processor tries access elements nearest neighbor adjacent partitions 
accesses neighboring unit stride locality non unit stride spatial locality 
results bring important methodological point 
applications impact long cache lines depends application parameters particularly data set size number processors 
seen example ocean processor partition fits cache depends parameters 
example provided fft 
default point fft matrix complex doubles processors processor reads submatrix points processor transpose phase 
element bytes submatrix bytes 
byte lines see spatial locality problem size 
going byte lines reduce cold rate transpose sharing rate transposes reduce number upgrades required 
dramatic example provided radix 
permutation phase program processor reads keys contiguously partition array writes scattered form histogram values array 
pattern writes processors second array average writes different processors interleaved array granularity keys number keys sorted radix number processors respectively 
exact pattern dependent distribution keys substantial false sharing clearly depends compares cache line size 
see sharing rate drop line size ratio line 
point true sharing rate continues drop false sharing rate rises dramatically making large cache lines hurt performance 
class applications data structures records representing independent logical program units molecules fields records accessed differently different phases computation 
examples water water spatial barnes hut fmm 
processor applications may read certain fields particles owned written processors 
fields constitute integer multiple cache line size sharing misses perfect spatial locality 
see applications byte line size water programs having better spatial locality barnes fmm 
fields located cache line fields particle owned updated processor phase computation see false sharing long cache lines 
effect seen barnes fmm 
programs true sharing misses continue drop larger lines linearly false sharing misses start grow outweigh true sharing reduction byte lines 
cache lines larger single record false sharing records may result 
water barnes fmm processor particles contiguous array records assignment particles processors changes dynamically processor particles usually contiguous 
third class applications highly unstructured access patterns irregular data structures 
graphics programs raytrace volrend radiosity fall class 
radiosity main data structures written read 
access patterns unstructured making difficult analyze predict impact line size changes problem size number processors 
limited experiments shown indicate spatial locality true shared data change locality patterns relative impact false sharing increases number processors 
rate low false sharing may matter 
raytrace volrend incur little false sharing mediocre spatial locality 
false sharing small main data structures read 
primary sharing happens image plane relatively accesses 
reason poor spatial locality access patterns read data highly unstructured processor touches small field voxel polygon may different touches field 
volrend example capacity rate increases line size mb caches due increased fragmentation cache conflicts 
methodologically indicates especially applications volrend working set issues fact re evaluated larger line sizes 
scaling problem size increased form polygons voxels primary effect larger capacity rate 
spatial locality scene data change improves image image larger 
opposite effect obtained reducing problem size increasing number processors 
shows results fft ocean radix raytrace kb caches operating point important working set fit cache 
expected rates higher capacity misses increase substantially 
spatial locality traffic bytes flop bytes breakdown traffic versus line size mb way associative caches 
false sharing trends change significantly compared results mb caches properties fundamental data structures access patterns program sensitive cache size 
key new interaction see ocean application capacity misses show better spatial locality true sharing misses mb cache case 
capacity misses dominate effect long cache lines positive 
indicates impact cache line size network traffic 
remote network traffic seen results ignoring top component local traffic bar 
shows traffic bytes instruction line size varied 
points observe graph 
traffic decreases line size lu fft rate decreases linearly overhead amortized larger lines applications network traffic increases substantially line sizes larger 
bandwidth assumptions machine may re examined line size changes 
reduction rate important increased traffic depends latency bandwidth provided machine latency hidden 
second network traffic requirements splash suite small large line sizes exception radix 
large bandwidth requirements reflect false sharing problems large line sizes 
bandwidth possible concern radix stress test 
constant overhead network transaction comprises significant fraction total traffic small line sizes 
actual data traffic increases line size increased total traffic usually minimum bytes 
results previous studies shows network traffic distributed shared address space multiprocessor usually minimum cache line sizes bytes 
summarize addition showing programs high bandwidth requirements long cache lines spatial locality incur false sharing characterization emphasizes methodological points important users splash suite understand behavior individual applications choosing line size studies 
line size effects predicted programs excellent spatial locality volrend raytrace significant false sharing relative types misses radix fmm barnes radiosity 
barnes cholesky fft fmm remote shared remote cold remote capacity traffic bytes instr lu ocean water water sp radiosity raytrace volrend radix remote writeback remote overhead local data true shared data spatial locality false sharing program depend problem size number processors working sets fit cache capacity misses may different spatial locality sharing misses 
effects understood sufficient evaluate effects spatial locality single set parameters 
thresholding effect relationship line size problem size number processors cases relationship acute line size easy parameter prune architectural studies cache size 
concluding remarks splash application suite designed provide parallel programs evaluation architectural ideas tradeoffs 
performing evaluation difficult task owing large number interacting degrees freedom 
example memory system parameters cache size associativity line size quantitatively qualitatively impact results study application parameters number processors 
extremely time consuming perform complete sensitivity analyses parameters 
evaluation done simulation expensive forced smaller problem machine sizes really evaluate 
combinations application machine parameters choose evaluate representative realistic usage programs blind sweeps space may appropriate 
programs effectively architectural evaluation important understand relevant characteristics particularly regard determining realistic unrealistic regions parameter space regions change important parameters scaled 
tried provide necessary understanding splash programs methodological guidelines 
characterized programs important behavioral axes described characteristics scale key application machine parameters 
axes concurrency load balancing working set sizes communication computation ratio traffic spatial locality 
hope characterization allow people understand necessary growth rates decide effects changing certain parameters predicted determined experimentally prune design space avoiding unrealistic redundant operating points 
provided specific guidelines pruning space example looking knees flat regions characteristic curves working sets rate versus cache size curve knees curves bandwidth associativity understanding parameter values knees occur scale understanding prune entire regions possible 
course careful pruning ensure characteristic displays knees care regard parameter 
acknowledgments supported arpa contract number dabt 
steven woo supported hughes aircraft doctoral student fellowship 
supported overseas scholarship program ibm japan authors gratefully acknowledge funding sources 
bai david bailey 
fft external hierarchical memory 
journal supercomputing march 
blm guy blelloch charles leiserson bruce maggs greg plaxton stephen smith marco zagha 
comparison sorting algorithms connection machine cm 
proceedings symposium parallel algorithms architectures pp 
july 
bra brandt 
multi level adaptive solutions boundary value problems 
mathematics computation 
den peter denning 
working set model program behavior 
communications acm 
dsr michel dubois jonas krishnan 
detection elimination useless misses multiprocessors 
proceedings th international symposium computer architecture pp 
may 
susan eggers randy katz 
effects sharing cache bus performance parallel programs 
proceedings third international conference architectural support programming languages operating systems asplos iii pp 
april 
fow fortune wyllie 
parallelism random access machines 
proceedings tenth acm symposium theory computing may 
gol stephen goldschmidt 
simulation multiprocessors accuracy performance 
ph thesis stanford university june 
gre leslie greengard 
rapid evaluation potential fields particle systems 
acm press 
anoop gupta wolf dietrich weber 
cache invalidation patterns shared memory multiprocessors 
ieee transactions computers july 
hhs chris holt mark heinrich pal singh edward rothberg john hennessy 
effects latency occupancy bandwidth distributed shared memory multiprocessors 
stanford university technical report 
csl tr 
january 
hsa pat hanrahan david larry rapid hierarchical radiosity algorithm proceedings sig graph 
hos chris holt pal singh 
hierarchical body methods shared address space multiprocessors 
proceedings seventh siam international conference parallel processing scientific computing pp 
feb 
nil jason marc levoy volume rendering scalable shared memory mimd architectures proceedings boston workshop volume visualization october 
pap patel 
low overhead coherence lution multiprocessors private cache memories 
proceedings th international symposium computer architecture pp 

edward rothberg pal singh anoop gupta 
working sets cache sizes node granularity issues large scale multiprocessors 
proceedings th international symposium computer architecture pp 
may 
sgl pal singh anoop gupta marc levoy parallel visualization algorithms performance architectural implications ieee computer july 
swg pal singh wolf dietrich weber anoop gupta 
splash stanford parallel applications shared memory 
computer architecture news march 
josep torrellas monica lam john hennessy 
false sharing spatial locality multiprocessor caches 
ieee transactions computers june 
tue dean tullsen susan eggers 
limitations cache prefetching bus multiprocessor 
proceedings th international symposium computer architecture pp 
may 
steven cameron woo pal singh john hennessy 
performance advantages integrating message passing cache coherent multiprocessors 
stanford university technical report 
csl tr december 
steven cameron woo pal singh john hennessy 
performance advantages integrating block data transfer cache coherent multiprocessors 
proceedings sixth international conference architectural support programming languages operating systems asp los vi pp 
october 
