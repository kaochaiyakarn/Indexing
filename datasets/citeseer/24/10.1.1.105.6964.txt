appears fourteenth national conference onarti cial intelligence providence rhode island aaai press 
empirical evaluation bagging boosting richard maclin computer science department university minnesota mn email umn edu ensemble consists set independently trained classi ers neural networks decision trees predictions combined classifying novel instances 
previous research shown ensemble accurate single ensemble 
bagging breiman boosting freund schapire relatively new popular methods producing ensembles :10.1.1.133.1040:10.1.1.32.9399
evaluate methods neural networks decision trees classi cation algorithms 
results clearly important facts 
rst bagging produces better classi er individual component classi ers relatively tting generalize better baseline neural network ensemble method 
second boosting technique usually produce better ensembles bagging susceptible noise quickly data set 
researchers investigated technique combining predictions multiple classi ers produce single classi er breiman clemen perrone wolpert :10.1.1.32.9399
resulting classi er referred ensemble generally accurate individual classi ers making ensemble 
popular methods creating ensembles bagging breiman boosting arcing freund schapire :10.1.1.133.1040:10.1.1.32.9399
methods rely resampling techniques obtain di erent training sets classi ers 
previous demonstrated methods ective decision trees drucker cortes breiman quinlan little empirical testing neural networks es copyright american association arti cial intelligence www aaai org :10.1.1.32.9399
rights reserved 
david opitz department computer science university mt email opitz cs umt edu new boosting algorithm 
comprehensive evaluation bagging boosting methods creating ensembles neural networks compare results similar tests decision trees 
tested algorithms data sets nd interesting results 
rst bagging produces accurate ensembles component classi ers relatively tting 
hand neural networks simple ensemble technique component networks di er initial random weight settings surprisingly bagging 
boosting varied results 
shows little gain performance individual classi ers times showing signi cant gains bagging 
tests demonstrate varied performance boosting partially explained sensitivity noise set training examples boosting may susceptible tting 
classi er ensembles illustrates basic framework classi er ensemble 
example neural networks basic classi cation method conceptually classi cation method substituted place networks 
network ensemble network network case trained training instances network 
example predicted output networks oi combined produce output ensemble 
researchers alpaydin breiman krogh vedelsby lincoln demonstrated ective combining scheme simply average predictions network :10.1.1.32.9399
combining output classi ers useful disagreement 
obviously combining identical classi ers produces gain 
hansen salamon proved average error network ensemble output combine network outputs network network input classi er ensemble neural networks 
rate example component classi ers ensemble independent production errors expected error example reduced zero number classi ers combined goes nity assumptions rarely hold practice 
krogh vedelsby proved ensemble error divided term measuring average generalization error individual classi er term measuring disagreement classi ers 
formally showed ideal ensemble consists highly correct classi ers disagree possible 
opitz shavlik empirically veri ed ensembles generalize 
result methods creating ensembles center producing classi ers disagree predictions 
generally methods focus altering training process hope resulting classi ers produce di erent predictions 
example neural network techniques employed include methods training di erent topologies di erent initial weights di erent parameters training portion training set alpaydin drucker hansen salamon maclin shavlik 
concentrate popular methods bagging boosting try generate disagreement classi ers altering training set classi er sees 
bagging classi ers bagging breiman bootstrap efron tibshirani ensemble method creates individuals ensemble training classi er random redistribution training set :10.1.1.32.9399
classi er training set generated randomly drawing replacement examples size original training set original examples may repeated resulting training set may left 
individual classi er ensemble generated di erent random sampling training set 
boosting classi ers boosting freund schapire encompasses family methods :10.1.1.133.1040
focus methods produce series classi ers 
training set member series chosen performance earlier classi er series 
boosting examples incorrectly predicted previous classi ers series chosen examples correctly predicted 
boosting attempts produce new classi ers ensemble better able correctly predict examples current ensemble performance poor 
note bagging resampling training set dependent performance earlier classi ers 
examine new powerful forms boosting arcing breiman ada boosting freund schapire :10.1.1.133.1040:10.1.1.32.9399
bagging methods choose training set size classi er probabilistically selecting replacement examples original training examples 
bagging probability selecting example equal training set 
probability depends example misclassi ed previous classi ers 
methods initially set probability example methods recalculate probabilities trained classi er added ensemble 
ada boosting sum misclassi ed instance probabilities currently trained classi er ck 
probabilities trial generated multiplying probabilities ck incorrectly classi ed instances factor renormalizing probabilities sum equals 
ada boosting combines classi ers ck weighted voting ck weight log 
revision described breiman reset weights equal restart 
arcing updates probabilities somewhat di erently 
ith example training set value mi refers number times example misclassi ed previous classi ers 
probability pi selecting example part classi er training set de ned pi mi mj breiman chose value power empirically trying di erent values breiman :10.1.1.32.9399
ada boosting arcing combines classi ers unweighted voting 
table summary data sets 
shown number examples data set number output classes discrete features describing examples number input output hidden units networks network trained 
features neural network dataset cases classes continuous discrete inputs outputs hiddens epochs breast cancer credit credit diabetes glass heart cleveland hepatitis house votes hypo ionosphere iris kr vs kp labor letter promoters ribosome bind satellite segmentation sick sonar soybean splice vehicle results evaluate performance bagging boosting performed experiments number data sets drawn uci data set repository murphy aha 
report bagging boosting error rates data set neural network decision tree ensembles error rate simply single network single decision tree 
report results simple network ensemble approach creating ensemble networks network varies randomly initializing weights network 
methodology results averaged standard fold cross validation experiments 
fold cross validation data set rst partitioned sets set turn test set classi er trains sets 
fold ensemble networks created total networks fold cross validation 
parameter settings neural networks include learning rate momentum term weights initialized randomly 
number hidden units epochs training section 
chose number hidden units number input output units 
choice criteria having hidden unit output hidden unit inputs hidden units minimum 
number epochs number examples number parameters topology network 
decision trees tool quinlan pruned trees suggested quinlan empirically produce better performance 
datasets data sets drawn uci repository emphasis ones previously investigated researchers 
table gives characteristics data sets chose 
data sets chosen vary number dimensions including type features data set continuous discrete mix number output classes number examples data set 
table shows architecture training parameters table test set error rates data sets single neural network classi er ensemble individual network trained original training set di ers networks ensemble random initial weights ensemble networks trained randomly resampled training sets bagging ensemble networks trained weighted resampled training sets boosting resampling arcing method ada method single decision tree classi er bagging ensemble decision trees boosting ensemble decision trees 
neural network boosting boosting dataset standard simple bagging arcing ada standard bagging ada breast cancer credit credit diabetes glass heart cleveland hepatitis house votes hypo ionosphere iris kr vs kp labor letter promoters ribosome bind satellite segmentation sick sonar soybean splice vehicle neural networks experiments 
experimental results table shows neural network decision tree error rates data sets described table neural network methods decision tree methods discussed 
discussion drawn results simple ensemble bagging approaches produces better performance just training single classi er 
data sets glass kr vs kp letter segmentation soybean vehicle gains performance quite signi cant 
aspect data sets share involve predictions classes suggests ensembles may important type data 
interesting bagging produces results sim ilar simple ensemble 
shows simply changing initial weight settings neural network nearly ective causing necessary changes network predictions creating small changes training set 
boosting results varied 
arcing produces results worse single classi er times signi cantly outperforms single signi cantly outperforms bagging kr letter segmentation vehicle 
ada boosting results extreme 
certain data sets kr vs kp letter sonar ada boosting produces signi method including arcing 
data sets ada boosting produces results worse single classi er wisconsin credit heart cleveland 
boosting methods extremely ective hand reduction error rate pts diabetes noise rate soybean large noise rate segmentation noise rate promoters noise rate random bagging boosting arcing boosting ada simple bagging boosting arcing ada neural network ensembles reduction error rate compared single neural network classi er data sets varying levels noise introduced data set 
graphed percentage point reduction error noise segmentation data set single network method error rate bagging method error rate graphed percentage point reduction error rate 
boosting methods fail hinder performance 
interesting note boosting methods signi cantly outperform bagging letter segmentation vehicle domains suggests positive ects may greatest multiple classes predicted 
freund shapire suggested poor performance boosting results training set training sets may emphasizing examples noise creating extremely poor classi ers 
argument especially pertinent ada boosting reasons 
rst obvious reason method updating probabilities emphasizing noisy examples 
second reason classi ers combined weighted voting 
previous krogh shown optimizing combining weights lead tting unweighted voting scheme generally resilient problems tting 
evaluate hypothesis boosting may prone tting performed second set experiments ensemble neural network methods 
introduced noise di erent data sets 
created di erent noisy data sets performed fold cross validation averaged results 
show reduction error rate ensemble methods compared single neural network classi er 
results demonstrate noise level grows cacy simple bagging ensembles generally increases noise indicates features example input output features randomly changed feature values 
arcing ada boosting ensembles gains performance smaller may decrease 
note ect extreme ada boosting supports hypothesis ada boosting ected noise 
suggests boosting poor performance certain data sets may explained tting noise data sets 
compare results neural network approaches approaches 
results nearly 
neural networks perform signi cantly better decision trees data sets decision trees outperform neural networks data sets strong drawn 
interesting thing note performance bagging boosting neural networks generally correlates performance bagging boosting 
suggests advantages disadvantages bagging boosting independent classi er depend domain applied 
results suggest boosting methods prone tting noted possible explanations emphasizing noisy examples training sets weighted voting networks 
plan perform experiments determine tting occurs explained factors 
boosting methods extremely domains plan investigate novel approaches retain bene ts boosting 
goal create learner essentially push start button run 
try preserve bene ts boosting preventing tting noisy data sets 
plan compare bagging boosting methods methods introduced 
particular intend examine stacking wolpert method training combining function avoid ect having weight classi ers 
compare bagging boosting methods approach creating ensemble 
approach uses genetic search nd classi ers accurate di er predictions 
presents empirical evaluation bagging breiman boosting freund schapire neural networks decision trees :10.1.1.133.1040:10.1.1.32.9399
results demonstrate bagging ensemble nearly outperforms single classi er 
results show arcing ensemble greatly outperform bagging ensemble single classi er 
data sets arcing shows small zero gain performance single classi er 
similarly show ada boosting ensemble outperform methods including arcing ada boosting produce worse performance single classi er 
tests demonstrate performance methods boosting declines amount noise data set increases result extreme ada boosting 
general technique bagging probably appropriate problems properly applied boosting arcing ada may produce larger gains accuracy 
research partially supported university minnesota aid authors 
wish mike henderson bagging boosting versions 
david opitz completed portion university minnesota 
alpaydin 
multiple networks function learning 
proceedings ieee international conference networks volume 
breiman 
bagging predictors 
machine learning 
breiman 
bias variance arcing classi ers 
technical report tr uc berkeley berkeley ca 
breiman 
stacked regressions 
machine learning 
clemen 
combining forecasts review annotated bibliography 
journal forecasting 
drucker cortes 
boosting decision trees 
mozer hasselmo eds advances neural information processing systems volume 
cambridge ma mit press 
drucker cortes jackel lecun vapnik 
boosting machine learning algorithms 
proceedings eleventh international conference machine learning 
new brunswick nj morgan kaufmann 
efron tibshirani 
bootstrap 
new york chapman hall 
freund schapire 
experiments new boosting algorithm 
proceedings thirteenth international conference machine learning 
morgan kaufmann 
hansen salamon 
neural network ensembles 
ieee transactions pattern analysis machine intelligence 
krogh vedelsby 
neural network ensembles cross validation active learning 
tesauro touretzky leen eds advances neural information processing systems volume 
cambridge ma mit press 
lincoln 
synergy clustering multiple back propagation networks 
touretzky ed advances neural information processing systems volume 
san mateo ca morgan kaufmann 
maclin shavlik 
combining predictions multiple classi ers competitive learning initialize neural networks 
proceedings fourteenth international joint conference arti cial intelligence 
murphy aha 
uci repository machine learning databases machine readable data repository 
university irvine department information computer science 
opitz shavlik 
actively searching ective neural network ensemble 
connection science 
opitz shavlik 
generating accurate diverse members neural network ensemble 
mozer hasselmo eds advances neural information processing systems volume 
cambridge ma mit press 
perrone 
improving regression estimation averaging methods variance reduction extension general convex measure optimization 
ph dissertation brown university providence ri 
quinlan 
bagging boosting 
proceedings thirteenth national conference onarti cial intelligence 
aaai mit press 
krogh 
learning ensembles tting useful 
mozer hasselmo eds advances neural information processing systems volume 
cambridge ma mit press 
wolpert 
stacked generalization 
neural networks 
