ftfs design fault tolerant distributed file system matt evans senior thesis faculty computer science engineering department university nebraska lincoln supervision professor steve goddard may address need manageable way scale systems handle larger volumes data higher application loads reliable fashion 
high level design distributed file system removes traditional bottlenecks client server designs excellent fault tolerance features 
design general realistically implemented variety ways nearly operating system 
contents related design overview 
datastructures 
unique node identifier 

superblock 
disk pointer layout 
node 
disk layout 

file 
file 

file 
file 
file 

determining free space 

file 

recovering faults 

applications arguments design evaluating design argument scalability 
argument fault tolerance 
argument transparency 
appendix notes locking notes selection algorithms cache issues list figures difference nfs serverless approach 
arrow represents 
example ftfs cluster nodes replication level 

node copy superblock superblock points copies root node 

structure traditional ffs node taken der 
ftfs node quite similar ffs node allow variable amount 
overview ftfs node layout 
table comparing replication level spaced disk pointers extra space node 
central philosophies unix support semantics accessible file 
hand follows file system central component unix derived system 
unix variants currently employed great success industry academia due reasonable amount reliability existent implementations 
enterprises grow manner organizations reliant network computing services scalability fault tolerance traditional unix networks continue stretched far original design 
address needs approaches taken 
common widely deployed writing tried true method throwing money problem 
bigger machines bigger disk arrays faster network interfaces able keep demands network implementors situations far 
major shortcoming large centralized hi capacity data server single point failure nature 
addition systems cost prohibitive minimal configurations 
systems suffer time conditions traditional machine 
rarely data server function near full capacity operate degraded state period maintenance 
approach popular late simple clustering fail services 
current designs identical unix servers sharing large traditional data store common scsi bus sil 
idea machines cluster monitor machine fails remaining machine masquerade failed machine network typically ip addresses machine addition 
situation desirable number reasons 
foremost scalability solution quite limited 
associated capacity fault limits single cabinet disk storage system 
storage needs cluster grow maximum capacity shared disk store upgrade path usually cost prohibitive possible 
additionally level fault tolerance dictated underlying storage medium usually raid 
implementations proprietary hardware fault management detection opaque network administrator 
additional problem current cluster designs lack reliability fail mechanisms 
technology continually improving early versions unix clustering solutions detect failures incorrectly 
correctly functioning machines incorrectly identified remaining machines cluster attempt takeover resources presumed failed machine 
fail mechanism problematic required human intervention customer discontinued entirely returned running machines non clustered fashion 
combined compar long amount time required successful fail current solutions optimum need scalable fault tolerant file system unix platform evident 
computing load popular web servers departmental file application servers goes need scalability imperative minimum time 
unix transition properly scalable distributed environment needs distributed file system correctly efficiently manage faults growth 
address problems designed new filesystem ftfs 
main contributions research scalability fault tolerance 
goals include non specialized hardware low cost deployment easy integration current systems 
keep mind provide merely design ftfs 
implemented ftfs proper scope undergraduate thesis 
furthermore design expected design evolve implementation begins 
shortcomings design predicted time implementation 
addressed issues aware currently certainly issues known implementation attempted 
provide demonstrate serverless fault tolerant distributed filesystem practical purposes transparent user significant redesign existing operating systems 
spoken unix environments concepts ftfs need tied unix operating systems 
fact design propose general applied nearly existing file system architecture modifying existing code 
generality leaves opportunity research flexibility implementation 
rest thesis organized follows 
section discusses related research 
section gives design ftfs discussions design choices 
section details possible applications research background research motivated 
section provides discussion design ftfs meets goals hoped 
section provides overview done terms additional design decisions actual implementation 
related distributed file systems past 
file systems don natively support fault tolerance underlying storage scheme fault tolerant file system transparently benefit specifically exploit underlying fault tolerance 
knowledge widely file system unix environment effectively combines fault tolerance scalable distributed nature 
commonly distributed file system unix network file system nfs 
nfs advantages 
simple install manage deploy 
supported file system backing file system 
supported different varieties unix nfs client packages versions microsoft windows 
problem nfs fault tolerance 
nfs scenario strong distinction clients server 
server data services requests clients 
drawbacks immediately obvious 
nfs server single point failure file sharing traffic go nfs server 
aspect machine network disk system cpu bottleneck prevents scalability lb 
efforts improve nfs extend usefulness shortcomings 
sun microsystems original designer implementer nfs high availability nfs solution clustering scheme similar previously mentioned unix clustering solutions 
begins address reliability providing redundant machines machine fault tolerated suffers scalability problems inherent general unix clustering current implementations extremely limited number supported nodes 
research improving various aspects file systems unix environment 
focus research performance 
bsd derived ffs file system optimized version ufs original unix file system 
svr traditionally commercial tend developed file systems typically expansions ffs 
notable exception silicon graphics mid introduced xfs primarily distinguished traditional unix file systems log nature 
xfs revisits way file system meta data recorded migrates filesystem design motif away traditional christmas tree nodes unix approach resembling way rdbms packages organize disks extents indexes 
additionally log file systems utilize analogous rdbms transaction log journal records state transactions changes file system meta data 
fundamental change file system design key benefits notable massive improvement file system reliability recovery 
traditional unix file systems fsck utility examine repair connectivity file system tree event improper shutdown operating system log file system simply examine events transaction log compare actual state disk addressed extent sil 

incomplete transactions rolled back similar rdbms operates file system consistent 
algorithm runs effectively constant time size transaction log need related size file system 
comparatively fsck node file system slower process requiring multiple passes order file system repaired 
xfs log file systems greatly enhanced file system performance reduced downtime event failure inherent fault tolerance distributed features 
realizing silicon graphics product called xfs logical volume manager supports striping xfs file systems 
functionality allows fault tolerance single computer system employs requires twice disk space non fault tolerant solution 
cost prohibitive typically mission critical situation operating system files mirrored file system 
user data typically file system inherently fault tolerant reside disk unit hardware fault tolerance third party raid system 
interesting file system similar name berkeley xfs adn 
xfs distributed xfs publications serverless distributed file system 
conceived part berkeley project aims build clustering distributed services top traditional unix similar sgi xfs performance oriented nature 
xfs members distributed system share single unified view file system members client server xfs need configured way adn 
xfs excels scalability areas 
exhibit linear performance gains connected nodes adn 
area xfs currently weak fault tolerance 
xfs research done previous projects 
dash protocol distributed cache coherency large smp machines lfs logging file system bsd zebra ho file system employed networked raid 
xfs combines technologies fully distributed cache coherent log file system uses parity drives order provide tolerance fault 
approach fault tolerance give considerable space savings block duplication limited number faults handle limiting fault occur 
fault tolerance appear central design goal xfs 
related ftfs university minnesota gfs global file system 
gfs serverless distributed file system 
received funding government commercial sectors alike including nasa goddard flight veritas veritas sells commercial high performance file system various unix vendors 
gfs approach relies new trend computing called storage area network 
multiple machines attached fiber channel scsi bus multiple disk device servers 
gfs requires disk devices support additional set commands scsi protocol hard disk manufacturer supports 
foundation gfs new addition scsi command set allows individual disk drives support form distributed locking disk controllers file system software layer manipulate 
gfs certainly distributed aspect distributed system machines attached san 
implementation dependent san hardware disk drives support protocol 
additionally gfs papers mention fault tolerance ability clear circumstances faults tolerated dealt 
uncertainty fault tolerance capability requirement defined hardware solution implements storage area network gfs unattractive reliability scalability reasons 
gfs address scalability issue allowing machines connect gfs traditional ip 
idea virtually extend storage area network encapsulating protocol san inside ip packets purposes communicating non attached machines 
widely distributed file system andrew file system afs designed jointly ibm carnegie mellon university 
design goals afs included scalability fault tolerance security 
afs dedicated file servers store shared file data afs clients 
afs enforces client server distinction 
fact afs node strictly client strictly server 
network congestion avoided network topology data management 
afs assumes afs server near group client machines 
local afs server expected data local clients require 
client requires resource nearby server request go larger backbone network serviced distant afs server 
data migrated server help balance client loads 
afs relies aggressive file caching stateful protocol reduce network traffic improve performance 
client nodes afs file system cache kbyte chunks files accessed local disks 
afs servers maintain cache consistency notifying clients cache invalidations 
afs design reduces server load stipulating name lookups done client nodes client nodes cache directory 
afs problem free file system 
afs clients required local disks cache data primarily stored 
possibility diskless afs workstation client disk size factor performance 
afs servers dedicated machines afs performance relies heavily afs server nearby terms network hops clients accessing data 
implementation afs clientside caching cache coherence severely impacts performance seen clients 
study demonstrated data locally cached afs times slower accessing local file due complicated cache management 
afs implementation doesn adequately implement unix file system semantics changes files written back afs server unix close system call 
leads great deal unpredictability extremely fault prone 
afs support painful process data migration file system designs 
data moved afs server original afs server knows redirect clients new location data 
alleviates need uniform directory update place afs client server system 
afs demonstrated scalable especially wide area networks wans significant performance reliability issues 
despite shortcomings afs transarc able take afs codebase create dce distributed file system dfs 
dfs addresses problems afs 
firstly dfs implemented unix vnode layer 
allows dfs server dfs client data dfs server accessed users processes server addition course client server dfs 
dfs provides stronger consistency guarantees better implements unix filesystem semantics 
locking scheme finer granularity afs locks done write level opposed close 
dfs excellent availability features dfs replicated dfs servers 
allows client read requests distributed various dfs servers copies fileset 
dfs inherits scalability afs improves afs areas reliability unix integration 
dfs provide level fault tolerance 
dfs features come price terms complexity 
dfs part open software foundation distributed computing environment osf dce 
typically extremely expensive 
furthermore available wide variety operating systems architectures 
complexity integration dce requires running dce dce directory service dce remote procedure call services 
interesting distributed file system coda file system js 
primary distinguishing feature supports disconnected operation client disconnected file servers continue operate shared data 
controlled client caching automatic reconnection mechanism detects resolves conflicts reconnection file servers 
distributed computing environment logical grouping files directories coda outgrowth afs mandates strong distinction clients servers 
scalability similar afs 
js data shows occurances multiple users modifying files various time intervals finds extremely uncommon 
study done cmu computer science afs servers show behavior atypical environments 
notably afs coda cache updating close system call 
furthermore granularity distribution limited entire files authors acknowledge area 
coda authors go mention coda conceivably replace afs installation network coda primary feature disconnected operation specifically useful faulty networks laptops voluntarily taken offline users travel location location 
furthermore server replication coda available afterthought carry afs 
coda built assumption significant failure prohibits continuously connected operation clients servers designed mind 
effectively implement useful disconnected operation users specify files want locally cached workstation configuration files 
coda cache manager caches files local drive client disconnected cached copies operations including modifications client reconnect server client server negotiate synchronizing state file system 
non resolvable conflicts server client cache resolved human intervention 
coda provides intriguing useful possibility computer users slow networks frequently laptops access shared file systems manual data migration large granularity ideal distributed file system focusing scalable cluster computing 
design key distributed computing unix environment file system distributed reliable simple integrate wide variety current unix operating systems 
file system distributed machines able utilize file systems data convenient familiar paradigm 
level scale number nodes distributed environment goes performance degrade point detrimental 
file system reliable able tolerate wide variety faults provide seamless uninterrupted data availability users 
implies cluster recognize recover disk failures entire node failures network partitioning 
desirable aspect reliable distributed file system easy integrate current unix environments 
means shouldn require major shift filesystem require large monetary investment advanced hardware 
design ftfs file system attempts satisfy conditions 
hardware agnostic requiring common file system network services semantics implemented effectively 
provides useful level performance typical multi user distributed environment 
tolerates pre determined number disk machine faults minor performance degradation visible effect 
concreteness explicit comparison bsd unix file system ffs 
structures file system concepts bsd file system wanted understood widely available code knowledgebase start 
ftfs designed extension bsd feel design general easily implemented best features wide variety modern file systems 
presentation terms bsd concepts concreteness show 
rest section organized manner 
section gives overview design 
section discusses data structures ftfs 
section elaborates layout ftfs file system describing allocation mechanisms 
section introduce operation ftfs discussing read occurs 
section describing file system writes 
section discusses detecting faults fault model assumed design 
section discusses rebalancing algorithm ftfs maintains utilization load distribution 
section discusses file system recovers various faults 
overview major design paradigms ftfs distribution replication 
reliability achieved replicating objects avoid single points failure 
proper replication strategy access methods replication strategy system designed way fault tolerant 
distribution goes hand hand replication 
distributing objects nodes file system collective help implement replication policy fault tolerance characteristics improve scalability de centralizing aspects file system usage 
replication increases bandwidth required file system writes increase function file system replication level independant number nodes filesystem 
file system scaled nodes added network utilization write increase 
replication allows parallelization reads tend dominate file system access tan 
ends ftfs designed serverless 
ftfs nfs client nfs client nfs server nfs client nfs client ftfs client server ftfs client server ftfs client server ftfs client server ftfs client server difference nfs serverless approach 
arrow represents request data 
single point failure 
ftfs file system coupled network machines portion ftfs meta data donate portion local ftfs collective machine ftfs collective client server 
typical configuration machine reserve partition local disk space ftfs block storage 
single machine data blocks available locally single machine file system meta data locally 
machine copy superblock file system node collective knows nodes talk order successfully traverse file system metadata subsequently access requested data block 
user level machine ftfs collective appear contain large local file system 
true data blocks ftfs file system local node expected data blocks need fetched network machines ftfs collective 
distributed aspect file system apparent 
ftfs implementation hides details user sees local file system larger local available 
machine ftfs cluster view file system machine need mount file system place local tree 
significant aspect ftfs portion data storing data actual operating system user files analagous block traditional superblock root inode file foo node superblock file foo node directory src superblock node root inode superblock node root inode directory src superblock example ftfs cluster nodes replication level 
file system directory block part file system meta data duplicated multiple times collective data block appears multiple different nodes 
meta data ftfs file system contains things list machines collective 
meta data contains pointers nodes meta data meta data distributed replicated 
making data placement policy axiom block data written times node collective guarantee entire cluster able survive faults number times block exists collective 
consider 
nodes ftfs cluster configured way fault tolerance 
object file system total copies exist 
nodes fail loss file system availability 
notice object system pointers copies immediate children 
notice situation node stores required information traverse superblock eventual file src foo 
hand object file system property accessed different nodes 
major complicating factor asynchronous nature distributed environments 
single machine local file system code doesn need worry kernels modifying data kernel structures long operating system doesn crash file system consistency guaranteed 
distributed file system node collective able provide semantics unix file access application 
requires things file locking complicated distributed system 
deadlock avoidance major issue 
caching modern file systems keep rest system 
node directory src file foo major difficulty massively distributed designs issue cache coherency 
assume node node members ftfs collective 
node shared object cached node wants modify object node cache invalidated 
silicon graphics alleviated problem hardware designing origin series scalable symmetric multi processor machines 
algorithm directory cache coherence stanford dash handle distributed cache invalidation efficiently 
ftfs similar exactly mechanism employed dash 
difficulty affects mentioned race conditions 
race conditions happen processes logically sequential non interruptible implemented non sequentially 
multi tasking operating systems certainly distributed systems physical sequentiality rules governing logical sequentiality vary 
consider process needs verify condition condition perform task 
sequential system problem 
operation atomic 
consider multi tasking distributed system 
process verifies condition 
suspended scheduler process allowed run modifies condition 
original process continued point execution verifying condition 
process state restored believes condition checked correct executes incorrectly step task 
problem atomicity preserved 
distributed file system chance working correctly atomic operations required higher level programming paradigms semantics atomicity preserved 
non trivial complicated issue smp computers 
typically sort locking limit access critical sections code 
distributed system performing locking cause race conditions 
great care taken designing locking scheme ensure deadlock avoidance fairness 
file system locking see appendix data structures ftfs easy implement real operating system data structures similar pre modern operating systems 
vastly complex traditional filesystems ftfs needs additional data structures modifications traditional ones 
subsection overviews unique node identifier node map data structures introduce modified versions superblock node structures borrow modify conventional file systems 
berkeley design xfs unique machine identifier partition block number structure ftfs disk pointer 
reviewing superblock file system starting point file system 
data structure typically physical disks known size compile time 
operating system looks determine parameters file system completely mounting file system general 
superblock essential file system usable stores parameters file system created data required reading structure file system 
node abbreviation index node 
traditional unix file systems including ffs tree design 
nodes tree store attributes file creation time file owner file size 
addition nodes contain list raw disk blocks body data file 
node operating system structure users rarely concerned contents node opposed contents file node identifies 
important fields contained node list 
give mapping logical ranges bytes file block locations underlying storage device 
operating system abstraction file contiguous sequence bytes reality file may data spread different tracks disk case ftfs different disks node different nodes cluster 
unique node identifier unique node identifier value uniquely identifies node ftfs cluster 
currently propose bit unsigned value 
allows nodes cluster 
clearly potential ftfs vastly scalable exists 
node map node map mechanism managing nodes collective status disk resources 
effectively configuration file instance ftfs filesystem 
node map primarily way corresponding network addresses unique node identifier 
file system data organized tuples shown 
member ftfs collective copy node map correlate file system information network addresses 
reasons directly network addresses file system 
instance machines multiple network addresses 
choosing place meta data superblock node superblock node superblock node superblock node root inode root inode root inode superblock node copy superblock superblock points copies root node 
defined question question straightforward answer 
machines dynamic network addresses 
putting network information directly metadata impossible difficult allow machines join cluster 
decoupling network addresses node names file system information gracefully handle re numbering networks renaming nodes reconstruction failed nodes new hardware 
superblock similar superblock traditional file system 
duplicated single node ftfs cluster 
assumed super block changes created 
duplicating superblock nodes means node start collective condition master server single point failure cluster 
internals superblock somewhat different 
root node filesystem replicated superblock list locations copies root node 
superblock replication level file system number total copies node data block file system 
shows nodes copy file system superblock 
installation replication level copies root node 
superblock pointers copy root node 
example capacity nodes completely fail 
node disk pointer layout unique machine identifier partition block number field sizes bits ftfs disk pointer 
disk pointer described figures fundamental unit identification ftfs meta data 
figures describe contains unique node identifier partition disk block 
items uniquely describe block ftfs cluster 
current design assumes full bit disk pointer partitioned 
breakup provides staggering file system size 
allows nodes 
node allows partitions 
partition disk blocks 
assuming byte disk blocks partition tb maximally configured ftfs cluster comprised nodes having partitions partition held tb storage 
gives tb storage node total nodes gives total cluster storage size tb 
node node ftfs file system complicated traditional ffs derived filesystem 
traditional node total disk pointers ftfs node replication level previously mentioned 
guarantee point file system ability tolerate faults 
increase number disk pointers causes headaches 
notably ftfs node considerably larger traditional ffs node 
consider traditional ffs node stored disk shown 
relevant fields di db di ib 
actual disk pointers give block numbers file 
note defined respectively 
traditional disk node static bytes 
shows layout ftfs node 
notice relocated disk pointer blocks node 
dimensional arrays max copies defined 
room node maximum total copies disk pointers faults tolerated node size 
standard size der terabytes disk pointers correlate logical blocks file file system blocks see node brought memory vfs vnode layer additional fields added memory version aid operating system managing file system see der struct dinode int di mode permissions see 
int di file link count 
union int ffs old user group ids 
ino lfs inode number 
di int di size file byte count 
int di atime access time 
int di access time 
int di modified time 
int di modified time 
int di ctime inode change time 
int di inode change time 
ufs di db direct disk blocks 
ufs di ib indirect disk blocks 
int di flags status flags 
int di blocks blocks held 
int di gen generation number 
int di uid file owner 
int di gid file group 
int di spare reserved currently unused structure traditional ffs node taken der 
define max copies struct ftfs dinode int di mode permissions see 
int di file link count 
union int ffs old user group ids 
int lfs inode number 
di int di size file byte count 
int di atime access time 
int di access time 
int di modified time 
int di modified time 
int di ctime inode change time 
int di inode change time 
int di flags status flags 
int di blocks blocks held 
int di gen generation number 
int di uid file owner 
int di gid file group 
int di spare reserved currently unused disk pointers start offset ftfs di db max copies direct disk blocks 
ftfs di ib max copies indirect disk blocks 
fill remainder inode start file rounds size inode helps small files ftfs node quite similar ffs node allow variable amount disk pointers 
traditional node data variable length chunk disk pointers corresponding dependant replication level set file system creation time remainder inode space filled file data overview ftfs node layout 
realize node standard header file meant represent disk layout node 
reality superblock contain value copies number actual disk blocks node determined file system creation time 
basic size node known compile time 
defined max copies 
astute reader may noticed changed type disk block currently defined unsigned bit integer 
basic size node bytes 
large compared byte ffs node 
may quite space inefficient 
realize site want ability tolerate faults file system 
furthermore file system structures generally units powers 
size node bytes 
gives room upto way fault tolerance room left 
bytes event file system created way fault tolerance bytes left node 
site decides copies disk pointers expect significantly extra space 
decided extra space store actual file data 
way get utilization node space regardless amount replication file system 
small files file blocks needed entire file fit inside node 
obvious performance benefits disk reads network traffic avoided 
gives higher level overview ftfs node partitioned main areas 
see matter file system replication level set node utilizing copies disk pointers replication level space extra space table comparing replication level spaced disk pointers extra space node 
efficient space 
shows table correlating replication level internal space usage node 
clear additional level replication adds bytes disk pointers node 
time full copies fit smallest files inside node remaining free space 
files larger just disk blocks allocated 
inclusion file data node simply optimization helps small files penalize file size penalize large files 
disk layout larger node sizes mentioned section disk representation data individual nodes largely unimportant 
ftfs cares nodes object nodes object stored 
allocation free space management maximizing file system performance handled identically best solutions modern non distributed file systems offer 
instance retaining bsd ffs example employ cylinder group concept intra disk survivability avoid fragmentation problems 
change necessary disk structures ffs increased node superblock size 
details irrelevant ftfs 
just come completely new way storing disk data 
making specification allow flexibility implementation 
fact possible implement ftfs stackable vfs layer top ffs file system matter 
large file traditional file system simulated linear group disk blocks 
ftfs meta data storage implemented separate file traditional file system 
ideally maximum performance ftfs raw disk interfaces highly vfs stands virtual file system 
object oriented architecture unix kernels employ support multiple types file systems 
see details 
efficient disk representation utilizing journaling developed techniques 
regardless underlying storage mechanisms ftfs fault tolerance scalability features 
consider advantage element design 
allocation algorithm major complication distributed file system serverless allocation algorithm distributed 
ftfs relies active replication tan redundancy fault tolerance node initiate allocation process node presumably generate request file system write 
designing distributed allocator topic large thesis 
initial efforts focused designing algorithm number nodes cluster sizes disks reliably predict additional copies block placed cluster 
advantages able calculate position copies knowing location original tremendous 
minor changes traditional small unix node required 
smaller disk pointers 
lend perfect fault tolerance 
assuming perfect meta data integrity scheme efficient 
event metadata corruption copies lost 
policy need sort way replicating information guarantee safety data loss 
focusing maximum fault survivability decided meta data massively redundant 
allocator significantly complex block item go 
invariant copy object node object exist times system order survive faults object different nodes 
allocator optimal 
designed provide fault tolerance correct 
far design ftfs extended traditional ffs necessary provide new features 
allocation mechanism similarly extension 
design ffs performance observations original fs ffs allocator takes advantage ffs cylinder groups avoid fragmentation manage free space 
concreteness example ftfs extend scheme 
assume node disk resembles standard ffs disk 
new object allocated node initiating write selects nodes hold copies object 
nodes perform local allocations report back node initiated write necessary information 
see action consider example nodes replication level 
ll call nodes respectively 
node generates allocation request 
described node generates alloc request 
node sets lock parent object object allocating 
lock locks instances object cluster total copies 
node uses algorithm select nodes copies new object reside 
assume nodes selected 
node sends allocate messages 
nodes perform allocations local disk space 
node keeps track blocks uses local allocation 
completing individual allocation individual node respond information particular node 
receives answers node updates parent object point newly allocated data 
holds locks copy object safely modify necessary parent objects 
assuming report back correctly release locks parent objects exit allocation routine 
note step algorithm assumes complete allocations correctly report back time interval 
words interaction assumes faults 
event fault guarantee correctness significant additional steps taken guarantee correctness 
issues addressed section 
file system reads common filesystem operation read consider high level ftfs filesystem read flow 
user program initiates request access ftfs filesystem 
operating system goes service request seeing requested object resides ftfs filesystem uses appropriate ftfs specific service routines 
neglecting caching ftfs layer decide get data 
give detailed example file system read operation 
ftfs file system mounted machine file system tree 
user process wants read file home src example home ftfs file system 
give detailed description read operation takes place note example locking caching ignored 
caching addressed appendix locking discussed appendix see appendix comments selection algorithms detailed example operation traditional file system see ch 
operating system kernel starts root directory vnode tree 
goes directory node vnode tree sees underlying file system ftfs type file system 

vfs layer accesses superblock mounted ftfs file system 
unix filesystem expected superblock cached traditional unix way file system originally mounted 

ftfs superblock read 
contains names nodes node specific block numbers root file system 
refer see superblock points root nodes 
ffs file system convention assumes root node node number 
ftfs needs know machines root node located superblock store information may allow possibility node number root node 

algorithm choose machines contact ask node number received superblock 
assuming faults machine returns node requested 
root node ftfs file system 
node contain list nodes hold data blocks containing directory entries root file system 

select machines corresponding block number contact machine retrieve directory block root file system 
machine responds requested data block holds directory entry root file system 

traditional unix methods reading directory block see list names node numbers 
caveat files stored times different machines 
directory entry modified contains unique node identifiers node numbers just single node number ffs 

algorithm select nodes contact get node need contain directory block 
contact chosen node ask node number corresponded node entry root node 
vnode virtual node 
abstraction memory version node inside unix kernel 
vnodes necessary file system operations done vfs layer unaware specifics particular file systems underlying physical layout please see appendix notes algorithm 
targeted machine responds requested node directory block 
method read directory block find list nodes 
algorithm select nodes copy contact asking appropriate node 
replies requested node 

read directory block listing files src 
see thatf different nodes nodes corresponding node number stored directory block 

algorithm select nodes copy starting node contact ask appropriate node 
replies requested node 

node src look see find data inside file 
standard ffs node pointers data blocks indirection nodes contain pointers data blocks 
described figures ftfs node multiples disk pointers disk pointers described 

algorithm select nodes request data block send request node listed disk block 
assuming faults contacted server returns information network completed file system read 
mentioned issue locking left example 
ensure correct operation accessing shared data structure request lock 
done operation object return lock 
interactions locking mechanism system buffer cache see appendix appendix describes interaction traditional buffer cache distributed setting 
file system writes file system writes course complimentary operation reads 
distributed system considerably complex especially regards interacting buffer cache locking system 
general filesystem write occurs things happen exclusive locks object granted client doing writing 
caches client object cached invalidated 
src file system scope data written node holding copy object 
positive responses received node holding copy object 
client frees exclusive locks 
mirroring example section consider write operation file home src pay attention locking caching issues important doing writes distributed environment 

operating system kernel starts root directory vnode tree 
goes directory node vnode tree sees underlying file system ftfs type file system 

vfs layer accesses superblock mounted ftfs file system 
unix filesystem expected superblock cached traditional unix way file system originally mounted 

ftfs superblock read 
contains names nodes node specific block numbers root file system 
refer see superblock points root nodes 
ffs file system convention assumes root node node number 
ftfs needs know machines root node located superblock store information may allow possibility node number root node 

algorithm choose machines contact ask node number received superblock 
assuming faults machine returns node requested 
root node ftfs file system 
node contain list nodes hold data blocks containing directory entries root file system 

select machines corresponding block number contact machine retrieve directory block root file system 
machine responds requested data block holds directory entry root file system 

traditional unix methods reading directory block see list names node numbers 
caveat files stored times different machines 
directory entry modified see footnote section please see appendix notes algorithm contains unique node identifiers node numbers just single node number ffs 

algorithm select nodes contact get node need contain directory block 
contact chosen node ask node number corresponded node entry root node 

targeted machine responds requested node directory block 
method read directory block find list nodes 
algorithm select nodes copy contact asking appropriate node 
replies requested node 

read directory block listing files src 
see thatf different nodes nodes corresponding node number stored directory block 

point locking 
order write tof sure exclusive access tof guarantee node get lock nodes copies node 
avoid deadlock locking algorithm deadlock avoidance scheme acquiring individual lock nodes 
granted exclusive write locks copies node proceed 
nodes attempting lock node lock requests denied return lock event failure determined node holding locks failed 

going modifying disk data ll need invalidate cached copies data cluster wide 
traditional systems require broadcast node cluster point directory cache algorithm know nodes cached data modified 
knowledge guarantee nodes need cache invalidations contacted invalidation message 
invalidations performed expected small subset total cluster 
limit performance penalty due src file system scope see appendix see appendix ll adn directory cache schemes implementations cache invalidation certainly keep caching positive attribute file system 
cache invalidation done actual write example may possible cache invalidation parallel 
depends desired semantics implementation 

write modifications data blocks node file 
replicate changes machines copies data item 
event need write additional data requires allocation ll need call distributed allocator holding locks object 
locking granularity nodes conflict allocator 

sure correctly updated file system sure receive write completion nodes 
non buffered writing important writes immediate help maintain system wide integrity 
journaling techniques weak consistency models may improve facet 

received writes complete free locks hold 
apparent steps mirror read operation described section 
detecting faults order file system handle faults gracefully able detect 
ftfs assume faults fail silent opposed byzantine tan 
goes wrong node just stops responding requests 
hand byzantine fault continue responding requests return incorrect answers 
scenario ftfs node return incorrect results result programming errors detected corrected 
furthermore tan shown nodes required provide way fault tolerance presence byzantine faults 
observation general file server gives correct answer answer incorrect answer 
detecting faults issue tell node failed 
node failed fails respond request node 
level failure transient assume communications link reliable 
reliable communications layers exist reasonable assumption 
tan intermittent faults discussed group transient faults 
guaranteed message delivery put timers critical requests glean useful approximation node alive status 
generally way differentiate transient fault fault message 
transport reliable node simply busy respond message sender impatient contacts node information 
consider nodes node sends request node 
node receives message queues extremely high load 
node expects reply node reasonable amount time get 
node note request node copy object wanted 
node responds node go business 
time node may may get processing queue messages 
send message back node assumption node failed 
sends back message sure node status 
point simply decide node effectively 
usually messages sent reasonable time interval hasn responded 
number messages allowable time limit configurable network environment average utilization nodes 
best results nodes cluster come consensus status presumed node 
node receive replies node node clearly network fault 
consensus voting algorithms mentioned tan 
summary majority nodes receive message attempts node node assumed having failed 
manual intervention system administrator required restore 
note possible node fault 
consider node detects physical media errors file read 
traditional operating systems notify system administrator condition retry operation assuming 
node performing operation detects sort localized fault interest creating byzantine condition blindly return result rest cluster 
point node declare unhealthy send message nodes cluster saying responding requests preferably node request vote take place 
read request node nodes nodes contacted determine correctness failed node answer 
recording results votes keeping eye standard error logs various nodes system administrator determine health system 
node repeatedly opinion vote reporting media errors local drives prudent administrator quickly physically repair node 
clearly possibilities classifying faults appropriate action 
selection consensus algorithm important 
course serverless run reasonable amount time clusters large number nodes 
importance lessened algorithm doesn block parts file system operation run parallel file system operations affect node question 
file system rebalancing important part ftfs collective operation rebalancing algorithm 
nodes free disk space added cluster existing data rebalanced nodes 
copies objects moved nearly full nodes placed new empty nodes 
improves load distribution 
rebalancing potentially time consuming operation need run best implemented process 
conditions frequency run vary site site point left administrator decide rebalancing employed 
basic operation rebalancing algorithm follows determine data placed new node 
new node amount data find node utilized select object node stores copy target node copy 
get exclusive lock copies object addition get exclusive lock copies parent object object copy object new node verify copy delete object node moved update parent object reflects new location child object release locks obviously doesn address possibility utilized node having blocks migrated new node believe possibility quite slim 
event algorithm pick new node move data tell system administrator buy new nodes 
file system startup obviously mandated parameters specific implementation ftfs method file system startup provided example comparison bsd way doing things 
retain unix semantics mounting file system ftfs provide mount command understands ftfs filesystems 
mount command adapted different file systems expected changes usage required 
general mount takes options arguments point global file system mount new filesystem special file file system mounted resides 
mount point parameter meaning unchanged 
special file quite different 
order effectively mount ftfs file system node nodes ftfs file system 
know find ftfs superblock 
purpose supply file contains node map described section 
expected node map file reside directory describes configuration file system 
program list nodes contact role play ftfs file system purpose consult node map file 
file contain mapping hostnames unique node identifiers 
additionally unique node identifier node map file contain device names constitute node contributions ftfs 
copy superblock need determine local hostname find corresponding unique node identifier pick listed contribution ftfs cluster 
able provide copy superblock 
superblock means associating network addresses unique node identifiers finish file system startup 
file system creation file system startup disk file system representation file system creation highly dependant implementation paradigm 
return example extending bsd ffs 
ffs new partition prepared usage program places filesystem meta data structures new disk 
accomplished new file system mounted 
ftfs similar program needed initialize file system meta data 
parameters needed creation time 
replication level need determined written superblock 
ftfs serverless point initial started unique node identifier nodes file system created told root node copies created file system started 
initial approach program simple possible handling single node 
run node initial cluster system administrator construct initial node map point portion ftfs implementation handled network communication need started told listen requests node 
node node map nodes cluster create initial distributed meta data root node copies 
point node map copied machine ftfs file system mounted 
managing nodes ftfs file system running nodes added upgraded time 
new node added cluster needs told having new entry inserted node map 
requires new unique node identifier pre formatted partition new node may provide storage cluster members 
node added administrator choose rebalance file system populate new node rely allocation mechanism gradually populate new node 
determining free space general measure free space file system adding free space partitions nodes 
value reveal additional data written file system maintain policy objects exist different nodes 
need total blocks nodes write block data 
simply divide number total free blocks node disproportionately large amount free blocks artificially inflate value 
say total free blocks nodes divided constitutes upper bound available space 
hand calculate lower bound 
node cluster free blocks available know blocks written regardless replication level furthermore represents number nodes write blocks block reside node copy block 
boundaries actual amount usable space writing additional data file system 
finding best way distribute data optimally fill file system remaining disk blocks problem self 
consider problem described section see section long replication level equal number nodes cluster 
strictly requirement violated 
nodes replication level calculate optimal way fill blocks number free blocks node 
final restriction block written additional copies copy come unique node original resides 
answer problem amount free space available assuming allocator im optimal strategy values 
depending computation time calculate optimum fill implementations may decide simply restrict stated free space stated lower bound 
pragmatically sites ftfs file systems want add nodes additional disks existing nodes having seriously pay attention free disk space 
determining full conditions question file system full causes different problem conventional file system 
case ffs fixed size resources cause file system full 
file system simply fill available disk blocks allocate nodes 
situation result new files created 
event node existing files expanded available disk blocks 
ftfs semantics data written bit trickier 
object exist times different nodes write new object able find nodes space hold object assuming object minimum addressable size file system 
event object blocks size able perform write nodes block available single node hold allocation requested 
ideally implementations ftfs get away fixed number nodes limitation especially ftfs nodes large 
problem determining full conditions basically come counting number free blocks node 
disk store mechanisms calculating fairly trivial 
long greater equal nodes free blocks write object block size maintain fault tolerance requirements 
say available ffs reserves percentage blocks allocations fast fragmentation doesn get severe 
ffs file system full may space left 
file system recovery important aspect fault tolerance faults tolerated system restored non degraded state 
cases consider 
single node failure failure equal nodes replication level failure greater nodes 
consider case second case simply 
single node recovery consider worst case 
node completely failed 
new computer new disk equivalent larger size brought replace 
case straightforward handle 
degraded condition replacement node brought online occurs new node prepared member collective 
includes disk formatting installation ftfs software 
unique node identifier set match failed machine node replacing 
program similar rebalancing program run new node 
searches ftfs cluster objects supposed copies node failed 
copies objects local disk 
done data written time live rebuild 
process lets ftfs collective know rebuilding state filesystem changes logged new node rebuilds 
rebuilding done log played back 
log playback near complete filesystem writes suspended node completed rebuild 
minimizes amount time 
leave open problem 
log playback scenario easily realized implementation ftfs meta data logging journaling 
step assume solution requires file system writes suspended short amount time solution requires suspension file system writes 
new node rebuilt just filesystem collective informed node available synchronized 
filesystem unlocked filesystem writes things continue planned 
support replacing node 
feel rebalancing total system data allow scenario possible disk prices continue drop disk sizes continue increase doesn worthwhile support event new node upgraded disk capacity versus failed node time rebalancing software run 
recovering faults event faults file system continue correct operation 
data unavailability data loss 
situation file system taken offline nodes restored traditional offline backup systems 
manner ftfs implemented plays major role process rebuilding failed file system 
considering ffs example possibilities exist 
individual nodes data archived tape offline storage nodes rebuilt backups assuming node dumped way time entire system 
realistically backup entire global file system 
backup individual node require excessive amount time create restore assuming today backup technology expected file system sizes actual ftfs file systems 
options terribly attractive 
ftfs solid fault tolerance behavior reason 
administrators urged repair faults sustained gracefully 
supporting object time traditional unix filesystems timestamps kept object create time modified time accessed time 
easily supported ftfs 
third trickier 
concept time access clear distributed environment 
request object request serviced single node object accessed node 
support traditional non replicated notion time objects time fields updated 
generates filesystem writes 
furthermore handy know copies objects getting accessed times test load balancing algorithms filesystem 
updating time single node require filesystem write context distributed locking cache invalidation expensive 
reasons updating times probably considered 
certainly written implementation ftfs node answer node node answer block requested nodes respond block node information certainly different 
way avoid performance problems mentioned implementors 
design ftfs lend traditional time semantics 
applications arguments design ideas concerning design ftfs primarily result industry experience managing administering networks unix machines support large number users handle massive amount connected storage 
realized need fault tolerant scalable distributed file system years ago attempting design hardware software infrastructure internet service provider isp working time 
need zero downtime isp industry customers demand trouble free internet service reliable access data 
furthermore painless scalability hardware software systems essential order maintain customer satisfaction additional customers sign service agreements 
fundamental problem observed partition problem effectively distribute computing resources available 
manually managing physical devices nodes stored data data needed users system components extremely difficult requirements quite dynamic 
continual growth nature problem demanded solution scalability limits 
scalability limits far capacity single memory image machine amount storage supported single machine secondary storage subsystem options 
currently existing machines come close scalability requirements expensive guaranteed solve aspects problem 
instance machine support network traffic concurrent requests existing operating system handle open file descriptors efficiently 
regardless single machine solution proposed eventually task exceeds capacity said single machine hardware bottleneck limit machine capacity software limitation 
clearly way overcome limits distributed computing 
number physical machines grows distributed environment managing hopelessly complex automation 
complexity automation software complex 
problem re creating illusion single massive computing resource transparently 
mentioned early key illusion unix environment properly distributed file system allows massive scalability 
combined fault tolerance features ftfs designed provide illusion single shared file system extremely large number nodes dynamically increased survive configurable number complete machine disk failures operate gracefully recover time faults repaired 
assuming properly implemented distributed file system way fault tol erance dynamically consider possibility scalable distributed computing 
increase web serving capacity busy web site multiple machines configured act webserver site 
data website shared nodes ftfs distributed file system 
node view data 
machine single point failure access data configured serve section 
machines equally adept handling sort request website equally distributed machines collective software unl gan router distributes network connections group machines 
order scale cluster increase capacity additional node added 
machines cluster told include new machine disk resources part filesystem immediately start putting copies data 
new machine immediately access existing data start housing copies new data new data gets written file system 
additionally properly distributed file system mechanism rebalance file system new machines quickly just utilized rest cluster 
design addresses section 
mechanism automatic rebalancing file system expanding cluster machines traditional unix file system semantics database server exception rdbms packages proprietary methods dealing disks files special rules file record locking simple just turning machine issuing command mount existing file system new machine 
data scalability purposes pure file serving immediately 
added total capacity collective increases 
existing nodes benefit additional space 
addition new node node cluster statistically handles requests member cluster perceived performance gain 
application server scalability effected just simply long application uses traditional unix file systems semantics instance application service started new node immediately access data collective 
configuration problem scaling large websites handle traffic largely goes away 
interesting side effect level fault tolerance file system coupled rebalancing aspect ability upgrade place mean intentionally creating fault 
consider downing node 
rest nodes notice failure continue operation 
node taken aspect configuration graded disk storage processing power operating system web server software physical ram network interface 
long level fault file system set acceptably high node safely left offline time need perform necessary upgrades 
upgrades finished hardware software upgraded machine seen operating properly machine rejoin ftfs collective 
single node recovery algorithm described section followed rebalancing algorithm described section individual nodes hardware software upgraded rejoin cluster cluster maintains zero downtime 
individual bottlenecks addressed manner time affecting uptime entire cluster 
possible computing resources appears zero downtime regardless required maintenance failures 
stemming experience useful scenario faulttolerant distributed file system employed computer lab environment 
currently computer labs mount central file server stores user data applications entire lab share 
central file application server represents single point failure scalability limiting factor 
machine computer lab contribute amount local disk space ftfs file system store user data 
level redundancy chosen high lab machines taken line repairs need 
scenario better utilize commodity disks typical lab machine require large machine central file application server needed 
final interesting ftfs file system traditional file application server great degree scalability fault tolerance 
ftfs implemented standard vfs vnode interface follows possible export ftfs file systems machine machines shared file system technologies nfs cifs common internet file system allow highly available easily traditional file server solution legacy clients 
effect mimic ha high availability solutions available nfs service platforms wouldn suffer failover penalty time current node scalability limit evaluating design implementation ftfs test evaluation somewhat qualitative 
demonstrate file system design sound developed design sufficiently smb server message block technology commonly known windows file print sharing xfs adn gives treatment benefit 
various operations file system support described provide trivial algorithm 
cases speculate algorithms improved 
concreteness applicable compare design bsd ffs file system understood documented widely 
greatest test design show support file access semantics reasonable operating system providing fault tolerance distribution scalability transparent manner 
argument scalability benchmarks done adn argument serverless distributed filesystems scalable 
recall adn showed linear performance gains nodes added 
berkeley xfs team successfully demonstrated real world benefits serverless approach 
argued ftfs serverless xfs ftfs doesn rely storage servers directory managers xfs 
ftfs node perfectly equal responsibilities nodes added storage servers directory managers overloaded 
manual reconfiguration required re optimize system addition node adding disk space system administrator need run rebalancing software artificial restriction scalability ftfs node 
example node structure set max copies parameter 
effectively limits level redundancy ftfs collective copies simultaneous faults 
sites want able handle 
certainly change ftfs node handle 
changing value copies gives node bytes best rounded bytes 
mechanism including file free node space situation 
words sites wishing go default node size maximum changing parameter creating file system 
problem highlight possible design shortcoming ftfs 
modified node approach taken direct adaptation traditional file system 
advanced ways managing meta data redundancy certainly exist 
schemes developed ftfs modified take advantage retaining basic design properties 
aspect scalability ftfs upgraded live denying services clients upgrades performed 
ftfs file systems scaled ways requires file system taken offline 
administrators choose add capacity ftfs file system adding additional node 
alternatively system administrator ensure system automatically runs new resource added 
section new node immediately start contributing total storage capacity entire file system 
alternative upgrade nodes storage capacity 
simply involves preparing additional disk partition ftfs described section 
partition ready need added node map 
adding new node rebalance file system simply allocation scheme fill new partition needed 
additional partitions added file system live node 
additional nodes added time outstanding faulty nodes 
event nodes currently system administrator prefer new node rebuild data stored node 
argument fault tolerance design ftfs provides inherent fault tolerance features 
primary means massive duplication important data control 
important data centralized aspect ftfs cluster decision making centralized 
initial design ftfs meant avoid fail silent faults tan ftfs serverless node acts behalf ftfs handle byzantine faults employing voting algorithm 
tan faults classified transient intermittent 
bread butter fault domain ftfs fail fault 
ftfs handle transient faults grace 
instance levels timeout fault detection 
nodes fail respond timeout flagged transient 
time second timeout node selected responds correctly assume recovered 
respond correctly flag node failed request operator intervention 
timeout values implementation decisions site dependant parameters 
point view ftfs intermittent faults viewed way intermittent fault resolves smaller threshold ftfs recover gracefully 
crosses larger time threshold get flagged fault operator intervention required 
highlight importance choosing values timeouts method employed classify detect faults 
argument transparency ftfs extension traditional file system similar 
commenting transparency file system consider transparent 
typical enduser ftfs quite familiar compared traditional file systems 
software unmodified 
hopefully thing users notice increased availability performance 
system administrators hand notice major differences 
ftfs need startup software 
unix world typically custom mount command 
see section description file system startup 
issue managing node map controls nodes members ftfs filesystem 
file system creation significantly involved described section 
tradeoff ftfs file system running largely left administrator decides start upgrading cluster 
just transparent ftfs depend implementation 
ftfs designed traditional unix file systems mind 
supports locking caching unix semantics described respectively 
straightforward add ftfs unix vfs vnode architecture locking cache issues handled correctly 
logical step start implementation ftfs manner 
spoken unix environments concepts ftfs need tied unix operating systems 
proof concept networking facilities ftfs implemented completely simulated benchmarked experimental results factored back filesystem design 
stage design ftfs little sense try performance optimizations may force decisions prove incorrect assumptions 
eventual goal course provide file system implemented real operating system kernel appear just file system users processes seamlessly providing file services allowing scenarios outlined previous section 
proper implementation ftfs file system take thousands assuming design 
highly perfectly designed ftfs attempt 
proper implementation addresses different features rebuilding rebalancing scalability take quite long time implement considerably longer tune performance 
general principles ftfs object replication distribution concepts foundation fault tolerant distributed file system 
algorithms techniques structures improved 
instance originally started attempt construct data placement algorithm distributed file system modular arithmetic calculate block described section locations needing disk pointers 
belief allocator section sub optimal 
polished implementation ftfs require research area 
general algorithms trivial ones 
demonstrate serverless fault tolerant distributed file system practical purposes transparent user significant redesign existing operating systems 
noted feel significant research opportunities exist caching locking load balancing issues mentioned 
room design ftfs provide features weren explicitly mentioned 
notably possible change level replication needing re create file system 
recall structure ftfs node shown described section 
default replication level supports maximum copies predict users replication levels lower 
possible raise lower number semi dynamically 
possible lower amount replication program run simply deletes copies object modifies meta data superblock accordingly 
assuming site running maximum amount replication free file system space possible increase level replication 
see assume node contain file 
increase level replication need create new duplicate node associated file node objects 
modify parent data structures accordingly 
increase amount disk pointers node file fit inside 
file re written disk node reorganized 
caveat ignored file data stored inside node example 
additionally replication level set node basis filesystem basis tune file systems storage object importance 
require moving replication level field super block node 
small program written changes replication level object similar method program described change replication level entire file system 
lastly concepts ftfs re applied ftfs 
consider descriptions ftfs nodes collectives 
consider collective ftfs collectives node highest level collective connected wide area network wan links 
major problem physically distributed environments replication synchronization data 
number sites need access certain data volunteer resources site pool larger collective 
sites collective course identical view shared data replication synchronization geographically disperse sites happen automatically file system layer 
furthermore number sites amount replication chosen ensure file system operation spite network partitions faults 
level algorithms designed network partitions handle networks part fault recovery 
theoretically possible create shared file system spans entire organization site geographic region fault tolerant access volume data greatly exceeds store locally 
help ftfs highly available allow useful larger number situations extended support notion disconnected operation successfully implemented coda file system js 
borrowing disconnection model coda allow graceful operation face arbitrary network partitioning 
retrofitting coda features ftfs may prove especially difficult ftfs supports stringent semantics coda 
providing support disconnected operation key feature distributed file system claims fault tolerant 
adn thomas anderson michael dahlin neefe david patterson drew roselli randolph wang 
serverless network file systems december 
der theo 
openbsd ultra secure unix operating system 
www www openbsd org 
gan gan 
prototype web server clustering system 
master thesis university nebraska lincoln may 
ho hartman ousterhout 
zebra striped network file system 
acm trans 
computer systems 
js james kistler satyanarayanan 
disconnected operation coda file system 
proceedings thirteenth acm symposium operating systems principles 
lb bil lewis daniel berg 
multithreaded programming pthreads 
press prentice hall 
ll james laudon daniel lenoski 
sgi origin ccnuma highly scalable server 
www www sgi com origin images isca pdf 
lenoski laudon gharachorloo gupta hennessy 
directory cache coherence protocol dash multiprocessor 
proceedings th international symposium computer architecture may 
marshall kirk mckusick keith bostic michael karels john quarterman 
design implementation bsd operating system 
addison wesley 
kenneth andrew barry jonathan erickson nygaard christopher steve david matthew keefe 
gfs umn edu 
bit shared disk file system linux 
www sil silicon graphics 
iris failsafe product overview 
www www sgi com products software failsafe html 
sun microsystems 
sun enterprise cluster failover white 
www www sun com clusters wp html 
sun microsystems 
sun enterprise cluster architecture technical white 
www www sun com clusters wp html 
tan andrew tanenbaum 
distributed operating systems 
prentice hall 

unix internals new frontiers 
prentice hall 
appendix notes locking sort locking ordering employed operations require mutually exclusive access 
furthermore filesystem correctly support semantics higher level apis user expectations 
revisit examples previously mentioned discuss locking ordering sort needed 
general prefer shared readers single writers locks known readers writers locks 
style locking allows simultaneous nodes reading modifying object node write modify object mutual exclusion required 
reader locks unlocked writer wait acquire exclusive lock doing modifications 
rationale style locking assume reads prevalent writes optimization multiple reads happen simultaneously 
filesystem distributed designed maximum scalability locking scheme distributed 
improves performance regard lock contention decentralization raises complexity increases latency locking mechanisms 
correct functionality maximum scalability design goals xfs filesystem lock managers serving locks amounts data fully serverless distributed locking built design ftfs 
provide maximum scalability save problem running elections event failed lock manager 
order avoid deadlock distributed locking mechanism ordering scheme multiple required locks requested order resource different nodes cluster 
easy visualize necessary 
imagine nodes trying write file 
assume simplicity copies file original replication cluster 
requesting node locks file location requesting node locks file location able get second lock need deadlock occur 
furthermore spinlock situation nodes race condition locking latency substantial possibility livelock real 
agreed ordering scheme essential locking protocol 
ordering generated nodes function collective ordering scheme obvious 
node required unique node identifier unique cluster sense value 
course semantics traditional unix filesystem see see sections limits number nodes namespace identifier field disk pointer layout reserves bits unique node identifier nodes single cluster probably relevant issue projected design requirements ftfs 
hand ftfs someday implemented destined live human body maximum unique starts look limitation 
regardless implementation specific best left realm 
notes selection algorithms algorithm choose node contact quite simple reasonably non trivial 
read operations obvious algorithm pick item list go 
obvious draw backs 
load balancing performed whatsoever request object goes place copies item available collective 
bit better algorithm take measures load balancing 
important note load balancing things load mean things 
primary load node measured amount network traffic 
disk utilization 
cpu utilization 
regardless things load decided load question measured assuming determine constitutes load machine measure utilized load balancing measurements reported node collective wishing decision 
complicated utilizes network bandwidth connecting nodes discussed bottleneck distributed filesystem performance obstacle scalability 
choice load balancing scheme lru algorithm 
words deciding node contact simply pick contacted longest time ago 
mean longest time ago node mean longest time ago object 
best load balancing probably considering node lru scheme require bookkeeping 
furthermore node collective making lru decisions possible evenly distributing loads nodes situation develop nodes choose sequence nodes contact lru algorithm 
promote balancing guarantee overloading chosen machines nodes 
distributed lru algorithm sort took account decisions nodes probably designed combat shortcoming 
require network traffic choice making unattractive 
better algorithm load balancing simply pick random node 
attempt take consideration busy node time long run nodes equally utilized assuming random number generator 
random algorithm bad points 
foremost appear pathological state transition causes random algorithm exhibit worst case performance 
worst scenario random algorithm short load spikes individual resources statistically load spikes evenly distributed 
random algorithm attempt favor lightly nodes heavily nodes 
doesn suffer worst case scenarios 
doesn require additional network disk resources decision requires knowledge nodes decision algorithms 
algorithm random algorithm reasons ease implementation 
event re selecting node allocate reserve new space careful 
specifically selection algorithm allocation cognizant possibility nodes may room hold requested objects 
allocator associate free space value node collective periodically update list busy 
give preference full nodes new allocations 
cache issues point reader may noticed single client performance ftfs significantly reduced compared traditional filesystems 
expected latency involved multi step network locking fetching 
overhead requirement correct design 
surprised significantly intelligent approaches handling distributed locking easily applied ftfs lowering general latency 
lost 
expect ftfs benefit similarly filesystems host operating system buffer cache 
typical hit ratios near operating system caching able hide performance drawbacks ftfs cases 
caching come price 
cache coherency major issue ftfs completely serverless distributed 
modifying shared data structure necessarily requires cache invalidation data structure copies exist entire collective 
global broadcast cache invalidation quickly saturate interconnect effectively simulate shared bus design 
fact lb shown shared bus design cpu memory interconnect saturated cpus 
scaling machine architectures number cpus required serious redesign interconnects 
lessons learned cpu memory interconnect world lost distributed file system designers 
berkeley xfs addresses cache invalidation issue modified version algorithm stanford dash machine adn 
dash directory cache schemes eliminates need global cache invalidation broadcasts 
dash machine processors memory divided nodes node having cpus small amount memory 
memory available node system high speed hypercube interconnect 
node contains address range total system memory 
arrangement provides fast access memory node requesting cpus pose problems consistency 
sending cache invalidations node system memory modification saturate interconnect matter fast 
solve cache coherency problem node router chip handles communications nodes 
node router chip knows portions memory responsible keeps track requests memory nodes 
cache invalidation performed router chip responsible cached memory knows nodes typically small subset total system nodes send invalidations 
cache management scheme fully distributed minimal shared interconnect bandwidth 

