cognitive science 
forward models supervised learning distal teacher michael jordan department brain cognitive sciences massachusetts institute technology david rumelhart department psychology stanford university internal models environment important role play adaptive systems general particular importance supervised learning paradigm 
demonstrate certain classical problems associated notion teacher supervised learning solved judicious learned internal models components adaptive system 
particular show supervised learning algorithms utilized cases unknown dynamical system intervenes actions desired outcomes 
approach applies supervised learning algorithm capable learning multi layer networks 
revised version mit center cognitive science occasional 
wish michael mozer andrew barto robert jacobs eric james mcclelland helpful comments manuscript 
project supported part rr awarded biomedical research support program division research resources national institutes health atr auditory visual perception research laboratories siemens human frontier science program awarded ce naval research 
learning algorithms connectionist networks seen progressive weakening assumptions relationship learner environment 
classical supervised learning algorithms asthe perceptron rosenblatt lms algorithm widrow ho strong assumptions output units adaptive units network teacher provides desired states output units 
early development algorithms recognized powerful supervised learning algorithms realized weakening rst assumption incorporating internal units adaptively recode input representation provided environment rosenblatt 
subsequent development algorithms boltzmann learning hinton sejnowski backpropagation lecun parker rumelhart hinton williams werbos provided means training networks adaptive nonlinear internal units 
second assumption weakened learning algorithms require explicit teacher developed becker hinton grossberg kohonen linsker rumelhart zipser 
unsupervised learning algorithms generally perform sort clustering feature extraction input data assumptions statistical topological properties input ensemble 
examine detail notion teacher supervised learning paradigm 
argue teacher liability commonly assumed assumption environment provides desired states output network weakened signi cantly abandoning supervised learning paradigm altogether 
feel appropriate interpretation role teacher crucial range problems paradigm applied 
issue wish address best illustrated way example 
consider skill learning task faced basketball player learning shoot baskets 
problem learner nd appropriate muscle commands propel ball goal 
di erent commands appropriate di erent locations goal visual scene mapping visual scenes muscle commands required 
learning algorithm underly acquisition mapping 
clearly clustering feature extraction visual input su cient 
di cult see apply classical supervised algorithms problem teacher provide muscle commands targets learner 
target information provided learner terms outcome movement sights sounds ball passing goal 
general scenario suggested example shown 
intentions provided inputs learning system 
learner transforms intentions actions transformed environment outcomes 
actions proximal variables variables learner controls directly intention action outcome learner environment distal supervised learning problem 
target values available distal variables outcomes proximal variables actions 
outcomes distal variables variables learner controls indirectly intermediary proximal variables 
learning process target values assumed available distal variables proximal variables 
point view outside learning system distal supervised learning task mapping intentions desired outcomes 
point view learner problem nd mapping intentions actions composed environment yield desired distal outcomes 
learner discover components proximal action vector minimize components distal error 
distal supervised learning problem temporal component 
environments ects actions instantaneous linger mix ects actions 
outcome point time uenced previous actions 
exists set variables static relationship desired outcomes learner direct control variables 
consider example basketball player 
ight ball depends velocity arm moment release static relationship motor control system able control release velocity directly 
system outputs forces torques variables static relationship distal outcome 
remainder describe general approach solving distal supervised learning problem 
approach idea supervised learning general form phase procedure 
rst phase learner forms predictive internal model forward model transformation actions distal outcomes 
transformations known priori internal model generally learned exploring outcomes associated particular choices actions 
auxiliary learning problem supervised learning problem error internal predicted outcomes actual outcomes 
internal model partially learned indirect manner solve mapping intentions actions 
idea internal model augment capabilities supervised learning algorithms proposed werbos perspective di ers certain respects 
number developments idea kawato munro nguyen widrow robinson fallside schmidhuber werbos unpublished jordan rumelhart 
close ties approach techniques optimal control theory kirk adaptive control theory goodwin sin narendra parthasarathy 
discuss relationships remainder attempt comprehensive 
distal supervised learning forward models section section general approach solving distal supervised learning problems 
describing assumptions environment learner 
assume environment characterized state function output function time step learner produces action 
conjunction state environment action determines state corresponding state sensation note sensations output vectors current formalism outcomes language introductory section 
state function output function determine state dependent mapping actions sensations 
current assume learner access state environment address issues relating state representation state estimation 
state representations involve delayed values previous actions sensations ljung involve internal state variables induced part learning procedure mozer bachrach 
state input learner produces action choice time indices equations focus output time framework learning algorithm alters previous values states inputs actions 
learner environment composite performance system consisting learner environment 
system mapping inputs sensations 
training data fpi specify desired input output behavior composite system 
note implicit loop environment output time depends state time cf 
equation 
goal learning procedure appropriate adjustments action mapping data obtained interacting environment 
distal supervised learning problem set training pairs fpi pi input vectors corresponding desired sensations 
example basketball problem input high level intention shooting basket desired sensation corresponding visual representation successful outcome 
note distal supervised learning problem mention actions learner acquire inputs desired sensations speci ed 
point view outside learning system training data specify desired input output behavior composite performance system consisting learner environment see 
point view learner problem nd mapping inputs actions resulting distal sensations target values 
learner nd mapping inputs actions placed series environment yield desired pairing inputs sensations 
note may action yields desired sensation state distal supervised learning problem may underdetermined 
basketball example may patterns motor commands yield desired sensation seeing ball pass goal 
forward models learner assumed able observe states actions sensations model mapping actions sensations 
forward model internal model produces predicted sensation state action 
forward model predicts consequences environment forward model learning forward model prediction error 
action context state vector 
shown forward model learned comparing predicted sensations actual sensations resulting prediction error adjust parameters model 
learning forward model classical supervised learning problem teacher provides target values directly output coordinate system learner 
distal supervised learning describe general approach solving distal supervised learning problem 
consider system shown learner placed series forward model environment 
composite learning system mapping inputs predicted sensations 
suppose forward model trained previously perfect model environment predicted sensation equals actual sensation actions states 
engineering literature learning process referred system identi cation ljung 
learner forward model composite learning system 
composite system maps inputs predicted sensations context state vector 
treat composite learning system single supervised learning system train map inputs desired sensations data training set 
desired sensations treated targets composite system 
supervised learning algorithm training process algorithm constrained alter forward model composite system trained 
xing forward model require system nd optimal composite mapping varying mapping inputs actions 
forward model perfect learning algorithm nds globally optimal solution resulting state dependent input action mapping perfect sense yields desired composite input output behavior placed series environment 
consider case imperfect forward model 
clearly imperfect forward model yield imperfect input action map composite system trained obvious way di erence desired sensation predicted sensation error term 
di erence predicted performance error readily available output composite system unreliable guide true performance learner 
suppose ignore output composite system substitute performance error error term training composite system see 
performance error goes zero system correct input action map regardless inaccuracy forward model 
inaccuracy forward model manifests bias learning process need prevent performance error going zero 
consider example algorithms steepest descent 
forward model inaccurate system move downhill reach solution region movement direction steepest descent 
summarize propose solve distal supervised learning problem training composite learning system consisting learner forward model environment 
procedure solves implicitly input action map learner forward model composite system trained performance error 
forward model held xed composite system trained 
training composite system map inputs distal targets 
training forward model precede training composite system forward model need perfect need pre trained state space 
ability system utilize inaccurate forward model important implies may possible interleave training forward model composite system 
remainder discuss issues interleaved training inaccuracy forward model choice error term detail 
rst turn interesting special case general distal supervised learning problem learning inverse model environment 
inverse models inverse model internal model produces action function current state desired sensation 
inverse models de ned condition yield identity mapping placed series environment 
inverse models important domains 
example environment viewed communications channel message transmitted may desirable undo distorting ects environment placing series inverse model carlson 
second example shown arises control system design 
controller receives desired sensation input nd actions cause actual sensations close possible desired sensations controller invert inverse model environment inverse model controller 
transformation actions sensations 
approach objective utilize explicit inverse model environment controller 
forward models uniquely determined environment inverse models generally 
environment characterized mapping actions sensations generally nite number possible inverse models 
worth noting inverses exist possible achieve particular desired sensation state 
shall discuss issues existence uniqueness important implications problem learning inverse model 
general approaches learning inverse models supervised learning algorithms distal learning approach alternative approach refer direct inverse modeling cf 
jordan rosenbaum 
describing approach 
direct inverse modeling direct inverse modeling treats problem learning inverse model classical supervised learning problem widrow stearns 
shown idea observe input output behavior environment train inverse model directly reversing roles inputs outputs 
data provided algorithm sampling action space observing results sensation space 
direct inverse modeling shown viable technique anumber domains atkeson miller drawbacks limit usefulness 
environment characterized mapping actions sensations direct inverse modeling technique may unable nd inverse 
di culty nonlinear mappings yield nonconvex inverse images control system design normally involves number additional constraints involving stability robustness goal generally invert environment nearly possible subject additional constraints 
environment inverse model direct inverse modeling approach learning inverse model 
problematic direct inverse modeling 
consider situation shown 
nonconvex region left inverse image point sensation space 
suppose points labelled sampled learning process 
points correspond sensation training data seen direct inverse modeling procedure input paired targets 
supervised learning algorithms resolve inconsistencies averaging multiple targets form averaging depends particular cost function 
shown gure average points lying nonconvex set necessarily lie set 
globally optimal minimum cost solution direct inverse modeling approach necessarily correct inverse model 
example behavior section 
second drawback direct inverse modeling goaldirected 
algorithm samples action space regard particular targets errors sensation space 
direct way nd action corresponds particular desired sensation 
obtain particular solutions learner sample su ciently wide range actions rely interpolation 
important emphasize direct inverse modeling restricted learning inverse models applicable general distal set convex pair points set points line points lie set 
action space sensation space convexity problem 
region left inverse image point right 
arrow represents direction mapping learned direct inverse modeling 
points lying inside inverse image averaged learning procedure yielding vector represented small circle 
point solution inverse image convex 
supervised learning problem 
distal learning approach learning inverse model methods described earlier section directly applicable problem learning inverse model 
problem learning inverse model treated special case distal supervised learning problem input vector desired sensation equal equation 
inverse model learned placing learner forward model series learning identity mapping composite system 
fundamental di erence distal learning approach direct inverse modeling approach averaging regions action space distal learning approach nds particular solutions action space 
globally optimal solution distal learning set vectors performance interesting analogy drawn distal learning approach indirect techniques solving systems linear equations 
numerical linear algebra solving explicitly generalized inverse coe cient matrix solutions generally indirectly applying gaussian elimination sides equation ga identity matrix 
errors fy zero 
true irrespective shapes inverse images targets vectors lying outside inverse image average vector shown yield zero performance error globally optimal 
nonconvex inverse images fundamental di culties distal learning framework direct inverse modeling 
true distal learning approach fundamentally goal directed 
system works minimize performance error works directly nd solutions correspond particular goals hand 
cases forward mapping distal learning procedure nds particular inverse model 
additional information particular structure input action mapping way predicting possibly nite set inverse models procedure nd 
discussed procedure constrained nd particular inverse models certain desired properties 
distal learning backpropagation section describe implementation distal learning approach utilizes machinery backpropagation algorithm 
important emphasize outset backpropagation algorithm implement distal learning approach 
supervised learning algorithm long capable learning mapping composite network includes previously trained subnetwork particular boltzmann learning applicable jordan 
introducing useful shorthand describing backpropagation layered networks 
layered network described parameterized mapping input vector output vector vector parameters weights 
classical paradigm procedure changing weights discrepancy target vector actual output vector magnitude discrepancy measured cost functional form sum squared error output units network 
generally desired minimize cost 
backpropagation algorithm computing gradients cost functional 
details algorithm rumelhart intention develop simple notation hides details 
achieved formally chain rule di erentiate weight vector respect equation shows algorithm computes gradient ectively multiplies error vector transpose jacobian matrix backpropagation algorithm forms matrix explicitly backpropagation essentially factorization matrix jordan equation describes results computation performed backpropagation 
backpropagation computes gradient cost functional respect activations units network 
particular cost functional di erentiated respect activations input units yield refer equation backpropagation weights equation backpropagation activation 
computations carried pass algorithm backpropagation activation needed intermediate step backpropagation weights computation 
remainder section formulate broad categories learning problems lie scope distal learning approach derive expressions gradients arise 
simplicity assumed derivations task learn inverse model inputs distal targets assumed identical 
formulations distal learning framework focus di erent aspects distal learning problem di erent strengths weaknesses 
rst approach local optimization formulation focuses local dynamical structure environment 
assumes learner able predict state transitions information available locally time depends prior knowledge adequate set state variables describing environment 
naturally applied problems target values provided moment time extended problems target values provided intermittently demonstrate section 
computations needed local jacobian matrix vector function simply rst derivative matrix rst partial derivatives 
entries matrix partial derivatives output activations respect weights network 
gain insight transpose matrix arises backpropagation consider linear network described weight matrix 
rows incoming weight vectors output units network columns outgoing weight vectors input units network 
passing vector forward network involves inner product vector incoming weight vectors 
operation corresponds multiplication passing vector backward network corresponds inner product vector outgoing weight vectors 
operation corresponds multiplication rows columns optimization formulation performed feedforward networks problem stability 
second approach optimization trajectories formulation focuses global temporal dependencies particular target trajectories 
computation needed obtain dependencies complex computation needed local optimization formulation exible 
extended cases set state variables known priori naturally applied problems target values provided intermittently time 
potentially problem stability computations obtaining gradient involve dynamical process 
local optimization rst problem formulation discuss local optimization problem 
assume process generates target vectors stationary consider general cost functional ef unknown function state action action output parameterized inverse model form weight vector 
optimizing directly collecting statistics ensemble states actions utilize online learning rule cf 
widrow stearns incremental changes weights instantaneous value cost functional jn online learning algorithm changes weights time step stochastic gradient gradient jn step size 
compute gradient chain rule applied equation jacobian matrices evaluated time 
rst third factors expression easily computed rst factor describes propagation derivatives output units inverse model action units weights inverse model third factor distal error 
origin second factor problematic dependence assumed unknown priori 
approach obtaining estimate factor parts system acquires parameterized forward model appropriate subdomain state space 
model form vector weights predicted sensation 
second distal error propagated backward forward model ectively multiplies distal error estimate transpose jacobian matrix 
putting pieces algorithm learning inverse model estimated stochastic gradient expression describes propagation distal error backward forward model inverse model weights changed 
network architecture computations take place shown 
network straightforward realization block diagram 
composed inverse model links state units input units action units forward model links state units action units predicted sensation units 
learning forward model learning forward model formulated optimization problem cost functional ef form equation 
choice procedure nding set weights minimize cost entirely independent choice procedure optimizing equation convenient base learning forward model stochastic gradient jacobian matrix time 
gradient computed propagation derivatives forward model requires additional hardware required learning inverse model 
note error term function output forward model activation ow forward model estimated jacobian matrix varies function activations hidden units output units model 
state units input units inverse model forward model state units action units predicted sensation units feedforward network includes forward model 
action units output units system 
name source performance error environment environment prediction error environment model predicted performance error environment model error signals table error signals sources important clarify meanings error signals equations 
shown table error signals formed variables prediction error performance error predicted performance error error signals available learner signals available individually target actual sensation provided environment predicted sensation available internally 
learning forward model prediction error clearly appropriate error signal 
learning inverse model performance error predicted performance error 
performance error see equation advantage system learn exact inverse model forward model approximate 
reasons rst equation preserves minima cost functional equation zeros estimated gradient 
inaccurate jacobian matrix remove zeros estimated gradient points zero introduce additional zeros spurious local minima 
second estimated gradients obtained approximate forward model positive inner product stochastic gradient equation expected step algorithm downhill cost 
algorithm principle nd exact inverse model forward model approximate 
may advantages predicted performance error 
particular may easier situations obtain learning trials internal model external environment rumelhart smolensky mc hinton sutton 
internal trials thought form mental practice case backpropagation weights planning case backpropagation activation 
procedures lead improved performance forward model su ciently accurate 
exact solutions procedures forward model exact 
modularity cases unknown mapping actions sensations decomposed series simpler mappings modeled independently 
example may preferable model state function output function separately modeling single composite function 
cases jacobian matrix factored chain rule yield estimated stochastic gradient estimated jacobian matrices expression obtained propagating derivatives backward corresponding forward models learned separately 
optimization trajectories complete inverse model allows learner synthesize actions needed follow desired trajectory 
local optimization formulation ectively section included completeness needed remainder 
assume learning inverse model primary concern learning particular target trajectories secondary 
learning rule equation nds actions invert dynamics environment current point state space regardless point desired trajectory 
terms network architectures approach leads feedforward networks model local forward inverse state transition structure see 
current section consider specialized problem formulation focus particular classes target trajectories 
formulation variational calculus closely allied methods optimal control theory kirk lecun 
algorithm results form backpropagation time rumelhart hinton williams recurrent network incorporates learned forward model 
algorithm di ers algorithm inverts relationship actions sensations current point state space moves current state desired trajectory 
consider ensemble target trajectories fy de ne cost functional nx ef index target trajectories unknown function state action action parameterized function state target previous formulation base learning rule stochastic gradient gradient evaluated particular sample trajectory nx gradient cost functional obtained calculus variations see lecun narendra parthasarathy 
letting represent vector partial derivatives respect letting represent vector partial derivatives respect appendix shows gradient recurrence relations state units input units state units action units predicted predicted state sensation units units recurrent network forward model 
boxes labeled unit delay elements 
jacobian matrices evaluated time step stands jacobian matrices derivatives state function 
expression describes backpropagation time recurrent network incorporates forward model state function output function 
shown recurrent network essentially network explicit connections unit delay elements state current state 
backpropagation time propagates derivatives backward recurrent connections described recurrence relations equations 
local optimization case equations computing gradient alternatively thought special case backprop error signals state units cf 
jordan 
involve multiplication performance error series transpose jacobian matrices unknown priori 
approach estimating unknown factors learn forward models underlying mappings propagate signals backward models 
jacobian matrices equations replaced estimated quantities computing estimated stochastic gradient sections pursue presentation distal learning approach context problem domains 
rst section describes learning static environment second section describes learning dynamic environment 
sections utilize local optimization formulation distal learning 
static environments environment said static ect action independent history previous actions 
static environments mapping actions sensations characterized set state variables 
environments provide simpli ed domain study learning inverse mappings 
section illustrative static environment focus issues ects nonconvex inverse images transformation sensations actions problem goal directed learning 
problem consider learning forward inverse kinematics joint planar arm 
shown conguration arm characterized joint angles corresponding pair cartesian variables 
function relates variables forward kinematic function 
obtained closed form elementary cos cos cos sin sin sin link lengths 
forward kinematic function mapping cartesian position inside boundary workspace nite number joint angle con gurations achieve position 
implies inverse kinematic relation function nite number inverse kinematic functions corresponding particular choices points inverse images cartesian positions 
problem learning inverse kinematic controller arm nding particular inverse possible inverse mappings 
joint planar arm 
controller arm forward inverse mappings associated arm kinematics 
simulations simulations reported joint angle con gurations arm represented vector cos cos cos vector joint angles 
ectively restricts motion joints intervals respectively assuming component con guration vector allowed range interval 
cartesian variables represented real numbers ranging 
simulations variables represented directly real valued activations units network 
units represent joint angle con gurations units represent cartesian positions 
details simulations provided appendix nonconvexity problem approach learning inverse mapping provide training pairs learner observing input output behavior environment reversing role inputs outputs 
approach referred earlier direct inverse modeling proposed domain inverse kinematics 
idea randomly sample points joint space real arm evaluate forward kinematic function obtaining training pairs learning controller 
controller learned optimization cost functional ef output controller 
discussed earlier di culty direct inverse modeling approach optimization cost functional equation necessarily yield inverse kinematic function 
problem arises nature forward kinematic function cf 

particular randomly sampled points happen map endpoint training data provided controller 
particular manner inconsistency resolved depends form cost functional sum squared error equation yields arithmetic average points map endpoint 
average joint space necessarily yield correct result cartesian space inverse images nonlinear transformations necessarily convex 
implies output controller may error system converged minimum cost functional 
demonstrate inverse kinematics joint arm convex 
see nonconvexity expected ect direct inverse modeling procedure conducted simulation feedforward network hidden layer learn inverse kinematics joint arm 
simulation provided target vectors network sampling randomly uniform distribution joint space 
input vectors obtained mapping target vectors cartesian space equation 
initial value root mean square rms joint space error ltered rst trials 
learning trials ltered error reached asymptote value 
eld plotted providing desired cartesian vectors inputs network obtaining joint angle outputs mapping outputs cartesian space equation 
resulting vector eld shown 
seen substantial error positions workspace learning algorithm converged 
training continued loci errors continue shift rms error remains approximately constant 
error partially due nite learning rate random sampling nonconvexity kinematics 
dotted con guration average joint space solid con gurations 
procedure see widrow stearns error remains learning rate taken zero 
account error due nonconvexity inverse kinematic relation 
note example error observed reproduced lower left portion 
demonstrate distal learning approach nd particular inverse kinematic mapping 
performed simulation initialized incorrect controller obtained direct inverse modeling 
simulation utilized forward model trained previously forward model trained direct inverse modeling trials 
grid evenly spaced positions cartesian space provide targets second phase distal learning procedure 
trial error cartesian space passed backward forward model change weights controller 
learning trials passes grid targets resulting vector eld plotted 
shown gure vector error decreases zero workspace controller converging particular inverse kinematic function 
grid necessary procedure works cartesian positions sampled randomly trial 
near asymptotic performance direct inverse modeling 
vector represents error particular position workspace 
additional constraints virtue distal learning approach ease possible incorporate additional constraints learning procedure bias choice particular inverse function 
example minimum norm constraint realized adding penalty term form propagated errors output controller 
temporal smoothness constraints realized incorporating additional error terms form 
constraints de ned sites network including output units hidden units forward model 
possible provide additional contextual inputs controller learn multiple contextually appropriate inverse functions 
aspects distal learning approach discussed detail jordan 
goal directed learning direct inverse modeling learn goal directed manner 
learn speci cartesian target procedure sample su ciently large region joint space rely interpolation 
heuristics may restrict search certain regions joint space heuristics essentially prior knowledge near asymptotic performance distal learning 
nature inverse mapping equally incorporated distal learning procedure 
distal learning fundamentally goal directed 
performance error speci cartesian target capable nding exact solution particular target small number trials 
demonstrated simulation shown 
starting controller shown particular cartesian target successive trials 
shown network reorganizes error small vicinity target 
additional trials error target zero oating point resolution simulation 
approximate forward models conducted additional simulation study ects inaccuracy forward model 
simulation varied number trials allocated learning forward model 
controller trained rms criterion target positions 
shown results demonstrate accurate controller inaccurate forward model 
fewer trials needed learn target positions criterion accurate forward model learning rate accurate forward models relatively slight 
reasonably goal directed learning 
cartesian target lower right portion gure successive trials 
error vectors close zero vicinity target 
rapid learning obtained forward model trained trials average rms error forward model trials compared trials 
comparisons direct inverse modeling problems output variables unrealistic acquire inverse model entire workspace 
cases goal directed nature distal learning particularly important allows system obtain inverse images restricted set locations 
forward model learned restricted region action space general priori method determining appropriate region space sample 
distal learning goal directed acquisition inverse model inherently goal directed acquisition forward model 
direct inverse modeling distal learning entirely goaldirected problem important consider reasonable acquire inverse model forward model non goal directed controller training trials criterion forward model training trials number trials required train controller rms criterion function number trials allocated training forward model 
point runs 
manner 
issue problem dependent depending nature function learned nature class functions represented learner nature learning algorithm 
worth noting inherent tradeo complexity inverse model forward model due fact composition identity mapping 
tradeo suggests complementarity classes problems direct inverse modeling distal learning appropriate 
believe distal learning generally useful inaccurate forward model generally acceptable inaccurate inverse model 
cases may preferable learn inaccurate forward model speci cally inverted desired set locations learning inaccurate inverse model directly relying interpolation 
dynamic environments step dynamic models illustrate application distal learning problems environment state consider problem learning control joint robot arm 
controlling dynamic robot arm involves nding appropriate torques cause arm follow desired trajectories 
problem di cult nonlinear couplings motions links ctitious torques due rotating coordinate systems 
arm consider link version arm shown previously 
con guration point time described joint angles cartesian variables 
kinematic function relates joint angles cartesian variables obtained letting equal zero equation cos cos sin sin link lengths 
state space arm space positions velocities links 
essence robot arm dynamics mapping torques applied joints resulting angular accelerations links 
mapping dependent state variables angle angular velocity 
represent vector joint angles angular velocities angular accelerations respectively represent torques 
terminology earlier sections constitute state action 
convenience take represent state see discussion 
obtain analog state function equation di erential equation derived angular motion links standard newtonian lagrangian dynamical formulations craig inertia matrix matrix terms vector torque due gravity 
interest physics equations se functional relationships de ne 
particular obtain state function rewrite equation solving accelerations yield existence assured craig 
equation expresses state dependent relationship torques accelerations moment time state variables torque acceleration computed substitution equation 
refer computation forward dynamics arm 

controller arm forward inverse mappings associated arm dynamics 
inverse mapping torques accelerations obtained interpreting equation proper manner 
state variables acceleration substitution equation yields corresponding torques 
algebraic computation refered inverse dynamics 
clear inverse dynamics forward dynamics complementary computations substitution equation equation yields requisite identity mapping 
relationships torques accelerations states summarized 
useful compare gure kinematic example shown 
kinematic case dynamic case forward inverse mappings learned xed functions instantaneous values relevant variables 
dynamic case due fact structural terms dynamical equations terms explicit functions state time 
dynamic case thought generalization kinematic case additional contextual state variables needed index mappings learned 
instantiation acceleration playing role state 
general systems described di erential equations convenient de ne notion state terms time derivative state variables accelerations case arm dynamics 
de nition entirely consistent development preceding sections di erential equations equation simulated discrete time computer numerical algorithm compute accelerations de ned equation convert positions velocities current time step positions velocities time step 
perspective essentially underlying local optimization formulation distal learning 
ampli cation noise di erentiated signals realistic implementations forward dynamical models utilize positions velocities accelerations 
cases numerical integration equation incorporated part forward model 

learning dynamic forward model forward model arm dynamics network learns prediction acceleration position velocity torque appropriate teaching signal network actual acceleration yielding cost functional ef prediction function position velocity torque weights appropriate ensemble control trajectories cost functional minimized set weights best approximates forward dynamical function equation 
important di erence kinematic problems dynamic problems generally infeasible produce arbitrary random control signals dynamical environments considerations stability 
example equation allowed stationary white noise stochastic process variance approaches nity random walk 
yields data little learning model 
closely related approaches overcome problem 
rst approach produce random equilibrium positions arm random torques 
de ne new control signal augmented arm dynamics kv kp xed constants kp kv 
random control signal equation acts virtual equilibrium position arm hogan augmented dynamics generate training data learning forward model 
second approach utilizes equation di ers rst approach choice control signal 
random controls target trajectories controls trajectories utilized second phase learning train forward model 
approach equivalent simple xed gain proportional derivative pd feedback controller stabilize system set trajectories generate training data 
auxiliary feedback controller similar feedback error learning kawato direct inverse modeling atkeson miller approaches 
discussed second approach advantage require forward model learned separate phase 
pd controller device output weighted sum position errors velocity errors 
position errors velocity errors multiplied xed numbers gains summed 
feedforward controller ff fb feedback controller arm forward model composite control system 
composite control system composite system controlling arm shown 
control signal diagram torque sum components ff fb ff feedforward torque fb optional feedback torque produced auxiliary feedback controller 
feedforward controller learning controller converges model inverse dynamics arm 
early phases learning feedforward controller produces small random torques major source control provided error correcting feedback controller 
feedforward controller begins learned produces torques allow system follow desired trajectories smaller error role feedback controller diminished 
limit feedforward controller converges perfect inverse model feedforward torque causes system follow desired trajectory error feedback controller discussed statement entirely accurate 
learning algorithm provides form error correcting feedback control 
silent assuming disturbances 
system shifts automatically feedback dominated control feedforward dominated control course learning see atkeson kawato miller 
error signals utilized learning inverse dynamics prediction error performance error prediction error train forward model discussed previous section 
forward model partially learned performance error training inverse model 
error propagated backward forward model feedforward controller weights changed 
process minimizes distal cost functional ef simulations arm modeled rigid body dynamics assuming mass uniformly distributed links 
links modeled thin cylinders 
details physical constants provided appendix simulation forward dynamics arm carried fourth order runge kutta algorithm sampling frequency hz 
control signals provided networks sampled hz 
standard feedforward connectionist networks simulations 
feedforward networks simulation controller forward model connectivity shown box labelled arm replaced forward model 
controller forward model feedforward networks single layer logistic hidden units 
simulations state variables torques accelerations represented directly real valued activations network 
details networks simulations provided appendix nal simulation reported learning forward model learning inverse model carried separate phases 
forward model learned initial phase random process drive augmented dynamics equation 
random process white noise position signal chosen uniformly workspace shown 
learning forward model terminated ltered rms prediction error reached rad noted possible include numerical integration part forward model learn mapping output predicted state 
approach may preferred systems di erentiation noisy signals concern 
workspace grey region target paths 
trajectories move left right paths shown 
trial trial performance learned trajectories 
learning 
learning trials 
learning auxiliary feedback controller learning forward model system learned control arm paths shown 
target trajectories minimum jerk trajectories second duration 
auxiliary proportional derivative pd feedback controller position gains rad velocity gains rad 
shows performance particular trajectory learning pd controller th learning trial 
corresponding waveforms shown 
middle graphs gures show feedback torques dashed lines feedforward torques solid lines 
seen early phases learning torques learning 
top graphs dotted line angle solid line actual angle 
middle graphs dotted line feedback torque solid line feedforward torque 
time tangential velocity time time torque torque time time angle angle learning 
top graphs dotted line angle solid line actual angle 
middle graphs dotted line feedback torque solid line feedforward torque 
time tangential velocity time time torque torque time time angle angle generated principally feedback controller phases torques generated principally feedforward controller 
learning auxiliary feedback controller interesting consequence goal directed nature forward modeling approach possible learn inverse dynamic model auxiliary feedback controller 
see case rst note minimum jerk trajectories smooth trajectories change slowly time 
implies successive time steps essentially repeated learning trials input vector controller converges rapidly solution local region state space 
trajectory evolves solution tracks input controller produces reasonably torques prior learning 
put way distal learning approach form error correcting feedback control parameter space controller 
error correction eventually give way convergence weights system learn inverse model useful feature algorithm tends stabilize arm learning 
behavior demonstrated simulations shown 
gure shows performance rst learning trial function learning rate 
results demonstrate changing learning rate essentially changes gain error correcting behavior algorithm 
learning rate set system produces nearly perfect performance rst learning trial 
feature algorithm important clarify meaning learning curves obtained distal learning approach 
shows learning curves 
lower curve rms error obtained learning rate 
upper curve rms error obtained learning rate temporarily set zero learning trial 
setting learning rate zero allows ects learning evaluated separately error correcting behavior 
curves clearly reveal early trials main contributor performance error correction learning 
combining forward dynamics forward kinematics combining forward dynamic models section forward kinematic models preceding section possible train controller cartesian target trajectories 
dynamic model kinematic model learned parallel essentially performance decrement associated combined system 
simulations nd learning times increase approximately percent cartesian targets joint angle targets 
mu mu mu mu mu mu performance rst learning trial function learning rate 
rad rms error trial mu mu rms error zero non zero learning rates 
learning forward model controller simultaneously distal learning approach involves forward model train controller learning forward model precede learning controller 
necessary learn forward model entire state space learning controller local forward model generally su cient 
discussed distal learning approach require exact forward model approximate forward models su ce 
facts conjunction smooth trajectories imply possible learn forward model controller simultaneously 
auxiliary feedback controller needed stabilize system initially forward model begins learned learning algorithm tends stabilize system 
controller begins learned errors decrease ects feedback controller diminish automatically 
system bootstraps inverse model 
simulation shown demonstrates feasibility approach 
trial trial learning forward model controller simultaneously 
performance learning target trajectories 
performance learning trials 
architecture previous experiments system learned target trajectories starting small random weights controller forward model 
time step passes backpropagation algorithm required pass prediction error change weights forward model second pass performance error change weights controller 
auxiliary proportional derivative pd feedback controller position gains rad velocity gains rad 
shown gure system converges acceptable level performance learning trials 
simultaneous learning procedure requires presentations target trajectories achieve level performance comparable phase learning procedure simultaneous procedure fact cient phase learning dispenses initial phase learning forward model 
advantage weighed certain disadvantages particular possibility instability enhanced error gradients obtained partially learned forward model 
practice nd necessary smaller step sizes simultaneous learning approach phase learning approach 
preliminary experiments shown worthwhile choose specialized representations enhance speed forward model converges 
done separately state variable input torque input 
dynamic environments simpli ed models previous section demonstrated temporal component distal supervised learning problem addressed knowledge set state variables environment 
assuming prior knowledge set state variables tantamount assuming learner prior knowledge maximum delay time action issued time ect observed sensation vector 
current section preliminary results aim broaden scope distal learning approach address problems maximum delay known see werbos 
simple example problem robot arm required certain con guration time unknown trajectory open interval unconstrained 
approach solving problems learn step forward model arm dynamics backpropagation time recurrent network includes forward model controller jordan kawato 
problems involving delayed temporal consequences feasible desirable learn dynamic forward model environment environment complex solving task hand require knowledge evolution state variables 
consider example problem predicting height splash water stones varying size dropped pond 
useful step dynamic model learned uid dynamics pond 
control problem produce particular desired heights may necessary model uid dynamics detail 
simple forward model predicts integrated quantity splash height function size stone may su ce 
jordan jacobs illustrated approach distal learning solve problem learning balance inverted pendulum moving cart 
problem generally posed avoidance control problem corrective information provided environment signal indicate failure occured barto sutton anderson 
delay actions forces applied cart failure signal unknown arbitrarily large 
spirit foregoing discussion jordan jacobs assumed undesirable model dynamics cart pole system controller learned backpropagation time recurrent network includes step dynamic model plant 
unique trajectory may speci ed enforcing additional constraints temporal evolution actions explicit target information assumed provided nal time step 
kawato backpropagation time implemented spatially unrolled network gradients change activations weights idea onestep forward dynamic model 
see nguyen widrow application kinematic problem 
approach adopted jordan jacobs involves learning forward model output integrated quantity estimate inverse time failure 
estimate learned temporal di erence techniques sutton 
time steps failure occurs target value forward model unity output forward model error term change weights 
time steps temporal di erence error term yields increasing arithmetic series trajectory leads failure 
learned output forward model provide gradient learning controller 
particular desired outcome balancing pole described goal maximizing time failure algorithm learns controller zero minus output forward model distal error signal 
forward model jordan jacobs di ers important way forward models described 
time depends actions controller mapping forward model learn depends xed properties environment controller 
controller changed learning algorithm mapping forward model learn changes 
forward model updated continuously learning controller 
general problems forward model learns estimate integral dynamics learning forward model controller proceed parallel 
temporal di erence techniques provide distal learning approach enhanced functionality 
possible learn long term predictions adjust controllers basis quantities distal time 
learn multi step forward models 
conjunction backpropagation time provide exible set techniques learning actions basis temporally extended consequences 
discussion argued supervised learning paradigm broader commonly assumed 
distal supervised learning framework extends supervised learning problems desired values available distal technique considered example supervised learning algorithms solve reinforcement learning problem see 
consequences learner actions actions 
signi cant weakening classical notion teacher supervised learning paradigm 
section provide discussion class problems treated distal supervised learning framework 
discuss possible sources training data contrast distal supervised learning reinforcement learning 
training data obtained 
provide support argument distal supervised learning realistic classical supervised learning necessary consider possible sources training data distal supervised learning 
discuss sources refer imitation envisioning 
common ways humans acquire skills imitation 
skills dance athletics learned observing person performing skill attempting replicate behavior 
cases teacher may suggest particular patterns limb motion direct instruction appear necessary component skill acquisition 
case point acquisition children acquire speech hearing speech sounds receiving instruction articulators 
conception distal supervised learning problem involves set intention desired outcome training pairs 
learning imitation clearly desired outcomes available learner 
regard intentions possibilities 
learner may know able infer intentions person serving model 
alternatively idiosyncratic internal encoding intentions viable long encoding consistent 
example child acquiring speech may drink may observe person obtaining water uttering form water may utilize acoustic representation water distal target learning articulatory movements expressing desire drink person uses water re 
learner acquiring inverse model simulations reported intention obviously available desired outcome 
conception distal supervised learning problem set training pairs course abstraction elaborated dealing complex tasks 
complex task dance presumably easy determine choice sensory data distal targets learning procedure 
learner may alter choice targets achieved skill 
learner may need decompose task simpler tasks set intermediate goals 
suspect role external teachers help representational issues provide proximal targets directly learner 
source data distal supervised learning paradigm process refer envisioning 
envisioning general process converting goals corresponding sensory realization regard actions needed achieve goals 
envisioning involves deciding look feel perform task 
process presumably involves general deductive inductive reasoning abilities experience similar tasks 
point want emphasize envisioning need refer actions needed carry task problem solved distal learning procedure 
comparisons reinforcement learning alternative approach solving class problems discussed reinforcement learning algorithms barto sutton 
reinforcement learning algorithms assumption environment provides evaluation actions produced learner 
evaluation arbitrary function approach principle applicable general problem learning basis distal signals 
reinforcement learning algorithms updating probabilities emitting particular actions 
updating procedure evaluations received environment 
evaluation action favorable probability associated action increased probabilities associated actions decreased 
conversely evaluation unfavorable probability action decreased probabilities associated actions increased 
characteristic features reinforcement learning algorithms di er important ways corresponding features supervised learning algorithms 
supervised learning algorithms existence signed error vector evaluation 
signed error vector generally obtained comparing actual output vector target vector 
signed error vector small corresponding favorable evaluation algorithm initiates changes 
signed error vector large corresponding unfavorable evaluation algorithm corrects current action favor particular alternative action 
supervised learning algorithms simply increase probabilities alternative actions choose particular alternatives directionality signed error vector 
important distinguish learning paradigms learning algorithms 
learning algorithm utilized variety learning paradigms failure distinguish paradigms algorithms lead misunderstanding 
particularly true reinforcement learning tasks supervised learning tasks close relationships evaluative signals signed error vectors 
signed error vector converted evaluative signal bounded monotonic function norm signed pointed barto sutton anderson distinction reinforcement learning supervised learning signi cant learner repertoire actions 
error vector su ces reinforcement learning algorithms supervised learning problems 
conversely signal converted signed error vector machinery discussed see munro supervised learning algorithms reinforcement learning problems 
de nition learning paradigm manner problem naturally posed algorithm solve problem 
case basketball player example assuming environment provides directional information far left long short di erent assuming environment provides evaluative information form better best 
furthermore learning algorithms di er algorithmic complexity applied paradigms reinforcement learning algorithm solve supervised learning problem ine cient algorithms take advantage directional information 
conversely supervised learning algorithms solve reinforcement learning problems ine cient extra machinery required induce signed error vector 
summary suggested di erence reinforcement learning supervised learning reliance teacher feel argument mistaken 
distinction supervised learning paradigm reinforcement learning paradigm lies interpretation environmental feedback error signal evaluative signal coordinate system signals provided 
problems involving distal credit assignment may better conceived supervised learning problems reinforcement learning problems distal feedback signal interpreted performance error 
number di culties classical distinctions unsupervised reinforcement supervised learning 
supervised learning generally said dependent teacher provide target values output units network 
viewed limitation domains teacher 
environment provide sensory information consequences action employed making internal modi cations just teacher provided information learner directly 
idea learner rst acquires internal model allows prediction consequences actions 
internal model mechanism transforming distal sensory information consequences actions proximal information making internal modi cations 
phase procedure extends scope supervised learning paradigm include broad range problems actions transformed unknown dynamical process compared desired outcomes 
rst illustrated approach case learning inverse model simple static environment 
showed method utilizing forward model environment important advantages alternative method building inverse model directly 
advantages especially apparent cases unique inverse model 
showed idea extended usefully case dynamic environment 
case simply elaborate forward model learner controller take account current state environment 
showed approach combined temporal di erence techniques build system capable learning sensory feedback subject unknown delay 
suggested comparative study learning facilitated making distinction learning algorithms learning paradigms 
avariety learning algorithms applied particular instance learning paradigm important paradigmatic aspects learning problem nature interaction learner environment nature quantities optimized tradeo algorithmic complexity arise di erent classes learning algorithms applied problem 
research needed delineate natural classes levels paradigms algorithms clarify relationships levels 
believe research provide theoretical basis making distinctions candidate hypotheses empirical study human learning 
atkeson 

associative content addressable memories control robots 
ieee conference control 
san francisco ca 
barto 

chemotaxis cooperativity exercises neuronal learning strategies 
durbin mitchison eds computing 
reading ma addison wesley publishers 
barto sutton anderson 

neuronlike adaptive elements solve di cult learning control problems 
ieee transactions systems man cybernetics smc 
becker hinton 

spatial coherence internal teacher neural network 
tech 
rep crg tr 
toronto university 
carlson 

communication systems 
new york mcgraw hill 
craig 

robotics 
reading ma addison wesley publishers 
gelfand 

calculus variations 
englewood cli prentice hall 
goodwin sin 

adaptive ltering prediction control 
englewood cli nj prentice hall 
grossberg 

competitive learning interactive activation adaptive resonance 
cognitive science 
hinton sejnowski 

learning relearning boltzmann machines 
rumelhart mcclelland eds parallel distributed processing volume 
cambridge ma mit press 
hogan 

organising principle class voluntary movements 
journal neuroscience 
jordan 

mental practice 
unpublished dissertation proposal center human information processing university california san diego 
jordan 

serial order parallel distributed processing approach 
technical report 
la jolla ca university california san diego 
jordan 

supervised learning systems excess freedom 
coins tech 
rep 
amherst ma university massachusetts computer information sciences 
jordan rosenbaum 

action 
posner ed foundations cognitive science 
cambridge ma mit press 
jordan 

motor learning degrees freedom problem 
ed attention performance xiii 
hillsdale nj erlbaum 
jordan jacobs 

learning control unstable system forward modeling 
touretzky ed advances neural information processing systems 
san mateo ca morgan kaufmann 
kawato 

computational schemes neural network models formation control arm trajectory 
miller iii sutton werbos eds neural networks control 
cambridge mit press 
kawato furukawa suzuki 

hierarchical neural network model control learning voluntary movement 
biological cybernetics 
kirk 

optimal control theory 
englewood cli nj prentice hall 
kohonen 

self organized formation topologically correct feature maps 
biological cybernetics 


neural model adaptive hand eye coordination single postures 
science 
lecun 

learning scheme asymmetric threshold networks 
proceedings 
paris france 
lecun 

de apprentissage 
unpublished doctoral dissertation universite vi 
linsker 

self organization perceptual network 
computer 
ljung 

theory practice identi cation 
cambridge mit press 
miller 

sensor control robotic manipulators general learning algorithm 
ieee journal robotics automation 
mozer bachrach 

discovering structure environment exploration 
touretzky ed advances neural information processing systems 
san mateo ca morgan kaufmann 
munro 

dual back propagation scheme scalar reward learning 
proceedings ninth annual conference cognitive science society 
hillsdale nj erlbaum 
narendra parthasarathy 

identi cation control dynamical systems neural networks 
ieee transactions neural networks 
nguyen widrow 

truck backer upper example neural networks 
proceedings international joint conference neural networks 
piscataway nj ieee press 
parker 

learning logic 
tech 
rep tr 
cambridge ma mit sloan school management 
robinson fallside 

dynamic reinforcement driven error propagation networks application game playing 
proceedings neural information systems 
american institute physics 
rosenblatt 

principles neurodynamics 
new york spartan 
rumelhart 

learning sensorimotor programs parallel distributed processing systems 
japan joint seminar competition cooperation neural nets ii 
unpublished presentation 
rumelhart hinton williams 

learning internal representations error propagation 
rumelhart mcclelland eds parallel distributed processing volume 
cambridge ma mit press 
rumelhart smolensky mcclelland hinton 

schemata sequential thought processes pdp models 
rumelhart mcclelland eds parallel distributed processing volume 
cambridge ma mit press 
rumelhart zipser 

feature discovery competitive learning 
rumelhart mcclelland eds parallel distributed processing volume 
cambridge ma mit press 
schmidhuber 

line algorithm dynamic reinforcement learning planning reactive environments 
proceedings international joint conference neural networks 
piscataway nj ieee press 
sutton 

temporal credit assignment reinforcement learning 
coins tech 
rep 
amherst ma university massachusetts computer information sciences 
sutton 

learning predict methods temporal di erences 
machine learning 
sutton 

integrated architectures learning planning reacting approximating dynamic programming 
proceedings seventh international conference machine learning 
werbos 

regression new tools prediction analysis behavioral sciences 
unpublished doctoral dissertation harvard university 
werbos 

building understanding adaptive systems statistical numerical approach factory automation brain research 
ieee transactions systems man cybernetics 
widrow ho 

adaptive switching circuits 
institute radio engineers western electronic show convention convention record part 
widrow stearns 

adaptive signal processing 
englewood cli nj prentice hall 
appendix obtain expression gradient equation utilize continuous time analog derive necessary condition convert result discrete time 
simplify exposition compute partial derivatives respect actions weights resulting equations converted gradients weights premultiplying transpose jacobian matrix 
represent action trajectory represent sensation trajectory 
trajectories linked forward direction dynamical equations action vector assumed depend current state target vector functional minimized integral dt continuous time analog equation suppressed subscript simplify notation 
represent vectors time varying lagrange multipliers de ne lagrangian lagrange multipliers sensitivities cost respect variations respectively 
sensitivities partial derivatives problem converted discrete time interested solving 
necessary condition optimizing solution satisfy euler lagrange equations gelfand dt dt moment time 
equations equivalent function space familiar procedure setting partial derivatives equal zero 
substituting simplifying obtain euler approximation equations written discrete time recurrence relations sampling period discrete approximation 
utilize recurrence relations discrete time network sampling period absorbed network approximations continuous time mappings 
network approximation include identity feedforward component account initial autoregressive term equation 
equation transpose jacobian matrix yields equations main text 
appendix networks simulations standard feedforward connectionist networks see rumelhart hinton williams 
activation functions input units output units networks linear hidden units logistic asymptotes 
input target values kinematic arm simulations joint angles represented vector cos cos cos cartesian targets scaled lie fed directly network 
dynamic arm simulations variables joint angles angular velocities angular accelerations torques scaled fed directly network 
scaling factors chosen scaled variables ranged approximately 
initial weights initial weights chosen randomly uniform distribution interval 
hidden units single layer hidden units networks 
attempt optimize number hidden units connectivity 
parameter values learning rate kinematic arm simulations 
momentum set 
dynamic arm simulations learning rate cases simulation shown learning rate manipulated explicitly 
momentum dynamic arm simulations 
appendix dynamic arm modeled rigid body mechanics 
link lengths proximal link distal link 
masses links kg kg 
mass assumed distributed uniformly links 
moments inertia links centers mass ii mil yielding kg kg proximal distal links respectively 

