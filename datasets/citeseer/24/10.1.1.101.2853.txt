global optimization statistical functions simulated annealing william university north carolina nc gary southern methodist university dallas tx john rogers north texas state tx may statistical methods rely numerical optimization estimate model parameters 
unfortunately conventional algorithms fail 
converge assurance global local optimum 
test new optimization algorithm simulated annealing econometric problems compare common conventional algorithms 
simulated annealing find global optimum fail difficult functions robust algorithm 
promise simulated annealing demonstrated econometric problems 
computational performed university texas center high performance computing 
hans gary david randolph beard herman michael david goldberg stan kerr david levy daniel richard michael helpful suggestions comments previous drafts 
anonymous referees provided valuable suggestions 
albert helpful literature survey 
computational help art ava 
sarah barron jan byrd robert luther keeler john university texas center high performance computing provided invaluable help 
econometric methods nonlinear squares generalized method moments maximum likelihood method rely optimization estimate model parameters 
methods lay theoretical foundation estimators numerically estimate models 
unfortunately may difficult algorithms may fail 
maximum likelihood method cramer lists number unpleasant possibilities algorithm may converge reasonable number steps may head infinitely large parameter values loop point time 
algorithm may difficulty ridges plateaus 
faced difficulties researcher reduced trying different starting values cramer finch 

algorithm converges assurance converged global local optimum conventional algorithms distinguish 
sum poor match power methods numerical algorithms implement 
new different optimization algorithm simulated annealing potentially improves match 
explores function entire surface tries optimize function moving uphill downhill 
largely independent starting values critical input conventional algorithms 
escape local optima go find global optimum uphill downhill moves 
simulated annealing stringent assumptions regarding function conventional algorithms need continuous 
relaxed assumptions easily deal functions ridges plateaus 
optimize functions defined parameter values 
compares 
implementation simulated annealing conventional algorithms econometric models 
results may help guide researchers 
model example literature multiple minima judge 
pp 
second rational expectations exchange rate model 
third model efficiency study banking industry translog cost frontier system fourth fits neural network chaotic time series 
models vary greatly size 
judge model parameters exchange rate model neural network cost study 
multivariable optimization algorithms library represent conventional algorithms 
sensitive starting values different sets cases 
introduces extensions algorithm 
modification checks global optimum achieved allows researcher restrict optimization subset parameter space useful understanding function 
third extension allows researcher determine critical initial parameter algorithm final directs selection parameters control robustness algorithm 
allows researcher minimize execution time algorithm 
find simulated annealing quite powerful 
judge function simulated annealing finds global optimum routines 
rational expectations model conventional algorithms optimize offer reason failure 
diagnostic tool simulated annealing determines problem 
problem eliminated conventional algorithms rarely find optimum simulated annealing easily 
conventional algorithms optimize translog cost frontier model simulated annealing easily 
neural network simulated annealing finds better optimum conventional algorithms 
shown nature function virtually impossible method find global optimum 
essentially simulated annealing provides features cost increase execution time single run algorithm 
compared multiple runs conventional algorithms test different starting values competitive 
cases simulated annealing competitive single run conventional algorithm 
shown appendix computer power increasing rapidly resources needed simulated annealing soon available researchers difficult problems 
organized follows 
section ii describes simulated annealing detail compares conventional algorithms section iii compares performance test problems algorithms 
section introduces extensions simulated annealing 
section iv briefly reviews extensions introduces 
summarizes strengths weaknesses simulated annealing 
appendix examines global optimization algorithm genetic algorithm finds simulated annealing superior 
ii 
algorithms conventional algorithms limitations conventional optimization algorithms newton raphson fletcher powell try move uphill maximization assumed general discussion algorithms iterative manner 
general terms starting point determine best direction step length head uphill 
moving process repeated stopping criteria achieved 
statistical packages sas tsp rats algorithms 
general reviews judge 
press 
rigorous analysis dennis schnabel 
operation similar blind man walking hill knowledge hill comes passes feet 
hill predictable fashion reach top easy imagine confounding terrain 
conventional algorithms function predictable assuming function approximately quadratic 
unfortunately statistical functions may approximately quadratic 
implicit assumption common conventional algorithms function optimum local optimum global optimum 
kendall stuart show asymptotically likelihood functions optimum small sample properties unknown 
taken easy see conventional algorithms may difficulty finding maximum 
section shows simulated annealing takes different approach optimizing functions involves significantly fewer limitations 
simulated annealing continuous variable problems simulated annealing roots thermodynamics studies system thermal energy 
description cooling metal motivates algorithm 
slow cooling annealing metal arrives low energy state 
inherent random fluctuations energy allows annealing system escape local energy minima achieve global minimum 
cooled quickly escape local energy minima fully cooled may contain energy annealed metal 
simulated annealing attempts minimize analogue energy manner similar annealing find global minimum 
details press 
pp 

early simulated annealing algorithms considered combinatorial systems system state depends configuration variables 
best known traveling salesman problem tries find minimum trip distance connecting number cities 
combinatorial simulated annealing successfully computer circuit design kirkpatrick 
wong 
pollution control special case programming drexl neural networks wasserman schwartz reconstruction structures image processing 

global optimization algorithms introduced years 
include adaptive random search 
genetic algorithms goldberg filled function method multi level methods kan method stochastic differential equations 

vanderbilt 

modified simulated annealing continuous variable problems 
implementation simulated annealing continuous variable problems appears offer best combination ease robustness study 
complete description summary follows algorithm pseudo computer code provided appendix essential starting parameters maximize function initial temperature starting vector parameters step length note vectors length number parameters model 
upper case refers vectors lower case scalars exception temperature 
function evaluation starting point value recorded 
new chosen varying element uniformly distributed random number element function value computed 
greater accepted set algorithm moves uphill 
largest recorded best current value optimum 
equal metropolis criteria decides acceptance thermodynamics motivates criteria 
value computed compared uniformly distributed random number 
greater new point accepted updated algorithm moves downhill 
rejected 
factors decrease probability downhill move lower temperatures larger differences function value 
note decision downhill moves contains random element 
steps elements parameters set user step length vector adjusted moves accepted 
goal sample function widely 
greater percentage points accepted relevant element enlarged 
temperature increases number rejections decreases percentage acceptances 
times loops temperature reduced 
new temperature 
lower temperature downhill move number rejections increase step lengths decline 
addition point tried new temperature current optimum 
smaller steps starting current optimum focuses attention promising area 
algorithm ends comparing values largest function values temperature reduction optimum function value 
differences algorithm terminates 
criteria helps ensure global maximum reached 
note simulated annealing builds rough view surface moving large step lengths 
temperature falls step length decreases slowly focuses promising area 
algorithm escape local maxima downhill moves 
eventually algorithm converge function global maximum 
simulated annealing potential advantages conventional algorithms 
escape local maxima 
thermodynamic terms conventional algorithms quench simply heading current hill regard simulated annealing moves uphill downhill 
function need approximately quadratic 
fact need differentiable 
successfully demonstrate simulated annealing parabolic function punctured holes 
benefit step length gives researcher valuable information function 
element large function flat parameter 
determined function evaluations points better measure flatness gradient evaluation single point 
simulated annealing identify corner solutions corner functions don exist region 
important advantage simulated annealing maximize functions difficult impossible optimize 
demonstrated section test problems 
sole drawback simulated annealing required computational power problem disappearing disappeared 
appendix describes continuing improvements occurring high performance computing 
briefly power cray supercomputer easily put desktop trend shows sign slowing 
simulated annealing attractive option difficult functions 
iii 
simulated annealing compared conventional algorithms comparison framework previous section demonstrated simulated annealing promise optimizing statistical functions 
see promise holds simulated annealing compared conventional algorithms different statistical problems 
purpose see simulated annealing works problems section iv discusses improving algorithm 
multivariate optimization algorithms math library edition chosen comparison simulated annealing 
library chosen quality availability 
routines simplex algorithm conjugate gradient algorithm numerical derivatives quasi newton algorithm numerical derivatives 
numerical derivative versions chosen computing analytical derivatives difficult functions quite complex simplex algorithm derivatives 
conventional algorithms sensitive starting values single test different randomly selected starting values 
number gives algorithm chance finding optimum 
plus typical researcher 
range select terribly critical 
simulated annealing starting values problem computer time constraints prevented approach 
cases results verified way 
test functions judge 
pp 
provide test function 
nonlinear squares problem written minimization form local minima 
parameters observations 
small artificial serves convenient test literature optimization algorithms local minima 
seen conventional algorithms distinguish simulated annealing difficulty 
note optimized minimization form 
second function derives rational expectations version monetary theory exchange rate determination woo 
model consists structural relationships log levels money stock price level exchange rate output respectively star superscripts denote variables domestic economy 
equations represent standard stock adjustment money demand equations interest rate adjustment coefficients countries 
purchasing power parity uncovered interest parity hold 
stochastic specification model completed assuming exogenous variables system evolve stable ar representation parameter matrix roots det bl lie unit circle 
solution estimation procedure follows outlined employs stable vector arma representation variables system implied model 
derive representation write model parameter matrices containing elements vector structural disturbances 
solution form results stable arma representation elements restricted analytically cancel unstable root ar polynomial 
indicated estimates parameters model may obtained minimization respect system parameters ar parameters subject complex nonlinear restrictions imposed parameters elements transition maximum likelihood model model minimization sum squares described wilson 
third problem study firm production efficiency system frontier cost function input share equations see lovell 
error terms cost equation capture effects types inefficiency random error 
error terms share equations linked error terms cost equation errors input share selection results inefficiency raising costs 
making assumptions concerning various disturbance terms estimation system carried minimizing function derived likelihood function 
estimation carried real world data institutions involves parameters observations 
fourth test function comes neural network literature 
neural nets economic applications see baum white surveys follows gallant white 
neural net approximate unknown function neural networks organized layers input layer output layer 
neural net vector length input layer 
vectors weights layer 
layer hidden output input output layers seen 
hidden layer single hidden layer neural network 
function hidden unit activation function logistic gallant white vector length hidden output layer weights 
gallant white propose neural net approximate unknown function generates chaotic series 
test approach chaotic series length generated discrete version mackey glass equation useful generator data study chaotic economic financial series generates data similar financial markets 
vectors chosen minimize neural net parameters chosen approximates function generates difficult optimization problem conventional wisdom function local minima assertion quantified 
analysis element multiplied element known bias term 
values yield parameters train fit neural net function 
values chosen gallant white perform fitting function series length problem moderate size 
computing environment computing done university texas center high performance computing austin cray mp cray mp supercomputers computing done site level performance needed 
programs written fortran fortran includes features simplify writing programs arrays extensions cft compiler versions 
bit representation floating point numbers single precision cray represents double precision vax ibm mainframe pc worlds 
cray difficulty 
floating point error occurs program terminates 
different ibm mainframe world software correction takes place execution continues 
appear results generated cray may carry computers 
cray handles unusually wide range numbers floating point errors occur algorithm region solution 
situations computer floating point errors better 
cray lack floating point errors limit lessons learned 
results judge function table shows results algorithms judge model 
algorithms run times different starting values model parameters ones algorithms 
uniformly distributed random number generator selected simulate researcher uncertainty best starting value 
routines suggestions algorithm inputs maximum number function evaluations set large value avoid early termination 
neutral values chosen 
inputs simulated annealing reported table 
top table lists results algorithms 
cases converged local minima global minimum value nearby local minimum value 
conventional algorithms split simulated annealing global minimum 
course foreseen conventional algorithms split optima design different starting values 
demonstrate simulated annealing find global optima 
effect early simple comparison 
remaining part table lists mean number function evaluations mean execution time runs 
expected simplex algorithm takes longer conventional algorithms press 

longer time simulated annealing largely due conservative suggestions parameters appropriate difficult functions 
fact grid search resolution accomplished number function evaluations note fine resolution depends low dimensionality problem 
practice coarser grid search coupled conventional algorithm 
section iv shows number function evaluations reduced 
simulated annealing requires execution time conventional algorithms 
simulated annealing real promise difficult functions 
table shows results simulated annealing run 
lists values input parameters 
exception values suggested initial temperature chosen step length approximately parameters different temperatures tried achieved 
size ensures function searched 
correspondingly initial value algorithm adjusts initial parameter important 
parameter chosen ensure solution converged global maximum 
slightly larger final tolerances quasi newton simplex algorithms conjugate gradient documentation silent matter 
table lists intermediate output temperature falls 
selected temperatures current best parameter values resulting function value shown 
listed number downhill uphill moves 
downhill evaluations accepted uphill moves accepted metropolis criteria 
results indicate step length chosen correctly half functions evaluations accepted recall step length adjusts ensure 
number function evaluations temperature 
step length parameters shown 
temperature falls observe algorithm closing global minimum 
falling temperature metropolis criteria causes fewer uphill moves accepted 
rise rejections turn shrinks step length 
conjunction algorithm starting current optimum new temperature smaller step length focuses algorithm promising area 
final part table shows results produced successful termination algorithm 
function shows promise simulated annealing global minimum times 
results conclusive having trouble finding global minimum conventional algorithms performed 
faced function researcher try different starting values find global minimum 
functions provide difficult tests 
results rational expectations model objective function rational expectations exchange rate model difficult minimize part effectively exist parameter values 
regions function value complex elements covariance go infinity 
regions encountered objective function value set force termination conventional algorithms effect simulated annealing described 
unfortunately way routines terminate regions encountered 
values slightly different marked reason failure 
conventional algorithms experienced great difficulty 
runs simplex quasi newton algorithms half terminated due floating point errors indicated algorithm region solution due large values needed cause floating point error cray 
accounting runs reached regions function exist simplex algorithm possible solutions quasi newton algorithm smaller objective function values simplex algorithm produced 
possible solutions zero gradients values model parameters varied greatly behavioral parameters ar coefficients forecasting equation 
conjugate gradient algorithm performed slightly better 
run resulted floating point error ended algorithm entered region 
runs reported objective function values just slightly larger reported quasi newton algorithm values quite bit larger 
quasi newton result behavioral parameters varied greatly value gradients quite zero 
conventional algorithms failed optimize function runs resulted set parameter values solution zero gradient 
indicate function appears flat subset model parameters 
conventional wisdom feature rational expectations models 
simulated annealing conventional algorithms experienced difficulty 
converged different optima different starting values seeds random number generator 
function modified search restricted region parameter space find explanation 
parameter region boundary marked simulated annealing chooses value inside range proceeds usual 
outside objective function calculated large value returned caused point rejected 
limits algorithm inside boundary 
previously mentioned modification objective function regions exist excludes algorithm regions 
ability focus area minimization distinct advantage simulated annealing 
region enlarged successive runs interesting phenomena occurred optimal value parameter interest rate elasticity money demand followed upper bound 
suggested function decreased parameter algorithm minimum near 
explore situation minima plotted elasticity varying parameters held constant 
plots showed interest rate elasticity parameter achieved minimum boundary 
function appeared ditch parameter boundary expanded minimum point function followed wandering ditch 
explains problems conventional algorithms 
simulated annealing useful diagnostic tool difficult functions 
noted may possible identify problems conventional algorithms examining hessian 
unfortunately idea done routines allow user recover hessian 
continue comparison algorithms interest rate elasticity parameter set experiments repeated 
reduces number parameters model 
table contains results conventional algorithms modified function 
category lists outcomes final value objective function 
includes errors function evaluation solutions 
sum possibilities yields number non floating point error returns floating point errors immediately terminate program function value unknown 
second category lists number floating point error returns 
cray supports wide range floating point values caused control algorithm floating point errors indicate algorithm far removed solution size values produce floating point errors cray 
number runs previous category sums total number runs 
category reports minimum value default possibility 
final category lists results reported algorithm 
simplex algorithm performed poorly floating point error terminations terminations near presumed minimum 
quasi newton algorithm somewhat better finding presumed minimum total times 
reported errors 
moderate number runs researcher difficulty interpreting results obtained algorithms 
conjugate gradient algorithm correct minimum times 
objective function values count model parameters similar identical 
algorithm proved fairly useful researcher function half runs terminated successfully 
simulated annealing algorithm employed 
different starting values check algorithm results due computational load function method 
algorithm run twice different starting values model parameters different seed random number generator inside algorithm 
generator selects points inside step length analysis selects uphill moves metropolis criteria 
algorithm follows completely different path runs 
terminates point assurance simulated annealing global minimum 
change runs judge function set discussed 
selection initial temperature important consideration 
initial temperature low step length small area containing optimum may missed 
high step length large excessively large area searched 
step length extremely large floating point errors may result running large values function 
method employed find 
set temperature reduction parameter 
finding minimum goal quickly find temperature step length began decline 
occurred temperature 
initial temperature chosen step length slightly smaller lower temperature allowed shorter execution time 
interestingly temperature parameters step length implies function quite narrow parameters 
function modified include bounds 
lower bound set upper model parameter range include economically plausible values 
model run bounds solution bounds experience translog cost frontier model discussed section showed floating point errors occur due large parameter values run function 
initial runs cray job classification allowed jobs execute minutes corresponds approximately longer job classification long turn times days time exploratory runs category 
runs different random number generator seeds starting values algorithm converged approximately objective function value 
difference reported optimal model parameters order step length runs minimizing region 
implies converging point 
certain run job classification resulted solution region 
successful runs conventional algorithms point 
results shown table 
test algorithm starting values mentioned shorter runs random number seed shorter run run extended job category point 
different paths taken optimum 
addition elements gradient point approximately zero 
note optimal function value differs best quasi newton algorithm insignificant amount 
seen half function evaluations accepted expected 
simulated annealing demonstrated advantages rational expectations model 
able identify modification introduced reason conventional algorithms minimize function 
second problem eliminated simulated annealing consistent finding minimum conventional algorithms generous accounting 
particularly true simplex quasi newton algorithms considered generously 
single run simulated annealing requires substantially greater execution time ameliorated large number runs conventional algorithm researcher sure robustness estimated results 
section iv shows execution time simulated annealing substantially reduced model point competitive single run conventional algorithm 
results translog cost frontier model objective function translog cost frontier model easier rational expectations function errors evaluation floating point errors 
table reports results conventional algorithms format follows table 
part table shows quasi newton algorithm unstable experienced floating point errors 
unfortunately run executions problem 
instance simplex run executed unintentionally hour minutes function evaluations 
clearly starting values useful 
exploring runs extended execution times may simply exhibiting pathological behavior number function evaluations limited 
number function evaluations simplex algorithm limited yields maximum execution time seconds 
similar choice conjugate gradient algorithm runs resulted run executions limit chosen constrained execution time run minutes surely time find optimum 
constraint function evaluations applied quasi newton algorithm experience problem 
table shows floating point errors occurred simplex algorithm 
runs terminated normally reached function evaluation limit 
normal returns yielded objective function values approximately appears plateau height model parameter values varied dramatically 
category function evaluations included best value objective function 
slightly smaller value earlier trial unlimited function evaluations 
function evaluations value 
starting value lead number function evaluations constrained 
exceedingly slow progress minimum may indicate function steep valleys simplex algorithm may difficulty traversing press 

confirmation hypothesis demonstrated 
simplex algorithm conjugate gradient algorithm experienced floating point errors runs ended due function evaluations 
runs produced best function values values ranged 
case elements gradient larger absolute value point forward reported gradient values absolute value terms 
runs consumed execution time runs 
terminated reported problems gradient 
runs elements gradient quite large values elements ranged report surprising 
confirms explanation offered problem simplex algorithm 
quasi newton algorithm yielded terminations due floating point errors indicating algorithm far removed solution size values produce floating point errors cray 
runs resulted returns reported normal gradient elements larger runs algorithm terminated abnormally 
gradient values 
best objective function value anomaly negative parameter values greater absolute value result discarded grounds nonsensical 
best value elements gradient large 
table shows results simulated annealing 
avoid floating point errors due large parameter values run algorithm parameter space function restricted lower bound set upper 
initial temperature produced initial step length vector elements averaged teens reasonable span expected parameters values 
rational expectations model preliminary runs different starting values seeds random number produced minima range 
run longer execution time produced refined estimate minimum 
check gradient point examined 
values typically 
values smaller produced conjugate gradient quasi newton algorithms value cause concern 
smaller chosen algorithm rerun 
results shown table 
note value objective function 
gradient elements approximately 
plot showed estimated value minimum 
gradient value occurred gradient algorithm difficulty extreme slight asymmetry function area 
simulated annealing able find minimum runs different conventional algorithms unable find minimum 
offers strong evidence simulated annealing useful algorithm optimization statistical functions 
note execution times simulated annealing conventional algorithms comparable 
results neural network table shows results conventional algorithms neural net table contains results obtained simulated annealing 
starting values chosen range preliminary showed optimal values range 
top table shows translog cost frontier model quasi newton algorithm experienced problem floating point errors 
run executions problem limits placed number function evaluations ensure sets starting values run reasonable time 
runs simplex algorithm terminated function evaluations 
minimum function value 
conjugate gradient algorithm yielded simple different story runs terminated due problems line search 
minimum function value 
quasi newton algorithm generated varied results 
runs terminated floating point errors indicates severe problem algorithm floating point errors occur large exponents algorithm surely away region possible interest 
runs terminated floating point errors reported resulted normal terminations reported possible problems step tolerance terminated function evaluations 
minimum function value best conventional algorithms 
initial runs simulated annealing temperature rises turn increases step lengths find initial temperature element step length vector cover region values 
bounds search avoided floating point errors corner solutions observed initial runs 
runs produced desired initial step lengths 
optimizing run initial temperature 
addition number function evaluations limited conserve execution time set conjunction parameters reduce likelihood algorithm quench achieve small terminal step length table shows achieved 
standard values parameters simulated annealing 
preliminary nature table shows optimal value function order magnitude smaller runs conventional algorithms 
addition execution time comparable total execution time different sets starting values conventional algorithms 
check result different starting value different seed run 
unfortunately similar function value different 
anticipated appear multiple optima function 
effort find global optimum parameter space run reduced shrinking bounds information gained solutions 
optimal vector values absolute value bounds reduced 
allowed function steeper 
combination smaller execution time desired terminal temperature allowed larger quenching 
optima twice 
patterns noticed solutions exploited search th th optimum 
vectors elements quite large typically absolute value remaining small elements typically absolute value 
table demonstrates pattern 
bounds adjusted search smaller region optima twice 
best optima value runs 
analysis neural net revealed interesting phenomena large values optimal vectors swamped values term create large absolute values logistic function bounded approximately 
fact best optima greater considerable extent neural net reduced linear combination elements vector 
simulated annealing better optimum conventional algorithms time incapable finding global optimum 
understand outcome analysis neural net 
rough lower bound number local optima calculated nature local optima examined 
enumeration local optima impractical method 
measure volume accurately called content dimensions see kendall average local optima determined local optimizer 
content total solution space determined 
division content spaces yield rough estimate number local optima 
quasi newton algorithm find local optima produced error free optima 
different length vectors random directions drawn local optima vector restart quasi newton algorithm 
length vectors increased algorithm longer converged starting point vector 
longest vector lead convergence back local optima radius convergence 
conservative different vectors length drawn time length increased converged starting point vector 
local minima examined number chosen due floating point computer time constraints average radius convergence 
hypersphere dimensions radius content see kendall 
yields content average local optima determining content solution space fraught difficulties 
instance local optima come widely spaced tight clusters average distance local optima generate content yield large value 
avoid complicated analysis content solution space taken content hyper rectangle created closest local optima bounding hyperplanes perpendicular axes cartesian space 
surely underestimates true content easy find proc cluster sas quite conservative 
content dividing content average optima yields order local optima 
optima pose problem local optima just larger convex surface egg titled ends 
surface global optimization algorithm average surface find global minimum 
unfortunately case neural net 
average value function sampled points parameters randomly sampled bounds vector set simulated annealing results suggest average value 
largest optima conventional algorithms combined total runs smallest previously reported 
small range large average value function neural net surface appears egg titled curved appreciable degree distorted 
relative values optima appear local optima need examined find global optima 
number local optima hardly appears current foreseen computers 
iv 
improvements simulated annealing review previously introduced improvements improvements simulated annealing introduced 
allows researcher test simulated annealing global optimum 
rerunning algorithm different starting value different seed random number generator entirely different sequence points selected algorithm 
optimum higher degree confidence global optimum 
modification useful understanding unmodified rational expectations model neural net 
second improvement restricts search area subset parameter space 
useful diagnostic tool difficult functions restrict parameters chosen algorithm large values run statistical function may cause floating point errors 
useful functions flat variables low initial temperature restrict search variables 
minimizing function think putting function deep 
points selected outside yield large values rejected 
forces algorithm inside region interest 
modification essential translog cost frontier rational expectations models 
final improvement allows researcher determine initial temperature essential parameter algorithm 
shown quite useful 
tailoring algorithm function section iii showed simulated annealing clearly superior conventional algorithms difficult statistical optimization problems 
benefit come cost simulated annealing may require substantially execution time 
section introduces modification minimize execution time 
essentially tailors simulated annealing minimum level robustness required optimize function 
approach chooses appropriate function hand 
values greatly influence robustness number function evaluations control quickly temperature declines number function evaluations performed temperature 
smaller values suggested maximum respectively chosen runs 
values increased algorithm run different random number generator seed starting value resulting entirely different sequence sampled points 
optimum reasonable degree assurance global optimum reached 
different optimum achieved procedure repeated larger values runs carefully monitor intermediate results 
results bit artificial optimum known useful guide 
approach illustrated judge function 
runs different starting values seeds local global minimum algorithm 
increased runs terminated incorrectly terminated correctly 
required average function evaluations decrease function evaluations parameter suggestions see table 
temperature fell quickly relatively evaluations temperature function evaluations accepted 
controls fast step length adjusted increased limits values successful evaluations essential operation algorithm 
rational expectations function yielded results 
runs runs terminated correctly failed 
failures easy detect terminated different optimal values initial temperature reductions uphill moves appears algorithm caught local optimum couldn escape parameter values 
increased runs terminated normally 
abnormal run characteristics previous ones 
successful runs required average function evaluations reduction table 
produced execution time seconds better sets runs reported table comparable single runs 
technique produced reduction translog cost frontier model reduction function evaluations reported table 
small reduction produced runs temperature reductions lead uphill moves exactly phenomena occurred rational expectations functions 
extremely steep gradients function surprising algorithm got stuck local optimum 
method neural net global optimum 
functions global optimum section shows translog cost frontier model requires high performance computer successful optimization number function evaluations needed simulated annealing may quite reasonable functions 
simulated annealing immediately useful researchers functions 
tests developed algorithm simulated annealing different statistical models require optimization 
compared conventional optimization algorithms library simulated annealing shown advantages 
diagnostic tool understand conventional algorithms fail 
second step regions parameter space function exist 
importantly optimize functions conventional algorithms extreme difficulty simply optimize 
test model synthetic example literature minima 
simulated annealing correctly differentiated global local minima conventional algorithms 
second test function rational expectations exchange rate model 
conventional algorithms failed simulated annealing able identify reason 
correcting problem conventional algorithms optimum time successes algorithm 
simulated annealing able find optimum easily 
third test function efficiency study banking industry translog cost frontier model 
conventional algorithms able optimize simulated annealing easily 
fourth test function neural network simulated annealing better optimum conventional algorithms 
simulated annealing able find global optimum shown nature function virtually impossible method find global optimum 
introduced number extensions simulated annealing 
important allows researcher tailor algorithm function minimize execution time algorithm 
allows simulated annealing find global optimum actual econometric problems commonly available computers 
appendix computer performance useful widely benchmark computer performance econometric probably linpack benchmark 
system linear equations solved number floating point operations second counted mflops 
appropriate benchmark involves floating point operations arrays 
table dongarra shows number floating point operations second linpack benchmark existing computers processor 
cray mp currently top cray line replaced mp 
neural net described ran mp ran mp 
ibm es new top ibm line 
cray cray widely machine 
ibm top ibm mainframe line early ibm pc math intel known 
table computer mflops cray mp cray mp ibm es model vf cray ibm ibm pc table shows performance high performance desktop computers 
ibm rs hp workstations list price slightly lower priced versions due early 
apple ibm announced joint agreement see pollack part says version rs cpu 
club america hawk iii fastest intel computers addition line microprocessors began intel ibm pc 
table computer mflops hp model ibm rs model club america hawk iii table bell shows predictions computer performance 
see floating point performance risen posed rise substantially near 
fast personal computer half performance ibm premier mainframe early rs hp approach surpass cray power 
quick test rs evaluations judge function run 
required seconds execution time mp took seconds 
turnaround time mainframes supercomputers rs extremely competitive 
just harbinger machines 
table computer mflops leading microprocessor vector processor appendix simulated annealing algorithm set initial parameters values set controls fast adjusts set starting values model parameters set cover entire range interest set convergence criteria set temperature reduction factor set initial temperature set times tolerance achieved termination set times function adjustment set times loop reduction calculate opt opt convergence times times uniform random number calculate apply metropolis criteria accepted opt opt opt adjust half trials accepted change iterations opt report opt opt start current best optimum opt reduce continue appendix experience genetic algorithm genetic algorithms mimic natural selection function optimization 
value function corresponds fitness organism parameters function correspond genes organism 
genes produce fittest function values generation mate pass successful traits generation 
sequence generations function optimized organism environment 
described goldberg discussion follows genetic algorithms successfully areas including pipeline operation structural optimization job shop scheduling filter design 
optimizing non continuous functions 
noted goldberg originator genetic algorithm workers developed degree nsf presidential young investigator 
simulated annealing genetic algorithm uses probabilistic decision rules escape local optima 
slowly optimizes function opposed greedily optimizing conventional algorithms 
described numerous differences simulated annealing genetic algorithm 
genetic algorithm parameters function optimized coded character string commonly binary digits 
instance integers coded binary string bbb 
useful codings clearly follow 
strengths genetic algorithm short substrings called schemata contain important information 
coding example maximizing function substring quite important 
analysis involved turns population size order schemata 
different characteristics population explored relatively small population 
algorithm starts population strings elements string assigned random value 
strings decoded function optimized evaluated 
fittest strings produce largest function value maximizing function selected mated fit strings 
actual selection procedures including replacement stochastic deterministic methods 
comes goldberg string probability selection weighted th function value generated string sum function values generation 
basic mating mechanism simple crossover strings interchange randomly selected point 
instance th binary strings interchange element yields 
mating selection continues members chosen generation 
small probability mutation mating 
binary coded string implemented nonzero probability bit may switch 
turns mutation important high rate mutation hinders optimization process adding unnecessary noise system 
process creating new generations old ones selection mating mutation continues termination criteria comes play 
simple implementations may run interactively researcher terminates process appropriate 
obtained pascal code simple genetic algorithm outlined goldberg modified functions vector inputs 
variation coding scheme integers implemented user defined floating point 
full bits element implemented lower order bits useless optimal higher order bits determined 
vector inputs 
discussion illustrates crossover mating key information short strings largest part coded largest 
gain experience algorithm test functions unimodal function variables optimized running generations population generation simple genetic algorithm experienced difficulty 
generation average value function exceed maximum value function 
fact quick initial improvement generations average value varied 
problem average function value generation slightly worse best function value th worst substantially worse instance generation average best worst 
usual selection rules genes produce average values selected best genes little improvement occurs 
problem goldberg recommends scaling function transforms generation function values distribution lead better performance 
case scaling function stretch distance average largest values superior genes selected 
unfortunately linear scaling population 
developed third scaling function quadratic provide improvement average function value performance 
simple genetic algorithm reaches higher scaling variation average minimum maximum function values generation 
part function broader smaller values 
different scaling functions needed different functions 
example shows may hard find appropriate scaling function 
judge function explored 
function written minimization form 
form values positive recall sum squares values far minimum function values substantial 
simple genetic algorithm maximizes functions selection rule requires positive function values 
considerations mind function value maximized simple genetic algorithm experienced difficulty nearing optimum 
problem stems th selection rule probability selecting string weighted near optimum approximately recall minimum 
near optimum little relative difference function values produced bad strings 
result little preference strings selection mating 
problem specific function stems selection rule difficulty selecting close function values 
experience difficulty relatively flat surface 
point apparent continuous problems genetic algorithm need development 
function optimized encountered difficulty algorithm forced modify 
knowledgeable user able correct problems typical user optimization algorithm expected modify algorithm suit function 
genetic algorithm usable experienced users continuous function problems 
noted goldberg continuous function optimization genetic algorithm comes peer reviewed sources 
particular refinement goldberg currently working called messy genetic algorithms 
algorithms closely mimic natural selection particular capability finding isolated optima proves troublesome algorithms 
solutions table comparison algorithms judge function bounds starting values number runs algorithm sa simplex conjugate quasi newton simulated gradient annealing mean number function evaluations execution time mp sec 
sec 
sec 
sec 
solutions categorized minimum terminate 
minimum value global minimum 
algorithm twice reports global step failed locate lower point current value 
current may approximate local minimizer accuracy possible step tolerance may large 
terminations algorithms reported normal 
report number function evaluations 
number smaller conservative values parameters simulated annealing employed 
see section iv 
table simulated annealing judge function sample output selected temperatures initial parameters algorithm initial parameters objective function resulting function value temperature current optimal parameters current optimal function value downhill accepted uphill rejected moves stepsize temperature current optimal parameters current optimal function value downhill accepted uphill rejected moves stepsize temperature current optimal parameters current optimal function value downhill accepted uphill rejected moves stepsize temperature current optimal parameters current optimal function value downhill accepted uphill rejected moves stepsize return simulated annealing normal solution parameters optimal function value final stepsize final temperature number accepted moves number function evaluations execution time mp seconds table conventional algorithms modified rational expectations model bounds starting values 
number runs algorithm simplex conjugate gradient quasi newton results final value objective function complex huge covariance matrix elements possible solution total non floating point error returns floating point error returns minimum value program reported results covers non floating point errors normal line search abandoned failed find lower value execution time mp min sec 
min sec 
min sec 
possible solutions final values objective function ranged 
returns produce objective values 
corresponding gradients approximately zero parameter values nearly identical 
slight variation objective values due termination criteria 
returns due huge elements covariance matrix 
identical objective values near zero gradients approximately equal parameter values 
algorithm reports line search integration abandoned 
error gradient may cause value near presumed minimum rest distance away 
algorithm reports global step failed locate lower point current value 
current may approximate local minimizer accuracy possible step tolerance may large final values parameters objective values gradient indistinguishable normal results 
table simulated annealing modified rational expectations model final step lengths final temperature number accepted moves number function evaluations execution time mp minutes seconds table conventional algorithms translog cost frontier model bounds starting values 
number runs algorithm simplex conjugate gradient quasi newton total non floating point error returns floating point error returns minimum value program reported results covers non floating point errors normal function evaluations line search abandoned failed find lower value execution time mp min sec 
min sec 
min sec 
value occurred iteration terminated number function evaluations exceeded 
execution time approximately seconds 
gradients small greater absolute value 
routine terminated exceeded function evaluations 
value anomaly parameter values greater absolute value 
evidently away reasonable parameter values function nonsensical 
best value 
values contained zero gradient 
typically gradient elements zero ranged 
described footnote number function evaluations limited 
described footnote number function evaluations limited 
iterations took execution time run 
resulting objective values ranged 
gradient values quite large 
algorithm reports line search integration abandoned 
error gradient may cause algorithm reports global step failed locate lower point current value 
current may approximate local minimizer accuracy possible step tolerance may large 
typical gradient values approximately 
table simulated annealing translog cost frontier model final step length final temperature number accepted moves number function evaluations execution time mp minutes seconds table conventional algorithms neural net bounds starting values 
number runs algorithm simplex conjugate gradient quasi newton total non floating point error returns floating point error returns minimum value program reported results covers non floating point errors normal function evaluations line search abandoned failed find lower value execution time mp min sec 
min sec 
min sec 
value occurred iteration terminated number function evaluations exceeded 
elements gradient small largest element absolute value 
largest element gradient absolute value 
routine terminated normally 
described footnote number function evaluations limited 
maximum number function evaluations 
function evaluations iterations promising function values 
time algorithm reported line search integration abandoned 
error gradient may cause algorithm reports global step failed locate lower point current value 
current may approximate local minimizer accuracy possible step tolerance may large 
table simulated annealing neural net solution vectors vector row solution vector final function value final temperature number accepted moves number function evaluations execution time mp minutes seconds parisi francesco 
global optimization algorithm stochastic differential equations 
acm transactions mathematical software 
baum eric neural nets economists 
economy evolving complex system 
philip anderson kenneth arrow david pines eds 
new york addison wesley 
bell gordon 
high performance computers science engineering 
communications acm 
mark johnson myron stein 
generalized simulated annealing function optimization 
technometrics 

image processing simulated annealing 
ibm journal research development 
martini 
minimizing multimodal functions continuous variables simulated annealing algorithm 
acm transactions mathematical software 
cramer econometric applications maximum likelihood methods 
new york cambridge university press 
dennis robert schnabel 
numerical methods unconstrained optimization nonlinear equations 
englewood cliffs nj prentice hall 
dick 
better way control pollution 
nature 
dongarra jack performance various computers standard linear equations software 
computer science department univeristy knoxville tn 
september 
drexl simulated annealing approach zero knapsack problem 
computing 
finch stephen nancy henry jr probabilistic measures adequacy numerical search global maximum 
journal american statistical association 
gary lovell 
measuring cost efficiency banking econometric linear programming evidence 
journal econometrics 
gallant ronald white 
learning derivatives unknown mapping multilayer feedforward networks 
neural networks forthcoming 
goldberg david genetic algorithms search optimization machine learning 
reading ma addison wesley 
judge george griffiths carter hill helmut chao lee 
theory practice econometrics nd ed 
new york john wiley sons 
kan rinnooy 
stochastic global optimization methods part ii multi level methods 
mathematical programming 

kendall course geometry dimensions 
new york hafner 
kendall maurice alan stuart 
advanced theory statistics 
th ed 
new york macmillan 
kirkpatrick gelatt jr vecchi 
optimization simulated annealing 
science 
chung ming white 
artifical neural networks econometric perspective 
working 
economics department university illinois urbana champaign il 
pollack andrew 
apple main ally 
new york times national edition 
october 

luc eric walter alain jean francois 
general purpose optimizer implementation applications 
mathematics computers simulation 
press william brian flannery saul teukolsky william vetterling 
numerical recipes art scientific computing 
new york cambridge university press 
richard computational problems methods 
handbook econometrics 
volume 
eds 
new york north holland 
filled function method finding global minimizer function variables mathematical programming 

michael solution estimation linear rational expectations models 
journal econometrics 
th 

reconstruction structures new application combinatorial optimization 
computing 
wilson estimation parameters multiple time series models journal royal statistical society series 
vanderbilt david 
monte carlo simulated annealing approach optimization continuous variables 
journal computational physics 
peter fast performance compatible processors 
personal workstation june 
wasserman philip tom schwartz 
neural networks part everybody interested 
ieee expert spring 
wong leong liu 
simulated annealing vlsi design 
boston kluwer academic publishers 
woo wing monetary approach exchange rate determination rational expectations dollar rate 
journal international economics 

