hierarchical als algorithms nonnegative matrix tensor factorization andrzej cichocki shun ichi amari dept ee warsaw university technology pan warsaw poland institute telecommunications teleinformatics acoustics university technology poland riken brain science institute shi japan cia amari brain riken jp 
new alternating squares als algorithms nonnegative matrix factorization nmf extensions nonnegative tensor factorization ntf robust presence noise potential applications including multi way blind source separation bss multi sensory multi dimensional data analysis nonnegative neural sparse coding 
propose local cost functions simultaneous sequential minimization leads simple als algorithm works sparsity constraints determined system sensors sources determined model 
extensive experimental results confirm validity high performance developed algorithms especially usage multi layer hierarchical nmf 
extension proposed algorithm multidimensional sparse component analysis smooth component analysis proposed 
problem formulation nonnegative matrix factorization nmf multi way extensions nonnegative tensor factorization ntf parallel factor analysis models sparsity non negativity constraints proposed promising quite efficient tools processing sparse signals images general data 
viewpoint data analysis nmf ntf provides nonnegative usually sparse common factors hidden latent components physiological meaning interpretation 
nmf ntf sparse component analysis sca variety applications ranging neuroscience chemometrics 
propose new hierarchical alternating squares algorithms nmf ntf 
incorporating regularization penalty terms local squared euclidean norms able achieve sparse local representations desired solution alleviate problem getting stuck local minima 
davies 
eds ica lncs pp 

springer verlag berlin heidelberg cichocki 
amari impose nonnegativity sparsity constraints ntf standard nonnegativity constraints model xq eq xq frontal slices matrices observed tensor data signals dq diagonal scaling matrices aj mixing basis matrix represents unknown sources hidden nonnegative sparse components eq represents th frontal slice tensor representing noise error 
special case model simplifies standard nmf model 
objective estimate set nonnegative matrices dq problem converted tri nmf model applying averaging frontal slices section develop alternative algorithm converts problem simple tri nmf model condition frontal slices xq dimension ad xq dq diag dq dq eq scaled matrix sources 
system linear algebraic equations represented equivalent scalar form follows xit eit equivalently vector form aj sj sj rows columns 
simple model provides improved performance noise frontal slices correlated 
majority nmf ntf algorithms bss applications works assumption held known estimated svd 
propose nmf algorithm determined case signal representations sparse 
objective estimate mixing basis matrix sources subject nonnegativity sparsity constraints 
locally regularized als algorithm known adaptive algorithms nmf alternating minimization squared euclidean distance expressed frobenius norm df subject nonnegativity constraints elements ir air jt aand nonnegative regularization coefficients controlling sparsity matrices 
usually common factors matrices normalized unit length column vectors rows respectively forced sparse possible 
hierarchical als algorithms basic approach nmf alternating minimization alternating projection specified cost function alternately minimized respect sets parameters aij time optimizing set arguments keeping fixed 
consider minimization set local squared euclidean cost functions ja aj js sj subject nonnegativity constraints elements aij apsp aj ri columns basis mixing matrix sj rows local parameters controlling sparsity level individual vectors penalty terms ja aj aij js sj enforce sparsification columns rows respectively 
construction set local cost functions follows simple observation observed data decomposed approximately follows apsp generally 
gradients cost function respect unknown vectors aj sj expressed ajs ajs aj ajs scalars added component wise 
equating gradient components zero assuming enforce nonnegativity constraints simple half rectifying nonlinear projection obtain new set sequential learning rules sj aj aj max small constant avoid numerical instabilities usually 

practice necessary normalize iteration step column vectors aj row vectors unit length vectors sense norm lp norm 
special case norms cichocki 
amari algorithms simplified neglecting denominator 
estimating normalized matrices estimate diagonal matrices follows dq diag xq 
applied simple nonlinear half wave rectifying projection max element wise order impose nonnegativity constraints 
nonlinear projections filtering applied extract sources necessary nonnegative specific properties 
proposed method easily extended semi nmf semi ntf nonnegativity constraints imposed preselected sources selected columns matrix available 
furthermore simple nonlinear half rectifying projection apply complex nonlinear projections filtering estimate bipolar sources specific properties example sources bounded sparse smooth 
order estimate sparse bipolar sources apply known adaptive soft hard shrinking nonlinear transformations nonlinear projection defined psr psr adaptive threshold 
alternatively may apply power nonlinear element wise transformation psp sign sis small coefficient controls sparsity density level individual sources 
order achieve smoothness estimated sources may apply local averaging operator ma arma models low pass filtering gradually enforces level smoothness iterative process 
possible extensions improvements deal factorization problem efficiently adopt approaches constrained optimization multi criteria optimization minimize simultaneously cost functions alternating switching sets parameters 
simple algorithm extended improved respect convergence rate performance 
different cost functions estimation rows matrix 
furthermore columns estimated simultaneously 
example minimizing set cost functions respect sj simultaneously cost function normalization columns aj unit norm obtain new als learning algorithm rows updated locally row row matrix updated globally columns aj simultaneously sj xs ss normalization scaling columns unit length 
hierarchical als algorithms secondly standard gradient descent approach apply quasi newton method estimation matrix hessian df ii ss rij ij df diagonal block structure blocks simplify update newton method simple form df adf ri ss rj matrix ha may ill conditioned especially sparse due levenberg marquardt approach control ill conditioning hessian 
developed nmf ntf algorithm sj ss ij exp index current alternating step ij rj identity matrix 
alternating minimization technique nmf convex selection initial conditions important 
algorithms initialized random uniform matrices 
minimize risk getting trapped local minima cost functions steering technique comes simulated annealing approach 
solution triggered exponential rule 
problems set heuristically 
multi layer nmf ntf order improve performance ntf algorithms proposed especially ill conditioned badly scaled data reduce risk getting stuck local minima non convex alternating minimization computations developed simple hierarchical multi stage procedure combined multi start initializations perform sequential decomposition nonnegative matrices follows 
step perform basic decomposition factorization xq available ntf algorithm 
second stage results obtained stage build new tensor estimated frontal slices defined step perform similar decomposition new available frontal slices different update rules 
con decomposition account achieved components 
process repeated arbitrarily times stopping criteria satisfied 
step usually obtain gradual improvements performance 
ntf model form xq final results dq cichocki 
amari fig 
left original sparse source signals middle observed mixed signals randomly generated mixing matrix determined case right estimated source signals new algorithm layers achieved performance follows db db respectively physically means build system layers cascade connections mixing subsystems 
key point approach learning update process find parameters matrices performed sequentially layer layer 
fact hierarchical multi layer approach necessary apply order achieve high performance proposed algorithms 
simulation results ntf algorithms extensively tested difficult benchmarks signals images various statistical distributions additive noise preliminary tests real eeg data 
due space limitations selected simulations results figs 
synthetic benchmark illustrated fig left contains sparse non negative weakly statistically dependent source components 
sources mixed randomly generated full rank matrix typical mixed signals shown fig middle 
results obtained new algorithm illustrated fig right average signal interference sir level greater db 
proposed algorithms alternating techniques perform non convex optimization estimated components initial condition dependent 
estimate performance statistical sense histograms mean sir samples estimation fig 
tested different algorithms combination algorithms algorithm als algorithm quasi newton hierarchical als algorithms fig 
histograms mean sir samples estimating monte carlo analysis performed algorithms layers left als right quasi newton discussion main objective motivation derive simple algorithms suitable determined determined cases 
proposed generalized flexible cost function controlled sparsity penalty allows derive family robust efficient alternating squares algorithms nmf ntf 
exploiting gradient hessian properties derived family efficient algorithms estimating nonnegative sources number sensors smaller number hidden nonnegative sources assumption sources sufficiently sparse strongly overlapped 
unique modification standard als algorithm authors best knowledge time cost function algorithms applied nmf ntf 
proposed algorithm gives better performance ans speed ordinary als algorithm nmf applications algorithm 
implemented discussed algorithms matlab 
algorithms may promising applications sparse component analysis smooth component analysis em factor analysis relax problems getting stuck local minima better standard als algorithm 
motivated proposed models areas data analysis especially eeg fmri signal image processing multiway blind source separation ii model reduction selection iii sparse image coding 
preliminary experiments promising 
models extended imposing additional natural constraints smoothness continuity closure local rank selectivity account prior knowledge specific generally multi way data 
cichocki 
amari obviously challenging open issues remaining global convergence optimal choice associated parameters 

cichocki amari adaptive blind signal image processing new revised improved edition 
john wiley new york 
dhillon sra generalized nonnegative matrix approximations bregman divergences 
neural information proc 
systems vancouver canada 
polak shashua sparse image coding non negative tensor factorization 
international conference computer vision iccv pp 

controlling sparseness non negative tensor factorization 
leonardis bischof pinz 
eds 
eccv 
lncs vol 
pp 

springer heidelberg 
hoyer non negative matrix factorization sparseness constraints 
journal machine learning research 
hansen herrmann parnas parallel factor analysis exploratory tool wavelet transformed event related eeg 
neuroimage 
bro multi way analysis applications chemical sciences 
john wiley sons new york 
oja plumbley blind separation positive sources globally convergent gradient search 
neural computation 
lee seung learning parts objects nonnegative matrix factorization 
nature 
berry browne langville plemmons algorithms applications approximate nonnegative matrix factorization 
computational statistics data analysis press 
cichocki amari hori extended smart algorithms non negative matrix factorization 
zadeh 
eds 

lncs lnai vol 
pp 

springer heidelberg 
kim choi monaural music source separation nonnegativity sparseness shift invariance 
rosca pr haykin 
eds 
ica 
lncs vol 
pp 

springer heidelberg 
cichocki non negative matrix factorization quasi newton optimization 
zadeh 
eds 

lncs lnai vol 
pp 

springer heidelberg 
cichocki nonnegative matrix factorization constrained second order optimization 
signal processing 
cichocki multilayer nonnegative matrix factorization 
electronics letters 
murray kreutz delgado learning sparse overcomplete codes images 
journal vlsi signal processing 
kreutz delgado murray rao lee sejnowski dictionary learning algorithms sparse representation 
neural computation 
cichocki signal processing 
technical report laboratory advanced brain signal processing riken japan 
