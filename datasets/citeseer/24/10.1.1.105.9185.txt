programming aid message passing systems min wu daniel gajski number processors complexity problems solved increase programming multiprocessing systems di cult error prone 
discusses programming assistance automation concepts application program development tool message passing systems called 
performs scheduling handles communication primitive insertion automatically 
algorithms critical path method scheduling processes statically 
generates performance estimates program quality measures help programmers improving algorithms programs 
commercial message passing systems introduced ipsc system ncube fps series connection machine 
approach program machines single program multiple data spmd style 
style processing element pe runs program executes di erent code segments depending pe id data local memory 
programmers nd approach easy develop parallel programs 
achieve balanced load pes spmd style trivial task computation load di erent especially problems irregular structures 
cases computation loads overloaded pes moved underloaded pes 
extreme programs spmd style 
school thought believes problems complex left programmers 
programmers error prone handle tedious chores commu nication primitive insertion 
example system deadlock common problem di cult detect program developed 
developed pro grams portable machine di erent communication primitives 
school thought believes restructuring compilers extract parallelism restructure sequential programs parallel programs automatically 
complete analysis exploiting parallelism complex 
furthermore parallelism revealed way restricted algorithms embodied sequential programs 
advocate approach extremes 
believe parallelization complex problem fully solved expert 
program development tools assist users solve creative chores algorithm design 
hand parallelization chores automated especially scheduling synchronization 
program development tools generate performance estimates quality measures guide programmers improving programs algorithms 
way optimal performance obtained increased productivity 
describe programming aid named automatic schedul ing synchronization message passing systems 
takes partitioned pro gram input generates parallel code execution message passing machines 
generates performance estimates quality measures parallel code 
discuss static scheduling problems new scheduling algorithms 
algorithms mapping synchronization discussed 
particular issues programming message passing systems computer aided programming tools discussed section ii 
programming model described sec tion iii 
scheduling mapping synchronization insertion discussed sections iv vi respectively 
annotated examples section vii illustrate method demonstrate usefulness programming aid 
ii 
computer aided programming programmer writing programs message passing system partition prob lem processes group processes tasks assign synchronization primitives proper execution 
program segment partitioned called process counted unit computation 
processes executing di erent exchange data data communication time overhead slows computation 
overhead sizable maximum parallelism program may generate optimum speedup 
reason processes merged form task thought allocation 
thenumber operations task de nes operation granularity number data items task de nes data granularity 
best speedup obtained proper granularity 
purpose modi es granularity merging processes tasks 
merging processes tasks task assigned pe topology 
reduce network tra tasks exchange data mapped neighboring pes 
possible assignment heuristic attempt minimize total communication tra network 
proper communication tasks di erent pes synchronization primitives inserted program 
tasks partitioning scheduling mapping synchronization automated interactive programming aid 
research orts demonstrated usefulness program development tools multiprocessing 
types tools 
provides software development environment debugging facilities 
poker parallel programming environment message passing systems provides graphic representation communication structure 
accepts program code inserted synchronization primitives produces report parallel access anomalies pairs statements access location simultaneously 
type tool performs program transformation 
tools type theory program restructuring 
performs sophisticated dependency analysis including advanced interprocedural ow analysis 
identi es parallel loops extracts global variables provides simple explanation facility 
information obtain parallelism eliminate dependencies reduce ciency losses 
transforms control dependencies data dependencies 
tests loops independence provide partitioning synchronization mechanisms non parallel loops 
camp partitions parallel non parallel loops reduces dependencies process alignment minimum distance algo rithms 
extracts parallelism eliminates dependencies ciency loss processor suspension reduced 
camp inserts synchronization primitives estimates performance partitioning strategy 
iii 
programming aid aims increase programming productivity take advantage tedious tasks computers perform better humans 
takes user partitioned program input automatically allocates partitions pes inserts proper synchronization primitives needed 
program development methodology shown 
designer develops proper algorithm performs partitioning writes program set procedures 
program looks sequential program run sequential machine purpose debugging 
program automatically converted parallel program message passing target machine optimization 
generates performance estimates including execution time communication time suspension time pe network delay channel 
explanation facility displays data dependencies pes parallelism load distribution 
designer satis ed result attempt rede ne partitioning strategy size partitions information provided performance estimator explanation facility 
shows organization program synthesis optimization module 
lexer parser recognize data dependencies user de ned partitions 
graph generation submodule generates macro data ow graph node represents process 
scheduling submodule assigns processes tasks minimizing execution time graph 
mapping submodule maps task physical pe topology minimizing network tra mapping little ect execution time data ow graph improper mapping increase network tra increases network contention 
scheduling mapping completed synchronization module inserts communication primitives 
code generator generates target machine code pe 
facilitate automation program development programming style program composed set procedures called main program 
procedure indivisible unit computation scheduled processor 
grain sizes procedures determined programmer modi ed 
shows example parallel gaussian elimination algorithm partitions matrix columns 
procedures called times 
control dependencies ignored procedure call executed input data procedure available 
data dependencies de ned single assignment parameters procedure calls 
communications invoked procedures 
words procedure receives messages begins execution sends messages nished computation 
static program number procedures known program execution 
data dependencies procedural parameters de ne macro data ow graph 
macro data ow graph generated directly main program directed graph start point 
example shows macro data ow graph program 
note parallel parts messages transferred procedures shown 
macro data ow graph consists set nodes fn set edges fe eeg 
corresponds procedure node weight represented procedure execution time 
example nodes correspond procedure nodes represent procedure 
edge corresponds message transferred procedure procedure weight edge equal transmission time message 
example edges connecting andn correspond message called vector rst iteration edge connecting message called matrix 
scheduled single pe weight edge connecting zero 
static scheduling number nodes known program execution 
execution running corresponding procedure 
transmission time message estimated message startup time message length communication channel bandwidth 
assume message transmission time neighbor communication 
non neighbor communication takes little time 
assume network tra heavy 
network contention ignored model 
programming style modularity necessary developing general application programs 
system independent communication primitives speci ed program 
program executed sequentially parallel 
furthermore dynamic scheduling simpler partitions program procedures 
iv 
scheduling section discuss methods scheduling macro data ow graph 
describe static nonpreemptive scheduling algorithms homogeneous multiprocessors 
critical path scheduling addressed hu 
critical path algorithm proved near optimal 
basically critical path algorithm assigns label node longest path node point 
performs bounded number pes 
ramamoorthy developed algorithms determine minimum number pes required process program shortest pos sible time 
exhaustive search acceptable large programs 
proposed alternative method reduce number pes ciency algorithm dependent estimate number processors 
importantly algorithms model transmission time assumed data transmission pes take time 
true message passing systems 
data transmission time signi cant factor ects performance system considered modeling 
kru lewis model assigned data communication time weights edges 
scheduling algorithm suitable ne granularity 
objective functions considered scheduling 
rst function time process graph bounded number pes 
second number pes required process graph shortest possible time 
rst modify critical path algorithm bounded number pes 
modi ed critical path mcp algorithm able ciently determine minimum number pes required execute program new algorithm called mobility directed md scheduling 
algorithm generates solutions number pes required 
de ne scheduling algorithms succinctly de ne concepts 
describe soon possible asap late possible bindings macro data ow graph order determine mobilities 
asap binding earliest possible execution assigned node 
asap binding created moving forward macro data ow graph 
example asap binding macro data ow graph shown 
node bound time interval sends message nodes andn messages arrive time nodes bound time interval starting performing asap binding node assigned asap start time denoted ts ni 
conversely latest possible start time assigned binding obtained moving backward macro data ow graph 
binding shown 
similarly performing binding node assigned time denoted tl ni 
moving range node ni de ned time interval ts ni ni may start execution delaying execution node critical path 
length range de ned mobility node ni ni tl ni ts ni 
relative mobility node de ned ni ni ni ni execution time node 
de ne tf ni tl ni ni nishing time node ni 
interval node ni de ned time interval ts ni ni 
example asap time node time 
moving range mobility 
relative mobility node 
moving ranges mobilities relative mobilities nodes listed table sorted relative mobilities 
nodes mobility equal zero identify critical path graph 
length critical path minimum time required program execution 
obviously critical path macro data ow graph critical paths possible 
nodes critical path scheduled single pe weight edge connecting zero critical path shortened 
note shortening critical path may generate new critical paths 
describe scheduling algorithm macro data ow graph number pes 
algorithm critical path algorithm sethi 
algorithm modi ed critical path mcp scheduling 
step perform binding assign resulting time tl ni node graph 
step create list ni consists tl ni descendants increasing order 
sort lists increasing order lexicographically 
order create node list step schedule rst node earliest execution 
delete node repeat step empty 
iso binding performed depth rst search 
step requires log operations generate list lists requires time log 
sorting lists requires log operations 
complexity step log 
complexity step 
complexity algorithm log 
discussing scheduling method minimize number pes required execute graph give fact node scheduled pe nodes scheduled 
fact necessary su cient condition scheduling node pe 
assume node np scheduled pe nodes nm nm nm scheduled 
moving intervals nodes intersect moving interval np scheduled pe assume moving intervals nodes nm nm nm intersect moving interval np 
np scheduled pe np min tf np tl nm max ts np ts nm nm tf np np calculated node np scheduled pe ni weight nm exist ts nm andw nm nm exist tl nm 
node scheduled pe describe mobility directed algorithm schedule macro data ow graph unbounded number pes 
algorithm mobility directed md scheduling 
step calculate relative mobilities nodes 
include nodes initially 
step group nodes minimum relative mobility 
node predecessors ni scheduled rst pe fact 
pe ni inserted rst node node sequence pe satis es inequality listed fact 
ni scheduled rst pe schedule second pe 
ni scheduled pe edges connecting ni nodes scheduled pe changed zero 
step ni scheduled node nj pe add edge weight zero nj ni graph 
ni scheduled node nj pe add edge weight zero nj ni graph 
ensure deadlock 
check adding edges form loop 
schedule ni available space 
step recalculate relative mobilities modi ed graph 
delete ni repeat step empty 
time step includes asap bindings calculations 
complexity step 
takes operations schedul ing node pe operations calculating relative mobilities 
complexity algorithm 
mapping mapping algorithm section maps virtual pes previous section physical pes connected prede ned topology 
mapping algorithm generates minimum amount tra reducing network con 
best results tra scheduling algorithm balances network tra 
tra scheduling requires exible path routing gener ates larger overhead 
network tra heavy simpler algorithms minimize total network tra may 
problems solved message passing systems category algorithm minimize total network tra mapping problem may described follows virtual pe graph consisting nt nodes edges 
virtual pe graph called task graph generated scheduling 
node graph corresponds task edge corresponds messages transferred tasks 
weight edge ei sum transmission time messages tasks 
task graph mapped system graph 
system graph consists ns nodes es edges ns nt 
node corresponds physical pe edge connection pes weight 
task graph mapped system graph communications nearest neighbor communications routing necessary mapping optimal 
oth certain pairs tasks connected non neighboring pes 
corresponding message routed shortest path pes 
distance pes de ned number hops shortest path pe 
objective function ei di di distance pes tasks connected ei mapped 
stands total communication tra example show task graph system graph respectively 
task graph mapped system graph shown 
better mapping shown 
may algorithms quadratic assignment problem obtain optimal mapping 
algorithm heuristic algorithm hanan minimize total communication tra 
generates initial assignment constructive method improved iteratively obtain better solution 
lee aggarwal described experimental results hypercube topologies showed algorithm works guarantee optimum solution 
vi 
synchronization synchronization message passing systems carried communication primi tives 
basic communication primitives receive 
message passing systems supply synchronous communication primitives supply syn chronous asynchronous primitives 
asynchronous primitives usually communica tion processors handle network activities reduces load main processors 
communication primitives exchange messages processors 
properly ensure correct sequence computation 
rst generation message passing systems communication primitives inserted programmers 
pos sible insert primitives automatically reducing programmer load eliminating insertion errors 
programming model partitions programs procedures message exchanges take place procedures primitive insertion relatively easy 
communication primitive insertion may described follows 
scheduling mapping node macro data ow graph allocated pe 
edge exits node node belongs di erent pe send primitive inserted node 
similarly edge comes node di erent pe receive primitive inserted node 
message sent particular pe message need sent pe 
message sent broadcasting applied sending message pes separately 
insertion method described ensure communication sequence correct 
example possible cases shown 
order sends incorrect reordered shown 
hand order sends order receives needs exchanged shown figures respectively 
send rst strategy case 
reorder receives order sends obtain solution shown 
entire communication primitive insertion algorithm described 
algorithm communication insertion 
assume scheduling mapping macro data ow graph assigned pe ni function mapping node number pe number 
step node ni nj ni nj insert send primitive node ni pe ni denoted ek ni nj insert receive primitive node nj pe nj denoted ek nj ni 
message sent pe eliminate sends transfer message pe 
pe sequence em nm pm em nm pm step pair pes say extract em nm pe form subsequence sp extract em nm form subsequence rp 
step segment subsequence sp node number exchange order sends order receives de ned subsequence rp 
step resultant subsequences matched rp reordered order sp 
reordering sends receives may necessary system supporting typed messages 
kind systems message transmission reordering may reduce message waiting time demand system communication bu ers 
vii 
examples comparison example illustrate scheduling synchronization 
md algorithm schedule macro data ow graph 
node scheduled rst node 
scheduled weight edge connecting inducing modi cation mobilities 
node scheduled 
macro data ow graph scheduled pes 
note sending vector rst iteration node pe means sending nodes unnecessary 
communication primitives inserted order second third receives pe exchanged pe receives message 
shows generated code pes 
note main program 
data structure 
initial matrix loaded pes pe obtains part matrix demanded computation 
consequently memory space compacted demanded allocated pe 
example rst second third columns matrix loaded pe fourth fth columns pe 
similarly third fourth fth columns resulting matrix unloaded pe rst second columns pe 
furthermore reduce number message transfers consequently time initiate messages messages packed sent 
example rst columns initial matrix may message sent 
shows example 
gauss seidel algorithm solve laplace equations converges faster jacobi algorithm 
algorithm update procedure step may value ai iteration obtained newly generated values iteration old values iteration 
algorithm assumes particular updating sequence 
ai depends values iteration values iteration obtained simultaneously 
matrix partitioned rows columns 
th macro data ow graph pes md algorithm 
pes available mcp algorithm applied 
resultant scheduling shown 
currently running sun workstation unix 
takes seconds schedule program processes pes 
examples tested 
experience developing programs takes time manual program development 
debugging easier deadlock programs developed 
compare manually generated codes generated codes 
codes generated algorithms 
di erence automatic scheduling applied generated codes manually generated codes 
scheduling method manually generated codes brie described follows data domain partitioned equally reduces dependencies partitions 
pe assigned partition spmd program coded pes 
programmer sophisticated scheduling working load heavy human produce error free results 
sophisticated scheduling performed automatic tool 
problems gaussian elimination laplace equa tions dynamic programming resulted improvement speed see fig ures 
problems regular structures manual scheduling usually leads unbalanced load distribution pes computation amount data partition di erent 
hand automatic scheduling moves nodes overloaded pes underloaded pes achieves better load balance 
regular problems matrix multiplication bitonic sort automatic scheduling gives performance similar manual scheduling shown figures 
kinds problems shows better performance size matrix evenly divided number pes 
table shows comparison gaussian elimination mcp md scheduling algorithms 
algorithms deliver performance md algorithm useful reducing number pes required execute program 
execution times manually generated codes listed comparison 
compares execution times random scheduling mcp scheduling laplace equations 
improvement speed obtained algorithm 
surprisingly cases random scheduling algorithm generates better performance manual scheduling 
grain size procedure ects quality generated parallel codes 
large granularity reduces parallelism small granularity increases overhead 
moderate grain size gives best performance 
shows performance di erent grain sizes procedures 
execution times nodes message transmission times obtained estimation performance ected di erence estimated value real value studied 
tables show results laplace equations bitonic sort respectively 
laplace equations node weight estimated matrix size bitonic sort node weight estimated array length 
results show di erence estimated value real value little ect performance 
viii 
number pes increase pro gramming multiprocessing systems di cult error prone 
optimal parallelization may complicated simple problems 
early experi ments programming hypercube systems revealed conceptualization program execution di cult optimization complex problems aged 
program development tool helps programmers develop parallel programs automating part parallelization tasks back annotating quality measures programmers necessity 
experimental results show approach better manual methodology respects 
increases programming productivity 
pro de ne partitions indicating task allocation communication primitives 
second communication primitive insertion performed synchronization errors eliminated 
programming errors may de sequentially running program 
program development better parallel codes uses scheduling algorithms 
resulted substantial performance increases demonstrated previous section 
generates target machine codes automatically programs developed tool portable 
programs may run di erent message passing systems shared memory systems 
tool developed variety languages di erent applications 
uses static scheduling applied static problems 
processes created starting execution 
estimate computation amount ofeach process known compile time 
drawback static scheduling program rescheduled run machine 
dynamic version investigation 
able schedule macro data ow graph thousands processes caused mainly memory space limitation 
large graph may partitioned strongly connected subgraphs scheduling 
authors anonymous reviewers thorough comments caused improve presentation level detail 
concurrent processing new direction scienti computing proc 
national computer conf pp 

system user guide edition 
computer research division arcadia california 
hayes mudge stout microprocessor hypercube supercomputer ieee micro vol 
pp 
oct 
gustafson scott architecture homogeneous vector supercomputer proc 
int conf 
parallel processing pp 
aug 
tucker robertson architecture applications connection machine ieee computer pp 
aug 
karp programming parallelism ieee computer vol 
pp 
may 
seitz cosmic cube communications acm vol 
pp 
jan 
karp babb ii comparison parallel fortran dialects ieee software pp 
sep 
padua kuck lawrie high speed multiprocessor compilation techniques ieee trans 
computers vol 
pp 
sep 
wu gajski computer aided programming message passing systems problems solution ieee proceedings feb 
gajski essential issues multiprocessor systems ieee computer vol 
pp 
june 
gajski wu programming environments multiprocessors proc 
int seminar scienti supercomputers pp 
feb 
sarkar partitioning scheduling parallel programs multiprocessors 
mit press 
snyder parallel programming poker programming environment ieee computer pp 
july 
mcdowell anomaly detection parallel fortran programs proc 
workshop parallel processing hep may 
allan baumgartner kennedy porter eld semiautomatic parallel programming assistant proc 
int conf 
parallel processing pp 
aug 
gajski camp programming aide multiprocessors proc 
int conf 
parallel processing pp 
aug 
wu gajski programming aid message passing systems proc 
rd siam conf 
parallel processing scienti computing dec 
hu parallel sequencing assembly line problems operations research vol 
pp 

adam chandy dickson comparison list scheduling parallel processing systems communications acm vol 
pp 
dec 
kohler preliminary evaluation critical path method scheduling tasks multiprocessor systems ieee trans 
computers vol 
pp 
dec 
ramamoorthy chandy gonzales optimal scheduling strategies multiprocessor system ieee trans 
computers vol 
pp 
feb 
fernandez levy optimal scheduling homogeneous multiprocessors proc 
ifip congress pp 
north holland publ 

lewis grain size determination parallel processing ieee software pp 
jan 
davidson shriver local microcode compaction techniques computing surveys vol 
pp 
sep 
sethi computer job shop scheduling theory ch 
ch algorithms minimal length schedules 
wiley 
shen interprocessor tra scheduling algorithm multiple processor networks ieee trans 
computers vol 
pp 
apr 
hanan review placement quadratic assignment problems siam rev vol 
pp 
apr 
lee aggarwal mapping strategy parallel processing ieee trans 
computers vol 
pp 
apr 
table mobilities nodes node moving range mobility relative mobility table performance comparison mcp md algorithms gaussian matrix number execution time msecs size pes mcp md manual table effect estimation performance laplace equations problem size execution time msecs di erence real estimated speed table effect estimation performance bitonic sort problem size execution time msecs di erence real estimated speed development tool 
program synthesis optimization 
parallel gaussian elimination algorithm 
macro data ow graph 
asap binding 
binding 
mapping task graph system graph mapping optimal mapping 
synchronization insertion case 
synchronization insertion case 
scheduling 
target machine code pe 
macro data ow graph parallel laplace equation algorithm 
scheduling mcp algorithm 
performance comparison gaussian performance comparison laplace equations performance comparison dynamic programming performance comparison matrix multiplication performance comparison bitonic sort comparison random mcp scheduling laplace equations comparison di erent grain sizes laplace equations 
