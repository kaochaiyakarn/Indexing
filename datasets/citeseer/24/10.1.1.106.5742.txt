shared memory consistency models tutorial sarita adve kourosh gharachorloo department electrical computer engineering rice university houston texas western research laboratory digital equipment palo alto california rice university ece technical report western research laboratory research report september parallel systems support shared memory abstraction widely accepted areas computing 
writing correct efficient programs systems requires formal specification memory semantics called memory consistency model 
intuitive model sequential consistency greatly restricts performance optimizations commonly uniprocessor hardware compiler designers reducing benefit multiprocessor 
alleviate problem current multiprocessors support relaxed consistency models 
unfortunately models supported various systems differ subtle important ways 
furthermore precisely defining semantics model leads complex specifications difficult understand typical users builders computer systems 
purpose tutorial describe issues related memory consistency models way understandable computer professionals 
focus consistency models proposed hardware shared memory systems 
models originally specified emphasis system optimizations allow 
retain system centric emphasis uniform simple terminology describe different models 
briefly discuss alternate programmer centric view describes models terms program behavior specific system optimizations 
performed sarita adve university wisconsin madison kourosh gharachorloo stanford university 
wisconsin sarita adve partly supported ibm graduate fellowship 
currently supported national science foundation 
ccr ccr texas advanced technology program funds rice university 
stanford kourosh gharachorloo supported darpa contract partly supported fellowship texas instruments 
submitted ieee possible publication 
copyright may transferred notice version superseded 
shared memory single address space abstraction provides advantages message passing private memory abstraction presenting natural transition uniprocessors simplifying difficult programming tasks data partitioning dynamic load distribution 
reason parallel systems support shared memory gaining wide acceptance technical commercial computing 
write correct efficient shared memory programs programmers need precise notion memory behaves respect read write operations multiple processors 
example consider shared memory program fragment represents fragment program splash application suite 
shows processor repeatedly allocating task record updating data field record inserting record task queue 
tasks left processor updates pointer head point record task queue 
processors wait head non null value dequeue task pointed critical section access data field dequeued record 
programmer expect memory system ensure correct execution program fragment 
important requirement value read data field dequeued record written record 
commercial shared memory systems possible processors observe old value data field value prior write field leading behavior different programmer expectations 
initially pointers null integers 
pn tasks null task critical section task data head null insert task task queue head head head head head task queue value read return 
critical section 
data memory consistency model shared memory multiprocessor provides formal specification memory system appear programmer eliminating gap behavior expected programmer actual behavior supported system 
effectively consistency model places restrictions values returned read shared memory program execution 
intuitively read return value write memory location 
uniprocessors precisely defined program order order memory operations appear program 
case multiprocessors 
example write read field record related program order reside different processors 
intuitive extension uniprocessor model applied multiprocessor case 
model called sequential consistency 
informally sequential consistency requires memory operations appear execute time operations single processor appear execute order described processor program 
referring back program model ensures reads data field dequeued record return new values written processor 
sequential consistency provides simple intuitive programming model 
disallows hardware compiler optimizations possible uniprocessors enforcing strict order shared memory operations 
reason number relaxed memory consistency models proposed including supported commercially available architectures digital alpha sparc ibm powerpc 
unfortunately vast variety relaxed consistency models proposed literature differ subtle important ways 
furthermore complex non uniform terminology describe models difficult understand compare 
variety complexity leads misconceptions relaxed memory consistency models described 
goal tutorial article provide description sequential consistency relaxed memory consistency models way understandable computer professionals 
understanding important performance enhancing features incorporated system designers correctly widely programmers 
achieve goal describe semantics different models simple uniform terminology 
focus consistency models proposed hardware shared memory systems 
original specifications models emphasized system optimizations allowed models 
retain system centric emphasis descriptions enable capturing original semantics models 
briefly describe alternative programmer centric view relaxed consistency models 
view describes models terms program behavior terms hardware compiler optimizations 
readers interested pursuing formal treatment system centric programmer centric views may refer previous 
rest article organized follows 
short note concerned memory consistency model system 
describe programming model offered sequential consistency implications sequential consistency hardware compiler implementations 
describe relaxed memory consistency models simple uniform terminology 
part article describes programmer centric view relaxed memory consistency models 
memory consistency models care 
interface programmer system effect memory consistency model pervasive shared memory system 
model affects programmability programmers reason correctness programs 
model affects performance system determines types optimizations may exploited hardware system software 
due lack consensus single model portability affected moving software systems supporting different models 
memory consistency model specification required level interface defined programmer system 
machine code interface memory model specification affects designer machine hardware programmer writes reasons machine code 
high level language interface specification affects programmers high level language designers software converts high level language code machine code hardware executes code 
programmability performance portability concerns may different levels 
summary memory model influences writing parallel programs programmer perspective virtually aspects designing parallel system including processor memory system interconnection network compiler programming languages system designer perspective 
memory semantics uniprocessor systems high level uniprocessor languages simple sequential semantics memory operations 
semantics allow programmer assume memory operations occur time sequential order specified program program order 
programmer expects read return value write location sequential program order 
fortunately illusion sequentiality supported efficiently 
example sufficient maintain uniprocessor data control dependences execute operations program order location controls execution 
long uniprocessor data control dependences respected compiler hardware freely reorder operations different locations 
enables compiler optimizations register allocation code motion loop transformations hardware optimizations pipelining multiple issue write buffer bypassing forwarding lockup free caches lead overlapping reordering memory operations 
sequential semantics uniprocessors provide programmer myth reality memory consistency model applies systems allow multiple copies shared data caching 
current systems sequentially consistent 
memory consistency model affects design hardware 
relationship cache coherence protocols memory consistency models cache coherence protocol inherently supports sequential consistency ii memory consistency model depends system supports invalidate update coherence protocol 
memory model system may defined solely specifying behavior processor memory system 
relaxed memory consistency models may hide read latency 
relaxed consistency models require extra synchronization 
relaxed memory consistency models allow chaotic asynchronous algorithms 
illustrates counter examples 
mentions commercial systems sequentially consistent 
article describes memory consistency model affects aspects system design including optimizations allowed compiler 
article discusses cache coherence protocol part memory consistency model 
aspects include order processor issues memory operations memory system write executes atomically 
article discusses memory consistency model allow invalidate update coherence protocol 
article describes memory consistency model affected behavior processor memory system 
models described article allow hiding read write latencies 
relaxed models discussed article require extra synchronization program 
particular framework requires operations distinguished labeled correctly 
models provide safety nets allow programmer enforce required constraints achieving correctness 
models discussed article allow chaotic asynchronous algorithms 
system centric models programmer reason correctness algorithms considering optimizations enabled model 
programmer centric approach simply requires programmer explicitly identify operations involved race 
chaotic algorithms approach may provide higher performance algorithms depend sequential consistency correctness 
myths memory consistency models 
pn memory programmer view sequential consistency 
simple intuitive model allow wide range efficient system designs 
understanding sequential consistency commonly assumed memory consistency model shared memory multiprocessors sequential consistency formally defined lamport follows 
definition multiprocessor system sequentially consistent result execution operations processors executed sequential order operations individual processor appear sequence order specified program 
aspects sequential consistency maintaining program order operations individual processors maintaining single sequential order operations processors 
aspect appear memory operation executes atomically instantaneously respect memory operations 
sequential consistency provides simple view system programmers illustrated 
conceptually single global memory switch connects arbitrary processor memory time step 
processor issues memory operations program order switch provides global serialization memory operations 
provides examples illustrate semantics sequential consistency 
illustrates importance program order operations single processor 
code segment depicts implementation dekker algorithm critical sections involving processors flag variables flag initialized 
attempts enter critical section updates flag checks value 
value indicates tried enter critical section safe enter 
algorithm relies assumption value returned read implies write occurred write read operations 
read flag return value prohibiting entering critical section 
sequential consistency ensures requiring program order memory operations maintained precluding possibility processors reading value entering critical section 
illustrates importance atomic execution memory operations 
shows processors sharing initialized 
suppose processor returns value written read ofa writes processor returns value written forb 
atomicity aspect sequential consistency allows assume effect write seen entire system time 
guaranteed see effect write execution return value read ofa sees effect write sees effect write 
implementing sequential consistency section describes intuitive abstraction sequential consistency shown realized practical system 
see uniprocessors preserving order operations location basis initially flag flag initially flag flag flag flag critical section critical section examples sequential consistency 
sufficient maintaining sequential consistency multiprocessors 
register considering interaction sequential consistency common hardware optimizations 
separate issues program order atomicity describe implementations sequential consistency architectures caches consider effects caching shared data 
part section describes interaction sequential consistency common compiler optimizations 
architectures caches chosen canonical hardware optimizations illustrative examples typical interactions arise implementing sequential consistency absence data caching 
large number common hardware optimizations lead interactions similar illustrated canonical examples 
apparent key issue correctly supporting sequential consistency environment caches lies maintaining program order operations processor 
illustrates various interactions discussed 
terms indicate order corresponding memory operations execute memory 
write buffers bypassing capability optimization consider illustrates importance maintaining program order write read operation 
shows example bus shared memory system caches 
assume simple processor issues memory operations time program order 
optimization consider compared abstraction write buffer bypassing capability 
write processor simply inserts write operation write buffer proceeds waiting write complete 
subsequent reads allowed bypass previous writes write buffer faster completion 
bypassing allowed long read address match address buffered writes 
constitutes common hardware optimization uniprocessors effectively hide latency write operations 
see write buffers violate sequential consistency consider program 
program depicts dekker algorithm shown earlier 
explained earlier sequentially consistent system prohibit outcome reads flags return value 
outcome occur example system 
processor buffer write allow subsequent read bypass write write buffer 
reads may serviced memory system write serviced allowing reads return value 
optimization safe conventional uniprocessor bypassing operations different locations lead violation uniprocessor data dependence 
example illustrates reordering easily violate semantics sequential consistency multiprocessor environment 
read flag write head write head write data head write flag shared bus flag flag read flag write flag general interconnect head write data memory data general interconnect read head memory memory write buffer read data read head overlapped writes read data data non blocking reads flag critical section data head data head flag flag flag canonical optimizations may violate sequential consistency 
critical section head 
data head 
data overlapping write operations second optimization illustrates importance maintaining program order write operations 
shows example system general non bus interconnection network multiple memory modules 
general interconnection network alleviates serialization bottleneck bus design multiple memory modules provide ability service multiple operations simultaneously 
assume processors issue memory operations program order proceed subsequent operations waiting previous write operations complete 
key difference compared previous example multiple write operations issued processor may simultaneously serviced different memory modules 
example program fragment illustrates optimization violate sequential consistency example simplified version code shown 
sequentially consistent system guarantees read return value written 
allowing writes overlapped system shown easily violate guarantee 
assume variables reside different memory modules shown 
write head may injected network write reached memory module writes complete program order 
possible processor observe new value obtain old value 
common optimizations coalescing writes cache line write buffer digital alpha processors lead similar reordering write operations 
allowing writes different locations reordered safe uniprocessor programs example shows reordering easily violate semantics sequential consistency 
way remedy problem wait write operation reach memory module allowing write operation processor injected network 
enforcing order typically requires response writes notify issuing processor write reached target 
response useful maintaining program order write subsequent read systems general interconnection networks 
non blocking read operations third optimization illustrates importance maintaining program order read read write operation 
consider supporting non blocking reads system represented repeated 
early risc processors stall return value read operation blocking read current generation processors capability proceed past read operation techniques non blocking lockup free caches speculative execution dynamic scheduling 
shows example overlapping reads processor violate sequential consistency 
program previous optimization 
assume ensures writes arrive respective memory modules program order 
allowed issue read operations overlapped fashion possibility read arrive memory module write read reaches memory module write leads non sequentially consistent outcome 
overlapping read write operation problems analogous optimization commonly current processors 
architectures caches previous section described complications arise due memory operation reordering implementing sequential consistency model absence caches 
caching replication shared data similar reordering behavior violate sequential consistency 
example level write cache lead reordering similar allowed write buffer bypassing capability reads follow write program order may serviced cache write completes 
implementation caches take precautions maintain illusion program order execution operations processor 
notably read processor hits processor cache processor typically read cached value previous operations program order complete 
replication shared data introduces additional issues 
presence multiple copies requires mechanism referred cache coherence protocol propagate newly written value cached copies modified location 
second detecting write complete preserve program order write operations requires transactions presence replication 
third propagating changes multiple copies inherently non atomic operation making challenging preserve illusion atomicity writes respect operations 
discuss issues detail 
cache coherence sequential consistency definitions cache coherence referred cache consistency exist literature 
strongest definitions treat term virtually synonym sequential consistency 
definitions impose extremely relaxed ordering guarantees 
specifically set conditions commonly associated cache coherence protocol write eventually visible processors writes location appear seen order processors referred serialization writes location 
conditions clearly sufficient satisfying sequential consistency requires writes locations just location seen order processors explicitly requires operations single processor appear execute program order 
term cache coherence define consistency model 
view cache coherence protocol simply mechanism propagates newly written value cached copies modified location 
propagation value typically achieved invalidating eliminating copy updating copy newly written value 
view cache coherence protocol memory consistency model interpreted policy places early late bound new value propagated processor 
detecting completion write operations mentioned previous section maintaining program order write operation typically requires response signal completion write 
system caches response may generated soon write reaches target memory module 
may sufficient designs caches 
consider code system similar depicted enhanced write cache processor 
assume processor initially cache 
suppose proceeds write previous write data reaches target memory value propagated invalidation update message 
possible read new value return old value cache violation sequential consistency 
problem avoided waits cache copy updated invalidated proceeding write 
write line replicated processor caches system typically requires mechanism acknowledge receipt invalidation update messages target caches 
furthermore messages need collected memory processor issues write processor issues write notified completion 
processor consider write complete notification 
common optimization acknowledge invalidation update message soon received processing node potentially actual cache copy affected design satisfy sequential consistency long certain ordering constraints observed processing incoming messages cache 
maintaining illusion atomicity writes sequential consistency requires memory operations appear atomic instantaneous propagating changes multiple cache copies inherently non atomic operation 
motivate describe conditions ensure appearance atomicity presence data replication 
problems due non atomicity easier illustrate update protocols examples assume protocol 
motivate condition consider program 
assume processors execute memory operations program order time 
possible violate sequential consistency updates writes processors reach processors different order 
processors initially register register example serialization writes 
return different values reads register register may assigned values respectively making writes appear non atomic 
violation sequential consistency possible systems general interconnection network messages travel different paths network guarantees provided order delivery 
violation avoided imposing condition writes location serialized processors see writes location order 
serialization achieved updates invalidates location originate single point directory ordering messages source destination preserved network 
alternative delay update invalidate sent updates invalidates issued behalf previous write location acknowledged 
motivate second condition consider program fragment update protocol 
assume variables initially cached processors 
furthermore assume processors execute memory operations program order time waiting described writes location serialized 
possible violate sequential consistency system general network processor reads new value update processor update update ofa reads new value proceeds read value cache gets update ofa 
appear see write different times making write appear non atomic 
analogous situation arise invalidation scheme 
violation sequential consistency occurs allowed return value write seen update generated write 
possible restriction prevents violation prohibit read returning newly written value cached copies acknowledged receipt invalidation update messages generated write 
condition straightforward ensure invalidation protocols 
update protocols challenging invalidations updates directly supply new values processors 
solution employ phase update scheme 
phase involves sending updates processor caches receiving updates 
phase processor allowed read value updated location 
second phase confirmation message sent updated processor caches confirm receipt 
processor updated value cache receives confirmation message second phase 
processor issued write consider write complete phase 
compilers interaction program order aspect sequential consistency compiler analogous hardware 
specifically program fragments discussed far compiler generated reordering shared memory operations lead violations sequential consistency similar hardware generated reorderings 
absence sophisticated analysis key requirement compiler preserve program order shared memory operations 
requirement directly restricts uniprocessor compiler optimization result reordering memory operations 
include simple optimizations code motion register allocation common sub expression elimination sophisticated optimizations loop blocking software pipelining 
addition reordering effect optimizations register allocation lead elimination certain shared memory operations turn violate sequential consistency 
consider code 
compiler register allocates doing single read register reading value register loop may terminate executions single read returns old value 
loop guaranteed terminate sequentially consistent execution code 
source problem register prohibits observing new value written 
summary compiler shared memory parallel program directly apply common optimizations uniprocessor compiler sequential consistency maintained 
comments apply compilers explicitly parallel programs compilers parallelize sequential code naturally information resulting parallel program generate determine optimizations safely applied 
summary sequential consistency discussion clear sequential consistency constrains common hardware compiler optimizations 
straightforward hardware implementations sequential consistency typically need satisfy requirements 
processor ensure previous memory operation complete proceeding memory operation program order 
call requirement program order requirement 
determining completion write typically requires explicit message memory 
additionally cache system write generate invalidate update messages cached copies write considered complete generated invalidates updates acknowledged target caches 
second requirement pertains cache systems concerns write atomicity 
requires writes location serialized writes location visible order processors value write returned read invalidates updates generated write acknowledged write visible processors 
call write atomicity requirement 
compilers analog program order requirement applies straightforward implementations 
furthermore eliminating memory operations optimizations register allocation violate sequential consistency 
number techniques proposed enable certain optimizations hardware compiler violating sequential consistency having potential substantially boost performance discussed 
discuss hardware techniques applicable sequentially consistent systems hardware support cache coherence 
technique automatically prefetches ownership write operations delayed due program order requirement issuing prefetch exclusive requests writes delayed write buffer partially overlapping service delayed writes operations preceding program order 
technique applicable cache systems invalidation protocol 
second technique speculatively services read operations delayed due program order requirement sequential consistency guaranteed simply rolling back read subsequent operations infrequent case read line gets invalidated updated read issued straightforward implementation 
technique suitable dynamically scheduled processors roll back machinery deal branch mispredictions 
techniques supported generation microprocessors mips intel enabling efficient hardware implementations sequential consistency 
latency hiding techniques non binding software prefetching hardware support multiple contexts shown enhance performance sequentially consistent hardware 
techniques beneficial conjunction relaxed memory consistency 
shasha snir developed compiler algorithm detect memory operations reordered violating sequential consistency 
analysis implement hardware compiler optimizations reordering operation pairs analyzed safe reordering compiler 
algorithm shasha snir exponential complexity new algorithm proposed spmd programs polynomial complexity 
algorithms require global dependence analysis determine operations different processors conflict similar alias analysis analysis difficult leads conservative information decrease effectiveness algorithm 
remains seen hardware compiler techniques approach performance relaxed consistency models 
remainder article focuses relaxing memory consistency model enable optimizations constrained sequential consistency 
relaxed memory models alternative sequential consistency relaxed memory consistency models proposed academic commercial settings 
original descriptions models widely varying specification methodologies levels formalism 
goal section describe models simple uniform terminology 
original specifications models emphasized system optimizations enabled models retain system centric emphasis descriptions section 
focus models proposed hardware shared memory systems relaxed models proposed software supported sharedmemory systems complex describe scope 
formal unified system centric framework describe hardware software models formal description models framework appears previous 
section describing simple methodology characterize various models describe model methodology 
characterizing different memory consistency models categorize relaxed memory consistency models key characteristics relax program order requirement relax write atomicity requirement 
respect program order relaxations distinguish models relax order write read writes read read write 
cases relaxation applies operation pairs different addresses 
relaxations parallel optimizations discussed section 
respect write atomicity requirement distinguish models allow read return value processor write cached copies accessed location receive invalidation update messages generated write write visible processors 
relaxation described section applies cache systems 
consider relaxation related program order write atomicity processor allowed read value previous write write visible processors 
cache system relaxation allows read return value write write serialized respect writes location invalidations updates write reach processor 
example common optimization allowed relaxation forwarding value write write buffer read processor 
cache systems common example processor writes write cache reads value cache write complete 
consider relaxation separately safely applied models violating semantics model models explicitly specify optimization original definitions 
instance relaxation allowed sequential consistency long program order atomicity requirements maintained discuss previous section 
furthermore relaxation safely applied models discussed section 
summarizes relaxations discussed 
relaxed models typically provide programmers mechanisms overriding relaxations 
example explicit fence instructions may provided override program order relaxations 
generically refer mechanisms safety net model discuss types safety nets provided model 
model may provide subtle ways enforcing specific ordering constraints simplicity discuss straightforward safety nets 
provides overview models described remaining part section 
shows straightforward implementation model efficiently exploit program order write atomicity relaxation relax write read program order relax write write program order relax read read read write program orders read write early read write early relaxations allowed memory models 
program order relaxations apply operation pairs accessing different locations 
relaxation rw read read safety net order order order write early write early sc serialization instructions ibm tso pc pso wo synchronization release acquire release acquire alpha mb rmo various powerpc sync simple categorization relaxed models 
indicates corresponding relaxation allowed straightforward implementations corresponding model 
indicates relaxation detected programmer affecting results program cases 
read write early relaxation detectable sc wo alpha powerpc models 
read write early relaxation possible detectable complex implementations 
relaxation example commercial systems providing relaxation order alphaserver cray sequent balance order alphaserver cray rw order alphaserver cray read write early cray read write early alphaserver cray commercial systems relax sequential consistency 
relaxations described mentions safety nets provided model 
indicates relaxations detectable programmer affect results program 
gives examples commercial systems allow relaxations 
simplicity attempt describe semantics models respect issues instruction fetches multiple granularity operations byte word operations semantics defined models 
sections describe model detail discuss implications model hardware compiler implementations 
discussion implicitly assume constraints satisfied 
assume models require write eventually visible processors writes location serialized 
requirements trivially met shared data cached usually met hardware cache coherence protocol presence shared data caching 
second assume models enforce uniprocessor data control dependences 
models relax program order reads write operations maintain subtle form multiprocessor data control dependences constraint inherently processor designs aware easily maintained compiler 
relaxing write read program order set models discuss relax program order constraints case write followed read different location 
models include ibm model sparc total store ordering model tso processor consistency model pc differs processor consistency model defined goodman 
key program order optimization enabled models allow read reordered respect previous writes processor 
consequence reordering programs fail provide sequentially consistent results 
violations sequential consistency illustrated occur due enforcement remaining program order constraints 
models differ allow read return value write 
ibm model strictest prohibits read returning value write write visible processors 
processor issues read address previous pending write read delayed write visible processors 
tso model partially relaxes requirement allowing read return value processor write write serialized respect writes location 
sequential consistency read allowed return value processor write visible processors 
pc model relaxes constraints read return value write write serialized visible processors 
shows example programs illustrate differences models 
consider safety net features models 
enforce program order constraint write read ibm model provides special serialization instructions may placed operations 
serialization instructions special memory instructions synchronization compare swap non memory instructions branch 
referring back example program placing serialization instruction write processor provides sequentially consistent results program executed ibm model 
contrast ibm tso pc models provide explicit safety nets 
programmers read modify write operations provide illusion program order maintained write read 
tso program order appears maintained write read part read modify write replaced read modify write 
replace read read modify write write read modify write dummy write writes back read value 
similarly replacing write read modify write requires writing back desired value regardless read returns 
techniques applicable designs provide flexibility read modify write instructions 
pc program order write read appears maintained read replaced part read modify write 
contrast tso replacing write read modify write sufficient imposing order pc 
difference arises tso places stringent constraints behavior read modify writes specifically tso requires writes location appear occur read write read modify write pc requires writes location 
initially flag flag initially flag flag register register register flag register flag register result register register result register register register differences tso pc 
result program part possible tso pc models allow reads flags occur writes flags processor 
result possible ibm read processor issued write processor done 
consequently read flag processor issued write flag processor done 
program part 
result shown possible pc allows return value write write visible 
result possible ibm tso 
consider safety net enforcing atomicity requirement writes 
ibm need safety net relax atomicity 
tso safety net write atomicity required write followed read location processor atomicity achieved ensuring program order write read read modify writes described 
pc write guaranteed appear atomic read may return value write part replaced read modify write 
reasoning read modify write operations ensure required program order atomicity models scope 
disadvantages relying read safety net models tso pc 
system may implement general read modify write appropriately replace read write 
second replacing read read modify write incurs extra cost performing write invalidating copies line 
course safety nets add overhead specific read write operations part read modify write operations 
furthermore programs frequently depend write read program order write atomicity correctness 
relaxing program order write followed read improve performance substantially hardware level effectively hiding latency write operations :10.1.1.52.9935
compiler optimizations relaxation beneficial practice 
reason reads writes usually finely interleaved program reordering optimizations effectively result reordering respect reads writes 
compiler optimizations require full flexibility reordering operations program order ability reorder write respect read sufficiently flexible 
relaxing write read write write program orders second set models relax program order requirement eliminating ordering constraints writes different locations 
sparc partial store ordering model pso example model describe 
key additional hardware optimization enabled pso previous set models writes different locations processor pipelined overlapped allowed reach memory cached copies program order 
respect atomicity requirements pso identical tso allowing processor read value write early prohibiting processor reading value processor write write visible processors 
referring back programs figures pso allows non sequentially consistent results 
safety net provided pso imposing program order write read enforcing write atomicity tso 
pso provides explicit instruction imposing program order writes 
way support implementation fifo write buffers insert write buffer delay retiring writes buffered writes buffered retired completed 
counter determine writes completed write sent memory system increments counter write decrements counter counter value indicates previous writes complete 
referring back program inserting writes ensures sequentially consistent results pso 
previous set models optimizations allowed pso sufficiently flexible useful compiler 
relaxing program orders final set models consider relax program order operations different locations 
read write operation may reordered respect read write different location 
discuss weak ordering wo model flavors release consistency model models proposed commercial architectures digital alpha sparc relaxed memory order rmo ibm powerpc models 
alpha models allow reordering reads location 
referring back models violate sequential consistency code examples shown 
key additional program order optimization allowed relative previous models memory operations read operation may overlapped reordered respect read operation 
hardware flexibility provides possibility hiding latency read operations implementing true non blocking reads context static order dynamic order scheduling processors supported techniques non blocking lockup free caches speculative execution 
models group allow processor read write early 
powerpc models straightforward implementations allow read return value processor write early 
possible complex implementations wo alpha rmo achieve 
programmer perspective implementations wo alpha rmo preserve illusion write atomicity 
unique model respect programmers rely atomicity complex implementations potentially violate atomicity way affect result program 
models may separated categories type safety net provided 
wo models distinguish memory operations type provide stricter ordering constraints types operations 
hand alpha rmo powerpc models provide explicit fence instructions imposing program orders various memory operations 
describes models greater detail focusing safety nets 
implications compiler implementations models group discussed section 
weak ordering wo weak ordering model classifies memory operations categories data operations synchronization operations 
enforce program order operations programmer required identify operations synchronization operation 
model intuition reordering memory operations data regions synchronization operations typically affect correctness program 
operations distinguished synchronization effectively provide safety net enforcing program order 
briefly describe simple way support appropriate functionality hardware 
processor provide counter keep track outstanding operations 
counter incremented processor issues operation decremented previously issued operation completes 
processor ensure synchronization operation issued previous operations complete signaled zero value wo read followed write program order related multiprocessor data control dependence mentioned section assume write delayed read complete write read complete 
special shared sync acquire release ordinary distinguishing operations release consistency 
counter 
furthermore operations issued previous synchronization operation completes 
note memory operations synchronization operations may reordered overlapped respect 
weak ordering model ensures writes appear atomic programmer safety net required write atomicity 
release consistency compared weak ordering release consistency provides distinctions memory operations 
pictorially depicts classification memory operations 
operations distinguished ordinary special 
categories loosely correspond data synchronization categories wo 
special operations distinguished sync operations 
syncs intuitively correspond synchronization operations correspond asynchronous data operations special operations synchronization 
sync operations distinguished acquire release operations 
intuitively acquire read memory operation performed gain access set shared locations lock operation spinning flag set 
release write operation performed permission accessing set shared locations unlock operation setting flag 
flavors release consistency differ program orders maintain special operations 
flavor maintains sequential consistency special operations second flavor maintains processor consistency operations 
depict program order constraints models operations different locations 
notation implies operation type precedes operation type program order program order enforced operations 
constraints follows acquire release special special 
write read program order special operations eliminated acquire release special special special write followed special read 
enforcing program order pair operations achieved distinguishing labeling appropriate operations information 
imposing program order write read operation requires read modify write operations analogous pc model 
write ordered ordinary write read modify write needs release write write special write 
similarly write appear atomic read modify write operations replace appropriate operations analogous pc model 
mentioned earlier writes may appear non atomic complex implementations 
preserving atomicity write achieved labeling sufficient operations special explaining done precisely difficult simple framework article 
note model accompanied higher level abstraction described section relieves need programmer directly reason lower level specification large class programs 
alpha rmo powerpc alpha rmo powerpc models provide explicit fence instructions safety nets 
alpha model provides different fence instructions memory barrier mb write memory barrier 
mb instruction maintain program order memory operations mb memory operations mb 
instruction provides guarantee write operations 
alpha model require safety net write atomicity 
sparc rmo model provides flavors fence instructions 
effectively instruction customized order combination previous read write operations respect read write operations bit encoding specify combination read read read write write read write write orderings 
fact order write respect read alleviates need read modify writes achieve order required sparc tso pso models 
similar tso pso rmo model require safety net write atomicity 
powerpc model provides single fence instruction called sync instruction 
imposing program order sync instruction behaves similar mb instruction alpha model exception 
exception sync placed reads location possible second read return value older write read reads appear occur program order 
create subtle correctness problems programs may require read modify write operations analogous pc enforce program order reads location 
powerpc differs alpha rmo terms atomicity allows write seen early processor read analogous pc read modify write operations may need write appear atomic 
compiler optimizations models previous sections models relax program orders provide sufficient flexibility allow common compiler optimizations shared memory operations 
models wo compiler flexibility reorder memory operations consecutive synchronization special operations 
similarly alpha rmo powerpc models compiler full flexibility reorder operations consecutive fence instructions 
programs operations instructions infrequently compiler gets large regions code virtually optimizations uniprocessor programs safely applied 
alternate abstraction relaxed memory models flexibility provided relaxed memory models described previous section enables wide range performance optimizations shown improve performance substantially 
higher performance accompanied higher level complexity programmers 
furthermore wide range models supported different systems requires programmers deal various semantics differ subtle ways complicates task porting programs systems 
programming complexity arises due system centric specifications typically provided relaxed memory models 
specifications directly expose programmer reordering atomicity optimizations allowed model require programmer consider behavior program presence optimizations order reason correctness 
provides incentive devise higher level abstraction programmers provides simpler view system allows system designers exploit types optimizations 
relaxed models described programmer ensure correctness program sufficient safety nets fence instructions conservative operation types read modify write operations impose appropriate ordering atomicity requirements memory operations 
difficult problem identifying ordering constraints necessary correctness 
example consider program executing model weak ordering wo 
example sufficient maintain orders correctness maintain program order write head operations write head processors maintain program order read head operations 
write read head behave synchronization operations identifying appropriate program orders automatically maintained model wo 
recognizing issue models wo accompanied informal conditions programmers ensure correct behavior 
example weak ordering requires programmers identify synchronization operations 
informal nature conditions ambiguous applied wide range programs operations really identified synchronization 
lot cases programmer resort reasoning low level reordering optimizations determine sufficient orders enforced 
exposing performance enhancing optimizations directly programmer done specification programmer centric specification requires programmer provide certain information program 
information system determine certain optimization applied violating correctness program 
provide formal programmer centric specification need define notion correctness programs 
obvious choice sequential consistency natural extension uniprocessor notion correctness commonly assumed notion correctness multiprocessors 
second information required programmer defined precisely 
summary programmer centric approach memory consistency model described terms program level information provided programmer 
systems model exploit information perform optimizations violating sequential consistency 
previous explored various programmer centric approaches 
example data race free drf approach explores information required allow optimizations similar enabled weak ordering 
properly labeled pl approach provided definition release consistency simpler way reason type optimizations exploited 
programmer centric approaches exploiting aggressive optimizations described unified framework designing programmer centric models developed explore design space models 
illustrate programmer centric approach concretely section describes type information may provided programmer enable optimizations similar exploited weak ordering model 
describe information conveyed programmer system 
example programmer centric framework recall weak ordering intuition memory operations classified data synchronization data operations executed aggressively synchronization operations 
key goal programmer centric approach formally define operations distinguished synchronization 
operation defined synchronization operation forms race operation sequentially consistent execution operations defined data 
sequentially consistent execution operation forms race operation operations access location operations write intervening operations operations consideration 
consider example example 
sequentially consistent execution program write read data separated intervening operations write read 
operations data operations 
operations separated operations synchronization operations 
note programmer reasons sequentially consistent executions program deal reordering optimizations order provide information 
system design viewpoint operations distinguished synchronization need executed conservatively operations distinguished data executed aggressively 
particular optimizations enabled weak ordering model safely applied 
furthermore information enables aggressive optimizations exploited weak ordering 
shown programmer centric framework requires programmer identify operations may involved race synchronization operations 
operations may distinguished data initially locations data head head 
data providing information memory operations 
distinguish data start races 
distinguish synchronization don know don care deciding distinguish memory operation 
synchronization 
operation may conservatively distinguished synchronization operation programmer sure particular operation involved race 
don know option important reasons 
programmer trivially ensure correctness conservatively distinguishing operations synchronization course performance gains potentially allows faster path initial working program 
potential benefit don know option allows programmer incrementally tune performance providing accurate information subset memory operations performance critical areas program simply providing conservative information remaining operations 
course correctness guaranteed programmer incorrectly distinguishes race operation data 
providing appropriate information system requires mechanism programming language level distinguish memory operations mechanism passing information form hardware level 
describe mechanisms section 
mechanisms distinguishing memory operations section describes possible mechanisms conveying information required framework described previous section 
conveying information programming language level consider programming languages explicit parallel constructs 
parallel programming support provided language may range high level parallelism constructs doall loops low level memory operations achieving synchronization 
mechanism conveying information memory operations depends support parallelism provided language 
languages specify high level paradigms parallel tasks synchronization restrict programmers paradigms 
example consider language allows parallelism expressed doall loops 
correct doall loops implies parallel iterations loop access location accesses write 
information memory operations implicitly conveyed operations high level program involved race 
slightly lower level language may provide library common synchronization routines programmer restricted achieve synchronization calls routines 
case programmer sufficient synchronization calls eliminate races operations program 
similar case doall loops information memory operations visible programmer excluding operations synchronization routines implicitly conveyed 
course compiler writers library routines ensure operation types synchronization data additional operations introduced implement constructs doall loops synchronization routines conveyed lower levels hardware 
programmer may allowed directly memory operations visible program level synchronization purposes memory location flag variable 
case programmer explicitly convey information operation types 
way associate information static instructions program level 
example language may provide constructs identify specific static regions code synchronization data dynamic operations generated region code implicitly identified synchronization data 
option associate data synchronization attribute shared variable address 
example language may provide additional type declarations allow programmer identify variables synchronization purposes 
type generality mechanisms provided programming language affects ease conveying required information 
example method type declarations indicate operation type default operations considered data indicated beneficial data operations frequent 
hand making synchronization type default simpler bring initial working program potentially decrease errors requiring programmers explicitly declare aggressive data operations 
conveying information hardware information conveyed programming language level ultimately provided underlying hardware 
compiler responsible appropriately translating higher level information form supported hardware 
similar mechanisms programming language level information memory operations may associated specific address ranges memory instruction corresponding operation 
way associate information specific address ranges treat operations specific pages data synchronization operations 
associating information specific memory instruction done ways 
option provide multiple flavors memory instructions providing extra opcodes distinguish memory operations 
second option unused high order bits virtual memory address achieve address shadowing 
memory instructions compare swap load locked store conditional may treated synchronization default 
commercial systems provide functionality directly communicating information memory operations hardware 
information transformed explicit fence instructions supported hardware level impose sufficient ordering constraints 
example provide semantics synchronization operations weak ordering hardware supports alpha memory barriers compiler precede follow synchronization operation memory barrier 
discussion strong evidence relaxed memory consistency models provide better performance possible sequential consistency enabling number hardware optimizations 
increase processor speeds relative memory communication speeds increase potential benefit models 
addition providing performance gains hardware level relaxed memory consistency models play key role enabling important compiler optimizations 
reasons led commercial architectures digital alpha sun sparc ibm powerpc support relaxed memory models 
furthermore nearly architectures support form explicit fence instructions indicates commitment support relaxed memory models 
unfortunately existing literature memory consistency models vast complex targeted researchers area typical users builders computer systems 
article uniform intuitive terminology cover issues related memory consistency models representative industry today goal reaching wider community computer professionals 
disadvantage relaxed memory consistency models increase programming complexity 
complexity arises specifications literature expose programmer low level performance optimizations enabled model 
previous addressed issue defining models higher level abstraction abstraction provides illusion sequential consistency long programmer provides correct program level information memory operations 
language standardization efforts high performance fortran led high level memory models different sequential consistency 
example forall statement high performance fortran specifies computation set array indices copy copy semantics computation index affected values produced computation indices 
choice best memory consistency model far resolved benefit active collaboration language hardware designers 
done part dissertation research 
indebted respective advisors mark hill anoop gupta john hennessy direction dissertation 
especially mark hill suggesting need encouraging write 
dwarkadas anoop gupta john hennessy mark hill yuan yu willy zwaenepoel valuable comments earlier versions 
andreas steve scott wolf dietrich weber information products developed sun microsystems cray research hal computer systems respectively 
sarita adve 
designing memory consistency models shared memory multiprocessors 
phd thesis computer sciences department university wisconsin madison december 
available technical report 
sarita adve mark hill 
weak ordering new definition 
proceedings th annual international symposium computer architecture pages may 
sarita adve mark hill 
unified formalization shared memory models 
ieee transactions parallel distributed systems june 
francisco janice stone charles barton 
formal specification powerpc shared memory architecture 
technical report computer science technical report rc ibm research division watson research center january 
michel dubois christoph fay briggs 
memory access buffering multiprocessors 
proceedings th annual international symposium computer architecture pages june 
kourosh gharachorloo 
memory consistency models shared memory multiprocessors 
phd thesis stanford university 
kourosh gharachorloo sarita adve anoop gupta john hennessy mark hill 
programming different memory consistency models 
journal parallel distributed computing august 
kourosh gharachorloo sarita adve anoop gupta john hennessy mark hill 
specifying system requirements memory consistency models 
technical report csl tr stanford university december 
available computer sciences technical report university wisconsin madison 
kourosh gharachorloo anoop gupta john hennessy :10.1.1.52.9935
performance evaluation memory consistency models shared memory multiprocessors 
fourth international conference architectural support programming languages operating systems pages april 
kourosh gharachorloo anoop gupta john hennessy 
techniques enhance performance memory consistency models 
proceedings international conference parallel processing pages august 
kourosh gharachorloo anoop gupta john hennessy 
hiding memory latency dynamic scheduling sharedmemory multiprocessors 
proceeding th annual international symposium computer architecture pages may 
kourosh gharachorloo anoop gupta john hennessy 
revision memory consistency event ordering scalable shared memory multiprocessors 
technical report csl tr stanford university april 
kourosh gharachorloo dan lenoski james laudon phillip gibbons anoop gupta john hennessy 
memory consistency event ordering scalable shared memory multiprocessors 
proceedings th annual international symposium computer architecture pages may 
ibm system principles operation 
ibm may 
publication number ga file number 
arvind krishnamurthy katherine yelick 
optimizing parallel spmd programs 
languages compilers parallel computing 
leslie lamport 
multiprocessor computer correctly executes multiprocess programs ieee transactions computers september 
cathy may ed rick simpson hank warren editors 
powerpc architecture specification new family risc processors 
morgan kaufmann publishers 
dennis shasha marc snir 
efficient correct execution parallel programs share memory 
acm transactions programming languages systems april 
richard sites editor 
alpha architecture manual 
digital press 
sparc architecture manual 
sun microsystems january 
version 
david weaver tom editors 
sparc architecture manual 
prentice hall 
sparc international version 

