arxiv cs cs dl aug fresh look reliability long term digital storage mary baker hp labs palo alto ca shah hp labs palo alto ca roussopoulos harvard university cambridge ma tj giuli stanford university ca emerging web services email photo sharing web site archives need preserve large amounts quickly accessible data indefinitely 
case applications demands large scale storage systems long time horizons require re evaluate traditional storage system designs 
examine threats long lived data perspective account just hardware software faults faults due humans organizations 
simple model long term storage failures helps reason various strategies addressing threats cost effective manner 
model show important strategies increasing reliability long term storage detecting latent faults quickly automating fault repair faster cheaper increasing independence data replicas 
frequent headlines remind bits bits stored expensive professionally administered data centers vulnerable loss damage 
despite emerging web services mail gmail photo sharing archives internet archive require large volumes data stored indefinitely 
economic viability services depends storing data low cost 
customer acceptance services depends data remaining unaltered accessible low latency 
satisfying requirements long periods time easy fast cheap reliable disks available threats data confined storage subsystem 
unfortunately true 
economics high volume manufacturing provide choice consumer grade drives trademarks mentioned belong respective owners 
david rosenthal stanford university libraries ca petros maniatis intel research berkeley ca harvard university cambridge ma cheap fairly fast fairly reliable enterprise grade drives vastly expensive faster little reliable 
show enterprise drive fourteen times expensive consumer drive reduces bit errors year idle lifetime 
short lived data level reliability pose problem long lived data faults inescapable 
long term storage faces threats storage system 
include obsolescence data formats long term attacks data economic structural volatility organizations sponsoring storage 
case long term reliable storage problem deserves fresh look significant differences traditional storage problems addressed literature 
start motivating need digital preservation storing immutable data long periods 
list threats data survival examples real systems examine design philosophy current storage systems insufficient long term storage 
understand implications problem better introduce simple reliability model replicated storage systems designed address long term storage threats 
model inspired reliability model raid extensions interpretation model take holistic device oriented approach 
model includes wider range faults replicas subject 
explicitly incorporates latent faults occur long detected correlated faults fault causes multiple faults result error 
model incorporates highlights importance detection process latent faults 
model simplistic highlights needed areas gathering reliability data helps evaluate strategies improving reliability systems long lived data 
example able answer questions dangerous latent faults long time periods 
quite dangerous better replicate archive tape disk 
disk better increase mean time visible faults latent faults 
significantly decreases better increase replication system increase independence existing replicas 
replication increasing independence help strategies proposed believe worth revisiting context long term storage 
conclude important strategies increasing reliability long term storage detecting latent faults quickly automating repair faults faster cheaper increasing independence data replicas 
includes increasing geographic administrative organizational third party independence diversity hardware software 
hope analysis primitive motivate renewed look design storage systems preserve data decades volatile hostile environments 
need long term preservation preserving information decades centuries proved beneficial cases 
th century bc shang chinese astronomers inscribed eclipse observations oracle bones animal bones shells 
years researchers records bc estimate total clock error accumulated just hours derived value viscosity earth mantle weight 
longitudinal studies field medical research depend accurate preservation detailed patient records decades 
scientists began study residents framingham massachusetts understand large increase heart disease victims 
data collected decades research scientists discovered major risk factors modern medicine knows contribute heart disease 
ussr sent probes surface venus collect data photographs 
resulting photographs low quality relegated science history 
years american scientist able modern digital image processing algorithms enhance photographs reveal detailed images 
timescales decades centuries contrast typical year lifetime computing hardware similar lifetimes attributed digital media 
just scientific data expected persist timescales 
legislation require companies organizations keep electronic records decades 
addition consumers analog assets mail photographs persist decades happily digital versions online services 
associated marketing literature encourages expect similar longevity 
threats long term preservation traditional short term storage applications anticipate variety threats list threats long term storage longer diverse 
threats media software obsolescence particular long term preservation 
threats natural disaster human error threats short term storage applications probability occurrence higher longer desired lifetimes archival data 
section list threats long term storage provide real examples motivate 
take approach identifying failures concentrating just storage device faults faults environment processes support surrounding storage systems 
large scale disaster 
large scale disasters floods fires earthquakes acts war anticipated long desired lifetimes archival data 
disasters typically manifested types threat media hardware organizational faults case data centers affected attack world trade center 
human error 
ways data lost users operators accidentally deleting marking content need accidentally purposefully deleting data discover need 
instance rumors suggest large professionally administered digital repositories suffered administrative errors caused data loss replicas organizations hosting repositories unwilling report publicly see 
errors affect hardware tapes lost transit software required driver infrastructure turning air conditioning system server room preservation application runs 
human error increasingly cause system failures 
component faults 
view system components expected fail including hardware software network interfaces services 
hardware components suffer transient re coverable faults caused temporary power loss catastrophic faults power surge controller card 
software components including firmware disks suffer bugs pose risk stored data 
systems assume network transfers disseminate content terminate specified time period deliver content unaltered 
initial large collections repository errorprone 
third party components easily preserved sources problems 
external license servers companies run longer exist decades application data archived 
domain names vanish reassigned fails pay registrar persistent url resolve resolver service fails preserve data care storage system client 
media faults 
storage medium component particular interest 
affordable digital storage medium completely reliable long periods time medium degrade degradation medium errors cause bit faults called bit rot 
storage media subject sudden loss bulk data instance disk crashes 
bit rot particularly troublesome occurs warning detected late repairs 
familiar example cd roms 
studies cd roms indicate despite sold reliable decades years years stored accordance manufacturer recommendations 
disks subject similar problems 
instance previously readable sector unreadable 
sector readable contain wrong information due previously misplaced sector write due problems vibrations 
media hardware obsolescence 
time media hardware components obsolete longer able communicate system components fault 
problem particularly acute removable media long history remaining theoretically readable suitable reader 
examples include track tape inch video laser discs 
evolution industry specification pcs difficult purchase commodity pc built floppy drive indicating floppy disks considered obvious ubiquitous cheap storage medium endangered 
software format obsolescence 
similarly software components obsolete 
manifested format obsolescence bits data encoded remain accessible information longer correctly interpreted 
proprietary formats widespread vulnerable 
instance digital camera companies proprietary raw formats recording raw data cameras 
formats undocumented ceases exist support format photographers lose vast amounts data 
loss context 
metadata generally context includes information layout location interrelationships stored objects subject provenance content processes algorithms software needed manipulate 
preserving contextual metadata important preserving actual data hard recognize required context time collect 
encrypted information particularly challenging example preservation decryption keys essential alongside preservation encrypted data 
unfortunately long periods time secrets decryption keys tend get lost leak get broken 
problem threat short term storage applications assets live long context lost information uninterpretable 
attack 
traditional repositories subject long term malicious attack reason expect digital equivalents exempt 
attacks include destruction censorship modification theft repositories contents disruption services 
attacks short term long term legal illegal internal external 
motivated ideological political financial legal factors rights employee dissatisfaction 
attacks threat short term storage researchers usually focus short term intense attacks slowly attacks long term repositories 
abuse computer systems involves insiders digital preservation system anticipate attack completely isolated external networks 
examples attacks include cases sanitization government websites conform administration world view 
organizational faults 
system view long term storage include merely technology organization embedded 
organizations die bankruptcy change missions 
storage technology support needs survive 
system planning envisage possibility asset represented preserved content transferred successor organization properly disposed data exit strategy storage services mistakes assets dependent single service lost 
example organizational change large closed research lab requested lab research projects copied tape sent labs 
unfortunately knew tapes allowed documentation contents 
clear project data useful current researchers time passed identify tape volume data huge reconstruct index 
example user lost digital photos stored purchase required interval receive email warnings due having failed send updated email address 
services data exit strategy easy bulk retrieval original high resolution format customer photos 
economic faults 
organizations materials preserve large budgets apply problem declare success just managing get collection put online 
unfortunately provides plan maintaining collection accessibility quality 
ongoing costs power cooling bandwidth system administration equipment space domain registration renewal equipment 
information digital form vulnerable interruptions money supply information budgets digital preservation expected vary possibly zero time 
budget issues affect ability preserve collections desired libraries subscribe fewer monographs 
motivating investment preservation difficult better tools predict long term costs especially target audience preserved information exist time decisions 
budget issue purchase storage system usually easier plan money spent shorter lifespan 
dangerous assumptions threats understood succeeded solving problems 
lose data 
part reason system designs fail take perspective practice key potentially dangerous assumptions 
include visibility faults independence faults money apply exotic solutions described 
fault visibility assumption faults detected time error causes occur silently 
called la tent faults sources latent faults media errors best known 
head crash detectable bit rot uncovered affected faulty data accessed audited 
example sector disk unreadable detected read sector 
sector readable contain incorrect information due previous misplaced sector write 
silent media errors faults occur frequently assumed instance schwarz suggest silent block faults occur times disk faults 
aggregate archives internet archive supply users data items high rate average data item accessed infrequently 
detecting loss corruption user requests access renders average data item vulnerable accumulation latent faults 
need guard latent faults long recognized larger systems increases storage capacity brought attention problem commodity storage 
important example iron file system uses redundancy single disk address latent faults file system metadata structures 
media faults types latent faults caused threats human error accidental deletion overwriting materials discovered materials needed 
component failure reliance failed system component third party component longer available discovered data depending component accessed 
media hardware obsolescence failure obsolete seldom media reader discovered information associated medium needs read 
impossible costly purchase reader 
software format obsolescence accessing old information discover format belonging application longer run 
loss context discover missing crucial metadata saved data try sense data 
instance preserved encryption key 
attack results successful censorship corruption attack data repository discovered apparent accessing data long attack 
general solution latent faults detect quickly possible indicated model 
instance scrubbing detect media faults evidence data corruption attack 
preserving data important scrubbing performed data repaired replica error correction codes 
higher layer prevent latent faults making detecting need migrate content old new media old new data formats lost ability read old medium interpret old data format 
example detecting need re encrypt old materials new keys old keys considered obsolete 
independence assumption researchers designers correctly point data replication strategy preventing data loss assumption replicas fail independently 
alas practice faults independent hope 
logged fault large disk farm disk drives uc berkeley months showed significant correlations 
example power units shared drives single power outage affect large number drives 
single power outage accounted machine restarts 
temperature vibrations tightly packed devices machine room sources correlated media failures 
perspective sources fault correlation corresponding threats large scale disaster single large disaster destroy replicas data 
geographic replication clearly helps care taken ensure provides sufficient independence 
example disaster new york city data center destroyed 
system failed replicated data center side river failover worked correctly 
unfortunately sites sufficiently distant chaos streets prevented staff getting backup data center 
eventually unable continue unattended 
human error system administrators human fallible 
unfortunately systems powerful able destroy modify data restriction 
replicas unified administrative control single human error cause faults 
component faults replicas information dependent external component instance license server loss component causes correlated faults replica 
loss context losing metadata associated archival data cause correlated faults replicas 
instance materials encrypted key replicas loss key replicas useless 
attack attacks cause correlated faults 
instance flash worm affect replicas 
organizational faults long term storage anticipate failure organization service 
increasing visibility digital assets developing simple exit strategies data important minimizing dependence single organization 
indicated model best way avoid correlated faults independence replicas 
simply increasing replication ensure independence replicas geographically administratively 
unlimited budget assumption biggest threats digital preservation economic faults 
lot money apply solutions preserving data information people see live forever hands organizations unlimited budgets 
solutions synchronous mirroring raids widely dispersed geographic replicas affordable long term organizations 
qualitative attempt compare costs strategies solutions explore estimation costs remains difficult area richly deserving 
analytic model models patterson chen useful reasoning reliability different replicated storage system designs 
section build models incorporate effect latent correlated faults reliability arbitrary unit replicated data 
model agnostic unit replication bit sector file disk entire storage site 
attempt develop model interpreted general holistic fashion 
coarse grained model helpful reasoning relative impact broader range faults detection times repair times correlation 
model helps point strategies increase reliability data need measure real systems resolve tradeoffs strategies 
start simple definition latent faults 
derive mean time failure mirrored data face immediately visible latent faults 
extend equation include effect correlated faults 
discuss implications equation long term storage 
latent visible fault fault time types replica faults 
time flows left right 
top visible fault sad face detected recovery begins immediately 
successful recovery fault corrected smiley face 
bottom latent fault occurs sad face happens fault detected 
fault detected visible faults recovery takes place 
fault types assume types faults immediately visible latent shown 
visible faults time occurrence detection negligible 
causes faults include entire disk controller errors 
denote mean time visible fault mv associated mean time repair mrv 
latent faults time occurrence detection significant 
examples include writes bit rot unreadable sectors data stored obsolete formats 
denote mean time latent fault ml mean time repair mrl 
consider latent faults detectable finite mean time occurrence detection denoted mdl 
assumptions model assumptions 
build model starting simplest assumptions increase complexity needed 
start additional information simplest assumption regarding processes generate faults latent visible memoryless 
probability fault occurring time independent past 
assumption leads exponential distribution mttf 
mttf mean time fault 
parts derivation consider case mttf approximation holds mttf mttf 
mttf approximation similar ones simplify expression exponential prob ability similar patterson fundamental model 
initially assume faults occur independently 
subsequently revise assumption introducing correlated errors exponentially distributed increased rate occurrence 
simplicity model increased rate multiplicative correlation factor assumed latent visible faults 
furthermore undetectable faults exist ignore analysis 
effect reliability dominate rate occurrence significant 
case turn detectable faults developing detection mechanism correct employing redundancy 
remain main vulnerability stored data 
reliability mirrored data successive faults copy fails initial fault repaired event call double fault 
double fault leads data loss mirrored repli cas section equal rate double fault failures mean time data loss 
section derive expression quantity represents reliability mirrored data understand affected visible latent correlated faults 
note double fault rate meaningful regardless faults causing failures detected detectable 
section reliability analysis perspective data perspective user errors go unnoticed 
estimate need estimate probability second fault occur fault 
refer period window vulnerability 
types faults need consider window vulnerability type illustrated 
consider visible fault average mrv 
latent visible faults occur 
probability visible fault occurs mrv mv mrv mv 
obtain result approximation eqn 
probability latent fault occurs mrv ml mrv ml 
difference st nd latent visible visible latent combinations double faults resulting data loss 
axis indicates type fault sad face axis indicates type second fault second sad face 
fault occurs window vulnerability occurrence second fault lead data loss 
visible faults window consists recovery period 
latent faults window includes time detect fault 
correlated second fault occur window 
arises different rates fault occurrence 
consider latent fault average mrl mdl 
latent visible faults occur difference probabilities occurrence simply due different rates occurrence visible latent faults 
probability visible fault occurs mdl mrl mv probability latent fault occurs mdl mrl ml mrl mdl mv mrl mdl ml 
note mdl large equations hold combined approaches 
calculate total double fault failure rate follows account correlated faults assume probability second fault conditioned occurrence exponentially distributed faster rate parameter 
introduce multiplicative correlation factor reduces mean time subsequent fault initial fault occurs 
case equations multiplied undoubtedly vast simplification faults correlate practice 
modeling correlations accurately relies modeling particular system instantiation component interactions considered black art 
simplification 
alternative introduce distinct mttf correlated faults independent mttf done raid chen 
combining previous equations accounting correlated faults ml mv mv ml mrv ml mrl mdl mv implications understand implications equation investigate behavior various operating ranges 
consider cases visible faults frequent latent faults vice versa 
consider case latent fault occurs long 
briefly discuss implications cases touch reliability metrics higher levels redundancy 
consider case visible fault rate dominates latent fault rate mrl mdl mrv mv ml 
mv mrv mv ml ml mrv ml mrl mdl mv 
case effect latent faults negligible equation appropriately resembles original raid reliability model 
hand latent fault rate dominates visible fault rate mrl mdl mrv ml mv ml mrl mdl mv ml mv ml mv mrv ml mrl mdl mv 
equation indicates latent faults term right side counts fraction frequent reduce mdl negate visible faults result double failures additional ml factor reliability result replication 
second term counts fraction latent faults result consider case visible fault rate double failures 
dominates mrv mv ml latent faults frequent non negligible 
window vulnerability latent fault occurs long latent faults play role increasing failure rate 
long detection time latent faults long mdl ml repair times long mrl ml mrl mdl ml 
case equation ensuring single latent fault extremely lead double fault failure 
result approximation mv mrv mv ml holds latent faults rates non negligible ml mv specializations model point important implications 
equations varies quadratically mv ml particular minimum mv ml 
consider occurrence rates fault types improve system reliability 
careful sacrifice happen practice mv ml anti correlated depending hardware choice detection strategy 
second equation indicates latent faults frequent important reduce detection time just repair time 
specifically consider seagate cheetah disk mv hours bandwidth mb capacity gb leading mrv minutes 
schwarz assume latent faults times visible faults resulting ml hours 
scrubbing justify approximations leading equation 
applying equation substituting achieve years 
gives probability data loss years plug exponential distribution 
hand replica times year suggested schwarz mdl hours half scrubbing period 
applying equation reliability increases significantly 
correlated errors years gives chance data loss years 
implies proactive searching latent faults appropriate 
agrees 
third model correlation multiplicative factor affects reliability regardless type fault 
continuing example assume suggested chen 
years gives chance data loss years 
conservative assumption correlation factor vary orders mag 
obtain reasonable lower bound consider correlated mean time second order magnitude larger re time mv mrv mrv mv example bug firmware recovery code raid controller cause mean time mean time recover 
obtain specific lower bound value assume values mv mrv mrl resulting gives range orders magnitude 
fourth latent faults infrequent equation indicates attempting detect latent faults relying lengthy recovery procedures fix leave system vulnerable 
example ml mv mrv remain years leading data loss years 
case system handling latent faults data susceptible double fault failures visible latent faults initial latent fault 
replication correlated faults section show additional replication offer additional reliability independence 
simplify reliability analysis higher degrees replication assume instrumented system mdl negligible assume latent visible faults similar rates repair times 
roughly estimate system degree replication extending analysis lines mirrored data 
calculate probability successive compounding faults initial fault leave system integral copy recover 
calculate probability successive faults occurring vulnerability windows previous fault 
simplicity analysis assumes vulnerability windows length mrv overlap exactly 
case probability th copy fails previous failed copies roughly mrv mv probability successive copies fail previously failed copies product probabilities mrv mv fault occurs rate mv mean time data loss mv mv mrv mv mrv equation shows increasing level replication geometrically increases high degree correlated errors ge decrease offsetting gains additional replicas 
strategies simple model reveals number strategies reducing probability data loss 
generally describe terms familiar hardware media faults applicable kinds faults 
instance addition detecting faults due media errors scrubbing detect corruption data loss due attack 
example similar process cycling data albeit reduced frequency detect data endangered formats convert new formats longer interpret old formats 
increase mv example storage media subject catastrophic data loss disk head crashes 
increase ml example storage media subject data corruption formats subject obsolescence 
reduce mdl example auditing data frequently detect latent data faults raid scrubbing 
reduce mrl example automatically repairing latent data faults alerting operator 
reduce mrv example providing hot spare drives recovery start immediately operator replaced drive 
increase number replicas survive simultaneous faults 
increase increasing independence replicas 
rest section examine practicality costs techniques implementing strategies 
provide examples systems techniques 
increase mv ml seagate specifications gb consumer drive visible fault probability year service life gb enterprise cheetah fault probability 
cheetah costs times byte quoted bit error rate cheetah drives spend year life idle suffer cheetah bit errors 
fold increase cost consumer enterprise disk drives yields approximately half probability service fault probability bit fault 
long term storage applications gb versus gb 
prices com 
requirements latency individual disk bandwidth minimal large incremental cost enterprise drives hard justify compared smaller incremental cost sufficiently independent replicas consumer drives 
reduce mdl expensive approach addressing latent faults due media errors detect faults soon possible repair 
way detect faults audit replicas reading data computing checksums comparing replicas 
assuming unrealistically detection process perfect latent faults occur randomly mdl half interval audits way reduce audit frequently 
way put reduce mdl devoting disk read bandwidth auditing reading data suggests systems achieve reasonable balance auditing versus normal system usage 
line replicas disk copies significant advantages line copies tape backups reasons 
cost auditing line copy includes cost retrieving storage mounting reader returning storage 
considerable especially line copy secure site storage 
second line media designed accessed frequently involve errorprone human handling 
audited audit process significant cause faults limit determined part system strategy powering components 
auditing line copies hand significant cause highly correlated faults error prone human handling media media degradation caused reading process 
audit strategy particularly important case digital preservation systems probability individual data item accessed user disk lifetime vanishingly small 
system depend user access trigger fault detection recovery long time accesses latent faults build swamp recovery mechanisms 
system aggressively audit replicas minimize mdl 
lockss system example system 
note relying line replicas security fool proof 
line storage may reduce chances attacks may vulnerable insider attacks 
harder audit damage due attacks may persist longer 
reduce mrl mrv mean time repair latent media fault normally far mean time detect similarly mean time repair visible fault far mean time occurrence reducing mean time repair important reducing window system vulnerable correlated faults 
case line replicas major advantage repair times media faults short media access times 
human intervention needed process repair cause additional correlated faults 
repairing line media incurs high costs long delays potential correlated faults auditing line media 
increase replication line media common approach increasing replication 
processes auditing recovering faults line backup copies slow expensive error prone 
options disk replication strategies include replication raid systems raid systems simple mirrored replicas 
replication raid systems provide geographical administrative independence replicas 
opt geographic administrative independence replicas extra single site reliability provided raids worth extra cost system wide perspective 
cost disparity enterprise grade drives consumer grade drives see adding simple mirrored replicas non raid configurations cost effective approach increasing replication reliability 
oceanstore example large number replicas cheap disks :10.1.1.115.4299
increase independence just months study correlated faults observed apparently caused disks sharing power cooling scsi controllers systems sharing network resources 
model suggests cases far lower rates correlated faults increasing independence replicas critical increasing reliability long term storage 
long term storage systems reduce probability correlated faults striving diversity possible hardware software geographic location administration avoiding dependence components single organizations 
examples include hardware disks array come single manufacturing batch 
firmware hardware age point lifetime failure curve 
increased cost incurred giving supply chain efficiencies bulk purchase hardware diversity difficult 
note replacing components large archival system impossible 
new storage added rolling time differences storage technologies vendors time naturally provides hardware heterogeneity 
software systems software vulnerable epidemic failure studied natural diversity systems campus reduce vulnerability 
increased costs caused encouraging diversity terms merely purchasing training administration difficult option organizations 
british library system unusual explicitly planning develop diversity hardware software time 
speed malware find networked systems sharing vulnerability increasing diversity platform application software effective strategy increasing 
geographic location systems line backup store replicas site despite additional storage handling charges implies 
digital preservation systems british library establish line replicas different location despite possible increased operational costs doing 
administration human error common cause correlated faults replicas 
british library system unusual ensuring single administrator able affect replica 
probably effective cost effective attempts implement dual key administration administrator approve potentially dangerous action 
crisis shared pre conceptions cause operators mistake 
components system designs avoid dependence third party components preserved time 
determining sources dependence tricky sources detected running systems isolation see breaks 
example running system network domain name service certificate authority determine system dependent services 
lockss example system built independent survival services 
organization view preservation systems important support organizational independence 
instance importance collection extends current organization easy costeffective exit strategy collection organization ceases exist 
example couples want survival babies photos depend home system maintenance requires continued 
summary main techniques increasing reliability long term storage replication independence replicas auditing replicas detect latent faults automated recovery reduce repair times costs 
tradeoffs unfortunately strategies necessarily orthogonal adverse affects reliability 
consider effects auditing automated recovery 
possible tradeoffs costs increased independence administrative domains diversity hardware software cover 
auditing necessary detecting latent faults previously described increases frequency media access increase visible latent media errors costs due increased consumption power system administrative resources 
previous suggests possible achieve appropriate balance increases reliability considerably performing audit background opportunistically legitimate data accesses require powering corresponding system components 
unfortunately audit process introduce channels data corruption 
example attacking distributed system audit protocol designed carefully distributed protocol 
automated recovery reduce costs speed recovery times buggy compromised attacker introduce latent faults 
dangerous visible faults seemingly having recovered turn latent ones 
data gathering simple model reliability replicated storage points areas great need data validate model evaluate potential utility reliability strategies described 
particular little published types distribution latent faults due media errors due threats describe 
correlations result latent faults poorly un 
desirable application model choosing instance levels replication audit 
assume disaster tolerance geographically independent replica systems 
better system audit storage internally 
better audit replicas 
answering question requires understanding minimum mttf visible latent faults mdl different audit strategies recovery strategies costs replicating information internally geographically costs auditing internally versus sites 
gather information instrument existing systems 
start log occurrences visible faults detection latent faults occurrences data loss 
approach detecting latent faults cycle proactively storage logging checksums immutable objects 
ideally fault occurrences include timestamps vertical information location fault block sector disk file application information determine distributions rates faults consider testing validity assumptions 
additionally log information recovery procedures performed replacement disks recovery tape duration outcomes 
data measure mean recovery times combined previous information validate model 
employ metadata configuration media hardware software estimate costs 
log smart data disks external information application workloads offered processes spawned file system network statistics administrator changes 
mine information identify perform root cause analysis correlated faults 
efforts underway gather information existing systems 
instance groups uc santa cruz hp labs processing just failure data large archives internet archive 
need information different kinds storage systems different replication architectures 
related section review related showing address problems correlated latent faults arise large amounts data remain unaltered accessible low latency low cost 
start low level approaches focus single devices raid arrays move stack 
evidence correlated faults comes studies disk farms 
logs fault large disk farm disk drives uc berkeley months shows significant correlations 
study focuses primarily media drive failures power failures system software dependency failures 
chen explore tradeoffs performance reliability raid systems noting system crashes correlated faults bit errors latent faults greatly reduce reliability predicted original raid 
kari dissertation appears comprehensive analysis latent disk failures 
model shows greatly reduce reliability raid systems presents scrubbing algorithms adapt disk activity idle time flush latent errors 
contrast explore broader space includes application faults distributed replication 
enterprise storage systems recognized need address latent correlated faults 
network appliance storage threat model includes disk failures disk failure latent fault discovered recovery 
employ parity suggest periodic scrubbing disks improve reliability 
schwarz show opportunistic scrubbing piggy backed disk activity performs 
depend disk detect latent fault check data 
exploration includes higher layer failures 
database vendors implemented application level techniques detect corruption 
db threat model includes failure write database page spanning multiple sectors atomically 
page consistency bits page modifying checking read write 
bits detect forms corruption affecting consistency bits 
tandem nonstop systems write checksums disk data compare data read back 
iron file system file system threat model includes latent faults silent corruption disk 
protects file system metadata data checksums disk replication 
alternative tightly coupled replication raid loosely coupled distributed replication 
saltzer suggested digital archives need geographically distributed replicas survive natural disasters proactively migrate new media survive media obsolescence heavy duty forward error correction correct corruption accumulate data rarely accessed 
ideas inform design british library digital archive 
distributed peer peer storage architectures proposed provide highly available persistent storage services including eternity ser vice intermemory cfs oceanstore past :10.1.1.110.5867:10.1.1.159.9358:10.1.1.16.1952:10.1.1.115.4299
threat models vary include powerful adversaries eternity service multiple failures 
oceanstore cryptographic sharing proliferate partial replicas recover data 
past replicate files 
weatherspoon model compares reliability approaches :10.1.1.121.9064
model include latent errors correlated errors operational human errors takes account storage bandwidth requirements approach 
identifies correlations replicas geographic administrative informs replication policy reduce correlation effects 
deep store lockss system share belief preserving large amounts data long periods major design challenge 
deep store addresses cost issues eliminating redundancy lockss network appliance approach reducing system administration large numbers loosely coupled replicas low cost hardware 
recognize threats bit rot format obsolescence long term preservation 
motivate need long term storage digital information examine threats information 
extended reliability model incorporates latent faults correlated faults detection time latent faults reason possible strategies improving long term reliability systems 
cost strategies important limited budget key threats digital preservation 
find important strategies auditing detect latent faults soon possible automating repair fast cheap reliable possible increasing independence data replicas 
clearly progress 
data characterize terms model 
model points data collection projects useful 
instance need data mean time different types latent faults repair times faults detected levels correlation different kinds faults 
currently seeking sources data instrumenting systems gather 
digital preservation system rarely suffer incidents cause data loss 
total experience base available designers systems grow slowly time making difficult identify fix problems undoubtedly arise 
past incidents indicate practice difficult accumulate experience base 
host organization typical response incident causing data loss cover 
details known grapevine true story part experience base 
system similar nasa aviation safety reporting system established operators storage systems submit reports incidents resulting data loss read anonymized form learn improve reliability systems 
th congress united states america 
public law health insurance portability accountability act aug 
th congress united states america 
public law act july 
amia 
fact sheet estimating tape life 
www org publication resources guidelines tape life html 
anderson riedel interface scsi vs ata 
proceedings nd usenix conference file storage technologies mar usenix pp 

anderson eternity service :10.1.1.16.1952
proceedings st international conference theory applications cryptology prague czech republic 
arl association research libraries 
arl statistics 
www arl org stats pub intro html 
baker keeton martin traditional storage systems don help save stuff forever 
proc 
st ieee workshop hot topics system dependability 
bartlett commercial fault tolerance tale systems 
ieee transactions dependable secure computing 
chen lee gibson katz patterson raid high performance reliable secondary storage 
acm computing surveys june 
chen goldberg gottlieb yianilos prototype implementation archival intermemory 
international conference digital libraries berkeley ca usa pp 

corbett english goel kleiman leong sankar row diagonal parity double disk failure correction 
rd usenix conference file storage technologies san francisco ca mar 
dabek kaashoek karger morris stoica wide area cooperative storage cfs :10.1.1.159.9358
proceedings eighteenth acm symposium operating systems principles chateau lake louise banff ab canada oct pp 

moore epidemiological approaches heart disease framingham study 
american journal public health mar 
diffie perspective decrypting secret strong security 
news com com 
html jan 
gibson redundant disk arrays reliable parallel secondary storage 
phd thesis university california berkeley apr 
google gmail 
gmail google 
com gmail help html june 
gmail different 
just gets archived needed 
gray szalay stoughton online scientific data curation publication archiving 
tech 
rep msr tr microsoft research july 
hansen hotmail customer files 
news com com hotmail customer files html june 
hole department education delete years research website 
www 
org edu ed info htm 
cd 
www pc active nl asp 
www com news 
committee inst 
dia 
tape backup vis vis online backup 
harmony july 
bhagwan marzullo voelker surviving internet catastrophes 
proceedings usenix annual technical conference apr 
kari latent sector faults reliability disk arrays 
phd thesis computer science department helsinki university technology espoo finland 
keeney kowalski moore rogers insider threat study computer system sabotage critical infrastructure sectors 
www gov report pdf may 
keeton anderson backup appliance composed high capacity disk drives 
hotos may 
kubiatowicz bindel chen czerwinski eaton geels gummadi rhea weimer wells zhao oceanstore architecture global scale persistent storage :10.1.1.115.4299
proceedings th international conference architectural support programming languages operating systems cambridge ma usa nov pp 

lampson sturgis crash recovery distributed data storage system 
tech 
rep xerox palo alto research center june 
lau personal communication sept 
lazarus precious photos disappear 
san francisco chronicle feb 
gun auction site shut disk disaster 
www uk gun auction site shut 
myth year cd rom 
slashdot apr 
university california santa cruz 
personal communication apr 
maniatis roussopoulos giuli rosenthal baker lockss peer peer digital preservation system 
acm transactions computer systems feb 
martin personal communication feb 
white house web scrubbing 
washington post dec www com ac wp dyn article node content id dec notfound true 
mohan disk read write optimizations data integrity transaction systems write ahead logging 
icde 
nasa 
aviation safety reporting system 

arc nasa gov 
neumann stanford business school hit windows computer disaster 
ncl ac uk risks html subj apr 
oclc 
persistent uniform resource locator 
purl 
oclc org 
oppenheimer ganapathi patterson internet services fail done 
th usenix symp 
internet technologies systems march 
pang yau hung hsiang chou 
earth rebound lower mantle viscosity analysis ancient chinese eclipse records 
pure applied geophysics sep 
patterson gibson katz case redundant arrays inexpensive disks raid 
acm sigmod chicago il usa june pp 

patterson gibson katz case redundant arrays inexpensive disks raid 
proceedings acm sigmod international conference management data chicago il usa june pp 

agrawal arpaci dusseau arpaci dusseau iron file systems 
proceedings th symposium operating systems principles 
reason human error 
cambridge university press 
reuters 
time warner says employee data lost data 
www reuters com 
may 
rosenthal digital preservation network appliance openbsd 
proceedings san mateo ca usa sept 
rowstron druschel storage management caching past large scale persistent peer peer storage utility :10.1.1.110.5867
proceedings eighteenth acm symposium operating systems principles chateau lake louise banff ab canada oct pp 

saltzer fault tolerance large archival systems 
proceedings fourth sigops european workshop bologna italy sept 
schwarz xin miller long ng disk scrubbing large archival storage systems 
th international symposium modeling analysis simulation computer telecommunication systems mascots oct pp 

seagate 
st configuration specifications 
www seagate com support disc specs ata st html sept 
seagate 
cheetah 
www seagate com cda products enterprise tech html 
characterizing large storage systems error behavior performance benchmarks 
phd thesis cs div univ california berkeley berkeley ca usa oct 
working group 
raw problem 
org 
tom stalk stacks 

ucsd edu htm 
towers personal communication july 
waldman mazi res censorship resistant publishing system document 
proceedings th acm conference computer communications security philadelphia pa usa nov pp 

weatherspoon kubiatowicz erasure coding vs replication quantitative comparison :10.1.1.121.9064
iptps mar 
weatherspoon kubiatowicz introspective failure analysis avoiding correlated failures peer peer systems 
proceedings international workshop reliable peer peer distributed systems oct 
images reveal hot venus 
bbc news jan 
pollack long deep store archival storage system architecture 
international conference data engineering 
