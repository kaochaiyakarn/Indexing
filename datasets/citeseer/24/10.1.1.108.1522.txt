trade depth impurity pruning decision trees fournier cr cnrs universit de dex france fournier info fr pruning methods decision trees minimize classification error rate 
uncertain domains sub trees lessen error rate relevant point populations specific interest give representation large data file 
propose new pruning method called pruning takes account complexity sub trees able keep sub trees leaves yielding determinate relevant decision rules keeping ones increase classification efficiency 
decision trees successfully different decision making classifications tasks 
broadly speaking decision tree built set training data having attribute values class name 
result process represented tree nodes specify attributes branches specify attribute values 
leaves tree correspond sets examples class elements attributes available 
construction decision trees described breiman 
important known monograph classification trees 
number standard techniques developed example basic algorithms id cart 
survey different methods decision tree classifiers various existing issues landgrebe 
areas sure priori impossible build tree correctly classifies examples 
situations decision tree algorithms tend divide nodes having examples main drawback appears see resulting trees tend large overspecified 
branches especially bottom due sample variability statistically meaningless say due noise sample 
branches built pruned 
want build set rules building tree 
example give dividing node elements remaining attributes independent class quinlan termination criterion test 
happen remaining attribute taken individually improve classification taken classify data 
know better generate entire tree prune see example 
section briefly presents overview existing decision trees pruning methods sets question pruning uncertain domains 
focus quality index tree able point sub populations interest large populations 
quality index define pruning method uncertain domains take account complexity sub trees 
section propose new quality index called depth impurity quality index uses depth trees 
index infer new pruning method decision trees denoted pruning appropriate uncertain domains usual methods classification error rate method bound possible tree classification 
able give efficient description data file oriented priori classification elements highlight interesting sub populations classification error rate decrease 
think major point decision trees uncertain domains 
section examples real world domains pruning tool optimize final decision tree 
pruning uncertain domains motivations overview existing decision trees pruning methods principal methods pruning decision trees examined 
cestnik bratko suggest approach upgrading minimization expected classification error method 
method uses laplace probability estimates 
cestnik bratko improvements original method 
method need assume uniform initial distribution class degree pruning affected number classes produces trees pruned various degrees 
agreement mingers state better propose family trees pruned different degrees single tree 
gelfand ravishankar delp propose method data set divided equal subsets iteratively tree build subset pruned 
empirical evaluation waveform recognition problem suggests superiority method cost complexity pruning method 
esposito malerba semeraro question decision tree pruning search state space represented directed acyclic graph vertices possible pruned trees 
call vertices graph 
edge obtained pruning branch depth 
problem tree pruning seen problem visiting different vertices aim searching vertex highest value evaluation function estimates quality tree 
exhibit evaluation functions known pruning methods order framework 
unified view tree pruning problem allows easier comparison pruning methods emphasize strengths weakness 
empirical comparison concludes iterative growing pruning algorithm generally gives best results quite unstable meaning experiments produces highest error rate 
holder considers intermediate decision trees sub trees entire decision tree generated breadth order 
pruning strategy computes number splits leading minimum error intermediate decision tree classification error rate calculated software 
pruning methods minimizing classification error rate element node classified frequent class node 
estimated test file statistical methods cross validation bootstrap 
cart pruning algorithm crawford compares techniques concludes bootstrap better small learning sets 
pruning methods inferred situations built tree classifier 
section describes quality index tree allows prune trees aim highlight interesting sub populations 
method classification error rate claim major point uncertain domains 
see main differences methods 
quality index pruning possible attribute selection criteria building decision trees exist know theoretical level criteria derived impurity measure perform comparably 
term impurity usual machine learning community 
called criteria criteria concave maximum criteria impurity measure characteristics defined concave function 
commonly criteria shannon entropy family software gini criterion cart algorithms criteria 
introduce notations set quality index position pruning section methods seen 
call class examples data set values node attribute defined values consider criterion denoted see fig 

splitting node attribute know see sub nodes yielded rate impurity measure details impurity measure viewed combined measure impurity sub nodes induced value criterion node reflects appropriately chosen attribute divides data 
minimum value reached subnodes induced pure respect examples value imply value maximum value frequency distributions subnodes induced equal 
mingers proposes method called critical value pruning estimates relevance node value selection attribute criterion 
method needs choice threshold depends criterion data study rules choices hardly elicited 
experiments uncertain situations flaw pruning process take account size root pruned tree 
value criterion permits comparison divisions node sub tree built node 
fortunately theoretical considerations embedding criteria consistently yield quality index see details 
denote root tree built criterion leaves 
define attribute values correspond branch leading leaf example 
take mean impurity leaves rescaling yields quality index tree measures difference impurity root mean impurity leaves difference normalized value shown leaves pure identical 
depend criterion 
trees mean impurity leaves impurities roots different 
quality index allows define straightforward pruning method called pruning goes criterion build tree 
pruning consists pruning highest quality 
produces family nested trees spreading initial large sub tree tree tree restricted root 
curve global quality index function number pruned tree gives pragmatic hard automate method pruning process 
frequency distributions root leaves seen section pruning methods classification error rate 
claim important point quality index able give prominence sub trees high value quality index sub trees improve classification rate 
example consider sub tree depicted fig 
classification error rate equal rate root 
fig 
node resp 
second value indicates number examples having resp 
second value sub tree lessen error rate root leaves sub tree interest points specific population constant value remaining population impossible predict value quality index sub tree means explains initial impurity 
tree interesting decrease number errors 
limit index quality obvious limit index quality take account complexity tree 
example consider denoted trees leaves pure 
accordance definition see equation quality index equal 
depth instance clear prefer simpler furthermore far fewer nodes 
idea commonly decision trees pruning process 
example cost complexity method breiman trade classification error rate sub tree number nodes 
methods seen section method classification error rate opinion drawback uncertain domains 
claim index values quality deep tree relevant small deeper tree understandable reliable 
example fig 
prefer denoted tree root root index trees quality 
di di di examples values di di properly define new quality index takes account structure sub tree means impurities depths leaves 
section presents index called depth impurity corresponds trade depth impurity leaf tree 
quality index impurity measure tied decision tree algorithm 
depth impurity quality index pruning framework index quality formulate question quality index 
claim quality index tree maximum value conditions satisfied leaves pure 
ii depth 
comparison quality index see section equation framework adds constraint ii rescale value quality index respect root tree 
equation defines quality index depth impurity tree note quality index tree root leaves rate impurity measure normalized note impurity defines purity stems quality relevant properties keep interesting sub trees index measure uncertain measure domains introducing damping function able take account depth leaves deeper leaf lower quality 
denoted led address question defining damping function argument depth node denoted 
minimal set constraints decreasing 
constraint corresponds damping process necessary satisfy condition ii framework 
furthermore add constraints tends total number attributes 
constraint means leaf depth similar total number attributes unreliable sensible value quality close minimum value constraint allows values damping function purity impurity leaf rough estimates 
achievement linear damping constraint suggested algorithmic considerations see allows linear computation indicated 
number elementary steps process limited computational cost pruning see section particularly low tractable large databases 
choice modify result pruning process see section 
translated mathematical form set constraints leads choose exponential function proof 
deduce get equality implies exponential function 
choose function total number attributes note exponential function chosen 
come back tree fig 
notice tree root denoted greater value root impurities leaves trees equal complex 
taken account recall theses trees value quality index regard algorithmic point view computation expensive 
easily write values sons words computation requires solely values trees roots nodes directly connected root proof 
previously call son node leaves tree root follows easily definition see equation rewritten ensuring computed node step point allow low computational cost pruning method 
move subject 
pruning purity root 
lower purity root infer cost due complexity useful benefit explanation generated cut replaced root leaf original tree 
cost complexity method breiman uses similar process means trade cost benefit cut sub trees cost classification error rate 
straightforward call pruning method pruning goes sub trees consists root pruning original tree satisfy condition leads straightforward way prune sub trees main idea compare experiments noticed degree pruning quite bound uncertainty embedded data 
practice means damping process adjusted data order obtain situations relevant number pruned trees 
introduce parameter denoted control damping process higher extensive pruning stage sub trees cut 
equation gives damping function updated parameter usual total number attributes find equation 
pruning produces family pruned trees spreading initial large tree tree restricted varying root 
see section easy select automatically best pruned tree main aim stage 
curves function function number pruned nodes give pragmatic method pruning process 
furthermore eager obtain automatically single best pruned tree procedure requiring test file final tree indicate way 
maximizes experiments designed induction software called uncertain decision trees produces decision trees 
ort criteria available attribute selection criterion depends kind data aim wished user see 
indicates quality index node prunes trees pruning 
available web address info fr 
included general data mining tool box major part devoted treatment missing values 
section describe results obtained running real world databases 
perform gain criterion shannon entropy commonly 
data set medical database coming university hospital grenoble france runs thrombosis 
second known crx data set taken uci database repository 
thrombosis thrombosis common pathology lead pulmonary life patient 
included general medical study gathering french medical centers order propose recommendations prevent deep thrombosis 
know pathology rests complex clinical findings hard domain 
words expected quality trees may poor 
data set composed instances described attributes 
class 
initial large tree nodes pure leaves 
depth small number instances ones unreliable produce rules classify new instances 
training file classification error rate 
fig 
depicts function varying fig 
function number pruned nodes leaves included 
general shape curves fig 
fig 
indicates relevant stages pruning stopped precisely pruning value number flat segment curve obtain smallest tree fixed quality 
notice best value reached pruning intermediate point phenomenon expected usual test file training file come back briefly point expresses high level uncertainty data 
function nodes results thrombosis function number pruned tab 
gives classification error rate column corresponding unpruned tree 
crx crx concerns approval credit facilities data set contains examples described continuous nominal attributes 
class 
errors table error rate thrombosis mis classified easier domain thrombosis initial tree pure leaves tab 
shows low classification error rate 
errors table error rate crx mis classified previously fig 
depicts function varying fig 
function number pruned nodes leaves included 
observe values better thrombosis domain expected result 
figs 
strengthen fact crx data uncertain thrombosis ones crx achieve value start cut sub trees 
stage reached pruning step removes nodes thrombosis data 
shows knowledge supplied tree sharing tree pure leaves led expect result pruning significantly increase quality tree training set 
function nodes results crx function number pruned proposed new quality index decision trees uncertain domains realizes trade impurities depth leaves tree 
stemming pruning method able keep sub trees lessen error rate point populations specific interest usual methods classification error rate 
claim major point uncertain domains 
normalize regard root tree order compare trees frequency distributions nodes different number instances 
means able compare sub trees different depth 
direction test file improve choice best pruned tree 
move stage select pruned tree reflects general sound knowledge studied data 
breiman friedman olshen stone 
classification regression trees 
statistics probability series 
wadsworth belmont 
buntine niblett 
comparison splitting rules induction 
machine learning pages 
catlett 
large decision trees 
proceedings twelfth international joint conference artificial intelligence ijcai pages sydney australia 
cestnik bratko 
estimating probabilities tree pruning 
kodratoff editor proceedings european working sessions learning ewsl lecture notes artificial intelligence 
pages porto portugal 
springer verlag 
crawford 
extensions cart algorithm 
international journal man machine studies 
cr robert 
pruning method decision trees uncertain domains applications medicine 
proceedings workshop intelligent data analysis medicine pharmacology ecai pages budapest hungary 
cr robert 
theoretical framework decision trees uncertain domains application medical data sets 
baud wyatt editors th conference artificial intelligence medicine europe lecture notes artificial intelligence 
pages grenoble france 
springer verlag 
cr robert 
uncertain domains decision trees ort versus criteria 
th conference information processing management uncertainty knowledge systems pages paris france 

esposito malerba semeraro 
decision tree pruning search state space 
brazdil editor proceedings european conference machine learning ecml lecture notes artificial intelligence 
pages vienna austria 
springer verlag 
fayyad irani 
attribute selection problem decision tree generation 
proceedings tenth national conference artificial intelligence pages cambridge 
ma aaai press mit press 
gelfand ravishankar delp 
iterative growing pruning algorithm classification tree design 
ieee transactions pattern analysis machine intelligence pages 
holder 
intermediate decision trees 
proceedings fourteenth international joint conference artificial intelligence ijcai pages montr qu bec canada 
mingers 
expert systems rule induction statistical data 
journal operational research society pages 
mingers 
empirical comparison pruning methods decision tree induction 
machine learning pages 
niblett 
constructing decision trees noisy domains 
proceedings nd european working sessions learning ewsl pages bled yugoslavia 
sigma press 
quinlan 
effect noise concept learning volume machine learning artificial intelligence approach 
morgan kaufmann publishers los altos ca 
quinlan 
induction decision trees 
machine learning pages 
quinlan 
simplifying decision trees 
international journal man machine studies 
quinlan 
programs machine learning 
morgan kaufmann san mateo 
cr mvc preprocessing method deal missing values 
knowledge systems 
landgrebe 
survey decision tree classifier methodology 
ieee transactions systems man cybernetics pages 
schaffer 
overfitting avoidance bias 
machine learning pages 
