solving pomdps searching space finite policies nicolas meuleau kee kim leslie pack kaelbling anthony cassandra computer science dept box brown university providence ri nm kek arc cs brown edu solving partially observable markov decision processes pomdps highly intractable general part optimal policy may infinitely large 
explore problem finding optimal policy restricted set policies represented finite state automata size 
problem intractable show complexity greatly reduced pomdp policy constrained 
demonstrate empirical results branch method finding globally optimal deterministic policies gradient ascent method finding locally optimal stochastic policies 
application domains partially observable markov decision process pomdp realistic model completely observable counterpart classic mdp :10.1.1.107.9127:10.1.1.53.7233
complexity resulting lack observability limits application pomdps dramatically small decision problems 
difficulties optimal bayesian solution technique policy produces may complete previous history system determine action perform 
optimal policy may infinite approximate level able implement finite machine 
problem calculation requires reformulating problem continuous space belief functions harder simple finite computation sufficient optimize completely observable mdps 
solve huge pomdp 
may impossible just represent optimal policy memory sense restrict search policies reasonable way calculable finite machine 
knowing policy representable possibly infinite state automaton constraint want impose policy representable finite state automaton call finite policy graph 
previous approaches implicitly rely similar hypothesis authors search optimal reactive memoryless policies mccallum searches space policies finite horizon memory wiering schmidhuber hql learns finite sequences reactive policies peshkin look optimal memory policies :10.1.1.14.5408
examples particular cases finite policy graphs set extra structural constraints case node transition action choice possible graph 
note general finite policy graphs remember events arbitrarily far past 
just limited number events memorize 
study problem finding best policy representable finite policy graph size possibly simple constraints structure graph 
idea searching explicitly optimal finite policy graph pomdp new 
early lave proposed heuristic approach finding optimal decision trees 
hansen proposed policy iteration algorithm outputs optimal controller :10.1.1.41.7263
solution techniques explicitly belief space classical bayesian optimal solution pomdps output policy graphs optimal solution 
approach uses em find controllers optimal finite horizon 
characteristic property algorithms scale respect size problem 
drawback execution time increases quickly size policy graph complexity policy looking 
general adapted large pomdps relatively simple policies perform near optimally 
characteristic approach refer value optimal bayesian solution anymore just want best graph constraint imposed number nodes 
note optimality criterion bayesian approach expected discounted cumulative rewards expectation relative prior belief states 
evaluate solution produced relative optimal performance problem may solved notion belief space 
development relies basic property finite state controllers stressed hansen close parr russell ham theorem 
pomdp finite policy graph sequence pairs state pomdp node policy graph constitutes markov chain 
going farther define new mdp cross product state space set nodes decision choice action node conditioned observation 
working cross product mdp presents advantages allows calculate differentiate value fixed policy calculate upper lower bounds value attainable completing partial policy establish complexity results 
properties develop implementations classic search procedures global local majority computation consists solving bellman equations cross product mdp 
important point structure pomdp policy graph reflected cross product mdp 
accelerate solution bellman equations execution solution techniques 
words algorithms propose exploit structure pomdp find relatively quickly best general finite policy graph size 
leverage sufficient may limit search space imposing structure policy graph structure speed solution cross product mdp instance limit finite memory architectures mentioned 
organized follows 
give quick pomdps policy graphs define cross product mdp 
show finding best deterministic finite policy graph np hard problem 
really easy way solve problem 
propose possible approaches global branch bound search finding best deterministic policy graph local gradient descent search finding best stochastic policy graph 
algorithms solving bellman equations cross product mdp 
take full advantage preexisting structure pomdp policy graph 
typically algorithms adapted structured pomdps large number states small number observations simple policies perform 
give empirical evidence approach allows solution pomdps size far limits classical solutions 
pomdps finite policy graphs pomdps partially observable markov decision process pomdp defined tuple finite set states finite set observations finite set actions underlying markov decision process mdp optimized way initial state aim maximize expected discounted cumulative reward discount factor 
optimal solution mapping specifies action perform possible state 
optimal expected discounted reward value function defined unique solution set bellman equations remarkable property mdps exists optimal policy executes action state 
unfortunately policy partially observable framework residual uncertainty current state process 
pomdp framework policy general rule specifying action perform time step function previous history complete sequence observation action pairs time 
particular kind policy called reactive policies rps condition choice action previous observation 
represented mappings probability distribution starting state structure policy graphs representing reactive policies 
degrees freedom choices action nodes 
policy reactive realizes expected cumulated reward classical bayesian approach allows determine policy maximizes value 
updating state distribution belief time step depending observations 
problem re formulated new mdp belief states original states 
generally optimal solution reactive policy 
sophisticated behavior optimal balance exploration exploitation 
unfortunately bayesian calculation highly intractable searches continuous space beliefs considers possible sequence observations 
finite policy graphs policy graph pomdp graph nodes labeled actions arcs labeled observations arc emanating node possible observa tion 
system certain node executes action associated node 
implies state transition pomdp eventually new observation depends arrival state underlying mdp 
observation conditions transition policy graph destination node arc associated new observation 
policy representation possibly countably infinite policy graph 
policy chooses different action possible previous history represented infinite tree branch possible history 
reactive policies correspond special kind finite policy graph nodes observations pomdp arcs labeled observation go node 
stochastic policy graph probability distribution actions attached node single action transitions node random arrival node depending starting node observation 
notation set nodes graphs current node time probability choosing action node probability moving node node observation probability distribution initial node conditioned observation cases want impose extra constraints policy graph 
limit restriction constraints consist restricting set possible actions executable nodes restricting set possible successors nodes observations 
note forcing graph implement rp represents set restriction constraint defined 
consider sophisticated sets constraints section 
cross product mdp advantage representing policy policy graph cross product pomdp policy graph finite mdp 
interesting point structure pomdp policy graph represented cross product mdp 
allow develop relatively fast implementations classical techniques solve problem 
calculating value policy graph theorem hansen closely related parr russell ham theorem 
theorem policy graph pomdp sequence node state pairs generated constitutes markov chain 
influence diagram proves property depends transition ma trix markov chain influence diagram proving markov property cross product mdp 
dotted arrows represent dependencies take account represented formulations 
shown theorem valid general framework 
way calculate expected immediate reward associated pair value function policy solving fundamental equation matrix form stochastic matrix invertible matrix value policy independent starting node state joint probability distribution differentiating value respect parameters graph enable climb gradient 
solving cross product mdp 
complete mdp de fined pair choose action wait new observation chose node 
equivalent choosing action mapping determines node function observation 
action space cross product mdp theorem tuple markov decision process finite stationary fundamental equation mdp matrix form maximization applied row row 
expand equation maximization replaced maximization moved equation expected optimal reward independent starting state node stationary optimal policy cross product mdp mapping note optimal policy generally implementable policy graph may associate different actions node depending state node coupled 
words need know current state policy 
agent policy graph basically embedded cross product mdp partial observability product state sees cross product mdp fact pomdp projection solution fundamental equation useful algorithms represents upper bound performance attainable implementable policy 
branch bound algorithm finding optimal deterministic policy graph 
note addition restriction constraints policy defined section invalidate theorem 
just limits set possible actions states cross product mdp reduces complexity solution 
consequence branch bound algorithm able find best graph restriction constraints 
computational leverage 
important point computation performed algorithms follow consists solving bellman fundamental equation fixed policy sake finding optimal deterministic policy 
done successive approximations algorithm called value iteration case 
complexity algorithm times number iterations 
important point structure transition matrix exploited executing back ups 
components structure structure pomdp sparse transition matrix pomdp provides leverage allows speed successive approximation iterations 
branching factor pomdp average number possible successors state complexity reduced instance deterministic transitions reduce complexity factor way sparse observation matrix exploitable 
instance deterministic observations reduce complexity factor structure policy graph leverage gained structure pomdp sufficient choose restrict search space imposing structural constraints graph structure speed calculation 
extreme adopted solution look best rp 
case gain factor complexity 
say constraining policy graph section 
problem policy structured leverage gained bigger just addition effect structures 
instance evaluating rp completely deterministic problem done finding optimal policy graph consider problem finding best policy graph size pomdp 
littman showed finding best rp pomdp np hard problem 
generalize result finite policy graph number nodes set restriction constraints 
theorem pomdp set restriction constraints problem finding optimal deterministic policy graph satisfying constraints np hard 
proof straightforward finding best deterministic policy graph equivalent finding best deterministic rp cross product pomdp 
result follows littman theorem 
techniques solving np hard problems may classified groups global search local search approximation algorithms 
classic techniques global search section local search section 
consider possible approximation algorithm section 
global search heuristically guided search find best deterministic policy graph size restriction constraints imposed actions structure 
branch bound algorithm systematically enumerates set possible solutions bounds quality partial solutions exclude entire regions search space 
lower bound partial policy greater upper bounds useless explore partial policies 
possible extension considered time 
algorithm guaranteed find optimal solution finite time 
note approach generalization previous algorithm proposed littman 
algorithm limited policy graphs representing rps pomdps particular structure state transitions observations deterministic problem achievement task goal state reached soon possible 
formalism proposed handles kind pomdp kind policy graph restriction constraints 
littman gives details aspects algorithm reader refer complete brief description give 
ordering free parameters 
tree possible policies expanded depth breadth best way picking free parameters policy considering possible assignment values 
game pruning branches upper bound lower bounds comparison added 
size tree expanded process strongly depends order free parameters picked 
case important free values come free values words building solution assign actions nodes fix structure graph 
pruning possible possible structures expanded due nature upper bound see 
implementation parameter expanded issue symmetry policy graph space 
instance absence restriction constraints permute role played different nodes changing policy 
policy graph represented leaves tree 
avoid enumerating equivalent graphs imposing arbitrary rule expanding tree 
instance impose index action attached node ways greater equal index action node simple trick improve greatly performance heuristic search merely dividing execution time upper bounds 
partial solution general finite policy graph restriction constraints initially time specify action node transition add constraint 
get upper bound solving cross product mdp explained second part section product specified policy graph corresponds rp po mdp policy graph better optimal solution cross product mdp 
note upper bound nice monotonicity property increase fix free parameter equal true value policy graph graph com completely pletely specified 
hand long value specified optimal policy solving cross product mdp equivalent optimal policy original mdp choice action independent depends calculated upper bound equal value optimal policy cross product mdp 
pruning done long value specified parameter considered expending tree 
lower bounds 
algorithm searches depth order values complete policies expanded determine lower bounds best performance attainable extending partial policy 
find lower bound partial policy completing random calculating value resulting complete policy 
improvement consists performing simple local optimization having completed policy 
implementation heuristic technique solution cross product mdp complete partial policy 
calculate performance complete policy solving equation cross product mdp 
complexity 
calculation upper lower bounds node expanded tree requires solving bellman equations cross product mdp 
done structure pomdp policy 
reduce number iterations successive approximation executed calculation store partial policy value function calculating upper bound 
start computation upper bound child partial policy starting value parent 
different gain lot time trick 
memory space needed increases dramatically 
calculate bounds relatively quickly real problem nodes necessary expand reaching optimum 
worst case complete tree possible solutions expanded represents complexity exponential number degrees freedom policy graph 
practice simulations showed fewer nodes expanded 
note adding simple constraints policy reduces complexity solution cross product mdp size search space number nodes expanded 
local search approach try find best stochastic policy graph treating problem classical non linear numerical optimization problem 
value policy graph continuous differentiable respect policy parameters calculate gradient climb different ways 
develop possibilities climbing gradient focus calculation gradient just depict simple implementation gradient ascent 
note gradient may calculated exactly approach guaranteed converge local optimum 
topology search space number local optima depends things structure pomdp hand constraints imposed policy 
introducing constraints policy hope reduce execution time algorithm change landscape multimodal 
calculating gradient 
value policy graph equation 
policy parameter 
value function interested gradient respect policy parameters consider partial derivative respect variables calculated easily starting 
main difficulty calculation gradient inverting matrix want exploit structure successive approximation basic update rule matrix 
useful structure complexity complete back matrix inverted inverse calculate gradient respect parameter minor acceleration obtained value previous point start iterative computation value new point 
reduce number iterations step matrix wise dp complexity way computing gradient complexity performing dp calculate explicitly perform classical vector wise dps complexity structure compute solving implies vector wise dp complexity calculate get iterating vector wise square complexity dp 
total complexity calculation neglecting calculation 
unfortunately tion re done policy parameter depend divided complexity factor multiplied number free variables graph 
approach useful cases fewer free variables policy graph cross product states 
instance looking best reactive policy indirect calculation allows gain factor climbing gradient 
climbing gradient consists updating free value rule step size parameter 
case problem somewhat complicated parameters optimize probabilities ensure stay valid inside simplex update 
numerous ways doing including renormalizing soft max function 
implementation chose project calculated gradient simplex apply reach edge simplex 
reach edge gradient points outside simplex project gradient edge applying 
related 
idea gradient algorithm solving pomdps pursued authors :10.1.1.14.5408
main difference authors monte carlo estimation gradient exact calculation limit rps general approach 
jaakkola exponentially discounted criterion average reward time step 
companion propose stochastic gradient descent approach learning finite policy graph trial interaction process 
approaches monte carlo approach watkins learning applicable problem 
instance learning observation action pairs find guarantee convergence optimal rp pomdp 
instance wiering schmidhuber hql learns finite sequences rps 
load unload problem locations agent starts unload location receives reward time returns place passing load location 
problem partially observable agent distinguish different locations load unload perceive loaded 
monte carlo approach works strong structural constraints graph applied finding general finite policy graphs 
note littman reported observing great superiority terms execution time global branch bound search monte carlo approach case graph constrained encode simple rp 
simulations architectures sets structural constraints showed similar results general monte carlo approach compete 
introducing structural constraints majority computation perform bellman back ups cross product mdp algorithms outlined take advantage preexisting structure pomdp 
leverage insufficient problem big difficult techniques 
case may restrict search space imposing structural constraints policy graph 
instance simple solution consists defining neighborhood nodes graph allowing transitions neighboring node 
cor responds set restriction constraints forced take value zero points algorithm applied 
extreme solution consists limiting search reactive policies completely fixed advance 
complex sets constraints instance limit search policy representable finite sequence rps particular rules governing transition rp wiering schmidhuber hql 
instances include finite horizon memory policies mccallum external memory policies peshkin 
architectures described terms restriction constraints equality constraints different parameters graph previous results algorithm extended particular 
words previous algorithm find best rp sequence length execution time ga load unload problem dfh bfh number states simulations results obtained load unload problem execution time algorithms function size problem ga gradient ascent dfh depth heuristic search bfh breadth heuristic search 
best policy finite horizon memory best policy external memory size 
show np hard solve problems 
gain lose impose structure policy graph 
general imposing structural constraints reduces number parameters node help techniques modifies topology search space influences gradient descent approach 
point best graph constraints better best graph constraints constraints decrease value best solution 
happen nodes may required reach best performance constraints 
consider instance load unload problem represented 
simple problem solved optimally node policy graph sequence rps hql 
rp encoded node graph sequence rps encoded nodes 
number parameters node smaller unconstrained case 
general adding structure interesting choose architecture fits problem hand 
question previous knowledge problem hand optimal solution 
simulation results experiments simple load unload problem increasing number locations see algorithms scale increasing problem size compare 
partially observable stochastic maze agent go starting state marked goal marked 
problem partially observable agent perceive true location orientation presence absence wall side square defining current state 
problem stochastic non zero probability slipping agent know attempt move consequence actual position maze 
easy pomdp results obtained represent kind upper bound performance algorithms 
perform better harder problem 
experiment set optimal value gradient algorithm started center simplex policy graph initialized uniform distributions 
measured time execution algorithm function case gradient ascent stopped reached optimal 
heuristic search uses stochastic calculation upper bounds average measure runs 
set big value necessary accommodate big state spaces learning rate gradient descent optimized 
results 
show heuristic search clearly outperforms gradient algorithm numerically unstable number states increases kind absorbing problem 
second set experiments wanted measure far algorithms go terms problem size problem difficult simple load unload 
set partially observable mazes regular structure represented size varies states 
mazes particularly easy different optimal paths 
minimal number nodes solving action policy reactive 
time required depth branchand bound algorithm find optimal solution optimal number nodes shown 
see simpler version algorithms starting node fixed 
policy uniform distributions unstable local optimum 
execution time min maze problem dfh number states performance branch bound algorithm maze problem execution time function number states 
solve partially observable maze states hours 
represents performance far capacities classic approaches solving pomdps 
note number states grows measured complexity linear number states 
studied problem finding optimal policy representable finite state automaton size possibly simple structural constraints 
approach passes continuous intractable belief state space 
showed nphard problem anyway 
proposed classic search techniques developed efficient implementations allow structure problem accelerate computation 
sufficient bigger leverage gained imposing structure policy 
algorithms limited necessity enumerate iteration complete state space pomdp 
companion propose indirect learning algorithm avoids bottleneck 
astrom 
optimal control markov decision processes incomplete state estimation 
math 
anl 
appl 
baird moore 
gradient descent general reinforcement learning 
advances neural information processing systems 
mit press cambridge ma 
dean hanks 
decision theoretic planning structural assumptions computa tional leverage 
journal ai research appear 
cassandra 
exact approximate algorithms partially observable markov decision processes 
phd thesis brown university 
cassandra kaelbling littman 
acting optimally partially observable stochastic domains 
proceedings twelfth national conference artificial intelligence 
hansen 
improved policy iteration algorithm partially observable mdps 
advances neural information processing systems 
mit press cambridge ma 
hansen 
finite memory control partially observable systems 
phd thesis department computer science university massachusetts amherst 
hansen 
solving pomdps searching policy space 
proceedings eighth conference uncertainty artificial intelligence pages madison wi 
hauskrecht 
planning control stochastic domains imperfect information 
phd thesis mit cambridge ma 

optimal control complex structured processes 
phd thesis university france 
howard 
dynamic programming markov processes 
mit press cambridge 
jaakkola singh jordan 
reinforcement learning algorithm partially observable markov problems 
advances neural information processing systems 
mit press cambridge ma 
kaelbling littman cassandra 
planning acting partially observable stochastic domains 
artificial intelligence 
littman 
memoryless policies theoretical limitations practical results 
animals animats proceedings third international conference simulation adaptive behavior 
mit press cambridge ma 
mccallum 
overcoming incomplete perception utile distinction memory 
proceedings tenth international machine learning conference amherst ma 
mccallum 
reinforcement learning selective perception hidden state 
phd thesis university rochester rochester ny 
meuleau peshkin kim kaelbling 
learning finite state controllers partially observable environments 
proceedings fifteenth conference uncertainty artificial intelligence appear 
parr russell 
reinforcement learning hierarchies machines 
advances neural information processing systems 
mit press cambridge ma 
peshkin meuleau kaelbling 
learning policies external memory 
proceedings sixteenth international conference machine learning appear 
puterman 
markov decision processes discrete stochastic dynamic programming 
wiley new york ny 
lave 
markov decision processes probabilistic observation states 
management science 
smallwood sondik 
optimal control partially observable markov decision processes finite horizon 
operations research 
sondik 
optimal control partially observable markov decision processes infinite horizon discounted costs 
operations research 
sutton barto 
reinforcement learning 
mit press cambridge ma 
watkins 
learning delayed rewards 
phd thesis king college cambridge 
wiering schmidhuber 
hq learning 
adaptive behavior 
williams 
theory connectionist systems 
technical report nu ccs northeastern university boston ma 
