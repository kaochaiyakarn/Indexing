active learning total cost annotation active learning promises reduce cost annotating labeled datasets trainable human language technologies 
contrary expectations creating labeled training material hpsg parse selection reusing models gains may negligible negative 
serious implications showing additional cost saving strategies may need adopted 
explore strategy model annotation automate decisions 
best results show reduction annotation cost compared labeling randomly selected data single model 
methods uncertainty sampling cohn query committee seung shown dramatically reduce cost creating highly informative labeled sets speech language technologies 
experiments assume model fixed ahead time model currently developing training material 
complex tasks clear idea best model task time annotation practice need reuse labeled training material models 
show brittle variety natural reuse scenarios example jason miles osborne school informatics university edinburgh edinburgh eh lw uk miles inf ed ac uk allowing model improve quality reusing labeled training material different machine learning algorithm performance models significantly undermined training material created key knowing model able material selected relatedness may means determine prior annotation leading chicken egg problem 
reusability results demonstrate additionally strategies adopted ensure reduce total cost annotation 
osborne showed ensemble models increase model performance produce annotation savings incorporated process 
obvious step automating decisions 
consider simple automation strategy reduces annotation costs independently examine effect reusability 
find semi automation high quality models eliminate performance gap reuse scenarios 
weak models show semi automation random sampling effective improving reusability demonstrating cause caution show standard assumption reuse selecting model strategy combines semiautomated annotation able achieve highest annotation savings date complex task parse selection hpsg reduction annotation cost compared labeling randomly selected data best single model 
parse selection briefly describe environment parse selection models performance 
treebank treebank project provides tools annotated training material creating parse selection models english resource grammar erg flickinger 
erg hand built broad coverage hpsg grammar provides explicit grammar treebank 
approach advantage analyses coverage sentences convey information just phrase structure contain derivations semantic interpretations basic dependencies 
sentence records analyses licensed erg indicates annotators selected contextually correct 
selecting distinguished parses simply enumerating parses presenting annotator annotators discriminants disambiguate parse forest rapidly described section 
report results third growth contains english sentences appointment scheduling travel planning domains verbmobil 
sentences parses unique preferred parse identified 
sentences words parses average 
modeling parse selection standard feature grammars mainly log linear models parse selection johnson 
log linear models conditional probability analysis ti sentence set analyses 
ti mk exp fj ti wj fj ti returns number times feature occurs analysis wj weight model mk normalization factor sentence 
parse highest probability taken preferred parse model 
limited memory variable metric algorithm determine weights 
regularize loglinear models labeled data necessary set hyperparameters short supply simpler perceptron models parse selection assign scores probabilities 
scores computed inner product analysis feature vector parameter vector score ti mk fj ti wj preferred parse highest score analyses 
voted perceptrons better performance reuse experiments described section really wish model potentially worse log linear model 
useful map perceptron scores probabilities exponentiating renormalizing score pp ti mk exp score ti mk normalizing constant 
previous parse selection models equations single model feature set 
possible improve performance ensemble parse selection model 
create ensemble model called product model experts formulation hinton ti 
mn nj ti mj note individual model mi defined distribution usually taken fixed set models 
constant ensure product distribution sums set possible parses 
product model effectively averages contributions individual models 
simple model sufficient show enhanced performance multiple models 
parse selection performance osborne describe distinct feature sets configurational ngram conglomerate utilize various structures available derivation trees phrase structures semantic interpretations elementary dependency graphs 
incorporate different aspects parse selection task crucial creating diverse models product parse selection models methods 
models created subset conglomerate feature set feature set 
features semantic interpretations 
main feature sets train log linear models ll config ll ngram ll conglom product ensemble feature sets ll prod equation 
additionally perceptron conglomerate feature set conglom 
include loglinear model uses feature set ll perceptron parse selection accuracy measured exact match 
model awarded point picks parse sentence parse correct analysis indicated corpus 
deal ties accuracy model ranks parses highest best parse 
results chance baseline selecting parse random base models product model table 
fold crossvalidation results training data estimation test split evaluation 
see section details 
model perf 
model perf 
ll config ll prod ll ngram ll ll conglom conglom chance table parse selection accuracy 
measuring annotation cost aid identification best parse licensed erg annotation environment provides local discriminants mark true false properties analysis sentence order disambiguate large portions parse forest 
annotator need inspect parses parses narrowed quickly usually exponentially sentences large number parses 
interestingly means labeling burden relative number possible parses number constituents parse 
data discriminants needed annotate sentence recorded 
typically ambiguous sentences require discriminant values set reflecting extra effort put identifying best parse 
showed osborne discriminant cost provide accurate approximation annotation cost assigning fixed unit cost sentence 
discriminants basis calculating annotation cost evaluate effectiveness different experiment conditions 
specifically set cost annotating sentence number discriminants value set human annotator plus indicate final eyeball step annotator selects best parse remaining ones 
discriminant cost examples averages ranges 
active learning suppose set examples labels dn 
extended new labeled example 
information gain model maximized selecting labeling adding new example dn noise level low bias variance model dn minimized cohn 
practice selecting data points labeling model variance bias maximally minimized computationally intractable approximations typically 
approximation uncertainty sampling 
uncertainty sampling called tree entropy hwa measures uncertainty model set parses sentence conditional eyeball step taken contain information occurred apply cost step uniformly examples 
distribution assigns 
hwa measure quantify uncertainty fus mk mk log mk denotes set analyses produced erg sentence mk model 
higher values fus mk indicate examples learner uncertain calculating fus trivial conditional log linear perceptrons models described section 
uncertainty sampling defined approach 
improved simply replacing probability single log linear perceptron model product probability en log set models 
mn 
mentioned earlier parse selection potentially problematic sentences vary length number parses 
measures extra normalization major differences experimenting variety normalization strategies 
random sampling baseline uncertainty sampling osborne show uncertainty sampling produces results compared methods 
experimental framework experiments fold crossvalidation strategy randomly selecting roughly sentences test set selecting samples remaining roughly sentences training material 
run begins single randomly chosen annotated seed sentence 
round new examples selected annotation randomly chosen fixed sized sentence subset random selection uncertainty sampling models reach certain desired accuracies 
select examples annotation round exclude examples parses 
parameter settings examples label stage produce substantially different results reported 
results usually terms amount labeling necessary achieve performance levels 
say method better method performance level annotation required 
performance metric parse selection accuracy described section 
reusing training material considered selecting labeled training set tuned needs particular model 
typically wish reuse labeled training material natural question ask general training sets created improved feature set improved learner previously created training set useful 
selects highly idiosyncratic datasets able reuse datasets example better label datasets random sampling 
realistic situation models typically change evolve time problematic training set inherently limits benefit attempts improve model 
baselines evaluate model able reuse data selected labeling model selecting data randomly 
provides essential baseline reuse situations going useful ought outperform model free approach 
reuse model 
standard scenario determine reused data model selects data 
evaluate variety reuse scenarios 
refer model selector model reusing labeled data reuser 
models differ machine learning algorithm feature set 
measure relatedness spearman rank correlation rankings models assign parses sentence 
relatedness models calculated average rank correlation examples tested fold parse selection experiment available training material 
shows complete learning curves ll config reuses material selected ll conglom random sampling 
graph shows self reuse effective strategies idealized situation commonly assumed active learning studies 
graph reveals random sampling effective selection ll conglom nearly accuracy reached 
see material selected ll conglom effective ll config selected reason explained relatedness selector models ll config ll conglom ll config average rank correlation ll config correlation 
accuracy selector ll config selector ll conglom selector rand selector annotation cost learning curves ll config reusing material different selectors 
table relationship relatedness reusability fully 
shows annotation cost incurred various reach accuracy material selected various models 
list ordered top bottom rank correlation models 
lines provide baselines ll prod ll conglom ll config select material 
show amount material needed models random sampling 
rest gives results selector differs reuser 
performance level percent increase annotation cost self reuse 
example cost discriminants required ll prod reach performance level reuses material selected ll conglom increase discriminants needed ll prod selects 
similarly discriminants needed ll conglom reach reusing material selected ll config increase discriminants ll conglom needs selection 
seen table reuse leads increase cost self reuse reach level performance 
increase general inversely related rank correlation models 
furthermore considering reusing model individually relationship entirely inversely related performance levels exception conglom ll selecting ll config level 
reason models related generally easy see 
example ll config ll conglom highly related ll prod components 
cases ll prod beats random sampling large amount 
ll related ll conglom ll config explained fact feature set subset conglom set 
contains features 
accordingly material selected ll generally reusable ll conglom ll config 
encouraging case ll conglom reusing material selected ll represents common situation initial model develop corpus continually improved 
particularly striking aspect revealed table random sampling overwhelmingly better strategy little labeled material 
tends select examples ambiguous higher discriminant cost 
examples may highly informative selector model cheap far effective reused model 
considering unit cost sentence costs discriminant cost assigns variable cost sentence generally effective random sampling reuse accuracy levels 
example unit cost random sampling better selection ll reuse rank selector reuser corr 
dc incr dc incr dc incr ll prod ll prod ll conglom ll conglom ll config ll config ll config ll prod ll conglom ll prod ll config ll conglom ll conglom ll config conglom ll config ll ll conglom ll ll prod ll ll config ll config rand ll prod rand ll conglom rand ll config table comparison various selection reuse conditions 
values discriminant cost dc percent increase incr cost material selected reuser 
ll config accuracy 
ll divergent ll config selections truly sub optimal ll config particularly initial stages 
results shows blindly expected reduce total cost annotation 
data tuned models useful data models depends degree relatedness models consideration 
may may provide cost reductions consider effect semi automating annotation reducing total cost annotation semi automated labeling corpus building generally viewed selecting examples scratch labeling examples 
inefficient especially dealing labels complex internal structures model may able labeling possibilities 
domain exploit fact may partial information example label presenting top best parses annotator navigates best parse set discriminants relevant set parses 
value fixed proportional ambiguity sentence simply select parses model assigns probability higher chance 
advantage reducing number parses annotator model uses training material reduces uncertainty 
true best parse top annotator cost record number discriminants needed identify subset plus calculation parses advantage fewer discriminants parses need inspected 
best parse best subset question record annotation cost 
discriminant decisions reducing subset valid useful identifying best parse entire set incur penalty fact annotator confirm case 
determine cost situations add usual full cost annotating sentence 
encodes feel reasonable reflection penalty decisions taken best phase valid context parses 
performance level 
rand 
ll prod 
rand nb 
ll prod nb table cost ll prod reach performance levels best automation nb 
table shows effects semi automated labeling ll prod 
seen random selection costs reduce dramatically best automation compare rows 
early winner basic uncertainty sampling row eventually reaches higher accuracies quickly 
mixture semi automation provides biggest gains reach accuracy best uncertainty sampling row reduces cost nbest random sampling row basic uncertainty sampling row 
similar patterns hold best automation ll config 
provides view accumulative effects best automation uncertainty sampling ideal situation reuse model 
ensemble models best automation show massive improvements see largest reductions best automation ensemble models ll prod uncertainty sampling best automation row table reaches accuracy cost compared needed ll config random sampling automation 
best annotation saving cost reduction 
closing reuse gap previous section semi automated labeling experiments involve reuse 
models expected evolve best automation fill cost gap created reuse 
test considered reusing examples best model ll allow benefit labeling decisions annotation savings naturally decrease best labeling 
accuracy ll product best uncertainty sampling ll product best random sampling ll product random sampling ll config random sampling annotation cost learning curves accumulative improvements annotation scenario starting random sampling ll config nbest automation uncertainty sampling 
prod selected different models best automation combined strategy 
ll config ll conglom selectors gap entirely closed costs reuse virtually equal ll prod selects examples best table row 
gap closes best automation weaker ll model 
performance table row falls far short ll prod selecting best table row 
gap closes nbest automation random sampling ll table line 
performance level 
nb 
nb rand table cost ll prod reach performance levels reuse situations best automation nb ll uncertainty sampling random sampling rand 
interestingly weak selector ll best automation combined random sampling effective combined uncertainty sampling 
reason clear 
typically selects ambiguous examples weak model difficulty getting best parse best 
gains informative examples selected surpassed gains come easier labeling random sampling 
situations best automation beneficial gap introduced reuse reduced 
nbest automation results increase cost 
true allow reuse discriminants select best parse best subset best parse subset 
related large body machine learning literature natural language processing nlp 
nlp primarily focused uncertainty sampling hwa tang 
hwa considered reuse examples selected parser uncertainty sampling 
performed better sequential sampling half effective self selection 
considered reuse respect models relatedness 
compare reuse performance random sampling showed previously stronger baseline sequential sampling corpus osborne 
hwa 
showed parsers outperforms closely related training labeling automated 
approach requires strict independence assumptions 
discussion considered creating labeled data task understood model substantially change 
prudent consider improving model example ensemble techniques semi automating labeling task 
naturally cost associated creating model turn need factored total cost 
genuine uncertainty model labeled data going eventually best strategy may random selection especially form automated annotation 
acknowledgments markus becker crim dan flickinger alex lascarides stephan andrew smith 
pc pc rosie hard dedication 
supported edinburgh stanford link rosie project 
david cohn zoubin ghahramani michael jordan 

active learning statistical models 
tesauro touretzky leen editors advances neural information processing systems volume pages 
mit press 
dan flickinger 

building efficient grammar exploiting types 
natural language engineering 
special issue efficient processing hpsg 
hinton 

products experts 
proc 
th int 
conf 
artificial neural networks pages 
rebecca hwa miles osborne anoop sarkar mark steedman 

corrected training statistical parsers 
proceedings icml workshop continuum labeled unlabeled data pages 
icml 
rebecca hwa 

sample selection statistical grammar induction 
proc 
joint sigdat conf 
emnlp vlc pages hong kong china 
rebecca hwa 

minimizing training corpus parser acquisition 
proc 
th conference natural language learning toulouse 
mark johnson stuart geman stephen cannon chi stephan riezler 

estimators stochastic unification grammars 
th annual meeting acl 
stephan kristina toutanova stuart shieber christopher manning dan flickinger thorsten brants 

lingo treebank motivation preliminary applications 
proc 
th international conference computational linguistics taipei taiwan 
miles osborne jason 

ensemble active learning parse selection 
proc 
hlt naacl boston 
seung manfred opper haim sompolinsky 

query committee 
computational learning theory pages 
min tang luo salim roukos 

active learning statistical natural language parsing 
proc 
th annual meeting acl pages philadelphia pennsylvania usa july 
