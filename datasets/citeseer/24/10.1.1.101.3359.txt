naive bayes models probability estimation daniel cs washington edu department computer science engineering university washington seattle wa usa january naive bayes models widely clustering classification 
seldom general probabilistic learning inference estimating computing arbitrary joint conditional marginal distributions 
show wide range benchmark datasets naive bayes models learned em accuracy learning time comparable bayesian networks context specific independence 
coupled linear inference time attractive alternative particularly large domains 
naive bayes models quite popular classification clustering due simplicity efficiency accuracy 
naive bayes model simple due naive assumption input variable 
xn conditionally independent class cluster variable assumption permits inference done linear time respect number classes number input variables 
conditional independence rarely holds practice naive bayes models tend perform optimally class prediction scenarios domingos pazzani 
naive bayes models successfully applied classification task spam filtering sahami clustering task grouping stars spectra cheeseman stutz interesting problems 
tasks common important class cluster variable spam filtering wish know grouping email spam non spam clustering stars may know number clusters ahead time wish discover clusters assign new stars 
machine learning problems wish estimate full joint probability distribution input variables 
xn 
may ask arbitrary marginal conditional distributions variables 
type problem popular approach naive bayes models bayesian networks 
bayesian networks pearl represent variable node directed acyclic graph 
assuming variable independent class variable assume variable conditionally independent parents graph 
bayesian networks utilizing context specific independence offer additional flexibility independencies depend specific values parents 
selecting optimal factorization np hard chickering models learned relatively efficiently 
bayesian networks flexible simplistic suffer critical weakness exact inference complete 
words models may learned quickly may take exponential time 
compensate weakness approximate inference 
popular methods gibbs sampling belief propagation 
gibbs sampling easy implement guaranteed converge correct distribution 
unfortunately take great iterations converges 
furthermore detecting convergence difficult convergence criteria exist come guarantees 
belief propagation tends converge quickly iterations 
unfortunately convergence guaranteed converge may converge correct distribution 
belief propagation simplest form compute arbitrary joint distributions marginal distribution unspecified variable 
generalized belief propagation answer complex queries computationally expensive yedidia 
techniques applicable problems gibbs sampling robustness priority belief propagation speed priority 
unfortunately applications satisfactory 
consider task inferring products web pages interest different web site visitors 
gibbs sampling slow accomodate visitors real time especially busy site produce hundreds queries second 
belief propagation fast guarantee accuracy 
clearly bayesian networks limitations 
gibbs sampling belief propagation apply particular application modeler choose halt gibbs sampling implement belief propagation verify accuracy 
solution inference method model 
naive bayes mixture model exact inference linear model size exponential number variables 
demonstrate learning time accuracy comparable state art bayesian network methods 
final advantage naive bayes simple implementation 
related demonstrate empirically naive bayes models effective general probabilistic learning inference knowledge manner new 
related illustrates 
autoclass system cheeseman naive bayes model clustering clustering viewed special case joint probability estimation 
autoclass proposed model learn joint distributions data expectation maximization em dempster 
main differences stated goal cluster discovery versus general estimation inference details learning algorithm 
breese applied naive bayes cluster model similar autoclass collaborative filtering 
applications clustering goal infer user preferences discover clusters 
represents application naive bayes general probabilistic learning inference 
mixture trees mt model meila jordan mixture model component tree structured bayesian network 
allows exact inference done time kn number variables number mixture components 
mixture model joint inference employ simpler naive bayes model assuming variables wholly independent component 
outline remainder organized follows 
section give additional background bayesian networks 
section covers naive bayes estimator nbe model learning arbitrary joint distributions 
section describe experiments evaluating accuracy inference time naive bayes models relative bayesian networks 
sections contain discussion 
bayesian networks bayesian network utilizes notion conditional independence compactly represent joint probability distribution set variables 
xn pearl 
assume variables discrete discretized 
bayesian network typically represented directed acyclic graph node variable 
graph encodes assertion variable conditionally independent non descendents parents graph 
variable conditional probability distribution cpd describes distribution variable configuration parents 
cpds completely describe joint probability variables 
xn xi xi popular type cpd conditional probability table 
conditional probability table provides distribution target variable possible configuration parent variables 
table takes exponential space number parent variables target node 
decision trees reduce space required exploiting context specific independence allowing distributions reused multiple configurations parent variables 
friedman goldszmidt chickering decision trees allow complex models stored space 
bayesian network representation advantages 
represents joint probability distribution variables relatively little space 
store probability table describing likelihood configuration variables require exponential space 
bayesian network contrast deal smaller distributions consisting variable parents 
second means factorization may fit problems 
thinking variable parents causes come independencies consider medical application set risk factors cause set diseases turn cause set symptoms 
corresponding bayesian network arcs risk factor related disease disease related symptom 
disease depends risk factors symptom depends diseases local probability distributions relatively small 
model encodes knowledge domain fairly intuitive manner 
learning graph structure bayesian network known missing data cpd may learned independently data 
structure unknown search structure 
finding optimal structure np hard chickering structures greedy algorithms 
heckerman 
describe widely hill climbing algorithm learning bayesian network structure parameters cpds tables 
algorithm starts initial graph structure considers step individual arc additions deletions reversals maintain acyclic graph 
modifications evaluated bayesian dirichlet score integrates likelihood training data possible parameters structure 
structures weighted structure prior incorporate prior knowledge prevent overfitting excessively complex structures 
process similar decision trees cpds modifications described chickering 
related algorithm implemented microsoft research toolkit experiments chickering 
inference exact inference bayesian network complete 
exact inference algorithms tractable small specially structured bayesian networks 
focus approximate inference algorithms popular gibbs sampling belief propagation 
gibbs sampling repeatedly samples variable network conditioned markov blanket gilks 
distribution positive configuration probability zero set samples generated guaranteed converge true joint distribution network 
words consecutive samples may highly correlated unordered set samples eventually approximates set samples joint distribution described bayesian network 
conditional marginal probabilities may computed counting frequency specific configurations samples 
setting evidence variables fixed values may compute conditional probabilities 
belief propagation message passing algorithm originally perform exact inference tree structured bayesian networks 
node network receives messages neighbors expected distribution node broadcasts messages expected distributions neighbors yedidia 
manner local information eventually propagates entire network 
unfortunately inference longer exact convergence longer guaranteed bayesian networks loops 
belief propagation useful practice loopy graphs 
generalized belief propagation groups nodes clusters increase accuracy flexibility belief propagation 
ordinary belief propagation provides marginal distribution variable generalized belief propagation gives joint distribution cluster 
see yedidia details belief propagation generalized belief propagation 
naive bayes probability estimation simpler faster alternative bayesian networks propose naive bayes estimation nbe naive bayes mixture model general purpose probabilistic learning inference 
additional background naive bayes general probabilistic model follows naive bayes schema 
nbe different application joint probability estimation clustering classification 
learning algorithm briefly discuss inference 
naive bayes naive bayes models named naive assumption variable xi conditionally independent class cluster variable making assumption probability distribution may efficiently represented product simpler distributions 
xn xi naive bayes model special case bayesian network variables 
xn parent xi parents 
efficiency gains general bayesian networks come restricted structure 
common uses naive bayes classification represents class predicted xi variables represent observable attributes 
example naive bayes spam filter represent classes spam non spam xi represent ith word candidate email 
component distributions learned training data simply counting word occurrences labeled spam non spam 
example applications naive bayes naive independence assumption clearly false words email depend spam legitimate email 
models tend perform quite practice theoretical analysis shown naive bayes optimal loss cases domingos pazzani 
naive bayes clustering goal learn underlying structure data 
classification task represents cluster variable directly observed 
cases number clusters unknown inferred 
autoclass system example naive bayes clustering system automatically discover classes star spectra dna introns exons landsat data cheeseman stutz 
clustering models typically learned expectation maximization em iterative process consisting alternating steps dempster 
expectation step step data points fractionally assigned clusters partially learned model 
maximization step step cluster distributions updated maximize likelihood training data 
iterating steps model guaranteed converge local maximum likelihood 
nbe application naive bayes models problem general probabilistic learning inference 
clustering problem cluster variable observed learned em 
fact clustering problem really special case general learning primary goal discover interpret structure data 
nbe classes clusters indexed typically referred components mixture composes entire probability distribution 
procedure input training set hold set initial number components convergence thresholds em add 
initialize component 
repeat add new mixture components initialized random examples remove initialization examples repeat step fractionally assign examples mixture components step adjust parameters maximize likelihood fractional assignment 
log highest seen far save 
log fails improve ratio em iteration 
log fails improve ratio add iteration 
execute step step twice examples return 
learning table nbe iterative learning algorithm 
section algorithm training nbe model data 
nbe model learned iterating steps extension adding mixture components refinement training convergence 
extension step important initializing model adding mixture components partially learned model 
adding components learn model avoid needing train model multiple times different numbers initial components approach taken autoclass cheeseman 
refinement step uses em find best parameters components prunes low weight components speed search 
iteration double number components added model expected running time constant factor optimal running time running time know optimal number components starting 
test convergence avoid overfitting hold set refinement step process 
examples processed learning algorithm final iterations em run training data held regular 
pseudocode listed table 
sections describe extension refinement steps greater detail 
model extension model extension begins randomly selecting points training set 
points analogous initial cluster centers 
algorithm generates components initial points inducing probability distribution variable component 
combine single observation random point low weight dirichlet prior variable marginal distribution estimated entire training set 
tests set weight dirichlet prior equivalent counts 
yields probability xi random point original variable value xi distribution xj variable values xj xm relative variable value xm training data 
covered experiments handle continuous variable distributions 
initialized gaussians mean center variance equal fixed fraction variable global variance 
believe technique sense general random sampling points include points regions high probability ought clusters 
dirichlet prior fractional variance allow slightly different points probabilistically included component 
points initializing components removed training data considered refinement step order avoid overfitting points 
model refinement refinement step iterate em algorithm initial component distributions converge local likelihood maximum 
step em model evaluated hold set 
log likelihood hold set model highest far cache current model 
log likelihood hold set decreases increases fraction em refinement step ends 
log likelihood decreased revert cached model 
protects worse model running steps em 
steps em refinement step prune low weight components 
accomplish sorting mixture components ci relatively frequencies ci keeping components total probabilities sum 
step em takes time linear number components pruning speed learning significantly 
imposes indirect limit model complexity model components lose component step 
best model data uses components pruning step yield worse model 
avoided increasing pruning threshold appropriately dataset 
entire refinement step passes little improvement hold set run final steps em best model held data included terminate entire learning procedure 
learning methods addition procedure described experimented alternate method selecting number components 
main difference adding components model learned pick optimal number components start hold data 
dataset partially trained model random components mixture components initialized points training set 
models performed early stopping running iterations em training convergence 
early stopping perform effective model selection time discussed meek 
log likelihood hold data model components worse model components pick optimal number components 
having selected number components train model convergence hold data test overfitting running final iterations em 
alternate method yields models equivalent accuracy takes significantly longer 
hypothesis adding components iteratively allows easy components learned faster fewer components start 
additional subtleties may captured additional components require training partially trained model 
final number components may em iterations run fewer components 
general believe effective method training mixture models applicable 
simply chose procedure fast effective 
time complexity time complexity procedures number em iterations run total number components considered number variables numbers training examples including hold set 
number em iterations required depend dataset modeled convergence criteria 
unfortunately analytical upper bounds number iterations may required 
number methods exist speed em certain circumstances 
observed faster learning adding components incrementally reducing iterations em 
pruning components low weight sped algorithm 
partial steps lazy em may provide additional speed ups large datasets described thiesson 
alternately optimization technique conjugate gradient ascent quasi newton methods require substantially fewer iterations em press 
inference nbe naive bayes model inference methods new apply naive bayes model 
marginal queries set query variables state component variable set hidden variables 
assume component variable part query observed training data 
refer fixed state query variables 
compute marginal probability directly summing possible states naive bayes assumption variable independent fact may express probability terms learned distributions xi zi xj zi variables affect probability computation may safely ignored xj expression product sums sum containing terms total time required compute 
note naive bayes inference time constant number hidden variables exact bayesian network inference worst case exponential number hidden variables 
conditional probabilities computed efficiently ratios marginal probabilities 
refer set conditioning variables conditional query refer fixed state marginal probabilities numerator denominator computed times respectively 
conditional probability computed time 
note running time remains independent number hidden variables 
full probability distribution multiple variables specifies probability exponential number states storing probabilities take exponential space time 
efficiently generate reduced naive bayes model incorporates evidence 
reduced model variables removed evidence incorporated adjusting component weights 
approach adopt experiments multiple query variables 
empirical evaluation section describe empirical evaluation naive bayes models terms learning time time required learn probabilistic model data modeling accuracy extent learned model predicts test set inference time rate model answer queries 
datasets order gauge effectiveness model real data selected datasets including uci machine learning repository blake merz subset eachmovie dataset jester collaborative filtering dataset subset kdd cup data 
basic statistics dataset table 
uci datasets range theme census data chess endgames variables size examples 
uci datasets designed classification joint probability estimation provide variety real world multivariate datasets 
treat class variable differently variables learned joint distributions variables 
furthermore nbe models learned significantly different naive bayes classification models typically mixture components classes 
example letter recognition features classes nbe learned model components 
eachmovie dataset set movie ratings testing recommender systems collected compaq month period 
subset speed learning 
additionally decreased sparsity transforming zero movie rating scale boolean dislike ratings rating implied 
jester dataset testing recommender systems consists ratings jokes movies 
kdd cup data consists clickstream purchase data online retailer kohavi 
datasets examples real world problems joint probability estimation primary goal 
movie ratings example task predict unrated movies user enjoy 
easily inferred probability distribution harder classification task movie treated separately 
dataset partitioned training set test set 
standard partition existed 
assigned examples test set probability 
datasets fewer examples performed way cross validation single pair training test sets 
training set selected standard hold set 
data algorithms prevent overfitting 
case cross validated datasets hold set generated splits 
training examples randomly selected inclusion hold set probability depended size training set 
training sets fewer examples third training data examples tenth sizes examples 
continuous variables discretized 
discretization method states chose value cut offs equally partitioned non missing training values states possible 
case continuous variables predominantly value values resulted fewer useful states states zero vs values greater zero 
discrete variables missing values assigned separate state 
continuous variables discretized experiments nbe represent directly replacing multinomial models gaussians 
nbe mixture model effective distribution continuous variable mixture gaussians 
component initialization learning identical discrete case 
handle continuous variables gaussian log gaussian models 
unfortunately gibbs sampling belief propagation readily accomodate continuous variables 
gibbs sampling continuous samples discretized smoothed handle number continuous configurations 
belief propagation assumes compact representations probability distributions difficult continuous case 
avoid continuous variables due inference complications bayesian networks due limitations inherent nbe 
training nbe models trained described section additional details 
experiments initial components training sets points initial components smaller training sets 
bayesian network implementation chose microsoft research toolkit chickering freely available easy represents state art bayesian network learning algorithms 
greatest strengths uses decision trees conditional probability tables cpts local nbe training time training time learning times bayesian networks toolkit nbe 
point represents benchmark datasets considered experiments 
probability models node 
decision trees permits context specific independence conditional independence independence variables may contingent observed value third variable 
practical advantage relationships learned space decision trees don need store separate probability combination parent variables 
learning attempt infer missing data values models probability variable value missing part distribution 
configured nbe handle missing values way 
avoids overfitting parameter penalty free parameter model 
default value different datasets better larger smaller values 
hold set avoid overfitting nbe appropriate hold set automatically tuning 
began conservative value learns simple models iteratively increased factors 
iteration trained model training set hold set removed tested hold set 
log likelihood hold set began decrease process halted 
trained model best known value training data including hold set 
simple baseline algorithm marginal distribution variable 
equivalent naive bayes model component 
tried running autoclass system cheeseman accuracy worse marginal distribution cases probably designed tuned class discovery general inference 
furthermore process repeatedly learning relearning models different numbers components resulted long training times 
learning time measured learning time methods datasets running windows xp ghz gb ram 
datasets learning times order magnitude faster nbe faster 
model clear advantage learning times 
learning times model dataset displayed graphically 
models required learn missing data making missing distinct value results look different 
inference fast nbe readily handle missing data 
contrast bayesian networks take substantial performance hit doing due comparatively slow approximate inference 
investigate differences inference time section 
nbe log likelihood variable log likelihood variable modeling accuracy comparative average log likelihoods benchmark datasets 
evaluate accuracy nbe models relative bayesian networks context specific independence trained models dataset method compared log likelihoods test data model 
cross validated datasets averaged log likelihoods train test splits 
results algorithm dataset columns table 
log likelihoods computed natural logarithm 
nbe models demonstrated greater accuracy generated datasets 
tailed paired test nbe performed significantly significantly better datasets performed better 
differences appear statistically significant actual differences accuracy typically small 
compares average log likelihoods nbe datasets dividing log likelihood number variables dataset 
better isolated letter speech musk datasets close 
nbe showed clear advantage remained competitive 
query speed accuracy applications interested inferring distribution variables possibly conditioned evidence 
example may wish determine probability set diseases set symptoms evidence 
applications interested speed accuracy marginal conditional queries 
area expect nbe shine perform exact inference linear time respect model size 
exact inference bayesian networks take exponential time respect number variables model 
implemented popular approximation algorithms bayesian networks gibbs sampling belief propagation 
algorithms including nbe inference implemented profiled optimized speed 
gibbs sampling tried different configurations settling offered best speed accuracy tradeoffs 
explored increasing iterations accuracy ceased improve course just preliminary hack involved monitoring actual score test data 
tried improve running randomly initialized chains testing mixing gelman diagnostic gilks sampling confidence probability true value described degroot schervish 
oddly methods ran minimum number iterations cases suggesting datasets may easy gibbs sampling convergence diagnostics may imperfectly detect convergence 
difficult select scheme fair terms speed accuracy ran iterations gibbs sampling unnecessarily poorly ran gibbs sampling take unnecessarily long 
decided fixed numbers iterations method simple interpretable subject subtle interactions datasets convergence diagnostics 
ran gibbs sampling scenarios single chain burn iterations sampling iterations chain separate chains burn iterations sampling iterations chains burn iterations sampling iterations chain 
iteration consisted sampling hidden variable 
increasing number iterations help significantly 
rao blackwellized gibbs sampler sample gave partial counts states single count chosen state 
time sampled state variable computed probability equaled states markov blanket inference 
ordinary gibbs sampling states chosen random count new configuration 
rao blackwellized version gave fractional counts possible configurations result sampling variable corresponding probabilities 
yields increased accuracy fewer samples 
single variable case rao blackwellization gives fractional counts states iteration 
multiple query variables possible state receive zero counts 
avoid zero probabilities gave configuration fractional prior count chosen prior counts summed 
example query possible configurations gave state initial count 
implemented standard belief propagation algorithm yedidia bayesian network converting pairwise markov network 
unfortunately decision tree local models converted potential function matrices yield exponential blow size 
belief propagation failed larger datasets required gigabytes memory store pairwise markov network 
cases able compensate applying belief propagation simpler model 
generated simpler models training alternate networks smaller values 
belief propagation run original bayesian network gb ram complex network 
manner able apply belief propagation datasets 
belief propagation offers guarantee convergence correctness standard form computes conditional probabilities single variables 
computing joint probabilities variables gibbs sampling form generalized belief propagation required 
omit belief propagation experiments 
convergence criterion stopped probability node changed iteration 
decreasing threshold significantly change results 
marginal queries compared inference speed accuracy marginal queries variables 
belief propagation computes distributions single variables omit experiment 
dataset queried marginal distributions random sets variables 
marginal query log likelihood test example assess accuracy inference method 
averaging queries computed expected log likelihood random marginal query 
hindsight accurate test marginal query test examples realization late rerun experiments 
methods asymptotically equivalent infinite test examples infinite queries methods eventually converge true expected value 
inference time compute average time required compute single marginal distribution 
slowness gibbs sampling test set examples dataset 
running gibbs sampling chains sampling iterations time examples dataset 
setup gibbs sampling took second half hour marginal query 
contrast nbe averaged milliseconds query 
results speed accuracy displayed top 
nbe accurate accurate version gibbs sampling accurate version gibbs sampling accurate nbe dataset 
additional iterations gibbs sampling increase accuracy gibbs sampling vastly slower nbe fastest version times slow slowest version times slow 
surprised discover nbe accurate musk accurate isolated letter speech datasets clearest advantage 
furthermore datasets slowest gibbs sampling requiring half hour query isolated letter speech minutes query musk 
suggests bayesian network models particular dataset better inference time may prohibitive yield worse results nbe 
single variable conditional queries compared inference speed accuracy single variable queries conditioned varying amounts evidence 
easier gibbs sampling ways 
evidence conditional queries reduces number variables gibbs sampling sample iteration greatly reduces size space gibbs sampling explore 
tend reduce gibbs sampling running time increasing accuracy 
second querying single variables gibbs sampling fewer configurations keep track multivariate case 
increases expected counts configuration avoids configurations counts 
querying single variables allows try alternate inference method belief propagation 
generated queries dataset hiding random variable values examples corresponding test set 
inference method determine distribution hidden variable 
know true value hidden variable able estimate inference method accuracy average log likelihood true values 
average log likelihood case crude approximation kullback leibler divergence 
ran experiments hidden variables trend remained 
shows results timing accuracy center graphs 
analysis follows 
belief propagation faster nbe inference datasets datasets variables hidden 
spite fast datasets performance remains mediocre due overhead incurred large state spaces generated markov network 
worst averaged seconds query hidden variables shuttle dataset 
efficient representation yield speed ups scope investigation 
terms accuracy belief propagation datasets run 
failure run datasets critical weakness 
nbe accurate faster time 
gibbs sampling faster conditional queries marginal queries remained slower nbe take mere samples variables took times long nbe exact inference 
running iterations gibbs sampling takes longer 
letter recognition dataset chains iterations took gibbs sampling seconds query hidden variables nbe averaged milliseconds query 
gibbs sampling better accuracy belief propagation winning majority datasets failing 
surprised find held true single chain sampling iterations 
wins overwhelming 
compares nbe gibbs sampling terms log likelihood variables hidden 
graphs variables hidden similar 
accuracy log likelihoods close outliers isolated letter speech forest cover type musk breast cancer wisc favor bayesian networks 
shows increasing number chains sampling iterations affected datasets 
suggests gibbs sampling conditional queries datasets converges quickly 
multiple variable conditional queries tested speed accuracy gibbs sampling naive bayes inference inferring conditional distributions variables 
dataset hid random variables test examples asked joint distribution hidden variables 
hidden variables query variables 
represents greater challenge gibbs sampling single variable case collect counts joint configurations 
implementation nbe inference described section evidence built reweighted mixture models model conditional joint distributions unseen variables 
linear time respect model size need approximation 
speed accuracy results displayed bottom 
single variable experiments gibbs sampling typically took orders magnitude longer nbe 
worst gibbs took seconds variable query jester nbe averaged milliseconds query 
surprising nbe inference linear gibbs sampling scales number samples taken number nbe log likelihood gibbs log likelihood comparison nbe gibbs sampling log likelihoods single variable conditional queries hidden variables 
dataset plot pair points corresponding gibbs sampling configurations chain sampling iterations chains sampling iterations 
pair connected line lines visible results experiments different 
suggests single variable conditional queries converge quickly 
sampled variables 
observe relative accuracy gibbs sampling somewhat worse partially remedied running gibbs sampling longer 
expected multivariate distributions states single variable distributions state tend fewer counts accurate probability estimate 
increase number inferred variables situation gets worse gibbs sampling nbe unaffected 
surprised discover gibbs sampling 
shows relative log likelihoods gibbs nbe variables hidden 
variable hidden rao blackwellized gibbs sampling equivalent exact inference 
variables hidden number possible configurations huge expect gibbs sampling accurate nbe uses exact inference times 
find gibbs sampling remains competitive datasets 
suggests conditional inference problems posed experiments especially challenging ones gibbs sampling 
gibbs sampling maintained accuracy scenarios remained slower nbe 
furthermore joint queries greater complexity intractable gibbs sampling size state space increases exponentially number hidden variables 
nbe readily compute joint distribution variables variables evidence gibbs sampling simply equipped handle 
discussion experiments show simple naive bayes models achieve comparable accuracy bayesian networks wide variety benchmark datasets 
furthermore naive bayes inference compute exact probabilities linear time respect model size 
model size potentially exponential model learned exponential number training examples 
marginal queries gibbs sampling accuracy worse orders magnitude longer run 
gibbs sampling achieved comparable accuracy conditional experiments remained slow 
belief propagation usually faster gibbs sampling accurate applied datasets 
method theoretically stronger converge distribution data 
experiments suggest naive bayes models fit real world datasets 
bayesian network provides greater modeling accuracy faster inference time naive bayes may offset cost 
argued bayesian networks give insight domain dubious nbe log likelihood variable conditional queries variable gibbs log likelihood variable nbe log likelihood variable conditional queries variables gibbs log likelihood variable comparison nbe gibbs sampling log likelihoods conditional queries query variables 
gibbs sampling results running chains sampling iterations 
log likelihoods variable queries divided order directly comparable single variable queries 
demonstrates nbe gibbs sampling achieve comparable accuracies datasets queries varied complexity 
instability 
furthermore bayesian network visualization preclude separate naive bayes model fast accurate inference 
final advantage naive bayes models easier implement leading algorithms learning bayesian network structure parameters 
entire implementation required lines including whitespace comments 
usually overlooked naive bayes models effective model general probabilistic learning inference 
experiments nbe demonstrate simple model necessarily yield significantly worse accuracy 
furthermore nbe offers efficient exact inference 
nbe orders magnitude faster best bayesian network approximate inference algorithms headache selecting inference algorithm suitable convergence criteria 
nbe gives flexibility general probabilistic inference headaches plague bayesian network implementations 
see main directions nbe 
modifications model improve accuracy 
example mixture trees mt model seen extension naive bayes allows dependencies input variables addition component variable meila jordan 
unfortunately study compared performance bayesian networks real world datasets 
second direction application similar mixture models relational domains 
probabilistic relational models feature interactions objects inference tends particularly difficult 
nbe model applied relational domains efficient inference problems tractable 
applied specific real world domains 
real world data types queries generated may reflect real world modeling goals 
real world case studies reveal domains nbe particularly suited 
nbe efficiently handle queries infeasible bayesian networks nbe increase number tractable domains types queries may answered 
author compaq providing eachmovie dataset ken goldberg providing jester dataset blue martini providing kdd cup data contributors uci data repository 
material supported national science foundation graduate research fellowship 
blake merz 

uci repository machine learning databases 
breese heckerman kadie 

empirical analysis predictive algorithms collaborative filtering 
proceedings th annual conference uncertainty artificial intelligence pp 

cheeseman kelly self stutz taylor freeman 

autoclass bayesian classification system 
proceedings fifth international conference machine learning pp 

san mateo california 
cheeseman stutz 

bayesian classification autoclass theory results 
fayyad piatetsky shapiro smyth uthurusamy eds advances knowledge discovery data mining 
aaai press mit press 
chickering geiger heckerman 

learning bayesian networks np hard technical report msr tr 
microsoft research technical report 
chickering heckerman meek 

bayesian approach learning bayesian networks local structure 
proceedings thirteenth conference uncertainty artificial intelligence 
providence ri morgan kaufmann 
chickering 

toolkit technical report msr tr 
microsoft redmond wa 
degroot schervish 

probability statistics third edition 
addison wesley 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society 
domingos pazzani 

optimality simple bayesian classifier zero loss 
machine learning 
friedman goldszmidt 

learning bayesian networks local structure 
proceedings twelfth conference uncertainty artificial intelligence pp 

san francisco ca morgan kaufmann 
gilks richardson spiegelhalter 

markov chain monte carlo practice 
chapman hall crc 
heckerman geiger chickering 

learning bayesian networks 
machine learning 
kohavi brodley mason zheng 

kdd cup organizers report peeling onion 
sigkdd explorations 
www ecn purdue edu kddcup 
meek thiesson heckerman 

learning curve sampling method applied model clustering 
journal machine learning research 
ai statistics 
meila jordan 

learning mixtures trees 
journal machine learning research 
pearl 

probabilistic reasoning intelligent systems networks plausible inference 
san francisco ca morgan kaufmann 
press teukolsky vetterling flannery 

numerical recipes cambridge uk cambridge university press 
sahami dumais heckerman horvitz 

bayesian approach filtering junk mail 
learning text categorization papers workshop 
madison wisconsin aaai technical report ws 
thiesson meek heckerman 

accelerating em large databases 
machine learning 
yedidia freeman weiss 

exploring artificial intelligence new millennium chapter understanding belief propagation generalizations 
morgan kaufmann publishers datasets accurate nbe datasets accurate nbe datasets accurate nbe marginal query accuracy gibbs gibbs gibbs number query variables single variable conditional accuracy gibbs gibbs gibbs bp number hidden variables multiple variable conditional accuracy gibbs gibbs gibbs number query variables inference time ratio vs nbe inference time ratio vs nbe inference time ratio vs nbe gibbs gibbs gibbs marginal query speed dataset single variable conditional speed gibbs gibbs gibbs bp dataset multiple variable conditional speed gibbs gibbs gibbs dataset query speed accuracy marginal conditional queries 
accuracy graphs left plot number datasets nbe achieves worse log likelihood inference method 
inference speed graphs right show time ratio inference method dataset relative nbe 
different number query hidden variables picked number yielding lowest ratios 
time ratios plotted logarithmic scale 
datasets ordered increasing time ratio indices refer nth smallest ratio single dataset particular 
note belief propagation produced results datasets remaining required excessive memory gb 
additional details see sections 
name attr 
train ex 
test ex 
nb acc 
wm acc 
marg 
acc 
auto imports abalone adult annealing anonymous audiology auto mpg breast cancer wisc bupa car census chess endgames connect choice credit screening forest cover type glass identification hepatitis housing house votes image segmentation ionosphere iris types isolated letter speech king rook vs king labor negotiations landsat letter recognition monks problem musk new thyroid nursery page blocks pima indians diabetes poisonous mushrooms promoter servo shuttle flare soybean large splice junction thyroid disease comb 
tic tac toe waveform yeast zoo eachmovie subset jester kdd cup subset table summary datasets results 
cross validated datasets train test set sizes averaged train test splits 

