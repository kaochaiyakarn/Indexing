learning probabilistic relational models nir friedman lise getoor daphne koller avi pfeffer hebrew university stanford university stanford university stanford university large portion real world data stored commercial relational database systems 
contrast statistical learning methods flat data representations 
apply methods forced convert data flat form losing relational structure database 
builds probabilistic relational models prms describes learn databases 
prms allow properties object depend probabilistically properties object properties related objects 
prms significantly expressive standard models bayesian networks show extend known statistical methods learning bayesian networks learn models 
describe parameter estimation structure learning automatic induction dependency structure model 
show learning procedure exploit standard database retrieval techniques efficient learning large datasets 
experimental results real synthetic relational databases 
relational models common representation structured data 
enterprise business information marketing sales data medical records scientific datasets stored relational databases 
relational databases multi dollar industry 
growing interest making sophisticated huge amounts data particular mining databases certain patterns regularities 
explicitly modeling regularities gain deeper understanding domain may discover useful relationships 
model fill unknown important information 
institute computer science hebrew university jerusalem israel computer science department stanford university gates building stanford ca uncertainty probabilistic reasoning example may interested predicting person potential money bank deposits international travel business connections arrest records known associates jensen 
case may interested classifying web pages belonging student faculty member project attributes web page related pages craven 
unfortunately inductive learning algorithms capable handling data relational form 
restricted dealing flat set instances separate attributes 
methods typically flattens relational data removing richer structure 
process loses information crucial understanding data 
consider example problem predicting value attribute certain entity person money 
attribute correlated attributes entity attributes related entities financial transactions conducted person people involved transactions transactions conducted people order flatten problem need decide advance fixed set attributes learning algorithm task 
want learning algorithm deal multiple entities properties reach entity characteristics properties entities related 
inductive logic programming ilp lavrac dzeroski primary learning framework capability 
ilp algorithms learn logical horn rules determining order predicate holds 
ilp excellent solution settings may inappropriate 
main limitation deterministic nature rules discovered 
domains examples encounter interesting correlations far deterministic 
goal learn refined probabilistic models represent statistical correlations properties entity properties related entities 
model reasoning entity entire rich structure knowledge encoded relational representation 
starting point structured representation probabilistic models exemplified bayesian networks bns 
bn allows provide compact rep resentation complex probability distribution fixed set attributes random variables representation exploits locality influence domains 
build developments field bayesian networks 
deep understanding statistical learning problem models heckerman heckerman role structure providing appropriate bias learning task :10.1.1.156.9918
second development representations extend attribute bn representation incorporate richer relational structure koller pfeffer ngo haddawy poole combine advances 
key contributions show techniques bayesian network learning extended task learning complex models 
contribution generalizes koller pfeffer preliminary topic 
start describing semantics probabilistic relational models 
examine problems parameter estimation structure selection class models 
deal crucial technical issues distinguish problem learning relational probabilistic models learning bayesian networks 
provide formulation likelihood function appropriate setting show interacts standard assumptions bn learning 
search coherent dependency structures significantly complex case learning bn structure introduce necessary tools concepts effectively 
describe experimental results synthetic real world datasets discuss possible extensions applications 
underlying framework relational model describe relational model generic terms closely related language entity relationship models 
generality allows framework mapped variety specific relational systems including probabilistic logic programs ngo haddawy poole probabilistic frame systems koller pfeffer 
learning results apply frameworks 
vocabulary relational model consists set classes set relations entity type associated set attributes attribute takes values fixed domain values relation typed 
vocabulary defines schema relational model 
consider simple genetic model inheritance single gene determines person blood type 
person copies chromosome containing gene inherited mother inherited father 
possibly contaminated test attempts recognize person blood type 
schema contains classes person blood test relations father mother test 
attributes person name gender chromosome chromosome inherited father inherited mother 
attributes blood test serial number date contaminated result 
instance schema defines set entities entity type entity attribute instance associated attribute value denoted relation specifies holds 
interested describing probability model instances relational schema 
attributes name social security number fully determined 
label attributes fixed 
assume known instantiation schema 
attributes called probabilistic 
skeleton structure relational schema partial specification instance schema 
specifies set objects class values fixed attributes objects relations hold objects 
leaves values probabilistic attributes unspecified 
completion skeleton structure extends skeleton specifying values probabilistic attributes 
final definition turn useful notion slot chain 
relation project th th arguments obtain binary relation view slot denote elements holds 
relational algebra notation objects set called relatives concatenate slots form longer slot chains defined composition binary relations 
chain appropriately typed 
probabilistic relational models proceed definition probabilistic relational models prms 
basic goal model uncertainty values non fixed probabilistic attributes objects domain discourse 
words skeleton structure want define probability distribution completions skeleton 
probabilistic model consists components qualitative dependency structure parameters associated dependency structure defined associating attribute set parents pa 
correspond formal parents instantiated different ways different objects intuitively parents attributes direct influences distinguish types formal parents 
attribute depend probabilistic attribute formal dependence induces corresponding dependency individual objects object depend probabilistically attribute depend attributes related objects slot chain 
understand semantics formal dependence individual object recall represents set objects relatives cases slot chain guaranteed single valued specify probabilistic dependence multiset notion aggregation database theory gives precisely right tool address friedman getoor koller pfeffer prm structure simple genetics domain 
fixed attributes shown regular font probabilistic attributes shown italic 
dotted lines indicate relations entities solid arrows indicate probabilistic dependencies 
issue depend probabilistically aggregate property multiset 
natural useful notions aggregation mode set frequently occurring value mean value set values numerical median maximum minimum values ordered cardinality set formally language allows notion aggregate takes multiset values ground type returns summary 
type aggregate arguments 
allow types aggregate reports size multiset 
allow parent semantics depend value define obvious way 
returning genetics example consider attribute blood test result 
result blood test depends contaminated blood test contaminated parent 
result depends genetic material person tested 
test single valued add blood test test blood test test chromosome parents 
shows structure simple prm domain 
set parents define local probability model conditional probability distribution cpd specifies precisely set parents recall parents simple attribute relation aggregate set relatives set values ground type 
tuple values cpd specifies distribution parameters cpds comprise skeleton structure schema want local probability models define probability distribution completions skeleton 
note skeleton determines set objects model 
associate random variable probabilistic attribute object skeleton determines relations uncertainty probabilistic reasoning objects set relatives associated object relationship chain note assuming relations objects specified disallowing uncertainty relational structure model 
define coherent probabilistic model skeleton ensure probabilistic dependencies acyclic random variable depend directly indirectly value 
consider parents attribute xa 
parent define edge parent define edge say dependency structure acyclic relative skeleton directed graph defined variables acyclic 
case define coherent probabilistic model complete instantiations consistent proposition acyclic relative defines distribution completions briefly sketch proof proposition showing construct bn probabilistic attributes skeleton construction reminiscent knowledge model construction approach wellman 
construction merely thought experiment learning algorithm constructs network 
network node variable aggregate quantities required parents 
parents aggregate random variables attributes participate aggregation relations specified cpds random variables correspond probabilistic attributes simply cpds described cpds random variables correspond aggregate nodes capture deterministic function particular aggregate operator 
easy verify probabilistic dependencies acyclic induced bayesian network 
construction suggests way answering queries relational model 
compile corresponding bayesian network standard tools answering queries 
skeleton compile prm bayesian network prm expresses information resulting bn 
bn defines probability distribution fixed set attributes prm specifies distribution skeleton different skeletons set number entities domain vary relations entities 
way prms bns set rules order logic set rules propositional logic rule parent grandparent induces potentially infinite set ground propositional instantiations 
parameter estimation move task learning prms 
learning parameters prm dependency structure known 
words structure determines set parents attribute task learn parameters define cpds structure learning particular training set take complete instance task relatively straightforward interest 
addition crucial component structure learning algorithm described section 
key ingredient parameter estimation likelihood function probability data model 
function captures response probability distribution changes parameters 
usual likelihood parameter set defined probability data model usual typically log function key insight equation similar log likelihood data bayesian network heckerman 
fact likelihood function bayesian network induced structure skeleton 
main difference standard bayesian network parameter learning parameters different nodes network forced identical 
understood theory learning bayesian networks 
consider task performing maximum likelihood parameter estimation 
goal find parameter setting maximizes likelihood estimation simplified decomposition log likelihood function summation terms corresponding various attributes different classes 
terms square brackets maximized independently rest 
maximal likelihood estimation reduces independent maximization problems cpd 
multinomial cpds maximum likelihood estimation done sufficient statistics case just counts different values attribute parents jointly take 
proposition assuming multinomial cpds maximum likelihood parameter setting consequence proposition parameter learning prms reduced counting sufficient statistics 
need count vector sufficient statistics cpd 
counting done straightforward manner standard databases queries 
note proposition shows learning parameters prms similar learning parameters bayesian networks 
fact view learning parameters bn prm induces skeleton 
discussed learned parameters reasoning skeletons induce completely different bn 
cases maximum likelihood parameter estimation robust overfits training data 
bayesian approach uses prior distribution parameters smooth irregularities training data significantly robust 
see section bayesian framework gives metric evaluating quality different candidate structures 
due space limitations briefly describe alternative approach 
roughly speaking bayesian approach introduces prior unknown parameters performs bayesian conditioning data evidence compute posterior distribution parameters 
apply idea setting recall prm parameters composed set individual probability distribution conditional distribution form 
bayesian approaches learning bayesian networks heckerman assumptions 
assume parameter independence priors parameters different independent 
second assume prior dirichlet distribution 
briefly dirichlet prior multinomial distribution variable specified set hyperparameters distribution parameters dirichlet details see degroot 
parameter prior satisfying assumptions posterior form 
product independent dirichlet distributions parameters computed easily 
proposition complete assignment prior satisfies parameter independence dirichlet hyperparameters posterior product dirichlet distributions hyperparameters updated posterior evaluate probability new data 
case bn learning assume instances iid implies independent value parameters 
evaluate new instance need posterior parameters 
probability new instance probability possible parameter value weighted posterior probability values 
case bns term rewritten simply instance probability expected value parameters mean posterior dirichlet parameter 
suggests expected parameters evaluating new data 
formula expected parameters analogous bns proposition assuming multinomial cpds prior independence dirichlet priors hyperparameters friedman getoor koller pfeffer unfortunately expected parameters proper bayesian solution computing probability new data 
possible complications 
problem setting assumption iid data violated 
specifically new instance conditionally independent old ones parameters 
consider genetics domain assume new data involves information mother person database 
case new object changes probability attributes simply old posterior parameters reason new instance 
problem occur new data related training data new data essentially disjoint database scheme 
interestingly problem disappears attributes new objects parents attribute training set 
genetics example means insert new people database long ancestors people database 
second problem involves formal justification expected parameters values 
argument depends fact probability new instance linear value parameter 
parameter 
assumption violated consider probability complex database involving multiple instances class 
case integral probability new data parameters longer reduced computing probability relative expected parameter value 
correct expression called marginal likelihood new data section scoring structures 
note posterior sharply peaked seen training instances approximate term expected parameters proposition single instance 
practice expected parameters learned model 
structure selection move challenging problem learning dependency structure automatically opposed having user 
important issues need addressed 
determine dependency structures legal need evaluate goodness different candidate structures need define effective search procedure finds structure 
legal structures consider different dependency structures important sure dependency structure choose results coherent probability models 
guarantee property see proposition skeleton acyclic relative course easily verify candidate structure acyclic relative skeleton training database 
want guarantee acyclic relative databases may encounter domain 
guarantee acyclicity arbitrary database 
simple approach uncertainty probabilistic reasoning ensure dependencies attributes respect order stratified 
precisely say directly depends parent parent aand relatives class require directly depends attributes precede order 
simple approach clearly ensures acyclicity limited cover important cases 
consider genetic model 
genotype person depends genotype parents person depending directly person chromosome clearly violates requirements simple approach 
model apparent attribute level resolved level individual objects person ancestor 
resolution acyclicity relies prior knowledge domain 
allow learning algorithm deal dependency models allow user give algorithm prior knowledge 
allow user assert certain slots guaranteed acyclic guaranteed partial ordering relative say guaranteed acyclic components guaranteed acyclic 
prior knowledge determine legality certain dependency models 
start building graph describes direct dependencies attributes 
graph yellow edge parent parent edge green guaranteed acyclic red 
note edges different colors attributes 
intuition dependency green edges relates objects ordered acyclic order 
edges combined intra object dependencies yellow edges cause cyclic dependency 
take care dependencies prior knowledge form cycle 
intuition suggests definition colored dependency graph stratified cycle graph contains green edge red edges 
proposition colored dependency graph stratified skeleton slots jointly acyclic defines coherent probability distribution assignments notion stratification generalizes special cases considered 
guaranteed acyclic relations edges dependency graph colored yellow red 
graph stratified acyclic 
genetics example relations suffices check dependencies objects yellow edges acyclic 
proposition stratification colored graph determined time linear number edges graph 
omit details algorithm lack space relies standard graph algorithms 
note easy expand definition stratification situations prior knowledge involves sets guaranteed acyclic relations set order objects grid north south ordering east west ordering 
simply color graph colors check cycle contains edges exactly color yellow red 
evaluating different structures know structures legal need decide evaluate different structures order pick fits data 
adapt bayesian model selection methods framework 
formally want compute posterior probability structure instantiation bayes rule score composed main parts prior probability structure probability data assuming structure 
component defines prior structures 
assume choice structure independent skeleton context bayesian networks simple uniform prior possible dependency structures 
unfortunately assumption setting 
problem may infinitely possible structures 
genetics example person genotype depend genotype parents grandparents simple natural solution penalizes long indirect slot chains having log proportional sum lengths chains appearing second component marginal likelihood parameter independent dirichlet prior integral decomposes product integrals simple closed form solution 
simple generalization ideas bayesian score bayesian networks 
proposition complete assignment satisfies parameter independence dirichlet hyperparameters marginal likelihood equal dt gamma function marginal likelihood product simple terms corresponds distribution term depends hyperparameters sufficient statistics marginal likelihood term dominant term probability structure 
balances complexity structure fit data 
balance explicitly asymptotic relation marginal likelihood explicit penalization mdl score see heckerman 
note bayesian score requires assign prior parameter values possible structure 
infinitely alternative structures formidable task 
case bayesian networks class priors described single network heckerman 
priors additional property structure equivalent guarantee marginal likelihood structures strong sense equivalent 
notions defined richer structures defer issue 
simply assume simple dirichlet prior uniform defined attribute parent set 
structure search test determining structure legal scoring function allows evaluate different structures need provide procedure finding legal high scoring structures 
bayesian networks know task np hard chickering 
prm learning hard bn learning bn simply prm class relations hope find efficient procedure finds highest scoring structure 
resort heuristic search 
simplest algorithm greedy hill climbing search score metric 
maintain current candidate structure iteratively improve 
iteration consider set simple local transformations structure score pick highest score 
deal local maxima random restarts 
bayesian networks decomposability property score significant impact computational efficiency search algorithm 
decompose score sum local scores corresponding individual attributes parents 
search algorithm considers modification current structure parent set single attribute different component score associated change 
need reevaluate particular component leaving unchanged results major computational savings 
problems simple approach 
discussed previous section infinitely possible structures 
second atomic steps search expensive process computing sufficient statistics requires expensive database operations 
restrict set candidate structures step search afford database operations necessary evaluate 
propose heuristic search algorithm addresses issues 
high level algorithm proceeds phases 
phase set potential parents attribute standard structure search restricted space structures parents advantage approach precompute view correspond friedman getoor roller pfeffer ing expensive computations joins aggregation required definition parents precomputed views 
sufficient statistics subset potential parents easily derived view 
construction decomposability score allows steps search say greedy hill climbing done efficiently 
success approach depends choice potential parents 
clearly wrong initial choice result poor structures 
friedman examines similar approach context learning bayesian networks propose iterative approach starts structure possibly attribute parents select sets structure :10.1.1.122.6284
apply search procedure get new higher scoring structure 
choose new potential parents new structure reiterate stopping improvement 
remains discuss choice different phases 
simplest approach setting set attributes successive phases consist attributes related slot chains length course new attributes require aggregation sidestep issue possible aggregates attribute 
scheme expands set potential parents iteration 
usually results large set potential parents 
refined algorithm adds parents add value reasonable ways evaluating additional value provided new parents 
discussed friedman context learning bayesian networks :10.1.1.122.6284
results suggest evaluate new potential parent measuring change score family add current parents 
choose highest scoring current parents new set potential parents 
approach allows significantly reduce size potential parent set resulting view cause significant degradation quality learned model 
implementation experimental results implemented learning algorithm top postgres object relational database management system 
required counts obtained simply database selection queries cached avoid performing query twice 
search process created temporary materialized views corresponding joins different relations views computing counts 
tested proposed learning algorithm domains real synthetic 
domains different characteristics 
movie database contains relations movie actor appears relates actors movies played 
uncertainty probabilistic reasoning database contains movies actors 
database simple structure presents kind problems encounters dealing real data missing values large domains attributes inconsistent values 
fact algorithm able deal kind real world problem quite promising 
algorithm learned model shown 
model reasonable close consider correct 
learned genre movie depended decade film process color black white decade depended film process 
learned interesting dependency combining relations role type played actor movie depends gender actor genre movie 
second database artificial genetic database similar example quite different challenges 
thing recursive nature domain allows arbitrarily complex joins defined 
addition probabilistic model domain fairly subtle 
person relevant attributes chromosome domain related attributes person mother father 
gold standard model generate data structure model shown earlier 
trained algorithm datasets various sizes ranging 
data set size consisted family tree containing people average blood tests person 
evaluated algorithm test set size 
shows log likelihood test set learned models 
cases algorithm learned model correct structure scored 
small minority cases algorithm got stuck local maxima learning model incorrect structure scored quite poorly 
seen scatter plots show median log likelihood learned models quite reasonable outliers 
standard techniques random restarts deal local maxima 
discussion defined new statistical learning task learning probabilistic relational models data 
shown ideas bayesian network learning carry new task 
shown raises new challenges 
scaling ideas large databases important issue 
believe achieved closer integration technology database systems including indices query optimization 
furthermore lot extracting information massive data sets including finding frequently occurring combinations values attributes 
believe ideas help significantly computation sufficient statistics 
important possible extensions 
obvious treatment missing data hidden variables 
extend standard techniques expectation maximization missing data prm learned movie domain real world database containing movies actors learning curve showing generalization performance prms learned genetic domain 
shows databases size shows log likelihood test set size 
sample size show independent learning experiments 
curve shows median log likelihood models function sample size 
task see koller pfeffer preliminary related models 
complexity inference large databases missing values cost naive application algorithms prohibitive 
clearly domain calls new inference algorithms new learning algorithms avoid repeated calls inference large problems 
interesting issue automated discovery hidden variables 
preliminary answers question context bayesian networks friedman context ilp context simple binary relations hofmann 
combining ideas extending complex framework significant interesting challenge 
direction extends class models consider 
assumed relational structure specified probabilistic attribute values determined 
richer class prms roller pfeffer allow probabilities structure model example uncertainty set objects model number children couple relations objects blood crime scene 
ultimately want techniques help automatically discover interesting entities relationships hold world 
acknowledgments relationship probabilistic dependency nir friedman supported michael trust 
lise getoor daphne koller avi pfeffer supported onr contract darpa hpkb program onr aro muri program integrated approach intelligent systems generosity sloan foundation powell foundation 
chickering 
learning bayesian networks np complete 
fisher 
lenz editors learning data artificial intelligence statistics springer verlag 
craven dipasquo freitag mccallum mitchell nigam slattery 
learning extract symbolic knowledge world wide web 
proc 
aaai 
degroot 
optimal statistical decisions 
mcgraw hill new york 
friedman nachman peer 
learning bayesian network structure massive datasets sparse candidate algorithm 
submitted 
friedman 
learning belief networks presence missing values hidden variables 
proc 
icml 
heckerman geiger chickering 
learning bayesian networks combination knowledge statistical data 
machine learning 
heckerman 
tutorial learning bayesian networks 
jordan editor learning graphical models 
mit press cambridge 
ma 
hofmann puzicha jordan 
learning dyadic data 
nips 
appear 
jensen 
prospective assessment ai technologies fraud detection case study 
aaai workshop approaches fraud detection risk management 
koller pfeffer 
learning probabilities noisy firstorder rules 
proc 
pages 
koller pfeffer 
probabilistic frame systems 
proc 
aaai 
lavrac dzeroski 
inductive logic programming techniques applications 
ellis horwood 
ngo haddawy 
answering queries contextsensitive probabilistic knowledge bases 
theoretical computer science 
poole 
probabilistic horn abduction bayesian networks 
artificial intelligence 
wellman breese goldman 
knowledge bases decision models 
knowledge engineering review 
friedman getoor koller pfeffer 
