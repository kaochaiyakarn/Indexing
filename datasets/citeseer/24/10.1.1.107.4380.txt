automated state abstraction options tree algorithm anders jonsson andrew barto department computer science university massachusetts amherst ma barto cs umass edu learning complex task significantly facilitated defining hierarchy subtasks 
agent learn choose various temporally actions solving assigned subtask accomplish task 
study hierarchical learning framework options 
argue take full advantage hierarchical structure perform option specific state abstraction scale larger tasks state abstraction automated 
adapt mccallum tree algorithm automatically build option specific representations state feature space illustrate resulting algorithm simple hierarchical task 
results suggest automated option specific state abstraction attractive approach making hierarchical learning systems effective 
researchers field reinforcement learning focused considerable attention temporally actions :10.1.1.32.8206:10.1.1.25.1865
term temporally describes actions take variable amounts time 
motivation temporally actions exploit hierarchical structure problem 
things hierarchical structure natural way incorporate prior knowledge learning system allowing reuse temporally actions policies learned tasks 
learning hierarchy significantly reduce number situations learning agent needs discriminate 
framework options extends theory reinforcement learning include temporally actions 
cases accurately executing option policy depend state features available learning agent 
features relevant differ option option 
hierarchical learning system possible perform option specific state abstraction irrelevant features specific option ignored 
option specific state abstraction hierarchical learning system save memory development compact state representations accelerate learning generalization induced abstraction 
dietterich introduced action specific state abstraction hierarchy temporally actions 
approach requires system developer define set relevant state features action prior learning 
complexity problem grows increasingly difficult hand code state representations 
way remedy problem automated process constructing state representations 
apply mccallum tree algorithm individual options achieve automated option specific state abstraction 
tree algorithm automatically builds state feature representation starting distinctions different observation vectors 
specification state feature dependencies necessary prior learning 
section give brief description tree algorithm 
section introduces modifications necessary tree algorithm suitable learning hierarchical system 
describe setup experiments section results section 
section concludes discussion 
tree algorithm tree algorithm retains history transition tt instances tt rt st composed observation vector st time step previous action reward rt received transition st tt previous instance 
decision tree tree sorts new instance tt components assigns unique leaf tree 
distinctions associated leaf determined root leaf path 
leaf action pair algorithm keeps action value estimating discounted reward associated executing utility leaf denoted maxa algorithm keeps model consisting estimated transition probabilities pr lk expected immediate rewards computed transition instances 
model performing sweep value iteration execution action modifying values leaf action pairs pr lk lk lk reinforcement learning algorithms update action values learning prioritized sweeping 
tree algorithm periodically adds new distinctions tree form temporary nodes called fringe nodes performs statistical tests see added distinctions increase predictive power tree 
distinction perceptual dimension observation previous action history index indicating far back current history dimension examined 
leaf tree extended subtree fixed depth constructed permutations distinctions path leaf 
instances associated leaf distributed leaves added subtree fringe nodes corresponding distinctions 
statistical test kolmogorov smirnov ks test compares distributions discounted reward leaf node policy action fringe node policy action 
distribution discounted reward associated node policy action composed estimated discounted reward individual instances tt rt tt pr lk lk lk ks test outputs statistical difference dl distributions nodes lk 
tree algorithm retains subtree distinctions leaf sum ks statistical differences fringe nodes subtree larger sum ks differences subtrees exceeds threshold 
tree extended leaf subtree new distinctions subtrees dl dl lj lj lj tree extended action values previous leaf node passed new leaf nodes 
restrict number distinctions agent time imposing limit depth tree 
length history algorithm needs retain depends tree size size state set 
consequently algorithm potential scale large tasks 
previous experiments tree algorithm able learn compact state representation satisfactory policy complex driving task 
version tree algorithm suitable continuous state spaces developed successfully robot soccer 
adapting tree algorithm options turn issue adapting tree algorithm options hierarchical learning architectures 
finite markov decision process state set option consists set states option initiated closed loop policy choice actions termination condition state gives probability option terminate state reached 
primitive actions generalize options terminate time step 
easy define hierarchies options policy option select options 
local reward function associated option facilitate learning option policy 
tree algorithm suitable performing option specific state abstraction tree simultaneously defines state representation policy representation 
separate tree assigned option algorithm able perform state abstraction separately option modifying policy 
options different levels hierarchy operate different time scales transition instances take different forms 
scheme need add notion temporal abstraction definition transition instance definition transition instance option form ot rt st st observation vector time step ot option previously executed option terminating time duration rt discounted rt sum rewards received execution ot previous instance 
options level hierarchy executed time experience different sequence transition instances 
tree algorithm conditions tree option keep history instances base distinctions instances 
tree algorithm developed infinite horizon tasks 
option terminates may executed time associated history finite segments corresponding separate executions option 
transition taxi task instance recorded execution independent instance recorded previous execution 
consequently allow updates segments 
modifications tree algorithm applied hierarchical learning options 
intra option learning options operate parts state space choose actions possible learn option behavior generated execution options 
process called intra option learning action values option updated actions executed associated option 
update occurs action executed non zero probability executed 
similarly base distinctions tree associated option transition instances recorded execution option 
adding instances recorded execution option history associated option 
associating instance vector leaves tree option approach require additional memory keeping multiple copies instance 
scheme introduce vector rewards rt ro instance ro discounted sum local rewards option associated ot experiments tested version tree algorithm taxi task agent taxi moves grid :10.1.1.32.8206
taxi assigned task delivering passengers locations destination chosen random set pick drop sites taxi agent observation vector composed position taxi location current passenger passenger destination actions available taxi pick drop passenger delivered new passenger appears random pickup site 
rewards provided taxi delivering passenger aid taxi agent introduced navigate options letting denote set observation vectors gp location action including moving walls policy getting agent trying learn 
cardinal directions 
introduced local reward identical global reward provided agent exception reaching application tree algorithm taxi problem history option maximum length instances 
length exceeded oldest instance history discarded 
expanding tree considered instances history 
set expansion depth expansion threshold distinctions tree case 
algorithm lower threshold agent able distinctions difficult case accumulate evidence statistical difference accept distinction 
tree algorithm go back reconsider distinctions tree important reduce number incorrect distinctions due sparse statistical evidence 
implementation compared distributions discounted reward leaves contained instances 
taxi task fully observable set history index tree algorithm zero 
exploration system softmax strategy picks random action probability performs softmax 
normally tuning softmax temperature provides balance exploration exploitation tree evolves new value may improve performance 
avoid re tuning random part ensured actions executed regularly 
designed set experiments examine efficiency intra option learning 
randomly selected execute randomly selected new position taxi reached ignoring issue delivering passenger 
learning run assigned tree containing single node option 
set runs algorithm intra option learning set regular learning trees different options share instances 
second set experiments policies options taxi task learned parallel 
allowed policy task choose options navigate 
reward provided task sum global reward local reward option currently executed cf 

passenger delivered new taxi position selected randomly new passenger appeared randomly selected pickup site 
results results intra option learning experiments shown 
graphs intra option learning solid regular learning broken averaged independent runs 
tuned set learning runs give maximum performance 
intervals time steps trees options saved evaluated separately 
evaluation consisted fixing target repeatedly navigating target time steps randomly repositioning taxi time target reached repeating targets adding rewards 
results conclude intra option learning converges faster regular learning intra option learning achieves higher level performance 
faster convergence due fact histories associated options fill quickly intra option learning 
higher performance achieved amount evidence larger 
target option reached execution option reached times execution option 
second set experiments performed learning runs duration tree evaluation intra option regular time steps comparison intra option regular learning time steps 
shows example resulting trees 
nodes represent distinctions drawn circles leaf nodes shown squares cases omitted 
denotes distinction previously executed option order navigate pick drop letters denote distinction corresponding observation 
note tree distinction positions lower part grid 
places example navigate right branch algorithm suboptimal distinction 
distinction smaller number leaves sufficient represent optimal policy 
trees contain total leaf nodes 
runs number leaf nodes varied average 
leaf nodes visited making actual number states smaller 
comparable results dietterich hand coded representation containing states 
compared distinct states flat representation task distinct states policies require abstraction result significant improvement 
certainly memory required store histories taken account 
believe memory savings due option specific state abstraction larger tasks significantly outweigh memory requirement trees 
shown tree algorithm options hierarchical learning system 
results suggest automated option specific state abstraction performed algorithm attractive approach making hierarchical learning systems effective 
testbed small believe important step automated state abstraction hierarchies 
incorporated intra option learning tree algorithm method allows learning agent extract information training data 
results show intra option learning significantly improve performance learning agent performing option specific state abstraction 
main motivation developing hierarchical version tree algorithm automating state abstraction new definition transition instance enables history structured hierarchically useful learning solve problems partially observable domains 
examine performance option specific state abstraction tree algorithm larger realistic tasks 
plan develop version task navigate navigate trees different policies navigate navigate tree algorithm goes back tree distinctions 
potential improve performance algorithm correcting nodes incorrect distinctions 
acknowledgments authors tom dietterich providing code taxi task andrew mccallum valuable correspondence regarding tree algorithm ted perkins reading providing helpful comments 
funded national science foundation 
ecs 
opinions findings recommendations expressed material authors necessarily reflect views national science foundation 
dietterich :10.1.1.32.8206

hierarchical reinforcement learning maxq value function decomposition 
artificial intelligence research 
dietterich 
state abstraction maxq hierarchical reinforcement learning 
solla leen 
muller eds advances neural information processing systems pp 

cambridge ma mit press 

emergent hierarchical control structures learning reactive hierarchical relationships reinforcement environments 
meas mataric eds animals animats 
cambridge ma mit press 
mccallum 
reinforcement learning selective perception hidden state 
phd thesis computer science department university rochester 
parr russell 
reinforcement learning hierarchies machines 
jordan kearns solla eds advances neural information processing systems pp 

cambridge ma mit press 
precup sutton 
multi time models temporally planning 
jordan kearns solla eds advances neural information processing systems pp 

cambridge ma mit press 
singh 
reinforcement learning hierarchy models 
proc 
th national conf 
artificial intelligence pp 

menlo park ca aaai press mit press 
sutton precup singh 
intra option learning temporally actions 
proc 
th intl 
conf 
machine learning icml pp 

morgan kaufman 
sutton precup singh 
mdps semi mdps framework temporal abstraction reinforcement learning 
artificial intelligence 
uther veloso 
generalizing adversarial reinforcement learning 
aaai fall symposium model directed autonomous systems 

