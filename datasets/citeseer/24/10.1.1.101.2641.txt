decision trees mpi collective algorithm selection problem graham fagg george jack dongarra innovative computing laboratory university tennessee computer science department volunteer blvd knoxville tn usa fagg dongarra cs utk edu october selecting close optimal collective algorithm parameters collective call run time important step achieving performance mpi applications 
explore applicability decision trees mpi collective algorithm selection problem 
construct decision trees measured algorithm performance data analyze decision tree properties expected run time performance penalty 
cases considered results show decision trees generate reasonably small accurate decision function 
example broadcast decision tree leaves able achieve mean performance penalty 
similarly combining experimental data reduce broadcast generating decision function combined decision trees resulted relative performance penalty 
results indicate decision trees applicable problem widely domain 
performance mpi collective operations crucial performance mpi applications 
reason significant efforts gone design implementation efficient collective algorithms homogeneous heterogeneous cluster environments 
performance algorithms varies total number nodes involved communication system network characteristics size data transferred current load applicable operation performed segment size operation pipelining 
selecting best possible algorithm segment size combination method instance collective operation important 
ensure performance mpi applications collective operations tuned particular system 
tuning process involves detailed profiling system possibly combined communication modeling analyzing collected data generating decision function 
run time decision function selects close optimal method particular collective instance 
approach relies ability decision function accurately predict algorithm segment size particular collective instance 
alternatively construct memory decision system queried searched run time provide optimal method information 
order approaches feasible memory footprint time takes decisions need minimal 
studies applicability decision trees mpi collective algorithm method selection problem 
assume system interest benchmarked detailed performance information exists available collective communication methods information focus efforts investigating algorithm feasible way generate static decision functions 
proceeds follows section discusses existing approaches decision making algorithm selection problem section provides background information algorithm section discusses mapping performance measurement data input section presents experimental results section concludes discussion results 
related mpi collective algorithm selection problem addressed mpi implementations 
ft mpi decision function generated manually visual inspection method augmented matlab scripts analysis experimentally collected performance data 
approach results precise albeit complex decision function 
mpich mpi implementation algorithm selection bandwidth latency requirements algorithm switching points predetermined implementers 
tuned collective module open mpi algorithm selection done ways compiled decision function user specified command line flags rule run length encoding scheme tuned particular system 
possibility view problem data mining task algorithm selection problem replaced equivalent classification problem 
new problem classify collective parameters collective operation communicator size message size detailed benchmarking possible methods takes significant amount time 
option performance profiles generated limited set performance measurements coupled performance modeling 
correct category method case run time 
major benefit approach decision making process studied topic engineering machine learning fields literature readily available 
decision trees extensively pattern recognitions cad design signal processing medicine biology 
construct statistical learning models build different decision functions matrix matrix multiplication algorithm selection 
consider methods decision function construction parametric modeling parametric geometry modeling non parametric geometry modeling 
non parametric geometry modeling uses statistical learning methods construct implicit models boundaries switching points algorithms actual experimental data 
achieve support vector method 
conceptually close non parametric geometry modeling done problem domain different mpi collective operations matrix matrix multiplication algorithm support vector methods 
best knowledge group approached mpi collective tuning process way 
algorithm supervised learning classification algorithm construct decision trees data 
applied data fulfills requirements attribute value description information single entry data described terms attributes 
attribute values discrete continuous cases attribute value may missing ignored predefined classes training data divided predefined classes categories 
standard requirement supervised learning algorithms discrete classes classes clearly separated single training case belongs class 
predict continuous class values cost transaction sufficient data algorithm utilizes inductive generalization process searching patterns data 
approach patterns distinguishable random occurrences 
constitutes sufficient amount data depends particular data set attribute class values general statistical methods generate tests require reasonably large amount data logical classification models generated classification models represented decision trees set production rules 
hunt method decision tree construction set training cases set classes ck tree constructed recursively testing cases contains cases belong class cj leaf node created denoted belong cj class contains cases leaf node created class belongs selected outside source 
algorithm selects frequent class parent node contains cases belong class find test split set single class collections cases 
test single attribute value selected results mutually exclusive outcomes 
set split subsets tn set ti contains cases outcome oi 
algorithm called recursively subsets hunt method decision tree construction 
algorithm initial decision tree constructed variation hunt method decision tree construction 
main difference similar decision tree building algorithms test selection evaluation process case 
utilizes information gain ratio criterion maximizes normalized information gain partitioning accordance particular tests 
define gain ratio look information conveyed classified cases 
consider set training cases 
select single case decide belongs class cj probability message freq cj conveys log freq cj bits information 
average amount information needed identify class case set computed weighted sum case information amounts info freq cj log freq cj set partitioned subsets outcomes test compute similar information requirement ti info ti information gained partitioning accordance test computed gain info predecessor method id algorithm gain criterion equation select test partition 
gain criterion biased high frequency data 
ameliorate problem normalizes information gain amount potential information generated dividing subsets split info ti ti log condition selects test partition set available cases defined gain gain ratio split info selects test maximizes gain ratio value 
initial decision tree constructed pruning procedure initiated decrease tree size decrease estimated error rate tree 
additional parameters affect resulting decision tree weight specifies minimum number cases outcomes test 
default value meaning test accepted outcomes test cases 
prevents near trivial splits result flat really wide trees confidence level prediction tree error rates affects pruning process 
lower confidence level higher amount pruning take place attribute grouping create attribute value groups discrete attributes possibly infer patterns occurring sets cases different values attribute occur values attribute windowing enable construction multiple trees portion test data select best performing tree 
mpi collectives performance data collective algorithm performance information particular system extract information optimal methods 
optimal method particular system method achieves lowest duration particular set input parameters 
collected performance data described collective name communicator message size attributes 
collective name attribute discrete values broadcast reduce communicator message size attributes continuous values 
predefined set classes case contains methods optimal data points 
class names consist algorithm name segment size decision tree message size communicator size message size ring message size linear communicator size communicator size communicator size message size message size linear message size message size linear message size communicator size linear communicator size communicator size linear communicator size table decision tree nano cluster 
numbers parentheses leaves represent number training cases covered leaf number cases misclassified leaf 
example linear kb kb 
classes defined construction data input parameters belong single class 
far sufficient data requirement concerned performance measurement data contains considerable number data points communicator message size range 
cover single possible communicator message size training data set usually contains data points feel type problem collected data sufficient give reasonable results 
table shows simple decision tree constructed data collective nano cluster 
goal construction decision functions provide functionality generate decision function source code constructed decision trees internal nodes replaced corresponding statement leaf nodes return decision method index name 
rules program supplied software release purpose 
experimental results analysis release implementation quinlan construct decision trees existing performance data broadcast reduce collectives collected cluster university tennessee knoxville nano cluster located lawrence berkeley national laboratory 
cluster dual intel xeon tm processor nodes ghz fast ethernet mx interconnects 
experimental data cluster gathered fast ethernet interconnect 
nano cluster dual intel xeon tm processor nodes ghz pci express interconnect 
experiments tested decision trees constructed different weight confidence level constraints 
windowing data relatively sparse comparison complete communicator message size domain size expect benefit utilizing available data points 
communicator message sizes described continuous attributes able grouping functionality 
constructed decision trees collective just broadcast set collectives similar set available implementations 
linear binary pipeline algorithms expected similar decision functions broadcast reduce 
analysis broadcast decision trees shows different decision maps broadcast collective cluster 
considered different broadcast algorithms linear binomial binary splitted binary pipeline different segment sizes segmentation kb kb kb 
measurements covered communicator sizes processes message sizes kb range total data points 
original performance data set data points 
command pruning pruning line size errors size errors predicted error table broadcast decision tree statistics corresponding data 
size refers number leaf nodes tree 
errors terms misclassified training cases 
data set training cases 
upper left corner find exact decision map generated experimental data 
subsequent maps generated decision trees constructed decision map representation decision tree output particular communicator message sizes ranges 
details algorithms refer 
broadcast decision maps cluster measured 
different colors correspond different method indices 
trees generated specified command line parameters 
axis scale logarithmic 
bright red color represents linear algorithm segmentation shades green represent binomial segmentation kb segments dark blue binary algorithm kb segments gray splitted binary algorithm kb segments shades yellow pipeline algorithms kb kb segments 
specifying different values weight confidence level parameters 
statistics trees table 
exact decision map exhibits trends considerable amount information intermediate size messages kb kb small communicator sizes 
decision maps generated different trees capture general trends 
amount captured detail depends weight determines initial tree built confidence level affects tree pruning process 
heavier trees require branches contain cases limiting number fine grained splits 
lower confidence level allows aggressive pruning results coarser decisions 
looking decision tree statistics table see default tree leaves predicted misclassification error 
slightly heavier tree gives decrease tree size leaves maintains predicted misclassification error 
increase tree weight decrease confidence level produce tree leaves reduction size increase predicted misclassifications 
goal construct reasonably small decision trees provide run time performance mpi collective interest 
goal number misclassified training examples main merit need consider 
determine quality resulting tree terms collective operation performance consider performance penalty tree 
performance penalty relative difference performance obtained methods predicted decision tree experimentally optimal ones comm msg tx comm msg comm msg comm msg tx comm msg operation duration method predicted tree communicator message sizes comm msg comm msg operation duration experimentally optimal method 
mean performance penalty decision tree mean value performance penalties communicator message sizes interest 
command performance penalty decision tree line min max mean median table performance penalty broadcast decision trees corresponding data table 
table provides performance penalty statistics broadcast decision trees considering 
see minimum mean median performance penalty values low low indicating simplest tree considered provide run time performance 
simplest tree lower performance penalty indicates percent misclassified training cases translate directly performance penalty tree 
interesting consider case maximum performance penalty 
trees incur communicator size message size 
data point exact tree selects binary algorithm segmentation ms decision trees select binomial algorithm segmentation ms 
additionally tree data points performance penalty 
combined decision trees mpi collective operations grouped categories data exchange pattern scan 
reasonable expect similar collectives similar decision functions system 
decided analyze decision trees generated experimental data collected broadcast reduce collectives system 
implementations collectives symmetric linear binomial binary pipeline implementations 
broadcast supports splitted binary algorithm equivalent reduce implementation expected able handle cases correctly 
training data experiment contained attributes collective name communicator size message size set predetermined classes broadcast case 
combined broadcast reduce decision maps cluster reduce exact reduce reduce broadcast exact broadcast broadcast 
color meaning 
shows decision maps generated combined broadcast reduce decision tree 
left maps rows exact decisions collectives experimental data 
remaining maps generated querying combined decision tree 
figures generated decision tree leaves predicted misclassification error generated decision tree leaves predicted misclassification error 
table provides detailed information combined broadcast reduce trees considered 
mean performance penalty combined tree collectives 
command pruning pruning line size errors size errors predicted error table statistics combined broadcast reduce decision trees corresponding data 
size refers number leaf nodes tree 
errors terms misclassified training cases 
data set training cases 
mean performance penalty combined decision tree collectives 
tree index denotes decision tree 
structure combined broadcast reduce decision trees reveals test type collective occurs time third level tree 
subtrees somewhat similar structure see table 
considering message sizes range see broadcast reduce variant binomial algorithm 
switching points shifted larger communicator sizes smaller messages reduce non segmented version binomial algorithm 
message size message size collective broadcast message size binomial message size binary collective reduce message size binomial message size communicator size binomial communicator size binomial 
table segment combined broadcast reduce decision tree 
discussion studied applicability decision trees mpi collective algorithm method selection problem 
assumed system interest benchmarked detailed performance information exists available collective communication methods 
information focused investigating decision trees feasible way generate static decision functions 
publicly available implementation constructed decision trees existing performance data broadcast reduce collectives 
evaluated decision trees constructed different weight confidence level parameters 
results show decision trees generate reasonably small accurate decision function mean performance penalty existing performance data measurement error trees considered 
example broadcast decision tree leaves able achieve mean performance penalty 
tree points communicator message size ranges tested incur performance penalty 
similar results obtained reduce 
additionally combined experimental data reduce broadcast generate combined decision trees 
trees able produce decision functions relative performance penalty collectives 
indicates possible information mpi collective operation generate reasonably decision function collective 
findings demonstrate algorithm decision trees applicable problem widely domain 
plan decision trees reevaluate decision functions ft mpi tuned collective module open mpi integrate decision trees mpi collective testing performance measurement framework occ 
acknowledgments supported los alamos computer science institute funded rice university subcontract university subcontract 
automatic mpi counter profiling users results cray proceedings message passing interface developer user conference pp 

pipelining overlapping mpi collective operations th ieee conference local computer network bonn germany pp 
iee computer society october 
tr ff efficient reduction algorithms non number processors message passing parallel systems proceedings mpi lecture notes computer science springer verlag 
chan van de optimizing collective communication proceedings ieee international conference cluster computing pp 

thakur gropp optimization collective communication operations mpich international journal high performance computing applications vol 
pp 

bal mag pie mpi collective communication operations clustered wide area systems proceedings seventh acm sigplan symposium principles practice parallel programming pp 
acm press 
efficient implementation reduce scatter mpi journal systems vol 
pp 

fagg gabriel dongarra performance analysis mpi collective operations proceedings th ieee international parallel distributed processing symposium ipdps pds workshop washington dc usa ieee computer society 
quinlan programs machine learning 
san mateo california morgan kaufmann publishers 
fagg gabriel chen london dongarra extending mpi specification process fault tolerance high performance computing systems proceedings international supercomputer conference ics 
fagg dongarra tuned flexible high performance collective communication component developed open mpi proccedings th austrian hungarian workshop distributed parallel systems innsbruck austria september 
murthy automatic construction decision trees data multi disciplinary survey data mining knowledge discovery vol 
pp 

demmel bilmes statistical models empirical search performance tuning international journal high performance computing applications vol 
pp 

vapnik statistical learning theory 
new york ny wiley 
quinlan source code www com personal accesses september 

