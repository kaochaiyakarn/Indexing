
fast query optimized kernel machine classification incremental approximate nearest support vectors dennis decoste dennis decoste jpl nasa gov jet propulsion laboratory caltech oak grove drive pasadena ca usa dominic dominic 
jpl 
nasa gov jet propulsion laboratory caltech oak grove drive pasadena ca usa support vector machines ker nel machines offer robust modern machine learning methods nonlinear classification 
relative alternatives linear methods decision trees neu ral networks orders mag slower query time 
exist ing methods attempt speedup query time reduced set compression 
burges anytime bounding :10.1.1.54.9934
decoste propose new ef ficient approach treating ker nel machine classifier special form nearest neighbor :10.1.1.6.1038
approach improves traditional nn determining query time query pre query analysis guided nal robust kernel machine 
demonstrate effectiveness high dimensional benchmark mnist data observing greater fold reduction number svs required query amortized pairwise mnist digit classifiers extra test errors fact happens fewer 

kernel machine km methods support vector machines svms kernel fischer promising popular meth ods modern machine learning research scholkopf smola 
representations scale linearly number training examples im exploring large nonlinear kernelized feature spaces exponentially larger original input dimensionality kms elegantly practically overcome classic curse dimensionality 
tradeoff power km query time complexity scales linearly num ber training examples making kms orders magnitude expensive query time popular machine learning alternatives deci sion trees neural networks 
example svm achieved lowest error rates mnist lecun benchmark digit recognition task de coste scholkopf classifies slower previous best neural network due digit recognizers having support vectors 

goal proportionality difficulty despite significant theoretical advantages kms heavy query time costs serious obstacle kms practical method choice cially common case potential gains terms reduced test error relatively modest compared heavy cost 
exam ple atypical practice simple linear method achieve test accuracy km improving percentages 
improvements significant useful raise serious bang buck issues hin der kms widely deployed especially tasks involving huge data sets 
data min ing query time constraints embedded board resource limited spacecraft real time robots 
troubling km costs identical query easy ones alternatives 
de cision trees classify faster harder ones 
ideal approach proceedings twentieth international conference machine learning icml washington dc 
uses simple linear classifier major ity queries correctly classify incurs heavier query time cost exact km relatively rare queries preci sion matters uses 
approximate km complexity proportional difficulty query 

summary approach approach propose directly find approximation ideal 
empirical observation achieve classification exact km small fraction nearest support vectors svs query 
exact km output weighted sum kernel values query svs approximate nearest neighbor nn classifier output sums weighted kernel values selected svs 
query time gather statistics mis leading nn relative outputs exact km representative set examples possible total number svs 
statistics derive upper lower thresh olds step ic identifying output levels variant nn strongly positively negatively reversal sign weaker sv neighbors remaining 
query time incrementally update query partial output stopping soon exceeds current step predetermined statistical thresholds 
easy queries early stopping occur early step 
harder queries stopping occur nearly svs touched 
key empirical observation approach tolerate approximate nearest neighbor orderings 
specifically experiments project svs query top principal component dimen sions compute neighbor orderings sub space 
ensures overhead nearest neighbor computations insignificant relative exact km computation 

kernel machines summary section reviews key kernel machine terminology 
concreteness loss generality terms common case binary svms 
data matrix labels vector kernel function regularization scalar binary svm classifier trained optimizing weighting vector satisfy quadratic programming qp dual form minimize xj ai xi subject ai number training examples yi label positive example negative th dimensional training example xi 
kernel avoids curse dimensionality implicitly projecting dimensional example vectors input space xi xj feature space vectors xi xj returning dot product xi xj xi 
xj 
popular kernels parameters include linear polynomial rbf exp normalized ik 
output query example km trained weighting vector defined suit able scalar bias determined training simple dot product kernel feature space pi xi 
exact km output computed cp xi xi example binary svm support vectors svs pi classifies sign 

related lower query costs early methods reducing query time costs kms focussed optimization methods compress km svs reduced set burges burges scholkopf scholkopf scholkopf :10.1.1.54.9934
involves approximating nn optimizing zi yi norm defined 
ut indicates matrix transpose 
assume loss generality columns non zero pi 
minimizes approximation cost yielding nz nz zi zi small achieved nz costly global optimization significant fold speedups little loss classification accuracy reported 
burges :10.1.1.54.9934
linear kernels known km com presses error single dimensional 
general shortcoming reduced sets output requires exactly nz kernel computations 
proposes sequential approach stopping early query soon partial output drops zero indicating application non face 
key problem reduced set approaches provide guarantees control con classification error intro duced approximations 
decoste de coste develop sequential methods quickly compute upper lower bounds km put query potentially step ic :10.1.1.6.1038
classification tasks bounds allow confidently soon bounds query move side zero 
com putation bounds involves ic term due incomplete cholesky decomposition 
de spite working test data sets head bounding approach useless sort high dimensional image classification tasks examine empirical 

nearest support vectors nsv key intuition new approach proposed incremental computa tion km output query sv step partial km output strong positively negatively able completely reverse course change sign remaining pik xi terms added 
encourage happen steps possible maxi mum speedup query time intelligently efficiently order svs query largest terms tend get added 
enable know leaning strong safely early gather statistics representative data query time see strongly par tial output lean step stopping lead classification sign exact km output 
approach surprising views km output computation 
form weighted nearest neighbor voting weights kernel values reflect inverse distances 
inspiration small nearest neighbor classifiers classify best vary query query cially near discriminant border prompting consider harness robustness km guide determination query 
due relation nearest neighbors call approach nearest support vectors nsv 
nsv distance scoring defined xi 
shows positive query example digit vs binary classification task mnist data followed top nearest svs train ing set largest score 
partial km outputs gk steps sv ordering shown sv 
fac tors score shown line text sv pi followed kernel value query sv 
surprisingly nearest svs query class 
importantly pik xi terms corresponding score ordered svs tend fol low steady somewhat noisy steps ward step backward progression soon remaining terms small overcome strong 
example second considerable smaller pik xi 
encouraging exploiting phenomena key approach 
classify query soon pre query worst case estimates slow drop occur indicate strong leaning kernel values approximated nsv ordering ensure nominal time costs described section 
exact kernel values shown approximate kernel values computed needed partial outputs incrementally computed 
km bias term example ac counts partial output starting lower product sv exact kernel value 

reversed sign remaining lower scoring examined 
summarizes query time algorithm 
trades speedup versus fidelity likelihood sign sign choice upper hk lower lk thresholds section de 
inputs query svs xi weights pi bias statistical thresholds ilk 
output approximation 
sort xi pi large st pk lk hk 
pseudo code query time nsv classification 

statistical thresholds nsv derive thresholds lk hk running algo rithm large representative sample pre query data gathering statistics concerning partial outputs 
reasonable starting point include entire training set just svs sample 
section discuss refinements 
gk denote steps 
natural initial approach thresholding denoted simple com pute lk minimum value gk gk 
identifies lk worst case wrong way leaning sample exact km classifies positive 
similarly hk signed maximum gk gk 
query data similar training data simple thresholds suffice clas queries exact km faster 

example nearest svs 
practice test training data dis tributions identical 
intro duce second method denoted adjusting thresholds account local variance extrema gk step specifically replace hk lk maxi mum minimum threshold values adjacent steps ic experiments sec tion smoothing window 
analysis suggests conservative re quired avoid introducing test errors 
ex periments narrower windows giving slightly tighter thresholds yield addi tional speedups suggesting moderate window size relative svs prob ably usually prudent 
case assume practice appropriate method smoothing choice selected sort pre query cross validation process see works best task data set 

tug war balancing sv vs sv approach suffices provide query speedups introducing significant test er noticed speedups signifi cantly suboptimal 
sorting solely algorithm leads relatively wide skewed thresholds slight imbalance number positive svs set sv pi versus negative svs sv pi 
example svm constrains sum sv sv values smaller set proportionally larger making early small gk tend class fewer svs 
results thresholds skewed larger desired 
particular desire est strongest scoring effectively cancel unnecessary explicit touch queries 
skewed especially difficult achieve 
overcome problem replace simple nsv sort ordering call tug war 
adjusting score ordering cumulative sums positive tive step equal possible 
results orderings alternate top scoring positive negative svs ways especially steps large values smaller widely varying 

fast approximate nsv ordering metric vp trees yianilos spatial kd trees indexing methods employed avoid expensive full linear scans nearest neighbor search 
high dimensional data targeted find indexing methods practically useless 
exam ple distribution kernel distances degree polynomial kernel mnist data narrow instance triangular inequalities met ric trees prune neighbor candidates 
minimum non zero distance greater half maximum distance forcing full linear scans 
find simple approximate low dimensional embeddings better data 
specifically pre query principal com ponent analysis pca matrix svs vt 
dimensional embeddings svs pre query query query projection 
small dimensional vectors larger original dimensional ones quickly compute approximate kernels approximately order needed 
cost nsv ordering insignificant part cost 
example empirical re sults indicate denoted pca mnist images gives sufficiently accu rate nsv orderings query time projection overhead equivalent computing dot product kernels touching svs query 
useful note excessively approx kernels 
small pca embedding dimension threshold approach unsafe just require query time steps thresholds proportionally wider 
threshold smoothing method sume practice pre query cross validation select appropriate levels methods approxima tion 
wide assortment existing ap proximate nearest neighbors methods com approach worth consideration 
particular suspect promising fastmap faloutsos lin ap proximate kernel pca scholkopf ex fastmap ability quickly find low dimensional nonlinear embeddings 
im prove kernel approximations currently get ing linear pca really just approximates dot products approx dot products kernel function 

enhancements enhancements basic approach useful described 

linear methods initial filters directly attempt achieve speed linear methods find useful train pre query linear svm initial filter overhead equivalent touching just sv 
done computing upper lower simple thresholds linear svm output partial output step main nonlinear km output 
avoid outlier training examples dom thresholds linear svm find better compute high threshold mean lin ear svm output positive leaning negative exam ples plus standard deviations similarly low threshold mean output negative leaning positive examples minus standard derivations gives tighter bounds 
training set larger varied expected query set true mnist reasonable note mnist 
case mentioned earlier believe choices ul pre query search process 
reported section linear filtering typically boosts amortized speedups mnist tasks additional factor introducing new test errors 

better thresholds data generation main concern approach potentially introduce large numbers test er respect exact km classifications queries fall statistical ex identified representative sample training set compute thresholds 
address concern begun considering methods generate additional data reasons believe current thresholds insufficient plausibly occur actual query sets 
promising method involves generating data falls convex hull training data plausibility occurs close large cluster svs class side discriminant border challenging sufficiency current thresholds 
tasks explored far ultimately required level care omit details note important area ensuring wider safer applicability nsv approach 
suspect query time checks running exact km machine say ran dom large query set comparing early stopping nsv method provide reasonable diagnostics detecting practice current set thresholds priate query set 
better checks probability proportional dissimilarity query training data 
short issue training test data distri butions significantly different classic known issue machine learning approaches nsv provide additional motivations contexts seriously explore issue 

nearest reduced set neighbors reduced sets summarized section place svs promising direction improve nsv 
principal compression speedups reduced sets largely disjoint nsv speedups ordering svs weighted similarity query 
initial attempts exploit reduced sets speedup nsv 
reduced sets involve costly global current simple greedy methods yield vectors ideally kernel values reduced set vectors near rarely 
sus nearly orthogonal reduced set vectors avoid key limitation current approach clusters nearly identical svs touched nsv near query 
fortunately mnist experiments common reflected narrow distri bution kernel distances nearest neighbor indexing methods degrade domain highly nonlinear kernels mentioned earlier 

experiments reproducibility comparison report pairwise digit classification tasks known benchmark mnist data set lecun 
data high input dimensionality large numbers svs typical sort challenging tasks approach intended help 
table details pairwise mnist cases 
rows labeled summarize input dimension number positive negative training examples 
rows summarize trained svms case 
row notes number test examples mnist total digit classes 
row indicates test errors exact svm classifica tion row similarly reports tests errors exact linear svm generally worse 
row indicates nsv method dis agreed classifications exact svm cases 
row illustrates linear filtering typically pruned test queries 
rows shows statistics steps required nsv query 
row computes speedup relative exact classification 
rows similarly report linear filtering indicating filtering improves factors 
rows similarly reports test examples linear filtering 
speedup generally significant case test digits pairwise class tend get small partial ex act km outputs seldom lean strongly early current nsv ap proach exploit 
suggests research required nsv hope achieve sort large speedups multi class classification platt enjoy binary ones 
plots queries vs case linear filtering exact svm puts axis steps query partial output gk exceeds thresholds axis 
illustrates proportionality diffi culty queries requiring largest tend 
solid lines thresholds 
note queries plot enable experiments trained svms digits case similar sized digit class sets training digits 
known verified fixed nn nearly svms mnist 
nsv performance matched simpler nn 
linear filter nsv disagrees filter skips min max mean median speedup linear filter nsv disagrees min max mean median speedup test data filter skips min max mean median speedup mnist experi vs ents 
vs 
vi ll solid lines indicating gk outside lk hk 


wo 
example proportionality difficulty vs 
initially experimented just cases vs hard vs easy develop method fix design choices window size lo sufficiency pca fast nsv ordering 
minimizes concerns overfitting design choices data enabling test robustness method running experiments pairwise classifiers linear filtering test queries pairwise classes 
speedup results summarized table mean speedup query cases 
table shows large speedup achieved disagreements 
investigation showed disagreements vs wash nsv correct wrong 
disagreements nsv correct re nsv achieving test errors exact svms 
claim expect usually happen encouraging especially best speedups achieved related reduced set typically came small significant price fold test errors burges scholkopf 
result consequence fact reduced set methods nsv approach tunes ag assumption available training data representative test data 
expect reduced set approaches better assumption hold easily detected query time 
table 
pairwise mnist classifiers 
lower sv counts 
upper amortized speedups queries 
table 
number test errors svm 
number nsv classification disagreements svm 

implementation issues common dot product kernels high cost query point svs domi computation exact km outputs 
classi fying large numbers dot prod computed optimized matrix multiply code cache efficient blas dgemm atlas note reduced set results directly fully compared new results various reasons including performed full way classification vs rest pairwise binary classifications explored 
whaley dongarra faster depending platform naive matrix multiply 
nsv approach greatly reduces number svs query point dot average order svs unique query point difficult efficient atlas 
projecting query point pca sorting nsv orderings adds overhead 
result best speedups measured actual clock time achieved far impressive nsv potential 
modern cache prefetch instructions carefully blocked computations suspect existing inefficiencies avoidable 
note fairness worst case nsv speedups may necessarily current memory cache hardware ideal factor exact km computations 

discussion proposed method essentially treats kernel machine query time improved nearest neighbor method 
empirical indicates tolerate quite approximate fast nearest neighbor computations effectively auto matically pick appropriate query time query guided pre query analysis km formance representative data large train set 
note approach applies form km classifier regardless training method kfd mika mpm lanckriet including linear quadratic pro gramming approaches 
report promising exciting speedups ob served date including lll fold average speedup pairwise mnist digit classification tasks offering compelling existence proof idea merit challenging tasks 
deeper understanding practical theoretical limitations tradeoffs thresholds introducing additional test errors re quired 
believe particularly promising direction address central issue involves finding recursive partitionings input space different thresholds appropriate 
involve applying variant nearest neighbors method leaves sort decision tree struc ture 
motivates positions wards study combine distinct im portant threads machine learning methods kernel machines nearest neighbors decision trees 
acknowledgments research carried jet propulsion laboratory california institute technology contract national aeronautics space ad 
burges 

simplified support vector decision rules 
intl 
conf 
machine learning icml 
burges scholkopf 

improving accuracy speed support vector machines 
nips 
decoste 

anytime interval valued outputs kernel machines fast support vector machine classifica tion distance geometry 
proceedings icml 
decoste 

anytime query tuned kernel machines cholesky factorization 
proceedings siam inter national conference data mining 
decoste scholkopf 

training invariant support vector machines 
machine learning 
faloutsos lin 

fastmap fast gorithm indexing data mining visualization traditional multimedia datasets 
acm sigmod intl 
conf 
management data 
lanckriet ghaoui bhattacharyya jordan 

minmax probability machine 
advances neural information processing systems nips 
lecun 

mnist handwritten digits dataset 
www research att com yann ocr mnist 
mika ratsch muller 

mathe programming approach kernel fisher gorithm 
nips 
platt 

large margin dags multiclass classifi cation 
nips 
torr scholkopf blake 

computationally efficient face detection 
intl 
conf 
computer vision iccv 
scholkopf smola burges 

fast approximation support vector kernel expansions interpretation clustering approximation feature spaces 

dagm symposium 
springer 
scholkopf mika burges muller smola 

input space vs feature space kernel methods 
ieee tions neural networks 
scholkopf smola 

learning kernels 
cambridge ma mit press 
whaley dongarra 

automatically tuned linear algebra software 
tr ut 
yianilos 

excluded middle vantage point forests nearest neighbor search technical report 
nec research institute princeton nj 
