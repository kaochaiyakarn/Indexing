mohammad ed spie proceedings bayesian inference inverse problems spie vol 
san diego july pp 

author information email knuth email arc nasa gov bayesian source separation localization kevin knuth dept neuroscience albert einstein college medicine ny dept speech hearing sciences cuny graduate center ny ny problem mixed signals occurs different contexts familiar acoustics 
forward problem acoustics consists finding sound pressure levels various detectors resulting sound signals emanating active acoustic sources 
inverse problem consists sound recorded detectors separate signals recover original source waveforms 
general inverse problem unsolvable additional information 
general problem called source separation techniques developed utilize maximum entropy minimum mutual information maximum likelihood 
previous demonstrated techniques recast bayesian framework 
demonstrates power bayesian approach provides natural means incorporating prior information source model 
algorithm developed utilizes information regarding statistics amplitudes signals emitted sources relative locations detectors 
prior information algorithm finds probable source behavior configuration 
inverse problem solved simultaneously performing source separation localization 
noted algorithm designed account delay times important acoustic source separation 
possible application algorithm separation electrophysiological signals obtained eeg meg 
keywords bayesian analysis blind source separation independent component analysis ica eeg meg 
familiar example source separation occurs context listener attempting pay attention speaker environment filled sound sources 
problem known cocktail party problem know brain typically excellent job focusing particular sound variety conditions 
important source separation problem appears context experimenter recording electromagnetic signals emitted particular neural source human brain brain processes producing additional signals 
typically problems sufficient information deduce source behavior 
required rely procedure inference 
inference depend additional information available 
person listening speaker visual information obtained speaker mouth movements helpful 
artificial sound separation algorithms typically rely prior information regarding statistics amplitudes particular sound signals 
contrast source localization techniques typically utilize prior information possible locations sources respect detectors nature propagation signals sources detectors 
demonstrate inference problems handled quite generally bayesian formulation types prior information included aid solution problem 
section discuss bayesian formalism treat problems general 
specific example bell sejnowski independent component analysis ica algorithm uses prior information signal statistics derived section 
section demonstrate bayesian formulation allows incorporate specific prior information source geometry mixing properties 
bayesian separation localization bsl algorithm demonstrated section compared bell sejnowski ica algorithm 
section demonstrate obtain desired source model parameters signals separated 

application bayesian techniques separation problem typical separation problem consists elements set sources emitting signals form medium signals travel set detectors medium record mixtures signals 
generally assumed independent sources emitting signals sn observes equal number independent mixtures xn mixing assumed linear instantaneous 
linear mixing operation written compact form 
noted assumptions regarding instantaneous mixing fact equal number sources detectors square necessary applying bayesian formalism greatly simplify mathematics 
difficulty problem related fact matrix mixing matrix source signals known 
problem inherently inference problem unsolvable inclusion additional prior information 
solve problem choose source model describes variables interest 
model simply unknowns equation additional parameters mixing matrix source signals depend 
bayes theorem allows write probability particular source model correct terms likelihood data additional prior probabilities represents prior information problem 
typically properties propagation signals medium depend source signals magnitudes simplify prior probability 
probability model degree believe correct proportional product likelihood data model product prior probabilities mixing matrix source signals prior information 
view equation describing acquisition new information changes believe model 
relationship data mixing matrix source signals equation necessary solve 
fact dimensions smaller find probability treating nuisance parameter marginalizing giving ds 
point assumptions specific problem included prior information 
likelihood term easily handled assumed mixtures linear instantaneous 
prior term describes prior knowledge form mixing matrix 
knowledge includes information propagation signals medium known transfer function forward problem information geometry sources detectors 
typically type information electromagnetic source location techniques 
second prior term describes knowledge form source signals 
prior typically focus blind source separation techniques 

blind source separation briefly outline derivation bell sejnowski ica algorithm probability model described equation 
similar derivations 
equation assumptions 
assume problem blind source separation know mixing process minimal knowledge regarding source signals 
lack knowledge mixing process reflected ignorance form mixing matrix values particular matrix elements aij 
assigning uniform prior probability expresses ignorance ds 
second assumption mixing noiseless linear instantaneous described equation reflected assignment delta function likelihood 
third assumption source signals statistically independent reflected factorizing prior product priors independent source 
account assignments einstein summation convention denote matrix multiplication rewrite equation ik ds pl sl represents prior probability amplitudes source change variables wi xi aik sk delta function allows evaluate multi dimensional integral lk 
det derived formula normalization factor probability matrix correct mixing matrix problem 
matter formula search probable mixing matrix mind look logarithm probability log log det log pl alk xk logarithm normalization factor implicit equation :10.1.1.48.120:10.1.1.48.120
perform separation interested inverse mixing matrix separation matrix searching mixing matrix search separation matrix maximizes probability correct mixing matrix 
discussion arbitrariness blind source separation case previous 
rewriting equation terms get log det log log :10.1.1.48.120
find maximum logarithm posterior probability respect variation matrix take derivative equation respect matrix elements wij ij log lk log det log pl ul wij aji log pl xk wij aji log pl ul wij um ji ij log ij log ji log im um ui ui aji pi introduced ui wij xk recognize ratio probability density ui derivative column vector :10.1.1.31.9597
implementation bell sejnowski ica algorithm consists writing equation matrix form data perform stochastic gradient ascent find separation matrix maximizes equation 

incorporating source geometry signal propagation information noted previous section blind source separation deals problems lack prior information mixing process 
physical applications case 
demonstrate bayesian formalism provides natural consistent means incorporate prior knowledge mixing process 
prior knowledge particular mixing process expressed prior probability term equation 
purpose demonstration choose simple non physical mixing process aspects acoustic mixing problem mixing problem 
assume sources spherically intensity dropping inverse square distance detector source 
addition assume time delay signal reach detector mixing instantaneous 
note radiation pattern approximation sound sources lack delay non physical 
lack delay appropriate sources radiation pattern appropriate neural sources produce potentials fields drop inverse cube distance source detector exhibit directional dependence 
assumptions regarding mixing process expect element mixing matrix written form aij rij rij distance source detector amplitude source 
amplitude included source model typically equation explicitly include amplitude source hyperparameter 
express prior probability matrix terms prior probabilities elements 
easiest way accomplish assume elements independent 
completely accurate knowledge distance source detector provides information distance source detector detector positions relative known 
prior probability expressed aij prior probability mixing matrix ij prior probability matrix element 
essential point examine model separation problem decide aspects focus attention 
model consists set sources source behavior sj compactly written vector time series amplitudes explicitly described set addition sources positions space denoted set distances relative detector positions di rij 
equation parameters combined form mixing matrix describes linear mixing equation 
may choose focus attention matrix inverse 
described previous section develop algorithm needs write probability model parameters interest conditional data prior information 
mixing signals primary interest may opt 
localization sources primary interest 
case ends performing search parameters affect solution linearly nonlinearly 
possible linear problem results prior information estimate source positions amplitudes probable matrix follow course 
prior knowledge usually consists information regarding source geometry source amplitude need relate prior probabilities elements mixing matrix prior probabilities source location amplitude 
easily performed rewriting joint probability model ij ij ij ij ij marginalizing source amplitude distance source detector da rij aij rij ij ij 
integrals go zero infinity source amplitudes distances sources detectors positive 
term aij rij assigned delta function derived belief equation accurately describes signal propagation noise negligible integral rij evaluated resulting aij da rij aij ij da ij ij ij rij 
prior probability integral prior probability source amplitude second prior aij probability source distance detector rij treatment assign uniform distribution source amplitude prior smallest largest possible values quantify knowledge distances sources detectors need take account knowledge relative positions sources detectors 
information mean variance source positions principle maximum entropy dictates gaussian prior describe knowledge 
need prior probability representing prior information regarding distance source detector source position 
similar procedure equations obtain maxent prior ij ij ij di ij di exp sinh dv represents position detector sv represents mean believed position source sj represents variance mean position 
simplify final algorithm choose similar prior allow easily integrate equation 
choose gamma prior ij rij ij variance sj mean distance detector source di aside advantage allowing obtain analytical solution gamma prior disallows negative values distance form similar maxent prior equation 
substituting priors integral get ij ij ij ij aij aij da ij ij aij understood refer source ij ij derived mean distance source detector variance source positions 
carrying final integration obtain ij ij ij ij ij ij aij ij aij incomplete gamma function 
conjunction equation provides prior probability density mixing matrix takes account knowledge amplitude sources geometry sources detectors nature signal propagation 
derived prior mixing matrix equation 
derivation bell sejnowski ica algorithm compute derivative logarithm probability ij log log det log pl xk log akl ij pi ui ji pi ui amn wij amn log akl concentrating derivative term amn amn log akl amn mn mn mn mn amn mn mn mn mn mn amn mn mn amn log akl amn mn mn mn mn amn mn mn mn mn mn mn mn mn amn mn mn mn 
applying formula matrix elements amn obtain matrix shall denote fact derivative mixing matrix respect inverse separation matrix express equation ij mn ij mn ij mi jn ui ami mn pi log aji pi einstein summation convention matrix element equation 
equation written matrix form ui pi log ma 
bell sejnowski algorithm easily implement stochastic gradient search algorithm find separation matrix satisfies maximum posteriori criterion 
initial guess separation matrix updated setting equal equation 
noted equation covariant object matrix derivative scalar respect matrix 
equation covariant transforming gradient appropriate metric 
shown accomplished post multiplying results final form stochastic gradient update term ui pi mw update rule stochastic gradient algorithm implemented learning rate obtained applying equation sample data 

demonstration separation algorithm separation algorithm applicable researcher prior information belief regarding positions sources amplitudes 
information form mean source position sv source associated variance mean sj mean prior source position determine mean distance th detector th source ij di parameters gamma prior representing knowledge ij distances detectors sources equation ij ij cutoffs ij 
displayed original source waveforms artificially mixed waveforms results ica results bsl 
top bottom original waveforms hail whistle photon noisy frequency glide spoken phrases live long captain re losing power warp engines dead jim 
mixtures uniform randomly mixed signals due inverse square law propagation 
ica accurately separates speech sounds separate hail photon 
result diagonal solution top waveform sum original signals second difference 
additional information bsl allows separation sounds inaccurate source amplitude density hail photon causes small amount mixing speech sounds 
amplitudes sources chosen minimum possible amplitude source maximum possible amplitude source measured units characteristic width prior source amplitude density 
final prior assign prior density source amplitudes 
speech sounds typically exhibit amplitude histograms unimodal hyper gaussian 
shown sigmoid function effective source amplitude prior distribution speech sounds hyper gaussian probability density results column vector equation having elements dg pi ui ui ui 
du ui ui 
priors explicitly defined stochastic gradient ascent performed find separation matrix applying algorithm described equations 
demonstrate bsl algorithm set artificially mixed signals 
source signals derived tv show star trek 
source waveforms speech hail similar tone whistle photon blast noisy frequency glide 
detectors sources randomly placed cubic volume side length 
source signals artificially mixed mixing matrix derived equation 
variances prior source positions chosen randomly uniform distribution ranging 
prior source positions randomly chosen normal distribution correct position mean previously determined variance 
cutoffs uniform amplitude distributions estimated amplitudes units characteristic width sigmoid function 
compare results obtained separation hail photon sounds ica bsl 
previously described ica accurately separate hail photon due fact source amplitude prior sharply peaked hail photon amplitude histograms broad 
lack accurate information results diagonal solution separated signals consist sum difference re scaled versions original source waveforms 
contrast bsl able information regarding source locations way signals propagate detectors offset inaccurate source amplitude information 
inaccurate source amplitude density information causes problems slightly mixing speech sounds 
show amplitude histograms hail photon ica diagonal solutions spoken phrase live long 
note amplitude histograms speech sound diagonal solution sharply peaked hyper gaussian 
hail photon hand broader amplitude densities 
mixing hail photon sounds form diagonal solution hyper gaussian density attained closely matches sigmoid prior 

estimation source parameters section briefly demonstrate source parameters source position estimated optimal mixing matrix 
basic idea bayes theorem marginalization write probability estimated source positions 
displayed sample waveforms amplitude histograms 
note hail photon source amplitude densities broad diagonal solution speech hyper gaussian 
da aj represents th column mixing matrix assumed earlier matrix elements aij independent express likelihood term equation product likelihoods element th column 
information regarding precision estimate mixing matrix assign gaussian density represent likelihoods equation 
variance represents precision estimated looking second derivative logarithm posterior probability equation 
assignment gaussian prior source position prior appropriate source mean position variance known 
equation describes uniform prior assigned source amplitudes 
probabilities equation evaluate probability position correct position source da aij exp da exp aij ij ij ij sj represents variance mean prior source position sij represents variance estimated value element aij mixing matrix 
implement search efficient find maximum logarithm equation 

graphs show probability position hail source calculated equation 
believed variance mixing matrix elements set variance set 
notice believe mixing matrix imprecise posterior probability dominated gaussian prior probability 
believe mixing matrix elements precise observe probability maximum distribution increased significantly 
addition locations near detectors excluded possible sites source 
locating source positions experiment plot posterior probability source position 
allows visualize position source uncertainty solution 
consider experiment involved mixture hail photon sounds 
source space dimensional detectors axial symmetry 
constraint source positions lie xy plane 
detectors located axis 
hail source located photon 
variance believed source positions chosen 
uniform amplitude prior extended photon hail 
true amplitude photon estimated hail 
algorithm separated signals percent error hail photon 
plot probability position hail source 
purposes illustration estimated variance values mixing matrix calculate probabilities 
shows posterior probability position hail source believe values separation matrix elements imprecise 
note distribution dominated gaussian source position prior additional information regarding mixing signals deemed inaccurate 
repeat calculation time assuming mixing matrix elements accurate 
additional information describing way signals mixed significantly modifies posterior probability 
probability density describing possible position source precisely localized greater magnitude 

summary demonstrated bayesian methodology provides natural logically consistent means prior information incorporated specific source separation problem 
artificial mixing problem exhibits features acoustic separation problems demonstrate methodology 
specific algorithm derived probably particularly useful intention demonstrate specific separation algorithm constructed 
algorithm demonstrated set sound signals artificially mixed randomly choosing source detector locations space demanding signal amplitude falls inverse square distance source 
current bsl algorithm ica separate signals 
algorithms assumptions regarding source amplitude densities bsl algorithm incorporated additional information regarding nature signal propagation amplitude signals believed positions sources 
additional information allowed bsl separate hail photon sounds separable conditions ica quality separated speech sounds suffered slightly 
important point incorporation additional information substitute improvement inaccurate information 
separation sounds hail photon better precise accurate source amplitude density information include additional information regarding source detector positions nature signal propagation 
hyper gaussian source priors shown useful describing knowledge amplitude densities speech signals shown inaccurate describing slowly varying waveforms recorded eeg meg 
slowly varying waveforms source amplitude densities multi modal form require hyperparameters model 
case signals amplitude densities difficult model advantageous incorporate additional information possible 
case additional information include possible source locations orientations nature signal propagation information nature signals 
true bayesian statistics simple information tremendous impact ability infer solution 
success ica excellent example situation 
critical researchers working specific source separation problems evaluate importance effect piece additional information 
bayesian solutions tendency difficult mainly due need marginalize complicated priors 
exclusion information particularly helpful solution may important inclusion overlooked details relevant problem 
conclude discussing important points regarding specific bsl algorithm derived 
assuming independence elements mixing matrix equation ignore symmetries imposed geometrical configuration detectors 
inclusion information may dramatically improve performance algorithm 
second access information mean positions sources respective variances assign maxent prior equation represent prior probability distances sources detectors 
assigning gamma prior maxent prior unknown bias introduced problem 
comparison performance priors performed maxent prior may require significantly numerical computations iteration step 
bsl algorithm developed designed find probable mixing matrix 
noted probable mixing matrix correspond probable separation matrix probable source locations 
due way probability densities transform change variables 
excellent example occurs context radiation physics 
wavelength light radiation greatest energy density correspond frequency light greatest energy density 

author irv hochberg herb vaughan advice support 
supported part albert einstein college medicine city university new york graduate center nih dc 


cherry experiments recognition speech ears acoust 
soc 
am 
pp 


bell sejnowski information maximization approach blind separation blind deconvolution neural comp 
pp 


knuth vaughan jr bayesian origin blind source separation electromagnetic source estimation published maximum entropy bayesian methods munich 


cardoso infomax maximum likelihood source separation ieee lectures signal processing pp 


comon independent component analysis new concept signal processing pp 


yang amari adaptive line learning algorithms blind separation maximum entropy minimum mutual information neural comp 
pp 


pearlmutter parra context sensitive generalization ica international conference neural information processing hong kong 

mackay maximum likelihood covariant algorithms independent component analysis draft wol ra phy cam ac uk mackay 

knuth difficulties applying blind source separation techniques eeg meg published maximum entropy bayesian methods 

abramowitz stegun handbook mathematical functions dover publications ny 

amari natural gradient works efficiently learning neural comp 
pp 

