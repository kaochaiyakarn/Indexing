chunk parsing revisited jun ichi tsujii crest jst japan science technology department computer science university tokyo school informatics university manchester tokyo ac jp tsujii chunk parsing conceptually appealing performance satisfactory practical 
show chunk parsing perform significantly better previously reported simple method maximum entropy classifiers phrase recognition level chunking 
experimental results penn treebank corpus show chunk parser give high precision parsing outputs high speed msec sentence 
parsing method searching best parse considering probabilities output maximum entropy classifiers show search method improve parsing accuracy 
chunk parsing tjong kim sang brants simple parsing strategy implementation concept 
parser performs chunking identifying base phrases convert identified phrases non terminal symbols 
parser performs chunking updated sequence convert newly recognized phrases non terminal symbols 
parser repeats procedure phrases chunked 
finishing chunking processes reconstruct complete parse tree sentence chunking results 
conceptual simplicity chunk parsing appealing satisfactory performance practical achieved parsing strategy 
sang achieved score penn treebank iob tagging method level chunking tjong kim sang 
large gap performance widely practical parsers charniak collins 
performance chunk parsing heavily dependent performance phrase recognition level chunking 
show chunk parsing strategy appealing give considerably better performance previously reported different approach phrase recognition enables build fast parser gives high precision outputs 
advantage open possibility full parsers large scale information extraction web corpus real time information extraction system needs analyze documents provided users run time 
organized follows 
section introduces chunk parsing strategy employed 
section describes method identifying chunks 
filtering methods reduce computational cost sections 
section explains maximum entropy classifier feature set 
section describes methods searching best parse 
experimental results penn treebank corpus section 
section offers concluding remarks 
np qp vbn nn vbd dt jj cd cd nns estimated volume light chunk parsing st iteration 
np np vbd dt jj qp nns volume light chunk parsing nd iteration 
chunk parsing strategy chunk parsing follow method proposed sang tjong kim sang 
figures show example chunk parsing 
iteration chunker identifies base phrases np estimated volume qp replaces phrase nonterminal symbol head 
head word identified head percolation table magerman 
second iteration chunker identifies np light converts phrase np 
chunking procedure repeated sentence chunked fourth iteration full parse tree easily recovered chunking history 
parsing strategy converts problem full parsing smaller simpler problems chunking need recognize flat structures base phrases 
sang iob tagging method proposed ramshaw marcus memory learning level chunking achieved score penn treebank corpus 
chunking sliding window approach performance chunk parsing heavily depends performance level chunking 
popular approach shallow parsing convert problem tagging task variety vp np vbd np volume chunk parsing rd iteration 
np vp volume chunk parsing th iteration 
machine learning techniques developed sequence labeling problems hidden markov models sequential classification svms kudo matsumoto conditional random fields sha pereira 
claims convert chunking problem tagging task 
classical sliding window method chunking consider subsequences phrase candidates classify machine learning algorithm 
suppose example perform chunking sequence 
np volume vbd 
consider sub sequences phrase candidates level chunking 

np volume vbd 

np volume vbd 

np volume vbd 

np volume vbd 

np volume vbd 

np volume vbd 
merit sliding window approach richer set features recognizing phrase sequential labeling approach 
define arbitrary features target candidate sequence nonterminal symbols target surrounding context general available sequential labeling approaches 
mention modeling methods sequence labeling allow define arbitrary features target phrase 
semi markov conditional random fields semi crfs modeling methods sarawagi cohen 
semi crfs give better performance sliding window approach incorporate features phrase candidates level chunking 
require additional computational resources training parsing semi crfs left 
biggest disadvantage sliding window approach cost training parsing 
phrase candidates length sequence naive application machine learning easily leads prohibitive consumption memory time 
order reduce number phrase candidates considered machine learning introduce filtering phases training parsing 
done rule dictionary 
done naive bayes classifier 
filtering cfg rule dictionary idea similar method proposed ratnaparkhi ratnaparkhi partof speech tagging 
tag dictionary tagger considers tag word pairs appear training sentences candidate tags 
similar method reducing number phrase candidates 
construct rule dictionary consisting cfg rules training data 
training parsing filter sub sequences match entry dictionary 
normalization rules training data cover rules unseen sentences 
take naive filtering method rule dictionary size rule dictionary original symbol normalized symbol nnp nns prp nn rbr rbs rb jjr jjs prp jj vbd vbz vbp null table normalizing 
original normalized number sentences number sentences vs size rule dictionary 
substantially lose recall parsing unseen data 
alleviate problem coverage rules conduct normalization rules 
convert symbols equivalent sets conversion table provided table 
conversion reduces sparseness rules 
normalize right hand side rhs rules heuristics 
cc converted 
converted 
shows effectiveness normalization method 
illustrates number rules increases rule dictionary add training sentences 
normalization number rules continues grow rapidly entire training set read 
normalization methods reduce growing rate considerably alleviates sparseness problem problems unknown rules 
filtering naive bayes classifier rule dictionary significantly reduced number phrase candidates difficult train parser entire training set rich set features 
reduce cost required training parsing propose naive bayes classifier filtering candidates 
naive bayes classifier simple requires little storage computational cost 
construct binary naive bayes classifier phrase type entire training data 
considered information features 
right hand side rhs cfg rule left adjacent nonterminal symbol 
right adjacent nonterminal symbol 
assuming conditional independence features compute probability filtering follows binary output indicating candidate phrase target type rhs cfg rule symbol left symbol right 
laplace smoothing method computing probability 
note information result rule application lhs symbol considered filtering scheme different naive bayes classifiers different lhs symbols phrase types 
table shows filtering performance training sections penn treebank 
set threshold probability filtering experiments reported 
naive bayes classifiers effectively reduced number candidates little positive samples wrongly filtered 
phrase recognition maximum entropy classifier candidates filtered phases perform classification maximum entropy classifiers berger 
construct binary classifier type phrases entire training set 
training samples maximum entropy consist phrase candidates filtered cfg rule dictionary naive bayes classifier 
merits maximum entropy classifier obtain probability classifier decision 
probability decision represents candidate correct chunk 
accept chunk probability larger predefined threshold 
thresholding scheme control trade precision recall changing threshold value 
regularization important maximum entropy modeling avoid overfitting training data 
purpose maximum entropy modeling inequality constraints tsujii 
modeling parameter tune gaussian prior modeling 
parameter called width factor 
set parameter experiments 
numerical optimization limited memory variable metric algorithm benson mor 
features table lists features phrase recognition maximum entropy classifier 
information adjacent non terminal symbols important 
unigrams bigrams trigrams adjacent symbols 
head information useful 
unigrams bigrams neighboring heads 
rhs cfg rule informative 
features rhss combined symbol features 
searching best parse deterministic parsing deterministic version chunk parsing straight forward 
need repeat chunking phrases chunked 
symbol candidates remaining candidates positives false negative adjp advp np pp vp table effectiveness naive bayes filtering representative nonterminals 
symbol unigrams symbol bigrams symbol trigrams head unigrams head bigrams symbol head unigrams cfg rule cfg rule symbol unigram cfg rule symbol bigram table feature templates chunking 
represent non terminal symbols target phrase respectively 
represent head target phrase respectively 
rhs represents right hand side cfg rule 
maximum entropy classifiers give contradictory chunks level chunking choose chunk larger probability ones 
parsing search tried perform searching chunk parsing order investigate extra effort searching gives gain parsing performance 
problem modeling chunk parsing provides explicit probabilistic distribution entire parse tree decisive way properly evaluate correctness parse 
consider score parse tree 
probability phrase maximum entropy classifier 
exploring possibilities chunking requires prohibitive computational cost reduce search space focusing uncertain chunk candidates search 
level chunking chunker provides chunks probabilities 
consider chunks probabilities predefined margin words chunks probabilities larger considered assured chunks fixed generate alternative hypotheses chunking 
chunks probabilities smaller simply ignored 
generate alternative hypotheses level chunking search best parse depthfirst manner 
iterative parsing tried iterative parsing strategy successfully probabilistic hpsg parsing 
parsing strategy simple 
parser starts low margin tries find successful parse 
parser find successful parse increases margin certain step tries parse wider margin 
experiments ran parsing experiments penn treebank corpus widely evaluating parsing algorithms 
training set consists sections 
section development data tuned feature set parameters parsing 
test set consists section report performance parser set 
evalb script provided sekine collins evaluating labeled recall precision lr lp parser outputs experiments carried server having ghz amd opteron cpu gb memory 
speed accuracy show performance achieved deterministic parsing 
table shows results 
parsed sentences section part speech pos tags 
trade precision recall controlled changing threshold recognizing chunks 
fifth row gives performance achieved default threshold precision recall low 
lowering threshold improve recall loss precision 
best score 
parsing speed high 
parser takes seconds parse entire section 
section contains sentences average time required parsing sentence msec 
parsing speed slightly dropped lower threshold 
table shows performance achieved search algorithm described section 
limited maximum number nodes search space increase nodes shown little improvement parsing accuracy 
search algorithm significantly boosted precisions recalls achieved score margin 
noted obtain gain tight margin 
need consider phrases having low probabilities order search 
parameter file collins prm threshold lr lp score time sec table parsing performance section sentences gold standard pos tags deterministic algorithm 
margin lr lp score time sec table parsing performance section sentences gold standard pos tags search algorithm 
advantages chunk parser parsing speed 
comparison show tradeoff parsing time performance collins parser collins chunk parser 
collins parser allows user change size beam parsing 
model gave better performance model beam size smaller 
chunk parser controlled trade changing maximum number nodes search 
uncertainty margin chunk recognition 
shows collins parser clearly outperforms chunk parser beam size large 
performance significantly drops smaller beam size 
break point sec msec sentence 
comparison previous table summarizes parsing performance section results previous studies 
order results directly comparable produced pos tags input parsers pos tagger tsujii trained sections wsj corpus 
table shows performance achieved score chunk parser collins parser time sec time vs score section 
xaxis represents time required parse entire section 
time required making hash table collins parser excluded 
lr lp score ratnaparkhi collins charniak kudo sang deterministic tagger poss deterministic gold poss search tagger poss search gold poss iterative search tagger poss iterative search gold poss table comparison 
parsing performance section sentences 
iterative parsing method section 
chunk parser achieved score deterministic parsing methods pos tagger tags 
score better achieved previous study chunk parsing points tjong kim sang 
search algorithms gave additional point improvement 
iterative parsing method achieved 
chunk parser showed considerably better performance previous study chunk parsing performance significantly lower achieved state art parsers 
discussion number possible improvements chunk parser 
rule dictionary reduce cost required training parsing 
rule dictionary parser fail identify correct phrase phrase contained rule dictionary 
applied normalization techniques order alleviate problem completely solved problem 
indicates face unknown rules constructed rule dictionary training data note dotted line saturate 
additional feature sets maximum entropy classifiers improve performance 
bottom parsing strategy allows information sub trees constructed 
need restrict head information partial parses 
researchers reported information partial parse trees plays important role achieving high performance bod collins duffy kudo expect additional features improve performance chunk parsing 
methods searching best parse sections room improvement 
search method device avoid repetitive computations nonterminal sequence parsing 
chart structure effectively stores partial parse results enable parser explore broader search space produce better parses 
chunk parser exhibited considerable improvement parsing accuracy previous study chunk parsing 
reason completely clear 
believe sliding window approach enabled exploit richer set features called iob approach main better performance 
combination iob approach state art machine learning algorithm support vector machines produce similar level performance 
preliminary experiments iob tagging method maximum entropy markov models achieved comparable performance sliding window method 
shown chunk parsing perform significantly better previously reported simple sliding window method maximum entropy classifiers level chunking 
experimental results penn treebank corpus show chunk parser give high precision parsing outputs high speed msec sentence 
show searching improve performance score reaches 
large gap accuracy chunk parser state theart parser produce better scores widely parser parsing speed really needed 
open possibility full parsing large scale information extraction 
steven benson jorge mor 

limited memory variable metric algorithm minimization 
technical report mathematics computer science division argonne national laboratory 
anl mcs 
adam berger stephen della pietra vincent della pietra 

maximum entropy approach natural language processing 
computational linguistics 
bod 

data oriented parsing 
proceedings coling 
thorsten brants 

cascaded markov models 
proceedings eacl 
eugene charniak 

maximum entropy inspired parser 
proceedings naacl pages 
michael collins nigel duffy 

new ranking algorithms parsing tagging kernels discrete structures voted perceptron 
proceedings acl 
michael collins 

head driven statistical models natural language parsing 
ph thesis university pennsylvania 
jun ichi jun ichi tsujii 

evaluation extension maximum entropy models inequality constraints 
proceedings emnlp 
taku kudo yuji matsumoto 

chunking support vector machines 
proceedings naacl 
taku kudo jun suzuki hideki 

boosting parse reranking subtree features 
proceedings acl 
david magerman 

statistical decision tree models parsing 
proceedings acl pages 
takashi miyao jun ichi tsujii 

efficacy beam thresholding unification filtering hybrid parsing probabilistic hpsg parsing 
proceedings iwpt 
lance ramshaw mitch marcus 

text chunking transformation learning 
proceedings third workshop large corpora pages 
adwait ratnaparkhi 

maximum entropy model part speech tagging 
proceedings emnlp pages 
adwait ratnaparkhi 

linear observed time statistical parser maximum entropy models 
proceedings emnlp pages 
sunita sarawagi william cohen 

conditional random fields information extraction 
proceedings icml 
fei sha fernando pereira 

shallow parsing conditional random fields 
proceedings hlt naacl 
erik tjong kim sang 

transforming chunker parser 
veenstra daelemans zavrel editors computational linguistics netherlands pages 
rodopi 
jun ichi tsujii 

bidirectional inference easiest strategy tagging sequence data 
proceedings hlt emnlp 
