competitive line statistics vovk computer learning research centre department computer science royal holloway university london egham surrey tw ex england vovk dcs rhbnc ac uk june radically new approach statistical modelling combines mathematical techniques bayesian statistics philosophy theory competitive line algorithms arisen decade computer science large degree influence dawid prequential statistics 
approach call competitive line statistics assumed data generated stochastic mechanism bounds derived performance competitive line statistical procedures guaranteed hold just hold high probability average 
reviews results area new material includes proofs performance aggregating algorithm problem linear regression square loss 
keywords bayes rule competitive line algorithms linear regression prequential statistics worst case analysis 
problem making rational decisions central problem science everyday life 
polynomials degree fit data sets 
take umbrella today tomorrow 
stocks buy sell year 
rarely readily choose best course action extensive infinite family potentially successful decision strategies 
decision strategy successful depend merits strategy events know 
day choose specific decision strategy naturally arrive problem family decision strategies find new decision strategy perform circumstances best circumstances decision strategy family 
task appear hopeless moderately interesting families decision strategies 
recall want constructed decision strategy perform best strategy family stochastic assumptions generation events 
specific loss function merging algorithm known long time bayesian mixture loss functions merging algorithms years 
usual statistical model family probability distributions reflecting knowledge assumptions piece world 
bayesian framework involves element prior distribution parameter set allows bayesian replace statistical model single probability distribution statistical model prior distribution replaced bayesian mixture 
formula reinterpreted generalized lies centre competitive line statistics 
competitive line statistics statistical model replaced decision pool family decision strategies statistician goal replace family single decision strategy 
recall competitive line statistician agnostic assume stochastic mechanism generating data assume existence mechanism 
bulk sections describe detail existing merging algorithms aggregating algorithm apply problem linear regression square loss function balanced review field section 
aggregating algorithm sample space decision space parameter space decision strategies decision pool indexed 
consider perfect information game players statistician usually called learner machine learning literature decision pool nature trial decision pool prediction interpreted decision recommended decision strategy statistician decision nature chooses outcome 
fixed loss function statistician goal ensure cumulative loss loss lt statistician lt decision strategies 
assuming number decision strategies pool finite possible prove wide class games statistician ensure lt statistician clt ln constants 
wide class games aggregating algorithm described section detail ensures holds optimal algorithm require decision pool finite 
possible improve expense deteriorating vice versa algorithm involves learning rate optimal constants depend 
constants games especially important perfectly games 
important statistics games log loss games assuming simplicity finite log loss game sample space decision space set probability distribution loss function ln 
game aggregating algorithm learning rate coincides bayesian mixture assuming uniform prior constants 
important especially problems regression square loss game 
assume outcomes exceed known bound loss generality take 
case game 
general game perfectly loss function strictly convex sense 
linear regression decision pool infinite surprisingly wide class problems possible derive bounds competitive line procedures see section 
methods line competitive statistics illustrated problem linear regression square loss 
competitive line algorithms problem discussed subsection consider detail just 
extend slightly protocol previous section assume trial nature outputs signal xt decision pool statistician making decisions 
assume signals taken ball ir radius decision pool indexed ball ir radius decision strategy recommends prediction xt trial applying aggregating algorithm decision pool gaussian prior standard bounds algorithm imply lt statistician lt nc ln 
elaborations proof inequality see section appendix assumptions signal parameter spaces bounded simplicity essential assumption responses bounded known constant 
see bound tight assume xt generated process probability probability 
easy check details see subsection statistician uses maximum likelihood estimator computing expected value difference right hand left hand sides exceed minute quantity known traditional statistics see chapter linear regression algorithms properties immediately extended apparently general regression problems 
example results immediately imply simplicity assume xt decision pool consists polynomials degree statistician strategy guaranteeing lt statistician inf lt ln squared norm polynomial coefficients decision pool consists splines degree nodes chosen priori statistician strategy guaranteeing ln 
lt statistician inf lt introductory section essentially revision conference version 
section discuss detail aggregating norms lp 
recall lp norm vector coordinates xi xi definition maxi xi 
known see bellman section 
algorithm applications particular games 
section state elaborations results mentioned linear regression proofs appendix 
section give brief review competitive line statistics inevitably important results mentioned 
concluding section briefly discuss limitations competitive line statistics put general context 
aggregating algorithm generic algorithm subsection describe aggregating algorithm aa specialized case perfectly games general 
tradition computational learning literature say expert decision strategy reader imagine decision strategy pool advocated expert pool experts decision pool 
assume set experts equipped algebra decision prediction space topological space equipped algebra generated open sets outcome space just set additional structure 
functions chosen decision pool required measurable 
fix learning rate put fix probability distribution pool prior distribution specifies initial weights assigned experts 
describe algorithm aggregating pseudo algorithm apa allowed permitted predictions mixtures sense permitted predictions 
generalized prediction defined function type permitted prediction identified generalized prediction defined 
apa suffers loss gt choosing generalized prediction gt actual outcome apa works follows 
trial 
statistician updates experts weights pt pt prior distribution 
weight expert prediction leads large loss gets 
recall equivalent definition pt pt measurable 
generalized prediction chosen apa trial weighted average experts predictions gt log pt normalized weights pt pt assuming denominator positive experts suffered infinite loss aa allowed choose prediction 
see proof lemma subsection see rules natural generalize bayes rule 
aa obtained apa replacing generalized prediction gt permitted prediction gt substitution function maps generalized prediction permitted prediction 
aa described requires minimax substitution function chosen convenient relax requirement order algorithm efficient important cases linear regression 
notice generalized predictions output apa form log ranging probability distributions stand set generalized predictions form 
say substitution function perfect generalized prediction 
assume substitution function exists case say game 
describe aa works difference apa outputs gt apa generalized prediction gt fixed perfect substitution function 
statistician follows aa aa learning rate prior write lt aa place lt statistician analogous notation apa algorithms 
proofs property apa see 
lemma learning rate prior lt apa log lt 
proof required deduce weight update rule formula computing 
natural move direction 
want achieve goal goal generalization formula bayesian mixture see subsection little choice compute 
noticing pt lt immediately follows assuming obtain gt lt apa lt apa log log lt lt log lt lt pt pt coincides 
reversing argument see necessary sufficient 
game lemma implies lt aa log lt 
particular finitely experts expert assigned number experts weight lt aa log lt log lt ln lt coincides recall game called perfectly learning rate 
discuss case game perfectly game perfectly interested 
case perfect substitution function exists need different criterion choosing substitution function choice depend 
define important notion curve 
put inf cg inf set 
related function cf 

shown continuous monotonic functions nondecreasing nonincreasing 
assume substitution function satisfies 
satisfy requirement provided infimum attained mild assumptions details see game 
natural way ensure assumption require arg min sup set pleasant feature definition independence 
approach computationally efficient situations require arg min sup min attained mild assumptions game depend 
assumption compatible typically incompatible 
crucial advantage assumption running aa need normalize weights pt log pt calculated unnormalized weights differ calculated normalized weights additive constant 
avoiding normalization weights trial computationally difficult way defining substitution function lead significant simplifications aa particular applications see subsection 
usually drop index 
game necessarily lt aa log lt inequalities remain true right hand side multiplied 
particular case experts cf 

log loss games lt aa lt ln assume simplicity sets finite 
log loss game defined set probability distributions loss function defined ln simplify notation stand formal 
prior probability distribution set 
decision strategy associate unique probability distribution expert prediction notation upper index convenient subsection variant conditional probability different values 

words expert bayesian subjective probability distribution trial expert quotes subjective probabilities past 

weight update rule pt pt pt pt normalized version cf 


pt identical posterior probability observing 


stand set sequences 
easy see integral predictive distribution bayesian mixture generalized prediction corresponds permitted prediction viz predictive distribution 
clear aa identical case apa corresponds bayesian mixture sense decision strategy corresponds trial aa outputs predictive distribution bayesian mixture past 
summarize case log loss game weight update rule identical bayes theorem aa identical predictive version bayesian mixture apa identical predictive version bayesian mixture fact expressed lemma 
subsection implicitly strong simplifying assumption decision pool measurable strategy depends nature past moves 

informal discussions talking merging decision strategies formal protocol decision pool free choose prediction trial particular possible decision strategies pool depend elements formally protocol 
extend protocol follows 
nature chooses signal xt decision pool prediction statistician chooses prediction ir nature chooses outcome 
signals xt stand elements outside original protocol decision strategies pool depend taken signal space 
extension assume assumption strong xt contain information wish assumption decision pool measurable strategy choosing words obtained xt past data 
xt applying measurable function 
assuming decision pool oblivious statistician signal space just element see obtained 
applying measurable function case define statistical model 
contains element probability distribution governing prequential statistical model discussed fuller description partially specified probability distributions see 
bayesian mixtures easy define special cases aa prequential statistical models 
seen bayesian mixture thing aa applied log loss game simplifying assumptions discussed previous 
difference aa bayesian mixture explicit 
decision pool associate statistical model assuming decision pool follows strategy depends past moves nature ft 

measure necessarily probability distribution 
ft lt define bayesian mixture respect prior distribution usual formula difference usual bayesian mixture arbitrary finite measures 
clear proof lemma pseudo prediction gt base logarithm conditional gt log 

apa essentially thing bayesian rule applied 
notice argument lemma obvious just definition bayesian mixture 
recall problem apa function gt necessarily correspond permitted prediction apply substitution function gt substitution function simple prediction game simple prediction game consider game call simple prediction game 
game statistician trying predict binary classification trial suffers loss mistake 
suppose pool contains finite number decision strategies 
clarify notions substitution function rest subsection apply aa predicting simple prediction game advice experts 
case aa weighted majority algorithm see :10.1.1.37.1595
case just possible outcomes say simple prediction game convenient represent prediction point plane 
permitted predictions simple prediction game depicted small filled circles 
convenient represent point plane 
possible mixtures permitted predictions log log total weight experts predict trial notice total weight experts predict trial trial 
pool weights pool predictions statistician statistician prediction outcome pool losses table example execution aa simple prediction game possible mixtures shown curve call curve connecting permitted predictions parametric equation curve parameter ranges 
clear simple prediction game see equals abscissa equivalently ordinate intersection curve straight line intersection corresponds gives equivalently log ln ln ln exp clear essentially substitution function satisfying line map prediction line map prediction exactly line defined arbitrarily 
notice mixture line means log log equivalent analogously mixture line equivalent 
means aa predicts weighted majority experts explains name weighted majority algorithm 
conclude subsection example execution aa simple prediction game 
table describes aa behaviour trials situation experts equal initial weights give predictions actual outcomes nature moves learning rate 
weights table unnormalized unnormalized version log log pt total unnormalized weight experts predict trial pt total unnormalized weight experts predict trial simplify ln ln results square loss game section discuss square loss game recall game ir shown game perfectly restriction 
haussler proved restriction removed :10.1.1.52.856
goal section prove facts find explicit expression substitution function square loss game 
consider restricted square loss game required 
lemma proven haussler give simple independent proof :10.1.1.52.856
lemma restricted square loss game proof represent analogously previous subsection restricted square loss game point plane :10.1.1.47.6918
set permitted predictions represented parametric curve ranges 
game curve left moving direction increases det computing derivatives find 
positive proportionality constants condition equivalently transformed lemma elaboration haussler result restriction assumed removed :10.1.1.52.856
asserts substitution function restricted game substitution function full game 
lemma fix 
range ir respectively square loss function 
probability distribution ir put log dp 
corresponding ir proof sufficient prove fixed function convex ir 
dp suffices notice log ln ln dp dp dp dp dp dp dp dp dp dp dp inequality follows schwarz inequality see section 
case square loss game yt minimax sense substitution function trial 
pool weights pool predictions statistician statistician prediction outcome pool losses table example execution aa square loss game explained subsection obtain efficient algorithms 
clear satisfy gives 
expression bounds :10.1.1.52.856
table gives example aa execution square loss game situation table 
assumed outcomes happen permitted lemmas smallest value game perfectly substitution function gt represented values gt gt enter gt log pt ln pt gt log pt ln pt remember :10.1.1.47.6918
case yt lemma replaced kivinen warmuth ask question weighted average perfect substitution function true log running probability distributions cf :10.1.1.37.1595

see weighted average perfect substitution function function concave 
square loss game equivalent simple calculations give true times worse constant lemma meet factor section 
games mentioned log loss square loss games perfectly 
important games perfectly simple prediction game ln exp ln exp take absolute loss game halved evident game perfectly sense true :10.1.1.47.6918
unfortunately important perfectly games game cease perfectly sets possible predictions outcomes widened case square loss game jump predictions outcomes allowed take values real line ir 
interesting characterize loss functions giving perfectly games allowed run ir 
obvious example game ir tanh tanh trivial examples 
interesting loss functions curves robust loss functions mentioned press section loss function andrew loss function ln cos tukey function important class games prediction probability games predictions outcomes probability distributions set simplicity assume finite form 

important probability games kullback leibler game hellinger game game loss functions ln ln respectively applications loss functions described 
notice kullback leibler game includes log loss game special case take degenerate 
result proven haussler assumption :10.1.1.52.856
lemma kullback leibler game 
aa kullback leibler game learning rate coincides bayesian mixture 
proof fix learning rate 
considering log loss game clear kullback leibler game weighted average see substitution function 
prove substitution function 
suppose required prove log equivalent transformations ln ln ln pe qe ln pe ln qe ln inequality follows concavity function 
write 
check function concave notice quadratic form negative definite kyk ky ky means quadratic forms sides sign simultaneously negative definite negative definiteness expression follows lyapunov inequality see section 
interesting class games connected finance simplest game kind cover game 
pk pk 
yk 
pk ln financial meaning game described 
kullback leibler game cover game generalization log loss game finite sample space suffices consider degenerate form 


degenerate regarded crisp events generalized fuzzy events 
yk yk interpreted weight evidence favour occurring :10.1.1.47.6918
cover universal portfolio algorithm aa applied game particular decision pool constant rebalanced portfolios cf :10.1.1.56.1067
aa cover algorithm independently aa obtained bridging weighted majority algorithm bayesian mixing rule 
cover game ignores transaction costs possibility short selling financial markets 
long short game introduced watkins takes possibility short selling account 
cover game game perfectly 
account transaction costs leads plethora new games cf 
blum kalai vovk watkins :10.1.1.44.5760
constants games mentioned :10.1.1.37.1595
noticed game perfectly loss function strictly convex sense 
exact statement binary case outcome space consists elements lemma open problem find simple general criterion :10.1.1.52.856:10.1.1.37.1595
regression section show detail apply aa problem linear regression square loss main assumption response variable bounded 
turns particular problem aa applied decision pool linear functions gaussian prior resembles different known ridge regression rr procedure 
general results aa deduce guaranteed bound difference aa performance best sense linear regression function performance 
show aa attains optimal constant bound constant attained rr procedure general times worse 
proofs relegated appendix 
explicit algorithm problem regression consider protocol interaction statistician nature modification protocol previous section 
nature chooses xt ir statistician chooses prediction ir nature chooses yt 
example xt meteorological data collected day yt day high temperature 
terms xt nature signals 
notice notation yt outcomes chosen nature notation conventional regression literature 
perfect information protocol player see player moves 
parameters protocol fixed positive number dimensionality regression problem upper bound value yt returned nature 
important algorithm playing game part statistician need know subsection give description regression algorithm rigorous derivation general aa appendix 
usually non trivial task represent aa computationally efficient form case line linear regression exception 
briefly describe main idea aa applied regression problem 
perfect substitution function square loss game experts indexed ir trial expert outputs prediction xt predictions averaged accordance experts weights applied resulting generalized prediction 
details see appendix 
fix 
algorithm follows ai trial read new xt ir xtx output prediction xt read new yt ir 
description matrix symmetrical positive definite ir unit matrix vector 
usual vectors identified column matrices stands transpose matrix algorithm denoted aar aa regression aar 
remembered principle aa applied problem regression different ways aar derived aa strong extra assumptions including gaussian prior decision pool 
upper bounds subsection state results describing predictive performance aar results stated rest section proven appendix 
recall decision pool consists linear functions xt xt ir trial expert statistician suffer loss yt xt yt respectively 
typical bound aar prove assuming signals xt parameter confined unit balls metrics respectively lt aar lt ln 
case xt inequality differs freund theorem additive constant :10.1.1.51.3156
freund noticed problems considered adversarial bounds competitive line statistics tiny amount worse bounds stochastic strategies nature shows manifestations phenomenon 
compact pools experts setting corresponds set possible weights bounded closed usually possible derive bounds statistician loss compared best expert loss 
case non compact pool need give statistician start remote experts 
specifically comparing statistician performance inf lt compare inf giving lt statistician start expert constant reflecting prior expectations complexity successful experts 
idea giving start statistician allows prove stronger results elaboration holds ln lt aar inf lt inequality assumes xt yt unbounded 
theorem positive integer lt aar inf lt ln det inf lt ln addition xt lt aar inf lt ny ln xtx 
inequality theorem implies inequalities suffices put put 
interpret term ln det xtx theorem notice rewritten ln ln det cov 
xn cov 
xn empirical covariance matrix predictor variables words cov 
xn covariance matrix random vector takes values 
xt equal probability 
see term typically close ln lower bounds simple argument sketched subsection shows bound tight certain weak sense 
xt yt interested problem classification absence signals nature 
suppose data generated stochastically process probability yt probability yt 
essentially assuming bernoulli model 
estimate trial maximum likelihood yt say 
expected loss estimator trial estimate mle pt pt yt pt yt replaced cumulative loss trials ln 
hand expected loss best expert trials yt yt yt yt yt yt yt yt 
see lt mle ln ln loss best expert trials 
obtain elt mle el ln mle improve average improvement ln ln minute 
see case nature adversarial follows simple stochastic strategy inequality tight 
weakness argument despite common belief mle reasonable estimator case bernoulli model desirable lower bounds applicable strategy statistician 
theorem step direction 
theorem fix number attributes upper bound yt 
exist constant stochastic strategy nature xt yt stochastic strategy statistician lt statistician inf cf 
inequality 
replacing inequality accurate lt ln ln ln ln see worst case bound aar better precise bound mle 
proof theorem see appendix exhibit suitable strategy nature just mixture processes considered beta prior 
comparisons ridge regression method proposed linear regression gauss squares method square loss game means choose best expert 
course expert best past may perform badly overcome danger overfitting rr procedure proposed 
simplest form procedure just implementation idea giving statistician start remote experts estimate take value inf attained 
explicit formula ai xtx see subsection positive ridge constant squares procedure corresponds 
estimate predict yt obtain rr prediction xt ai xtx xt line prediction algorithm formula computing predictions called rr procedure 
notice rr procedure similar aar obtained swapping lines aar ai trial read new xt ir output prediction xt xtx read new yt ir 
correspondingly predictions output aar formula similar ai xtx xt 
easy see rr procedure amounts applying squares procedure rr enlarged training set add observations specifically 
aei ei ith column unit matrix ridge constant 
aar add observation training set observation xt xt attributes example classified 
aar step shrinking predictions hopefully extra shrinking algorithm resistant overfitting 
possible deduce explicit relation aar prediction rr prediction rt trial done kostas sherman morrison formula xt xt xt xt xt xt rt xt xt xt rt xt xt xt notations bt sense aar rr ai bt 
see nature dependence xt rational function linear function divided quadratic function 
limiting behaviour rt different xt typically rt 
notice formula problem classification outcome space aar gives categorical prediction rr procedure 
seen rr gives results sensible framework yt goal compete best linear regression function 
example suppose nature generates outcomes xt yt odd 
yt 
trial 
rr accurately natural modification truncates predictions give prediction yt equal previous response suffer loss trials 
hand aar prediction close cumulative loss aar trials close best expert loss 
see rr procedure situation forced suffer loss times big aa loss 
interesting constant occurs theorem cf 
theorem 
lower bound proven subsection imply regression algorithm better rr adversarial framework 
idea proof theorem appendix lower bound performance rr situation expected loss rr optimal 
theorem asserts lt statistician inf lt ln rt statistician follows aar 
theorem shows rr violates inequality 
theorem number attributes upper bound yt fix 
nature strategy statistician follows rr lt statistician ln inf lt violated 
ln nature strategy ensures inequality theorem simple generates xt odd yt 
rr prediction obtained mean posterior distribution proven subsection aar prediction obtained sophisticated substitution function 
rr modification rr procedure experts predictions xt trial clipped averaging 
properties posterior mean substitution function studied kivinen warmuth result reproduced immediately implies theorem details see appendix :10.1.1.37.1595
theorem fixed lt rr inf lt ln det provided yt belong 
inf lt ln xtx clear remains true rr replaced rr particularly interesting question view better bound theorem 
review literature competitive line statistics probably performed competitive analysis bayesian mixing scheme log loss prediction game 
littlestone warmuth vovk introduced line algorithm called weighted majority algorithm authors simple prediction game :10.1.1.37.1595
algorithms bayesian mixing scheme weighted majority algorithm generalized aa proposed vovk 
aa proven optimal simple cases see haussler vovk watkins theorem stated :10.1.1.52.856
aa member wide family algorithms called multiplicative weight exponential weight algorithms 
call area competitive line statistics closeness ideology theory competitive line algorithms see 
adjective competitive refers inequalities analogous inequalities competitive line algorithms approach goal algorithm perform best retrospectively decision strategy 
adjective line means data xt yt arrive sequentially computer science analogue prequential prequential statistics 
rest section give brief review selected sub areas competitive line statistics especially closely connected material discuss connections competitive line statistics traditional statistics information theory 
exciting developments applications discussed interested reader consult literature sleeping experts world wide web applications see freund cohen singer boosting game theory linear programming freund schapire pruning decision trees helmbold schapire pereira singer competitive line regression review sub area competitive line statistics closely connected rest regression square loss function :10.1.1.22.4912:10.1.1.32.8918:10.1.1.14.6535
competitive approach regression started probably foster considered line variant rr procedure 
particular theorem asserts situation response variable takes values examples xt belong weights nonnegative satisfy lt ln lt loss trials line algorithm loss best trial linear regression function result similar inequality :10.1.1.47.6918
lower bounds 
cesa bianchi performed square loss competitive analysis standard gradient descent algorithm kivinen warmuth complemented competitive analysis modification gradient descent call exponentiated gradient algorithm :10.1.1.30.7849
bounds obtained cesa bianchi kivinen warmuth type trial lt cl constant specifically gradient descent exponentiated gradient :10.1.1.30.7849
see 
bounds hold assumptions gradient descent assumed norm weights data items bounded constant exponentiated gradient norm weights norm data items bounded 
interesting cases bound appear weak 
example suppose comparison class contains true regression function values corrupted gaussian noise assume yt xt independent random variables assume xt performance aar difference lt bounded logarithmic function concerns grow linearly inequality bound difference lt linear function bounds proven important advantages bounds example depends number parameters logarithmically case exponentiated gradient algorithm depend case gradient descent bound depends linearly :10.1.1.30.7849
bounds say bound lt gd inf lt theorem iv assumption xt noise free case lt 
naive implementation aar require arithmetic operations trial 
easy see ways implement efficiently example value inverse trial compute value sherman morrison formula section xt xt ta xt trial 
need arithmetic operations trial 
gradient descent algorithm exponentiated gradient algorithm require operations trial 
remains seen possible combine advantages aar gradient descent exponentiated gradient algorithm 
step direction done warmuth obtained bound theorem see theorem algorithm motivated methodology :10.1.1.30.7849
approach try aa different prior 
formally verify claim grows linearly model 
yt observed sequence responses 
xt observed sequence signals 
loss generality assume true value simplicity consider case 
squares estimate best expert cf 
residual sum squares txt final expression loss suffered true regression function grows law large numbers smaller chebyshev inequality non negative txt notice gaussian noise model violates assumption yt matter know advance xt 
cesa bianchi kivinen warmuth construct semi line algorithms algorithms priori upper estimate loss best regression function :10.1.1.30.7849
algorithms derive bounds type lt 
usual doubling trick cesa bianchi possible obtain bounds type lt pure line setting see cesa bianchi theorem iv 
example true regression function corrupted bounded linear function gaussian noise difference lt worse aa purely line require priori estimates 
yamanishi analyzed application aa wide class decisions pools obtained loss bounds analogous general setting 
decision pools examples weights examples xt known belong unit ball ir components nonnegative responses yt known belong :10.1.1.47.6918
uniform distribution prior aa yamanishi obtained analogue inequality lt aa inf lt ln ln 
comparing recall yamanishi assumes yt see coefficients leading term ln match kept mind special case general result theorem appeared yamanishi :10.1.1.47.6918
yamanishi proves non asymptotic results kind proves lower bounds matching upper bounds 
forster discovered surprising property aar predictions theorem minimize maximal extra loss compared best expert suffered trial gives simplified proof theorem 
feder singer obtain results similar case linear autoregression square loss 
case order autoregression known obtain bound lt universal algorithm inf lt ln bound follows theorem theorem similar form priori bound values taken yt predicted values yt 
yt stays bounded 
notice involves extra factor familiar subsection ln compared 
happened posterior mean substitution function see discussion subsection opportunistic approach aa 
distinctive feature competitive approach regression assumptions stochastic properties mechanism example section assumption data yt situations data generated partially known stochastic mechanism feature disadvantage advantage 
assumption yt bounded natural applications problem classification yt predicting student score ranging applications look artificial 
interesting relax remove assumption ideas tuning lower upper bounds vovk gammerman 
tracking best expert far interested competing best linear regressor components fixed weights 
herbster warmuth considers case allowed slowly change time 
special case general problem tracking best expert originated littlestone warmuth studied herbster warmuth auer warmuth vovk :10.1.1.37.1595
problem tracking best expert start pool basic decision strategies 
known strategies pool inflexible attain performance expect strategies hoping rarely switch strategies 
main approaches problem tracking best expert try modify known merging algorithms adaptive approach 
construct pool example start morning strategy pm switch strategy pm switch strategy apply aa static merging algorithm approach 
consider problem tracking best expert nature gives useful signals expert perform 
example suggested referee 
suppose start trial nature tells type situation type situation 
experts performs type situations performs type situations 
average experts may perform badly may aa applied original pool experts 
ensure performance aa applied extended pool containing original expert expert expert predicts expert type situations expert type situations expert predicts expert type situations expert type situations 
idea enlarging original decision pool works complicated cases tracking quite enlarged pool uncountably large original pool finite 
problems apparently different tracking best expert polynomials growing degree prediction discussed enlargement original pool allows apply aa see 
typical bounds total line loss merging algorithm tracking best expert sum loss best switching schedule basic strategies plus total cost switching say constant times number switches plus small overhead logarithm number basic strategies 
tracking best expert applied predicting disk idle times load balancing problems doubt find important applications 
bandit problems reinforcement learning mentioned philosophy competitive line statistics described works nature oblivious statistician decisions depend 
nature oblivious longer possible interpret inequality saying statistician performs worse best decision strategies followed strategy observed different sequence outcomes 
assumption nature oblivious justified predictions say atmosphere care predicting rain justified decisions different predictions portfolio selection small investor see subsection 
situation changing papers competitive line results bandit problems nature means oblivious 
turns possible study exploration exploitation tradeoff competitive line framework 
predictive complexity application aggregating algorithm generalize notion kolmogorov complexity li vitanyi excellent review :10.1.1.22.4912
idea apply aa merging algorithm universal decision pool containing computable decision strategy pool constructed universal turing machine 
loss resulting decision strategy decision strategy generalized sense data sequence called predictive complexity applied log loss game leads variant kolmogorov complexity 
fundamental concept se notion predictive complexity allows define notion randomness prediction games different log loss game details see 
standard notion logloss randomness grossly presently see 
application generalizing mdl principle games different log loss see 
information predictive complexity reader consult vovk watkins :10.1.1.30.7849
statistics information theory clear subsection log loss function plays fundamental role probability theory statistics especially bayesian statistics role log loss function prominent information theory interpreted code length connections usual statistical notions log likelihood maximum likelihood estimate mle observed fisher true lossless compression competitive line theory lossy compression require sophisticated loss functions 
information competitive line statistics clearer explain generalized different loss functions 
prior probability density learning rate usual set described earlier statistician predict pseudo predictions trial lt 
lt statistician log statistician permitted predictions obtain lt statistician log lt 
function lt generalization minus log likelihood function statistics 
simplicity consider case ir 
important characteristics minus log likelihood function maximum likelihood estimate observed fisher information arg min usual fisher information expectation observed fisher information 
functions long history statistics fisher idea dawid section maximum likelihood estimator sufficient complemented ancillary statistic simplify regularity conditions specifying precisely 
function rapidly vanishes outside interval centered length near inside interval close obtain statistician log const statistician ln const 
laplace approximation constant const evaluated ln 
cases order magnitude ordinal number trial 
gives log overhead perfectly games similar logarithmic term ln occurs 
terms analogous term ln occurring analysis log loss game generalizations particular rissanen theory stochastic complexity extended general loss functions yamanishi wallace theory minimum message length wallace boulton wallace freeman minimax regret analysis barron xie 
showed upper bound problem regression sense optimal viz shown coefficient ln improved 
related results rissanen proof term ln optimal 
traditional statistics usually assumed outcomes generated true distribution papers log loss averaged respect true distribution naturally leads considering kullback leibler distance see clarke barron 
natural assume results papers extended kullback leibler game see subsection proven subsection aa game exactly bayesian mixing rule considered 
clarke dawid assumptions true distributions entirely competitive line :10.1.1.23.2569
competitive line statistics concerned course line performance statistical procedures 
procedures rr batch line setting 
dawid prequential approach statistics recommends line performance measure quality batch algorithms competitive line results provide justification corresponding algorithms batch setting provided philosophy prequential statistics accepted 
potential objection line procedures batch setting result depend ordering batch 
notice case aa applied stationary experts ordering irrelevant 
example imagine batch records xt yt 
xt results medical tests performed patient yt patient diagnosis 
new patient test results xt aa produce prediction substitution function defined cf 
log lt lt 
vary function decision rule produced aa applied batch 
xt yt 
intuitively obtained gluing aa predictions possible new examples xt 
decision rule depend ordering say pool experts stationary decision taken expert trial depends signal xt words xt function remember pool experts considered section stationary xt xt 

xt yt function depend 
discussion lt yt xt completely clear far competitive line statistics developed limitations understood wait disclosed 
serious limitation interesting games especially non compact constants infinite mentioned infinite remove assumption response variable bounded square loss game 
limitation nature implicitly assumed oblivious 
advantages competitive line statistics turned clear generate lot interesting research computational learning community described previous section 
mentioned attempts alleviate limitations 
main ideas competitive line statistics summarized follows 
basic recipe 
set complete protocol interaction statistician environment including game played possible side information attributes object classified 

choose yardstick class decision strategies statistician 

decide prior probability distribution 

choose learning rate 
find bayesian mixture decision strategies yardstick class learning rate weights prior 

making decision replace generated bayesian mixture permitted decision 
situations want replace minimax approximation method aa different say choose trial action smallest average loss rt respect predictive distribution rt pt prior distribution say uniform exists method aa generalization weighted majority algorithm bayesian mixing scheme 
related ideas gr 
merging algorithms different aa steps may combined 
main modes application merging algorithms aa 
apply merging algorithm directly decision pool 

apply merging algorithm top algorithm merging squares predictions polynomials different degrees 
second option applied soon appears danger overfitting 
example suppose wishes squares method choose polynomial degree fit data uncertainty choose avoid overfitting underfitting data 
consider decision pool indexed decision strategy outputs trial pi xt prediction yt new instance xt pi degree polynomial best squares approximation past data 
xt yt 
applying aa decision pool able ensure lt aa lt log square loss assuming yt :10.1.1.47.6918
notice case particular value chosen kept aa different weights 
details see appendix natural approach allow degree polynomial grow slowly result aa applied situation stated proven theorem 
briefly discuss implement steps basic recipe starting choose decision pool 
problems conventional choice choice linear decision rules problem regression section 
theory predictive complexity see subsection suggests natural approach take decision pool large possible limit contain computable decision strategies 
case perfectly games able perform best computable decision strategy additive constant 
ideal picture spirit solomonoff satisfying feasible done needs distinguish degrees computability decision strategies additive constants need studied 
case log loss game attempt ideal picture feasible vovk 
current state theory recommended specific problems take decision pool containing potentially useful decision strategies possible maintaining feasibility resulting aggregated strategy 
concerns choosing prior view inequalities natural idea choosing bigger better 
ideal take levin universal enumerable see li vitanyi known priori prior computable :10.1.1.22.4912
choice universal constant prefix complexity prior right hand side increase log ln universal constant place 
rissanen take computable approximations universal enumerable 
rissanen gave relatively rigorous interpretation jeffreys idea noninformative priors priors reflecting statistician ignorance 
suggested prior ln integers prior density uniform middle type ln near left type ln near right :10.1.1.47.6918
popular approach choosing prior take optimal typically minimax criterion optimality see clarke barron barron xie freund :10.1.1.51.3156
difficult question choose loss function 
situations clear true loss function precisely evaluate consequences predictions situations know predictions 
decision situations loss function problem study properties perfectly 
curve inference situations conventional loss functions square loss 
concerns choosing learning rate situation decision pool finite countable natural vague principle expect performance strategy big weight bad close learn slowly expect strategy small weight performs big learn quickly badly 
want infinitesimal setting case absolute loss function way doing described cesa bianchi 
cesa bianchi discuss prior distribution set possible values natural take approximation universal prior 
case perfectly games natural thing take largest words minimize constraint 
place approach competitive line statistics general framework theory decision making uncertainty 
popular principles decision making uncertainty laplace principle wald minimax principle savage minimax regret principle criticized arbitrary 
competitive line statistics version minimax regret principle decision strategy acceptable regret simplest case loss aggregated strategy minus loss best decision strategy pool small maximum regret aa small experts 
restricted version minimax regret principle far objectionable general principle 
acknowledgments am grateful phil dawid meir feder mark herbster kostas manfred warmuth kenji yamanishi useful discussions illuminating comments manfred finding mistake earlier draft 
referee thoughtful suggestions greatly helped improve technical content presentation 
partially supported epsrc gr support vector bayesian learning algorithms gr predictive complexity recursion theoretic variants gr comparison support vector machine minimum message length methods induction prediction 
peter auer nicol cesa bianchi yoav freund robert schapire 
gambling casino adversarial multi armed bandit problem 
proceedings th annual symposium foundations computer science pp 

peter auer manfred warmuth 
tracking best disjunction 
machine learning 
manfred warmuth 
relative loss bounds line density estimation exponential family distributions 
manuscript 
extended appeared proceedings th conference uncertainty artificial intelligence pages san francisco ca 
morgan kaufmann 
andrew barron xie 
asymptotic minimax regret data compression gambling prediction 
ieee transactions information theory 
edwin richard bellman 
inequalities 
springer berlin 
avrim blum carl burch 
line learning metrical task system problem 
machine learning 
avrim blum adam kalai :10.1.1.44.5760
universal portfolios transaction costs 
machine learning 
alexander 
mathematical statistics russian 
nauka moscow 
nicol cesa bianchi fischer 
finite time regret bounds bandit problem 
th international conference machine learning pp 

morgan kaufmann 
nicol cesa bianchi yoav freund david haussler david helmbold robert schapire manfred warmuth 
expert advice 
journal association computing machinery 
nicol cesa bianchi david helmbold sandra 
bayes methods line boolean prediction 
algorithmica 
nicol cesa bianchi philip long manfred warmuth 
worstcase quadratic loss bounds line prediction linear functions gradient descent 
ieee transactions neural networks 
bertrand clarke andrew barron 
information theoretic asymptotics bayes methods 
ieee transactions information theory 
bertrand clarke andrew barron 
jeffreys prior asymptotically favourable entropy risk 
journal statistical planning inference 
bertrand clarke philip dawid :10.1.1.23.2569
line prediction experts log scoring rule 
manuscript 
william cohen yoram singer :10.1.1.14.6535
context sensitive learning methods text categorization 
acm transactions information systems 
thomas cover 
universal portfolios 
mathematical finance 
thomas cover erich ordentlich 
universal portfolios side information 
ieee transactions information theory 
philip dawid 
statistical theory prequential approach 
journal royal statistical society 
philip dawid 
probability forecasting 
kotz johnson read editors encyclopedia statistical sciences volume pp 

wiley interscience new york 
philip dawid 
inference likelihood prequential frames discussion 
journal royal statistical society 
markowsky wegman 
learning probabilistic prediction functions 
proceedings th annual ieee symposium foundations computer science pp 
los alamitos ca 
ieee computer society 
draper smith 
applied regression analysis 
wiley new york nd edition 
meir feder andrew singer 
universal data compression linear prediction 
proceedings ieee data compression conference 
rgen forster 
relative loss bounds generalized linear regression 
manuscript 
foster 
prediction worst case 
annals statistics 
yoav freund :10.1.1.51.3156
predicting binary sequence optimal biased coin 
proceedings th annual conference computational learning theory pp 
new york 
association computing machinery 
yoav freund robert schapire 
game theory line prediction boosting 
proceedings th annual conference computational learning theory pp 
new york 
association computing machinery 
yoav freund robert schapire 
decision theoretic generalization line learning application boosting 
journal computer system sciences 
yoav freund robert schapire 
adaptive game playing multiplicative weights 
games economic behavior 
yoav freund robert schapire yoram singer manfred warmuth 
combining predictors specialize 
proceedings th annual acm symposium theory computing new york 
association computing machinery 
peter gr 
minimum description length principle reasoning uncertainty 
phd thesis institute logic language computation universiteit van amsterdam 
david haussler kivinen manfred warmuth :10.1.1.52.856
sequential prediction individual sequences general loss functions 
ieee transactions information theory 
david helmbold long 
dynamic disk spin technique mobile computing 
proceedings nd annual acm international conference mobile computing networking 
david helmbold robert schapire 
predicting nearly best pruning decision tree 
machine learning 
mark herbster manfred warmuth 
tracking best expert 
machine learning 
mark herbster manfred warmuth 
tracking best regressor 
proceedings th annual conference computational learning theory pp 

association computing machinery 
yuri :10.1.1.30.7849
general linear relations different types predictive complexity 
proceedings th international conference algorithmic learning theory volume lecture notes artificial intelligence pp 

accepted publication theoretical computer science 
yuri 
linear relations square loss kolmogorov complexity 
proceedings th annual conference computational learning theory pp 
new york 
association computing machinery 
yuri :10.1.1.30.7849
complexity approximation principle rissanen approach real valued parameters 
european conference machine learning 
yuri vovk :10.1.1.37.1595
existence predictive complexity legendre transformation 
technical report tr computer learning research centre royal holloway university london march 
kivinen manfred warmuth :10.1.1.30.7849
exponential gradient versus gradient descent linear predictors 
information computation 
kivinen manfred warmuth :10.1.1.37.1595
averaging expert predictions 
paul fischer hans simon editors computational learning theory th european conference volume lecture notes artificial intelligence pp 
berlin 
springer 
ming li paul vit nyi :10.1.1.22.4912
kolmogorov complexity applications 
springer new york nd edition 
nick littlestone manfred warmuth :10.1.1.37.1595
weighted majority algorithm 
information computation 
pereira yoram singer :10.1.1.22.4912
efficient extension mixture techniques prediction decision trees 
proceedings th annual conference computational learning theory pp 

association computing machinery 
william press brian flannery saul teukolsky william vetterling 
numerical recipes cambridge university press cambridge 
raghavan ed 
special issue competitive line algorithms 
algorithmica 
rissanen 
universal prior integers estimation minimum description length 
annals statistics 
rissanen 
stochastic complexity discussion 
journal royal statistical society 
glenn shafer vovk 
probability finance game 
wiley new york 
appear 
ray solomonoff 
formal theory inductive inference 
parts ii 
information control 
akira vovk 
predicting nearly best pruning decision tree dynamic programming scheme 
submitted publication 
vovk 
probability theory brier game 
accepted publication theoretical computer science 
preliminary version ming li akira editors algorithmic learning theory lecture notes computer science volume pages 
full version technical report csd tr department computer science royal holloway university london revised february 
vovk 
randomness criterion 
soviet mathematics doklady 
vovk 
aggregating strategies 
case editors proceedings rd annual workshop computational learning theory pp 
san mateo ca 
morgan kaufmann 
vovk 
universal forecasting algorithms 
information computation 
vovk 
logic probability application foundations statistics discussion 
journal royal statistical society 
vovk 
competitive line linear regression 
jordan kearns solla editors advances neural information processing systems pp 
cambridge ma 
mit press 
vovk 
game prediction expert advice 
journal computer system sciences 
vovk 
competitive line statistics 
bulletin international statistical institute 
nd session proceedings volume book pp 

vovk 
stochastic prediction strategies 
machine learning 
vovk alex gammerman 
complexity approximation principle 
computer journal 
vovk alex gammerman 
statistical applications algorithmic randomness 
bulletin international statistical institute 
nd session contributed papers volume book pp 

vovk chris watkins 
universal portfolio selection 
proceedings th annual conference computational learning theory pp 
new york 
association computing machinery 
vladimir 
snooping help 
technical report tr computer learning research centre royal holloway university london 
downloaded www rhbnc ac uk 
vladimir 
sequences predictable 
technical report tr computer learning research centre royal holloway university london 
downloaded www rhbnc ac uk 
vladimir 
sub optimal measures predictive complexity 
technical report tr computer learning research centre royal holloway university london 
downloaded www rhbnc ac uk 
chris wallace boulton 
information measure classification 
computer journal 
chris wallace freeman 
estimation inference compact coding discussion 
journal royal statistical society 
kenji yamanishi 
generalized stochastic complexity applications learning 
proceedings conference information science systems volume pp 

kenji yamanishi 
decision theoretic extension stochastic complexity applications learning 
ieee transactions information theory 
kenji yamanishi 
minimax relative loss analysis sequential prediction parametric hypotheses 
proceedings th annual conference computational learning theory pp 
new york 
association computing machinery 
appendix 
proofs derivation aar arbitrary constant 
consider prior distribution set ir possible weights gaussian density value normalizing constant chapter theorem 
loss expert trials yt xtx loss apa log ir exp ai xtx 
accordance aa prediction log gt gt log ir ai xtx ir ai xtx log ir ai xtx ir ai xtx log ai xtx ai xtx ai xtx notice disappeared notation inf ir inf ir fact function transformed follows inf ir inf ir see transition justified notice horizontal translation ac positive definite quadratic form arbitrary vector coefficient front quadratic forms numerator denominator obtained horizontal translation adding constant 
clear ratio integrals depends additive constant written form 

upper bounds proof theorem proof inequality theorem derivation known properties general aa difficulties paragraph second inequality follows chapter theorem third inequality trivial 
see explanation steps derivation 
xt set cf 
subsection 
maximum expression exp sign attained point point minimum attained statement theorem 
regularized loss subtracting ai xtx expert see obtain log ir translation horizontal translation plus constant quadratic form ai xtx inf horizontal translation cf 
paragraph previous section equals log exp ai ir xtx directly evaluating integral see chapter theorem transform expression log det xtx log det xtx ln det xtx ln det lower bounds proof theorem xtx stochastic strategy nature xt 

consider case xt easy generalize arbitrary 
loss generality consider case general case obtained simple rescaling 
clear assuming yt assume yt xt shift yt see best value satisfy :10.1.1.47.6918
nature strategy follows generates beta distribution parameters large positive constant trial sets yt probability yt probability independently previous trials :10.1.1.47.6918
nature strategy known easy find best average strategy statistician bayesian strategy 
prior density proportional denote number trials obtain posterior density proportional trials 
nature generates trial probability pk dp conditional happened trials square loss proper scoring rule see dawid bayesian strategy statistician output pt trial 
fixed expected loss bayesian strategy trial ep ep pt yt ep pt ep yt ep pt pt pt ap ep ap equality chain follows fact holds conditionally trials third equality follows ep pt expected value fixed loss bayesian strategy trials tp ap statistician ln 
averaging prior distribution pa dp find elt statistician ln 
find expected cumulative loss trials fixed best expert ep yt ep ep yt ep pt pt 
obtain inf lt yt 
statement theorem immediately follows comparison able take large 
case stochastic strategy nature satisfy requirement theorem 
starts independently generating numbers pi :10.1.1.47.6918
beta distribution parameters trial sets xt 

ith component vector xt mod mod yt probability pi yt probability pi independently previous trials 
construction implies right hand side inequality statement theorem upper bound included constant lower bounds ii proof theorem subsection prove theorem nature play strategy described statement theorem 
prove yt statistician prediction repetition observed response 
trial observed xt yt 
take xt resp 
yt unit length measuring xs resp 
ys imagine data set infinite large approximation 
best squares approximation data set determined condition 
min differentiating function summing geometrical progressions obtain gives observing xt give prediction 
trial statistician completes proof yt 
equations obvious easy check ln ln ln 
derivation rr proof theorem recall rr estimate defined arg min 
derive rr prediction showing precisely mean posterior distribution 
known fact proof simple give 
prior associating expert probability forecasting system prediction trial gaussian distribution mean variance obtain posterior distribution proportional exp lt exp ai xtx cf 

posterior mean equals minimum expressions minimizing expression obtain original definition rr estimate minimizing second ai xtx obtain rr formula 
prove theorem suffices take proof subsection cf 


