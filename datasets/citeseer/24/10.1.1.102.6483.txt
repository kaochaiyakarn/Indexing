boosting leveraging ron meir gunnar tsch department electrical engineering technion haifa israel ee technion ac il www ee technion ac il research school information sciences engineering australian national university canberra act australia gunnar anu edu au anu edu au 
provide theoretical practical aspects boosting ensemble learning providing useful researchers field boosting seeking enter fascinating area research 
short background concerning necessary learning theoretical foundations weak learners linear combinations 
point useful connection boosting theory optimization facilitates understanding boosting enables move new boosting algorithms applicable broad spectrum problems 
order increase relevance practitioners added remarks pseudo code tricks trade algorithmic considerations appropriate 
illustrate usefulness boosting algorithms giving overview existing applications 
main ideas illustrated problem binary classification extensions discussed 
brief history boosting underlying idea boosting combine simple rules form ensemble performance single ensemble member improved boosted 

ht set hypotheses consider composite ensemble hypothesis tht 
denotes coefficient ensemble member ht combined learner hypothesis ht learned boosting procedure 
idea boosting roots pac learning cf 

kearns valiant proved astonishing fact learners performing slightly better random combined form arbitrarily ensemble hypothesis data available 
schapire mendelson smola eds advanced lectures machine learning lnai pp 

springer verlag berlin heidelberg boosting leveraging provide provably polynomial time boosting algorithm apply boosting idea real world ocr task relying neural networks base learners 
adaboost adaptive boosting algorithm cf :10.1.1.133.1040:10.1.1.32.8918
algorithm generally considered step practical boosting algorithms 
similar adaboost arcing algorithm convergence linear programming solution shown cf 

boosting scheme intuitive context algorithmic design step forward transparency taken explaining boosting terms stage wise gradient descent procedure exponential cost function cf 
:10.1.1.30.3515
interesting step practical applicability worth mentioning large parts early boosting literature persistently contained misconception boosting overfit running large number iterations 
simulations data sets higher noise content clearly show overfitting effects avoided regularizing boosting limit complexity function class cf 
section 
trying develop means achieving robust boosting important elucidate relations optimization theory boosting procedures 
developing interesting relationship opened field new types boosting algorithms 
options possible rigorously define boosting algorithms regression cf 
multi class problems unsupervised learning cf :10.1.1.10.5265
establish convergence proofs boosting algorithms results theory optimization :10.1.1.42.8795
extensions boosting algorithms :10.1.1.30.3515
boosting strategies quite successfully various real world applications 
instance earlier boosted ensembles neural networks ocr 
proposed non intrusive monitoring system assessing certain household appliances consume power 
boosting tumor classification gene expression data 
applications details refer applications section www boosting org applications 
attempt full balanced treatment available literature naturally presentation parts authors 
fact boosting literature clear bibliography extensive full treatment require book length treatise 
review differs reviews ones mainly choice material place emphasis robust algorithms boosting connections margin approaches support vector machines connections theory optimization :10.1.1.107.3285
discuss applications extensions boosting 
content review organized follows 
presenting basic ideas boosting ensemble methods section brief overview underlying theory section 
notion margin meir tsch connection successful boosting analyzed section important link boosting optimization theory discussed section 
subsequently section approaches making boosting robust proceed section presenting extensions basic boosting framework 
section presents short summary applications section summarizes review presents open problems 
boosting ensemble methods section includes brief definition general learning setting addressed followed discussion different aspects ensemble learning 
learning data pac property focus review section problem binary classification 
task binary classification find rule hypothesis set observations assigns object classes 
represent objects belonging input space denote possible outputs hypothesis possible formalization task estimate function input output training data pairs generated independently random unknown probability distribution xn yn correctly predict unseen examples 
case called hard classifier label assigned input 
takes termed soft classifier case label assignment performed sign 
true performance classifier assessed dp denotes suitably chosen loss function 
risk termed generalization error measures expected loss respect examples observed training set 
binary classification uses called loss yf event occurs zero 
loss functions introduced depending specific context 
unfortunately risk minimized directly underlying probability distribution unknown 
try estimate function close optimal available information training sample properties function class solution chosen 
need called induction principle 
particularly simple consists approximating minimum risk minimum empirical risk boosting leveraging xn yn 
law large numbers expects asn 
order guarantee function obtained minimizing attains asymptotically minimum stronger condition required 
intuitively complexity class functions needs controlled function arbitrarily low error sample may leads poor generalization 
sufficient condition preventing phenomenon requirement converge uniformly tol see details 
possible provide conditions learning machine ensure asymptotically empirical risk minimizer perform optimally small sample sizes large deviations possible overfitting occur 
small generalization error obtained simply minimizing training error 
mentioned way avoid overfitting dilemma called regularization size function class chooses function 
intuition formalized section simple function explains data preferable complex fits data occam razor 
boosting algorithms generate complex composite hypothesis may tempted think complexity resulting function class increase dramatically ensemble learners 
provably case certain conditions see theorem ensemble complexity saturates cf 
section 
insight may lead think complexity boosting limited encounter effect overfitting 
boosting procedures noisy realworld data turns regularization mandatory overfitting avoided cf 
section 
line general experience statistical learning procedures complex non linear models instance neural networks regularization term appropriately limit complexity models 
proceeding discuss ensemble methods briefly review strong weak pac models learning binary concepts 
sample consisting data points xn yn xn generated independently random distribution yn xn belongs known class binary functions 
strong pac learning algorithm property distribution probability larger algorithm outputs hypothesis pr 
running time algorithm polynomial dimension appropriately defined input space 
weak pac learning algorithm defined analogously required satisfy conditions particular pairs 
various extensions generalization basic pac concept 
meir tsch ensemble learning boosting leveraging consider combination hypotheses form 
clearly approaches selecting coefficients base hypotheses ht 
called bagging approach hypotheses ht chosen set bootstrap samples coefficients set see refined choices :10.1.1.32.9399
algorithm somewhat simplistic nice property tends reduce variance estimate discussed regression classification 
bagging quite improve performance complex unstable classifiers neural networks decision trees 
boosting combination hypotheses chosen sophisticated manner 
evolution adaboost algorithm understood 
intuitive idea examples misclassified get higher weights iteration instance examples near decision boundary usually harder classify get high weights iterations 
idea iterative reweighting training sample essential boosting 
formally non negative weighting 
assigned data step weak learner ht constructed weighting updated iteration weighted error incurred weak learner iteration see algorithm 
step weak learner required produce small weighted empirical error defined st iteration nd iteration rd iteration th iteration th iteration th iteration fig 

illustration adaboost toy data set color indicates label diameter proportional weight examples second third th th th iteration 
dashed lines show decision boundaries single classifiers th iteration 
solid line shows decision line combined classifier 
plots decision line bagging plotted comparison 
taken 
algorithm adaboost algorithm 
boosting leveraging 
input 
xn yn number iterations 
initialize 


train classifier respect weighted sample set obtain hypothesis ht ht calculate weighted training error ht set update weights yn ht xn log exp xn zt zt normalization constant 
break set 

output ft ht ht yn ht xn 
selecting hypothesis ht weight computed minimizes certain loss function cf 
step 
adaboost minimizes ab exp yn ht xn ft xn ft combined hypothesis previous iteration ft xn xn 
effective approach proposed logitboost algorithm cost function analogous lr log exp yn ht xn ft xn :10.1.1.30.3515
meir tsch important feature compared increases linearly negative values yn ht xn ft xn increases exponentially 
turns difference important noisy situations adaboost tends focus outliers noisy data 
section discuss techniques approach problem 
details concerning logitboost section 
adaboost shown computed analytically leading expression step algorithm 
new combined hypothesis weighting sample updated step algorithm 
initial weighting chosen uniformly called arcing algorithms proposed similar adaboost different loss function 
loss function leads different weighting schemes examples hypothesis coefficients 
known example arc uses fourth order polynomial compute weight examples 
arcing predecessor general leveraging algorithms discussed section 
variant called arc gv shown find linear combination solves linear optimization problem lp cf 
section 
adaboost algorithm binary hard weak learners 
subsequent real valued soft weak learners 
may find hypothesis weight hypothesis ht parallel minimized 
variants boosting algorithms emerged past years operate similarly adaboost possess pac boosting property 
pac boosting property refers schemes able guarantee weak learning algorithms transformed strong learning algorithms sense described section 
duffy helmbold reserve term boosting algorithms pac boosting property proved hold leveraging cases 
overly concerned pac learning se review terms boosting leveraging interchangeably 
principle leveraging approach iteratively selects hypothesis ht time updates weights implemented different ways 
ideally 
ht solves optimization problem hypothesis coefficients 
proposed 
contrast greedy approach original adaboost arc gv algorithm discussed weight hypothesis selected minimizing appropriate cost function :10.1.1.30.3515
show sections relates barrier optimization techniques coordinate descent methods 
additionally briefly discuss relations information geometry column generation techniques cf :10.1.1.135.1357:10.1.1.17.4755
section 
important issue context algorithms discussed review pertains construction weak learner step algorithm 
convex loss function allowed goes infinity margin goes minus infinity zero margin goes infinity 
boosting leveraging step weak learner constructed weighting basically approaches weighting consideration 
approach assume learning algorithm operate reweighted examples 
example weak learner minimizing cost function see section construct revised cost function assigns weight examples heavily weighted examples influential 
example squares error may attempt minimize yn xn weak learners easily adaptable inclusion weights 
approach proposed resampling data replacement distribution approach general applicable weak learner approach widely practice :10.1.1.133.1040
friedman considered sampling approaches general framework described section 
certain situations small samples powerful weak learners advantageous sample subset data full data set different subsets sampled iteration algorithm 
clear distinct advantage approaches 
learning theoretical foundations boosting existence weak learners seen adaboost operates forming re weighting data step constructing base learner order gauge performance base learner define baseline learner 
definition 

dn probability weighting data points lets subset positively labeled points similarly 
set yn dn similarly 
baseline classifier fbl defined fbl sign words baseline classifier predicts 
immediately obvious weighting error baseline classifier 
notion weak learner introduced context pac learning section 
definition limited applications 
context review define weak learner follows 
learner weak learner sample weighting able achieve weighted classification error see strictly smaller 
key ingredient boosting algorithms weak learner required exist order algorithm perform 
context binary classification demand weighted empirical error weak learner strictly smaller edge parameter quantifying deviation performance weak learner baseline classifier introduced definition 
consider weak learner outputs binary classifier data set xn yn pair xn yn weighted non negative weight dn 
demand meir tsch yn xn 
simple weak learners may possible find strictly positive value holds making assumptions data 
example consider dimensional xor problem corresponding labels 
weak learner restricted axis parallel half space clear achieve error smaller uniform weighting examples 
consider situations possible establish strict positivity set lower bound value 
consider mapping binary cube assume true labels yn yn xn 
wish approximate combinations binary hypotheses ht belonging intuitively expect large edge achieved find weak hypothesis correlates cf 
section 
class binary hypotheses boolean functions distribution correlation respect ch sup ed 
distribution free correlation ch ch 
shows log represented exactly sign ht words highly correlated target function exactly represented convex combinations small number functions sufficiently large number boosting iterations empirical error expected approach zero 
interestingly result related min max theorem section 
fig 

single convex set containing positively labeled examples separated negative examples gap 
results address case boolean functions known target function important question arises relates establishment geometric conditions existence weak learner guaranteed 
consider case input patterns belong class weak learners consisting linear classifiers form sign 
hard show distribution training set boosting leveraging distinct points absolute constant 
words find linear classifier achieves edge clear section advantage suffice guarantee generalization 
surprising claim holds arbitrarily labeled points generalization expected 
order obtain larger edge assumptions need data 
intuitively expect situations positively negatively labeled points separated conducive achieving large edge linear classifiers 
consider example case positively labeled points enclosed compact convex region rd remaining points located outside region distance labeled points see 
hard show case edge strictly larger zero independently sample size related number faces smallest convex polytope covers positively labeled points see discussion section 
general situations data may strongly overlapping 
order deal cases advanced tools need theory geometric discrepancy 
technical details development scope 
interested reader referred details 
general establishing geometric conditions existence weak learners types classifiers 
convergence training error zero seen previous section possible appropriate conditions guarantee weighted empirical error weak learner smaller 
show condition suffices ensure empirical error composite hypothesis converges zero number iterations increases 
fact anticipating generalization bounds section somewhat stronger result 
establish claim adaboost algorithm similar claims proven boosting algorithms cf 
section 
keeping mind may real valued function classification want take advantage actual value classification performed sign 
actual value contains information confidence sign predicted 
binary classification define margin example xn yn xn 
consider function defined meir tsch real valued function values 
empirical margin error defined xn 
obvious definition classification error fraction misclassified examples 
addition monotonically increasing 
note uses called margin error defined xn 
noting yf yf follows 
part upper bound generalization error section bound obtain tighter obtained 
theorem :10.1.1.31.2869
consider adaboost described algorithm 
assume round weighted empirical error satisfies ht empirical margin error composite hypothesis ft obeys ft 
proof 
proof case ht :10.1.1.31.2869
showing ft exp 
definition zt yn tht xn yn ht xn te zt yn ht xn definition ft follows exp tht rewrite note boosting leveraging exp tht exp xn zt exp xn zt 
induction 
find xn exp yn tht xn exp exp yn tht xn exp exp zt zt set log algorithm easily implies zt 
substituting result find yields desired result recalling noting 
special case considers training error find ft infer condition suffices guarantee ft 
example condition suffices 
clearly uses binary valued hypotheses sufficient achieve margin zero training examples cf 
lemma 
meir tsch holds positive constant 
fact case ft 
general may interested convergence empirical error zero due overfitting see discussion section 
sufficient conditions convergence error nonzero value 
generalization error bounds section consider binary classifiers 
learning algorithm viewed procedure mapping data set xn yn hypothesis belonging hypothesis class consisting functions 
principle interested quantifying performance hypothesis data 
data consists randomly drawn pairs xn yn data set generated hypothesis random variables 
loss function measuring loss incurred hypothesis classify input true label section expected loss incurred hypothesis expectation taken respect unknown probability distribution generating pairs xn yn 
sequel consider case binary classification loss event occurs zero 
case easy see pr probability misclassification 
classic result vapnik chervonenkis relates empirical classification error binary hypothesis probability error pr 
binary functions notation probability error empirical classification error 
presenting result need define vc dimension class binary hypotheses consider set points 
xn 
binary function defines dichotomy 
xn points 
allowing run elements generate subset binary dimensional cube denoted fx fx 
xn 
vc dimension vcdim defined maximal cardinality set points 
xn fx estimates vc dimension available classes hypotheses 
example ddimensional space finds hyperplanes vc dimension rectangles vc dimension 
results bounds vc dimension various classes 
improved version classic vc bound taken 
theorem 
class valued functions defined set probability distribution suppose samples 
xn yn generated independently random absolute constant integer probability samples length satisfies boosting leveraging vcdim log 
comment original bound contained extra factor log multiplying vcdim 
finite sample bound theorem proved useful theoretical studies 
stressed bound distribution free holds independently underlying probability measure shown theorem optimal rate precise minimax sense 
practical applications far conservative yield useful results 
margin generalization bounds spite claimed optimality bound reason investigate bounds improve 
endeavor may futile light purported optimality observe bound optimal distribution free setting restrictions placed probability distribution fact may want take advantage certain regularity properties data order improve bound 
order retain appealing distribution free feature bound want impose priori conditions idea construct called luckiness function data yields improved bounds situations structure data happens simple 
order effective classifier allow class classifiers real valued 
view discussion section candidate luckiness function margin loss defined 
probability error sign 
real valued classifiers define new notion complexity related vc dimension somewhat general 

sequence valued random variables generated independently setting equal probability 
additionally 
xn generated independently random law rademacher complexity class rn sup nf xn expectation taken respect xn 
rademacher complexity proven essential derivation effective generalization bounds 
basic intuition definition rn interpretation measure correlation class random set labels 
rich function classes expect large value rn small function classes achieve small correlations random labels 
special case class consists binary functions show rn vcdim 
real valued functions needs extend meir tsch notion vc dimension called pseudo dimension case show rn 
hard show vcdim sign 
theorem cf 
provides bound probability misclassification margin loss function 
theorem 
class real valued functions 
probability distribution suppose samples 
xn yn generated independently random integer probability samples length satisfies rn log 
order understand significance theorem consider situation find function achieves low margin error large value margin parameter 
words large values 
case second term smaller corresponding term recall rn classes finite pseudo dimension standard vc bounds 
note theorem implications concerning size margin 
particular assuming class finite vc dimension second term order vcdim 
order term decrease zero mandatory words order lead useful generalization bounds margin sufficiently large respect sample size main significance arises application specific class functions arising boosting algorithms 
recall boosting generates hypothesis written ft tht linear combination non negative coefficients base hypotheses 
ht belonging class consider class functions cot tht ht corresponding normalized function output boosting algorithm 
convex hull letting cot key feature application theorem boosting observation class real valued functions rn cot rn rademacher complexity cot larger base class 
hand ht general non linear functions linear combination tht represents potentially highly complex function lead low margin error ft 
combining observations obtain result improves bounds nature :10.1.1.31.2869
boosting leveraging corollary 
conditions theorem hold set cot 
integer probability samples length cot satisfies rn log 
bound form cot obtained bound depending cot grows linearly leading inferior results 
important observation complexity penalty independent number boosting iterations 
final comment add considerable amount devoted derivation called data dependent bounds second term depend explicitly data 
data dependent bounds depending explicitly weights weak learners 
addition bounds take account full margin distribution 
results particularly useful purpose model selection scope review 
consistency bounds theorems depend explicitly data potentially useful purpose model selection 
interesting question regarding statistical consistency procedures arises 
consider generic binary classification problem characterized class conditional distribution function assumed belong target class functions known case optimal classifier bayes classifier fb sign leading minimal error lb fb 
say algorithm strongly consistent respect sample size generates empirical classifier fn fn lb surely consistency may mainly theoretical significance reassuring guarantee procedure ultimately performs optimally 
turns cases inconsistent procedures perform better finite amounts data consistent ones 
classic example called james stein estimator section 
order establish consistency needs assume prove specific cases class functions cot dense consistency boosting algorithms established related previous 
includes rates convergence specific weak learners target classes point full proof consistency tackle issues 
shown specific algorithm converges function number iterations essentially issue optimization 
furthermore show function algorithm converges converges optimal meir tsch estimator sample size increases statistical issue 
approximation theoretic issue class weak learners sufficiently powerful represent underlying decision boundary addressed 
boosting large margins section discuss adaboost context large margin algorithms 
particular try shed light question conditions boosting yields large margins 
shown adaboost quickly finds combined hypothesis consistent training data :10.1.1.32.8918
indicated adaboost computes hypotheses large margins continues iterating reaching zero classification error :10.1.1.31.2869
clear margin large possible training examples order minimize complexity term 
assumes base learner achieves weighted training error adaboost generates hypothesis margin larger :10.1.1.31.2869
min max theorem linear programming see theorem finds achievable margin 
start brief review standard definitions results margin example hyperplane 
analyze asymptotic properties slightly general version called adaboost equivalent adaboost assuming problem separable 
show subset examples support patterns asymptotically having smallest margin 
weights asymptotically concentrated examples 
furthermore find adaboost able achieve larger margins chosen appropriately 
briefly discuss algorithms adaptively choosing maximize margin 
weak learning edges margins assumption concerning base learning algorithm pac boosting setting cf 
section returns hypothesis fixed set slightly better baseline classifier introduced definition 
means distribution error rate consistently smaller fixed 
recall error rate base hypothesis defined weighted fraction points misclassified cf 

weighting 
dn examples dn dn 
convenient quantity measure quality hypothesis edge applicable useful real valued hypotheses xn 
edge affine transformation case :10.1.1.133.1040
recall section margin function example xn yn defined xn cf 

boosting leveraging assume simplicity finite hypothesis class hj 
suppose combine possible hypotheses known theorem establishes connection margins edges noted connection boosting :10.1.1.133.1040
proof follows directly duality optimization problems 
theorem min max theorem 
min max xn max min yn wj hj xn dimensional probability simplex 
minimal edge achieved possible weightings training set equal maximal margin combined hypothesis refer left hand side edge minimization problem 
problem rewritten linear program lp min dn xn non optimal weightings max 
xn fw wj hj 
base learning algorithm guaranteed return hypothesis edge weighting exists combined hypothesis margin 
lower bound large possible exists combined hypothesis margin exactly hypotheses returned base learner 
discussion derive sufficient condition base learning algorithm reach maximal margin returns hypotheses edges exists linear combination hypotheses margin explains termination condition algorithm step 

theorem stated finite hypothesis sets result holds countable hypothesis classes 
uncountable classes establish results regularity conditions particular real valued hypotheses uniformly bounded cf 

meir tsch avoid confusion note hypotheses indexed elements hypothesis set marked tilde 
hj hypotheses returned base learner denoted 
ht output adaboost similar algorithms sequence pairs ht combined hypothesis ft 
relate theorem 
step adaboost compute weight hypothesis hj way ri hr hj 

easy verify wt hj 
note positive algorithm holds 
geometric interpretation norm margins margins frequently context support vector machines svms boosting :10.1.1.103.1189
called large margin algorithms focus generating hyperplanes functions large margins training examples 
study properties maximum margin hyperplane discuss consequences different norms measuring margin see section 
suppose examples space xn yn xn yn 
note elements feature space elements input space details 
interested separation training set hyperplane origin determined vector assumed normalized respect norm 
norm margin example xn yn respect hyperplane defined yn xn superscript specifies norm respect normalized default 
positive margin corresponds correct classification 
margin hyperplane defined minimum margin examples 
maximize margin hyperplane solve convex optimization problem max max min yn xn :10.1.1.103.1189
simplicity omitted normalization implicit theorem 
easily generalized general hyperplanes introducing bias term 
boosting leveraging form implies loss generality may take leading convex optimization problem max yn xn 

observe case wj obtain linear program lp cf 

formulation clear constraints typically active 
constraints correspond difficult examples called support patterns boosting support vectors svms 
theorem gives geometric interpretation normalize corresponds measuring distance hyperplane dual norm 
theorem 
point plane 
min denotes distance plane measured dual norm margin xn signed distance point hyperplane 
point correct side hyperplane margin positive see illustration theorem 
discuss special cases fig 

maximum margin solution different norms toy example norm solid norm dashed 
margin areas indicated lines 
examples label shown respectively 
maximize norm norm margin solved respectively 
norm leads fewer supporting examples sparse weight vectors 
section discuss relation lagrange multipliers constraints weights examples 
meir tsch boosting 
case boosting space spanned base hypotheses 
considers set base hypotheses generated base learner base hypothesis set constructs mapping input space feature space case margin example cf 
yn xn 
yn wj hj xn wj yn tht xn algorithm norm normalization loss generality assumed non negative assuming closed negation 
theorem maximizes norm distance mapped examples hyperplane feature space mild assumptions show maximum margin hyperplane aligned coordinate axes feature space wj zero cf 
section 
support vector machines 
feature space implicitly defined mercer kernel computes inner product examples feature space 
show kernel exists mapping additionally uses norm normalization euclidean distance measure distances mapped examples hyperplane feature space yn xn yn xn xi xi xj representer theorem shows maximum margin solution written sum mapped examples xn 
adaboost large margins seen section large value margin conducive generalization sense large margin achieved respect data upper bound generalization error small see discussion section 
observation motivates searching algorithms maximize margin 
boosting leveraging convergence properties adaboost 
analyze slightly generalized version adaboost 
introduces new parameter step algorithm chooses weight new hypothesis differently log log algorithm introduced unnormalized arcing exponential function adaboost type algorithm 
similar algorithm proposed see 
call adaboost 
note original adaboost algorithm corresponds choice 
moment assume chose iteration differently consider sequences specified running algorithm computed results obtained running algorithm 
address issue algorithm denoted adaboost able increase margin bound fraction examples margin smaller value 
start result generalizing theorem cf 
section case slightly different loss proposition 
edge ht th step adaboost 
assume 
ft xn algorithm progress products right hand side smaller 
suppose reach margin training examples obviously need assume defined 
question arises sequence find combined hypothesis iterations possible 
show right hand side minimized choice independent base learner performs 
proposition determine upper bound number iterations needed adaboost achieving margin examples maximum margin cf 
corollary 
assume base learner achieves edge adaboost converge solution margin examples log steps 
maximal margins 
methodology reviewed far analyze value maximum margin original adaboost algorithm converges asymptotically 
state lower bound margin achieved adaboost 
find size margin large theoretically theorem 
briefly discuss algorithms able maximize margin 
meir tsch long factor smaller bound decreases 
bounded away converges exponentially fast zero 
corollary considers asymptotic case provides lower bound margin running adaboost forever 
corollary 
assume adaboost generates hypotheses edges 
assume 
smallest margin combined hypothesis asymptotically bounded log log log log 
understand interaction difference small middle term small 
large assuming large choosing larger results larger margin training examples 

conditions corollary compute lower bound hypothesis coefficients iteration 
sum hypothesis coefficients increase infinity linearly 
shown suffices guarantee combined hypothesis large margin larger cf 
section 
section shown maximal achievable margin 
chosen small guarantee suboptimal asymptotic margin 
original formulation adaboost guarantee adaboost achieves margin 
gap theory led called arc gv algorithm marginal adaboost algorithm 
arc gv 
idea arc gv set margin currently achieved combined hypothesis depending previous performance base learner max min 
xn 
simple proof corollary arc gv asymptotically maximizes margin 
idea show converges maximum margin 
proof contradiction monotonically increasing bounded interval converge 
suppose converges value smaller apply corollary show margin combined hypothesis asymptotically larger leads contradiction chosen margin previous iteration 
shows arc gv asymptotically maximizes margin 
experimental results confirm analysis illustrate bound corollary tight 
definition arc gv slightly modified 
original algorithm max 
boosting leveraging marginal adaboost 
unfortunately known fast arc gv converges maximum margin solution 
problem solved marginal adaboost 
adapts different manner 
runs adaboost repeatedly determines line search procedure adaboost able achieve margin increased decreased 
algorithm show fast convergence maximum margin solution 
relation barrier optimization point discussed algorithms seen context barrier optimization 
illustrates optimization point view boosting algorithms related linear programming provide insight generate combined hypotheses large margins 
idea barrier techniques solve sequence unconstrained optimization problems order solve constrained optimization problem 
show exponential function acts barrier function constraints maximum margin lp obtained setting restricting wj max yn xn 
wj wj 
standard methodology exponential barrier function exp find barrier objective problem exp xn wj minimized respect fixed 
denote optimum 
show limit point solution 
holds sequence approximate minimizers approximation better decreasing 
additionally quantities dn exp wj xn chosen dn converge optimal lagrange multipliers constraints mild assumptions see theorem details 
solution edge minimization problem 
see connection boosting barrier approach chose wj sum hypothesis coefficients increases infinity cf 
section converges zero 
plugging choice obtains wj exp wj xn going details show function barrier functions log log barrier log entropic barrier 
meir tsch minimized adaboost algorithm 
essentially obtains exponential loss function adaboost 
seen context adaboost algorithm finds feasible solution margin 
clear important sum hypothesis coefficients goes infinity observation previous analysis cf 
converge constraints may enforced 
arc gv marginal adaboost extensions parameter optimized 
adaboost adaboost arc gv marginal adaboost understood particular implementations barrier optimization approach asymptotically solving linear optimization problem 
leveraging stagewise greedy optimization section focused mainly adaboost 
extend view general ensemble learning methods refer leveraging algorithms 
relate methods numerical optimization techniques 
techniques served powerful tools prove convergence leveraging algorithms cf 
section 
demonstrate convergence ensemble learning methods adaboost logistic regression lr techniques general :10.1.1.135.1357
algorithms common iteratively call base learning algorithm weak learner weighted training sample 
base learner expected return iteration hypothesis ht hypothesis set small weighted training error see large edge see 
hypotheses linearly combined form final composite hypothesis ft 
hypothesis coefficient determined iteration certain objective minimized approximately minimized fixed iterations 
adaboost logistic regression algorithm shown generate composite hypothesis minimizing loss function limit number iterations goes infinity :10.1.1.30.3515
loss depends output combined hypothesis ft training sample 
assumed conditions discussed detail performance base learner strict usually satisfied practice 
parts analysis hold strictly convex cost function legendre type cf 
needs demonstrate existence called auxiliary function cf 
cost function exponential logistic loss 
done general case mild assumptions base learning algorithm loss function 
family algorithms able generate combined hypothesis converging minimum loss function exists 
special cases adaboost logistic regression ls boost 
assuming mild conditions base learning algorithm loss function linear convergence rates type ft boosting leveraging ft fixed shown 
means deviation minimum loss converges exponentially zero number iterations 
similar convergence rates proven adaboost special case separable data cf 
section 
general case rates shown hypothesis set finite 
practice uses infinite sets hypotheses parameterized real valued parameters 
zhang shown order convergence algorithms ft fixed depending loss function 
preliminaries section show adaboost logistic regression leveraging algorithms generate combined hypothesis minimizing particular loss training sample 
composite hypothesis fw linear combination base hypotheses fw lin hh combination coefficients 
find combined function small classification error minimize loss yn sign fw xn 
loss non convex differentiable problem finding best linear combination hard problem 
fact problem provably intractable 
idea loss function bounds loss 
instance adaboost employs exponential loss ab fw exp xn logitboost algorithm uses logistic loss lr fw log exp xn seen loss functions bound classification error :10.1.1.30.3515
loss functions proposed literature 
shown infinite sample limit sample average converges expectation minimizing ab fw meir tsch loss yf loss squared loss logistic loss hinge loss fig 

exponential dashed logistic solid loss functions plotted yf 
bound loss 
lr fw leads classifier obtained minimizing probability error 
precisely ab exp yf set sign pr sign 
denote function minimizing ab 
shown minimizes pr sign achieves bayes risk 
similar argument applies loss functions 
observation forms basis consistency proofs cf 
section 
note exponential logistic losses convex functions 
problem finding global optimum loss respect combination coefficients solved efficiently 
loss functions approach multi class problems ranking problems unsupervised learning regression :10.1.1.32.8860:10.1.1.10.5265:10.1.1.160.9892
see section details approaches 
generic algorithm boosting algorithms common iteratively run learning algorithm greedily select hypothesis addition way weighting examples influence choice base hypothesis 
hypothesis chosen algorithm determines combination weight newly obtained hypothesis 
different algorithms different schemes weight examples determine new hypothesis weight 
section discuss generic method cf 
algorithm connection loss function minimized particular weighting schemes 
proposed algorithms derived scheme 
assume loss function additive form xn yn solve optimization problem general loss functions possible sake simplicity chose additive form 
boosting leveraging algorithm leveraging algorithm loss function 
input 
xn yn 
iterations loss function 
initialize xn yn 

train classifier obtain hypothesis ht set argmin ft ht update ft ft tht 
output ft min min lin ft xn yn 
fw xn yn 
step algorithm example weights initialized derivative loss function respect argument xn yn yn 
step algorithm compute weight wt assigned base hypothesis hj cf 
fw ft tht 
iteration algorithm chooses hypothesis ht corresponds weight coordinate weight updated minimize loss cf 
step ht ht selects single variable optimization time step algorithms called coordinate descent methods 
observe uses adaboost exponential loss algorithm ft xn yn yn exp xn 
distinction original adaboost algorithm cf 
algorithm multiplied labels 
reason different definition edge label 
consider happen chooses hypothesis coordinate 
case hope prove convergence minimum 
assume base learning algorithm performs selecting base hypotheses assume base learner find hypothesis maximal edge minimal classification error hard binary case ht argmax xn 
restrictive assumption cf 
:10.1.1.156.2440
significantly relax condition cf 

meir tsch discuss choice useful 
compute gradient loss function respect weight hypothesis hypothesis set ft xn yn xn xn 
ht coordinate largest component direction gradient vector evaluated optimizes coordinate expect achieve large reduction value loss function 
method selecting coordinate largest absolute gradient component called gauss method field numerical optimization 
known converge mild assumptions see details section 
performing explicit calculation loss functions ab see yields adaboost approach described detail algorithm 
case exp yf implying yn exp xn normalization constant factor yn form algorithm 
logitboost algorithm mentioned section derived similar fashion loss function lr see :10.1.1.30.3515
case find yn exp xn 
observe case weights assigned misclassified examples negative margins smaller exponential weights assigned adaboost 
may help explain logitboost tends sensitive noise outliers adaboost :10.1.1.30.3515
extensions adaboost logitboost algorithms bregman distances proposed earlier cf :10.1.1.17.4755
section 
dual formulation adaboost originally derived results online learning algorithms receives iteration example predicts label incurs loss 
important question setting relates speed algorithm able learn produce predictions small loss 
total loss bounded terms loss best predictor 
derive results bregman divergences generalized projections extensively 
case boosting takes dual view set examples fixed iteration base learning algorithm generates new hypothesis online example 
online learning techniques convergence online learning domain dual domain analyzed shown lead convergence results primal domain cf 
section 
primal domain hypothesis coefficients optimized weighting optimized dual domain 
assumes base hypothesis set closed negation equivalent choosing hypothesis largest absolute component direction gradient 
boosting leveraging adaboost interpreted entropy projection dual domain 
key observation weighting examples th iteration computed generalized projection hyperplane defined linear constraints uses generalized distance measure 
adaboost unnormalized relative entropy dn log dn dn dn dn 
update distribution steps algorithm solution optimization problem generalized projection argmin ht xn new weighting chosen edge previous hypothesis zero observed relative entropy new old distribution small possible 
projection new lies hyperplane defined corresponding constraint cf 

fig 

illustration generalized projections projects point plane finding point plane smallest generalized distance generalized distance equal squared euclidean distance projected point closest common sense point hyperplane 
bregman function finds projected point closeness measured differently 
lead general understanding boosting methods context bregman distance optimization information theory 
arbitrary strictly convex function legendre type called bregman function define bregman divergence generalized distance 
adaboost logistic regression algorithms understood framework algorithms iteratively project different hyper meir tsch planes 
precursor algorithms bregman algorithm known sequence converges point intersection hyperplanes minimizes divergence point adaboost uniform distribution 
solve optimization problem min xn 
interesting questions generalized projections relate coordinate descent discussed section 
shown see generalized projection dual domain hyperplane defined cf 
corresponds optimization variable corresponding primal domain 
particular uses bregman function corresponds coordinate wise descent loss function wj hj hj hj 
hj xn convex conjugate case adaboost uses dn log dn leading relative entropy zero uniform 
convergence results lot establishing convergence algorithms similar algorithm 
discuss approaches detail provide overview different results obtained past years cf 
table 
paragraphs briefly discuss aspects important proving convergence algorithms 
conditions loss function 
order prove convergence global minimum usually assume loss function convex local minimum global 
allows arbitrary loss functions show convergence local minimum gradient converges zero 
prove convergence usually assumes function reasonably smooth measure smoothness appears convergence rates 
results important second derivatives strictly positive loss function strictly convex function flat algorithm get stuck 
linear programming approach column generation simultaneously projection growing set hyperplanes considered 
details described section 
algorithms equality constraints replaced inhomogeneous inequality constraints projects hyperplane constraint satisfied 
case bregman functions convex conjugate 
boosting leveraging relaxing conditions base learner 
section assumption base learner returns hypothesis maximizing edge cf 

practice difficult find hypothesis exactly minimizes training error maximizes edge 
relaxations necessary 
simplest approaches assume base learner returns hypothesis edge larger say relaxed 
condition show adaboost training error converges exponentially zero 
edge hypothesis equal gradient respect weight mean gradient converge zero 
case adaboost happens converges minimum loss length solution vector converge cf 
section 
realistically assume base learner returns hypothesis approximately maximizes edge compared best possible hypothesis 
considered additive term converging zero relaxed multiplicative term relaxed 
summarize senses approximate edge maximization relaxed relaxed relaxed relaxed xn argmax xn nh xn xn argmax nh xn xn argmax nh xn fixed constants strictly monotone continuous function 
size hypothesis set 
convergence results size hypothesis set matter 
order able attain minimum assume set bounded closed cf 

finite hypothesis sets assumed order show convergence rates 
case classification base hypotheses discrete valued holds true 
practice wants real valued parameterized hypotheses 
hypothesis set uncountable 
presents general convergence results algorithm related algorithm depend size hypothesis sets applicable uncountable hypothesis sets 
note label yn may absorbed meir tsch regularization terms hypothesis coefficients 
section discuss regularization approaches leveraging algorithms 
important ingredient penalty term hypothesis coefficients seeks optimize min rj fw xn yn cp regularization constant regularization operator norm 
seen adaboost implicitly penalizes norm cf 
section true algorithm described 
norm discussed 
small fraction analyses literature allow regularization term 
note regularized loss functions optimality conditions base learner change slightly additional term depending regularization constant cf 

regularization functions discussed section 
convergence rates 
convergence result obtained adaboost relaxed condition base learner 
showed objective function reduced step factor 
decreases exponentially 
convergence speed directly depend size hypothesis class number examples 
generalized essentially exponentially decreasing functions logistic regression relaxed condition cf 

finite hypothesis sets strictly convex loss show exponential decrease loss minimum see constants depend heavily size training set number base hypotheses 
best known rate convex loss functions arbitrary hypothesis sets 
rate constants depend smoothness loss function 
approach mandatory regularize norm 
additionally proposed algorithm exactly fit scheme discussed see details 
convexity respect parameters 
practical implementations boosting algorithms weak learners parameterized finite set parameters 
optimization needs performed respect respect weak hypothesis directly 
loss function convex respect necessarily convex respect 
problem may cause numerical problems dwell review 
robustness regularization soft margins shown boosting rarely overfits low noise regime clearly higher noise levels boosting leveraging table 
summary results convergence greedy approximation methods ref 
cost function base hypothesis convergence rate learner set exponential relaxed uncountable global minimum loss loss zero lipschitz strict uncountable gradient zero convex lips strict uncountable global minimum possible convex posi strict countable global minimum tive mandatory essentially relaxed uncountable global minimum exp loss zero ing strictly convex differentiable strict finite global optimum dual problem linear strict finite global minimum mandatory squared loss relaxed uncountable global minimum loss mandatory strictly convex differentiable relaxed finite global minimum unique dual sol 
possible linear relaxed uncountable global minimum compact possible strictly convex relaxed uncountable global minimum compact mandatory convex strict uncountable global infimum mandatory includes piecewise linear convex functions soft margin insensitive loss 
extended relaxed 
technical assumptions 
functions usually refereed functions legendre type 

section summarize techniques yield state ofthe art results extend applicability boosting noisy case 
margin distribution central understanding boosting 
shown section adaboost asymptotically achieves hard margin separation algorithm concentrates resources hard learn examples 
hard margin clearly sub optimal strategy case noisy data kind regularization introduced algorithm meir tsch alleviate distortions single difficult examples outliers cause decision boundary 
discuss techniques approach problem 
start algorithms implement intuitive idea limiting influence single example 
trades influence margin discuss brownboost gives examples achieve large margins number iterations 
discuss prevents overfitting disallowing overly skewed distributions briefly discuss approaches flavor 
section discuss second group approaches motivated margin bounds reviewed section 
important insight minimal margin maximized margin distribution important accept small amounts training set leads considerable increments margin 
describe doom approach uses non convex monotone upper bound training error motivated margin bounds 
discuss linear program lp implementing soft margin outline algorithms iteratively solve linear programs 
techniques modifying adaboost margin loss function achieve better noise robustness 
doom lp approaches employ margins correspond penalized cost function 
section discuss issues related choices penalization term 
reducing influence examples weights examples hard classify increased step adaboost 
leads adaboost tendency exponentially decrease training error generate combined hypotheses large minimal margin 
discuss suboptimal performance hard margin classifier presence outliers mislabeled examples way analyze 
consider noise free case left 
estimate separating hyperplane correctly 
middle outlier corrupts estimation 
hard margin algorithms concentrate outlier impair estimate obtained absence outlier 
consider complex decision boundaries cf 
right 
overfitting problem pronounced generate complex functions combining hypotheses 
training examples mislabeled ones outliers classified correctly result poor generalization 
cartoons apparent adaboost algorithm large hard margins noise sensitive 
need relax hard margin allow possibility data 
algorithms address issue 
boosting leveraging fig 

problem finding maximum margin hyperplane reliable data left data outlier middle mislabeled example right 
solid line shows resulting decision boundary dashed line marks margin area 
middle left original decision boundary plotted dots 
hard margin implies noise sensitivity example spoil estimation decision boundary 
taken 

examples mislabeled usually difficult classify forced positive margin 
knew examples unreliable simply remove training set alternatively require large margin 
assume defined non negative mistrust parameter expresses mistrust example xn yn 
may relax hard margin constraints cf 
leading max yn xn priori chosen constant 
maximize margin modified constraints cf 
section 
obvious questions arise discussion determine second incorporate modified constraints boosting algorithm 
algorithm tries solve problems simultaneously 
uses example weights computed iteration determine examples highly influential hard classify 
assuming hard examples noisy examples algorithm chooses mistrust parameter iteration amount example xn yn influenced decision previous iterations rd note mark parameters hyperplane feature space denote coefficients generated algorithm see section 
meir tsch weights hypotheses examples respectively 
defines new quantity soft margin example xn yn replacement margin adaboost algorithm 
idea find solution large soft margin examples maximize case expects observe overfitting 
problems lacks convergence proof 
additionally clear underlying optimization problem modification done algorithmic level difficult relate optimization problem 
boosting algorithm achieved state art generalization results noisy data cf 

experimental evaluations algorithm best performing ones cf 

brownboost 
brownboost algorithm boosting majority bbm algorithm 
important difference bbm adaboost bbm uses pre assigned number boosting iterations 
algorithm approaches predetermined termination examples large negative margins eventually correctly labeled 
algorithm gives examples concentrates effort examples margin say small negative number 
influence difficult examples reduced final iterations algorithm 
similar spirit algorithm examples difficult classify assigned reduced weights iterations 
bbm needs pre specify parameters upper bound error weak learner cf 
section ii target error 
shown get rid parameter adaboost 
target error interpreted training error wants achieve 
noisy problems aim non zero training error noise free separable data set zero 
shown letting brownboost algorithm approach zero recovers original adaboost algorithm 
derive brownboost thought experiment inspired ideas theory brownian motion assume base learner returns hypothesis edge larger 
fix define new hypothesis equal probability random 
easy check expected edge 
edge small change combined hypothesis example weights small 
hypothesis may edge greater iterations depending need call base learner 
idea brownboost approach zero analyze dynamics differential equations 
resulting brownboost algorithm similar adaboost additional terms example weighting depend remaining number iterations 
addition iteration needs solve differential equation boundary conditions compute long hypothesis survives described cycle boosting leveraging determines weight 
details algorithm theoretical results performance 
idea algorithm promising aware empirical results illustrating efficiency approach 

skewness distributions generated boosting process suggests approach reducing effect overfitting impose limits skewness 
promising algorithm lines suggested earlier 
algorithm designed effectively malicious noise provably effective scenario appropriate conditions 
algorithm similar adaboost maintaining set weights boosting iteration cutoff weight assigned examples negative margins 
version described requires parameters input measures desired error rate final classifier ii measures guaranteed edge hypothesis returned weak learner 
parameters shown converge finite number steps composite hypothesis xn 
shown weights generated process obey weights large compare lp section 
convergence time larger adaboost compensated robustness property arising smoothness distribution 
observe operates binary real valued hypotheses 
promising algorithm dealing noisy situations kept mind fully adaptive need supplied advance cf 

aware applications real data 
approaches aimed directly reducing effect difficult examples 
optimization margins return analysis adaboost margin distributions discussed section 
consider base class binary hypotheses characterized vc dimension vcdim 
fact rn vcdim conclude probability random draw training set size generalization error function margins 
bounded vcdim log cf 
corollary 
recall surprisingly bound independent number hypotheses ht contribute meir tsch defining tht 
stated reason success adaboost compared ensemble learning methods bagging generates combined hypotheses large margins training examples :10.1.1.32.9399
asymptotically finds linear combination fw base hypotheses satisfying xn wj 
large margin cf 
section 
term zero second term small large 
algorithms proposed generate combined hypotheses larger margins adaboost 
shown margin increases generalization performance better data sets noise see 
problems large amount noise generalization ability degrades hypotheses larger margins see 
section discussed connections boosting margin maximization 
algorithms consideration approximately solve linear programming problem tend perform sub optimally noisy data 
margin bound surprising 
minimum right hand side necessarily achieved maximum hard margin largest 
event keep mind upper bound sophisticated bounds looking full margin distribution opposed single hard margin may lead different 
section discuss algorithms number margin errors controlled able control contribution terms separately 
discuss doom approach extended linear program lp problem allows margin errors 
additionally briefly discuss approaches solve resulting linear program 
doom 
direct optimization margins basic idea replace adaboost exponential loss function loss function normalized hypothesis coefficients 
order defines class admissible margin cost functions parameterized integer loss functions motivated margin bounds kind discussed section free parameter 
large value corresponds high resolution high effective complexity convex combination small margin smaller values correspond larger smaller effective complexity 
admissibility condition ensures loss functions appropriately take care trade complexity empirical error 
motivation omit derived optimal family margin loss functions margin bound 
unfortunately loss function non monotone non convex cf 
left leading difficult optimization problem np hard 
boosting leveraging idea proposed replace loss function piece wise linear loss function monotone non convex cf 
right 
optimization margin loss function global optimum difficult heuristics optimization proposed 
theoretically prove convergence local minimum cf 
section 
fig 

left optimal margin loss functions cm compared loss 
right piecewise linear upper bound functions cm loss taken 
despite mentioned problems doom approach promising 
experimental results shown noisy situations considerably improve performance compared adaboost 
additional regularization parameter needs chosen appropriately cross validation 
linear programming approaches 
discuss ideas aimed directly controlling number margin errors 
doing able directly control contribution terms separately 
discuss extended linear program lp problem analyze solution review algorithms solving optimization problem 
lp problem 
consider case set hj 
hypotheses 
find coefficients combined hypothesis fw extend linear problem solve linear optimization problem see call lp problem meir tsch max xn 
wj 

wj parameter algorithm 
force margins large obtains soft margin hyperplane 
dual optimization problem cf 
edge minimization problem additional constraint dn dn lagrange multiplier associated constraint th example size characterizes example influences solution 
sense lp implements intuition discussed section 
addition direct connection algorithm 
proposition shows immediate interpretation proposition property :10.1.1.160.9892
solution optimization problem possesses properties 
upper bounds fraction margin errors 

greater fraction examples margin larger 
slack variables enter cost function linearly gradient constant absolute size important 
loosely speaking due fact optimum primal objective function derivatives primal variables matter derivative linear function constant 
explicit example outside margin area satisfying xn shifted arbitrarily long enter margin area 
example exactly edge margin area xn local movement possible changing solution 
example xn yn margin area shown local movements allowed cf 
details 
column generation approach 
algorithm solving proposed see early 
uses column generation cg method known field numerical optimization section 
basic idea column generation iteratively construct optimal ensemble restricted subset hypothesis set iteratively extended 
solve considers dual optimization problem min xn 
dn dn 
see soft margin loss uses scaled version 
boosting leveraging iteration solved small subset hypotheses ht base learner assumed find hypothesis ht violates constraint cf 
relaxation section 
exists hypothesis added restricted problem process continued 
corresponds generating column primal lp 
hypotheses satisfy constraint current dual variables ensemble primal variables optimal constraints full master problem fulfilled 
resulting algorithm special case set algorithms known exchange methods cf 
section 
methods known converge cf 
finite infinite hypothesis sets 
best knowledge convergence rates known base learner able provide hypotheses columns expected converge faster simply solving complete linear program 
practice column generation algorithm halts optimal solution relatively small number iterations 
experimental results show effectiveness approach 
compared adaboost achieve considerable improvements generalization performance 
barrier algorithm 
idea derive barrier algorithm lines section 
problem reformulated removes constant objective fixes adds sum hypothesis coefficients multiplied regularization constant 
fact show modified problem solution find problems solution scaling 
corresponding barrier objective problem derived section 
sets parameters combination coefficients slack variables rn setting wecan find minimizing plugs obtains xn wj log exp 
sets regularize obtains formulation close logistic regression approach :10.1.1.135.1357
current approach understood leveraging algorithm section regularization 
furthermore approach zero scaled logistic loss converges soft margin loss max xn 
techniques discussed section easily derive algorithms optimize fixed certain precision 
needs take care norm regularization directly incorporated coordinate descent algorithm 
idea reduce certain precision reached optimization continued modified loss function 
chooses accuracy rate decrease appropriately obtains practical algorithm show asymptotic convergence finite hypothesis sets see 
meir tsch earlier realization idea proposed termed arc idea reformulate linear program soft margin non linear program maximum hard margin appropriately redefining margin similar section 
heuristic arc gv cf 
section employed maximize newly defined margin 
barrier approach arc led considerable improvements compared adaboost large range benchmark problems 
try solve similar optimization problem column generation algorithm expect minor differences 
practice column generation algorithm converges faster problem combination base learners cg approach example weights zero iterations margin examples larger margin constraints active 
base learner may problems generate hypotheses 
barrier approach smoother effect reduced considerably 
regularization terms sparseness discuss main choices regularizers drastic influence properties solution regularized problems 
consider optimization problems section regularization term objective min fw xn yn wj rj regularization constant loss function differentiable monotonically increasing function 
simplicity presentation assume hypothesis set finite statements hold countable hypothesis sets case uncountable sets assumptions required cf 
theorem 
say set hypothesis coefficients see sparse contains non zero elements 
set sparse contains non zero elements feature space assumed higher dimensional infinite dimensional 
interested formulations form optimization problem tractable time lead sparse solutions 
motivation aspect clear motivation sparsity leads superior generalization results smaller computationally efficient ensemble hypotheses 
case infinite hypothesis spaces sparsity leads precise representation terms finite number terms 
matlab implementation downloaded anu edu au software 
force non negative done loss generality hypothesis set closed negation 
boosting leveraging consider norm regularizer wj wj 
assume optimal solution 
regularizer linear dimensional subspace having norm restricting fw xn fw xn 
obtains additional constraints 
choose dimensional space leads objective value exists solution non zero entries solution sparse 
observations holds arbitrary loss functions concave regularizer norm 
note upper bound number non zero elements practice sparser solutions observed 
neural networks svms matching pursuit algorithms uses norm regularization 
case optimal solution expressed linear combination mapped examples feature space cf 
xn 
vectors xn sparse zero solution sparse linear combination non sparse vectors 
consider instance case vectors hj 
hj xn 
interpreted points dimensional space general position subset points span full dimensional space 
instance occurs probability points drawn independently random probability density supported space 
show fraction coefficients zero 
large coefficients non zero solution sparse 
holds norm regularization strictly convex regularizer 
observation intuitively explained follows dimensional subspace leading output fw xn training examples 
assume solution sparse small number large weights 
regularizer strictly convex reduce regularization term distributing large weights weights previously zero keeping loss term constant 
conclude type regularizer determines exists optimal hypothesis coefficient vector sparse 
minimizing convex loss function strictly concave regularizer generally lead nonconvex optimization problems potentially local minima 
fortunately norm concave convex 
employing norm regularization obtain sparse solutions retaining computational tractability optimization problem 
observation lend strong support norm regularization 
meir tsch extensions bulk discussed review deals case binary classification supervised learning 
general boosting framework widely applicable 
section brief survey extensions generalizations exist :10.1.1.44.207:10.1.1.72.7289:10.1.1.10.5265
single class classical unsupervised learning task density estimation 
assuming unlabeled observations xn generated independently random unknown distribution task estimate related density function 
difficulties task 
density function need exist distributions possess density 
second estimating densities exactly known hard task 
applications estimate support data distribution full density 
single class approach avoids solving harder density estimation problem concentrate simpler task estimating quantiles multivariate distribution 
far independent algorithms solve problem boosting manner 
mainly follow ideas proposed kernel feature spaces benefit better interpretability generated combined hypotheses see discussion :10.1.1.39.912:10.1.1.160.9892
algorithms proposed differ way solve similar optimization problems uses column generation techniques uses barrier optimization techniques cf :10.1.1.160.9892
section 
brevity focus underlying idea algorithmic details 
svm case computes hyperplane feature space spanned hypothesis set cf 
section pre specified fraction training example lie hyperplane time demand hyperplane maximal distance margin origin illustration see 
realized solving linear optimization problem min xn 

essentially class soft margin approach cf 
section labels equal 
derivation appropriate optimization algorithms similar 
multi class interest extending boosting multi class problems number classes exceeds 

xn yn origin outliers boosting leveraging fig 

illustration single class idea 
hyperplane feature space constructed maximizes distance origin allowing outliers 
taken 
data set class problem yn dimensional vector form 

witha th position xn belongs th class 
log likelihood function constructed see page yn log xn yn log xn yn th component vector yn xn model probability xn belongs th class 
standard softmax representation ef substituting obtain function similar form considered far 
function optimized stagewise fashion described section yielding solution multi class problem 
approach essentially :10.1.1.30.3515
approaches multi category classification suggested applied text classification 
recall boosting weights examples poorly classified previous weak learners amplified 
basic idea methods suggested maintain set weights examples labels 
boosting progresses training examples corresponding labels hard predict correctly get increasingly higher weights 
idea led development multi category algorithms titled adaboost mh adaboost details algorithms 
addition approaches extensive activity recoding multi class problems sequence binary problems meir tsch idea error correcting codes 
detailed description approach general context boosting setup 
special cases general approach called pairs approaches extensively prior development error correcting code approach 
case class classification problem constructs usually soft binary classifiers learns distinguish class rest 
multi category classifier uses highest ranking soft classifier 
pairs approach possible pairs classification problems learned form multi category classifier form majority voting 
soft classifiers helps removing possible redundancies 
connections multi class boosting algorithms column generations techniques discussed 
comment interesting problem relates situations data multi class multi labeled sense input may belong classes 
particularly important context text classification single document may relevant topics sports politics violence 
regression learning problem regression data set xn yn contrast classification case variable take real values restricted finite set 
probably approach addressing regression context boosting appeared problem addressed translating regression task classification problem see 
boosting algorithms binary classification described review greedy stage wise minimization smooth cost function 
surprising directly smooth cost function context regression trying estimate smooth function 
probably discuss regression context stage wise minimization appeared ls boost algorithms extended 
addressed regression connection barrier optimization essentially sided soft margin loss insensitive loss 
approach similar spirit approaches proposed see 
extended arbitrary strictly convex loss functions discussed connection infinite hypothesis sets semi infinite programming 
resulting algorithms shown useful real world problems 
briefly describe regression framework introduced boosting algorithms introduced 
algorithms operate scheme different cost functions algorithm 
iteration algorithm modifies sample produce new sample 
xn yn modified residual error stage 
ii distribution modified samples constructed base regression algorithm called 
iii base learner produces function edge definition boosting leveraging edge regression context see 
iv new composite regressor form formed selected solving dimensional optimization problem 
theoretical performance guarantees regression algorithms introduced analyzed similar fashion section binary classification 
assumption strictly positive edge obtained step shown training error converges exponentially fast zero analogous theorem classification 
significantly optimality base learners required claim similarly related results described section classification 
generalization error bounds derived analogous theorem 
bounds depend edge achieved weak learner 
localized boosting boosting approaches discussed review construct composite ensemble hypothesis form tht 
observe combination parameters depend input contribute equally point space 
alternative approach construction composite classifiers allow coefficients depend leading hypothesis form ht 
interpretation form functional representation appealing 
assume hypothesis ht represents expert assigns non negative weight prediction th expert assume 
input experts prediction ht prediction weighted confidence parameter 
note indicator functions input single hypothesis ht considered 
linear hypotheses think approach representation general non linear function piece wise linear functions 
type representation forms basis called mixture experts models 
important observation concerns complexity functions 
clearly allowing functions arbitrarily complex functions easily fit finite data set 
stresses importance regularization approach attempting construct mixture expert representation 
boosting approach construction mixture experts representations proposed termed localized boosting 
basic observations relationship classic mixture models statistics em algorithm proven effective applicability general greedy stagewise gradient descent approaches described section 
algorithm developed termed thought stagewise em algorithm similarly boosting algorithms single hypothesis added ensemble stage functions estimated step 
regularization achieved restricting simple parametric forms 
addition algorithm described detail gen meir tsch bounds similar theorem established 
algorithm applied real world data sets leading performance competitive state art algorithms 
extensions briefly mention extensions boosting algorithms 
method introduced learning multiple models different possibly overlapping sets features 
way robust learning algorithms constructed 
algorithm combining bagging boosting aimed improving performance case noisy data introduced 
procedure simply generates set bootstrap samples bagging generates boosted classifier sample combines results uniformly 
online version boosting algorithm shown comparable accuracy boosting faster terms running time 
extensions listed section 
evaluation applications choice weak learners boosting crucial ingredient successful application boosting algorithms construction weak learner 
pointed section weak learner weak guarantee adequate performance composite hypothesis 
hand overly complex weak learner may lead overfitting severe algorithm proceeds 
stressed section regularization addition astute choice weak learner plays key role successful applications 
empirically observed base learner performs quite slightly simple data hand best suited boosting 
uses bagging base learners slightly complex perform best 
additionally real valued soft hypotheses lead considerable better results 
briefly mention weak learners successfully applications 
decision trees stumps 
decision trees widely years statistical literature powerful effective easily interpretable classification algorithms able automatically select relevant features 
surprising successful initial applications boosting algorithms performed decision trees weak learners 
decision trees recursive partition input matlab implementation algorithm part extensive pattern recognition toolbox downloaded tiger technion ac il classification index htm 
boosting leveraging space set nested regions usually rectangular shape 
called decision stump simply level decision tree classifier formed splitting input space axis parallel fashion halting 
systematic approach effective utilization trees context boosting approaches logistic regression decision trees described 
showed boosting significantly enhances performance decision trees stumps 
neural networks 
neural networks extensively applications 
feed forward neural network far widely practice essentially highly non linear function representation formed repeatedly combining fixed non linear transfer function 
shown real valued continuous function arbitrarily approximated feed forward neural network single hidden layer long transfer function polynomial 
potential power neural networks representing arbitrary continuous functions easily lead overfitting effectively context boosting 
careful numerical experiments conducted demonstrated adaboost significantly improve performance neural network classifiers real world applications ocr 
kernel functions linear combinations 
problems neural networks weak learners optimization problems unwieldy 
interesting alternative forming weak learner linearly combining set fixed functions kgk 
functions gk example kernel functions gk xk centered training examples 
set infinite hypothesis set unbounded unbounded 
maximizing edge discussed section lead diverging keeping mind overfitting issues discussed section attempt restrict complexity class functions limiting norm 
discuss approaches 
norm penalized combinations 
set rn 
finding edge maximizing closed form solution 
xn argmax 


maximizes edge sign xk xn means adding exactly basis function gk iteration 
approach reduces case meir tsch single functions 
variant approach 
related classification function approach solving drug discovery task 
called active kernel functions set functions kernel functions hn kp xn kp svm kernel function parameterized variance rbf kernel 
case find example xn parameter maximize edge 
may lead hard non convex optimization problem quite efficient heuristics proposed 
norm penalized combinations 
way penalize norm combination coefficients 
case rn attempts solve argmin dn xn problem particularly simple solution xn 
similar approach form rbf networks connection regularized versions boosting 
evaluation benchmark data sets boosting leveraging applied benchmark problems 
adaboost algorithm early variants tested standard data sets uci repository compare favorably state art algorithms see example 
clear adaboost tends overfit data noisy regularization enforced 
experiments regularized forms boosting described section lead superior performance noisy real world data 
fact algorithms significantly better original versions boosting comparing favorably best classifiers available svms 
refer reader results benchmark studies 
results place regularized boosting techniques standard toolbox data analysis techniques 
applications list applications boosting leveraging methods growing rapidly 
discuss applications detail list range applications 
non intrusive power monitoring system 
regularized approaches described section arc applied problem power appliances monitoring 
difficult problem power companies handling short term peak loads additional power plants need built provide security peak load power failure 
boosting leveraging prerequisite controlling electric energy demand ability correctly non detect classify operating status electric appliances individual households 
goal develop non intrusive measuring system assessing status electric appliances 
hard problem particular appliances inverter systems non intrusive measuring systems developed conventional non inverter operating electric cf 

study presents evaluation machine learning techniques classify operating status electric appliances inverter purpose constructing non intrusive monitoring system 
study rbf networks nearest neighbor classifiers svms arc cf 
section compared 
data set available task small examples collection labeling data manual expensive 
result careful finding model parameters 
model parameters computationally expensive generally reliable leave method 
results reported demonstrate arc algorithm rbf networks base learner performs better average algorithms studied followed closely svm classifier rbf kernel 
results suggest goal control system balance load peaks feasible prospect distant 
tumor classification gene expression data 
micro array experiments generate large datasets expression values thousands genes dozen examples 
accurate supervised classification tissue samples high dimensional problems difficult crucial successful diagnosis treatment typical cases sample size range number features varies clearly potential overfitting huge 
goal predict unknown class label new individual basis gene expression profile 
task great potential value attempts develop effective classification procedures solve 
early applied adaboost algorithm data results disappointing 
applied logitboost algorithm decision trees base learners modifications achieved state art performance difficult task :10.1.1.30.3515
turned order obtain high quality results necessary preprocess data scoring individual feature gene discriminatory power non parametric approach details 
simple approach multi category classification led better results direct multi class approach log likelihood function cf :10.1.1.30.3515
section 
interestingly authors quality inverter system controls example rotation speed motor changing frequency electric current 
meir tsch results degraded little function number boosting iterations steps 
somewhat surprising small amount data danger overfitting 
success approach compared results achieved adaboost tends corroborate assertions concerning effectiveness logitboost approach noisy problems :10.1.1.30.3515
text classification 
problem text classification playing increasingly important role due vast amount data available web internal repositories 
problem particularly challenging text data multi labeled text may naturally fall categories simultaneously sports politics violence 
addition difficulty finding appropriate representation text open 
approaches boosting text classification 
particular approaches multi class multi label classification developed task 
weak learner simple decision stump single level decision tree terms consisting single words word pairs 
text categorization experiments reported applied standard text classification benchmarks reuters ap titles usenet groups demonstrated approach yielded general better results methods compared 
additionally observed boosting algorithms real valued soft weak learners performed better algorithms binary weak learners 
reported drawback approach large time required training 
applications 
briefly mention applications boosting leveraging methods problems 
group involved applications boosting approaches text classification task discussed 
example problems text filtering routing addressed ranking combining 
problem combining prior knowledge boosting call classification spoken language dialogue studied applications problem modeling auction price uncertainty introduced 
applications boosting methods natural language processing reported approaches melanoma diagnosis 
applications pose invariant face recognition lung cancer cell identification volatility estimation financial time series developed 
detailed list currently known applications boosting leveraging methods posted web boosting homepage www boosting org applications 
boosting leveraging general overview ensemble methods general boosting leveraging family algorithms particular 
boosting introduced theoretical framework order transform poorly performing learning algorithm powerful idea turned manifold extensions applications discussed survey 
shown boosting turns belong large family models greedily constructed adding single base learner pool previously constructed learners adaptively determined weights 
interestingly boosting shown derivable stagewise greedy gradient descent algorithm attempting minimize suitable cost function 
sense boosting strongly related algorithms known statistics literature years particular additive models matching pursuit 
boosting brought fore issues studied previously 
important concept margin impact learning generalization emphasized 
ii derivation sophisticated finite sample data dependent bounds possible 
iii understanding relationship strength weak learner quality composite classifier terms training generalization errors 
iv establishment consistency development computationally efficient procedures 
emphasis boosting notion margin clear boosting strongly related successful current algorithm support vector machine 
pointed boosting svm viewed attempting maximize margin norm procedure different 
optimization procedures cases different 
boosting constructs complex composite hypothesis principle represent highly irregular functions generalization bounds boosting turn lead tight bounds cases large margins guaranteed 
initial indicate boosting overfit soon realized overfitting occur noisy conditions 
observation regularized boosting algorithms developed able achieve appropriate balance approximation estimation required achieve excellent performance noisy conditions 
regularization essential order establish consistency general conditions 
conclude open questions 

possible derive boosting algorithms types cost functions point systematic approach selection particular 
numerical experiments theoretical results indicate choice cost function may significant effect performance boosting algorithms 

selection best type weak learner particular task entirely clear 
weak learners unable principle represent meir tsch complex decision boundaries overly complex weak learners quickly lead overfitting 
problem appears strongly related notoriously difficult problem feature selection representation pattern recognition selection kernel support vector machines 
note single weak learner boosting include multi scaling information svms fix kernel inducing kernel hilbert space 
interesting question relates possibility different types weak learners different stages boosting algorithm may emphasize different aspects data 

issue related previous question existence weak learners provable performance guarantees 
section discussed sufficient conditions case linear classifiers 
extension results general weak learners interesting difficult open question 

great deal devoted derivation flexible data dependent generalization bounds depend explicitly algorithm 
bounds usually tighter classic bounds vc dimension ample room progress final objective develop bounds model selection actual experiments real data 
additionally interesting develop bounds efficient methods compute leave error done svms 

optimization section discussed different approaches addressing convergence boosting algorithms 
result general far includes special cases analyzed 
convergence rates optimal additional effort needs devoted finding tight bounds performance 
addition question possible establish super linear convergence variant leveraging ultimately lead efficient leveraging algorithms 
algorithms parameterized weak learners case cost function minimized weak learners convex respect parameters see section 
interesting see problem circumvented designing appropriate cost functions 

klaus 
ller discussions contribution writing manuscript 
additionally sebastian mika takashi onoda bernhard sch lkopf alex smola tong zhang valuable discussions 
acknowledges partial support center electrical engineering department technion fund promotion research technion 
gratefully acknowledge partial support dfg ja mu nsf eu neurocolt ii 
furthermore gunnar tsch uc santa cruz tokyo fraunhofer berlin warm hospitality 
boosting leveraging 
abney schapire singer 
boosting applied tagging pp attachment 
proc 
joint sigdat conference empirical methods natural language processing large corpora 

akaike 
new look statistical model identification 
ieee trans 
automat 
control 

allwein schapire singer 
reducing multiclass binary unifying approach margin classifiers 
journal machine learning research 

anthony bartlett 
neural network learning theoretical foundations 
cambridge university press 

gl linder lugosi 
data dependent margin generalization bounds classification 
jmlr 

aslam 
improving algorithms boosting 
proc 
colt san francisco 
morgan kaufmann 

hlmann 
volatility estimation functional gradient descent high dimensional financial time series 
journal computational finance 
appear 
see stat ethz ch html 

barnes 
capacity control boosting convex hull 
master thesis australian national university 
supervised williamson 

bartlett lugosi 
model error estimation 
machine learning 

bartlett bousquet mendelson 
localized rademacher averages 
procedings colt volume lnai pages sydney 
springer 

bartlett mendelson 
rademacher gaussian complexities risk bounds structural results 
journal machine learning research 
appear 

bauer kohavi 
empirical comparison voting classification algorithm bagging boosting variants 
machine learning 

borwein 
legendre functions method random bregman projections 
journal convex analysis 

ben david long mansour 
agnostic boosting 
proceedings fourteenth annual conference computational learning theory pages 

bennett mangasarian 
multicategory separation linear programming 
optimization methods software 

bennett demiriz maclin 
exploiting unlabeled data ensemble methods 
proc 
icml 

bennett mangasarian 
robust linear programming discrimination linearly inseparable sets 
optimization methods software 


boosting algorithm regression 
gerstner 
nicoud editors proceedings icann int 
conf 
artificial neural networks volume lncs pages berlin 
springer 

bertsekas 
nonlinear programming 
athena scientific belmont ma 

bishop 
neural networks pattern recognition 
oxford university press 
meir tsch 
blumer ehrenfeucht haussler warmuth 
occam razor 
information processing letters 

boser guyon vapnik 
training algorithm optimal margin classifiers 
haussler editor proceedings th annual acm workshop computational learning theory pages 

bradley mangasarian 
feature selection concave minimization support vector machines 
proc 
th international conf 
machine learning pages 
morgan kaufmann san francisco ca 

bregman 
relaxation method finding common point convex sets application solution problems convex programming 
ussr computational math 
math 
physics 

breiman 
bagging predictors 
machine learning 

breiman 
bias variance arcing classifiers 
technical report statistics department university california july 

breiman 
prediction games arcing algorithms 
neural computation 
technical report statistics department university california berkeley 

breiman 
infinity theory predictor ensembles 
technical report berkeley august 

breiman friedman olshen stone 
classification regression trees 
wadsworth 


boosting polynomially bounded distributions 
jmlr pages 
accepted 

yu 
boosting loss regression classification 
amer 
statist 
assoc 
revised technical report stat dept uc berkeley august 

campbell bennett 
linear programming approach novelty detection 
leen dietterich tresp editors advances neural information processing systems volume pages 
mit press 


non intrusive appliance load monitoring system 
journal electric power research institute 

censor zenios 
parallel optimization theory algorithms application 
numerical mathematics scientific computation 
oxford university press 

cesa bianchi krogh warmuth 
bounds approximate steepest descent likelihood maximization exponential families 
ieee transaction information theory july 

chapelle vapnik bousquet mukherjee 
choosing multiple parameters support vector machines 
machine learning 

chen donoho saunders 
atomic decomposition basis pursuit 
technical report department statistics stanford university 

cohen schapire singer 
learning order things 
michael jordan michael kearns sara solla editors advances neural information processing systems volume 
mit press 

collins schapire singer 
logistic regression adaboost bregman distances 
machine learning 
special issue new methods model selection model combination 



stable exponential penalty algorithm superlinear convergence 
nov 

cortes vapnik 
support vector networks 
machine learning 
boosting leveraging 
cover hart 
nearest neighbor pattern classifications 
ieee transaction information theory 

cox sullivan 
asymptotic analysis penalized likelihood related estimates 
annals statistics 

crammer singer 
learnability design output codes multiclass problems 
cesa bianchi goldberg editors proc 
colt pages san francisco 
morgan kaufmann 

cristianini shawe taylor 
support vector machines 
cambridge university press cambridge uk 

della pietra della pietra lafferty 
inducing features random fields 
ieee transactions pattern analysis machine intelligence april 

della pietra della pietra lafferty 
duality auxiliary functions bregman distances 
technical report cmu cs school computer science carnegie mellon university 

demiriz bennett shawe taylor 
linear programming boosting column generation 
journal machine learning research 

hlmann 
boosting tumor classification gene expression data 
preprint 
see stat ethz ch boosting 

devroye gy rfi lugosi 
probabilistic theory pattern recognition 
number applications mathematics 
springer new york 

dietterich 
experimental comparison methods constructing ensembles decision trees bagging boosting randomization 
machine learning 

dietterich bakiri 
solving multiclass learning problems errorcorrecting output codes 
journal intelligence research 

watanabe 
modification adaboost 
proc 
colt san francisco 
morgan kaufmann 

drucker cortes jackel lecun vapnik 
boosting ensemble methods 
neural computation 

drucker schapire simard 
boosting performance neural networks 
international journal pattern recognition artificial intelligence 

duffy helmbold 
geometric approach leveraging weak learners 
fischer simon editors computational learning theory th european conference eurocolt pages march 
long version appear tcs 

duffy helmbold 
boosting methods regression 
technical report department computer science university santa cruz 

duffy helmbold 
leveraging regression 
proc 
colt pages san francisco 
morgan kaufmann 

duffy helmbold 
potential 
solla leen 
ller editors advances neural information processing systems volume pages 
mit press 

rigau 
boosting applied word sense disambiguation 
lnai proceedings th european conference machine learning ecml pages barcelona spain 

feller 
probability theory applications 
wiley chichester third edition 
meir tsch 
fisher jr editor 
improving regressors boosting techniques 

frean downs 
simple cost function boosting 
technical report dep 
computer science electrical engineering university queensland 

freund 
boosting weak learning algorithm majority 
information computation september 

freund 
adaptive version boost majority algorithm 
machine learning 

freund iyer schapire singer 
efficient boosting algorithm combining preferences 
proc 
icml 

freund schapire 
decision theoretic generalization line learning application boosting 
eurocolt european conference computational learning theory 
lncs 

freund schapire 
experiments new boosting algorithm 
proc 
th international conference machine learning pages 
morgan kaufmann 

freund schapire 
game theory line prediction boosting 
proc 
colt pages new york ny 
acm press 

freund schapire 
decision theoretic generalization line learning application boosting 
journal computer system sciences 

freund schapire 
adaptive game playing multiplicative weights 
games economic behavior 

freund schapire 
short boosting 
journal japanese society artificial intelligence september 
appeared japanese translation naoki abe 

friedman 
stochastic gradient boosting 
technical report stanford university march 

friedman hastie tibshirani 
additive logistic regression statistical view boosting 
annals statistics 
discussion pp technical report department statistics sequoia hall stanford university 

friedman 
bias variance loss dimensionality 
data mining knowledge discovery volume pages 
kluwer academic publishers 

friedman 
greedy function approximation 
technical report department statistics stanford university february 

frisch 
logarithmic potential method convex programming 
memorandum university institute economics oslo may 

graepel herbrich sch lkopf smola bartlett 
ller obermayer williamson 
classification proximity data 
willshaw murray editors proceedings icann volume pages 
iee press 


bagging stabilize reducing variance 
icann lecture notes computer science 
springer 

buc 
boosting mixture models semi supervised tasks 
proc 
icann vienna austria 

grove schuurmans 
boosting limit maximizing margin learned ensembles 
proceedings fifteenth national conference artifical intelligence 
boosting leveraging 
guruswami sahai 
multiclass learning error correcting codes 
proc 
twelfth annual conference computational learning theory pages new york usa 
acm press 

hart 
non intrusive appliance load monitoring 
proceedings ieee 

shirai 
decision trees construct practical parser 
machine learning 

hastie tibshirani friedman 
elements statistical learning data mining inference prediction 
springer series statistics 
springer new york 

hastie tibshirani 
generalized additive models volume monographs statistics applied probability 
chapman hall london 

haussler 
decision theoretic generalizations pac model neural net learning applications 
information computation 

haykin 
neural networks comprehensive foundation 
prentice hall second edition 

helmbold kivinen warmuth 
relative loss bounds single neurons 
ieee transactions neural networks 

herbrich 
learning linear classifiers theory algorithms volume adaptive computation machine learning 
mit press 

herbrich graepel shawe taylor 
sparsity vs large margins linear classifiers 
proc 
colt pages san francisco 
morgan kaufmann 

herbrich williamson 
algorithmic luckiness 
jmlr 


semi infinite programming theory methods applications 
siam review september 

huang 
zhou 
zhang chen 
pose invariant face recognition 
proceedings th ieee international conference automatic face gesture recognition pages grenoble france 

iyer lewis schapire singer singhal 
boosting document routing 
callan rundensteiner editors proceedings cikm th acm international conference information knowledge management pages mclean 
acm press new york 

james stein 
estimation quadratic loss 
proceedings fourth berkeley symposium mathematics statistics probability volume pages berkeley 
university california press 

jiang 
theoretical aspects boosting presence noisy data 
proceedings eighteenth international conference machine learning 

johnson preparata 
densest hemisphere problem 
theoretical computer science 

jordan jacobs 
hierarchical mixtures experts em algorithm 
neural computation 

kearns mansour 
boosting ability og top decision tree learning algorithms 
proc 
th acm symposium theory computing pages 
acm press 

kearns valiant 
cryptographic limitations learning boolean formulae finite automata 
journal acm january 

kearns vazirani 
computational learning theory 
mit press 
meir tsch 
kimeldorf wahba 
results spline functions 
math 
anal 
applic 

kivinen warmuth 
boosting entropy projection 
proc 
th annu 
conference comput 
learning theory pages 
acm press new york ny 

kivinen warmuth auer 
perceptron algorithm vs winnow linear vs logarithmic mistake bounds input variables relevant 
special issue artificial intelligence 

kivinen warmuth 
additive versus exponentiated gradient updates linear prediction 
information computation 


relaxation methods strictly convex regularizations piecewise linear programs 
applied mathematics optimization 


empirical margin distributions bounding generalization error combined classifiers 
ann 
statis 

krieger wyner long 
boosting noisy data 
proceedings th icml 
morgan kaufmann 

lafferty 
additive models boosting inference generalized divergences 
proc 
th annu 
conf 
comput 
learning theory pages new york ny 
acm press 

lebanon lafferty 
boosting maximum likelihood exponential models 
advances neural information processings systems volume 
appear 
longer version neurocolt technical report nc tr 

lecun jackel bottou cortes denker drucker guyon ller simard vapnik 
comparison learning algorithms handwritten digit recognition 
fogelman gallinari editors proceedings icann international conference artificial neural networks volume ii pages france 
ec 

lin pinkus 
multilayer feedforward networks activation function approximate function 
neural networks 

littlestone long warmuth 
line learning linear functions 
journal computational complexity 
earlier version technical report crl uc santa cruz 

luenberger 
linear nonlinear programming 
addison wesley publishing reading second edition may 
reprinted corrections may 

bor lugosi nicolas 
consistent strategy boosting algorithms 
proceedings annual conference computational learning theory volume lnai pages sydney february 
springer 


luo tseng 
convergence coordinate descent method convex differentiable minimization 
journal optimization theory applications 

mallat zhang 
matching pursuits time frequency dictionaries 
ieee transactions signal processing december 

mangasarian 
linear nonlinear separation patterns linear programming 
operations research 

mangasarian 
arbitrary norm separating plane 
operation research letters 
boosting leveraging 
meir 
geometric bounds boosting 
proceedings fourteenth annual conference computational learning theory pages 

meir 
existence weak learners applications boosting 
machine learning 

meir zhang 
consistency greedy algorithms classification 
procedings colt volume lnai pages sydney 
springer 

mason 
margins combined classifiers 
phd thesis australian national university september 

mason bartlett baxter 
improved generalization explicit optimization margins 
technical report department systems engineering australian national university 

mason baxter bartlett frean 
functional gradient techniques combining hypotheses 
smola bartlett sch lkopf schuurmans editors advances large margin classifiers 
mit press cambridge ma 

mason baxter bartlett frean 
functional gradient techniques combining hypotheses 
smola bartlett sch lkopf schuurmans editors advances large margin classifiers pages 
mit press cambridge ma 

matou sek 
geometric discrepancy illustrated guide 
springer verlag 

meir el yaniv shai ben david 
localized boosting 
proc 
colt pages san francisco 
morgan kaufmann 

meir zhang 
data dependent bounds bayesian mixture models 
unpublished manuscript 

mercer 
functions positive negative type connection theory integral equations 
philos 
trans 
roy 
soc 
london 


tuning cost sensitive boosting application melanoma diagnosis 
kittler roli editors proceedings nd workshop multiple classifier systems mcs volume lncs pages 
springer 

moody 
effective number parameters analysis generalization regularization non linear learning systems 
hanson moody lippman editors advances neural information processings systems volume pages san mateo ca 
morgan kaufman 


ller mika tsch tsuda sch lkopf 
kernel learning algorithms 
ieee transactions neural networks 

murata amari 
network information criterion determining number hidden units artificial neural network model 
ieee transactions neural networks 

nash 
linear nonlinear programming 
mcgraw hill new york ny 

richard patrice 
robust boosting algorithm 
proc 
th european conference machine learning volume lnai helsinki 
springer verlag 
meir tsch 
onoda tsch 
ller 
asymptotic analysis adaboost binary classification case 
niklasson bod ziemke editors proc 
int 
conf 
artificial neural networks icann pages march 

onoda tsch 
ller 
non intrusive monitoring system household electric appliances inverters 
rojas editors proc 
nc berlin 
icsc academic press canada switzerland 

sullivan langford caruana blum 
featureboost metalearning algorithm improves model robustness 
proceedings th icml 
morgan kaufmann 

russell 
experimental comparisons online batch versions bagging boosting 
proc 
kdd 

el yaniv meir 
variance optimized bagging 
proc 
th european conference machine learning 

poggio girosi 
regularization algorithms learning equivalent multilayer networks 
science 

quinlan 
programs machine learning 
morgan kaufmann 

quinlan 
boosting order learning 
lecture notes computer science 

tsch 
ensemble learning methods classification 
master thesis dep 
computer science university potsdam april 
german 

tsch 
robust boosting convex optimization 
phd thesis university potsdam computer science dept august str 
potsdam germany october 

tsch 
boosting durch optimierung 
wagner editor volume gi edition lecture notes informatics pages 
bonner 

tsch demiriz bennett 
sparse regression ensembles infinite finite hypothesis spaces 
machine learning 
special issue new methods model selection model combination 
neurocolt technical report nc tr 

tsch mika sch lkopf 
ller 
constructing boosting algorithms svms application class classification 
ieee pami september 
press 
earlier version gmd techreport 

tsch mika warmuth 
convergence leveraging 
neurocolt technical report royal holloway college london august 
short version appeared nips mit press 

tsch mika warmuth 
convergence leveraging 
dietterich becker ghahramani editors advances neural information processings systems volume 
press 
longer version neurocolt technical report nc tr 

tsch onoda 
ller 
soft margins adaboost 
machine learning march 
neurocolt technical report nc tr 

tsch sch lkopf smola mika onoda 
ller 
robust ensemble learning 
smola bartlett sch lkopf schuurmans editors advances large margin classifiers pages 
mit press cambridge ma 

tsch smola mika 
adapting codes embeddings 
nips volume 
mit press 
accepted 
boosting leveraging 
tsch warmuth mika onoda 
ller 
barrier boosting 
proc 
colt pages san francisco 
morgan kaufmann 

tsch warmuth 
maximizing margin boosting 
proc 
colt volume lnai pages sydney 
springer 

madigan richardson 
boosting methodology regression problems 
heckerman whittaker editors proceedings artificial intelligence statistics pages 

rissanen 
modeling shortest data description 
automatica 

robert 
bayesian choice decision theoretic motivation 
springer verlag new york 

schapire gupta riccardi bangalore alshawi douglas 
combining prior knowledge boosting call classification spoken language dialogue 
international conference speech signal processing 

rockafellar 
convex analysis 
princeton landmarks 
princeton university press new jersey 

schapire 
strength weak learnability 
machine learning 

schapire 
output codes boost multiclass learning problems 
machine learning proceedings th international conference pages 

schapire 
brief boosting 
proceedings sixteenth international joint conference artificial intelligence 

schapire 
boosting approach machine learning overview 
workshop nonlinear estimation classification 
msri 

schapire freund bartlett lee 
boosting margin new explanation effectiveness voting methods 
annals statistics october 

schapire singer 
improved boosting algorithms confidence rated predictions 
machine learning december 
proceedings th workshop computational learning theory pages 

schapire singer 
boostexter boosting system text categorization 
machine learning 

schapire singer singhal 
boosting rocchio applied text filtering 
proc 
st annual international conference research development information retrieval 

schapire stone mcallester littman 
modeling auction price uncertainty boosting conditional density estimations noise 
proceedings proceedings nineteenth international conference machine learning 

sch lkopf herbrich smola 
generalized representer theorem 
helmbold williamson editors colt eurocolt volume lnai pages 
springer 

sch lkopf platt shawe taylor smola williamson 
estimating support high dimensional distribution 
tr microsoft research redmond wa 

sch lkopf smola williamson bartlett 
new support vector algorithms 
neural computation 
neurocolt technical report nc tr 
meir tsch 
sch lkopf smola 
learning kernels 
mit press cambridge ma 

schwenk bengio 
boosting neural networks 
neural computation 


pac perceptron winnow boosting margin 
proc 
colt pages san francisco 
morgan kaufmann 


smooth boosting learning malicious noise 
proceedings fourteenth annual conference computational learning theory pages 

shawe taylor bartlett williamson anthony 
structural risk minimization data dependent hierarchies 
ieee trans 
inf 
theory september 

shawe taylor cristianini 
results margin distribution 
proceedings twelfth conference computational learning theory pages 

shawe taylor cristianini 
soft margin algorithms 
technical report nc tr neurocolt june 

shawe taylor 
strategy boosting regressors 
smola bartlett sch lkopf schuurmans editors advances large margin classifiers pages cambridge ma 
mit press 

singer 
leveraged vector machines 
solla leen 
ller editors advances neural information processing systems volume pages 
mit press 

tax duin 
data domain description support vectors 
verleysen editor proc 
esann pages brussels 
facto press 


boosting density function estimators 
proc 
th european conference machine learning volume lnai pages helsinki 
springer verlag 

tikhonov arsenin 
solutions ill posed problems 
winston washington 

tsuda sugiyama 
ller 
subspace information criterion non quadratic regularizers model selection sparse regressors 
ieee transactions neural networks 

valiant 
theory learnable 
communications acm november 

van der wellner 
weak convergence empirical processes 
springer verlag new york 

vapnik 
nature statistical learning theory 
springer verlag new york 

vapnik 
statistical learning theory 
wiley new york 

vapnik chervonenkis 
uniform convergence relative frequencies events probabilities 
theory probab 
applications 

von neumann 
zur theorie der 
math 
ann 

walker rambow 
spot trainable sentence planner 
proc 
nd annual meeting north american chapter computational linguistics 
boosting leveraging 
zemel 
gradient boosting algorithm regression problems 
leen dietterich tresp editors advances neural information processing systems volume pages 
mit press 

zhang 
statistical behavior consistency classification methods convex risk minimization 
technical report rc ibm research yorktown heights ny 

zhang 
general greedy approximation algorithm applications 
advances neural information processing systems volume 
mit press 

zhang 
dual formulation regularized linear systems convex risks 
machine learning 

zhang 
sequential greedy approximation certain convex optimization problems 
technical report ibm watson research center 


zhou jiang 
yang 
chen 
lung cancer cell identification artificial neural network ensembles 
artificial intelligence medicine 
