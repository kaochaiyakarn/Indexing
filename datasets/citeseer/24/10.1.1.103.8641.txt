university wisconsin computer sciences technical report dbmss modern processors time go 
ailamaki david dewitt mark hill david wood department computer science university wisconsin madison dewitt david cs wisc edu high performance processors employ sophisticated techniques overlap simultaneously execute multiple computation memory operations 
intuitively techniques help database applications increasingly compute memory bound 
unfortunately studies report improve database system performance extent scientific workloads 
database systems focusing minimizing memory latencies cache conscious algorithms sorting data placement step addressing problem 
best design high performance dbmss carefully evaluate understand processor memory behavior commercial dbmss today hardware platforms 
answer question time go database system executes modern computer platform 
examine commercial dbmss running intel xeon nt 
introduce framework analyzing query execution time dbms running server modern processor memory architecture 
focus processor memory interactions exclude effects subsystem memory resident database 
simple queues find database developers optimize data placement second level data cache optimize instruction placement reduce level instruction cache stalls expect execution time decrease significantly addressing stalls related subtle implementation issues branch prediction 
today database servers systems powerful processors designed sophisticated way fast computation memory access 
due sophisticated techniques hiding latency dbmss compute memory bound 
researchers design evaluate processors programs simpler dbmss spec linpack hope complicated programs dbmss take full advantage architectural innovations 
unfortunately studies commercial dbmss shown hardware behavior suboptimal compared scientific workloads 
significant amount effort improving performance database applications today processors 
focuses optimizing processor memory utilization divided categories evaluation studies cache performance improvement techniques 
category includes handful studies brought problem surface motivated community look 
studies presents results experiments single dbms running tpc benchmark specific platform 
second category includes papers propose algorithmic improvements better cache utilization performing popular tasks dbms sorting data placement techniques minimizing cache related waiting time 
majority results evaluation studies corroborate results showing behavior commercial dbms hardware platform 
results important order identify general trends hold database systems determine problems dbmss run faster 
addition better understand hardware time goes query execution need analytic framework models breakdown terms execution time 
analyze intuitive framework execution time breakdown commercial dbmss hardware platform pii xeon mt workstation running windows nt 
workload consists range selections joins running memory resident database order isolate basic operations identify common trends dbmss 
simple queries half execution time spent stalls 
careful analysis components stall time provides insight operation cache record size selectivity varied 
simplicity queries helped overcome lack source code dbmss 
results show average half execution time spent stalls implying database designers improve dbms performance significantly attacking stalls 
cases memory stalls due second level cache data misses level data stalls important implying data placement focus level cache level instruction cache misses second level instruction stalls important implying instruction placement focus level instruction caches 
significant stalls caused subtle implementation details branch mispredictions implying silver bullet mitigating stalls 
methodological result 
simple queries full tpc workloads provides methodological advantage results simply analyzed substantially similar breakdowns full benchmarks 
verify implemented ran tpc benchmark systems results substantially similar breakdowns simple queries 
rest organized follows section presents summary database workload characterization studies overview cache performance improvements proposed 
section describes vendor independent part study analytic framework breakdown execution time database workload 
section describes experimental setup hardware software conduct experiments 
section presents results 
directions contained section 
related database research focused improving query execution time mainly minimizing stalls due memory hierarchy executing isolated task 
variety algorithms fast sorting techniques propose optimal data placement memory sorting algorithms minimize cache misses overlap memory related delays 
addition cache conscious techniques blocking data partitioning loop fusion data clustering evaluated improve join aggregate queries 
studies targeted specific task concentrate ways faster 
hardware evaluation relational dbms running oltp workload concentrated multiprocessor system issues assigning processes different processors avoid bandwidth bottlenecks 
contrasting scientific commercial workloads tpc tpc relational dbms showed commercial workloads exhibit large instruction footprints distinctive branch behavior typically scientific workloads benefit large level caches 
study showed major bottleneck processor stalled time due cache misses running oltp workloads 
past years interesting studies evaluated database workloads multiprocessor platforms 
studies evaluate oltp workloads evaluate dss workloads studies 
studies agree dbms behavior depends workload dss workloads benefit oltp order processors increased instruction level parallelism memory stalls major bottleneck 
list exhaustive representative done evaluating database workloads 
studies presents results single dbms running tpc benchmark single platform difficult contrast dbmss identify detailed common behavior 
query execution modern processors better understand happens execution query designed framework analyze hardware behavior dbms moment receives query moment returns results 
operation today processors complex described detail accurate performance model 
addition complex queries produce different query plans different dbmss product optimized certain type queries 
section introduce framework describes role major hardware components terms execution time 
describe workload allows focus basic operations dbmss identify hardware components cause execution bottlenecks 
query execution time processor model determine time goes execution query understand processor works 
pipeline basic module receives instruction executes stores results memory 
pipeline works number sequential stages involves number functional components 
operation stage overlap operation stages 
fetch decode unit instruction pool dispatch execute unit cache cache cache retire unit simplified block diagram processor operation tc tb tr shows simplified diagram major stages processor pipeline 
fetch decode unit reads user program instructions instruction cache cache decodes puts instruction pool 
dispatch execute unit schedules execution tm instructions pool subject data dependencies resource availability temporarily stores results 
retire unit knows commit retire temporary results data cache cache 
cases operation may delay stall pipeline 
processor tries cover stall time doing useful techniques non blocking caches caches block servicing requests 
example retrieval cache cache fails request forwarded second level cache cache usually unified data instructions 
request fails forwarded main memory 
time retrieval pending caches process requests 
order execution instruction stalls instruction program comes execute operands depend results 
dispatch execute unit contains multiple functional units perform order execution instructions 
speculative execution branch prediction waiting branch instruction target address resolved algorithm guesses target fetches appropriate instruction stream 
guess correct execution continues normally wrong pipeline flushed retire unit deletes wrong results fetch decode unit fetches correct instruction stream 
branch misprediction incurs computation overhead computing wrong instructions stall time 
tc tm tb tr tl tl tl computation time stall time due cache misses hit stall time due cache misses hit tl stall time due data misses tl stall time due instruction misses stall time due dtlb misses stall time due itlb misses branch misprediction penalty stall time due functional unit unavailability stall time due dependencies instructions platform specific resource stall time table execution time components techniques stalls fully overlapped useful computation 
time execute query tq includes useful computation time tc stall time memory stalls tm branch misprediction overhead tb resource related stalls tr 
due unavailability execution resources functional units buffer space instruction pool registers 
discussed stall time overlapped 
equation holds tq tc tm tb tr table shows time breakdown smaller components 
dtlb itlb page table caches translation lookaside buffers translation data instruction virtual addresses physical ones 
section briefly discusses importance stall type terms easily overlapped aforementioned techniques 
detailed explanation hiding stall times 
significance stall components research improving dbms performance aims reducing tm memory hierarchy stall component 
order able experimental results effectively important determine significant different types stalls execution time 
order speculative execution help hide stalls case 
stalls impossible overlap critical performance 
possible overlap tl number cache misses high 
processor fetch execute instructions data available second level cache 
cache misses occur instructions processor execute hide stalls 
stalls related cache data misses overlap parallel requests main memory 
misses hidden useful computation penalty depends page table implementation processor 
processors successfully sophisticated techniques overlap data stalls useful computation 
instruction related cache stalls hand impossible hide cause serial bottleneck 
instructions available processor wait 
serial bottleneck occurs branch misprediction processor wait correct instruction stream loaded pipeline 
processors exploit spatial locality instruction stream instruction prefetching hardware 
instruction prefetching effectively reduces number cache stalls increase branch misprediction penalty 
related instruction execution tr resource stall time easier overlap instruction cache misses 
hidden depending degree instruction level parallelism program overlapped instructions functional units contention 
database workload large set workloads choose run dbms study performance 
workload study consists single table range selections table memory resident database running single user mode 
workload eliminates dynamic random parameters concurrency control multiple transactions isolates basic operations sequential access index selection 
addition allows studying processor memory behavior interference 
possible explain behavior system reasonable assumptions identify common trends different dbmss 
database contains basic table person defined follows create table person pkey integer null age integer null salary integer null rest fields rest fields stands list integer fields insignificant queries 
relation populated mb data uniform distribution values field age 
experiments basic queries person 
sequential range selection select avg salary person age hi age lo purpose query study behavior dbms executes sequential scan examine effects record size query selectivity 
hi lo define interval qualification attribute age 
reason aggregate opposed just selecting rows twofold 
dbms return minimal number rows measurements affected client server communication overhead 
storing results temporary relation affect measurements extra insertion operations 
second average aggregate common operation tpc benchmark 
query selectivity ranged submitted variations person different row sizes bytes 

indexed range selection range selection resubmitted constructing non clustered index person age 
variations selectivity record size 

sequential join select avg person salary person student person age student age examine behavior executing equijoin indexes database schema augmented relation student defined way person 
record size student bytes populated mb data 
query submitted variations person different record sizes bytes 
experimental setup pii xeon mt workstation conduct experiments 
mentioned section studies simulation carry experiments 
simulation necessary order evaluate architectural alternatives designs slow 
section describes hardware software presents experimentation methodology 
hardware counters pentium ii xeon processor run experiments full speed avoid approximations simulation impose conduct comparative evaluation dbmss 
hardware platform system contains pentium ii xeon processor running mhz mb main memory connected processor chip mhz system bus 
pentium ii powerful server processor order engine speculative execution instructions 
instruction set composed cisc instructions translated risc instructions ops decode phase pipeline 
levels non blocking cache system 
separate level caches instructions data second level cache unified 
cache characteristics summarized table 
characteristic instruction data unified size kb kb kb associativity way way way penalty cycles hit cycles hit main memory latency non blocking misses outstanding write policy write back write back write back software table pentium ii xeon cache characteristics experiments conducted popular commercial dbmss names disclosed due legal restrictions 
refer system system system system installed windows nt service pack 
dbmss configured way order achieve consistency possible 
buffer pool size large fit datasets queries 
effects excluded study objective measure pure processor memory performance 
wanted avoid measuring subsystem os 
define schema execute queries exact commands datasets dbmss vendor specific sql extensions 
measurement tools methodology pentium ii processor provides counters event measurement 
tool provided intel control counters 
set counters zero assign event codes read values pre specified amount time program completed execution 
measure events data report 
measured event user kernel mode 
measurements query main memory caches multiple runs query 
order distribute minimize effects client server startup overhead unit execution consisted different queries relation selectivity 
time executed unit measured pair events 
order increase confidence intervals experiments repeated times final sets numbers exhibit standard deviation percent 
set formulae numbers transformed meaningful performance metrics 
stall time component description measurement method tc computation time estimated minimum ops retired tm tl cache stalls misses cycles tl cache stalls actual stall time tl tl data stalls misses measured memory latency tl instruction stalls misses measured memory latency dtlb stalls measured itlb stalls misses cycles tb branch misprediction penalty branch mispredictions retired cycles tr functional unit stalls actual stall time dependency stalls actual stall time length decoder stalls actual stall time overlap time measured table method measuring stall time components counters measured stall times described section measuring individual components separately 
application framework experimental setup suffers deficiencies able measure event code available 
events measured actual stall time overlaps measured number events occurred query execution multiplied estimated penalty 
measurements memory subsystem showed workload latency bound bandwidth bound rarely uses third available memory bandwidth 
results penalty approximations fairly accurate andy glew provided invaluable help figuring correct formulae kim keeton shared ones 
probability queueing requests memory minimal 
table shows detailed list stall time components way measured 
contention conditions taken account 
xeon type stall stalls related instruction length decoder translation instructions ops 
results executed workload section commercial database management systems 
section overview execution time breakdown discuss general trends 
focus important stall time components analyze determine implications behavior 
experiments executed user mode time measurements shown section reflect user mode execution stated 
execution time breakdown query execution time sequential range selection indexed range selection computation memory stalls branch mispredictions resource stalls shows graphs summarizing average execution time breakdown queries 
bar shows contribution components tc tm tb tr percentage total query execution time 
middle graph showing indexed range selection includes systems system index execute query 
workload simpler tpc benchmarks computation time usually half execution time processor spends time stalled 
similar results oltp dss workloads studies measured dbms 
high processor stall time indicates importance analyzing query execution time 
processor clock rates join average query execution time breakdown time components 
increase stall times decrease computation component smaller fraction execution time 
memory stall time contribution varies different queries different database systems 
memory stalls vary queries highly dependent workload 
sequential range selection join queries cause fewer memory stalls indexed range selection 
plausible explanation index increases data instruction footprint execution 
memory component exhibits highest variation dbmss systems optimize memory stalls specific platform don analysis memory behavior yields cache stalls cache stalls account tm systems measured 
despite variation common ground research improving memory stalls necessarily having analyze dbmss 
minimizing memory stalls major focus database research performance improvement 
cases memory stall time tm accounts stall time components significant 
memory stall time entirely hidden bottleneck eventually shift stalls 
systems branch misprediction stalls account execution time resource stall time contribution ranges 
system exhibits tm tb dbmss queries highest percentage resource stalls execution time 
expected optimizing kinds stalls shifts bottleneck third kind 
research improving dbms performance focus minimizing kinds stalls effectively decrease execution time 
memory stalls order optimize performance main target database researchers past minimize memory stall time sequential range selection indexed range selection stalls stalls stalls stalls itlb stalls contributions memory components memory stall time stall time due latencies caused memory hierarchy 
techniques join cache conscious data placement proposed reduce cache misses penalty 
techniques successful context proposed closer look execution time breakdown shows significant room improvement 
section discusses significance memory stall components query execution time framework discussed section 
shows breakdown tm stall time components tl cache stalls tl cache stalls tl cache data stalls tl cache instruction stalls cache stalls dbmss 
graph type query 
graph shows memory stall time breakdown systems 
selectivity range selections shown set record size kept constant bytes 
obvious cache stall time insignificant 
reality contribution lower measurements cache stalls take account overlap factor upper bounds 
experiments cache rate number misses divided number memory usually exceeds 
cache rates low dbms executes query accesses private data structures accesses data relations 
accessed portion data fits cache misses due accessed data 
cache performance bottleneck dbmss evaluated 
queries run systems tl time spent data stalls significant components execution time 
dbmss cache data rate typically higher cache rate 
exception system optimizes data layout access second cache level 
case sequential range query system exhibits data rate results insignificant tl 
second level cache misses expensive cache misses data fetched main memory 
generally memory latency cycles observed 
discussed section multiple cache misses overlap 
measure upper bound tl number misses times main memory latency overlap hard estimate 
real tl significantly estimation workload bound memory latency bandwidth time execution uses third available memory bandwidth 
gap memory processor speed increases expect data access cache major bottleneck latency bound workloads 
fortunately size today caches increased mb continues increase 
pentium ii xeon experiments conducted cache mb experiments conducted kb cache 
stall time due misses level instruction cache tl major memory stall component dbmss 
results study reflect real cache stall time approximations 
xeon uses stream buffers instruction prefetching misses bottleneck despite previous results show improvement tl stream buffers shared memory multiprocessor 
explained section tl impossible hide cache misses cause serial bottleneck pipeline 
system dbms tl insignificant 
system smallest instruction footprint systems queries executed exhibits optimized instruction cache behavior minimizes tl execution time 
rest systems optimize instruction cache 
depending type query dbms tl accounts total execution time dbmss average contribution tl execution time 
techniques reduce cache stall time cache effectively 
unfortunately level cache size increase rate second level cache size large caches fast may slow processor clock 
new processors larger kb cache accessed pipeline stages trade size speed exists 
consequently dbmss optimized cache effectively possible 
may possible reduce cache stalls optimizing data placement second level cache 
processors caches obey inclusion rule information stored cache time subset information stored cache 
systems observe inclusion maintain consistency searching lowest memory hierarchy level information consistent consistent higher levels 
hand new data invalidates old information cache information invalidated cache 
information may data instructions cache unified 
invalidated information cache invalidate instructions cache causing stalls order bring back 
verified hypothesis running sequential range selection variable record sizes 
fields involved query salary age records 
records stored sequentially record size increases salary age fields subsequent tuples apart 
results invalidating cache lines contain instructions tl increases 
consequently way reducing tl processors caches observe inclusion rule cause undesired instruction invalidations 
processors inclusion xeon tl reduced partitioning cache section exclusive instruction exclusive data avoiding interference instructions data 
stall time caused cache instruction misses tl itlb misses insignificant experiments 
tl low second level cache misses 
low indicates systems instruction pages itlb store translations addresses 
branch mispredictions explained section branch mispredictions serious performance implications cause serial bottleneck pipeline cause instruction cache misses turn incur additional stalls 
branch instructions account total instructions retired branch misprediction rates experiments 
srs irs sj simple workload dbmss suffer branch misprediction stalls 
branch mispredictions depend accurately branch prediction algorithm predicts instruction stream 
branch misprediction rate number mispredictions divided number retired branch instructions vary significantly record size selectivity systems 
average rates systems shown left graph 
branch misprediction algorithm uses small buffer called branch target buffer btb store targets branches executed 
hit buffer activates branch prediction algorithm decides target branch previous history 
btb prediction static forward branch taken backward taken 
experiments btb misses time average corroborates previous results tpc workloads 
consequently sophisticated hardware implements branch prediction algorithm half time 
query execution time branch 
stalls cache stalls left branch misprediction rates 
srs sequential selection irs indexed selection sj join 
right system running sequential selection 
tb tl increase function increase selectivity 
addition btb rate increases branch misprediction rate increases 
shown larger btb entries improves btb rate oltp workloads 
mentioned section branch misprediction stalls tightly connected instruction stalls 
xeon connection tighter uses instruction prefetching 
experiments tl follows behavior tb function variations selectivity record size 
right graph illustrates system running range selection queries various selectivities 
database developers help reduce instruction cache stalls writing critical sections inner loops language perform jumps object oriented languages 
compiler produce code predictable 
hand processors able efficiently execute unoptimized instruction streams different prediction mechanism reduce stalls 
resource stalls resource related stall time time processor wait resource available 
resources include functional units execution stage registers handling dependencies instructions platform dependent resources 
contribution resource stalls query execution time srs irs sj contributions execution time dbmss 
srs sequential selection irs indexed selection sj join 
system index irs query excluded system results 
execution time fairly stable dbmss 
cases resource stalls dominated dependency functional unit stalls 
shows contributions systems queries 
system executing range selection queries dependency stalls important resource stalls 
dependency stalls caused low instruction level parallelism opportunity instruction pool instruction depends results multiple instructions completed execution 
processor wait dependencies resolved order continue 
functional unit availability stalls caused bursts instructions create contention execution unit 
memory account srs irs sj half instructions retired possible resources causing stalls memory buffer 
resource stalls artifact lowest level details hardware 
compiler produce code avoids resource contention exploits instruction level parallelism 
difficult instruction set cisc instruction internally translated smaller instructions ops 
easy way compiler see correlations multiple instructions optimize instruction stream processor execution level 
despite performance optimizations today database systems able take advantage improvements processor technology 
studies evaluated database workloads complex tpc benchmarks consider single dbms single platform 
variation platforms dbmss complexity workloads impossible thoroughly understand hardware behavior point view database 
simple query execution time framework analyzed behavior commercial dbmss running simple selection join queries modern processor memory architecture 
simple queries full tpc workloads provides methodological advantage results simply analyzed 
implemented ran tpc benchmark systems results substantially similar breakdowns simple queries 
results experiments suggest database developers pay attention data layout second level data cache data stalls major component query execution time cache stalls insignificant 
addition memory stalls dominated level instruction cache misses focus optimizing critical paths instruction cache 
performance improvements address stall components order effectively increase percentage execution time spent useful computation 
intel microsoft donating hardware operating system conducted experiments study 
andy glew help pentium ii counters microarchitecture valuable feedback miron livny suggestions design high confidence experiments 
arpaci dusseau arpaci dusseau culler hellerstein patterson 
high performance sorting networks workstations 
proceedings acm sigmod conference may 
gharachorloo bugnion 
memory system characterization commercial workloads 
proceedings th annual international symposium computer architecture pages june 
chilimbi hill larus 
cache conscious structure layout 
proceedings programming languages design implementation pldi may 
johnson liu 
evaluation multithreaded uniprocessors commercial application environments 
proceedings rd annual international symposium computer architecture may 
gray 
benchmark handbook transaction processing systems 
morgan kaufmann publishers nd edition 
hennessy patterson 
computer architecture quantitative approach 
morgan kaufman publishers ond edition 
heim 
evaluating branch prediction methods processor traces commercial application workloads 
conjunction hpca february 
intel 
pentium ii processor developer manual 
intel order number october 
keeton 
personal communication december 
keeton patterson raphael baker 
performance characterization quad pentium pro smp oltp workloads 
proceedings th international symposium computer architecture pages barcelona spain june 
pey zhang 
memory performance dss commercial workloads shared memory multiprocessors 
proceedings hpca conference 

larson graefe 
memory management run generation external sorting 
proceedings acm sigmod conference june 
lo eggers gharachorloo levy parekh 
analysis database workload performance simultaneous multithreaded processors 
proceedings th annual international symposium computer architecture pages june 
maynard 
contrasting characteristics cache performance technical multi user commercial workloads 
proceedings th international conference architectural support programming languages operating systems san jose california october 
nyberg gray lomet 
risc machine sort 
proceedings acm sigmod conference may 
ranganathan gharachorloo adve 
performance database workloads shared memory systems order processors 
proceedings th international conference architectural support programming languages operating systems san jose california october 
kant naughton 
cache conscious algorithms relational query processing 
proceedings th vldb conference santiago chile 

personal communication september 
glew 
personal communication september 
yeh patt 
level adaptive training branch prediction 
proceedings ieee micro pages november 
rosenblum bugnion herrod witchel gupta 
impact architectural trends operating system performance 
proceedings th acm symposium operating system principles pages december 

performance oltp application symmetry multiprocessor system 
proceedings international symposium computer architecture 
