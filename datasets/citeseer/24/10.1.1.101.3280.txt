representing sentence structure hidden markov models information extraction ray cs wisc edu department computer sciences university wisconsin madison wisconsin study application hidden markov models hmms learning information extractors ary relations free text 
propose approach representing grammatical structure sentences states model 
investigate objective function hmm training maximizes ability learned models identify phrases interest 
evaluate methods deriving extractors binary relations biomedical domains 
experiments indicate approach learns accurate models baseline approaches 
information extraction may defined task automatically extracting instances specified classes relations text 
research interested machine learning approaches including hidden markov models hmms extract certain relationships objects biomedical text sources 
evaluate contributions state art learning information extractors hmms 
investigate approach incorporating information grammatical structure sentences hmm architectures 
second investigate objective function hmm training emphasis maximizing ability learned models identify phrases interest simply maximizing likelihood training data 
experiments challenging real world domains indicate contributions lead accurate learned models 
automated methods information extraction valuable applications including populating knowledge bases databases summarizing collections documents identifying significant unknown relationships objects 
constructing information extraction systems manually proven expensive riloff interest machine learning methods learn information extraction models labeled training data 
hidden markov models successful approaches considered learning information extractors leek freitag mccallum mark craven craven wisc edu department biostatistics medical informatics university wisconsin madison wisconsin seymore freitag mccallum mccallum 
previous hmm approaches information extraction adequately address key aspects problem domains focused 
data processing complex natural language text 
previous approaches represented data sequences tokens approach sentences processed shallow parser represented sequences typed phrases 
second data processing include sentences relevant relations interest 
relevant sentences certain phrases contain information extracted 
previous approaches applying hmms focused training process maximizing likelihood training sentences adopt training method designed maximize probability assigning correct labels various parts sentences processed krogh 
approach involves coupling algorithm devised krogh null models intended represent data directly relevant task hand 
problem domain focused extracting instances specific relations interest abstracts medline database national library medicine 
medline contains bibliographic information abstracts biomedical journals 
example binary relation consider experiments subcellular localization relation represents location particular protein cell 
refer domains relation protein location 
refer instance relation tuple 
provides illustration extraction task 
top shows sentences medline 
bottom shows instance target relation subcellular localization extract second sentence 
tuple asserts protein ubc subcellular compartment called 
order learn models perform task training examples consisting passages text annotated tuples extracted needed 

report identification integral membrane enzyme 
enzyme ubc localizes catalytic domain facing 
subcellular localization ubc example information extraction task 
top shows part document wish extract instances subcellular localization relation 
bottom shows extracted tuple 
approach training test instance individual sentence 
aspects data difficult information extraction task involves free text ii genre text general grammatically simple iii text includes lot technical terminology iv sentences extracted 
terminology information extraction literature task inherently multiple slot extraction task 
interested extracting instances ary relations treat domain relation separate unary component extracted called single slot extraction 
consider relation discussed 
document may mention proteins locations relation holds certain pairs proteins locations 
experiments reported data sets representing different binary relations 
data set includes sentences represent positive instances labeled tuples extracted sentences represent negative instances labeled tuples 
positive instances labeled tuples unique tuples 
second data set binary relation characterizes associations genes genetic disorders 
refer relation disorder association domains relation gene disorder 
data set contains positive instances negative instances 
positive instances labeled tuples unique 
data sets negative instances near misses come population abstracts positive instances cases discuss concepts associated target relation 
target tuples subcellular localization relation collected yeast protein database ypd hodges target tuples disorder association relation collected online inheritance man database center medical genetics 
relevant medline abstracts gathered entries databases 
label sentences abstracts matched target tuples words sentence 
sentence contained words matched tuple taken positive stance 
sentence considered negative instance 
clear process automatic result noisy labeling 
sentence may words target tuple relation semantics may refer relation 
hand tuple relation may described synonymous words target tuple sentences tuples exist synonyms labeled incorrectly 
random sample positive negative sentences estimate amount noise introduced labeling process 
estimate confidence approximately sentences labeled incorrectly falsely labeled unlabeled subcellular localization data set 
believe disorder association data set noisy subcellular localization data set 
representing phrase structure hidden markov models hmms stochastic analogs finite state automata 
hmm defined set states set transitions 
state associated emission distribution defines likelihood state emit various tokens 
transitions state associated transition distribution defines likelihood state current state 
previous hmm approaches information extraction sentences represented sequences tokens 
hypothesize incorporating sentence structure models build results better extraction accuracy 
approach syntactic parses sentences process 
particular sundance system riloff obtain shallow parse sentence 
representation incorporate information provided sundance parse flattens sequence phrase segments 
phrase segment consists type describing grammatical nature phrase words part phrase 
positive training examples segment contains word words belong domain target tuple annotated corresponding domain 
refer annotations labels 
labels absent test instances 
shows sentence containing instance subcellular localization relation annotated segments shall discuss panels 
second phrase segment example noun phrase segment np segment contains protein name ubc label 
note types constants pre defined representation sundance parses labels defined respect domains relation trying extract 
note parsing accurate instance third segment really vp segment typed segment sundance 
states hmms represent annotated segments sentence 
segment state model annotated type label pair state emit segments type identical state type think segments label corresponding domain relation having implicit empty label 
enzyme ubc localizes catalytic domain facing np segment enzyme det np segment protein ubc unk enzyme enzyme np segment localizes unk protein ubc protein ubc pp segment unk localizes localizes np segment location prep pp segment art np segment catalytic domain location location vp segment facing unk location location np segment prep art catalytic catalytic unk domain domain facing facing art hmm input representations 
phrase representation sentence segmented typed phrases 
pos representation sentence segmented words typed part speech tags 
token representation sentence segmented untyped words 
representation labels protein location training sentences 
example segment enzyme emitted state type np segment regardless label 
state label corresponding domain relation plays direct role extracting tuples 
schematic architecture hidden markov models 
top shows positive model trained represent positive instances training set 
bottom shows null model trained represent negative instances training set 
phrase representation includes phrase types models states labels positive model additional labeled states type label combination occurs training set 
assume fully connected model model may emit segment type position sentence 
train test phrase models modify standard forward backward viterbi algorithms rabiner 
forward algorithm calculates probability sentence state model having emitted elements instance 
sentence represented sequence tokens algorithm recurrence start represent emission transition distributions respectively element instance ranges states transition modification involves changing part re follows word phrase segment type function returns type segment state described 
key aspects modification type segment agree type state order state emit ii emission probability words segment computed product emission probabilities individual words 
aspect analogous having states na bayes model words phrase 
note equation requires normalization factor define proper distribution sentences 
equations relative comparisons leave factor implicit 
modifications viterbi backward algorithms similar modification forward algorithm 
modifications forward backward algorithm train phrase models baum welch algorithm baum 
models consider hidden state training examples unambiguous path model example need baum welch 
assume fully connected model obtain transition frequencies considering segments various type label annotations adjacent 
smooth frequencies set possible transitions state estimates cestnik 
similar manner obtain emission frequencies words state summing segments positive start null start np segment protein np segment location np segment pp segment np segment pp segment 
general architecture phrase hmms 
top part shows positive model bottom part shows null model 
type label annotations training set 
smooth frequency counts estimates entire vocabulary words 
model constructed predict tuples test sentences 
viterbi algorithm modified described determine path sentence positive model 
consider sentence represent tuple target relation conditions hold 
likelihood emission sentence positive model greater likelihood emission null model refer positive null models respectively sentence segments 

viterbi path positive model segments aligned states corresponding domains relation 
example relation viterbi path sentence pass state label state label 
note phrases identified way extraction task quite complete phrases contain words belong extracted tuple 
consider example 
location phrase contains word addition location 
tuple extraction models include post processing phase extraneous words stripped away tuples returned 
address issue 
consider prediction correct model correctly identifies phrases containing target tuple subphrase 
possible multiple predicted segments domain relation 
case decide combinations segments constitute tuples 
simple rules 
associate segments order occur 
subcellular localization segment matching protein state associated segment matching state 

fewer segments containing element domain match domain construct remaining tuples 
instance predicted protein phrase location phrases create tuples experiments experiments section test hypothesis incorporating phrase level sentence structure model provides improved extraction performance terms precision recall 
test hypothesis comparing hidden markov models represent information grammatical structure sentences 
henceforth refer model described phrase model 
model compare call pos model representation shown 
model represents grammatical information associates type token indicating part ofspeech pos tag word determined sundance 
phrase model pos model represents sentences sequences tokens phrases 
model comparable size phrase model 
positive component model states labels states labels depending training set 
null component model states labels 
models consider call token models representation shown 
representation treats sentence simply sequence words 
investigate variants employ representation 
simpler hidden markov models representation refer token model states positive model state null model counting states 
states model types 
states positive model represent domains binary target relation remaining states labels 
role set states model tokens correspond domains target relation 
complex version model illustrated unlabeled states positive model 
define transitions train models way states specialize tokens come relation instances ii tokens interspersed domains relation instances iii tokens come relation instances 
training algorithm pos model identical phrase model 
training algorithm token models essentially type constraints tokens states 
consider prediction phrase model correct simply identifies phrases containing words tuple similar criterion decide predictions pos model token models positive start null untyped untyped protein start untyped untyped untyped untyped location architecture token model 
correct 
consider pos model token model predictions correct labeled states models identify sequences tokens contain words tuple 
models penalized extracting extra adjacent words actual words target tuple 
process hmm input data parsing cases sundance stemming words porter algorithm porter replacing words occur training set generic unknown token 
statistics token model emitting vocabulary words encountered prediction 
similarly numbers mapped token 
positive predictions ranked confidence measure computed ratio likelihood viterbi path sentence model likelihood model emit sentence confidence sentence segments likelihood probable path segments threaded state comparable value calculated forward algorithm 
construct precision recall graphs models varying threshold confidence measures 
data sets measure precision recall fold cross validation 
data partitioned sentences medline fold 
procedure ensures experiments model nature real application setting 
training sample negative instances equal number positive negative instances fold 
observed get better recall consistently doing 
shows precision recall curves subcellular localization data set 
curve phrase model superior curves token models 
low levels recall pos model exhibits slightly higher precision phrase model superior higher recall levels phrase model significantly higher recall endpoint 
results suggest value representing grammatical structure hmm architectures phrase model definitively accurate 
shows precision recall curves precision recall phrase model pos model token model token model precision vs recall models subcellular localization data set 
precision recall phrase model pos model token model token model precision vs recall models disorder association data set 
disorder association data set 
differences pronounced 
phrase model achieves significantly higher levels precision models including pos model 
recall endpoint phrase model superior models 
conclude experiments support hypothesis incorporating sentence structure models build results better extraction accuracy 
improving parameter estimates standard hmm training algorithms baum welch designed maximize likelihood data model 
specifically sentence training set baum welch method earlier tries find parameters hypothesize accurate models learned training objective function aims maximize likelihood predicting correct sequence labels sentence assume states phrases start positive model null model combined model architecture 
positive null models refer corresponding models 
labels implicit empty label 
known sequence labels sentence training set 
estimate parameters similar task optimizing parameters recover sequence states set observations mccallum 
krogh devised hmm training algorithm tries optimize criterion 
transforming objective function aims minimize negative log likelihood equation incremental update rule obtained parameter expected number times sentence correct paths model expected number times sentence paths model normalizing constant learning rate 
terms calculated forward backward procedure 
note update rule represents online training procedure 
previous experiments separate null model represent negative instances 
krogh algorithm configuration observe results accurate models 
null model described separate entity trained separately 
architecture krogh algorithm unable correct false positives training set doing require adjusting parameters positive model response negative instance 
remedy problem propose alternative having separate null model refer combined model 
combined model consists submodels sharing states 
schematic shown 
states allow training algorithm update parameters parts model response training sentence 
experiments evaluate algorithm train combined model configuration subcellular localization data sets 
compare models phrase model trained corresponding data sets previous experiments 
methodology experiment 
note combined model prediction simpler separate null model suffices consider viterbi path sentence model extract tuples 
train combined model convergence avoid overfitting 
set number iterations gradient descent fixed constant value 
precision krogh algorithm initial parameter estimates recall effect krogh algorithm combined model subcellular localization data set 
shows precision recall curves experiment subcellular localization data set 
precision recall curve show confidence intervals 
observe improvement precision model data set recall held nearly constant 
improvement small observed consistently various model architectures explored 
shows corresponding precision recall curves confidence intervals experiment disorder association data set 
difference initial model trained model pronounced 
model trained krogh algo precision krogh algorithm initial parameter estimates recall effect krogh algorithm combined model disorder association data set 
rithm significantly better precision initial model maintaining similar level recall 
conclude training algorithm appropriate task improve accuracy significantly 
contributions learning hidden markov models information extraction evaluated contributions challenging biomedical domains 
approach representing grammatical structure sentences hmm 
comparative experiments models lacking information shows approach learns extractors increased precision recall performance 
investigated application training algorithm developed krogh models 
algorithm consistently provides accuracy gain original models 
believe promising approaches task deriving information extractors free text domains 
acknowledgments research supported part nih lm nsf career award iis 
authors michael waddell building disorder association data set peter joseph tina rad jude shavlik critiquing initial draft 
baum baum 
equality associated maximization technique statistical estimation probabilistic functions markov processes 
inequalities 
center medical genetics center medical genetics johns hopkins university national center biotechnology information 
online inheritance man tm 
www ncbi nlm nih gov 
cestnik cestnik 
estimating probabilities crucial task machine learning 
proceedings ninth european conference artificial intelligence pages stockholm sweden 
pitman 
freitag mccallum freitag mccallum 
information extraction hmms shrinkage 
working notes aaai workshop machine learning information extraction orlando fl 
aaai press 
freitag mccallum freitag mccallum 
information extraction hmm structures learned stochastic optimization 
proceedings seventeenth national conference artificial intelligence austin tx 
aaai press 
hodges hodges payne 
yeast protein database ypd database complete proteome saccharomyces cerevisiae 
nucleic acids research 
krogh krogh 
hidden markov models labeled sequences 
proceedings twelfth international conference pattern recognition pages jerusalem israel 
ieee computer society press 
leek leek 
information extraction hidden markov models 
master thesis department computer science engineering university california san diego ca 
mccallum mccallum freitag pereira 
maximum entropy markov models information extraction segmentation 
proceedings seventeenth international conference machine learning pages stanford ca 
morgan kaufmann 
national library medicine national library medicine 
medline database 
www ncbi nlm nih gov pubmed 
porter porter 
algorithm suffix stripping 
program 
rabiner rabiner 
tutorial hidden markov models selected applications speech recognition 
proceedings ieee 
riloff riloff 
empirical study automated dictionary construction information extraction domains 
artificial intelligence 
riloff riloff 
sundance sentence analyzer 
www cs utah edu projects nlp 
seymore seymore mccallum rosenfeld 
learning hidden markov model structure information extraction 
working notes aaai workshop machine learning information extraction pages 
aaai press 
