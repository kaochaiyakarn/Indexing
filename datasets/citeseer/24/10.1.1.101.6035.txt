siam matrix anal 
appl 
society industrial applied mathematics vol 
pp 
best rank approximation higher order supersymmetric tensors phillip 
problem determining best squares sense rank approximation higher order tensor studied iterative method extends wellknown power method 
order power method proposed special important class supersymmetric tensors change 
simplified version adapted special structure supersymmetric problem deemed unreliable isnot guaranteed 
aim isto show symmetric version method converges assumptions convexity concavity functional induced tensor question assumptions satisfied practical applications 
version entails significant savings computational complexity compared unconstrained higher order power method 
furthermore novel method initializing iterative observed yield estimate global optimum initialization suggested 
proximity global optimum priori quantifiable 
course analysis important properties tensor implies square matrix unfolding studied 
key words 
supersymmetric tensors rank approximation higher order power method higherorder singular value decomposition ams subject classifications 
pii 


tensor order way array entries accessed indices 
example scalar tensor order vector tensor order matrix second order tensor 
tensors find applications diverse fields physics signal processing data analysis chemometrics psychology 
notion rank defined tensors order higher 
way done extension known expansion matrix sum rank terms 
rank order tensor minimum number rank tensors sum rank tensor order generalized outer product vectors entry ti despite similarity definitions ranks lower higher order tensors exhibit important differences 
example rank higher order tensor necessarily upper bounded tensor dimensions 
furthermore unique way extending higher orders singular value decomposition svd connection squares low rank approximation 
multilinear generalization matrix svd called higherorder singular value decomposition hosvd proposed studied may understood extension called tucker model way received accepted publication revised form october published electronically march 
www siam org journals html department informatics telecommunications division communications signal processing university athens athens greece di uoa gr 
partement communications image traitement de information institut national des communications rue charles fourier vry cedex france phillip int fr 
tensor tensor product induced multilinear mapping definition 
phillip arrays see 
despite similarities decomposition second order svd orthogonality vectors different terms truncation provide best squares ls sense low rank approximation tensor 
initialize higher order equivalent power method proposed determining best rank approximation nth order tensors 
important problem se ls reduced rank tensor approximation plays central role context blind source separation bss higherorder statistics hos 
problem separate recover statistically independent random processes xk aid observations linear mixture form hx mixing matrix source vector xk disturbance vector assumed unknown real simplicity 
noise vector commonly assumed gaussian independent common method recovering sources project observation vector vector chosen normalized kurtosis source estimate cum cum absolutely maximized 
shown maximization problem equivalent best approximating fourth order cumulant tensor rank 
higher order power method hopm employed 
noticed tensor supersymmetric entries remain unchanged permutation indices 
rich symmetry expected permit simplified version hopm applicable kind tensor 
unfortunately symmetric hopm hopm convergent general supersymmetric tensors claimed demonstrated example 
cases practical interest function ti ui ui uin convex concave symmetric version hopm shown converge local maximum restriction unit sphere 
gain symmetric version comes mainly consequent reduction computational complexity 
iterations usually required hopm converge fact times cheaper general hopm compensates resulting significant computational complexity savings 
requirements convexity concavity function met bss context source sign 
example communications negative turns concave 
aim hopm prove converges supersymmetric tensors induced polynomial forms enjoy property convexity concavity 
novel scheme initializing hopm fourth order tensors proposed observed outperform hosvd scheme 
allows priori quantification proximity globally optimal solution 
rest organized follows 
section introduces definitions basic quantities tensor operations 
problem stated section equivalence rank approximation problem maximizing associated functional characterization stationary points recalled symmetric case 
hopm general symmetric fact convergence hopm bss problem wellknown algorithm shown case mixed sign aswell 
best rank approximation supersymmetric tensors versions section hopm shown converge convexity concavity assumptions 
important properties tensor implies square matrix version proved section 
properties subsequent analysis important right hold supersymmetric tensor regardless associated function convex concave 
section develops new initialization method derives bounds performance 
problem computing rank approximation briefly discussed section 
section concludes 

notation 
vectors denoted bold lowercase letters bold uppercase letters denote tensors second order matrices 
higher order tensors denoted bold calligraphic uppercase letters 
symbol designates identity matrix dimensions understood context 
superscript employed transposition 
element tensor denoted ti indices assumed start 
symbol denote right kronecker product 
matrix vec signify mn vector built columns stacked 
inverse operator builds matrix vector called 
sake simplicity real tensors considered 
extension results tensors hermitian symmetry straightforward 

basic definitions 
section contains definitions useful follows 
detail earlier works briefly recalled 
definition supersymmetric tensor 
tensor called supersymmetric entries invariant permutation indices 
notions scalar product norm easily extended higher orders follows 
definition tensor scalar product 
scalar product tensors order dimensions si ti definition frobenius norm 
frobenius norm tensor order defined definition matrix unfoldings 
mode matrix unfolding ofan mn tensor order entries ti defined mn mn mn mn matrix columns mn dimensional vectors obtained varying index keeping indices fixed 
readily verified supersymmetric tensor mode matrix unfoldings equal 
square matrix version supersymmetric tensors follows 
order appearance mode context 
phillip definition square matrix unfolding 
square matrix unfolding ofan supersymmetric tensor order tm ti il jl il il il jl jl jl outer product generalized higher orders follows 
definition tucker product 
tucker product matrices dimension mn yields nth order tensor dimensions mn denoted ti lu weighted outer product defined follows 
definition weighted tucker product 
weighted tucker product core ln tensor ofn matrices dimensions mn ln yields nth order mn tensor denoted ti ln ln sl ln ln easily seen standard tucker product weighted product identity tensor ii 
tucker product vectors results rank tensor follows 
definition tensor rank 
rank mn tensor minimal number terms finite decomposition form mi dimensional column vectors 
way extending svd matrices higher order tensors see 
theorem hosvd 
mn tensor expressed ass form 
best rank approximation supersymmetric tensors orthogonal mn mn matrices core tensor size sin obtained fixing kth index properties orthogonality sin sin orthogonal possible values sense ordering sin sin sin sin sin mn 
matrix computed matrix left singular vectors mode unfolding 
core tensor determined supersymmetric case equal course core tensor supersymmetric 
multilinear svd reduces called higher order eigenvalue decomposition 
notation denote symmetric product 

problem statement 
best ls rank tensor approximation problem stated close connection maximization associated polynomial form unit sphere 
property higher order equivalent analogous property holding matrices going play central role subsequent developments 
proofs general nonsymmetric case 
theorem tensor rank approximation 
nth order supersymmetric tensor consider problem determining scalar vector rm rank tensor minimizes function subject having unit norm 
unit norm vector corresponds local minimum yields local maximum ti ui ui uin corresponding value 
connection similar results second order case clearer define functional maximization corresponds maximizing 
viewed nth order rayleigh quotient squared 
recall corresponding maximization problem matrices solved dominant eigenpair eigenvalue largest absolute value 
stationary points corresponding functional ut tu ut solutions tu ut tu phillip analogous result nth order case follows 
theorem characterization stationary points 
unit norm vector stationary point functional ti ui ui uin ui equivalently times 
hopm 

general case 
iterative leads known power method determining dominant eigenpair matrix 
tensorial equivalent suggested analyzed subsection 
look hopm general necessarily supersymmetric mn tensor algorithm 
higher order power method hopm 
initialization unit norm mn vector iteration 
best rank approximation supersymmetric tensors output product implemented 
products computed iteration 
algorithm shown converge local maximum corresponding value convergence proof relies fact multilinear function linear respect 
initial estimate observed lie basin attraction globally optimal solution setting equal dominant left singular vector mode matrix unfolding 
column matrix hosvd initialization method inspired holds matrix case best rank approximant provided dominant singular triple 
shown property hold anymore higher order arrays bounds derived approximation error 
similarities hosvd second order counterpart suggest compute initial estimate hopm 

symmetric case 
pointed supersymmetric convergence algorithm supersymmetric estimator equal 
intermediate results necessarily symmetric 
shown supersymmetric tensor stationary points hopm solutions determined roots appropriate nth order polynomial 
larger supersymmetric tensors algorithm proposed constrained version suggested deemed unreliable guaranteed monotonically increase 
algorithm suggested follows 
algorithm 
symmetric higher order power method hopm 
initialization unit norm vector iteration 
uk output expression rewritten times recall matrices equal supersymmetric phillip nonsymmetric hopm symmetric hopm iteration fig 

results hopm hopm supersymmetric tensor example 
special case common bss problem take alternative form terms square matrix unfolding 
note product needs computed iteration compared general hopm 
constrained version applicable fold reduction computational complexity results 
form optimized nonlinear respect sought vector rendering convergence proof algorithm applicable 
fact example demonstrates hopm converge supersymmetric tensors example 
consider supersymmetric tensor entries 
results application general hopm symmetric version hopm depicted 
curve general hopm depicts values uk plotted hopm 
algorithms initialized hosvd 
seen hopm iterations converge 
show algorithm convergent convex concave function recall meaning property 
definition convex concave function 
function values real domain convex subset rm said best rank approximation supersymmetric tensors convex convex subset concave function function negative convex 
definition square matrix unfolding readily verified written polynomial matrix form times times necessary sufficient condition twice continuously differentiable function convex concave open convex subset hessian second derivative matrix positive negative semidefinite 
convex concave matrix times times positive negative semidefinite implies nonnegative nonpositive condition matrix satisfied positive negative semidefinite 
example holds fourth order cumulant tensor output linear mixing system hx sources sign 
case denoting diagonal tensor fourth order cumulants corresponding matrix diag cum xi rao columnwise kronecker product 
notice convex concave hold quotient unit sphere convex set 
example 
consider supersymmetric tensor contains fourth order cumulants mixture observations mixing matrix second order tensors matrices necessary condition 
phillip fig 

function tensor example restricted unit sphere 
source fourth order cumulants cum xi 
resulting function concave nonpositive 
restriction unit sphere shown 
allow variate function plotted parameterized unit norm vector cos sin cos sin sin angles normalized interval cosines constrained nonnegative 
done value assumed invariant sign changes 
clearly seen longer concave 
theorem convergence hopm 
supersymmetric nth order tensor associated function convex concave rm algorithm converges local maximum minimum restriction unit sphere initialization saddle points crest lines leading saddle points 
proof 
consider case convex 
assumption implies set convex subset rm tangent hyperplane point fact supporting hyperplane 
fact expressed called sub gradient inequality holding vectors regardless distant may 
apply problem hand set obtain want show stationary point increasing monotonically 
suffices show right hand side positive best rank approximation supersymmetric tensors note unit norm vector cauchy schwarz inequality yields equality holds uk uk precisely formula gives uk algorithm 
view implies uk increasing convergence follows fact restriction bounded denoting eigenvalue largest modulus 
case concave treated replacing point need comment quantity take minimum value occurs uk uk minus sign necessary affect value uk recall 
distinction hopm derived lagrangian equations corresponding constrained minimization problem gradient descent procedure 
shown similar arguments employed case norm function hopm fact gradient recursion strategic choice step size parameter 

properties matrix section state prove properties tensor implies square matrix unfolding 
called vec permutation matrix plays central role subsequent analysis 
definition vec permutation matrix 
vec permutation matrix defined mn mn permutation matrix satisfies equality vec matrices explicit way defining theorem 
theorem explicit characterization 
entry equal unity zero 
theorem properties general order 
square matrix unfolding supersymmetric tensor order satisfies properties 
ii phillip proof 
recall definition 
symmetry follows easily fact ti il jl tj jl il call matrix consider entries say write indices ql ql qi readily seen theorem implies writing form ll ll li yields ll tl ll ql tk ll ql ll tp proves ii 
symmetric admits eigenvalue decomposition real eigenvalues orthonormal eigenvectors eigenvalues numbered 
confine attention fourth order supersymmetric tensors 
shall denote corresponding permutation matrix easy see symmetric 
define matrices matrix versions corresponding eigenvectors 
special important case information involving revealed 
theorem properties 
square matrix unfolding fourth order supersymmetric tensor satisfies properties best rank approximation supersymmetric tensors pt tp ptp eigenvectors positive symmetry ii admits eigenvectors negative symmetry 
corresponding matrices symmetric skew symmetric respectively 
iii eigenvectors having negative symmetry correspond zero eigenvalue 
iv rank proof 
proof follows easily symmetry property ii theorem 
ii prove eigenvectors enjoy symmetries 
take ith eigenpair premultiplying equation account equality pt tp yields shows eigenvector eigenvalue iis simple corresponding eigenvector unique sign factor case multiple eigenvalue choose eigenvector invariant space desired symmetry 
example possible choice normalized version eigenvector seen orthogonal rest 
follows definition cf 
symmetries satisfied eigenvectors equivalent respectively 
introduce subspaces px px 
subspaces orthogonal orthogonality implies 
follows py dim dim shown parameterizing resp equivalent parameterizing set symmetric resp skew symmetric matrices 
dim dim write orthogonal decomposition phillip result follows fact eigenvectors belong form orthonormal basis iii property tp yields ip follows 
iv proof iv follows directly ii iii 
corollary dominant eigenvector 
dominant eigenvector square matrix unfolding nonzero supersymmetric fourth order tensor satisfies 
equivalently matrix version symmetric 
proof 
proof follows theorem iii 
new initialization 
derive alternative initialization scheme hopm fourth order tensors observed effective hosvd approaching globally optimum point 
starting point inequality equality coincides dominant eigenvector 
global maximum cf 
attained written kronecker square general true 
suggests way computing initial estimate setting equal best ls sense kronecker square root equivalently arg min 
arg min ss 
symmetric see corollary problem solved setting equal unit norm eigenvector corresponds absolutely largest eigenvalue say 
note 
proposed initialization method involves symmetric matrix rank approximation problems new initialization 
dominant eigenvector 
dominant eigenvector 
written iu vec permutation matrix im multi respectively property stated 
best rank approximation supersymmetric tensors known identity vec abc vec choosing yields fact iu sign semi definite convex concave relation implies conjunction provides lower upper bounds initial value follows 
theorem bounds initial value 
value assumed suggested initialization applied supersymmetric tensor sign semi definite square matrix unfolding bounded absolutely largest eigenvalues matrices respectively 
initial value approaches global maximum vector approaches kronecker decomposability approaches 
example continued 
consider tensor 
corresponding matrix singular values agreeing theorem iii 
ran hopm tensor hosvd new initialization methods 
results shown 
cases iterations converge global minimum 
new initialization scheme seen lie closer globally optimal solution hosvd scheme 
fact example successive iterations nearly superfluous 
extensive simulations shown typical case tensors associated functional convex concave 
superior performance new method seen position initial estimates parameter space trajectories followed case shown 
lower upper bounds theorem respectively seen satisfied initial value assumed new initialization scheme 
note initial value suggested hosvd scheme meet lower bound 
examples initialization methods leads local extremum 
cases rare assumes suboptimal point value quite close optimal 
example 
tensor note nonzero eigenvalues sources 
number nonzero reveal number sources case phillip new method hosvd iteration fig 

results hopm hosvd new initializations 
fig 

visualization hopm algorithm initialization methods example 
hosvd initial estimate denoted small circle trajectory followed initial estimate provided proposed method denoted small square subsequent estimates best rank approximation supersymmetric tensors hosvd new method iteration fig 

results hopm initial estimate hosvd new method 
case algorithm trapped local minimum 
source cumulants cum xi 
shown hopm initialized aid hosvd trapped local minimum 
bounds theorem new initialization method yields initial value initial value provided method satisfy lower bound 
example 
converse seen happen tensor built cum xi 
shows new initialization method leads local minimum time 
see significant difference values local global minima 
shows evolution algorithm parameter space initializations 
example bounds theorem met initial values provided new initialization method hosvd respectively 
phillip new method hosvd iteration fig 

results hopm hosvd new initialization methods 
leads local minimum 
fig 

function tensor example 
notice local minimum corresponding value function quite close global minimum 
best rank approximation supersymmetric tensors fig 

visualization initial estimates trajectories followed hopm example initialization methods 
symbols 
best rank approximation 
known successively subtracting ls rank approximation matrix times results ls rank approximation 
wonder fact holds higher order tensors pointed 
unfortunately typical example demonstrates case 
example 
take tensor described example normalized unit norm determine best rank approximant tensor 
shown norm residue decreases practically zero done iterations rank 
interest note rank remains equal way 
iterative process new rank term constrained orthogonal previous ones studied 
depending definition orthogonality adopted fails certain provide valid rank approximation scheme 
process case tensor rank equal dimension case bss context mixing matrix tall fact case rank rank terms jointly determined minimizing norm subject constraint diagonal full column rank usually assumed orthonormal columns 
works coping problem include 
algebraic approach joint diagonalization matrices defined proposed 
note step initialization method part diagonalization 
problem recovery single source rank approximation method addresses 
applicable bss problems fat mixing matrices see example 
phillip rank terms subtracted fig 

norm residue remaining successively subtracting rank terms tensor example 
spike near iteration may due numerical artifacts 
challenging problem develops algebraic method determining assumption linear independence projectors spaces spanned columns 
seen equivalent matrix occurs having full column rank 
algebraic optimization approaches expanding supersymmetric tensor sum rank rank terms developed representation terms homogeneous polynomial 
problem expressing polynomial sum powers linear forms 
workable algorithms derived way appear limited small sized tensors 
hopm viewed special case alternating squares als iterative approach common problems multilinear model fitting multidimensional data 
als method change compute rank approximation supersymmetric tensor way rich symmetry problem exploited 


problem computing best ls sense rank approximation nth order supersymmetric tensor studied 
symmetric version higher order power method thought unreliable shown convergent tensors associated polynomial form convex concave 
new method initializing iterations developed fourth order case observed extensive simulations provide estimate lies closer globally optimal solution yielded hosvd 
proximity optimal solution priori quantifiable 
happens rarely initial estimate provided hosvd best rank approximation supersymmetric tensors scheme proposed basin attraction locally optimal solution 
serious problem cases encountered quality approximation corresponding local optimum observed quite close best attainable 
byproduct study rank approximation problem properties satisfied square matrix unfolding supersymmetric tensor derived 
applicability symmetric higher order power method accompanied significant reduction computational complexity general version 
convexity concavity assumptions required prove convergence plausible signal processing applications blind separation multiuser communications channels source signals sign 
easy task extend results obtained general problem computing best rank approximation supersymmetric tensor 
simply imposing symmetry constraint als method fitting models general tensors way hopm gave rise hopm result convergent procedure 

cardoso super symmetric decomposition fourth order cumulant tensor 
blind identification sources sensors proceedingsof ieee international conference acoust speech signal processing icassp toronto canada 

cardoso blind beamforming non gaussian signals proc 
iee pp 

comon blind channel identification extraction sources sensors proc 
spie conference advanced signal processing viii san diego 
comon tensor decompositions state art applications keynote address ima conf 
mathematics signal processing warwick uk 
comon chevalier blind source separation models concepts algorithms performance unsupervised adaptive filtering vol 
haykin ed john wiley new york 
comon decomposition sums powers linear forms signal process pp 

de lathauwer signal processing multilinear algebra ph dissertation katholieke universiteit leuven belgium 
de lathauwer comon de moor vandewalle higher order power method application independent component analysis proceedingsof international symposium nonlinear theory nv pp 

de lathauwer de moor vandewalle multilinear singular value decomposition siam matrix anal 
appl pp 

de lathauwer de moor vandewalle best rank rank rn approximation higher order tensors siam matrix anal 
appl pp 

golub van loan matrix computations nd ed press baltimore md 
multilinear algebra springer verlag berlin 
henderson searle vec permutation matrix vec operator kronecker products review linear multilinear algebra pp 

hyv rinen oja fast fixed point algorithm independent component analysis neural comput pp 

hyv rinen fast robust fixed point algorithms independent component analysis ieee trans 
neural networks pp 

tensor approximation signal processing applications structured mathematics computer science engineering vol 
contemp 
math 
ed ams providence ri 
orthogonal tensor decompositions siam matrix anal 
appl pp 

phillip singular value decompositions interactions way contingency tables multiway data analysis eds elsevier science publishers north holland pp 

gradient search interpretation super exponential algorithm ieee trans 
inform 
theory pp 

mccullagh tensor methods statistics chapman hall new york 
papadias globally convergent blind source separation multiuser kurtosis maximization criterion ieee trans 
signal process pp 

higher order power method revisited convergence proofs effective initialization proceedingsof ieee international conference acoust speech signal processing icassp istanbul turkey 
unimodal blind equalization criterion proceedingsof european signal processing conference eusipco tampere finland 
properties blind equalization criteria noisy multiuser environments ieee trans 
signal process pp 

rockafellar convex analysis princeton university press princeton nj 
weinstein super exponential methods blind deconvolution ieee trans 
inform 
theory pp 

sidiropoulos bro techniques signal separation signal processing wireless mobile communications single multi user systems giannakis hua stoica tong eds prentice hall englewood cliffs nj 
stewart matrix computations academic press new york 
