bundle adjustment modern synthesis bill triggs philip mclauchlan richard hartley andrew fitzgibbon inria rh ne alpes avenue de europe montbonnot france 
bill triggs inrialpes fr www inrialpes fr people triggs school electrical engineering information technology mathematics university surrey guildford gu xh mclauchlan ee surrey ac uk www ee surrey ac uk personal mclauchlan general electric crd ny hartley crd ge com dept engineering science university oxford parks road ox pj robots ox ac uk www robots ox ac uk survey theory methods photogrammetric bundle adjustment aimed potential implementors computer vision community 
bundle adjustment problem refining visual reconstruction produce jointly optimal structure viewing parameter estimates 
topics covered include choice cost function robustness numerical optimization including sparse newton methods linearly convergent approximations updating recursive methods gauge datum invariance quality control 
theory developed general robust cost functions restricting attention traditional nonlinear squares 
keywords bundle adjustment scene reconstruction gauge freedom sparse matrices optimization 
survey theory methods bundle adjustment aimed computer vision community especially potential implementors know little bundle methods 
results appeared long ago photogrammetry geodesy literatures little known vision gradually reinvented 
providing accessible modern synthesis hope duplication effort correct common misconceptions speed progress visual reconstruction promoting interaction vision photogrammetry communities 
bundle adjustment problem refining visual reconstruction produce jointly optimal structure viewing parameter camera pose calibration estimates 
optimal means supported part european commission esprit ltr project triggs uk epsrc project gr mclauchlan royal society fitzgibbon 
zisserman gr valuable comments 
version appear vision algorithms theory practice triggs zisserman szeliski eds springer verlag lncs 
parameter estimates minimizing cost function quantifies model fitting error jointly solution simultaneously optimal respect structure camera variations 
name refers bundles light rays leaving feature converging camera centre adjusted optimally respect feature camera positions 
equivalently independent model methods merge partial reconstructions updating internal structure structure camera parameters adjusted bundle 
bundle adjustment really just large sparse geometric parameter estimation problem parameters combined feature coordinates camera poses calibrations 
say applied similar estimation problems vision photogrammetry industrial metrology surveying geodesy 
adjustment computations major common theme measurement sciences basic theory methods understood easy adapt wide variety problems 
adaptation largely matter choosing numerical optimization scheme exploits problem structure sparsity 
consider schemes bundle adjustment 
classically bundle adjustment similar adjustment computations formulated nonlinear squares problems 
cost function assumed quadratic feature reprojection errors robustness provided explicit outlier screening 
flexible model really general 
modern systems non quadratic estimator distributional models handle outliers include additional penalties related overfitting model selection system performance priors mdl 
reason assume squares quadratic cost model 
cost modelled sum opaque contributions independent information sources individual observations prior distributions overfitting penalties 
functional forms contributions dependence fixed quantities observations usually left implicit 
allows different types robust non robust cost contributions incorporated unduly cluttering notation hiding essential model structure 
fits modern sparse optimization methods cost contributions usually sparse functions parameters object centred software organization avoids tedious displays chain rule results 
implementors assumed capable choosing appropriate functions calculating derivatives 
aim correct number misconceptions common vision literature optimization bundle adjustment slow statements appear papers introducing heuristic structure motion sfm iteration 
claimed slowness due general purpose optimization routine completely ignores problem structure sparseness 
real bundle routines efficient usually considerably efficient flexible newly suggested method 
bundle adjustment remains dominant structure refinement technique real applications years research 
linear algebra required variant presumably meant imply new technique especially simple 
virtually iterative refinement techniques linear algebra bundle adjustment simpler solves linear systems eigen decomposition svd complex iterative methods 
sequence vision workers resistant idea reconstruction problems planned advance results checked verify reliability 
system builders aware basic techniques application constraints difficult 
extraordinary extent weak geometry lack redundancy mask gross errors seldom appreciated 
point reconstructed accurately reconstruction just absolute position uncertainty 
coordinate frame uncertain located relative uncertain reconstructed features cameras 
feature camera uncertainties expressed relative frame inherit uncertainty statements meaningless frame uncertainty specified 
covariances look completely different different frames particularly object centred versus camera centred ones 
see 
tendency vision develop profusion ad hoc adjustment iterations 
bundle adjustment methods 
flexibility bundle adjustment gracefully handles wide variety different feature camera types points lines curves surfaces exotic cameras scene types including dynamic articulated models scene constraints information sources features intensities information priors error models including robust ones 
problems missing data 
accuracy bundle adjustment gives precise easily interpreted results uses accurate statistical error models supports sound developed quality control methodology 
efficiency mature bundle algorithms comparatively efficient large problems 
economical rapidly convergent numerical methods near optimal problem sparseness 
general computer vision reconstruction technology matures expect bundle adjustment predominate alternative adjustment methods way photogrammetry 
see inevitable consequence greater appreciation optimization notably effective problem structure sparseness systems issues quality control network design 
coverage touch aspects bundle methods 
start considering camera projection model parametrization bundle problem choice error metric cost function 
gives rapid sketch optimization theory 
discusses network structure parameter interactions characteristic sparseness bundle problem 
sections consider types implementation strategies adjustment computations covers second order newton methods adjustment algorithms covers methods order convergence ad hoc methods class discusses solution updating strategies recursive filtering bundle methods 
returns theoretical issue gauge freedom datum deficiency including theory inner constraints 
goes detail quality control methods monitoring accuracy reliability parameter estimates 
gives brief hints network design place shots ensure accurate reliable reconstruction 
completes body summarizing main giving provisional recommendations methods 
appendices 
gives brief historical overview development bundle methods literature 
gives technical details matrix factorization updating covariance calculation methods 
gives hints designing bundle software pointers useful resources internet 
ends glossary 
general cultural differences difficult vision workers read photogrammetry literature 
collection edited atkinson manual relatively accessible introductions close range aerial photogrammetry 
accessible tutorial papers include 
kraus probably widely photogrammetry textbook 
brown early survey bundle methods worth reading 
cited manual edited quite dated presentation bundle adjustment relevant 
wolf text devoted adjustment computations emphasis surveying 
hartley zisserman excellent textbook covering vision geometry computer vision viewpoint 
nonlinear optimization fletcher gill traditional texts nocedal wright modern 
linear squares bj superlative lawson hanson older text 
general numerical linear algebra golub van loan standard 
duff george liu standard texts sparse matrix techniques 
discuss initialization methods bundle adjustment detail appropriate reconstruction methods plentiful known vision community 
see 
notation structure cameras estimated parametrized single large state vector general state belongs nonlinear manifold linearize locally small linear state displacements denoted observations measured image features denoted corresponding predicted values parameter value denoted residual prediction error 
observations prediction errors usually appear implicitly influence cost function 
cost function gradient df dx hessian dx observation state jacobian dz dimensions nx nz 
dx projection model problem parametrization projection model development bundle adjustment considering basic image projection model issue problem parametrization 
visual reconstruction attempts recover model scene multiple images 
part usually recovers poses positions orientations cameras took images information internal parameters 
simple scene model collection isolated features points lines planes curves surface patches 
far complicated scene models possible involving complex objects linked constraints articulations photometry geometry dynamics great strengths adjustment computations reason thinking considerable vision ability take complex heterogeneous models stride 
predictive parametric model handled model predicts values known measurements descriptors basis continuous parametric representation world estimated measurements 
similarly possible camera models exist 
perspective projection standard affine orthographic projections useful distant cameras exotic models push broom rational polynomial cameras needed certain applications 
addition pose position orientation simple internal parameters focal length principal point real cameras require various types additional parameters model internal radial distortion :10.1.1.14.6358
simplicity suppose scene modelled individual static features xp imaged shots camera pose internal calibration parameters pi calibration parameters cc constant images depending cameras 
uncertain measurements ip subset possible image features true image feature xp image 
observation ip assume predictive model cc pi xp parameters derive feature prediction error cc pi xp ip cc pi xp case image observations predictive model image projection observation types measurements included 
estimate unknown feature camera parameters observations reconstruct scene minimize measure discussed total prediction error 
bundle adjustment model refinement part starting initial parameter estimates approximate reconstruction method 
essentially matter optimizing complicated nonlinear cost function total prediction error large nonlinear parameter space scene camera parameters 
go analytical forms various possible feature image projection models affect general structure adjustment network tend obscure central simplicity 
simply stress bundle framework flexible handle desired model 
different combinations features image projections measurements best regard black boxes capable giving measurement predictions current parameters 
optimization possibly second derivatives respect parameters needed 
take quite view situation collecting scene camera parameters estimated large state vector representing cost total fitting error function 
cost really function feature prediction errors ip cc pi xp 
observations ip constants adjustment calculation leave cost dependence projection model implicit display dependence parameters adjusted 
bundle parametrization bundle adjustment parameter space generally high dimensional nonlinear manifold large cartesian product projective feature rotation camera calibration manifolds nonlinear constraints state strictly speaking vector point space 
depending entities contains represented subject various types complications including singularities internal constraints unwanted internal degrees freedom 
arise geometric entities rotations lines projective points planes simple global parametrizations 
local parametrizations nonlinear singularities prevent covering parameter space uniformly variants euler angles rotations singularity affine point coordinates infinity 
global parametrizations constraints quaternions unwanted internal degrees freedom homogeneous projective quantities scale factor freedom points defining line slide line 
complicated compound entities matching tensors assemblies features linked coincidence parallelism orthogonality constraints parametrization delicate 
vision geometry error model essentially projective 
affine parametrization introduces artificial singularity projective infinity may cause numerical problems distant features 
principle equivalent different parametrizations profoundly different numerical behaviours greatly affect speed reliability adjustment iteration 
suitable parametrizations optimization uniform finite behaved possible near current state estimate 
ideally locally close linear terms effect chosen error model cost function locally nearly quadratic 
nonlinearity hinders convergence reducing accuracy second order cost model predict state updates 
excessive correlations parametrization singularities cause ill conditioning erratic numerical behaviour 
large infinite parameter values reached excessively finite adjustment steps 
parametrization usually behaved sense relatively small section state space 
guarantee uniformly performance state may represented state updates evaluated stable local parametrization increments current estimate 
examples consider points rotations 
points calibrated cameras vision geometry visual reconstructions intrinsically projective 
parametrization equivalently homogeneous affine distant points large displacements needed change image significantly 
space cost function flat steps needed cost adjustment large distant points 
comparison homogeneous projective parametrization zw behaviour near infinity natural finite conditioned long normalization keeps homogeneous vector finite infinity sending 
fact immediate visual distinction images real points near infinity virtual ones camera geometries admit virtual points bona fide projective constructs 
optimal reconstruction real point may virtual sense image noise happens push infinity 
reconstructed point wandering infinity back optimization 
sounds bizarre inescapable consequence fact natural geometry error model visual reconstruction projective affine 
projectively infinity just place 
affine parametrization acceptable points near origin close range convergent camera geometries disastrous distant ones artificially cuts away half natural parameter space hides fact sending resulting edge infinite parameter values 
homogeneous parametrization zw distant points spherical normalization 
rotations similarly experience suggests quasi global parameter rotation parametrizations euler angles cause numerical problems certain avoid singularities regions uneven coverage 
rotations parametrized quaternions subject local perturbations existing rotation wellbehaved parameter small rotation approximation rodriguez formula local euler angles state updates just state vectors represent points nonlinear space state updates represent displacements nonlinear space represented exactly vector addition 
assume locally linearize state manifold locally resolving internal constraints freedoms may subject produce unconstrained vector possible local state displacements 
taylor expansion form local cost model df dx dx estimate state update optimizes model 
displacement need structure representation behaved local parametrization represent generally able update state displacement produce new state estimate 
write operation may involve considerably vector addition 
example apart change representation updated quaternion dq need normalization corrected small rotation update form general give exact rotation matrix 
error modelling turn choice cost function quantifies total prediction image reprojection error model parametrized combined scene camera parameters main robust statistically error metrics total inlier outlier log likelihoods correctly allow presence outliers 
argue length poorly understood 
traditional treatments adjustment methods consider squares albeit data trimming robustness discussions robust statistics give impression choice estimator wholly matter personal data statistics 
bundle adjustment essentially parameter estimation problem 
parameter estimation paradigm consider optimal point estimators output definition single parameter vector minimizes predefined cost function designed measure model fits observations background knowledge 
framework covers practical estimators including maximum likelihood ml maximum posteriori map explicit bayesian model averaging 
regularization model selection terms easily incorporated cost 
typical ml cost function summed negative log likelihoods prediction errors observed image features 
gaussian error distributions reduces sum squared covariance weighted prediction errors 
map estimator typically add cost terms giving certain structure camera calibration parameters bias expected values 
cost function tool statistical interpretation 
extent lower costs uniformly better provides natural model preference ordering cost iso surfaces minimum define natural confidence regions 
locally regions nested ellipsoids centred cost minimum size shape characterized dispersion matrix inverse cost function hessian dx minimum 
residual cost minimum test statistic model validity 
negative log likelihood cost model gaussian error distributions twice residual variable 
desiderata cost function adjustment computations go considerable lengths optimize large nonlinear cost model reasonable require refinement improve estimates objective albeit statistical sense 
heuristically motivated cost functions usually guarantee 
lead biased parameter estimates severely biased ones 
large body statistical theory points maximum likelihood ml bayesian cousin maximum posteriori map estimators choice 
ml simply selects model total probability observed data highest saying thing different words total posterior probability model observations highest 
map adds prior term representing background information 
ml just easily included prior additional observation far estimation concerned distinction ml map prior observation purely terminological 
information usually comes independent sources 
bundle adjustment include covariance weighted reprojection errors individual image features measurements positions control points gps inertial sensor readings predictions uncertain dynamical models kalman filtering dynamic cameras scenes prior knowledge expressed soft constraints camera calibration pose values supplementary sources overfitting regularization description length penalties 
note variety 
great strengths adjustment computations ability combine information disparate sources 
assuming sources statistically independent model total probability model combined data product probabilities individual sources 
get additive cost function take logs total log likelihood model combined data sum individual source log likelihoods 
properties ml estimators apart obvious simplicity intuitive appeal ml map estimators strong statistical properties 
notable ones asymptotic apply limit large number independent measurements precisely central limit posterior distribution effectively gaussian particular mild regularity conditions observation distributions posterior distribution ml estimate converges asymptotically probability gaussian covariance equal dispersion matrix 
ml estimate asymptotically zero bias lowest variance unbiased estimator 
sense ml estimation method non asymptotically dispersion necessarily approximation covariance ml estimator 
asymptotic limit usually assumed valid designed photogrammetric measurement networks sampling empirical studies posterior likelihood surfaces suggest case clear small vision geometry problems weaker networks 
needed 
cost additive measurements type added entire cost surface grows direct proportion amount data nz 
means relative sizes cost derivatives size region minimum second order taylor terms dominate higher order ones remain roughly constant nz increases 
region total cost roughly quadratic cost function taken posterior log likelihood posterior distribution roughly gaussian 
curvature quadratic inverse dispersion matrix increases data added posterior standard deviation shrinks nz nx characterizes average standard deviation single observation 
nz nx essentially entire posterior probability mass lies inside quadratic region posterior distribution converges asymptotically probability gaussian 
happens proper isolated cost minimum second order taylor expansion locally valid 
approximation gets better data stronger curvature smaller higher order taylor terms 
result follows cram rao bound says covariance unbiased estimator bounded fisher information mean curvature posterior log likelihood surface log dx posterior probability parameters estimated estimate unbiased estimator true underlying value denotes positive asymptotically posterior distribution gaussian fisher information converges inverse dispersion curvature posterior log likelihood surface cost minimum ml estimate attains cram rao bound 
gaussian pdf cauchy pdf gaussian log likelihood cauchy log likelihood samples cauchy gaussian distribution cauchy gaussian beware treating bell shaped observation distribution gaussian 
despite narrower peak broader tails probability density function cauchy distribution look different gaussian top left 
negative log likelihoods different bottom left large deviations outliers probable cauchy variates gaussian ones right 
fact cauchy distribution infinite covariance 
effect incorrect error models clear incorrect modelling observation distributions disturb ml estimate 
extent inevitable error distributions stand influences fully predict control 
understand distortions unrealistic error models cause realize geometric fitting really special case parametric probability density estimation 
set parameter values geometric image projection model assumed observation error models combine predict probability density observations 
maximizing likelihood corresponds fitting predicted observation density observed data 
geometry camera model enter indirectly influence predicted distributions 
accurate noise modelling just critical successful estimation accurate geometric modelling 
important failure take account possibility outliers data values caused incorrect feature correspondences 
stress long assumed error distributions model behaviour data fit including inliers outliers properties ml estimation including asymptotic minimum variance remain valid presence outliers 
words ml estimation naturally robust need long realistic error distributions place 
distribution models inliers outliers called total distribution 
need separate classes ml estimation care distinction 
total distribution happens explicit mixture inlier outlier distribution gaussian locally uniform background outliers outliers labeled fitting likelihood ratio tests way essential estimation process 
important realize extent superficially similar distributions differ gaussian equivalently extraordinarily rapidly tails gaussian distribution fall away compared realistic models real observation errors 
see 
fact unmodelled outliers typically severe effects fit 
see suppose real observations drawn fixed unknown underlying distribution 
law large numbers says empirical distributions observed distribution set samples converge asymptotically probability 
model negative log likelihood cost sum log pmodel zi converges nz log pmodel dz 
model independent constant nz times relative entropy kullback leibler divergence log pmodel dz model distribution true 
model family include ml estimate converges asymptotically model predicted observation distribution minimum relative entropy 
see proposition 
follows ml estimates typically sensitive unmodelled outliers regions relatively probable highly improbable model large contributions relative entropy 
contrast allowing outliers occur causes relatively little distortion region probable large log pmodel 
summary possibility outliers non robust distribution models gaussians replaced realistic long tailed ones mixtures narrow inlier wide outlier density cauchy densities densities defined piecewise central peaked inlier region surrounded constant outlier region emphasize poor robustness due entirely unrealistic distributional assumptions maximum likelihood framework naturally robust provided total observation distribution including inliers outliers modelled 
fact real observations seldom cleanly divided inliers outliers 
hard core outliers feature correspondence errors grey area features reason specularity shadow poor focus motion blur 
accurately located features clearly outliers 
nonlinear squares basic parameter estimation methods nonlinear squares 
suppose vectors observations zi predicted model zi zi vector model parameters 
nonlinear squares takes estimates parameter values minimize weighted sum squared error sse cost function zi wi zi zi zi zi zi feature prediction error wi arbitrary symmetric positive definite spd weight matrix 
modulo normalization terms independent weighted sse cost function coincides negative log likelihood observations zi perturbed gaussian noise mean zero covariance squares useful statistical interpretation wi chosen approximate inverse measurement covariance zi non gaussian noise mean covariance gauss markov theorem states models zi case corresponds hard inlier outlier decision rule observation outlier region density constant observation influence fit 
similarly mixture case corresponds softer inlier outlier decision rule 
linear squares gives best linear unbiased estimator blue best means minimum variance weighted squares model converted unweighted wi zi zi zi satisfying wi li li calculated efficiently wi cholesky decomposition 
zi zi called standardized residual resulting unweighted squares problem minx zi standard form 
advantage optimization methods linear squares solvers place ones linear normal equation solvers allows problems handled stably 
peculiarity sse cost function indifference natural boundaries observations 
observations zi sources assembled compound observation vector 
weight matrices wi assembled compound block diagonal weight matrix diag 
wk weighted squared error original sse cost function zi wi zi 
general quadratic form sse cost preserved compounding arbitrary linear transformations mix components different observations 
place underlying structure visible block structure invariances hold essentially cost function simplify formulation squares considerably 
squares main problem squares high sensitivity outliers 
happens gaussian extremely small tails compared real measurement error distributions 
robust estimates choose realistic likelihood model 
exact functional form important general way expected types outliers enter 
single blunder correspondence error may affect observations usually leave unchanged 
locality basis 
decide observations affected weight eliminate remaining observations parameter estimates usual 
observations affected equally incorrect projection model know wrong able fix simple data cleaning 
adopt single layer robustness model observations partitioned independent groups group irreducible sense accepted weighted rejected independently groups 
partitions reflect types occur 
example feature correspondence errors common coordinates single image point naturally form group usually invalidated blunder image point affected 
coordinates appeared correct incorrect usually want discard safety 
hand stereo problems coordinates pair corresponding image points natural grouping point image useless correspondent 
henceforth say observation mean irreducible group observations treated unit model 
observations need scalars units probabilistically independent irrespective inliers outliers 
may possible useful better biased correct solution nonlinear estimators 
usual independent observation contributes independent term fi total cost function 
form depending expected total distribution inliers outliers observation 
natural family radial distributions negative log likelihoods form fi wi increasing function ds 
guarantee vanishes fi dz wi 
weighted sse robust variants sublinear tending constant distant outliers entirely ignored 
dispersion matrix determines spatial spread zi scale covariance finite 
radial form preserved arbitrary affine transformations zi group observations equal footing sense squares 
non gaussian radial distributions separable observations zi split independent subgroups combined larger groups destroying radial form 
radial cost models remarkable isotropy non robust sse exactly wanted ensures observations group left weighted 
example image features polluted occasional large outliers caused correspondence errors model error distribution gaussian central peak plus uniform background outliers 
give negative log likelihood contributions form log exp ip non robust weighted sse model ip ip ip wip squared weighted residual error variable correct model gaussian error distribution frequency outliers 
intensity methods gaussian log likelihood log likelihood models apply geometric image features intensity matching image patches 
case observables image gray scales colors feature coordinates error model intensity residuals 
get point projection model intensity simply compose assumed local intensity model obtained image template image matching point jacobians point intensity jacobians di full range intensity mod du els implemented framework pure translation affine quadratic patch deformation models model intensity predictions coupled affine spline patches surface coverage 
structure intensity bundle problems similar feature ones techniques studied applied 
go detail intensity matching note real basis feature methods 
feature detectors optimized detection localization 
localize detected feature accurately need match function image intensities region idealized template image feature appropriate geometric deformation model example suppose intensity matching model integration image patch current intensity prediction error local geometry patch translation warping intensity error 
cost gradient terms df du di similarly du cost hessian gauss newton approximation hu du di di feature du du model express function bundle parameters ju du dx corresponding cost gradient hessian contribution ju hx hu ju 
words intensity matching model locally equivalent quadratic feature matching features effective weight inverse covariance matrix wu hu 
image feature error models vision ultimately underlying intensity matching model 
feature covariances function intensity gradients di di highly variable du du tween features depending local gradient highly anisotropic depending directional gradients 
points intensity edge uncertainty large edge direction small edge 
implicit models observations naturally expressed terms implicit observation constraining model explicit observation predicting 
associated image error form 
example model curve observe points noisy images points may lie curve predict image curve exact position observation 
constraint noiseless image observed point lie noiseless image curve knew 
basically ways handle implicit models nuisance parameters reduction 
nuisance parameters approach model explicit adding additional nuisance parameters representing equivalent model consistent estimates unknown noise free observations 
direct way include entire parameter vector nuisance parameters solve constrained optimization problem extended parameter space minimizing subject 
sparse constrained problem solved efficiently sparse matrix techniques 
fact image observations subproblems optimizing fixed small typical simple 
spite extra parameters optimizing model significantly expensive optimizing explicit 
example estimating matching constraints image pairs triplets explicit representation pairs triplets corresponding image points features zi subject epipolar trifocal geometry contained 
smaller nuisance parameter vector wise 
case curve suffices include just nuisance parameter observation saying curve corresponding noise free observation predicted lie 
model exactly satisfies constraints converts implicit model unconstrained explicit curve nuisance parameters 
advantage nuisance parameter approach gives exact optimal parameter estimate jointly optimal consistent estimates noise free observations reduction alternatively regard observation vector fit parameters explicit log likelihood model 
transfer underlying error model distribution 
principle done marginalization density integrating giving point estimation framework approximated replacing integration maximization 
calculation easy general asymptotic limit order taylor expansion dh valid distribution dz marginalization maximization affine subspaces 
evaluated closed form robust distributions 
standard covariance propagation gives precisely applies dh dz dh dz dh dz dh dz covariance outlier free gaussian model reduced distribution remains gaussian albeit dependent covariance 
basic numerical optimization having chosen suitable model quality metric optimize 
section gives rapid sketch basic local optimization methods differentiable functions 
see details 
need minimize cost function parameters starting initial estimate minimum presumably supplied approximate visual reconstruction method prior knowledge approximate situation 
parameter space may nonlinear assume local displacements parametrized local coordinate system vector free parameters try find displacement locally minimizes reduces cost function 
real cost functions complicated minimize closed form minimize approximate local model function taylor expansion approximation current point usually give exact minimum luck improve initial parameter estimate allow iterate convergence 
art reliable optimization largely details happen luck local model minimize ensure estimate improved decide convergence occurred 
interested subjects professionally designed package details important 
second order methods local models quadratic taylor series df dx dx quadratic local model gradient vector hessian matrix assume hessian positive definite see 
local model simple quadratic unique global minimum explicitly linear algebra 
setting df zero stationary point gives newton step dx estimated new function value iterating newton step gives newton method 
canonical optimization method smooth cost functions owing exceptionally rapid theoretical practical convergence near minimum 
quadratic functions converges iteration general analytic ones asymptotic convergence quadratic soon estimate gets close solution second order taylor expansion reasonably accurate residual state error approximately squared iteration 
means number significant digits estimate approximately doubles iteration starting reasonable estimate log iterations needed full double precision digit accuracy 
methods potentially achieve rapid asymptotic convergence called second order methods 
high local optimization method achieved newton step asymptotically approximated 
despite conceptual simplicity asymptotic performance newton methods disadvantages guarantee convergence suitable step control policy added 
solving newton step equations takes time dense system prohibitive large cost reduced substantially bundle adjustment exploiting sparseness remains true newton methods tend high cost iteration increases relative methods problem size increases 
reason worthwhile consider approximate order methods occasionally efficient generally simpler implement sparse newton methods 
calculating second derivatives means trivial complicated cost function computationally terms implementation effort 
gauss newton method offers simple analytic approximation nonlinear squares problems 
methods build approximations way gradient changes iteration see krylov methods 
asymptotic convergence newton methods felt expensive luxury far minimum especially damping see active 
said newton methods generally require significantly fewer iterations order ones far minimum 
step control unfortunately newton method fail ways 
may converge saddle point minimum large steps second order cost prediction may inaccurate guarantee true cost decrease 
guarantee convergence minimum step follow local descent direction direction non negligible component local cost gradient gradient zero near saddle point negative curvature direction hessian reasonable progress direction little optimization runs slowly stalls greatly overshoots cost minimum direction 
necessary decide iteration converged limit large steps requested 
topics form delicate subject step control 
choose descent direction take newton step direction descends may near saddle point generally combination newton gradient directions 
damped newton methods solve regularized system find step weighting factor positive definite weight matrix identity gradient descent 
chosen limit step dynamically chosen maximum size trust region methods manipulated heuristically shorten step prediction poor levenberg marquardt methods 
descent direction progress usually assured line search method quadratic cubic cost models 
suggested newton step line search finds minimizes line simply estimate 
space details step control see 
note poor step control huge difference reliability convergence rates especially ill conditioned problems 
familiar issues advisable professionally designed methods 
gauss newton squares consider nonlinear weighted sse cost model prediction error weight matrix differentiation gives gradient hessian terms jacobian design matrix predictive model dz dx df dx wj dx wj zi dx formulae directly damped newton method zi dx term small comparison corresponding components prediction error small ii model nearly linear zi dx 
dropping second term gives gauss newton approximation squares hessian wj 
approximation newton step prediction equations gauss newton normal equations wj gauss newton approximation extremely common nonlinear squares practically current bundle implementations 
main advantage simplicity second derivatives projection model complex troublesome implement 
fact normal equations just methods solving weighted linear squares problem min 
notable method qr decomposition factor slower normal equations sensitive ill conditioning whichever solution method main disadvantage gauss newton approximation discarded terms negligible convergence rate greatly reduced 
dependence ignored amounts thing ignoring zi dx term qr method gives solution relative error compared normal equations condition number ratio largest smallest singular value machine precision double precision floating point 
experience reductions common highly nonlinear problems current step large residuals 
example near saddle point gauss newton approximation accurate predicted hessian positive semidefinite 
locally near linear bundle problems outlier free squares cost model evaluated near cost minimum gauss newton approximation usually accurate 
feature extraction errors characteristic scales pixels 
contrast nonlinearities caused nonlinear feature camera geometry perspective effects nonlinear image projection lens distortion 
typical geometries lenses effect varies significantly scale pixels 
nonlinear corrections usually small compared leading order linear terms bundle adjustment behaves near linear small residual problem 
note extend robust cost models 
works introducing strong nonlinearity cost function scale typical feature reprojection errors 
accurate step prediction optimization routine take account 
radial cost functions reasonable compromise take account exact second order derivatives retaining order gauss newton approximation predicted observations zi 
respectively second derivatives current evaluation point gauss newton approximation gi wi hi wi wi zi wi zi ji effects weights entire observation gi hi ii rank reduction curvature hi radial direction account way weight changes residual 
reweighting optimization methods include effect 
find true cost minimum gi evaluated exactly convergence may slowed owing inaccuracy especially mainly radial deviations produced non robust initializers containing outliers 
hi direction negative curvature wi zi reduce gauss newton model local unweighted sse linear squares methods 
simplicity suppose wi reduced premultiplying zi ji li wi 
minimizing effective squared error zi ji gives correct second order robust state update zi zi ji zi zi ji practice zi formulae limit small 
full curvature correction applied case 
useful sublinear outlier region 
reweighting vision handle projective homogeneous scale factors error weighting 
suppose image points generated homogeneous projection equation wherep homogeneous image projection matrix 
scale factor reweighting scheme take derivatives treating inverse weight constant iteration 
minimizing resulting globally bilinear linear squares error model give true cost minimum zeros gradient ignoring variations true cost gradient 
schemes precise bias substantial especially wide angle lenses close geometries 
constrained problems generally may want minimize function subject set constraints scene constraints internal consistency constraints parametrization constraints arising implicit observation model 
initial estimate solution try improve optimizing quadratic local model subject linear local model constraints linearly constrained quadratic problem exact solution linear algebra 
gradient hessian order expansion constraints dc introduce vector lagrange multipliers dx seek optimizes subject dx combining gives sequential quadratic programming sqp step ch ch ch optimum vanish generally non zero 
alternative constrained approach uses linearized constraints eliminate variables optimizes rest 
suppose order variables give partitions wherec square invertible 
cx solve terms 
substituting quadratic cost model effect eliminating leaving smaller unconstrained reduced problem matrices evaluated efficiently simple matrix factorization schemes 
method stable provided chosen conditioned 
works dense problems suitable sparse ones dense reduced hessian dense 
squares cost models constraints handled linear squares framework see 
general implementation issues going details mention points numerical practice large scale optimization problems bundle adjustment exploit problem structure large scale problems highly structured bundle adjustment exception 
professional cartography photogrammetric site modelling bundle problems thousands images tens thousands features regularly solved 
problems simply infeasible thorough exploitation natural structure sparsity bundle problem 
say sparsity 
factorization effectively formulae contain matrix inverses 
convenient short hand theoretical calculations numerically matrix inversion 
matrix decomposed cholesky lu qr factors directly linear systems solved forwards backwards substitution 
faster numerically accurate explicit inverse particularly sparse matrices bundle hessian factors quite sparse inverse dense 
explicit inversion required occasionally covariance estimates entries may needed diagonal blocks covariance 
factorization heart optimization iteration time spent done improve efficiency exploiting sparsity symmetry problem structure numerical stability pivoting scaling 
similarly certain matrices subspace projectors householder matrices diagonal low rank forms explicitly evaluated applied efficiently pieces 
stable local parametrizations discussed parametrization step prediction need coincide global store state estimate 
important finite uniform locally nearly linear possible 
global parametrization way complex highly nonlinear potentially ill conditioned usually preferable stable local parametrization perturbations current state step prediction 
scaling preconditioning parametrization issue profound rarely recognized influence numerical performance variable scaling choice units scale parameter generally preconditioning choice linear combinations parameters 
represent linear part general parametrization problem 
performance gradient descent linearly convergent optimization methods critically dependent preconditioning extent large problems seldom practically useful 
great advantages newton methods theoretical independence scaling issues scaling felt indirectly ways step control strategies including convergence tests maximum step size limitations damping strategies trust region levenberg marquardt usually implicit norm change linear transformations damping step non invariant gradient descent 
ii pivoting strategies factoring highly dependent variable scaling choose large elements pivot 
large mean little numerical cancellation occurred uneven scaling largest scale 
iii choice gauge datum may depend variable scaling significantly influence convergence 
reasons important choose variable scalings relate meaningfully problem structure 
involves judicious comparison relative influence unit error nearby point unit error distant camera rotation error radial distortion error advisable ideal hessian weight matrix observed scaling break hessian happens ill conditioned non positive iterations settling 
network structure adjustment networks rich structure illustrated toy bundle problem 
free parameters subdivide naturally blocks corresponding feature coordinates 
linear change coordinates tx ht newton step varies correctly gradient varies incorrectly newton gradient descent steps agree network graph parameter connection graph network graph parameter connection graph jacobian structure hessian structure toy bundle problem features images camera calibrations shared images shared images 
feature seen images 
camera poses unshared single image calibration parameters 
calibration parameters shared images 
parameter blocks interact joint influence image features observations joint appearance cost function contributions 
structure measurement network characterized graphically network graph top left shows features seen images parameter connection graph top right details sparse structure showing parameter blocks direct interactions 
blocks linked jointly influence observation 
cost function jacobian bottom left hessian bottom right reflect sparse structure 
shaded boxes correspond non zero blocks matrix entries 
block rows jacobian corresponds observed image feature contains contributions parameter blocks influenced observation 
hessian contains diagonal block edge parameter connection graph pair parameters couple common feature appear common cost contribution layers structure visible hessian 
primary structure consists subdivision structure camera submatrices 
note structure submatrix block diagonal features couple cameras features 
longer hold inter feature measurements distances angles points 
camera submatrix block diagonal example sharing unknown calibration parameters produces diagonal blocks 
secondary structure internal sparsity pattern structure camera hessian submatrix 
dense small problems features seen images larger problems quite sparse image sees fraction features 
jacobian structure described directly bipartite graph nodes correspond side observations parameter blocks influence 
parameter connection graph obtained deleting observation node linking pair parameter nodes connects 
example elimination graph processing see 
worthwhile bundle methods exploit primary structure hessian advanced methods exploit secondary structure 
secondary structure particularly sparse regular surface coverage problems grids photographs aerial cartography 
problems handled fixed nested dissection variable reordering 
irregular connectivities close range problems general sparse factorization methods may required handle secondary structure 
bundle problems means limited structures 
example complex scene models moving articulated objects additional connections object pose joint angle nodes linkages reflecting kinematic chain structure scene 
necessary add constraints adjustment coplanarity certain points 
greatest advantages bundle technique ability adapt arbitrarily complex scene observation constraint models 
implementation strategy second order adjustment methods sections cover implementation strategies optimizing bundle adjustment cost function complete set unknown structure camera parameters section devoted second order newton style approaches basis great majority current implementations 
notable characteristics rapid second order asymptotic convergence relatively high cost iteration emphasis exploiting network structure sparsity hessian dx efficiency 
fact optimization aspects standard concentrate entirely efficient methods solving linearized newton step prediction equations 
assume hessian nonsingular 
amended gauge freedom changing reached 
schur complement reduced bundle system schur complement consider block triangular matrix factorization ca ca ab cd ca ca ca square invertible matrix square invertible 
called schur complement invertible complementing gives ab cd bd dca bd equating upper left blocks gives woodbury formula bd bd ca ca usual method updating inverse nonsingular matrix update especially low rank bd see 
reduction consider linear system ab cd pre multiplying ca gives ab ca 
schur complement forward substitution find reduced system dx solve back substitute solve find ca ca schur complement forward substitution dx ax bx reduced system back substitution note reduced system entirely subsumes contribution rows columns network 
reduced pretend problem involve back substitution needed ignored 
basis recursive filtering methods 
bundle adjustment primary subdivision feature camera variables subsume structure ones get reduced camera system hcc xc hcc hcc hcs ss hsc hcc pp hpc gc gc hcs ss gs gc pp gp selects structure block camera 
hss block diagonal reduction calculated rapidly sum contributions individual features ins 
brown original method bundle adjustment finding reduced camera system solving gaussian elimination 
profile cholesky decomposition offers streamlined method achieving 
occasionally long image sequences camera parameters structure ones 
case efficient reduce camera parameters leaving reduced structure system 
triangular decompositions subdivided blocks factorization process continued recursively 
fact family block lower triangular diagonal upper triangular factorizations 
am am amn 
lm lm lmr 
dr 
urn see computational details 
main advantage triangular factorizations linear algebra computations matrix easier 
particular input matrix square nonsingular linear equations ax solved sequence recursions implicitly implement multiplication lc ci ii dd di ux xi ii bi lij cj di uij xj forward substitution ci diagonal solution back substitution forward substitution corrects influence earlier variables ones diagonal solution solves transformed system back substitution propagates corrections due variables back earlier ones 
practice usual method solving linear equations newton step prediction equations 
faster explicitly inverting multiplying diagonal blocks lii di uii set arbitrarily provided product lii di uii remains constant 
gives number known factorizations optimized different class matrices 
pivoting row column exchanges designed improve conditioning necessary cases ensure stability 
choosing lii dii gives block lu decomposition lu matrix representation block gaussian elimination 
pivoted rows standard method non symmetric matrices 
symmetric roughly half factorization saved symmetry preserving ldl factorization symmetric pivoting strategy preserve symmetry case permute columns way corresponding rows 
symmetric positive definite set get cholesky decomposition ll stable pivoting extremely simple implement 
standard decomposition method unconstrained optimization problems including bundle adjustment hessian positive definite near non degenerate cost minimum gauss newton approximation 
symmetric positive semidefinite diagonally pivoted cholesky decomposition 
case subset selection methods gauge fixing 
symmetric indefinite possible reduce stably 
bunch kaufman method 
diagonally pivoted ldl method mixture diagonal blocks 
augmented hessian lagrange multiplier method constrained optimization problems symmetric indefinite bunch kaufman recommended method solving constrained bundle problems 
faster gaussian elimination equally stable 
factorization matrix inversion 
inverses calculated factoring inverting triangular factor forwards backwards substitution multiplying explicit inverses rarely numerical analysis faster cases leave implicit forward backward substitution factorization multiplication inverse 
place inversion needed right calculate dispersion matrix inverse hessian asymptotically gives posterior covariance measure variability parameter estimates 
dispersion calculated explicit inversion factored hessian entries needed diagonal blocks key diagonal parameter covariances 
case efficiently calculates covariance entries corresponding just nonzero elements sparse factorization apply decompositions sparse matrices obviously avoid storing manipulating zero blocks 
subject 
sparse matrix decomposed zero positions tend rapidly fill non zero essentially decomposition repeated linear combination matrix rows generically non zero inputs fill depends strongly order variables eliminated efficient sparse factorization routines attempt minimize operation counts fill re ordering variables 
schur process fixed advance available freedom 
globally minimizing operations fill np complete reasonably heuristics exist see 
variable order affects stability pivoting speed goals conflict extent 
finding heuristics counts research problem 
algorithmically fill characterized elimination graph derived parameter coupling hessian graph 
create nodes blocks parameters visited elimination ordering step linking unvisited nodes currently linked current node 
coupling block block visited block corresponds non zero schur contribution stage subgraph currently unvisited nodes coupling graph current reduced hessian 
amount fill number new graph edges created process 
pattern matrices seek variable orderings approximately minimize total operation count fill elimination chain 
problems suitable ordering fixed advance typically giving standard pattern matrices band arrowhead matrices structure levels 
bundle hessian arrowhead matrix block tridiagonal matrix prominent pattern structure bundle adjustment primary subdivision hessian structure camera blocks 
get reduced camera system treat hessian arrowhead matrix broad final column containing camera parameters 
arrowhead matrices trivial factor reduce block schur complementation 

bundle problems independent images features complement image parameter block get reduced structure system 
common pattern structure block tridiagonal characterizes singly coupled chains sequences images pairwise overlap kalman filtering time recursions simple kinematic chains 
tridiagonal matrices factored reduced recursive block schur complementation starting 
factors block tridiagonal inverse generally dense 
pattern orderings natural unwise think immutable structure occurs levels deeper structure simply changes relative sizes various parameter classes may alternative orderings preferable 
difficult problems basic classes line ordering strategies 
bottom methods try minimize fill locally greedily step risk global short 
top methods take conquer approach recursively splitting problem smaller sub problems solved quasi independently merged 
top ordering methods common top method called nested dissection recursive partitioning 
basic idea recursively split factorization problem smaller sub hessian natural cholesky minimum degree reverse cuthill mckee bundle hessian irregular coverage problem local connections cholesky factor natural structure camera minimum degree reverse cuthill mckee ordering 
problems solve independently glue solutions common boundaries 
splitting involves choosing separating set variables deletion separate remaining variables independent subsets 
corresponds finding vertex graph cut elimination graph set vertices deletion split disconnected components 
partitioning variables reordered connected components separating set ones 
produces arrowhead matrix arrowhead matrix factored blocks reduction profile cholesky account internal sparsity diagonal blocks borders 
suitable factorization method diagonal blocks including recursive partitionings 
nested dissection useful comparatively small separating sets 
trivial example primary structure bundle problem camera variables separate structure independent features giving standard arrowhead form bundle hessian 
interestingly networks geometric temporal locality surface site covering networks video sequences tend small separating sets spatial temporal subdivision 
classic examples geodesic aerial cartography networks local connections spatial bisection gives simple efficient recursive decompositions 
sparse problems regular structure graph partitioning algorithms find small separating sets 
finding globally minimal partition sequence np complete effective heuristics exist 
currently active research field 
promising family multilevel schemes subsample graph partition spectral method refine result original graph 
algorithms suited graph visual segmentation matching 
bottom ordering methods bottom variable ordering heuristics exist 
probably widespread effective minimum degree ordering 
step eliminates variable coupled fewest remaining ones elimination graph node fewest unvisited neighbours minimizes number neighbours changed matrix elements flops step 
minimum de gree ordering computed quite rapidly explicit graph chasing 
related ordering minimum deficiency minimizes fill newly created edges step considerably slower calculate usually effective 
fill operation minimizing strategies tend produce somewhat fragmentary matrices require pointer index sparse matrix implementations see fig 

increases complexity tends reduce cache locality pipeline ability 
alternative profile matrices lower triangles store elements row non zero diagonal contiguous block 
easy implement see practically efficient long stored elements non zero 
orderings case aim minimize sum profile lengths number non zero elements 
profiling enforces multiply linked chain structure variables especially successful linear chain dimensional problems space time sequences 
simplest profiling strategy reverse cuthill mckee chooses initial variable preferably chain adds variables coupled variables coupled reverses ordering highly coupled variables get eliminated early causes disastrous fill 
sophisticated called banker strategies maintain active set variables coupled eliminated ones choose variable active set king neighbours variables levy minimize new size active set step 
particular banker algorithm reported perform geodesy aerial cartography problems 
automatic ordering methods pays initial hand appropriate enforce structure camera division order reduced camera system 
nodes particularly high degree inner gauge constraints ordering calculation usually run faster quality may improved removing graph placing hand 
ordering methods apply cholesky ldl decomposition hessian qr decomposition squares jacobian 
sparse qr methods implemented givens rotations efficiently sparse householder transformations 
row ordering important givens methods 
householder ones givens ones organization usual captures natural parallelism problem 
implementation strategy order adjustment methods seen large problems factoring hessian compute newton step expensive done efficiently complex 
section consider alternative methods avoid cost exact factorization 
newton step calculated methods generally achieve order linear asymptotic convergence close final state estimate error asymptotically reduced constant practice small factor step quadratically convergent newton methods roughly double number significant digits step 
order methods require iterations second order ones iteration usually cheaper 
relative efficiency depends relative sizes effects substantial 
large problems reduction iteration usually wheren problem size 
newton methods converge log iterations linearly convergent ones take respectively log log iterations reduction iteration 
unfortunately reductions means unusual practice reduction tends decrease increases 
order iterations consider number common order methods returning question slow 
gradient descent simplest order method gradient descent slides gradient ha 
line search needed find appropriate scale step 
problems gradient descent spectacularly inefficient hessian happens close multiple 
arranged preconditioning linear transform lx hl approximate cholesky factor left square root hl 
special case preconditioned gradient descent approximates newton method 
strictly speaking gradient descent cheat gradient linear form vectors vector define direction search space 
gradient descent sensitivity coordinate system symptom 
alternation simple approach alternation partition variables groups cycle groups optimizing turn groups held fixed 
appropriate subproblems significantly easier optimize full 
natural rediscovered alternation bundle problem resection intersection interleaves steps resection finding camera poses necessary calibrations fixed features intersection finding features fixed camera poses calibrations 
subproblems individual features cameras independent diagonal blocks required 
alternation ways 
extreme optimize perform step optimization group turn state update re evaluation relevant components group 
alternatively re evaluations simulated evaluating linearized effects parameter group update groups 
structure update xs hss gs xs xc selects structure variables camera ones updated camera gradient exactly gradient reduced camera system gc xs xs xc gc xs xc hcs xs gc hcs ss gc 
total update ss gs hss gs gc gc general cycle xs xc cc hcs ss cc hcs hcc correction propagation amounts solving system diagonal triangle zero 
cycled variables update full state 
nonlinear gauss seidel method 
alternatively split diagonal triangle correction back propagation term continue iterating hss xs hcs hcc xc gs gc hsc xs xc hopefully xs converges full newton step xc linear gauss seidel method applied solving newton step prediction equations 
alternation methods tend underestimate size newton step fail account cost reducing effects including back substitution terms 
successive relaxation sor methods improve convergence rate artificially lengthening update steps heuristic factor 
alternations applied bundle problem independent model times 
brown considered relatively sophisticated sor method aerial cartography problems early developing recursive decomposition method 
alternations effective traditional large scale problems shows compete smaller highly connected ones 
krylov subspace methods large family iterative techniques krylov subspace methods remarkable properties power subspaces span ak fixed increases 
krylov iterations predominate large scale linear algebra applications including linear equation solving 
earliest greatest krylov method conjugate gradient iteration solving positive definite linear system optimizing quadratic cost function 
augmenting gradient descent step carefully chosen multiple previous step manages minimize quadratic model function entire th krylov subspace th iteration exact arith space th 
longer holds round error nx iterations usually suffice find newton step 
iteration large gain explicit factorization 
convergence significantly faster eigenvalues tightly clustered away zero eigenvalues covered intervals ai bi convergence occurs bi ai iterations preconditioning see aims achieving clustering 
alternation methods range possible update re linearization choices ranging fully nonlinear method step solving newton equations exactly linear iterations 
major advantage conjugate gradient simplicity factorization needed multiplication full nonlinear method needed simply line search find cost minimum direction defined previous step 
disadvantage nonlinear conjugate gradient high sensitivity accuracy line search 
achieving required accuracy may waste function evaluations step 
way avoid information obtained conjugation process explicit building explicit approximation quasi newton methods bfgs method eigenvalue analyses bundle adjustment covariance see 
gauss newton steps flops diag 
precond 
conjugate gradient steps flops intersect line search steps flops intersect intersect line search steps flops example typical behaviour second order convergent methods near minimum 
projection small ill conditioned bundle problem variable directions 
second order methods converge quite rapidly exact gauss newton iterative diagonally preconditioned conjugate gradient linear solver newton equations 
contrast order methods resection intersection converge slowly near minimum owing inaccurate model hessian 
effects reduced extent adding line search 
need accurate line searches 
quasi newton approximation dense expensive store manipulate limited memory quasi newton methods get desired effect maintaining low rank approximation 
variants methods squares jacobian hessian constrained problems non positive definite matrices 
order methods slow 
understand order methods slow convergence consider effect approximating hessian newton method 
suppose local parametrization centred cost minimum cost function approximated quadratic near hx hx true hessian 
order methods predicted step linear gradient adopt newton state update approximation ha get iteration xk xk xk xk numerical behaviour determined projecting eigenvectors corresponding large modulus eigenvalues decay slowly asymptotically dominate residual error 
generic method converges linearly exponentially rate diverges greater 
course exact newton step converges single iteration ha 
eigen directions corresponding positive eigenvalues ha overestimates iteration damped convergence slow monotonic 
conversely directions corresponding negative eigenvalues ha underestimates iteration damped solution 
underestimated factor greater direction divergence 
shows example typical asymptotic behaviour second order methods small bundle problem 
ignoring camera feature coupling example approximate bundle methods ignore approximate diagonal feature camera blocks hessian 
amounts ignoring fact cost feature displacement partially offset compensatory camera displacement vice versa 
significantly estimates total stiffness network particularly large loosely connected networks 
fact diagonal blocks negligible compared diagonal ones seen ways looking forward gauge fixed full hessian singular owing gauge freedom 
diagonal blocks conditioned including diagonal ones entirely cancels gauge orbit directions 
gauge fixing removes resulting singularity change fact diagonal blocks weight counteract diagonal ones 
bundle adjustment certain known ambiguities poorly controlled parameter combinations dominate uncertainty 
camera distance focal length estimates structure depth camera baseline ones bas relief strongly correlated perspective weak strict ambiguities affine limit 
conditioned diagonal blocks hessian give hint ambiguities features cameras free network rigid appears treats fixed 
bundle adjustment local structure refinements cause ripples propagated network 
camera feature coupling information carried diagonal blocks essential 
diagonal model ripples propagate feature step iteration takes iterations cross re cross sparsely coupled network 
arguments suggest approximation ha bundle hessian suppresses significantly alters diagonal terms large slow convergence 
exactly observed practice methods tested near minimum convergence linear large problems extremely slow close 
iteration may zigzag converge slowly monotonically depending exact method parameter values 
line search behaviour improved significantly adding line search method 
principle resulting method converges positive definite ha 
accurate modelling highly desirable 
rounding errors exactly quadratic unknown cost function exact line searches minimum line exactly efficient generic line search methods conjugate gradient quasi newton require nx iterations converge 
large bundle problems thousands parameters prohibitive 
knowledge incorporated suitable preconditioner number iterations reduced substantially 
preconditioning gradient descent krylov methods sensitive coordinate system practical success depends critically preconditioning 
aim find linear transformation ht transformed near clusters eigenvalues separated origin 
ideally accurate low cost approximation left cholesky factor 
exactly evaluating give expensive newton method 
experiments tried conjugate gradient preconditioners diagonal blocks partial cholesky decomposition dropping elements smaller preset size performing cholesky decomposition 
methods competitive exact gauss newton ones strip experiments large problems preconditioned krylov method predominate especially effective preconditioners 
exact cholesky factor previous iteration quite effective preconditioner 
gives hybrid methods evaluated factored iterations newton step iterations preconditioned gradient descent conjugate gradient 
experiments shows relative performance methods synthetic projective bundle adjustment problems 
cases number points increases proportion number images dense factorization time number points images 
methods shown sparse gauss newton sparse cholesky decomposition variables ordered naturally features cameras dense gauss newton inefficiently ignoring sparsity hessian diag 
conj 
gradient newton step iterative conjugate gradient linear system solver preconditioned cholesky factors diagonal blocks hessian intersect state optimized alternate steps resection intersection 
spherical cloud problem points uniformly distributed spherical cloud points visible images camera geometry strongly convergent 
ideal conditions giving low diameter network graph conditioned nearly diagonal dominant hessian 
methods converge quite rapidly 
resection intersection competitive method larger problems owing low cost iteration 
unfortunately geometry testing computer vision algorithms atypical large scale bundle problems 
strip experiment representative geometry 
images arranged long strip feature seen overlapping images 
strip long thin weakly connected network structure gives large scale low stiffness modes correspondingly poor hessian conditioning 
diagonal terms critical approximate methods perform poorly 
resection intersection slower dense cholesky decomposition ignoring sparsity 
images fails converge iterations 
sparse cholesky methods continue perform reasonably natural minimum degree reverse cuthill mckee orderings giving similar run times case 
methods tested including resection intersection linear iteration cost total run time long chain geometries scaled roughly total operations total operations computation vs bundle size strong geometry intersect steps diag 
conj 
gradient steps sparse gauss newton steps 
images computation vs bundle size weak geometry intersect steps dense gauss newton steps diag 
conj 
gradient steps sparse gauss newton steps 
images relative speeds various bundle optimization methods strong spherical cloud weak strip geometries 
implementation strategy updating recursion updating rules convenient able update state estimate reflect various types changes incorporate new observations delete erroneous ones 
parameters may added deleted 
updating rules recursively incorporate series observations solving single batch system 
useful line applications rapid response needed provide preliminary predictions correspondence searches 
early development updating methods aimed line data editing aerial cartography workstations 
main challenge adding deleting observations efficiently updating factorization hessian covariance state update easily solving newton step equations assuming started un updated optimum gradient depends newly added terms 
hessian update needs relatively low rank saved recomputing batch solution 
squares rank number independent observations added deleted rank low bundle problems relatively parameters affected observation 
limitation updating seldom accurate batch solution owing buildup round error 
updating adding observations numerically stable deleting observations potentially ill conditioned reduces positivity hessian may cause previously pivot choices arbitrarily bad 
particularly problem observations relating parameter deleted repeated insertion deletion cycles time window filtering 
factorization updating methods woodbury formula covariance updating ones 
consider case parameters need added deleted adding deleting observation existing point existing image 
methods suggested 
mikhail woodbury formula update covariance simple approach inefficient problems features sparse structure exploited full covariance matrix dense normally avoid calculating entirety 
gr avoids problem maintaining running copy reduced camera system incremental schur complement forward substitution fold new observation re factorizing solving usual update 
effective features images larger numbers images inefficient owing re factorization step 
factorization updating methods currently recommended update methods applications allow existing factorization exploited handle number images features arbitrary problem structure efficiently numerically accurate woodbury formula methods 
givens rotation method equivalent rank cholesky update probably common method 
updating methods confusingly named literature 
mikhail method called kalman filtering dynamics actual filtering involved 
gr reduced camera system method called triangular factor update updates square reduced hessian triangular factors 
updates involving previously unseen feature image new variables added system 
easy 
simply choose put variables elimination sequence extend factors corresponding rows columns setting newly created positions zero unit diagonals ldl lu factor 
factorization updated usual presumably adding cost terms extended hessian nonsingular couple new parameters old network 
direct covariance update needed woodbury formula old part matrix fill new blocks equivalently invert representing old blocks new ones 
conversely may necessary delete parameters image feature lost support 
corresponding rows columns hessian rows columns deleted cost contributions involving deleted parameters removed usual factorization 
delete rows columns block matrix delete rows columns maintains gives correct trimmed blocks lower right corner aij min dk missing term lib db deleted column added update db ub update rows columns deleted permute deleted rows columns backwards 
possible freeze live parameters fixed current default values add extra parameters previously frozen ones 

case rows columns corresponding frozen parameters deleted added change cost function required 
deletion 
insert rows columns ab block matrix open space row column fill positions usual recursively defined values 
sum contribution lib db correct lower right submatrix cost cancelling contribution db ub 
recursive methods reduction update computation roughly quadratic size state vector new features images continually added situation eventually unmanageable 
limit compute 
principle parameter refinement stops observation update affects components state estimate covariance 
refinements sense trivial parameters directly coupled observation 
parameters eliminated reduction observation update applied directly reduced hessian gradient eliminated parameters updated simple back substitution covariances 
particular cease receive new information relating block parameters image fully treated feature invisible observations relating subsumed reduced hessian gradient remaining parameters 
required re estimate eliminated parameters back substitution 
need consider 
elimination process limitations 
dead parameters eliminated merge new observation problem need current hessian factorization entries parameter blocks relating 
reduction commits linearized quadratic cost approximation eliminated variables current estimates extent model correct remaining variables treated nonlinearly 
best view reduction half iteration full nonlinear optimization newton method full model implemented repeated cycles reduction solving reduced system back substitution cycle eliminated variables solving reduced system 
equivalently reduction evaluates just reduced components full newton step full covariance leaving option computing remaining eliminated ones wish 
reduction refine estimates relative camera poses fundamental matrices fixed set images reducing sequence feature correspondences camera coordinates 
conversely refine structure estimates fixed set features images reducing feature coordinates 
reduction basis recursive kalman filtering 
case time series system state vectors linked probabilistic transition rule dynamical model observations observation model 
parameter space consists affected observation independent subsumed components 
applying update effect applying 
reconstruction error reconstruction error vs time window size simple batch time window size data data data residual state estimation error sequential bundle algorithm progressively increasing sizes rolling time window 
residual error image shown rolling windows previous images batch method previous images simple reconstruction intersection performed independently camera location resection 
simulate effects decreasing amounts image data image measurements randomly deleted runs supplied image data 
main window size little effect strong data increasingly important data weaker 
combined state vectors times represents path states 
dynamical observation models provide observations sense probabilistic constraints full state parameters seek maximum likelihood similar parameter estimate path states 
full hessian block tridiagonal observations couple current state give diagonal blocks dynamics couples previous ones gives diagonal blocks differential observations included dynamics likelihood 
model large time steps sparse 
tridiagonal matrix hessian decomposed recursive steps reduction step schur complementing get current reduced block ht previous ht diagonal dynamical coupling current unreduced block observation hessian ht ht ht tt similarly gradient gt gt gt usual reduced state update xt gt forwards reduction process called filtering 
time step finds optimal linearized current state estimate previous observations dynamics 
corresponding unwinding recursion back substitution smoothing finds optimal state estimate time past observations dynamics 
usual equations kalman filtering smoothing easily derived recursion 
emphasize filtering merely half iteration nonlinear optimization procedure nonlinear dynamics observation models find exact maximum likelihood state path cyclic passes filtering smoothing 
long unbounded sequences may feasible run full iteration helpful run short sections smoothing back state estimates filtering forwards verify previous correspondences anneal effects nonlinearities 
traditional extended kalman filter optimizes nonlinearly just current state assuming previous ones linearized 
effects variable window size variable state dimension filter sequential bundle algorithm shown 
gauge freedom coordinates convenient device reducing geometry algebra come price arbitrariness 
coordinate system changed time affecting underlying geometry 
familiar leaves problems algorithmically need concrete way deciding particular coordinate system moment breaking arbitrariness ii need allow fact results may look quite different different choices represent underlying geometry 
consider choice coordinates visual reconstruction 
objects space reconstructed cameras features decide place coordinate system relative 
coordinate centred language place reconstruction relative coordinate system 
bundle adjustment updates uncertainties perturb reconstructed structure arbitrarily specify coordinate systems just current structure possible nearby 
ultimately comes constraining coordinate values certain aspects reconstructed structure features cameras combinations rest structure 
saying intrinsically coordinate frame specified held fixed respect chosen elements rest geometry expressed frame usual 
measurement science set coordinate system specifying rules called datum follow wider mathematics physics usage call gauge freedom choice coordinate fixing rules called gauge freedom 
gauge anchors coordinate system rigidly chosen elements perturbing elements effect coordinates 
changes coordinate system systematically changes coordinates features leaving coordinates fixed 
similarly uncertainties elements affect coordinates appear highly correlated uncertainties reconstructed features 
moral structural perturbations uncertainties highly relative 
form depends profoundly gauge especially changes state varies elements holds fixed 
effects disturbances restricted coordinates features disturbed may appear depending gauge 
visual reconstruction differences object centred camera centred gauges particularly marked 
object centred gauges object points appear relatively certain cameras appear large highly correlated uncertainties 
camera centred gauges camera appears precise object points appear large correlated uncertainties 
sees statements reconstructed depths uncertain 
may true camera frame object may reconstructed frame depends fraction total depth fluctuations simply due global uncertainty gauge just means frame 
sense judged fr 

pronounce dj 
camera location identical object points 
coordinates types geometric parametrization vision involve arbitrary choices subject gauge freedoms 
include choice homogeneous scale factors homogeneous projective representations supporting points supporting point representations lines planes plane plane parallax representations homographies homography epipole representations matching tensors 
case symptoms remedies 
general formulation general set follows take state vector set feature coordinates camera poses calibrations enter problem 
state space internal symmetries related arbitrary choices coordinates homogeneous scale factors embedded state vectors differ choices represent underlying geometry exactly image projections intrinsic properties 
change coordinates equivalence state space partitioned classes intrinsically equivalent state vectors class representing exactly underlying geometry 
classes called gauge orbits 
formally group orbits state space action relevant gauge group coordinate transformation group need group structure 
state space function represents intrinsic function underlying geometry constant gauge orbit coordinate system independent 
quantities called gauge invariants 
want bundle adjustment cost function quantify intrinsic merit chosen gauge invariant 
visual reconstruction principal gauge groups dimensional group similarity scaled euclidean transformations euclidean reconstruction dimensional group projective coordinate transformations projective reconstruction 
gauge freedoms 
examples include arbitrary scale factors homogeneous projective feature representations rescaling gauge groups 
ii arbitrary positions points point line parametrizations motion line groups 
iii underspecified homographies homography epipole parametrizations matching tensors 
example fundamental matrix parametrized left epipole inter image homography induced plane 
choice plane gives freedom ea arbitrary vector linear gauge group 
consider specify gauge rule saying possible underlying geometry near current expressed coordinates 
represented state space points matter choosing exactly point structure gauge orbit underlying geometry 
mathematically gauge orbits fill crossing state space gauge local transversal cross section 
see fig 

different gauges represent different geometrically equivalent rules 
results mapped gauges pushing gauge orbits applying local coordinate transformations vary depending particular structure involved 
transformations called transforms similarity transforms 
different gauges central state represent rules agree central geometry differ perturbed ones transform identity centre 
gauge state perturbations lie gauge cross section authorized 
want state perturbations correspondence perturbations gauge orbits parameter space cost function constant orbits gauge constraints fix coordinates nearby structure project orbits change gauge covariance depends chosen gauge gauge orbits state space gauge cross sections covariances 
underlying geometry 
state perturbation equivalent gauge gauge group small coordinate transformation pushes perturbed state gauge orbit meets gauge cross section 
state perturbations gauge orbits uninteresting change underlying geometry 
covariances averages squared perturbations gauge perturbations infinite permitted perturbations gauge orbits limit change cost 
covariance matrices gauge dependent fact represent ellipsoids tangent gauge cross section cost minimum 
look different different gauges 
states transforms map gauges projecting gauge orbits state equivalence classes 
note intrinsic notion orthogonality state space meaningless ask state space directions orthogonal gauge orbits 
involve deciding different structures expressed coordinate system gauge believes cross section orthogonal skewed 
gauge constraints near point state space cost minimum running state estimate 
nx dimension ng dimension gauge orbits 
cost function gradient hessian nx ng matrix columns span local gauge orbit directions exact gauge invariance gradient hessian vanish orbit directions hg 
note gauged hessian singular rank deficiency ng null space deficiency 
numerical optimization routines assume nonsingular modified gauge invariant problems 
singularity expression indifference come calculate state updates updates gauge orbit equivalent terms cost terms change underlying geometry 
need method telling routine particular update choose 
gauge constraints direct means doing 
gauge cross section spec suitable easily calculated infinitesimal action gauge group example spatial similarities columns ng state velocity vectors describing effects infinitesimal translations rotations changes spatial scale ified ways constrained form specify ng local constraints points ii parametric form specify function nx ng independent local parameters points example trivial gauge simply freezes values ng parameters usually feature camera coordinates 
case take parameter freezing constraints remaining parameters 
note gauge fixed problem longer gauge invariant purpose break underlying gauge invariance 
examples trivial gauges include visible points projective basis reconstruction fixing projective coordinates simple values ii fixing components projective camera matrix partially fixes projective gauge projective degrees freedom remain 
linearized gauge local linearizations gauge functions dd dx dx dy compatibility gauge specification methods requires dy 
transversal gauge orbits dg full rank ng full rank nx 
assuming perturbation xg order iff xg xg nx nx rank nx ng matrices characterize projection matrix pg implements linearized projection state displacement vectors gauge orbits local gauge cross section xg pg 
projection usually non orthogonal pg 
gauged covariance matrix vg plays role inverse hessian 
gives cost minimizing newton step xg vg asymptotic covariance xg 
pg vg special properties equivalent forms 
convenience display bd nonsingular symmetric ng ng matrix gauge vg hy dg dg pg pg vg pg vg pg dg hy vh vg pg pg pg pg dpg pg hpg vg vg relations summarized saying vg supported generalized inverse pg projects gauge orbits pg ii projects gauge cross section dpg pg pg xg vg pg vg iii preserves gauge invariants pg pg hpg 
vg rank nx ng 
null spaces transversal unrelated 
pg left null space right null space results easily proved inserting strategic factors hg dy hy dg dg hy dg ng ng including bd nonsingular bd hy dg dg state updates straightforward add gauge fixing bundle update equations 
consider constrained form 
enforcing gauge constraints xg lagrange multipliers gives sqp step xg vg dg dg xg vg dg atypical constrained problem 
typical cost functions gradient component pointing away constraint surface constrained minimum non vanishing force required hold solution constraints 
cost function derivatives entirely indifferent motions orbits 
actively forces state move gauge constraint force vanishes vanishes optimum constrained minimum value identical unconstrained minimum 
effect constraints correct gradual drift away happens occur term xg 
simpler way get effect add gauge invariance breaking term bd cost function positive ng ng weight matrix 
note bd unique minimum orbit point forx asf constant gauge orbits optimization bd orbit enforces returns orbit value global optimization find global constrained minimum cost function bd nonsingular newton step xg bd bd new inverse hessian 
identical sqp step sqp methods equivalent 
strategy works force required keep state gauge case weight infinite 
dense form practically useful dense slow factorize updating formulae 
consider parametric form suppose current reduced state estimate approximate get reduced system solve find xg necessary hy xg vg nx ng nx ng matrix generically nonsingular despite singularity case trivial gauge simply selects submatrices corresponding parameters solves 
trivial gauges dense risk substantial fill occur methods 
gauged covariance standard covariance propagation covariance gauge fluctuations xg xg hy vg 
xg moves vg represents rank nx ng covariance ellipsoid flattened 
trivial gauge vg covariance hy free variables padded zeros fixed ones 
vg linearized gauged covariance function dh dh vg usual 
dx dx gauge invariant constant gauge orbits just ordinary covariance 
intuitively vg dh dh vg depend gauge measure absolute uncertainty uncertainty relative dx dx features gauge 
just absolute frames absolute uncertainties 
best relative ones 
gauge transforms change gauge computation improve sparseness numerical conditioning re express results standard gauge 
simply matter transform pushing gauged quantities gauge orbits new gauge cross section assume base point unchanged 
fixed structure independent change coordinates achieves 
locally transform linearizes linear projection orbits spanned new gauge constraints implemented nx nx rank nx ng non orthogonal projection matrix pg defined 
projection preserves gauge invariants pg cancels effects projection gauge pg pg pg 
inner constraints wide range gauges significant impact appearance state updates covariance matrix useful ask gauges give smallest best behaved updates covariances 
useful interpreting comparing results gives beneficial numerical properties 
basically matter deciding features cameras care tying gauge stable average gauge induced correlations small possible 
object reconstruction resulting gauge usually object centred vehicle navigation camera centred 
stress choices matter superficial appearance principle gauges equivalent give identical values covariances gauge invariants 
way say gauge invariants find meaningful coordinate system independent values covariances 
fruitful ways create invariants locate features basis features gauge 
choice inner constraints choice stable basis compound features invariants measured 
including average features compound reduce invariants dependence basis features 
performance criterion minimize sort weighted average size state update covariance 
nx nx information weight matrix encoding relative importance various error components left square root ll local gauge minimizes weighted size state update xg weighted covariance sum trace trace vg frobenius norm vg inner constraints corresponding covariance vg state update xg vg usual 
nonsingular vg weighted rank nx ng pseudo inverse hl ll cholesky decomposition moore penrose pseudo inverse 
sketch proof forw diagonal 
gauge transversal form nonsingular premultiplying reduces form ng nx ng matrix follows pg vg xg vg trace vg trace trace criteria clearly minimized sod claimed 
nonsingular ll scaling coordinates hl eigen decomposition reduces diagonal transformation affects xg trace back substituting gives general result 
singular limiting argument similarly vg inner constraint minimizes frobenius norms vg interlacing property eigenvalues minimizes strictly non decreasing rotationally invariant function vg strictly non decreasing function eigenvalues 
inner constraints covariant global transformations provided transformed usual information matrix hessian way wt dt dx transformations seldom preserve form 
represents isotropic weighted sum points form preserved global euclidean transformations rescaled scalings 
extends points projective transformations camera poses planes non point features euclidean ones 
choice origin significant influence poses planes changes origin propagate rotational uncertainties translational ones 
inner constraints originally introduced geodesy case 
meaning entirely dependent chosen coordinates variable scaling 
bundle adjustment little recommend coordinate origin carefully chosen variables carefully pre scaled hl ll fixed weight matrix takes account fact covariances features camera translations rotations focal lengths aspect ratios lens distortions entirely different units scales relative importances 
gauge projection pg orthogonal symmetric 
free networks gauges divided roughly outer gauges locked predefined external features giving fixed network adjustment inner gauges locked recovered structure giving free network adjustment 
weight concentrated external inner constraints give outer gauge 
chosen inner gauges distort intrinsic covariance structure outer ones tend better numerical conditioning give representative idea true accuracy network 
useful slightly different fixed free distinction 
order control gauge deficiency gauge fixing method specify motions locally possible iteration 
indispensable local decisions enforce global gauge 
method globally fixed enforce global gauge inner outer globally free 
example standard photogrammetric inner constraints give globally free inner gauge 
require cloud reconstructed points translated rotated rescaled perturbations centroid average directions distances centroid remain unchanged 
specify cloud oriented scaled attempt correct gradual drift position may occur optimization iterations owing accumulation truncation errors 
contrast mclauchlan globally fixes inner gauge locking reconstructed centroid scatter matrix 
give numerical properties testing required determine improvement globally free inner gauge advantage fixing coordinate system direct comparisons solutions covariances possible 
numerically globally fixed gauge implemented including term simply applying rectifying gauge transformation estimate step drifts far chosen gauge 
dt vg pg tpg xg xg 
xg trace preserved 
means vanishes identically non point features camera parameters weighted identity matrix wi wi point generally form block point coordinates inter point weighting matrix 
implementation gauge constraints gauges principle equivalent worthwhile pay high computational cost gauge fixing step prediction methods requiring large dense factorizations pseudo inverses directly 
main computation done convenient low cost gauge results transformed desired gauge gauge projector pg dg probably easiest trivial gauge computation 
simply matter deleting rows columns corresponding ng preselected parameters chosen give reasonably conditioned gauge 
choice automatically subset selection method 
left intact factored usual final dense owing fill submatrix factored stable pivoted method factorization stopped ng columns completion 
remaining ng ng block corresponding block forward substituted gradient zero owing gauge deficiency 
corresponding rows state update set zero wanted back substitution gives remaining update components usual 
method effectively finds ng parameters constrained data chooses gauge constraints freeze setting corresponding xg components zero 
quality control section discusses quality control methods bundle adjustment giving diagnostic tests detect outliers characterize accuracy reliability parameter estimates 
techniques known vision go detail 
skip technical details interested 
quality control serious issue measurement science philosophical differences vision workers greatest insists equipment careful project planning exploitation prior knowledge thorough error analyses vision researcher advocates casual flexible point shoot approach minimal prior assumptions 
applications demand judicious compromise virtues 
basic maxim quality accuracy reliability absolute accuracy system depends imaging geometry number measurements theoretical precision system reliable face outliers small modelling errors forth 
key reliability intelligent redundancy results represent internally self consistent consensus independent observations aspect rely excessively just observations 
photogrammetric literature quality control deserves better known vision especially researchers working statistical issues 
gr give introductions examples effects poor design 
see 
papers squares cost functions scalar measurements 
treatment generalizes allow robust cost functions vector measurements slightly self consistent projector pg calculated 
applied pieces multiplying gauged newton step xg easily selected blocks covariance vg pg vg way expanding pg leading term remaining ones finding forwards substitution 
accuracy called precision photogrammetry preferred retain familiar meanings numerical analysis precision means numerical error number working digits accuracy means statistical error number significant digits 
traditional approach 
techniques considered useful data analysis reporting check design requirements realistically attainable project planning 
properties verified 
internal reliability ability detect remove large observations internal self consistency checks 
provided traditional outlier detection robust estimation procedures 
external reliability extent remaining undetected outliers affect estimates 
sensitivity analysis gives useful criteria quality design 
model selection tests attempt decide possible models appropriate certain parameters eliminated 
cost perturbations start analyzing approximate effects adding deleting observation changes cost function solution 
second order taylor expansion characterize effects 
respectively total cost functions observation included cost contribution observation 
gradients hessians letx unknown true underlying state minima optimal state estimates observation included 
residuals meaningful quantities outlier decisions unknown forced residuals 
unfortunately see biased 
bias small strong geometries large weaker ones produce uniformly reliable statistical tests correct 
fundamental result sufficiently behaved cost function difference fitted residuals asymptotically unbiased accurate estimate nz nx note combining values known evaluation points simulate value third unknown 
estimate perfect best circumstances 
usually observations test avoid having model times approximate effects adding removing observations 
working fact newton step implies change fitted residual systematically underestimates overestimates biases order nz nx negligible plenty data large low redundancies 
intuitively including improves estimate average bringing reduction overfits slightly bringing bad reduction 
alternatively reduction moving bought cost slight increase minimum morally charged sketch proof newton steps unbiased relatively high order central limit property ml estimators asymptotic distributions gaussian expectation asymptotically number free model parameters nx 
expanding leading term asymptotically normal distribution standard deviation order nz nx asx nz nx 
deleting observations usually evaluated corresponding factorization find newton step near requires vice versa addition 
provided usually sufficient place simple tests 
observation couples relatively state variables possible calculate relevant components fairly economically 
means select variables non zero approximation involves factorization inverse 
squares usually lower rank number independent observations woodbury formula calculate inverse efficiently 
inner reliability outlier detection robust cost models special needs done outliers just normal measurements happen owing large deviations 
non robust models squares explicit outlier detection removal essential inner reliability 
effective diagnostic estimate significance test distribution null hypothesis observation inlier 
squares cost model null distribution number independent observations contributing suitable significance threshold typical sided significance test usual approximate tests require fitted covariance matrix relatively tests run equivalent zi wi wi ji wi zi results additions ones deletions 
factorization usually fairly economical owing sparseness observation gradients 
equation nonlinear squares model residual error zi zi zi cost zi wi zi jacobian ji note dx zi induces change components observation residual influence immediately involved components zi required 
bias correction induced change weight matrix wi wi wi ji wi accounts 
non quadratic cost functions framework applies cost function native distribution negative log likelihood values gaussian principle analysis valid outlier causes relatively small perturbation practice observations repeatedly scanned outliers stage removing discovered outliers previously discarded observations inliers 
net result form estimator routine abruptly vanishing weight function outlier deletion just roundabout way simulating robust cost function 
hard inlier outlier rules correspond total likelihood functions strictly constant outlier region 
tests give needed outlier decisions fitted state estimates planning purposes useful know large gross error typically true state detected 
outlier detection uncertain fitted state estimates lower right corner components correspond block schur complement remaining variables 
adding changes term schur complement correction 
give average case result 
adjustment needed case average minimum detectable gross error simply outer reliability ideally state estimate insensitive possible remaining errors observations 
estimate particular observation influences final state estimate directly monitor displacement 
example define importance weighting state parameters criterion matrix monitor absolute displacements uh compare displacement covariance monitoring 
bound form positive semidefinite implies bound vh bound uh vh norm trace frobenius norm 
robust cost model globally bounded gives asymptotic bounds order nz nx state perturbation regardless outlier occurred 
non robust cost models inlier criterion limit squares observation model rejection test wi wi ji wi maximum state perturbation due declared observation zi ji wi wi wi ji wi wi ji trace ji trace ji uh nominal covariance zi 
note bounds changes estimated state 
directly control perturbations true 
combined influence nz nx observations summing sensitivity analysis section gives simple figures merit quantify network redundancy reliability 
cost contribution split parts visible residual fitted state base cost due state perturbation induced observation 
ideally state perturbation small stability residual large outlier detectability 
words masking factor small convenient intermediate form deriving bounds 
positive semidefinite matrices say dominates ifb positive semidefinite 
follows matrix matrix function non decreasing positive additions 
rotationally invariant non decreasing functions include non decreasing functions eigenvalues norm max trace frobenius norm vector positive ba aa proof conjugate reducing householder rotation reduce question equivalence diag 

bounds form ua ub follow vv norm trace frobenius norm 
mi observation mi wi ji wi zi zi wi wi ji wi zi normalized minimum value exact fit 
mi known outlier test mi 
masking mi depends relative size general depends functional form specific deviation involved 
robust cost models bound may bound mi outliers 
squares case form generally quadratic cost models robust models near origin mi depends direction zi size global matrix norm bound mi ji trace ji ll wi cholesky decomposition wi 
bounds equalities scalar observations 
stability state estimate determined total cost hessian information matrix implies small state estimate covariance small responses cost perturbations sensitivity numbers si trace hi useful measure relative amount information contributed observation 
sum model dimension si nx hi count parameters worth total information observation contributes 
authors prefer quote redundancy numbers ri ni si effective number independent observations contained zi 
redundancy numbers sum nz nx total redundancy system 
squares case si trace ji mi si scalar observations scalar outlier test ri 
sensitivity numbers defined subgroups parameters form trace uh orthogonal projection matrix selects parameters interest 
ideally sensitivities subgroup spread evenly observations large si indicates heavily weighted observation incorrectness significantly compromise estimate 
model selection necessary chose alternative models cameras scene additional parameters lens distortion camera calibrations may may changed images coplanarity non coplanarity certain features 
special models give biased results general ones tend noisy unstable 
consider nested models general model specialized specific freezing parameters default values zero skew lens distortion equal calibrations zero deviation plane 
parameter vector general model cost function parameter freezing constraints enforcing specialization number parameters frozen true underlying state xg optimal state estimate general model unconstrained minimum xs optimal state estimate specialized minimum subject constraints 
null hypothesis specialized model correct asymptotic limit xg xs gaussian constraints locally approximately linear width gaussian difference fitted residuals xs xg distribution xs xg happens irrespective observation distributions case adding observation observations cost function fits 
suitable decision threshold accept hypothesis additional parameters take default values specialized model general avoid fitting models linearized analysis 
suppose start fit general model xg 
linearized constraints xg xg xg wherec dc straightforward lagrange multiplier calculation gives dx xs xg xg ch xg xs xg ch xg conversely starting fit specialized model unconstrained minimum newton step xg xs xs xs xg xs xs xs residual cost gradient xs 
requires general model covariance equivalent factorization may worked 
suppose additional parameters simply appended model reduced parameter vector specialized model contains additional parameters 
general model cost gradient xs ys df dy hessian ha straightforward calculation shows xs ys xg yg ah xg yg xs ys ah equivalent factorization tests relatively inexpensive small amount respectively step sequential quadratic programming newton step results accurate methods converge rapidly 
softer way handle nested models apply prior peaked zero specialization constraints 
weak data override necessary constraints may accurately enforced 
stronger apply outlier test remove appears incorrect sticky prior prior similar robust distribution concentrated central peak wide flat tails hold estimate near constraint surface weak data allow data stronger 
heuristic rules model selection photogrammetry example deleting additional parameters excessively correlated correlation coefficient greater parameters appears cause excessive increase covariance parameters 
network design network design problem planning camera placements numbers images measurement project ensure sufficiently accurate reliable estimates needs measured 
say design merely outlining basic considerations giving useful rules thumb 
see chapter vol information 
factors considered network design include scene coverage occlusion visibility feature viewing angle field view depth field resolution workspace constraints geometric practice small models preferable greater stability predictive power computational cost 
threshold usually chosen comparatively large ensure general model chosen strong evidence 
strength accuracy redundancy 
basic quantitative aids design covariance estimation suitably chosen gauge see quality control tests 
expert systems developed practice designs personal experience rules thumb 
general geometric stability best convergent close wide baseline high perspective geometries wide angle lenses cover object possible large film ccd formats maximize measurement precision 
wide coverage maximizes overlap different sub networks network rigidity wide baselines maximize sub network 
practical limitations closeness workspace field view depth field resolution feature viewing angle constraints 
maximizing overlap sub networks important 
objects faces buildings images taken corner positions tie face sub networks 
large projects large scale overview images tie close ones 
covering individual faces surfaces overlap stability improved images range viewing angles strictly fronto parallel ones number images pan move pan move interleaved left looking right looking images simple fronto parallel track 
similarly buildings turntable sequences mixture low high viewpoints helps stability 
reliability usually plans see feature point images 
images principle suffice reconstruction offer little redundancy resistance feature extraction failures 
images internal reliability poor isolated outliers usually detected may difficult say images occurred 
image geometries widely spaced non aligned centres usually give isotropic feature error distributions image ones 
bundle adjustment include self calibration important include range viewing angles 
example flat compact object views taken regularly spaced points half angle cone centred object optical axis rotations views 
summary recommendations survey written hope making photogrammetric know bundle adjustment simultaneous optimization structure camera parameters visual reconstruction accessible potential implementors computer vision community 
main lessons extraordinary versatility adjustment methods critical importance exploiting problem structure continued dominance second order newton algorithms spite efforts simpler order methods converge rapidly 
finish giving series recommendations methods 
regarded provisional subject revision testing 
parametrization step prediction avoid parameter singularities strong nonlinearities ill conditioning 
conditioned local current value offset parametrizations nonlinear elements necessary achieve local step prediction parametrization different global state representation 
ideal parameter space error function isotropic near quadratic possible 
residual rotation quaternion parametrizations advisable rotations projective homogeneous parametrizations distant points lines planes features near singularity affine parametrizations affine infinity 
cost function cost realistic approximation negative log likelihood total inlier outlier error distribution 
exact functional form distribution critical undue weight outliers making tails distribution predicted probability outliers unrealistically small 
nb compared realworld measurement distributions tails gaussian unrealistically small 
ii dispersion matrix inlier covariance realistic estimate actual inlier measurement dispersion transition inliers outliers right place inlier errors correctly weighted fitting 
optimization method batch problems second order gauss newton method sparse factorization see hessian problem large exact sparse factorization impractical 
case consider iterative linear system solvers conjugate gradient newton step related nonlinear iterations conjugate gradient preferably limited memory quasi newton memory permits full quasi newton 
methods require hessian 
case pay investigate professional large scale optimization codes lancelot commercial methods nag see 
problem medium large dense unusual strong geometry alternation resection intersection may preferable second order method 
case successive relaxation sor better conjugate gradient better 
cases preconditioning critical 
line problems batch ones factorization updating matrix inverse updating re factorization 
time series problems investigate effect changing time window remember kalman filtering half iteration full nonlinear method 
factorization method speed preserve symmetry hessian factorization cholesky decomposition positive definite hessians unconstrained problems trivial gauge pivoted cholesky decomposition positive semi definite hessians unconstrained problems gauge fixing subset selection bunch kauffman decomposition indefinite hessians augmented hessians constrained problems 
gaussian elimination stable factor slower 
variable ordering variables usually ordered hand regular networks irregular ones close range site modelling experimentation may needed find efficient ordering method 
reasonably compact profiles profile representations simpler implement faster general sparse ones 
dense networks profile representation natural variable ordering features cameras cameras features whichever fewest parameters 
explicit reduced system implementation brown method case 
problem sort temporal spatial structure image streams turntable problems try profile representation natural simple connectivity banker complex connectivity orderings 
recursive line updating method useful case 
problem structure cartography surface coverage problems try nested dissection hand ordering regular problems cartographic blocks multilevel scheme complex ones 
profile representation may may suitable 
regular sparse networks choice clear 
try minimum degree ordering general sparse representation banker profile representation multilevel nested dissection 
automatic variable ordering methods try order especially highly connected variables hand invoking method 
gauge fixing efficiency trivial gauge subset selection method working gauge calculations project results gauge want applying suitable gauge projector pg 
strong reason external system output gauge probably inner gauge centred network elements care observed features reconstruction problem cameras navigation 
quality control network design robust cost function helps system reliability need plan measurements advance developed intuition check results outlier sensitivity modelling suitable quality control procedure 
underestimate extent low redundancy weak geometry general models gross errors undetectable 
historical overview appendix gives brief history main developments bundle adjustment including literature 
squares theory combining measurements minimizing sum squared residuals developed independently gauss legendre vol iv years robust estimation 
squares motivated estimation problems astronomy geodesy extensively applied fields gauss remarkable monograph contains complete modern theory squares including elements theory probability distributions definition properties gaussian distribution discussion bias gauss markov theorem states squares gives best linear unbiased estimator blue 
introduces ldl form symmetric gaussian elimination gauss newton iteration nonlinear problems essentially modern forms explicitly matrices 
supplement geodesy introduced gauss seidel iteration solving large nonlinear systems 
economic military importance surveying lead extensive squares developments nested dissection probably systematic sparse matrix method cholesky decomposition theory reliability measurement networks theories uncertain coordinate frames free networks 
return topics 
second order bundle algorithms electronic computers capable solving reasonably large squares problems available late basic photogrammetric bundle method developed air force duane brown workers 
initial focus aerial cartography late bundle methods close range measurements links geodesic squares possibility combining geodesic types measurements photogrammetric ones clear right close range means essentially object significant depth relative camera distance significant perspective distortion 
aerial images scene usually shallow compared viewing height focal length variations difficult disentangle depth variations 
gauss legendre squares blue gaussian distribution gaussian elimination brown calibrated bundle adjustment images recursive partitioning free network adjustment uncertain frames inner covariance constraints inner outer reliability data snooping brown self calibration error empirical camera models brown recursive partitioning image aerial block transforms criterion matrices gr photogrammetric reliability accuracy precision reliability parametrization model choice gr geometrically constrained globally enforced squares matching gauge freedom uncertainty modelling modern robust statistics model selection image matching modern sparse matrix techniques schematic history bundle adjustment 
start 
initially cameras assumed calibrated optimization object points camera poses 
self calibration estimation internal camera parameters bundle adjustment discussed implemented 
camera models greatly refined early investigation alternative sets additional distortion parameters 
stable carefully calibrated aerial photogrammetry cameras self calibration significantly improved accuracies factors 
lead rapid improvements camera design previously defects film non flatness corrected 
development lead brown collaborators 
see history 
brown initial bundle method uses block matrix techniques eliminate structure parameters normal equations leaving camera pose parameters 
resulting reduced camera subsystem solved dense gaussian elimination back substitution gives structure 
self calibration second reduction pose calibration parameters added way 
brown method probably vision researchers think bundle adjustment descriptions hartley 
reasonable choice small dense networks rapidly inefficient large sparse ones arise aerial cartography large scale site modelling 
larger problems natural sparsity exploited 
aerial cartography regular structure relatively straightforward 
images arranged blocks rectangular irregular grids designed uniform ground coverage formed parallel strips images forward overlap giving adjacent stereo pairs triplets side overlap known ground control points sprinkled sparsely block 
features shared neighbouring images images couple reduced camera subsystem calibration denotes internal camera parameters interior orientation photogrammetric terminology 
external calibration called pose exterior orientation 
photogrammetric network dense features visible images sparse features appear images 
corresponds directly density sparsity diagonal block feature camera coupling matrix bundle hessian 
share common features 
images arranged strip cross strip ordering reduced camera system banded block structure upper lower bands representing right left neighbours central band forward backward ones 
efficient numerical schemes exist matrices 
brown recursive partitioning method closely related geodesic method 
generalizations major families modern sparse matrix methods 
basic idea split rectangle halves recursively solving half gluing solutions common boundary 
algebraically variables reordered left half right half boundary variables representing coupling halves eliminated 
technique extremely effective aerial blocks similar problems small separating sets variables 
brown mentions adjusting block photos machine words memory photo blocks feasible mid 
regular networks site modelling ones may feasible choose appropriate variable ordering efficient line ordering methods exist see 
independent model methods approximate bundle adjustment calculating number partial reconstructions independently merging pairwise alignment 
individual models alignments separately optimal result suboptimal stresses produced alignment propagated back individual models 
doing amount completing full iteration optimal recursive decomposition style bundle method see 
independent model methods time standard aerial photogrammetry merge individual stereo pair reconstructions aerial strips global reconstruction block 
accurate bundle methods cases accuracy comparable 
order approximate bundle algorithms recurrent theme approximations iterative methods avoid solving full newton update equations 
plausible approximations rediscovered times especially variants alternate steps resection finding camera poses known points intersection finding points known camera poses linearized version block gauss seidel iteration 
brown group experimented block successive relaxation accelerated variant gauss seidel developed recursive decomposition method 
gauss seidel applied independent model problem time 
methods mainly historical interest 
large sparse problems aerial blocks compete efficiently organized second order methods 
inter variable couplings ignored corrections propagate slowly network typically step iteration iterations required convergence see 
quality control parallel algorithmic development important theoretical developments took place 
firstly dutch led long running working group formulated theory statistical reliability squares estimation 
greatly clarified conditions essentially redundancy needed ensure outliers detected residuals inner reliability remaining undetected outliers limited effect final results outer reliability 
gr adapted theory photogrammetry gave early correlation covariance model selection heuristics designed control fitting problems caused elaborate camera models self calibration 
datum gauge freedom secondly problem size sophistication increased increas ingly difficult establish sufficiently accurate control points large geodesic photogrammetric networks 
traditionally network viewed means fixed control coordinate system propagating control system coordinates known control points unknown ones 
viewpoint suboptimal network intrinsically accurate control apparent uncertainty simply due uncertain definition control coordinate system 
early studied problem developed free network approach coordinate system floated freely locked control points 
precisely coordinates pinned sort average structure defined called inner constraints 
owing removal control related uncertainties nominal structure covariances smaller easier interpret numerical bundle iteration converges rapidly 
introduced approach theory transforms coordinate transforms uncertain frames 
squares matching developments originally manually extracted image points 
automated image processing clearly desirable gradually feasible owing sheer size detail photogrammetric images 
feature direct region methods studied especially matching low contrast natural terrain cartographic applications 
rely form squares matching image correlation called photogrammetry 
correlation matching techniques remain accurate methods extracting precise translations images high contrast photogrammetric targets low contrast natural terrain 
starting gr workers combined region squares matching various geometric constraints 
multi photo geometrically constrained matching optimizes match multiple images simultaneously subject inter image matching geometry 
surface patch single search patch depth possibly slant simultaneously moves epipolar lines images 
initial versions assumed known camera matrices full patch bundle method investigated 
related methods computer vision include 
globally enforced squares matching stabilizes solution low signal regions enforcing continuity constraints adjacent patches 
patches arranged grid matched local affine projective deformations additional terms penalize mismatching patch boundaries 
related vision includes 
inter patch constraints give structure squares matching equations handled efficiently recursive decomposition 
matrix factorization appendix covers standard material matrix factorization including technical details factorization factorization updating covariance calculation methods 
see details 
terminology depending factorization stands lower triangular upper triangular diagonal orthogonal factors 
triangular decompositions matrix family block lower triangular diagonal upper triangular factorizations 


dr urn am am amn lm lm lmr lii di uii aii lij aij jj uij ii aij aij aij min dk aij min aik kk diagonal blocks dr chosen square invertible determined rank recursion follows immediately product aij ij min dk 
factorization linear equations solved forwards backwards substitution 
diagonal blocks chosen freely subject lii dii uii aii done factorization uniquely defined 
choosing lii dii uii aii gives block lu decomposition lu matrix representation block gaussian elimination 
choosing lii uii di aii gives decomposition 
ifa symmetric decomposition preserves symmetry ldl decomposition ldl symmetric positive definite set get cholesky decomposition ll ii aii recursively defines cholesky factor lii positive definite matrix aii 
scalar 
blocks chosen get conventional scalar forms decompositions 
decompositions obviously equivalent speed simplicity usual specific applies lu general matrices ldl symmetric ones cholesky symmetric positive definite ones 
symmetric matrices bundle hessian ldl cholesky times faster lu 
general form trivial specialize 
loop ordering ij block decomposition depends upper left submatrix elements row column wherem min 
allows considerable freedom ordering operations decomposition exploited enhance parallelism improve memory cache locality 
fill sparse factors tend denser decomposition progresses 
recursively expanding aik gives contributions form aik kk akl qq min 
zero path form non zero akl min block decomposition generically fill non zero 
amount fill strongly dependent ordering variables rows columns 
sparse factorization methods manipulate ordering minimize fill total operation counts 
pivoting positive definite matrices factorizations stable pivots aii remain positive definite 
generally pivots may ill conditioned causing decomposition break 
deal usual search part matrix large pivot step permute leading position proceeding 
policy full pivoting searches submatrix usually costly partial pivoting search just current column column pivoting row pivoting suffices 
pivoting ensures relatively conditioned ill conditioning long possible ultimately better conditioned column pivoting usual lu decomposition applied symmetric matrix destroys symmetry doubles workload 
diagonal pivoting preserves symmetry searching largest remaining diagonal element permuting row column front 
suffices positive semidefinite matrices gauge deficient hessians 
general symmetric indefinite matrices augmented hessians constrained problems diagonal pivots avoided fast stable symmetry preserving pivoted ldl decompositions block diagonal having blocks 
full pivoting possible bunch parlett decomposition bunch kaufman decomposition searches diagonal columns usually suffices 
method nearly fast pivoted cholesky decomposition reduces positive matrices stable lu decomposition partial pivoting 
method similar speed stability produces tridiagonal constrained hessian special properties owing zero block consider see equilibrium systems 
orthogonal decompositions squares problems alternative family decompositions orthogonal reduction jacobian dz rectangular matrix decomposed dx qr upper triangular orthogonal columns orthonormal unit vectors 
called qr decomposition identical right cholesky factor qr solution linear squares problem minx ax andr moore penrose inverse qr decomposition calculated finding series simple rotations successively zero diagonal elements form accumulating rotations various types rotations 
givens rotations fine grained extreme parameter rotations zero single element affect rows 
householder reflections coarser grained reflections hyperplanes vv designed zero entire diagonal column affecting elements diagonal row column 
intermediate sizes householder reflections case computationally equivalent equal sign corresponding givens rotation 
useful sparse qr decompositions methods see 
householder method common general owing speed simplicity 
givens householder methods calculate explicitly calculated directly explicitly needed 
stored factorized form series rotations householder vectors applied piecewise needed 
particular needed solve squares system calculated progressively part decomposition process 
cholesky decomposition qr decomposition stable pivoting long full column rank ill conditioned 
degenerate householder qr decomposition column exchange pivoting 
see information qr decomposition 
archetypical failure unstable ldl decomposition conditioned symmetric indefinite matrix 
fortunately small diagonal elements permuting dominant diagonal element diagonal leaving resulting block suffices stability 
profile cholesky decomp aij max lij profile cholesky forward subs xi bi lii max xk profile cholesky back subs step max yk yk yi yi yi lii complete implementation profile cholesky decomposition 
qr decomposition cholesky decomposition normal matrix calculate cholesky qr factor solve squares problems design matrix jacobian qr method runs fast normal cholesky square twice slow long thin observations relatively parameters 
qr numerically normal cholesky sense condition number ratio largest smallest singular value machine precision qr solution relative error normal matrix condition number solution relative error matters approaches relative accuracy solution required 
example accurate bundle adjustments need relative accuracies greater double precision floating point safely normal equation method qr method safe bundle jacobian 
practice gauss newton normal equation approach bundle implementations 
individual householder reflections useful projecting parametrizations geometric entities orthogonal constraint vector 
example quaternions homogeneous projective vectors want enforce spherical normalization 
order displacements orthogonal allowed 
parametrize directions move need basis vectors orthogonal householder reflection converts orthogonal directions vectors form contains rows reduce jacobians independent parameters dx orthogonal subspace post multiplying solved recover orthogonal premultiplying multiple constraints enforced successive householder reductions form 
corresponds exactly lq method solving constrained squares problems 
profile cholesky decomposition simplest sparse methods suitable bundle problems profile cholesky decomposition 
natural features cameras variable ordering efficient method dense networks features visible images giving dense camera feature coupling blocks hessian 
suitable variable ordering efficient types sparse problems particularly ones chain connectivity 
shows complete implementation profile cholesky including decomposition ll forward substitution back substitution indices nonzero entries index nonzero entry row desired overwrite decomposition save storage 
factorizations loops reordered ways 
operation counts different access patterns memory cache localities modern machines lead significant performance differences large problems 
store access consistently rows 
matrix inversion covariances solving linear equations forward backward substitutions faster explicitly calculating multiplying numerically 
explicit inverses rarely needed evaluate dispersion covariance matrix covariance calculation expensive bundle adjustment matter sparse may dense 
triangular decomposition obvious way calculate product lower triangular recurrence ll follows similarly transposed ii lii ji jj ki jk alternatively diagonal zero upper triangle linear system ua combined zero lower triangle give direct recursion ji ii ii ii jk ii ij ii ki ii kj ik ii ii symmetric case ji ij avoid roughly half 
blocks required diagonal ones recursion property blocks associated filled positions calculated calculating blocks associated unfilled positions 
precisely calculate ij non zero need block kl significant saving sparse bundle problems 
particular covariance reduced camera system feature variances feature camera covariances calculated efficiently equivalently hss block diagonal feature hessian reduced camera 
banker strategy effective ordering strategies 
holds way fill occurs decomposition 
suppose want find ij 
need kj non zero ajk di ajk kj associated filled position evaluated 
factorization updating line applications useful able update decomposition account usually low rank change letb cu au 
low rank update decomposed efficiently 
separating block low rank update different evaluating recursively merging resulting factors gives updated decomposition bi ci di di bi ci ci bi ci bi bi ci uij uij uij ci bi form update numerically additions positive usable invertible 
case update takes time di things equal kept small possible splitting update independent rows initial factorization updating row turn 
scalar cholesky form method rank update bb bi lii di lii lii di di bi bi di takes operations 
recursion rule equivalent forms derived reducing upper triangular matrix givens rotations householder transformations 
software software organization general purpose bundle adjustment code extensible object organization natural 
measurement network modelled network objects representing measurements bj bk bk cj ck ck accumulate cu updates ci ci bi bi error models different types features camera models depend 
obviously useful allow measurement feature camera types open ended 
measurements may implicit explicit different robust error models possible 
features may range points curves homographies entire object models 
types camera lens distortion models exist 
scene dynamic articulated additional nodes representing transformations kinematic chains relative motions may needed 
main purpose network structure predict observations jacobians free parameters integrate resulting order parameter updates back internal feature camera state representations 
prediction essentially matter systematically propagating values network heavy chain rule derivative propagation 
network representation interface numerical linear algebra supports appropriate methods forming solving sparse damped gauss newton step prediction equations 
fixed order sparse factorization may suffice simple networks automatic variable ordering needed complicated networks iterative solution methods large ones 
extensible bundle codes exist far aware currently available freeware 
implementations include carmen program camera modelling scene reconstruction iterative nonlinear squares 
modular design allows different feature measurement camera types incorporated including quite exotic ones 
uses sparse matrix techniques similar brown reduced camera system method bundle adjustment iteration efficient 
www ee surrey ac uk personal mclauchlan html library supporting development efficient computer vision applications 
contains support image processing linear algebra visualization soon publicly available 
bundle adjustment methods variable state dimension filter commercialized 
algorithms support sparse block matrix operations arbitrary gauge constraints global local parametrizations multiple feature types camera models batch sequential operation 
modular vision environment new lightweight version environment developed mainly universities oxford leuven general electric crd 
initial public release www robots ox ac uk include opengl user interface classes multiple view geometry numerics mainly wrappers established routines netlib see 
bundle adjustment code exists currently planned release 
software resources great deal useful numerical linear algebra optimization software available internet commonly fortran 
main repository netlib www netlib org 
useful sites include guide available mathematical software gams gams nist gov guide www fp mcs anl gov otc guide part mor wright guide book object oriented numerics page org 
large scale dense linear algebra lapack www netlib org lapack best package available 
optimized relatively large problems matrices size solving small ones size may faster older linpack routines 
libraries blas basic linear algebra subroutines interface low level matrix manipulations optimized versions available processor vendors 
fortran versions interfaces exist www netlib org lapack math nist gov lapack 
sparse matrices bewildering array packages 
boeing www netlib org html implements sparse bunch kaufman decomposition ordering methods 
iterative linear system solvers implementation seldom difficult methods implementations 
templates book contains code 
nonlinear optimization various older codes codes designed mainly large problems ftp info mcs anl gov pub lancelot www cse ac uk activity lancelot 
codes reputations large scale problems far aware tested bundle adjustment 
packages freely available 
commercial vendors nag ttp www nag uk www com optimization codes 
glossary glossary includes common terms vision photogrammetry numerical optimization statistics translations 
additional parameters parameters added basic perspective model represent lens distortion similar small image deformations 
distribution family wide tailed probability distributions including cauchy distribution gaussian 
alternation family simplistic largely outdated strategies nonlinear optimization iterative solution linear equations 
cycles variables groups variables optimizing turn holding fixed 
nonlinear alternation methods usually equations group gauss seidel methods propagate order corrections forwards cycle results order 
successive relaxation adds momentum terms speed convergence 
see separable problem 
alternation resection intersection na rediscovered bundle method 
asymptotic limit statistics limit number independent measurements increased infinity second order moments dominate higher order ones posterior distribution approximately gaussian 
asymptotic convergence optimization limit small deviations solution solution reached 
second order quadratically convergent methods newton method square norm residual step order linearly convergent methods gradient descent alternation reduce error constant factor step 
banker strategy see fill 
block possibly irregular grid overlapping photos aerial cartography 
bunch kauffman numerically efficient factorization method symmetric indefinite matrices ldl lower triangular block diagonal blocks 
bundle adjustment refinement method visual reconstructions aims produce jointly optimal structure camera estimates 
calibration photogrammetry means internal calibration cameras 
see inner orientation 
central limit theorem states maximum likelihood similar estimators asymptotically gaussian distributions 
basis perturbation expansions 
cholesky decomposition numerically efficient factorization method symmetric positive definite matrices ll lower triangular 
close range photogrammetric problem scene relatively close camera significant depth compared camera distance 
terrestrial photogrammetry opposed aerial cartography 
conjugate gradient cleverly accelerated order iteration solving positive definite linear systems minimizing nonlinear cost function 
see krylov subspace 
cost function function quantifying total residual error minimized adjustment computation 
cram rao bound see fisher information 
criterion matrix network design ideal desired form covariance matrix 
damped newton method newton method stabilizing step control policy added 
see levenberg marquardt 
data snooping elimination outliers examination residual errors 
datum coordinate system coordinates uncertainties measured 
principle example gauge 
dense matrix system equations known zero elements may treated having 
opposite sparse 
photogrammetric networks dense means diagonal structure camera block hessian dense features seen images 
descent direction optimization search direction downhill component locally reduces cost 
design process defining measurement network placement cameras number images satisfy accuracy quality criteria 
design matrix observation state jacobian dz dx direct method dense correspondence reconstruction methods directly cross correlating photometric intensities related descriptor images extracting geometric features 
see squares matching feature method 
dispersion matrix inverse cost function hessian measure distribution spread 
asymptotic limit covariance dispersion 
fly removal observations recalculating scratch 
inverse updating 
elimination graph graph derived network graph describing progress fill sparse matrix factorization 
empirical distribution set samples probability distribution viewed sum approximation distribution 
law large numbers asserts approximation asymptotically converges true distribution probability 
fill tendency zero positions nonzero sparse matrix factorization progresses 
variable ordering strategies seek minimize fill permuting variables factorization 
methods include minimum degree reverse cuthill mckee banker strategies dissection 
see 
fisher information parameter estimation mean curvature posterior log likelihood function regarded measure certainty estimate 
cram rao bound says unbiased estimator covariance inverse fisher information 
free gauge free network gauge datum defined internally measurement network predefined features fixed gauge 
feature sparse correspondence reconstruction methods geometric image features points lines homographies 
direct photometry 
see direct method 
filtering sequential problems time series estimation current value previous measurements 
smoothing correct integrating information measurements 
order method convergence see asymptotic convergence 
gauge internal external coordinate system defined current state small variations quantities uncertainties measured 
coordinate gauge called datum 
gauge constraint constraint fixing specific gauge forthe current state arbitrary small displacements 
fact gauge chosen arbitrarily changing underlying structure called gauge freedom gauge invariance 
rank deficiency transformation invariance cost function induces hessian called gauge deficiency 
displacements violate gauge constraints corrected applying transform linear form gauge projection matrix pg 
gauss markov theorem says linear system squares weighted true measurement covariances gives best minimum variance linear unbiased estimator blue 
gauss newton method newton method nonlinear squares problems hessian approximated gauss newton design matrix weight matrix 
normal equations resulting gauss newton step prediction equations wj jw 
gauss seidel method see alternation 
givens rotation rotation part orthogonal reduction matrix qr svd 
see householder reflection 
gradient derivative cost function parameters df dx gradient descent na optimization method consists steepest descent coordinate system gradient cost function 
hessian second derivative matrix cost function dx symmetric positive semi definite cost minimum 
measures stiff state estimate perturbations 
inverse dispersion matrix 
householder reflection matrix representing reflection hyperplane tool orthogonal reduction matrix qr svd 
see givens rotation 
independent model method suboptimal approximation bundle adjustment developed aerial cartography 
small local models reconstructed images glued tie features common boundaries subsequent adjustment relax internal stresses caused 
inner internal intrinsic 
inner constraints gauge constraints linking gauge weighted average reconstructed features cameras externally supplied system 
inner orientation internal camera calibration including lens distortion inner reliability ability resist outliers detect reject residual errors 
intersection optical rays 
solving feature positions corresponding image features known camera poses calibrations 
see resection alternation 
jacobian see design matrix 
krylov subspace linear subspace spanned iterated products square matrix vector tool generating linear algebra nonlinear optimization iterations 
conjugate gradient famous krylov method 
kullback leibler divergence see relative entropy 
squares matching image matching photometric intensities 
see direct method 
levenberg marquardt common damping step control method nonlinear squares problems consisting adding multiple positive definite weight matrix gauss newton hessian solving step 
levenberg marquardt uses simple rescaling heuristic setting trust region methods sophisticated step length 
methods called damped newton methods general optimization 
local model optimization local approximation function optimized easy optimize iterative optimizer original function 
second order taylor series model gives newton method 
local parametrization parametrization nonlinear space offsets current point 
optimization step give better local numerical conditioning global parametrization 
lu decomposition usual matrix factorization form gaussian elimination 
minimum degree ordering widely automatic variable ordering methods sparse matrix factorization 
minimum detectable gross error smallest outlier detected average outlier detection method 
nested dissection top divide conquer variable ordering method sparse matrix factorization 
recursively splits problem disconnected halves dealing separating set connecting variables 
particularly suitable surface coverage problems 
called recursive partitioning 
nested models pairs models specialization obtained freezing certain parameters prespecified values 
network interconnection structure features cameras measurements image points 
usually encoded graph structure 
newton method basic iterative second order optimization method 
newton step state update minimizes local quadratic taylor approximation cost function iteration 
normal equations see gauss newton method 
nuisance parameter parameter estimated part nonlinear parameter estimation problem value really wanted 
outer external 
see inner 
outer orientation camera pose position angular orientation 
outer reliability influence outliers final parameter estimates extent reliable presumably small weighted outliers may remain undetected 
outlier observation deviates significantly predicted position 
generally observation fit preconceived notion observations distributed removed avoid disturbing parameter estimates 
see total distribution 
pivoting row column exchanges designed promote stability matrix factorization 
point estimator estimator returns single best parameter estimate maximum likelihood maximum posteriori 
pose position orientation angle camera 
preconditioner linear change variables designed improve accuracy convergence rate numerical method order optimization iteration 
variable scaling diagonal part preconditioning 
primary structure main decomposition bundle adjustment variables structure camera ones 
profile matrix storage scheme sparse matrices elements nonzero row stored zero 
simplicity efficient quite zeros 
quality control monitoring estimation process ensure accuracy requirements met outliers removed weighted appropriate models parameters 
radial distribution observation error distribution retains gaussian dependence squared residual error wx replaces exponential form robust long tailed 
recursive filtering reconstruction methods handle sequences images measurements successive updating steps 
recursive partitioning see nested dissection 
reduced problem problem variables eliminated partial factorization leaving 
reduced camera system result reducing bundle problem camera variables 

redundancy extent observation small influence results incorrect missing causing problems 
redundant basis reliability 
redundancy numbers heuristic measure amount redundancy estimate 
relative entropy information theoretic measure badly model probability density fits actual mean log likelihood contrast log 
resection optical rays 
solving camera poses possibly calibrations image features corresponding feature positions 
see intersection 
resection intersection see alternation 
residual error predicted observation cost function value 
transformation transformation gauges implemented locally gauge projection matrix pg 
scaling see preconditioner 
schur complement ab cd ca see 
second order method convergence see asymptotic convergence 
secondary structure internal structure sparsity diagonal feature camera coupling block bundle hessian 
see primary structure 
self calibration recovery camera internal calibration bundle adjustment 
sensitivity number heuristic number measuring sensitivity estimate observation 
separable problem optimization problem variables separated subsets optimization subset significantly easier simultaneous optimization variables 
bundle adjustment separable structure cameras 
alternation successive optimization subset na approach separable problems 
separating set see nested dissection 
sequential quadratic programming sqp iteration constrained optimization problems constrained analogue newton method 
step optimizes local model quadratic model function linearized constraints 
sparse matrix zeros pays take advantage wilkinson 
state bundle adjustment parameter vector including scene camera parameters estimated 
sticky prior robust prior central peak wide tails designed estimate peak strong evidence 
subset selection selection stable subset live variables line pivoted factorization 
method selecting variables constrain trivial gauge constraints 
successive relaxation sor see alternation 
sum squared errors sse nonlinear squares cost function 
possibly weighted sum squares residual feature projection errors 
total distribution error distribution expected observations type including inliers outliers 
distribution maximum likelihood estimation 
trivial gauge gauge fixes small set predefined features cameras coordinates irrespective values features 
trust region see levenberg marquardt 
updating incorporation additional observations recalculating scratch 
variable ordering strategy see fill 
weight matrix information inverse covariance matrix matrix designed put correct relative statistical weights set measurements 
woodbury formula matrix inverse updating formula 
ackermann 
digital image correlation performance potential applications photogrammetry 
photogrammetric record 
amer 
digital block adjustment 
photogrammetric record 
anderson bai bischof demmel dongarra du greenbaum hammarling mckenney sorensen 
lapack users guide third edition 
siam press philadelphia 
lapack home page www netlib org lapack 

liu 
robust ordering sparse matrices 
siam matrix anal 
appl 
atkinson editor 
close range photogrammetry machine vision 
publishing house scotland 

transformations criterion matrices 
netherlands geodetic commission publications geodesy new series vol pages 

statistical concepts geodesy 
netherlands geodetic commission publications geodesy new series vol pages 

testing procedure geodetic networks 
netherlands geodetic commission publications geodesy new series vol pages 

geometrically constrained matching 
phd thesis eth zurich 
barrett berry chan demmel donato dongarra van der vorst 
templates solution linear systems building blocks iterative methods 
siam press philadelphia 
ake bj 
numerical methods squares problems 
siam press philadelphia pa 

linear squares computations givens transformations 
canadian surveyor 
boggs byrd rodgers schnabel 
users guide software weighted orthogonal distance regression 
technical report nistir nist gaithersburg md june 
boggs byrd schnabel 
stable efficient algorithm nonlinear orthogonal regression 
siam sci 
statist 
comput 

de synopsis ac ex 
instituto academia iv 
brown 
solution general problem multiple station analytical 
technical report rca mtp data reduction technical report tr patrick base florida 
brown 
close range camera calibration 
photogrammetric engineering august 
brown 
calibration close range cameras 
int 
archives photogrammetry 
unbound pages 
brown 
bundle adjustment progress prospects 
int 
archives photogrammetry 
number pages 
chen medioni 
efficient iterative solutions view projective reconstruction problem 
int 
conf 
computer vision pattern recognition pages ii 
ieee press 
cooper cross 
statistical concepts application photogrammetry surveying 
photogrammetric record 
cooper cross 
statistical concepts application photogrammetry surveying continued 
photogrammetric record 
cox 
theoretical statistics 
chapman hall 
de jonge 
comparative study algorithms reducing fill cholesky factorization 
bulletin od 

photogrammetric inner constraints 
photogrammetry remote sensing 
duff reid 
direct methods sparse matrices 
oxford university press 
faugeras 
seen dimensions uncalibrated stereo rig 
sandini editor european conf 
computer vision santa margherita ligure italy may 
springer verlag 
fitzgibbon zisserman 
automatic camera recovery closed open image sequences 
european conf 
computer vision pages freiburg 
fletcher 
practical methods optimization 
john wiley 

evaluation block adjustment results 
int 
arch 
photogrammetry iii 

geometric precision digital correlation 
int 
arch 
photogrammetry remote sensing 

feature correspondence algorithm image matching 
int 
arch 
photogrammetry remote sensing 

reliability block triangulation 
photogrammetric engineering remote sensing 

reliability analysis parameter estimation linear models applications problems computer vision 
computer vision graphics image processing 
forsyth ioffe 
bayesian structure motion 
int 
conf 
computer vision pages corfu 
gauss 
gesellschaft der wissenschaften zu 
gauss 
theoria theory combination observations subject errors 
siam press philadelphia pa 
originally published pars prior pars posterior 
translation commentary stewart 
george 
nested dissection regular finite element mesh 
siam numer 
anal 
george heath ng 
comparison methods solving sparse linear squares problems 
siam sci 
statist 
comput 
george 
liu 
computer solution large sparse positive definite systems 
prentice hall 
george 
liu 
householder reflections versus givens rotations sparse orthogonal decomposition 
lin 
alg 
appl 
gill murray wright 
practical optimization 
academic press 
gill golub murray saunders 
methods modifying matrix factorizations 
math 
comp 
golub van loan 
matrix computations 
johns hopkins university press rd edition 
golub plemmons 
large scale geodetic squares adjustment dissection orthogonal decomposition 
linear algebra appl 

bundle adjustment methods engineering photogrammetry 
photogrammetric record 
greenbaum 
behaviour slightly perturbed lanczos conjugate gradient recurrences 
linear algebra appl 
greenbaum 
iterative methods solving linear systems 
siam press philadelphia 
gr accuracy reliability statistics close range photogrammetry 
inter congress symposium isp commission page 
unbound pages stockholm 
gr precision reliability aspects close range photogrammetry 
int 
arch 
photogrammetry 
gr optimum algorithm line triangulation 
symposium commission iii helsinki 
gr adaptive squares correlation concept results 
intermediate research report associates ohio state university 
pages march 
gr adaptive 

gr algorithmic aspects line triangulation 
photogrammetric engineering remote sensing 
gr 
adaptive squares correlation geometrical constraints 
spie computer vision robots volume pages cannes 
gupta hartley 
linear cameras 
ieee trans 
pattern analysis machine intelligence september 

inversion normal equations analytical method recursive partitioning 
technical report rome air development center rome new york 
hartley 
euclidean reconstruction multiple views 
nd europe workshop invariance pages october 
hartley 
object oriented approach scene reconstruction 
ieee conf 
systems man cybernetics pages beijing october 
hartley 
lines points views trifocal tensor 
int 
computer vision 
hartley gupta chang 
stereo uncalibrated cameras 
int 
conf 
computer vision pattern recognition pages urbana champaign illinois 
hartley zisserman 
multiple view geometry computer vision 
cambridge university press 
hartley saxena 
cubic rational polynomial camera model 
image understanding workshop pages 

die mathematischen und der sie volume teil 
teubner leipzig 
hendrickson rothberg 
improving run time quality nested dissection ordering 
siam sci 
comput 
holm 
test algorithms sequential adjustment line triangulation 

irani cohen 
direct recovery planar parallax multiple frames 
vision algorithms theory practice 
springer verlag 
kanatani ohta 
optimal robot self localization reliability evaluation 
european conf 
computer vision pages freiburg 

non topographic photogrammetry 
society photogrammetry remote sensing 
karypis kumar 
multilevel way partitioning scheme irregular graphs 
parallel distributed computing 
karypis kumar 
fast highly quality multilevel scheme partitioning irregular graphs 
siam scientific computing 
metis code see cs umn edu karypis 
king 
automatic reordering scheme simultaneous equations derived network systems 
int 
numer 
meth 
eng 
kraus 
photogrammetry bonn 
vol fundamentals standard processes 
vol advanced methods applications 
available german english languages 
legendre 
pour la termination des des com tes 
paris 
appendix squares 
levy 
restructuring structural stiffness matrix improve computational efficiency 
jet propulsion lab 
technical review 
li 
hierarchical multi point matching simultaneous detection location 
phd thesis kth stockholm 

luong deriche faugeras 
determining fundamental matrix analysis different methods experimental results 
technical report rr inria sophia antipolis france 
mason 
expert system design close range photogrammetric networks 
photogrammetry remote sensing 
mason 
expert system design photogrammetric networks 
ph thesis institut sie und eth rich may 
meer 
bootstrapping regression model application rigid motion evaluation 
vision algorithms theory practice 
springer verlag 
mclauchlan 
gauge independence optimization algorithms vision 
vision algorithms theory practice lecture notes computer science corfu september 
springer verlag 
mclauchlan 
gauge invariance projective reconstruction 
multi view modeling analysis visual scenes fort collins june 
ieee press 
mclauchlan 
variable state dimension filter 
technical report university surrey dept electrical engineering december 
mclauchlan 
batch recursive algorithm scene reconstruction 
int 
conf 
computer vision pattern recognition hilton head south carolina 
mclauchlan murray 
unifying framework structure motion recovery image sequences 
grimson editor int 
conf 
computer vision pages cambridge ma june 
mclauchlan murray 
active camera calibration head eye platform variable state dimension filter 
ieee trans 
pattern analysis machine intelligence 

die eines 
zeitschrift 
mikhail 
recursive methods photogrammetric data reduction 
photogrammetric engineering 

zur freier 
zeitschrift 
mor wright 
optimization software guide 
siam press philadelphia 
morris kanade 
unified factorization algorithm points line segments planes uncertainty 
int 
conf 
computer vision pages bombay 
morris kanatani kanade 
uncertainty modelling optimal structure motion 
vision algorithms theory practice 
springer verlag 
nocedal wright 
numerical optimization 
springer verlag 
okutomi kanade 
multiple baseline stereo 
ieee trans 
pattern analysis machine intelligence 
proctor 
adjustment aerial triangulation electronic digital computers 
photogrammetric record 
ripley 
pattern neural networks 
cambridge university press 

accuracy improvement digital matching elevation digital terrain models 
int 
arch 
photogrammetry remote sensing 
roy cox 
maximum flow formulation camera stereo correspondence problem 
int 
conf 
computer vision bombay 
saad 
rates convergence lanczos block lanczos methods 
siam numer 
anal 
editor 
manual photogrammetry 
american society photogrammetry remote sensing falls church virginia usa 

reducing profile sparse symmetric matrices 
bulletin od 
noaa technical memorandum nos ngs national geodetic survey rockville md szeliski kang shum 
parallel feature tracker extended image sequences 
technical report crl dec cambridge research labs may 
szeliski kang 
shape ambiguities structure motion 
european conf 
computer vision pages cambridge 
szeliski shum 
motion estimation quadtree splines 
int 
conf 
computer vision pages boston 
triggs 
new approach geometric fitting 
available www inrialpes fr people triggs 
triggs 
optimal estimation matching constraints 
koch van gool editors structure multiple images large scale environments smile lecture notes computer science 
springer verlag 
strang van 
variance covariance transformations geodetic networks 

wang clarke 
separate adjustment close range photogrammetric measurements 
int 
symp 
photogrammetry remote sensing part 
wolf 
adjustment computations statistics squares surveying gis 
john wiley sons 
wrobel 
facets stereo vision fast vision new approach computer stereo vision digital photogrammetry 
conf 
fast processing photogrammetric data pages switzerland june 

