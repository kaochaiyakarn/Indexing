technical report computer sciences department university wisconsin madison nov framework combining symbolic neural learning jude shavlik computer sciences department university wisconsin madison shavlik cs wisc edu article describes approach combining symbolic connectionist approaches machine learning 
stage framework research groups reviewed respect framework 
stage involves insertion symbolic knowledge neural networks second addresses refinement prior knowledge neural representation third concerns extraction refined symbolic knowledge 
experimental results open research issues discussed 
keywords knowledge neural networks theory refinement prior knowledge rule extraction neural networks kbann algorithm algorithm shorter version appear machine learning 
framework combining symbolic neural learning jude shavlik computer sciences department university wisconsin madison years produced explosion amount research machine learning 
rapid growth occurred largely independently symbolic connectionist neural network machine learning communities 
fortunately years communities separate increasing amount research considered hybrid approaches 
reviews research combines symbolic neural network approaches artificial intelligence presents framework combining paradigms 
attempt define precisely essential differences symbolic connectionist approaches lead lengthy debate far scope article 
distinction needed coarse approximation symbolic approaches focus producing discrete combinations features neural approaches adjust continuous non linear weightings inputs 
assume understanding fundamental differences paradigms research issue focus research incorporates traditionally considered aspects camps 
large number ways combine symbolic connectionist ai 
example utgoff developed algorithm closely integrates decision trees perceptrons 
loosely coupled hybrid system high level decisions symbolically low level neural networks gallant pomerleau thorpe 
special issues journals hendler hinton additional approaches 
attempting comprehensive review symbolic connectionist hybrid methods explored focus framework illustrates 
framework learner inserts symbolic information sort neural network increasingly clear learner effective prior knowledge order perform geman bienenstock doursat 
neural representation uses training examples refine initial knowledge 
extracts symbolic information trained network 
research groups fits nicely framework promising results achieved 
remainder discusses research points open issues phases 
continuing noted steps somewhat independent researchers studied various combinations 
remainder article organized questions 
consider neural networks symbol oriented learning tasks 
review research addresses questions phases insertion refinement extraction symbolic information 
page combining symbolic neural learning initial symbolic information insert initial neural network examples refine final symbolic information extract final neural network 
framework combining symbolic neural learning 
neural networks symbol oriented learning tasks 
avoid connectionist methods learn tasks inherently deal symbols 
neural networks primarily applicable low level perceptual tasks 
argue section answer related questions years starting papers published simultaneously ijcai fisher mckusick mooney weiss followed studies atlas dietterich hild bakiri groups empirically compared symbolic learning algorithms quinlan id decision tree algorithm connectionist approaches rumelhart hinton williams backpropagation method training neural networks 
studies produce consistent results coarse summary trained neural networks comparable accuracies induced decision trees tasks considered symbol oriented 
appears worthwhile investigate neural learning methods produce refine symbolic information 
addition neural network approaches proven successful wide range real world tasks speech understanding lippmann handwritten character recognition le cun control dynamic systems jordan rumelhart gene finding mural language learning touretzky 
experiments strongly suggest connectionist learning powerful approach neural networks symbolic knowledge merits exploration 
important note connectionist architectures simple feed forward single hidden layer neural networks 
particular recurrent networks elman jordan feedback loops memory especially appealing application symbolic tasks sequential nature :10.1.1.117.1928
page combining symbolic neural learning get symbolic information neural networks 
assuming convinced merit framework techniques inserting symbolic information neural network needed 
think preexisting information prior knowledge task hand question neural networks effectively hints abu mostafa 
answer kbann approach towell shavlik noordewier towell creates knowledge artificial neural networks producing neural networks topological structure matches dependency structure rules approximately correct domain theory collection inference rules current task 
table shows correspondences domain theory neural network simple example approach mapping domain theory neural networks 
kbann applied successfully refining domain theories real world problems gene finding towell protein folding maclin shavlik press control simple chemical plant scott shavlik ray 
appendix presents application kbann problem human genome project 
appendix example shows complicated mapping reports kbann approach generalizes examples seen training 
various groups knowledge neural networks train faster standard neural networks oliver schneider giles shavlik towell presumably initial information choose starting point network 
importantly experiments shown knowledge networks generalize better examples standard networks methods inductive learning theory refinement giles maclin shavlik press mcmillan hofmann tresp scott towell towell tresp ahmad 
attribute improved generalization aspects insertion process 
domain theory produces useful inductive bias focusing attention relevant input features indicating useful intermediate suggest network topology 
schematically illustrates general performance kbann system 
plots generalization performance testset accuracy function number examples table 
correspondences domain theory neural network 
domain theory neural network final output units intermediate hidden units supporting facts input units antecedents rule highly weighted links page combining symbolic neural learning ii iii 
sample application kbann rule insertion algorithm 
frame contains simple domain theory frame ii shows dependency structure rules 
third frame shows network kbann creates 
thick lines iii correspond dependencies rules kbann sets weights links manner nodes highly active domain theory supports corresponding deduction 
thin lines frame iii represent links kbann adds network allow refinement domain theory neural training 
available training 
qualitatively similar results obtained different domains see towell noordewier towell shavlik scott maclin shavlik press specific generalization curves domains 
case kbann imperfect domain theory 
domain specific information kbann produced starting network small numbers training examples resulted statistically significantly better testset accuracy obtained standard artificial neural network ann initialized small random weights 
asymptotically testset accuracy learning systems converged experimental domains collect training examples 
asymptotic convergence suggests view value domain theory initial knowledge worth number training examples 
domains collecting large number examples impossible costly protein folding able utilize alternate sources information prove quite valuable 
towell shown kbann knowledge networks better refine domain theory purely symbolic theory refinement systems 
holds compares rules extracted trained network refined rules produced symbolic theory refinement systems results provide justification complex representational shifts framework 
convert rules kbann extracts disjunctions conjunctive rules usually great increase number rules approaches searching hypothesis space 
towell empirical results may page combining symbolic neural learning testset errors kbann standard ann domain theory learning amount training data 
generalization new examples function number training examples 
problem specific broader searching continuous weight space neural networks better real world problems searching combinatorial space discrete rules complex concepts representation may simpler 
deeper understanding relative merits symbolic connectionist purely symbolic approaches theory refinement important open research issue 
addition simple propositional rules shown early kbann researchers produced techniques mapping forms prior knowledge networks 
fu mahoney mooney map rules containing certainty factors 

map fuzzy logic rules mcmillan mozer smolensky gating networks jacobs map production rules 
scott 

map mathematical equations demonstrating kbann approach require logic oriented domain theories 
groups mapped generalized finite state grammars recurrent neural networks maclin shavlik press giles scott 
generalized finite state grammars particularly interesting theory refinement community view state dependent domain theories richer type domain theory usually studied subfield machine learning 
approaches differ kbann various degrees essential idea prior knowledge decide initialize neural network 
domain theories studied theory refinement literature propositional address simple classification tasks 
mentioned groups studying finite state automata means expressing approximately correct domain knowledge 
finite state domain theories allow system memory means page combining symbolic neural learning base decisions current input summary inputs 
shows architecture connectionist approach refining state dependent domain theories 
recurrent neural networks system bases output current input internal state world model planning problems 
addition calculating current output system select state 
state dependent domain theory initially configure network manner analogous shown 
open questions regarding knowledge insertion process 
know types prior knowledge inserted networks 
example methods lacking inserting order theories 
years seen progress inductive logic programming muggleton quinlan useful see neural networks refine rules containing variables 
needs devise methods dealing unbounded symbolic structures neural networks size usually fixed training 
recurrent networks provide method dealing unbounded structures pollack recursive auto associative memories provide 
relevant research teaching networks recognize context free grammars having learn stack das giles mozer das 
unbounded structures stacks handled fixed size networks altering resolution sense product information stored resolution equals constant 
state current output hidden unit topology determined domain theory current state current input system boundary 
refining finite state domain theories 
page combining symbolic neural learning towell shown knowledge networks deleting irrelevant information approximately correct domain theories handle impoverished domain theories 
shows qualitative nature experiments demonstrate domain theories need approximately correct prove beneficial 
towell added spurious antecedents rules existing domain theory expansion corrupted domain theory lead better generalization standard hidden layer network domain theory 
delete antecedents domain theory lead worse testset accuracy 
words kbann approach better discarding erroneous information discovering missing knowledge 
open issue knowledge neural networks deal domain theories incomplete 
return topic section 
converting symbolic information neural network representation followed connectionist learning shown useful research groups 
leaves central question insertion phase convert symbolic knowledge learning tasks powerful numeric optimization search methods applicable 
testset errors standard ann drop antecedents add antecedents amount domain theory noise 
effect generalization corrupting domain theory 
page combining symbolic neural learning network refinement guided symbolic knowledge 
prior knowledge inserted network refined enhanced 
simple way doing run backpropagation standard connectionist training procedure training examples 
ways symbolic information improve training symbolic learning methods ideas focus adjustment network weights topology alter backpropagation better match symbolic nature problem 
section discuss approaches 
ask symbolic learning approaches far 
answer domain theory refinement knowledge networks addresses incorrect theory problem explanation learning 
fact perspective initial motivation kbann research shavlik towell 
mitchell thrun proposed explanation non symbolic method training neural networks reinforcement learning tasks 
performing symbolic inductive learning conjunction neural learning 
mentioned utgoff perceptron trees method doing algorithm applicable refinement prior knowledge 
recall knowledge networks input features fall classes mentioned domain theory 
domain theory imperfect ignore input features typically connected units links 
towell shavlik proposed technique uses symbolic inductive learning identify input features weighted heavily 
preprocessing network led better generalization 
mentioned domain theory may missing number rules 
mapped network small 
order learn missing rules additional nodes added network training 
opitz shavlik developed algorithm interprets networks symbolically decide add new nodes 
changes standard connectionist learning motivated symbolic problems 
minimizing mean squared error cross entropy error function hinton better choice knowledge networks see towell explanation 
refining rules certainty factors requires different activation function nodes fu mahoney mooney 
may wish constrain weight changes maintain symbolic interpretation network mcmillan 
networks decay weights zero training hinton 
weights knowledge networks decay initial values hinton personal communication tresp encouraging network preserve knowledge initial domain theory 
open questions regarding symbolic information aid refinement step 
detect extra nodes needed generalize best places add 
folk wisdom says backpropagation networks layers hidden units error signal diffuse 
symbolic information focus back propagated error signal especially deep networks 
deep networks occur basing network topology dependency structure rule base problem exacerbated knowledge networks 
need prevent distributed representations hinton evolving training 
hidden units knowledge networks initially symbolic meaning page combining symbolic neural learning distributed representations undesirable way take advantage distributed representations 
summary central question refinement phase symbolic knowledge task hand guide network refinement 
extract symbolic knowledge trained neural networks 
third phase framework involves extracting symbolic information rules trained network need originally knowledge 
important 
rule extraction help understand black box network learned 
network produced scientifically interesting discovery nice explicit 
may wish trained system produce explanations decisions 
may want manipulate results learning system planner 
people developed methods extracting rules standard networks 
gallant saito nakano fu proposed algorithms consider various ways node weighted input exceed threshold convert situations rule 
approaches require exponential number rules terms number network weights re represent node 
towell shavlik developed method produces rule node 
algorithm extracted comprehensible rules maintaining accuracy trained network 
approach works knowledge networks requires weights cluster groups soft weight sharing technique nowlan hinton may improve performance towell shavlik algorithm standard networks 
mcmillan 
simply project trained nodes closest valid rule hayashi extracts small number fuzzy logic rules trained network 
provides sketch towell shavlik algorithm 
algorithm extracts rules trained knowledge networks assumes nodes trained network values near zero boolean constraint easily achieved activation function final stages training 
step see upper middle panel clusters incoming weights unit standard clustering algorithm hartigan 
weights links cluster replaced average cluster weights network 
algorithm analyzes cluster determine clusters irrelevant 
cluster average weight value discarded due boolean constraint stated activation unit qualitatively effected settings nodes simple case analysis demonstrates 
zero active maximum weighted input node 
active minimum weighted input node 
case specific activations impact 
final step rewrite regularized simplified node rule shown bottom panel 
algorithm produce rules simple clean see towell shavlik real world example illustrate essential aspects rule extraction technique 
page combining symbolic neural learning bias bias initial unit cluster average rewrite bias reduce 
sketch rule extraction algorithm 
methods analyze weights going nodes 
servan schreiber mcclelland giles 
different perspective 
investigate extracting finite state automata recurrent networks methods focus activation patterns hidden units 
approaches assume patterns represent sort internal state 
extraction algorithms cluster patterns view cluster state automaton 
step runs training examples trained network obtain state transitions traditional algorithms minimize automaton 
major question rule extraction measure comprehensibility 
extraction algorithm produce reasonably comprehensible rules measure hard compare alternative approaches 
second open issue relates refinement phase task altered support rule extraction 
possibly network constrained lie comprehensible portion weight space 
related hidden units knowledge networks generally symbolic names attached going labels extracted rules needs ensure symbol node correspondence altered training 
reason formation distributed representations training harmful 
conceptually clustering hidden unit activations promising area symbolic machine learning 
finding describing clusters provide insight distinctions network 
example sejnowski rosenberg manually analyzed clusters developed nettalk task success 
page combining symbolic neural learning wrap section central question rule extraction extract small comprehensible symbolic version trained network losing accuracy 
connectionist machine learning proven fruitful approach sense investigate systems combine strengths symbolic connectionist approaches ai 
past years researchers successfully developed number systems 
article summarizes view endeavor framework encompasses approaches different research groups 
framework see views combination symbolic neural learning stage process insertion symbolic information neural network partially determining topology initial weight settings network refinement network numeric optimization method backpropagation possibly guidance symbolic knowledge extraction symbolic rules accurately represent knowledge contained trained network 
components form appealing complete picture approximately correct symbolic information accurate symbolic information stages independently studied 
research summarized demonstrates combining symbolic connectionist methods promising approach machine learning 
material article invited talk international machine learning conference held aberdeen scotland 
wish geoff towell mick noordewier rich maclin gary scott mark craven dave opitz derek zahn charlie kevin members university wisconsin machine learning research group time major contributors ideas chapter 
discussions ray mooney tom dietterich geoff hinton substantially influenced discussion 
partially supported office naval research national science foundation iri department energy de fg er 
atlas cole connor el marks ii barnard 

performance comparisons backpropagation networks classification trees real world applications 
advances neural information processing systems vol 
touretzky ed san mateo ca morgan kaufmann 
abu mostafa 

learning hints neural networks 
journal complexity 


refinement approximate reasoning controllers reinforcement learning 
proceedings eighth international machine learning workshop pp 
evanston il morgan kaufmann 
servan schreiber mcclelland 

finite state automata simple recurrent networks 
neural computation 
page combining symbolic neural learning das giles sun 
hints successfully learn context free grammars neural network pushdown automaton 
advances neural information processing systems vol 
hanson giles eds san mateo ca morgan kaufmann 
dietterich hild bakiri 

comparative study id backpropagation english speech mapping 
proceedings seventh international conference machine learning pp 
austin tx morgan kaufmann 
elman 

finding structure time 
cognitive science 
fisher mckusick 

empirical comparison id back propagation 
proceedings eleventh international joint conference artificial pp 

detroit morgan kaufmann 
frasconi gori soda 

unified approach integrating explicit knowledge learning example recurrent networks 
proceedings international joint conference neural networks pp 

seattle ieee press 
fu 

integration neural heuristics knowledge inference 
connection science 
fu 

rule learning searching adapted nets 
proceedings ninth national conference artificial intelligence pp 

anaheim ca aaai press 
gallant 

connectionist expert systems 
communications acm 
bienenstock doursat 

neural networks bias variance dilemma 
neural computation 
giles sun chen lee chen 

higher order recurrent networks grammatical inference 
advances neural information processing systems vol 
touretzky ed san mateo morgan kaufmann 
giles miller chen chen sun lee 

learning extracting finite state automata second order recurrent neural networks 
neural computation 
hartigan 

clustering algorithms new york wiley 
hawley mcclure 

compilation analysis escherichia coli promoter dna sequences 
nucleic acids research 
hayashi 

neural expert system automated extraction fuzzy rules application medical diagnosis 
advances neural information processing systems vol 
lippmann moody touretzky eds san mateo ca morgan kaufmann 
hendler 
ed 

special issue hybrid systems symbolic connectionist 
connection science 
hinton 

learning distributed representations concepts 
proceedings eighth annual conference cognitive science society pp 

amherst ma erlbaum 
hinton 

connectionist learning procedures artificial intelligence 
hinton 
ed 

special issue connectionist symbol processing 
artificial intelligence 
jacobs jordan nowlan hinton 

adaptive mixtures local experts 
neural computation 
jordan 

attractor dynamics parallelism connectionist sequential machine 
proceedings eighth annual conference cognitive science society pp 
amherst ma erlbaum 
jordan rumelhart 

forward models supervised learning distal teacher 
cognitive science 
le cun boser denker henderson howard hubbard jackel 

backpropagation applied handwritten zip code recognition 
neural computation 
lippmann 

review neural networks speech recognition 
neural computation 
maclin shavlik 
press 
knowledge neural networks improve algorithms refining chou algorithm protein folding 
machine learning 
mahoney mooney 

combining neural symbolic learning revise probabilistic rule bases 
advances neural information processing systems vol 
hanson giles eds san mateo ca morgan kaufmann 
page combining symbolic neural learning watanabe kawamura 

system fuzzy inference structured neural network 
proceedings international conference fuzzy logic neural networks pp 
japan 
mcmillan mozer smolensky 

rule induction integrated symbolic subsymbolic processing 
advances neural information processing systems vol 
moody hanson lippmann eds san mateo ca morgan kaufmann 
mitchell thrun 

explanation neural network learning robot control 
advances neural information processing systems vol 
hanson giles eds san mateo ca morgan kaufmann 
mooney shavlik towell gove 

experimental comparison symbolic connectionist learning algorithms 
proceedings eleventh international joint conference artificial intelligence pp 

detroit morgan kaufmann 
extended version appeared machine learning 
mozer das 

connectionist chunker induces structure context free languages 
advances neural information processing systems vol 
hanson giles eds san mateo ca morgan kaufmann 
muggleton 

inductive logic programming london academic press 
noordewier towell shavlik 

training knowledge neural networks recognize genes dna sequences 
advances neural information processing systems vol 
lippmann moody touretzky eds san mateo ca morgan kaufmann 
nowlan hinton 

simplifying neural networks soft weight sharing 
advances neural information processing systems vol 
moody hanson lippmann eds san mateo ca morgan kaufmann 
oliver schneider 

rules task division augment connectionist learning 
proceedings tenth annual conference cognitive science society pp 
montreal erlbaum 
giles 

training second order recurrent neural networks hints 
proceedings ninth international conference machine learning pp 
aberdeen scotland morgan kaufmann 
neill 

escherichia coli promoters 
journal biological chemistry 
opitz shavlik 

heuristically expanding knowledge neural networks 
technical report madison wi university wisconsin computer sciences department 
pollack 

recursive distributed representations 
artificial intelligence 
pomerleau thorpe 

combining artificial neural networks symbolic processing autonomous robot guidance 
engineering applications artificial intelligence 
quinlan 

induction decision trees 
machine learning 
quinlan 

learning logical definitions relations machine learning 
hofmann tresp 

neural control rolling mills incorporating domain theories overcome data deficiency 
advances neural information processing systems vol 
moody hanson lippmann eds san mateo ca morgan kaufmann 
rumelhart hinton williams 

learning internal representations error propagation 
parallel distributed processing vol 
rumelhart mcclelland eds 
cambridge ma mit press 
saito nakano 

medical diagnostic expert system pdp model 
proceedings ieee international conference neural networks pp 

ieee press 
scott shavlik ray 

refining pid controllers neural networks 
neural computation 
sejnowski rosenberg 

parallel networks learn pronounce english text 
complex systems 
shavlik towell 

approach combining explanation neural learning algorithms 
connection science 
page combining symbolic neural learning touretzky 
ed 

special issue connectionist approaches language learning 
machine learning 
towell 

symbolic knowledge neural networks insertion refinement extraction 
doctoral dissertation madison wi university wisconsin computer sciences department 
towell shavlik noordewier 

refinement approximately correct domain theories knowledge neural networks 
proceedings eighth national conference artificial intelligence pp 
boston aaai press 
towell shavlik 

symbolic inductive learning improve knowledge neural networks 
proceedings tenth national conference artificial intelligence pp 
san jose ca aaai press 
towell shavlik 

interpretation artificial neural networks mapping knowledge neural networks rules 
advances neural information processing systems vol 
moody hanson lippmann eds san mateo ca morgan kaufmann 
longer version appear machine learning title extracting refined rules knowledge neural networks tresp ahmad 

network structuring training rule knowledge 
advances neural information processing systems vol 
hanson giles eds san mateo ca morgan kaufmann 
mural 

locating protein coding regions human dna sequences multiple sensor neural network approach 
proceedings national academy sciences 
utgoff 

perceptron trees case study hybrid concept representations 
proceedings seventh national conference artificial intelligence pp 

st paul mn morgan kaufmann 
weiss 

empirical comparison pattern recognition neural nets machine learning classification methods 
proceedings eleventh international joint conference artificial intelligence pp 

detroit morgan kaufmann 
appendix application kbann appendix summarizes study kbann produce method recognizing promoters coli dna towell promoters sites process expressing gene create protein begins 
table contains initial domain theory promoter recognition task underscore indicates position filled dna nucleotide 
rule says promoter involves subcategories contact conformation region 
second rule states contact involves regions subsequent rules define alternative ways regions appear 
set rules derived straightforward fashion biological literature describes dna sequences training neill 
input features sequential dna nucleotides 
special notation symbol simplify specification locations dna sequence 
biological literature counts locations relative site gene expression begins 
nucleotides location constitute example 
rule antecedents refer input features state starting location list sequence follow 
second rule conformation says adenine nucleotides start site 
adenine position nucleotides appear location 
kbann translates domain theory neural network topology shown 
clarity connections conformation input units shown 
notice order classify subsequence promoter region intermediate concepts page combining symbolic neural learning table 
imperfect domain theory dna promoters 
promoter contact conformation 
contact minus minus 
minus 
minus 
minus ttg ca 
minus ta 
minus 
minus 
minus 
minus ta 
conformation caa tt ac cc 
conformation aa conformation tg 
conformation aa 
promoter contact conformation minus minus dna sequence 
initial neural network promoter recognition kbann produces 
domain theory recognized 
recall algorithm adds additional weighted links shown additional sequence information relevant algorithm capture information backpropagation training 
sample promoters sequences refine initial neural network 
sample promoters obtained compilation produced hawley mcclure 
negative training examples derived selecting contiguous substrings sequence provided prof record wisconsin chemistry department 
virtue fact fragment bind rna polymerase protein initiates gene expression believed contain promoter sites record personal communication 
order get estimate kbann learned concept promoter standard experimental methodology called leave know jackknife testing cross validation 
technique operates training examples page combining symbolic neural learning testing example left 
procedure repeated times example excluded training set 
error rate number errors single test cases divided 
methodology applied learning algorithms standard backpropagation id quinlan 
standard backpropagation runs number hidden units network kbann produced 
convention input units connected hidden unit hidden unit connected output unit 
weights randomly initialized number near zero 
id empirical learning algorithm 
uses training data construct decision tree determining category example 
step new node added decision tree partitioning training examples value single informative feature 
feature chosen chosen partition examples maximizes statistical measure information gain 
table contains average error rate training examples learning algorithms original domain theory error rate fails exactly match examples 
cases algorithm correctly classified members training sets 
algorithm fully accounted training data kbann better job generalization error rate previously unseen examples substantially lower 
investigated nearest neighbor classification algorithm 
distance measure number mismatched nucleotides 
nearest neighbors selected majority class neighbors chosen classification novel example 
testing values lead lowest error 
table reports results neill ad hoc approach neill tailored specific task promoter recognition 
table 
error rates dna promoter experiment 
kbann standard backpropagation neill nearest neighbor id towell reports results additional learning algorithms including symbolic machine learning techniques refine domain theories alternates matched kbann generalization performance 
see towell shavlik results algorithm promoter problem 
promoter domain theory training examples available anonymous ftp ics uci edu university california irvine archive machine learning datasets domain theories 
page 
