siam comput 
society industrial applied mathematics vol 
pp 
second order perceptron algorithm nicol cesa bianchi alex claudio gentile 
kernel linear threshold algorithms support vector machines perceptron algorithms best available techniques solving pattern classification problems 
describe extension classical perceptron algorithm called second order perceptron analyze performance mistake bound model line learning 
bound achieved algorithm depends sensitivity second order data information best known mistake bound efficient kernel linear threshold classifiers date 
mistake bound strictly generalizes known perceptron bound expressed terms eigenvalues empirical data correlation matrix depends parameter controlling sensitivity algorithm distribution eigenvalues 
optimal setting parameter known priori analyze variants second order perceptron algorithm adaptively sets value parameter terms number mistakes far parameterless 
key words 
pattern classification mistake bounds perceptron algorithm ams subject classifications 
doi 


research linear threshold classifiers popularity kernel methods set mathematical tools efficiently represent complex nonlinear decision surfaces terms linear classifiers high dimensional feature space defined kernel functions :10.1.1.11.2062
extent statistical learning theories able explain kernel methods suffer curse dimensionality exhibit remarkable predictive power despite fact kernel induced feature space possibly infinite dimensions 
statistical results quantities margin provide superficial account way predictive power affected geometry feature space 
different approach analysis linear threshold classifiers mistake bound model line learning :10.1.1.130.9013
model similarly framework competitive analysis learning algorithm able sequentially classify sequence data points making number mistakes bigger best fixed linear threshold classifier hindsight predictor 
power approach resides fact excess loss learning algorithm respect predictor nicely bounded terms geometrical properties individual sequence algorithm run 
furthermore shown mistake bounds corresponding statistical risk bounds worse better obtainable direct statistical approach 
received editors july accepted publication revised form september published electronically march 
preliminary version appeared proceedings th annual conference computational learning theory colt pp 

research partially supported european commission project ist 
www siam org journals html department information sciences universit di milano italy cesa bianchi dsi unimi dti unimi 
universit dell italy gentile dsi unimi 
second order perceptron algorithm far best known mistake bound kernel linear threshold classifiers essentially achieved classical perceptron algorithm 
introduce extension standard perceptron algorithm called second order perceptron 
standard perceptron algorithm popular greedy method learning classifiers 
early sixties known performance perceptron algorithm governed simple geometrical properties input data 
perceptron algorithm essentially gradient descent order method improve introducing sensitivity second order data information 
second order algorithm combines perceptron gradient sparse incrementally computed version data correlation matrix 
analysis shows second order perceptron algorithm able exploit certain geometrical properties data missed order algorithm 
particular bounds second order algorithm depend distribution eigenvalues correlation matrix observed data sequence opposed bound order algorithm relying trace information 
typical situation second order bound substantially smaller order bound data lie flat ellipsoid eigenvalues correlation matrix sharply different magnitudes trace dominated largest 
basic form second order perceptron algorithm parameterized constant rules extent algorithm adapts data 
limit goes infinity algorithm firstorder perceptron algorithm limit second order bound reduces order 
value affects performance significant way 
best choice depends information learning task typically available ahead time 
develop variants algorithm prove corresponding mistake bounds 
variant adaptive parameter version second variant eliminates parameter replaces standard matrix inversion 
bounds prove able capture spectral properties data 
section take classical perceptron algorithm starting point defining motivating second order extension 
description steps intermediate algorithm called whitened perceptron algorithm serves illustration kind spectral behavior mean 
section concluded precise definition learning model description basic notation 

perceptron whitened perceptron 
classical perceptron algorithm processes stream examples xt yt time trials 
trial algorithm receives instance vector xt predicts value unknown label yt associated xt 
algorithm keeps weight vector wt representing internal state prediction time kernel linear threshold algorithms proposed analyzed years including relaxation methods alma variants thereof 
line algorithms perceptron mistake bounds coincide achieved standard perceptron algorithm 
superscript denotes transposition 
sgn denotes signum function sgn ifz sgn conventionally give positive sign zero 
cesa bianchi gentile yt sgn xt 
say algorithm mistake trial prediction yt label yt disagree 
case algorithm updates internal state simple additive rule wt wt yt xt moving old weight vector direction instance xt algorithm turned wrong right orientation determined yt yt 
yt yt weight update number mistakes equal number weight updates 
sequence examples linearly separable instance vectors xt consistently labeled lie positive yt negative yt side unknown target hyperplane normal vector passing origin 
known perceptron convergence theorem states perceptron algorithm guaranteed mistakes number examples linearly separable sequence max xs min xt implies cycling sequence linearly separable examples eventually lead perceptron algorithm compute weight vector classifying examples correctly yt sgn xt sequence 
convergence speed critically influenced degree linear separability examples expressed ratio vectors xt small projection component orthogonal large norm xt hardest ones perceptron algorithm 
situation illustrated 
consider case algorithm observes instance vectors xt lying dotted circles opposite sides 
vectors small components direction intuitively irrelevant target vector 
xt significantly mislead perceptron algorithm slowing convergence 
trial depicted mistaken trial algorithm sgn xt sgn xt yt 
new weight vector wt computed algorithm larger projection old vector wt 
algorithm prediction direction current weight vector 
step wt wt satisfactory progress turn algorithm related order second order perceptron algorithm described section 
call algorithm whitened perceptron algorithm 
strictly speaking incremental algorithm 
fact assumes instance vectors xt available labels yt hidden 
sake simplicity assume vectors span cor relation matrix xt full rank exists 
positive definite exists see chap 

whitened perceptron algorithm simply standard perceptron algorithm run transformed whitened sequence xt yt 
transformation called whitening transform see simplest form perceptron convergence theorem exploits measure progress wt fact linear separability assumptions measure steadily increases mistaken trials 
wt second order perceptron algorithm wt fig 

behavior perceptron algorithm extreme linearly separable cases 
denotes hidden target vector wt weight vector maintained algorithm trial xt instance vector observed trial 
assuming instances bounded euclidean length examples linearly separable margin vector sequence examples lie dotted lines running parallel decision boundary 
angle xt slightly larger degrees label yt assigned value 
hand xt holds algorithm mistake 
xt lies exactly dotted lines maximal length small projection direction meaning direction marked xt irrelevant simple additive rule perceptron algorithm new weight vector wt farther old 
effect reducing correlation matrix transformed instances identity matrix 
fact xt xt xt xt mm 
sake simplicity suppose original sequence examples linearly separable mint yt xt unit norm vector whitened sequence linearly separable 
fact hyperplane separates whitened sequence margin 
aforementioned convergence theorem number mistakes perceptron algorithm whitened sequence max xt max xt mu appreciate potential advantage whitening data note instance vectors xt correlated quadratic form xt tends cesa bianchi gentile wt pos 
example neg 
example fig 

scattering data whitened second order perceptron algorithm take advantage 
instance vectors lie flat ellipsoid enclosed ball radius examples linearly separable margin hyperplane normal vector aligned small axis ellipse 
instance vector xt projection xt small smaller 
difference standard perceptron algorithm worst case behavior essentially ruled norm instances lying dotted circles recall 
quite small expression maxt xt xt regarded measure correlation instance vectors 
instances look displayed separating hyperplane vector strongly correlated nondominant eigenvector instances small projected component bound right hand side significantly smaller corresponding mistake bound maxt xt classical perceptron algorithm 
sake clarity consider degenerate case data points evenly spread parallel lines margin ellipse sketched maximally squashed direction 
hard argue symmetric scattering data leads matrix min max minimal maximal eigenvalues respectively 
eigenvector associated min aligned second eigenvector associated max orthogonal parallel lines mentioned 
denote unit norm vector orthogonal unit norm min mu xt xt min max xt see section second order perceptron algorithm write xt xt min xt max xt xt step singular value decomposition svd see appendix 
combining simplifying conclude extreme case sketched number mistakes whitened perceptron algorithm bounded max xt mu xt note bound approaches norm instance vectors xt approaches just hardest case standard perceptron algorithm 
case standard perceptron bound whitened perceptron bound tends constant independent margin radius ball containing data 
learning model notation 
part section precisely describe learning model introduce notation notation earlier 
formal model consider known mistake bound model incremental learning introduced littlestone see investigated authors see :10.1.1.130.9013:10.1.1.37.1595
incremental sequential models learning proceeds trials 
trial algorithm observes instance vector xt rn vectors understood column vectors guesses binary label yt 
seeing vector xt true label yt associated xt revealed algorithm knows guess yt yt correct 
case say algorithm mistake call mistaken trial 
pair xt yt call example sequence examples sequence xt yt 
assumptions mechanism generating sequence examples 
similarly competitive analyses line algorithms goal learning algorithm minimize amount total number mistakes arbitrary sequence exceeds measure performance fixed classifier comparison class sequence comparison class consider set linear threshold classifiers parametrized unit norm vectors rn 
speak linear threshold classifier mean classifier sgn 
technical reasons number mistakes learning algorithm compared cumulative hinge loss see best linear threshold classifier comparison class 
hinge loss linear threshold classifier example margin max 
sequence examples xt yt xt yt 
note strict upper bound number mistakes sequence linearly separates margin yt xt 
prove bounds number mistakes having general form number mistakes inf inf spectral cesa bianchi gentile sequence examples spectral measures certain spectral properties arising interaction mistake bound shown reveals algorithm able optimally trade terms spectral single pass arbitrary data sequence 
aggressive adaptation successfully exploited settings different mistake bound model 
fact mentioned linear threshold classifiers generated second order perceptron algorithm easily shown probability mistake risk statistical model pattern classification see tightly related algorithm mistake bound 
risk bounds best achievable algorithm learning linear threshold classifiers 
rest organized follows 
section describes analyzes basic form second order perceptron algorithm shows formulate algorithm dual form allowing kernel functions 
section analyze variants basic algorithm variant adaptive parameter second variant computes pseudoinverse correlation matrix standard inverse 
toy experiments reported section 
section contains final remarks open problems 

second order perceptron basic form 
second order perceptron algorithm viewed incremental variant whitened perceptron algorithm means instances data sequence assumed known 
second order perceptron algorithm sparse whitening transform applied new incoming instance possibly small subset instances observed far 
basic form second order perceptron algorithm described takes input parameter 
compute prediction trial algorithm uses row matrix xk dimensional weight vector vk subscript indicates number times matrix vector updated trials 
initially algorithm sets empty matrix 
receiving tth instance xt algorithm builds augmented matrix st xk xt xt intended column st ain st xt predicts label yt xt yt sgn identity matrix addition guarantees inverse exists 
yt yt mistake occurs algorithm updates vk xk 
vector vk updated perceptron rule vk vk yt xt matrix xk updated xk st xk xt 
note update implies xk xt new matrix xk new vector vk prediction 
yt yt update takes place algorithm mistake driven :10.1.1.130.9013
just update matrix xk columns number mistakes algorithm far 
note perceptron algorithm wt depend xt st 
second order perceptron algorithm described linear threshold predictor 
algorithm viewed adaptation line binary classification ridge regression method 
analysis inspired analysis variant ridge regression introduced vovk studied warmuth forster warmuth 
variant instance vovk general aggregating algorithm called forward algorithm 
degenerate case xt update performed 
second order perceptron algorithm parameter 
initialization 
repeat 
get instance xt rn 
set st xk xt 
predict yt sgn xt wt ain st vk 
get label yt 
yt yt vk vk yt xt xk st 
fig 

second order perceptron algorithm parameter 
algorithm forward algorithm predict weight vector wt product inverse correlation matrix past trials linear combination past instance vectors 
cases current instance xt incorporated current correlation matrix prediction 
forward algorithm keep track past trials algorithm mistake 
complete qualitative analogy note analysis algorithm uses tools developed forward algorithm 
reader wonder nonlinear dependence current instance necessary 
matter fact predicted xk augmented matrix st resulting algorithm linear threshold algorithm 
easy show margin value yt xt achieved algorithm equal margin value algorithm multiplied positive quantity depending instances observed case weight vector wt computed algorithm run sequence examples xt yt defined wt argmin mt xs ys mt set trials ys xs 
way writing update rule shows version second order perceptron algorithm viewed sparse variant line ridge regression algorithm prediction rule yt sgn xt weight vector wt just wt argmin xs ys approach binary classification taken called regularized squares classification method 
reader want check approach having similar flavor 
comparison stresses role played sparsity algorithm efficient store potentially small submatrix data correlation matrix algorithm significantly efficient easy sequences mistakes 
second mistake driven property causes sparsity key feature deriving spectral bounds number mistakes hold arbitrary data sequences bounds currently known 
cesa bianchi gentile far 
linear regression framework considered binary classification inclusion current instance difference running algorithms sequence examples produce sequence predictions 
true second order perceptron algorithm pseudoinverse described section 
algorithms figures equivalently viewed generators linear threshold classifiers 
reason formulate algorithms directly way threefold 
equivalence hold general parameter changes time section 
order maintain uniform style exposition preferred keep difference figures little possible 
second including current instance prediction naturally suggested way analyzed algorithms 
third scenarios margin plays crucial role information filtering problems studied including current instance margin computation give slight improvement performance 

analysis 
claim theoretical properties algorithm 
theorem proved appendix theorem 
number mistakes second order perceptron algorithm run finite sequence 
examples satisfies inf min xm mu ln eigenvalues xm remarks order 
observe quantity xm mu bound lies mini maxi particular xm mu aligned eigenvector associated fact entails trade hinge loss term square root term bound 
second larger gets similar warping matrix ain st diagonal matrix 
fact reader see limit second order perceptron algorithm classical perceptron algorithm bound theorem takes form inf min trace matrix equals sum eigenvalues nonzero eigenvalues xm coincide nonzero eigenvalues xm im trace xm see xt maxt xt 
set indices mistaken trials 
setting maxt xt solve resulting bound gives inf min perceptron bound general nonseparable case 
shows certain sense second order perceptron algorithm strictly generalizes second order perceptron algorithm classical perceptron algorithm introducing additional parameter general larger algorithm resembles perceptron algorithm 
third linearly separable case mistake bound perceptron algorithm bound determined aforementioned trace inequality second order algorithm hand bound xm mu ln determined core spectral quantity xm mu ln 
quick comparison bounds suggests data sequences linearly separable hyperplanes normal vectors nearly aligned eigenvectors having small eigenvalues advantageous second order perceptron algorithm see 
precise quantitative comparison bounds go follows 
note order carry comparison need second order dependence eigenvalues introduce notation xm mu 
ln max ln na ln maximum achieved equal finding conditions data second order algorithm advantageous reduced finding conditions ln satisfied 
lemma proved appendix simple derivative argument shows strict inequality 
setting smaller compared smaller left hand side compared right hand side smaller second order bound compared order 
expected data tend flat irrelevant components direction large instance vectors convenient pick small value particular small setting independent 
hand lemma shows satisfied equality 
small best thing resort order algorithm 
lemma 
ln 
hold 
lima 
setting gets rand lim 
observe running time trial second order algorithm known sherman morrison formula see chap 
vector positive definite matrix xx 
cesa bianchi gentile formula allows perform matrix inversion incrementally 
particular positive definite matrix ain st equals ain xk xtx compute time dimensional vector ain xk xt sherman morrison formula obtain ain st time 

algorithm dual variables kernel functions 
section show second order perceptron algorithm equivalently formulated dual variables 
formulation allows run algorithm efficiently reproducing kernel hilbert space 
consequence able derive kernel version theorem mistake bound expressed terms eigenvalues kernel gram matrix 
recall kernel function see nonnegative function rn rn satisfying xi xj xm rn functions called positive definite 
kernel define linear space vk xi xi norm defined xi xj space completed adding limit points sequences vk convergent norm resulting space denoted hk called reproducing kernel hilbert space induced kernel 
classical examples kernel functions include called polynomial kernel positive integer gaussian kernel exp 
practice algorithm depending instance vectors xi inner products xj turned general kernel version just replacing standard inner products xj kernel inner products xi xj 
theorem slight modification dual formulation ridge regression algorithm derived 
theorem 
notation component vector components labels yi algorithm mistake trial component 
xt ain st xt aik gt xt gt st gram matrix ik dimensional identity matrix 
proof 
recalling vk st implies ain st ain st need prove second order perceptron algorithm ain st aik gt holds 
follows part exercise chap 

computational standpoint update rule algorithm exploit known adjustment formula partitioned matrices see chap 

formula relates inverse matrix inverses submatrices 
simple case positive definite matrix dimensional column vector scalar wehave vv computed multiplications 
notation theorem easy see aik gt aik xk xt xk xt formula inverse partitioned matrices follows dimensional matrix aik gt computed incrementally dimensional matrix aik xk extra dot products 
sweeping sequence examples needs dot products upper bounded theorem 
result kernel version theorem 
hinge loss function hk defined max yf 
corollary 
number mistakes dual second order perceptron algorithm kernel run finite sequence 
examples satisfies inf min xt ln numbers nonzero eigenvalues kernel gram matrix entries xi xj mand set indices mistaken trials 
considerations similar statement theorem apply 
note number nonzero eigenvalues kernel gram matrix general equal nature kernel functions dimension space hk large possibly infinite kernel gram matrix full rank 
note linear kernel xi xj xj corollary mistake bound theorem recovered exactly 
see observe xm mu xt nonzero eigenvalues matrix xm coincide nonzero eigenvalues gram matrix mxm 
quantity called schur complement augmented matrix matrix positive definiteness chap 

respect follows see cesa bianchi gentile 
getting rid parameter major drawback algorithm described input parameter fixed ahead time 
turns practical application value parameter significantly affects performance 
instance bad choice second order perceptron algorithm similar order cases perform far better 
section analyze variants basic algorithm variant section adaptive parameter version algorithm second variant section eliminates need trade parameter replacing warping matrix ain st matrix st pseudoinverse st 
second order perceptron adaptive parameter 
section refine arguments section motivating analyzing adaptive parameter version algorithm 
ideally resulting algorithm able learn fly best data sequence value minimizes bound theorem 
optimal clearly depends unknown quantities spectral structure data unknown target ideal choice able automatically turn second order algorithm order actual scattering data say spherical symmetry 
typical data second order algorithm take advantage 
argument note value theorem affects bound quantity ln turn depend examples algorithm mistake 
reasonable change mistaken trials keeps algorithm mistake driven 
second order algorithm adaptive parameter described 
algorithm feed algorithm increasing sequence parameter values ak indexed current number mistakes algorithm analyzed theorem 
proof theorem appendix reader see strictly increasing sequence ak results bound number mistakes 
theorem picked sequence grows linearly choice best simple upper bounding argument 
consider 
noted section ln na maxt xt 
cauchy schwarz inequality gets xt 
hand data sequence linearly separable margin xt 
interesting cases linear glance allows conclude viewed function right hand side grow linearly 
minimal growth speed achieved linear function reader note grows faster linearly linear resulting multiplicative constants larger 
second order perceptron algorithm parameter sequence ak ak ak 
initialization 
repeat 
get instance xt rn 
set st xk xt 
predict yt sgn xt wt akin st vk 
get label yt 
yt yt vk vk yt xt xk st 
fig 

second order perceptron algorithm increasing parameter sequence ak 
important point qualitative argument depend number nonzero eigenvalues matrix xm example change instances lived small subspace rn alternatively mapped instances high dimensional feature space kernel function see discussion corollary 
theorem uses ak cr small positive constant 
tuning captures right order growth ak multiplicative constant best choice depends unknown quantities 
theorem 
second order perceptron algorithm run parameter sequence ak cr finite sequence 
examples xt total number mistakes satisfies inf min am eigenvalues xm am cr ln ln am see algorithm adaptive parameter advantageous fixed parameter recall tuning argument 
shown second order algorithm able exploit spectral properties data small case tuning small values sake concreteness focus linearly separable case 
observed case minimal margin data 
emphasize tuning matches mentioned theorem scaling factor 
particular comparing bounds presence scaling factor simplifies analysis appendix cesa bianchi gentile theorems see theorem replaces am cr function having linear dependence price pay substitution additive term mild logarithmic dependence resulting bound inconvenient implicit form 
upper bound explicit solution clearly computed calculations get overly complicated add new insights heart matter 
feel justified omitting analytical detail 
section derive dual formulation algorithm prove result similar contained corollary 
general total number mistakes algorithm conveys relevant information specific dataset hand 
parameter scales number allows partially exploit information 
note instance second order algorithm making mistakes small algorithm tends behave order algorithm ak growing fast words view algorithm able detect second order structure suitable data processing trying turn order algorithm 
hand algorithm making mistakes ak tends remain small meaning current second order structure algorithm appears right dataset hand 
approach parameter tuning similar self confident tuning adopted 
note algorithm fixed incremental matrix inversion looks bit troublesome 
fact making change trial trial results matrix update longer low rank adjustment 
algorithm dual version update rule requiring quadratic number operations trial 

second order perceptron pseudoinverse 
radical way dealing trade parameter set zero replace inverse ain sts pseudoinverse sts done algorithm 
matrix sts exist coincides sts case sts nonsingular 
appendix collected relevant information connection svd 
classical learn 
result proved appendix theorem 
second order perceptron algorithm run finite sequence 
examples xt total number mistakes satisfies inf min ln rank xm minimum smallest positive eigenvalues matrices xk produced algorithm run 
inequality depends lower bound positive eigenvalues correlation matrices produced algorithm 
sense dependence substitutes dependence bound theorem 
fact adding matrix ain second order perceptron algorithm initialization 
repeat 
get instance xt rn 
set st xk xt 
predict yt sgn xt wt st vk 
get label yt 
yt yt vk vk yt xt xk st 
fig 

second order perceptron algorithm pseudoinverse 
current correlation matrix data viewed way setting lower bound positive eigenvalues resulting matrix 
observed section data linearly separable margin quantity linear set simplify bound takes form ln theorem bounds implicit form respect bounds explicit cost adding logarithmic terms 
decided carry calculations insightful 
comparing bounds theorem see second order algorithm pseudoinverse advantageous basic version effective dimension data relatively small 
bounds algorithm hard turn algorithm equivalent dual form 
exploit known methods incrementally compute pseudoinverse see called method reducing computational cost trial cubic quadratic 
unfortunately bounds generally useful presence kernel functions bounds linear dependence avoided worst case 
due simple fact time new instance xt lies outside column space previous matrix xk pseudoinverse st maps xt orthogonal space column space algorithm degenerate margin xt 
simulations toy problem 
purpose empirically verifying theoretical results ran second order perceptron algorithm sets kernel case equal giving rise vacuous bound 
note quadratic dependence essentially fictitious factor occurs numerator denominator inside logarithm 
cesa bianchi gentile fig 

projection relevant coordinates dimensional datasets simulations 
table algorithm mistakes st dataset mistakes nd dataset perceptron second order perceptron second order perceptron second order perceptron linearly separable data attributes 
datasets generated sampling gaussian distribution correlation matrix single dominant eigenvalue remaining eigenvalues having size sample realizations dominant eigenvalue times bigger 
dataset leftmost plot labels assigned separating hyperplane orthogonal eigenvector associated dominant eigenvalue 
second dataset rightmost plot labels assigned separating hyperplane orthogonal eigenvector associated nondominant eigenvalue natural ordering coordinates 
remarks theorem expect perceptron algorithm perform similarly datasets radius enclosing ball margin change datasets second order perceptron algorithm expected outperform perceptron algorithm second dataset 
conjecture supported results table 
results obtained running perceptron algorithm second order perceptron algorithm different values parameter epochs training set examples saving final classifier generated algorithm training mistake 
numbers table average number mistakes classifiers test set examples averages computed random permutations training set standard deviations shown parentheses 
normalizing instances alter significantly perceptron performance 
offer familiar context reader experienced empirical comparisons learning algorithms predictive performance experiments evaluated standard test error measure 
drawn test error number mistakes algorithms training 
quantity bound theoretical results different standard training error line model new mistake different classifier 
theory accounting second order perceptron algorithm relationship fraction mistakes line model test error developed 

open problems 
introduced second order perceptron algorithm new line binary classification algorithm learning functions 
algorithm able exploit certain spectral properties data sequence expressed interaction underlying target vector eigenvalues correlation matrix data 
second order perceptron algorithm retains standard perceptron key properties sparsity efficient dual variable representation 
allows efficiently run algorithm reproducing kernel hilbert space 
proved algorithm best known mistake bound efficient kernel classifiers date 
performance algorithm critically influenced input parameter optimal setting depends training set developed variants basic algorithm 
variant increases parameter function number mistakes 
second variant avoids altogether parameter replacing inverse correlation matrix ai xx pseudoinverse xx run simple experiment synthetic data give evidence theoretical properties algorithm basic form 
second order perceptron algorithm seen new line classification technique 
technique combined previous techniques shifting target technique approximate line large margin technique see 
shown analysis derive linear threshold classifiers statistical risk bounded probability quantities directly related mistake bounds theorems 
resulting data dependent generalization bounds similar spirit readily comparable bounds thm 

fact results derived involved covering numbers arguments terms correlation matrix xx sequence examples 
precisely results expressed terms large eigenvalues xx taken decreasing order magnitude effective number dimensions contrast bounds terms eigenvalues submatrix xx instances algorithm mistake 
sense sparsity solution produced algorithm directly reflected magnitude eigenvalues submatrix 
see go back primal variable form theorem observe adding rank matrices type xx correlation matrix xx results new correlation matrix eigenvalues larger 
mistakes equivalent high sparsity turn equivalent small eigenvalues 
observe application mistake bounds statistical learning setting straightforward whitened perceptron algorithm described section 
whitening matrix depends training data whitening transformation preserve stochastic independence whitened instance vectors xt directions extended 
briefly mention 
effective number dimensions depends instance margin data 
cesa bianchi gentile possible refine analysis second order perceptron algorithm adaptive parameter theorem 
linear growth ak certainly step line parameter adaptation somewhat unsatisfactory leaves leading coefficient ak unspecified 
aware incremental updating scheme algorithm primal dual form 
second possible refine analysis second order perceptron pseudoinverse theorem eliminate dependence third combine second order classification technology dual norm algorithms norm algorithms winnow weighted majority algorithms 
probably give rise new attractive algorithms learning sparse linear threshold functions 
appendix proof theorem 
fix arbitrary finite sequence 
set trials algorithm mistake 
ain ak ain xk study evolution vk mistaken trials 
tk trial kth mistake occurred 
vk vk yt xt vk yt xt yt yt implies vk vk yt xt vk yt vk xt xt vk xt xt yt yt implies xk st vk wt vk xt yt yt implies yt xt 
note ak recursively defined ak ak xtx applying sherman morrison formula get vk vk inequality holds vk xt xt vk inverse positive definite matrix positive definite 
get vk vk xt holding 
summing inequality equivalently yields ma vm xt det ak det ak lemma lemma appendix ln det ak ln det ak second order perceptron algorithm ln det am det ln det ain det ain ln eigenvalues xm conclude proof note exist am positive definite see chap 

pick unit norm vector cauchy schwarz inequality ma vm vm vm mu mu xm mu xm mu inequality follows definition holds 
putting solving gives statement theorem 
appendix proof lemma 
check limiting behavior function fixed 
prove remainder part note sign ar follows choice exists negative 
case viewed function convex decreasing 
minimal value achieved asymptotically 
finite value exists minimizes computing minimizing analytically easy 
resort approximation value vanishes changes concavity 
prove part set ln suffices show lim 
statement proved showing lim derivative ln smaller derivative omit details easy calculations 
second statement trivially follows ln 
cesa bianchi gentile appendix proof theorem 
proof refined version proof theorem proceeds lines 
assume kth mistake occurs processing example xt yt tk 
set trials mistake occurred 
ak akin xk 
ak ak ak ak xtx 
study evolution quantity vk mistaken trials 
proof theorem vk vk xt 
need bound vk 
see matrix ak sum nonsingular matrices ak ak ak xt nonsingular ak ak 
computing done general inversion formula ak ak ak xt positive definite matrix easily follows fact inverse positive definite matrix positive definite sum product positive definite matrices give rise positive definite matrices 
write vk vk vk vk vk 
hand inequality trivially true 
inequality holds 
plug back sum take account 
obtain ma vm xt det ak ak ak det ak applying lemma lemma appendix det ak ln det ak ak ak presence matrix term ak ak rest proof bit involved proof theorem 
want obtain bounds terms eigenvalues matrices rewrite rightmost side function eigenvalues matrices xk ith eigenvalue matrix xk 
write ln det ak det ak ak ak ln ak ak simple manipulations yield ln ak ak second order perceptron algorithm ak ln ak ak ln ak am ln obtained ma vm am ak ln ak ak ln ak ak ak ak ak ak ak recall ln am ln am reader see analogous presence spurious double sum term 
turn upper bounding double sum function proceed follows 
note inner sum zero 
continue assuming inner sum 
xk size rank min eigenvalues nonzero nonzero 
observed xk eigenvalues gram matrix xk 
trace matrix equals sum eigenvalues trace xk ak ln ak max ak ln ak ak ak ak ak min 
maximum achieved equal ak ln ak ak ak ak ak ln ak ak ak ak ak ln ak ak ak derivative argument shows function cesa bianchi gentile ln increasing 
see write ak ln ak ak ak ak ak ln ak ak ak ak ak ak ak ak ak ak ak ak ak ak ln ak ln ak ak cr ln ak ak cr cr ck ck ck dx ln 
ln bound ma vm proceed proof theorem yielding ma vm am xm mu am cr plug lower bound back previous upper bound 
rearranging obtain am xm mu ln am solving occurring numerator left hand side gives desired bound 
appendix pseudoinverse svd matrix 
section recall basic facts pseudoinverse svd matrix 
background material 
exploit facts prove technical lemmas subsequent sections 
lemmas build ancillary results proven 
second order perceptron algorithm pseudoinverse real matrix unique matrix satisfying conditions aa aa aa aa 
matrix nonsingular pseudoinverse coincides usual inverse order state properties pseudoinverse need exploit known connection pseudoinverse matrix svd 
follows focus symmetric positive semidefinite matrices 
allows simplify exposition replace notion singular value familiar notion eigenvalue 
rank svd matrix takes form orthogonal matrix uu diagonal matrix diagonal entries positive eigenvalues remaining entries zero 
columns form orthonormal basis span space spanned columns remaining columns form orthonormal basis orthogonal complement span 
recall case consideration span coincides null null space convenient form svd eigenvectors corresponding positive eigenvalues shown 
ur matrix columns dr diagonal matrix diagonal entries positive eigenvalues immediately sees finer svd holds ur dr positive semidefinite matrix admits svd 
svd matrix computing pseudoinverse simple matter 
particular easily show ur inverse nonsingular matrix dr properties relevant derived 
instance immediately see viewed linear transformations aa identical transformation span null transformation span aa span span 
property easily follows matrix identity aa ur ready prove lemma generalizes valuable lemma positive semidefinite matrices 
cesa bianchi gentile lemma 
arbitrary positive semidefinite matrix arbitrary vector rn xx span det det span det denotes product nonzero eigenvalues matrix note det det nonsingular 
proof 
lemma trivially verified 
continue assuming 
prove span 
lemma follows ba span 
write ba xx xx implies 
hand span apply sherman morrison formula conclude xx 
continue assuming span 
svd sense 
ur write xx symmetric matrix dr xx ur 
claim nonzero eigenvalues det det 
nonzero eigenvalues ek corresponding orthonormal eigenvectors 
easy verify eigenvalues corresponding orthonormal eigenvectors ure ure 
ek orthonormal matrix columns ek svd span 
bv equality follows orthogonality 
null rank rank nonzero eigenvalues 
computing nonzero eigenvalues fairly straightforward 
dr xx ur dr ir xx ur 
det dr det dr det easy verify direct inspection matrix ir xx ur eigenvalue multiplicity corre sponding eigenvectors forming orthogonal basis span eigenvalue urd corresponding eigenvector span know 
obtained det det det dr det ir xx ur det 
rearranging gives desired result 
second order perceptron algorithm lemma 
arbitrary positive semidefinite matrix 
set xx span 
span proof 
ba fact span xx aa xx xx fact span span lemma 
appendix proof theorem 
proof similar proof theorem 

set trials algorithm mistake ak xk 
investigate quadratic form vk changes mistaken trials 
suppose kth mistake occurred trial tk 
proceeding proof theorem show vk vk xt 
proof theorem xt span ak apply sherman morrison formula obtain vk vk 
hand case xt span ak apply lemma conclude vk vk 
cases vk vk 
combining obtain vk vk xt holding sum 
get ma mvm xt 
order upper bound right hand side proceed follows 
separate mistaken trials xt span ak trials rank ak rank ak mistaken trials xt span ak trials rank ak rank ak 
applying lemma count det ak det ak kind trials second kind 
upper bound worst possible case arranging kinds trials sequence account number trials second kind equal rank am 
assume general scenario rank ak ir ir ir cesa bianchi gentile ir notation right hand side rewritten follows follows assume xt xt xt det ak det ak lemma det ak det ak ln det ak det ak det ln det 
proceed bounding logarithmic terms 
construction rank rank equal number nonzero eigenvalues matrices 
denote positive eigenvalues positive semidefinite positive eigenvalues expressed nonnegative values number rank matrices form xt added order obtain ln det det ln ln max ln recall ln maximum achieved 
plug back maximize dr xt ln max dr ln second order perceptron algorithm ln maximum achieved required upper bound right hand side 
span am bound ma mvm proof theorem 
yields ma mvm xm mu plugging back combining gives xm ln mu solving occurring numerator left hand side gives 
hand span am wehave xm mu vacuously verified equality 
acknowledgments 
bob williamson fruitful conversations anonymous reviewers comments turned useful improving presentation 
aizerman braverman theoretical foundations potential function method pattern recognition learning autom 
remote control pp 

angluin queries concept learning machine learning pp 

auer cesa bianchi gentile adaptive self confident line learning algorithms comput 
system sci pp 

auer warmuth tracking best disjunction machine learning pp 

warmuth relative loss bounds line density estimation exponential family distributions machine learning pp 

ben israel generalized inverses theory applications john wiley sons new york 
block perceptron model brain functioning rev modern phys pp 

borodin el yaniv online computation competitive analysis cambridge university press cambridge uk 
cesa bianchi gentile generalization ability line learning algorithms ieee trans 
inform 
theory pp 

cesa bianchi freund helmbold haussler schapire warmuth expert advice acm pp 

cesa bianchi freund helmbold warmuth line prediction conversion strategies machine learning pp 

cristianini shawe taylor support vector machines cambridge university press cambridge uk 
devroye gy rfi lugosi probabilistic theory pattern recognition springer verlag new york 
duda hart stork pattern classification john wiley sons new york 
forster warmuth relative expected instantaneous loss bounds comput 
system sci pp 

cesa bianchi gentile forster warmuth relative loss bounds temporal difference learning machine learning pp 

gentile new approximate maximal margin classification algorithm machine learning res pp 

gentile robustness norm algorithms machine learning pp 

gentile warmuth linear hinge loss average margin proceedings conference advances neural information processing systems mit press cambridge ma pp 

grove littlestone schuurmans general convergence results linear discriminant updates machine learning pp 

herbster warmuth tracking best expert machine learning pp 

herbster warmuth tracking best linear predictor machine learning res pp 

ridge regression biased estimation nonorthogonal problems technometrics pp 

horn johnson matrix analysis cambridge university press cambridge uk 
kivinen warmuth relative loss bounds multidimensional regression problems machine learning pp 

kivinen warmuth auer perceptron algorithm vs winnow linear vs logarithmic mistake bounds input variables relevant artificial intelligence pp 

li long relaxed online maximum margin algorithm machine learning pp 

littlestone learning quickly irrelevant attributes abound new linear threshold algorithm machine learning pp :10.1.1.130.9013

littlestone redundant noisy attributes attribute errors linear threshold learning winnow proceedings th annual workshop computational learning theory morgan kaufmann san mateo ca pp 

littlestone warmuth weighted majority algorithm inform 
comput pp 

marcus minc linear algebra dover new york 
convergence proofs perceptrons proceedings symposium mathematical theory automata vol 
xii polytechnic institute brooklyn pp 

rifkin yeo poggio regularized squares classification advances learning theory methods model applications nato sci 
ser 
iii comput 
systems sci 
ios press amsterdam pp 

rosenblatt perceptron probabilistic model information storage organization brain psych 
rev pp 

saunders gammerman vovk ridge regression learning algorithm dual variables proceedings th international conference machine learning morgan kaufmann san francisco ca pp 

sch lkopf smola learning kernels mit press cambridge ma 
suykens van de de moor vandewalle squares support vector machines world scientific singapore 
vapnik statistical learning theory john wiley sons new york 
vovk aggregating strategies proceedings rd annual workshop computational learning theory morgan kaufmann san mateo ca pp 

vovk competitive line statistics internat 
statist 
rev pp 

williamson smola sch lkopf generalization bounds regularization networks support vector machines entropy numbers compact operators ieee trans 
inform 
theory pp 

