maximum entropy models natural language ambiguity resolution adwait ratnaparkhi dissertation computer information science faculties university partial ful llment requirements degree doctor philosophy professor mitch marcus supervisor dissertation professor mark steedman graduate group chair copyright adwait ratnaparkhi best aspect research environment opinion abundance bright people argue discuss ideas 
people penn feedback helped separate ideas bad ideas 
hope kept ideas thesis left bad ideas 
iwould people contribution education advisor mitch marcus gave intellectual freedom pursue believed best way approach natural language processing gave direction necessary 
mitch fascinating conversations personal professional years penn thesis committee members john la erty carnegie mellon university aravind joshi lyle ungar mark liberman extremely valuable suggestions comments thesis research 
mike collins jason eisner dan melamed stimulating impromptu discussions linc lab 
gratitude valuable feedback rough drafts papers thesis chapters 
breck baldwin paola merlo tom morton martha palmer je reynar anoop sarkar bangalore srinivas ircs interesting discussions natural language extended scope thesis 
especially je reynar pivotal role comprises chapter thesis 
iii past current kyle hart karin dimitris william schuler fei xia enjoyable discussions 
owe great debt salim roukos todd ward members speech recognition group ibm tj watson research center introducing maximum entropy modeling technology statistical natural language processing general 
employment programmer satisfying intellectual experience 
family role education 
parents brother sacri ces nance undergraduate education princeton 
parents teaching value tion young age 
wife amy steady support encouragement love di cult times graduate career 
owe success essential things family years 
iv maximum entropy models natural language ambiguity resolution adwait ratnaparkhi supervisor professor mitch marcus thesis demonstrates important kinds natural language ambiguities resolved state art accuracies single statistical modeling technique principle maximum entropy 
discuss problems sentence boundary detection part speech tagging phrase attachment natural language parsing text categorization maximum entropy framework 
practice maximum entropy models er advantages state art accuracy probability models tasks discussed form near state art accuracies outperform competing learning algo rithms trained tested similar conditions 
methods outperform require supervision form additional human involvement additional supporting resources 
knowledge poor features facts model data features linguis simple knowledge poor succeed approximating complex linguistic relationships 
reusable software technology mathematics maximum entropy framework essentially independent ofany particular task single software implemen tation probability models thesis 
experiments thesis suggest experimenters obtain state art ac wide range natural language tasks little task speci ort maximum entropy probability models 
vi contents iii maximum entropy framework 
representing evidence 
machine learning corpus approach 
learning maximum likelihood estimation exponential models learning maximum entropy framework 
maximum entropy simple example 
conditional maximum entropy models 
relationship maximum likelihood 
parameter estimation 
computation 
discussion 
special case non overlapping features 

machine learning techniques applied natural language brief review naive bayes 
vii statistical decision trees 
transformation learning 
decision lists 
interpolation 
decomposable models 
logistic regression models 

sentence boundary detection 
previous 
maximum entropy models sentence boundary identi cation 
outcomes 
contextual predicates 
feature selection decision rule 
system performance 

acknowledgments 
part speech tag assignment 
probability model 
features pos tagging 
contextual predicates 
feature selection 
testing model 
search algorithm 
experiments wall st journal 
specialized features consistency 
experiments corpora 
comparison previous 
viii 
parsing 
previous 
parsing maximum entropy models 
actions parser 
maximum entropy probability model 
search 
experiments 
dependency evaluation 
portability 
reranking top 
comparison previous 

unsupervised prepositional phrase attachment 
previous 
unsupervised prepositional phrase attachment 
generating training data raw text 
accuracy extraction heuristic 
statistical models 
generate 
generate 
generate 
experiments english 
experiments spanish 
creating test set 
performance spanish data 
discussion 
ix 
acknowledgments 
experimental comparison feature selection decision tree learn ing 
maximum entropy feature selection 
decision tree learning 
prepositional phrase attachment 
text categorization 

limitations maximum entropy framework convergence problems 
exact solution exists parameters converge 
exact solution exist parameters diverge 
parameter interaction 
smoothing 
binary valued features 

accuracy 
knowledge poor feature sets 
sentence detection 
part speech tagging 
parsing 
unsupervised prepositional phrase attachment 
count cuto 
software re usability 
discussion 

relevant proofs non overlapping features 
maximum likelihood maximum entropy 
xi list tables task nd probability distribution constraints 
way satisfy constraints 
uncertain way satisfy constraints 
best performance corpora 
performance corpora highly portable system 
performance wall street journal test data function training set size systems 
contextual predicates context bi 
sample data 
contextual predicates generated tagging table contextual predicates generated tagging table 
tagger search procedure 
wsj data sizes 
baseline performance development set 
top tagging mistakes training set baseline model 
performance baseline model specialized features 
errors development set baseline specialized models 
performance baseline specialized model tested consistent subset development set 
xii performance lob corpus crater corpus baseline feature set 
performance specialized model unseen test data 
tree building procedures parser 
comparison build check operations shift reduce parser 
contextual information probability models possible speci contexts speci context includes word include head word current tree th tree 
top bfs search heuristic 
speed accuracy randomly selected unseen sentences 
sizes training events actions features 
parse dependency syntax notation 
top dependency errors training set word 
top dependency errors training set part speech tag 
results sentences section words length wsj treebank 
evaluations marked ignore quotation marks 
evaluations marked collapse distinction advp prt ignore punctuation 
description training test sets 
portability experiments brown corpus 
see table train ing test sets 
obtain training data raw text 
frequent head word tuples 
frequent head word tuples 
accuracy unsupervised classi ers english 
accuracy unsupervised classi ers spanish 
proportions correct incorrect spanish data prepositions 
proportions correct incorrect spanish data con 
key probabilities ambiguous example rise num num 
xiii rst features selected ifs algorithm pp attachment 
maximum entropy decision tree dt experiments pp 
rst features selected ifs algorithm text categorization 
text categorization performance acq category 
summary maximum entropy models implemented thesis 
xiv list figures distribution tags word vs article 
distribution tags word vs annotator 
parse tree annotated head words 
initial sentence 
result pass 
result second pass 
result chunk detection 
application build join vp action 
proposed constituent shown 
application check action indicating proposed constituent gure complete 
build process tree marked 
encoding derivation contextual predicates 
observed running time top bfs section penn treebank wsj mhz ultrasparc processor mb ram sun ultra enterprise 
performance section function training data size 
axis represents random samples di erent sizes sections wall st journal corpus 
precision recall perfect reranking scheme top parses section wsj treebank function evaluation ignores quotation marks 
xv exact match perfect reranking scheme top parses section wsj treebank function evaluation ignores quotation marks 
test set performance function training set size 
accuracy pp attachment development set features added 
log likelihood pp attachment development set features added accuracy text categorization development set features added log likelihood text categorization development set features added xvi chapter thesis demonstrates single implementation statistical modeling technique principle maximum entropy conjunction knowledge poor informa tion sources su ces achieve state art performance tasks tremendous interest natural language processing community 
speci cally thesis discusses tasks sentence boundary detection part speech tagging prepositional phrase attachment parsing text categorization 
examples sentence boundary detection text fragment called white washington green responded 
computer program tell denote actual sentence boundaries part speech tagging sentences fruit ies banana 
time ies arrow 
words ies ambiguous 
rst sentence ies noun verb second sentence ies verb preposition 
computer program automatically accurately predict part speech ambiguous words ies prepositional phrase attachment sentences bought car credit card 
bought car 
computer program need know order know credit card refers bought refers car parsing natural language parser takes sentence input determines labelled syntactic tree structure corresponds interpretation sentence 
example di erent part speech assignments word ies likes lead di erent parse trees di erent interpretations np fruit ies np time vp np vp ies pp banana np arrow parsing requires resolution syntactic ambiguities arise interpretation sentence just noun verb ambiguities prepositional phrase attachments discussed earlier 
computer automatically predict plausible tree structures choose resolve structural ambiguities text categorization document topic task decide docu ment categorized topic 
document contains word money relevant topic mergers acquisitions computer best words documents predict topic task viewed classi cation problem objective estimate function cl 
maps object correct class typically prede ned set linguistic classes interested predicting consists words sentences textual material interest useful making prediction 
example sentence boundary detection potential sentence mark wish predict ftrue falseg classi es real spurious sentence boundary 
pos tagging word input sentence fall possible word wish predict sequence tags allowable pos tags word 
complex problems tagging parsing computationally convenient decompose sequence simpler classi cation problems 
example building classi er predict sequence tags simpler rst estimate classi er predicts pos tag single word apply times word 
likewise predicting parse tree predict sequence simpler actions time predict small part parse tree 
simpler classes predicted sequence classi er exploit previously completed classi cations order correctly predict nth class sequence 
exact details decomposing task problem speci art general methodology applicable complex linguistic prediction task 
classi cation functions tasks discussed thesis implemented maximum entropy probability models 
implement classi er cl 
conditional probability model simply choosing class highest conditional probability cl arg max yjx textual object class 
likewise probability models natu rally implement complex classi er cl sequence simpler probability calculations cl xn arg max ny xn yi yn xn yi informally called context history textual material available ith decision yi outcome ith decision 
maximum entropy framework probability class object depends solely features active pair feature de ned function maps pair 
features means experimenter feeds problem speci information maximum entropy model encode information useful correctly determining class 
importance feature determined automatically running parameter estimation algorithm pre classi ed set examples training set 
result experimenter need tell model information model automatically determine 
thesis provide experimental support claims regarding accuracy knowledge poor features reusability accuracy application maximum entropy modelling discussed accu racy near state art tuned models substantial task speci manner 
published results exceed require considerably domain expertise human ort part experimenter 
controlled experiments maximum entropy model implementation outperformed commercially available decision tree learning pack age 
knowledge poor features primary objective designing feature set maximize prediction accuracy feature sets thesis comparatively knowledge poor require deep linguistic knowledge ask elementary questions surrounding context 
feature sets thesis rely linguistic knowledge preprocessing semantic databases competing approaches easier specify easier port features approaches 
despite apparent simplicity features ectively approximate complex linguistic relationships particularly case parsing prepositional attachment tasks 
software reusability generality maximum entropy framework allows ex literally parameter estimation routine di erent tasks 
code parameter estimation essentially independent particular task single implementation su ces models thesis 
importantly maximum entropy models perform reasonably task despite fact tasks quite di erent nature complexity 
experimental results thesis suggest researchers re single implementation maximum entropy framework wide variety tasks expect perform state art accuracies 
chapter describes maximum entropy framework chapter discusses learn ing techniques natural language processing chapters discuss tasks sentence boundary detection part speech tagging parsing prepositional phrase attachment respectively 
chapter discusses comparative experiments feature selection learning techniques chapter discusses drawbacks technique chapter discusses 
chapter maximum entropy framework noted previous chapter problems natural language processing nlp re formulated classi cation problems task observe linguistic context band predict correct linguistic class 
involves constructing classi er cl turn implemented conditional probability distribution ajb probability class context contexts nlp tasks usually include words exact context depends nature task tasks context may consist just single word may consist words associated syntactic labels 
large text corpora usually contain information cooccurrence reliably specify ajb possible pairs words typically sparse 
challenge nd method partial evidence reliably estimate probability model maximum entropy probability models er clean way combine diverse pieces contextual evidence order estimate probability certain linguistic class occur ring certain linguistic context 
rst demonstrate represent evidence combine particular form probability model maximum likelihood frame discuss independently motivated interpretation probability model maximum entropy framework 
describe framework rst applies example problem applies nlp problems thesis 
discuss advantages combining evidence framework 
representing evidence represent evidence functions known contextual predicates features 
fa represents set possible classes interested predicting represents set possible contexts textual material observe contextual predicate function cp ftrue falseg returns true false corresponding presence absence useful information context history 
exact set contextual predicates cp cpm available varies problem problem supplied experimenter 
contextual predicates features functions form feature thesis form fcp ab cp true checks occurrence prediction contextual predicate cp 
actual set features problem determined feature selection strategy general speci problem 
show single feature selection strategy applied di erent problems thesis yields prediction accuracy 
feature thesis de ned machine learning literature usually de ned space possible contexts de nition feature borrowed past literature maximum entropy framework 
machine learning corpus approach thesis ts squarely called machine learning corpus approach natural language processing 
approach assume existence training sett bn set contexts bn annotated correct classes 
notion training set consists pairs boolean vectors contexts classes general vast number algorithms machine learning literature 
advantage conforming representation experimenters learning technique choice rigorous comparisons di erent learning techniques data 
learning maximum likelihood estimation exponential models way combine evidence weight features log linear exponential model ajb ky ky fj fj number features normalization factor ensure ajb 
parameter corresponds feature fj interpreted weight feature 
probability ajb normal ized product features active pair features fj fj 
weights probability distribution best training data obtained popular technique maximum likelihood estimation ajb log ajb arg max ky fj jg set models log linear form probability training sett conditional log likelihood training sett normalized number training events optimal probability distribution maximum likelihood criterion 
learning maximum entropy framework conceivably ways combine evidence form probabil ity distribution form independently motivated justi cation max imum entropy framework 
principle maximum entropy jaynes argues best probability model data maximizes entropy set probability distributions consistent evidence 
making inferences basis partial information probability distribution maximum entropy subject known 
unbiased assignment amount arbitrary assumption information 
rst illustrate maximum entropy modeling simple example de scribe framework applied natural language ambiguity problems thesis 
maximum entropy simple example example illustrates maximum entropy simple problem 
suppose task estimate joint probability distribution de ned 
furthermore suppose facts known 
constraint implicit probability distribution treated externally imposed constraint illustration purposes 
prediction task mutually exclusive observations mutually exclusive outcomes interested predicting 
example 

total table task nd probability distribution constraints suppose actual task determine probability rst year students receive grades suppose assign interpretation event student rst year student rst year student grade student grade observed fact students received grade implemented constraint 
implicit fact students received implemented constraint 
goal modeling framework fully estimate questions estimated percentage rst year students receive answered computing probability asp 
table represents probability distribution cells labelled values consistent constraints 
clearly nitely con sistent ways ll cells table way shown table 
principle maximum entropy recommends assignment table non committal assignment probabilities meets constraints formally maximum entropy framework fact total table way satisfy constraints total table uncertain way satisfy constraints implemented constraint model expectation feature epf de ned follows similarly fact implemented constraint epf fx yg epf epf fx yg objective maximum entropy framework maximize subject constraints 
fx yg log assuming features map event constraint feature expectation simply constraint sum cells table feature returns 
constrained maximum entropy problem solved trivially inspection iterative procedure usually required larger problems multiple constraints overlap ways prohibit closed form solution 
conditional maximum entropy models previous example features framework solve prob lems thesis assumes features linguistic prediction aand observable context ultimate goal nd estimate conditional probability ajb opposed joint probability 
conditional maxi mum entropy framework earlier berger lau rosenfeld optimal solution uncertain distribution satis es constraints feature expectations arg max ajb log ajb pfj pfj fj ajb fj important di erence simple example denotes conditional entropy averaged training set opposed joint entropy marginal probability ofb observed probability opposed model probability 
choice marginal probability borrowed earlier berger motivated fact model probability explicitly normalized space possible typically large practice 
model expectation fj computed di erently uses marginal probability important practical reasons 
pfj denotes observed expectation feature fj denotes observed probability xed training sample denotes set probability models consistent observed evidence 
relationship maximum likelihood general maximum likelihood maximum entropy frameworks di erent approaches statistical modeling case yield answer 
show maximum likelihood parameter estimation models form equivalent maximum entropy parameter estimation set consistent models 
arg max arg max fact described lagrange multiplier theory berger information theoretic arguments case joint model della pietra 
maximum likelihood criterion data closely possible maximum entropy criterion assume information linear constraints de ne include proof section show condition arg maxq equivalent condition arg maxp 
important note model form arbitrary maximum entropy solution arg maxp form 
duality maximum entropy principle appealing provides interpretation justi cation maximum likelihood estimation models form 
parameter estimation algorithm called generalized iterative scaling darroch ratcli gis nd values parameters gis procedure requires features sum constant ab kx fj constant 
condition true training set choose max kx fj add correction feature fl fl kx fj pair 
note existing features fl ranges greater 
theory correction constant enforce constraint pairs derived space possible 
summation event space practical correction constants derived training sets usually accurate practice training set large 
algorithm generalized iterative scaling gis 
procedure con verge pfj ep fj ep fj ajb fj ajb ly fj darroch ratcli shows limn see csiszar proof gis divergence geometry framework csiszar 
gis special case improved iterative scaling described berger della pietra nds parameters correction feature 
computation features gis procedure requires computation observed expectation pfj requires re computation model expectation iteration quantity pfj merely count normalized training set pfj fj nx fj ai bi number event tokens opposed types training bn computation involves summing context training set ep fj ajb fj importantly context training set excluded sum computation dominates running time iteration 
number training samples ais set predictions average number features active event running time iteration 
procedure terminated xed number iterations change log likelihood accuracy negligible 
problems thesis iterations rule thumb iterations rarely resulted signi cant accuracy gains 
proof darroch ratcli case joint model proof conditional model similar 
discussion biggest advantage framework allows virtually unrestricted ability represent problem speci knowledge form features 
exact linguistic informa tion corresponding feature dependent task inherent restriction kinds linguistic items feature encode 
features maximum en tropy model need statistically independent probability models thesis fully exploit advantage overlapping interdependent features 
tasks require sequence classi cation decisions tagging parsing highly features model nth decision sequence look previous classi cation decisions 
example ratnaparkhi estimates model part speech tagging context contains word tagged surrounding words results previous tagging decisions tags previous words 
example useful features part speech tagging fj fk determiner current word true noun previous tag determiner true observed pieces evidence corresponding features pfj clear intuitive interpretations 
pfj frequency training sample occurs determiner normalized number training samples frequency training sample determiner precedes noun normalized number training samples 
maximum entropy framework experimenters add modifying formalism exotic detailed forms evidence discovered 
example interesting feature fj adverb complex predicate true complex predicate true current word word number false feature help tagger correctly distinguish case adverb approximately cost case preposition talked 
features diverse nature extent feature fj contributes ajb weight automatically determined generalized iterative scaling algorithm described section 
result experimenters need focus orts discovering features 
special case non overlapping features maximum entropy framework reduces simple type probability model features overlap 
suppose contextual predicates corresponds predicate cpi 
cpi true cpi furthermore suppose predicate cpi features test cpi case iterative algorithm necessary compute ajb closed form estimate simply ratio counts ajb count cp true cp true count cp true cp predicate corresponds 
see section proof 
features form partitions event space described parameter estimation algorithm maximum entropy framework useful correct probability estimate derived raw counts 
emphasize true utility maximum entropy framework comes ability robustly combine features form partitions event space overlap arbitrarily complex ways 
rest thesis demonstrate maximum entropy framework discussed general handle modi cation wide range natural language problems 
items framework particular task set set possible set features fk model 
items fully speci ed facts outcomes set possible fa 
contextual predicates available contextual necessary su cient capture information context 
feature selection features model 
particular contextual predicate cpi may occur predictions may useful predicting 
cpi occurs aj aand ak model may feature aj feature ak subsequent chapters maximum entropy model described characteristics formal properties form model form constraints maximum entropy property relationship maximum likelihood estimation parameter estimation algorithm independent particular prediction task 
chapter machine learning techniques applied natural language brief review illustrate advantages disadvantages maximum entropy framework comparing machine learning algorithms applied natural language 
statistical corpus algorithms literature natural language learning restrict discussion general speci cally designed particular domain application 
discussion assume existence training sett bn contextual predicates cp cpm 
assume machine learning techniques discussed training set gather occurrence statistics outcome truth value contextual predicate cpi applied context cpi 
review natural language learning techniques motivate maximum entropy framework 
naive bayes naive bayes classi er derived bayes rule strong conditional inde assumptions observed evidence 
natural language applications text categorization lewis ringuette word sense dis gale 
bayes rule rewrite ajb ajb bja construct classi er arg max bja typically explicit computation bja impossible due sparse data experimenters avery strong conditional independence assumption bja cp cpm ja pi cpi ja parameters pi cpi ja derived directly cpi counts train ing data require iterative estimation algorithm maximum entropy models 
maximum entropy framework di ers naive bayes classi er inherent conditional independence assumptions allows experimenters encode dependencies freely form features expense iterative parameter estimation algorithm 
suppose cp cp contextual predicates results independent cp cp cp cp context de ne predicate cp cp cp clearly dependent cp 
maximum entropy framework features correlate cp cp cp prediction cp true cp true cp true violating independence assumptions 
contrast naive bayes probability model treat cp independent cp cp yield accurate probability estimates 
maximum entropy framework parameter estimation algorithm explicitly told interdependencies expressed features adjust feature weights account 
course may interdependencies expressed features case parameter estimation algorithm fail account 
statistical decision trees statistical decision tree class probability tree internal nodes represent tests leaves represent conditional probability distributions 
context corre sponds leaf decision tree 
path root leaf obtained rst applying test current node choosing branch child node corresponds outcome test recursively repeating process new child node 
conditional probability distribution associated pl compute ajb 
draw comparison binary decision trees internal node corresponds contextual predicate cp ftrue falseg left branches correspond false right branches correspond true 
assume bis context cp contextual predicate current node 
trace path root leaf start root take branch left child cp false take branch right recursively repeat process newly selected child node reach leaf 
path root leaf corresponding context uniquely determined sequence contextual predicates reaching results applying predicates cp denote predicate corresponds ith parent leaf denote value cp context associated leaf de ne ftrue falseg function returns true corresponds unique leaf true cp false functions leaves partition corresponds exactly leaf 
conditional probability pl ajb leaf true simply normalized frequency partition leaf derived raw counts follows pl ajb count true count true count returns raw counts training sett 
statistical decision trees similar maximum entropy models cope diverse non independent pieces information predicates 
implement decision tree leaves maximum entropy model features set predictions 
leaf decision tree corresponding maximum entropy model features form fl true predicates features check outcomes predicate maximum entropy probability ajb derived simply raw counts training set equation identical 
statistical decision trees grown training recursive partitioning algorithms described id quinlan quinlan cart breiman 
induction algorithms automatically grow cated tests conjuncts predicates simple tests predicates 
approach di ers conjunction predicates speci ed advance experimenter form features complicated features derived automatically simpler ones 
feature induction possible see berger della pietra algorithm incrementally grows conjuncts features maximum entropy framework 
important advantage maximum entropy models decision trees maximum entropy parameter estimation partition training par data sparse predicates leads uneven splits turn lead known data fragmentation problem distributions leaves unreliable correspond small partition training set 
data fragmentation particularly concern natural language processing predicates typically test words sparse nature 
past decision trees nat ural language black jelinek magerman relied host techniques alleviate data fragmentation clustering algorithms reduce amount sparseness predicates smoothing pruning algorithms yield better probability estimates 
contrast maximum entropy models thesis clustering smoothing techniques 
transformation learning transformation learning introduced brill non probabilistic technique incrementally learning rules maximize prediction accuracy 
transformation rule notation format outcome cpi true change outcomes cpi contextual predicate 
transformation learning begins initial state consists pairs context original training set default outcome frequent outcome best guess context 
learner iterates ith iteration selects transformational rule application pairs ti results highest score 
pairs ti re annotated selected rule create ti 
score transformational rule ith iteration usually related improves resemblance ti respect truth manually annotated training sett transformation learning strategy viewed greedy error minimization 
experimenter required specify initial state space transformations available learner scoring function 
context test data default outcome apply respective order transformations learned training phase 
past literature brill transformation learning claims rules learned procedure easier understand statistics comparable probabilistic approaches 
transformation learning extremely exible tasks part speech tagging brill prepositional phrase attachment brill resnik parsing brill 
maximum entropy models equally exible kinds evidence allow types tasks perform 
maximum entropy framework specify space possible transformations specify similar manner space possible features 
maximum entropy models di er context return probability distribution possible outcomes transformation learner returns outcome 
decision lists yarowsky applies learning technique decision lists natural language problem word sense disambiguation supervised unsupervised techniques 
decision lists yarowsky ectively rank di erent pieces evidence reliability unknown test events classi ed single reliable piece evidence available 
notation space outcomes consists elements fa reliability contextual predicate cpi absolute value conditional log likelihood ratio log true true ratio create sorted list contextual predicates outcomes cp cpn cp highest log likelihood ratio ai probable outcome cpi ai arg max fa true 
classifying test case decision list technique chooses outcome ai corresponds rst predicate cpi list cpi true 
conditional probabilities ratio smoothed technique applied word sense disambiguation see yarowsky details 
decision list technique allows experimenter diverse forms contextual evidence chooses outcome single piece reliable evidence 
maximum entropy models equally diverse forms evidence di er greatly probability estimates depend pieces evidence just single best 
yarowsky argues single best piece evidence su ces achieve high accuracies word sense disambiguation notes research needed validate claim tasks 
interpolation linear interpolation popular way combine estimates derived various pieces evidence 
example extensively language modeling goal compute wi combining estimates com ponent distributions wi wi wi component distribution pi estimated straight raw counts training data ectively weight re ects importance corresponding component probability distribution 
weights computed maximize likelihood held data see jelinek details 
technique generalized combine number probability models ajb ipi true pi true conditional probability distribution derived counts cpi training set predicate cpi associated weights estimate pi true 
interpolation technique assump tions underlying nature models combining general method integrating evidence 
maximum entropy models level generality techniques di er weight feature fj associated contextual predicate cp outcome weights maximum entropy model somewhat ner grained interpolation model associates weights predicates outcomes 
decomposable models decomposable models word sense disambiguation bruce pedersen prepositional phrase 
models expressed product marginal probabilities interdependent variables scaled marginal probabilities variables common terms 
notation contextual predicates cp cp cp cp cp interdependent cp conditionally independent cp cp probability cp cp cp written cp cp cp cp cp cp event cpi true abbreviated simply cpi 
iterative parameter esti mation algorithm necessary implement algorithm relevant marginal proba bilities obtained directly counts training data 
order compute joint probability decomposable model contextual predicates known priori induced automatically pedersen 
furthermore contextual predicates may away prohibits decomposition joint probability 
maximum entropy models di er decomposable models handle features 
maximum entropy framework interdependencies expressed features form model 
order account interdependencies expressed features maximum entropy models require computationally expensive iterative parameter estimation algorithm 
furthermore maximum entropy framework general handle interdependencies may expressible decomposable models 
logistic regression models logistic regression described common technique modeling ects explanatory variables binary valued come variable 
example success failure commonly outcomes probability observations fx xjg explanatory variables indicate success kx jxj xj real valued observations real valued parameters 
likewise probability failure outcome 
logistic regression model form special case maximum entropy model form show implement logistic regression model maximum entropy framework 
assume space represent failure success respectively assume features real valued binary valued 
feature features fk format fj xj xj observation explanatory variable presumably exists context 
probability observations lead success jb jb kj kj fj fj fj kx jfj ln fj de ned xj 
features de ned maximum entropy model probability model obtained gis algorithm equivalent logistic regression model parameters obtained maximum likelihood estimation 
simulation logistic regression assumes real valued features binary valued outcomes implementation thesis di ers assumes binary valued features multiple valued outcomes 
maximum entropy framework tasks thesis ers important advantages techniques 
allows exible features naive bayes decomposable probability models expense parameter esti mation capable evidence prediction decision list technique 
theory equally exible linear interpolation studies language modeling shown maximum entropy techniques perform better practice see rosenfeld 
transformation learning exible equally powerful technique goal nd single classi cation probability non probabilistic nature di cult rank sequences classi cations need chapters 
logistic regression models designed problems binary valued outcomes suited natural language tasks tagging parsing require probability models multiple valued outcomes 
decision tree probability models successfully scaled attack natural language parsing problem previous black jelinek magerman relied heavily word clustering technique brown 
believe clustering technique contiguous word bigrams designed gram language modeling preserve information necessary highly accurate syn tactic semantic disambiguation 
maximum entropy framework allows words directly concern data fragmentation 
furthermore direct representation words eliminates harmful assumptions imposed clustering gives option kind information vast number traditional approaches natural language processing 
hypothesis direct representation words maximum entropy framework yield accurate results clustered representation decision trees 
furthermore believe maximum entropy technique theoretically com way combine evidence techniques reviewed wish test theory manifest practice better prediction accuracy 
chapter sentence boundary detection chapter represents joint je rey reynar university 
task identifying sentence boundaries raw text received serious attention computational linguistics literature 
natural language tools part speech taggers parsers including ones discussed thesis assume text divided sentences discuss algorithms dividing accurately 
rst glance may appear postulating sentence boundary occur rence potential sentence nal punctuation mark su cient accurately divide text sentences 
punctuation marks exclusively mark sentence breaks 
example embedded quotations may contain sentence punctuation marks decimal point mail ad dresses indicate ellipsis abbreviations 
somewhat ambiguous appear proper names may multiple times emphasis mark single sentence boundary 
lexically rules written exception lists disambiguate di cult cases described 
lists exhaustive multiple rules may interact badly punctuation marks exhibit absorption properties 
sites logically marked multiple punctuation marks nunberg summarized white 
example sentence abbreviation followed additional period abbreviation contains note followed single president lives washington 
manual approach writing rules appears di cult time consuming due large number lexically rules need written due rule interactions need resolved 
alternative chapter presents solution maximum entropy model requires hints information corpus annotated sentence boundaries 
model trains easily performs comparably systems require vastly information 
previous sentence boundary detection palmer hearst describes system architecture called satz includes thorough review related sentence boundary detection 
satz architecture uses decision tree neural network disambiguate sentence boundaries 
neural network achieves accuracy corpus wall street journal articles lexicon includes part speech pos tag information 
increasing quantity training data decreasing size test corpus palmer hearst reports accuracy neural network decision tree 
results chapter initial larger test corpus 
riley describes decision tree approach problem 
performance approach brown corpus model learned corpus words 
liberman church suggest system quickly built divide newswire text sentences nearly negligible error rate build system 
maximum entropy models sentence boundary iden ti cation chapter systems identifying sentence boundaries max imum entropy models 
targeted high performance uses knowledge structure english nancial newspaper text may applicable text genres languages 
system uses domain speci knowledge aimed portable english text genres roman alphabet languages 
potential sentence boundaries identi ed scanning text sequences char separated whitespace tokens containing symbols 
systems information token containing potential sentence boundary contextual information tokens immediately left right 
wider contexts improve performance omitted 
outcomes outcomes probability model denotes potential sentence boundary actual sentence boundary denotes isn actual sentence boundary 
contextual predicates call token containing symbol marks putative sentence boundary candidate 
portion candidate preceding potential sentence boundary called pre portion called su system focused maximizing performance hints contextual templates pre su presence particular characters pre su candidate ms dr gen candidate corporate designator features word left candidate features word right candidate templates specify form information 
exact set tual predicates maximum entropy model potential sentence boundary marked example pre su null pre 
anlp chairman dr smith 
highly portable system uses identity ofthe candidate neighbor ing words list abbreviations induced training data 
speci cally templates pre su pre su list induced abbreviations word left candidate word right candidate word left right candidate list induced abbreviations information model example anlp chairman pre su null pre 
abbreviation list automatically produced training data tual questions automatically generated scanning training data question templates 
result hand crafted rules lists required highly portable system easily re trained languages text genres 
token training data considered abbreviation preceded followed whitespace contains sentence boundary 
feature selection decision rule potential sentence boundary token wish estimate joint probability distribution surrounding context occurring actual sentence boundary 
probability distribution maximum entropy model identical equation ajb ky fj contextual predicates deemed useful sentence boundary detection de scribed earlier encoded model features 
example useful feature fj pre feature allow model discover period word seldom occurs sentence boundary 
parameter corresponding feature hopefully boost probability pre features occur ring times training data retained model model pa rameters estimated generalized iterative scaling darroch ratcli algorithm described section 
experiments simple decision rule classify potential sentence boundary potential sentence boundary context actual sentence boundary system performance wsj brown sentences candidate marks accuracy false positives false negatives table best performance corpora 
system trained sentences words wall street journal text sections second release penn treebank marcus 
corrected punctuation mistakes erroneous sentence boundaries training data 
performance gures best performing system hand crafted list hon ori cs corporate designators shown table 
rst test set wsj palmer hearst initial test data second entire brown corpus 
brown corpus performance show importance training genre text testing performed 
table shows number sentences corpus number candidate punctuation marks accuracy potential sentence boundaries number false positives number false negatives 
performance wsj corpus expected higher performance brown corpus trained model nancial newspaper text 
possibly signi cant system performance portability new mains languages 
trimmed system uses information derived training corpus performs nearly test sets previous system shown table 
test false false corpus accuracy positives negatives wsj brown table performance corpora highly portable system 
training sentences considerably exist new domain language english experimented quantity training data train les overlapped palmer hearst test data sections 
number sentences training corpus best performing highly portable table performance wall street journal test data function training set size systems 
required maintain performance 
table shows performance wsj corpus function training set size best performing system portable system 
seen table performance degrades quantity training data decreases example sentences performance better baselines sentence boundary guessed potential site token nal instances sentence punctuation assumed boundaries 
chapter described approach identifying sentence boundaries performs comparably state art systems require vastly resources 
exam ple system riley performs better trains brown corpus uses times data system 
system palmer hearst requires pos tag information limits genres languages pos tag lexica pos tag annotated corpora train automatic taggers 
comparison system chapter require pos tags supporting resources sentence boundary annotated corpus 
easy inexpensive retrain system di erent genres text english text roman alphabet languages 
furthermore showed small training corpus su cient performance estimate annotating data achieve performance require hours comparison hours required generate pos tag lexical probabilities 
acknowledgments chapter reynar ratnaparkhi represents joint je rey reynar university 
je david palmer marti hearst giving data sentence detection experiments 
chapter part speech tag assignment natural language tasks require accurate assignment part speech pos tags previously unseen text 
due availability corpora manually annotated pos information taggers annotated text learn probability distributions rules automatically assign pos tags unseen text 
chapter presents pos tagger implemented maximum entropy frame learns probability distribution tagging manually annotated data wall street journal corpus penn treebank project marcus realistic natural language applications process words seen training data experiments chapter conducted test data include unknown words 
papers brill magerman reported tagging accu racy wall st journal corpus 
experiments chapter test hypothesis better context improve accuracy 
maximum entropy model suited experiments combines diverse forms contextual information principled manner 
chapter discusses features pos tagging experiments penn treebank wall st journal corpus 
discusses con problems discovered attempt specialized features word context 
lastly results chapter compared previous pos tagging 
probability model probability ajb represents conditional probability tag context history set allowable tags set possible word tag contexts 
probability model identical equation ajb ky fj usual parameter corresponds feature fj 
sequence words fw ang training data de ne bi context available predicting ai 
features pos tagging conditional probability history tag determined parameters corresponding features active fj 
feature may activate word tag history encode informa tion help predict spelling identity previous tags 
example de ne contextual predicate ing return true current word ends su ing 
useful feature fj bi ing bi true vbg feature exists feature set model corresponding model param eter contribute probability wi ends ing vbg model parameter ectively serves weight certain tual predictor case su ing probability observing certain tag case vbg 
vbg penn treebank pos tag progressive verb 
condition contextual predicates wi rare wi wi rare pre wi jxj su wi jxj wi contains number wi contains uppercase character wi contains hyphen wi ti ti ti xy wi wi wi wi table contextual predicates context bi word stories communities developers tag dt nns jj nns cc nns position contextual predicates table sample data contextual predicates generated automatically training data scanning bi templates table 
generation contextual predicates tagging unknown words relies hy distinction rare words training set similar unknown words test data respect spellings help predict tags 
technique rare words training data tagging unknown words test data developed independently baayen sproat observe pos tags words occur hapax reliable predictors pos tags unknown words 
rare word predicates table spellings apply rare words unknown words test data 
rare word denotes word occurs times training set 
count chosen subjective inspection words training data 
wi wi stories wi wi wi communities ti nns ti ti dt nns table contextual predicates generated tagging table wi wi stories wi communities wi ti ti ti nns pre wi pre wi pre wi wel pre wi su wi su wi ed su wi led su wi wi contains hyphen table contextual predicates generated tagging table example table contains excerpt training data table contains contextual predicates generated scanning current word 
table contains predicates generated scanning current word occurs times training data classi ed rare 
feature selection behavior feature occurs sparsely training set di cult predict statistics may reliable 
model uses heuristic feature occurs times data unreliable ignores features counts 
speci cally contextual predicate cp returned true presence particular prediction times feature model form testing model cp true test corpus tagged sentence time 
testing procedure requires search enumerate candidate tag sequences sentence tag sequence highest probability answer 
search algorithm search algorithm top breadth rst search bfs similar beam search maintains sees new word highest probability tag sequence candidates point sentence 
tag sequence candidate fa ang conditional probability wn bi history corresponding ith word 
ny features look current word features form wi word ti tag 
cuto corresponding de nition rare words kind feature 
addition search procedure consults tag dictionary automatically constructed training data entries form word tn word word training set tn tags word occurs training set 
search procedure needs tag word exists tag dictionary tags entry tag dictionary considered tag candidates tag dictionary search procedure explores tags tagset tagging table describes search procedure detail 
running time dominated inner loop containing insert function word insert kt sequences heap insertion costs log kt size tagset number tag sequences maintain 
running time word sentence log kt 
experiments wall st journal order conduct tagging experiments wall st journal data split contiguous sections shown table 
feature set search algorithm tested debugged training development sets cial test result unseen test set table 
performances tagging model baseline feature set derived table tag dictionary shown table 
experiments increasing signi cantly increase perfor mance development set adversely ects speed tagger 
tag dictionary gave apparently insigni cant improvement accu racy experiments reduces average number tags explored word signi cantly speeds tagger 
kt elements heap known heap created better implementation created heap pass known linear time function case search procedure running time 
current asymptotically slower implementation exible allows experiment search strategies heap elements known heap constructed 
advance 
sm tag sequence produce new sequences sm 
necessary procedure consults tag dictionary insert 
void inserts sequence heap extract 
returns tag sequence highest score removes length input sentence empty tag sequence empty heap hi contains tag sequences length insert initialize empty sequence sz min sz sm advance extract hi insert sp hi return extract hn table tagger search procedure dataset sentences words unknown words training development test table wsj data sizes total word unknown word sentence accuracy accuracy accuracy tag dictionary tag dictionary table baseline performance development set word correct tag proposed tag frequency rb dt rbr jjr rb wdt rb rp jjr rbr wdt rb dt rp dt wdt jj rb yen nn nns chief nn jj rp ago rb rb jj rp table top tagging mistakes training set baseline model number di cult words development set performance table performance baseline model specialized features specialized features consistency maximum entropy model allows arbitrary binary valued features context additional specialized word speci features correctly tag residue baseline features model 
features typically occur infrequently training set consistency yield reliable statistics 
specialized features model noise perform poorly test data 
features designed words especially problematic model 
top errors model training set shown table clearly model trouble words 
hypothesized better features context surrounding correct tagging mistakes words 
specialized features word constructed conjoining certain features baseline model question word 
features ask previous tags surrounding words additionally ask identity current word specialized feature word table fj ai bi wi ti ti dt nns ai wi current word bi ti ti previous tags bi 
table shows results experiment specialized features con structed di cult words added baseline feature set 
di cult words certain way times training set tagged baseline model 
set di cult words model performs accuracy development set insigni cant improvement base line accuracy 
table shows change error rates development word baseline model errors specialized model errors chief executive ago yen table errors development set baseline specialized models set frequently occurring di cult words 
words specialized model yields little improvement specialized model performs worse 
lack improvement implies feature set impoverished training data inconsistent 
simple consistency test graph pos tag assignments word function article occurs 
consistently tagged words roughly tag distribution article numbers vary 
represents pos tag unique integer graphs pos annotation training set function article points scattered show density 
seen gure usually annotated tag denotes preposition tag denotes rb adverb observed probability choice depends heavily current article 
examination tagging distribution changes precisely annotator changes 
uses integers denote pos tags shows tag distribution function annotator implies tagging errors word due mapping article annotator le doc wsj treebank cdrom 
pos tag pos tag article distribution tags word vs article annotator distribution tags word vs annotator training size words test size words baseline specialized table performance baseline specialized model tested consistent subset development set inconsistent data 
words ago chief executive yen exhibit similar bias 
specialized features may ective words ected annotator bias 
simple solution eliminate inter annotator inconsistency train test model data created annotator 
results experiment shown table 
total accuracy higher implying singly annotated training test sets consistent improvement due specialized features higher modest implying features need improvement intra annotator inconsistencies exist corpus 
experiments corpora tagger evaluated lob corpus johansson contains samples british english crater corpus sanchez leon contains samples spanish telecommunications domain 
templates cre ate baseline feature set shown table wall st journal experiments lob crater corpus experiments 
performance maximum entropy tagger corpora baseline feature set shown table 
tagset crater corpus detailed consists tags mapped smaller set tags 
maximum entropy tagger trained single annotator training data obtained extracting articles tagged treebank cdrom 
training data overlap development test set chapter 
single annotator development set portion development set annotated 
word vocabulary tag dictionary baseline experiment 
corpus accuracy word unknown word sentence lob corpus crater corpus table performance lob corpus crater corpus baseline feature set sentences corpus tested remaining sentences results shown table 
preprocessing tokenization experiments lob corpus described van halteren 
comparison previous corpus pos taggers literature markov modeling techniques weischedel merialdo statistical deci sion tree techniques jelinek magerman transformation learning brill 
maximum entropy tagger chapter combines advantages methods 
uses rich feature representation transforma tion learning generates tag probability distribution tag decision decision tree markov model techniques 
weischedel provide results battery tri tag markov model experiments probability observing word sequence fw tag sequence ft tng jt ny ti ny furthermore unknown words computed heuristic uses set pre determined endings endings wall st journal corpus approximation works maximum entropy model giving unknown word accuracy weischedel despite indepen dence assumptions 
diverse information sources added statistically dependent approximations rely independence sumptions may adequately model data 
contrast maximum entropy model combines diverse non local information sources making independence assumptions features 
pos tagger component decision tree statistical parsing system described jelinek magerman 
total word accuracy wall st journal data magerman similar chapter 
techniques require word classes brown help prevent data fragmentation sophisticated smoothing algorithm mitigate ects fragmentation occurs 
decision trees maximum entropy training procedure recursively split data su er unreliable counts due data fragmentation 
result word class hierarchy smoothing algorithm required achieve level accuracy 
brill presents transformation learning data driven non probabilistic approach pos tagging uses rich feature representation performs total word accuracy unknown word accuracy brill 
representation brill somewhat similar chap ter 
brill looks words away current word feature set chapter uses window 
unknown words brill uses separate transformation learner uses pre su additions deletions chapter 
tagger chapter brill sepa rate model unknown words uses features tagging known words previous tag context spelling features order tag unknown words 
transformation learning non probabilistic probabilistic component larger model 
contrast tagger chapter provides probability total word accuracy unknown word accuracy sentence accuracy table performance specialized model unseen test data tagging decision probability calculation structure predicted pos tags noun phrases entire parse trees demonstrated chapter 
claimed advantages maximum entropy tagger taggers realized wall st corpus apparently evident lob corpus van halteren reports maximum entropy tagger outperformed taggers tested including transformation learning tagger hmm tagger 
suspect taggers approached performance limit roughly wall st journal due inherent noise corpus taggers approached similar limit noisy lob corpus 
implementation chapter state art pos tagger evidenced accuracy unseen wall st journal data shown table 
model specialized features perform better baseline model discovery re nement features di cult inconsistencies training data 
model trained tested data single annotator performs higher accuracy baseline model produce consistent input applications require tagged text 
chapter parsing task natural language parser take sentence input return syntactic representation corresponds semantic interpretation sentence 
example parsers sentence return parse tree format np buy cars tires vp buy np cars pp np non terminal labels denote type phrase pp stands prepositional phrase 
accurate parsing di cult subtle aspects word meaning tires parser view dramatically ect interpretation sentence 
example sentence buy cars money parser propose parses np np vp buy np cars pp vp buy np cars np money pp np money parses grammatical sense typical context free grammar english generate structures corresponds interpretation sentence 
parser needs detailed semantic knowledge certain key words sentence order distinguish correct parse needs know money refers buy car 
man buys fast cars big tires parse tree annotated head words parsers currently show superior accuracies freely occurring text classi ed statistical corpus automatically learn syntactic se mantic knowledge parsing large corpus text called treebank manually annotated syntactic information 
order evaluate accuracy statistical parser rst train subset treebank test non overlapping subset compare labelled syntactic constituents proposes labelled syntactic constituents annotation treebank 
labelled constituent accuracies best parsers approach roughly tested freely occurring sentences wall st journal domain 
previous corpus parsers di er simplicity representation de gree supervision necessary agree resolve parse structure ambiguities looking certain cooccurrences constituent head words ambiguous parse 
head word constituent informally word best represents meaning constituent gure shows parse tree annotated head words 
parsers vary greatly head word information disambiguate possible parses input sentence 
black introduces history parsing decision tree probability models trained treebank score di erent derivations sentences produced hand written grammar 
jelinek magerman train history decision tree models treebank parser require explicit hand written grammar 
decision trees black jelinek magerman look words directly repre sent words bitstrings derived automatic clustering technique 
contrast mooney uses rich semantic representation training decision tree decision list techniques drive parser actions 
papers statistics pairs head words conjunction chart parsing techniques achieve high accuracy 
parsers collins collins chart parsing techniques head word bigram statistics derived treebank 
charniak uses head word bigram statistics probabilistic context free grammar goodman uses head word bigram statistics tic feature grammar 
collins goodman charniak collins general machine learning algorithms develop specialized statistical estimation techniques respective parsing tasks 
parser attempts combine advantages approaches 
uses natural direct representation words conjunction general machine learning technique maximum entropy modeling 
argue successful simple representation general learning technique combination minimizes human ort maintains state art parsing accuracy 
parsing maximum entropy models parser constructs labelled syntactic parse trees actions similar standard shift reduce parser 
sequence ang construct completed parse tree called derivation explicit grammar dictates actions allowable actions lead formed parse tree allowable maximum entropy probability models score action 
maximum entropy models trained examining derivations parse trees large hand corrected corpus example parse trees 
individual scores actions pass procedure actions description pass tag pos tag tag assign pos tag set word second pass chunk start join assign chunk tag pos tag third pass build start join word assign current tree con start new constituent label join bel set previous check decide current constituent complete table tree building procedures parser derivation compute score derivation parse tree 
parsing sentence parser uses search procedure ciently explores space possible parse trees attempts nd highest scoring parse tree 
actions parser actions parser produced procedures take derivation fa ang predict action create new derivation fa 
actions procedures designed possible complete parse tree exactly derivation 
procedures called tag chunk build check applied left right passes input sentence rst pass applies tag second pass applies chunk third pass applies build check 
passes procedures apply actions procedures summarized table 
typically parser explores di erent derivations parsing sentence illustration purposes gures trace possible derivation sentence saw man telescope constituent labels part speech pos tags university treebank marcus 
actions procedures scored maximum entropy probability models information local context compute probability distributions 
detailed discussion probability models occur section passes pass allows local context 
example model chunk procedure output tag left right context models build check procedures output tag chunk left right contexts 
procedures implemented left right pass model chunk output tag right context models build check output tag chunk right context 
pass rst pass takes input sentence shown gure uses tag assign word pos tag 
result applying tag word shown gure 
tagging phase described chapter detail 
integrated parser search procedure parser need commit single pos tag sequence 
second pass second pass takes output rst pass uses chunk determine phrase chunks sentence phrase constituent children consist solely pos tags 
starting left chunk assigns word pos tag pair chunk tag start join 
shows result second pass 
chunk tags chunk detection consecutive sequence words wm wn grouped chunk wm assigned start wm wn assigned join result chunk detection shown gure forest trees serves input third pass 
granularity chunks possible constituent labels chunks determined treebank train parser 
examples constituents marked chunks wall st journal domain penn treebank include procedure actions similar shift reduce parser action check shift check reduce cfg rule proposed constituent build start join determines subsequent reduce operations table comparison build check operations shift reduce parser man telescope initial sentence noun phrases asa director adjective phrases years old quanti er phrases 
chunking second pass di ers literature ramshaw marcus church nds chunks constituent labels just noun phrase chunks 
multi pass approach similar approach parser abney rst nds chunks pass attaches pass 
prp vbd saw dt nn man dt result pass nn telescope start np prp vbd saw np prp start np prp start np dt join np nn man result second pass vbd saw dt np nn man dt start np np result chunk detection start vp vbd saw join vp dt np nn man dt dt nn telescope np nn telescope application build join vp action start np prp start vp vbd saw join vp dt np nn man dt np nn telescope proposed constituent shown join np nn telescope start np prp start vp vbd saw join vp dt np nn man dt np nn telescope application check action indicating proposed constituent gure complete 
build process tree marked third pass third pass alternates build check completes remaining constituent structure 
build decides tree start new constituent join incomplete constituent immediately left 
accordingly annotates tree start constituent label join matches label incomplete constituent immediately left 
build processes leftmost tree start join annotation 
shows application build action join vp 
build control passes check nds proposed constituent decides complete 
proposed constituent shown gure rightmost sequence trees tm tn tm annotated start tm tn annotated join check decides proposed constituent takes place forest actual constituent build 
constituent nished build processes tree forest tn 
check answers proposed constituent chunk constituents formed second pass 
shows result check looks proposed constituent gure decides 
third pass terminates check constituent spans entire sentence 
table compares actions build check operations standard shift reduce parser 
actions check correspond shift reduce actions respectively 
important di erence shift reduce parser creates constituent step reduce procedures build check create steps smaller increments 
maximum entropy models shift reduce parsing novel knowledge shift reduce parsing techniques popular natural lan guage literature 
aho describe shift reduce parsing techniques program ming languages detail marcus uses shift reduce parsing techniques natural language briscoe carroll describe probabilistic approaches lr parsing atype shift reduce parsing 
maximum entropy probability model parser uses history approach black probability px ajb score action procedure ftag chunk build de pending context available time decision 
conditional models px estimated maximum entropy framework described chapter 
advantage framework arbitrarily diverse information context computing probability action procedure context rich source information general di cult know exactly information useful parsing 
implement inexact intuitions parsing constituent head words useful 
combinations head words useful 
speci information useful 
allowing limited lookahead useful 
intuitions implemented maximum entropy framework features feature assigned weight corresponds useful modeling data 
describe outcomes contextual predicates feature selection strategy parsing models px furthermore show mere handful guidelines su cient completely describe feature sets parsing models 
describe probability models px compute score parse tree 
outcomes outcomes conditional probability models tag chunk build check exactly allowable actions tag chunk build check procedures listed table 
contextual predicates features chapter require contextual predicates look information partial derivation context 
contextual predicate form cp ftrue falseg checks presence absence useful information context band returns true false accordingly 
implementation maximum entropy framework feature format cp true contextual predicate cp express cooccurrence relationship action linguistic fact context captured cp 
contextual predicates procedure denoted table speci es guidelines templates ftag chunk build check templates linguistic hints specify information specify location useful information context templates indices relative tree currently modi ed 
example current tree th tree cons looks constituent label head word start join annotation rd tree forest 
actual contextual predicates obtained automatically recording certain aspects context speci ed templates procedure derivations trees treebank 
example actual contextual predicate cp cp build derived automatically template cons cp true th tree label np head word false order obtain predicate exist derivation manually parsed ex ample sentences build decides action presence partial derivation th tree constituent label np head word 
con head words necessary algorithm black magerman 
contextual predicates look head words especially pairs head words may reliable predictors procedure actions due sparseness training set 
lexically contextual predicate exist corresponding speci contextual predicates look context omit words 
example templates cons cons cons cons omit head word st tree th tree th st tree respectively 
speci contextual predicates allow model provide reliable probability estimates words history rare 
speci predicates enumerated table existence indicated default predicates table return true context speci frequent predicates provide reasonable estimates model encounters context contextual predicate unreliable 
contextual predicates attempt capture intuitions parsing information discussed earlier 
example predicates derived templates cons look constituent head words predicates derived templates cons look combinations head words 
predicates derived templates cons look speci information predicates derived templates cons limited lookahead 
furthermore information expressed predicates local parsing action place 
contextual predicates tag discussed chapter look previous words tags current word words 
contextual predicates chunk look previous words tags chunk labels current words tags 
build uses head word information previous current trees chunks check looks surrounding words head words children proposed constituent 
intuitions contextual predicates linguistically deep result information necessary parsing speci ed concisely templates 
feature selection feature selection refers process choosing useful subset set possible maximum entropy model corresponding procedure contextual predicates encode training events tx possible actions procedure set possible model px cp true cp contextual predicate cp occurs action potentially feature 
features occur infrequently reliable sources evidence behavior training events may represent behavior unseen data 
example contextual predicates table form reliable features 
simple feature selection strategy assume feature occurs times noisy discard 
feature selection count cuto yield minimal feature set selected features redundant 
practice yields feature set noise free computational expense 
selected features procedure model sx cp true cp ax tx approach burden deciding contribution selected feature modeling data falls parameter estimation algorithm 
model categories description templates tag see chapter chunk word pos tag chunk tag nth leaf 
chunk tag omitted 
default returns true context 
build cons head word con cons cons cons pos label cons cons start join annotation nth tree 
start join annotation omitted 
cons cons cons cons cons cons cons cons cons 
cons cons cons punctuation constituent join contains current tree contains current tree spans entire sentence current tree tence default returns true context 
check head word constituent pos label nth tree label proposed constituent 
rst child resp 

proposed con production xn production constituent label parent constituent pos labels children xn proposed constituent surround label proposed constituent pos tag word nth leaf left constituent default right ofthe constituent returns true context 
surround surround surround surround table contextual information probability models possible speci contexts speci context includes word include head word current tree th tree 
start np prp start vp vbd saw join vp dt np nn man action join vp encoded follows vertical bar separates information subtree comma separates information di erent subtrees 
tilde denotes constituent label opposed part speech tag action contextual predicates default cons np man cons np cons vbd saw cons vbd cons starts np cons starts np cons cons cons np telescope cons np cons vbd np cons vbd saw np cons vbd np man cons vbd saw np man cons np cons np man cons np cons np man cons np np cons np man np cons np man np cons np man np telescope cons np man np telescope cons vbd np cons vbd np man cons vbd saw np man cons vbd np man cons vbd saw np man cons starts np vbd np cons starts np vbd np man cons starts np vbd np man cons starts np vbd saw np man cons starts np vbd saw np man dt np nn telescope encoding derivation contextual predicates scoring parse trees probability models estimated de ne function score search procedure uses rank derivations incomplete complete parse trees 
notational convenience de ne follows ajb tag ajb action tag chunk ajb action chunk build ajb action build check ajb action check deriv fa ang derivation parse necessarily complete ai action tree building procedure 
design tree building procedures guarantee ang derivation parse score merely product conditional probabilities individual actions derivation score ai deriv bi context ai decided 
search search heuristic attempts nd best parse de ned arg max score trees trees complete parses input sentence heuristic employs breadth rst search bfs similar chapter explore entire frontier explores top scoring incomplete parses frontier terminates complete parses hypotheses exhausted 
furthermore ang possible actions procedure derivation context sorted decreasing order consider exploring actions fa amg hold probability mass de ned follows max mx seconds sentence length observed running time top bfs section penn treebank wsj mhz ultrasparc processor mb ram sun ultra enterprise 
threshold 
search uses tag dictionary constructed training data described chapter reduces number actions explored tagging model 
parameters search heuristic experiments reported chapter table describes top bfs semantics supporting functions 
emphasized parser commit single pos chunk assignment input sentence building constituent structure 
passes described section integrated search parsing test sentence input second pass consists best distinct pos tag assignments input sentence 
likewise input third pass consists best distinct chunk pos tag assignments input sentence 
top bfs described exploits observed property individual steps correct derivations tend high probabilities avoids searching parameters optimized development set separate training test sets 
advance 
dm applies relevant tree building procedure returns list new derivations action probabilities pass threshold insert 
void inserts heap extract 
removes returns derivation highest score completed ftrue falseg returns true complete derivation empty heap heap completed parses input sentence hi contains derivations length jcj hi empty break non sz min sz dp advance extract hi completed dq insert dq insert dq hi table top bfs search heuristic seconds sentence precision recall table speed accuracy randomly selected unseen sentences large fraction search space 
practice constant amount ofwork advance step derivation derivation lengths roughly proportional sentence length expect run linear observed time respect sentence length 
con rms assumptions linear observed running time 
expected parsing accuracy degrades reduced accuracy shown table 
experiments experiments measure accuracy portability parser measure potential gain re scoring parser output 
experiments con treebank widely statistical natural language processing community wall st journal treebank release university pennsylvania marcus 
maximum entropy parser trained sections roughly sentences wall st journal corpus tested sec tion sentences comparison 
table describes number training events extracted wall st journal corpus number actions resulting probability models number selected features resulting prob ability models 
words part speech tags constituent labels constituent boundaries penn treebank training testing 
annotation function tags semantic properties constituents null elements indicate traces coreference removed training testing 
previous literature statistical parsing measures proposed black comparing proposed parse corresponding correct treebank parse recall precision correct constituents constituents correct constituents constituents cb crossing brackets constituents violate constituent boundary cb zero crossing brackets ifp contains constituents violate constituent boundaries precision recall measures constituent correct exists constituent label spans words part speech tags counted constituents 
recall precision crossing brackets measures averaged constituents test set crossing brackets mea sure averaged sentences test set 
table shows results parseval measures results slightly forgiving measures magerman shows maximum entropy parser compares favorably state art systems magerman collins goodman charniak collins shows results collins better precision recall 
parser mooney performs labelled precision recall wall st journal domain uses test set comprised sentences frequent words recovers di erent form annotation comparable parsers table 
shows ects training data size versus performance 
procedure number training events number actions number features tag chunk check build table sizes training events actions features word man buys cars big tires modi es man buys root buys cars tires table parse dependency syntax notation dependency evaluation easier diagnose errors dependency syntax notation opposed phrase structure notation 
phrase structure tree annotated head words converted dependency syntax notation shown table word tagged word modi es head word sentence tagged symbol root 
see eisner example penn treebank dependency parsing evaluation 
table shows top dependency errors parser training set sections wsj treebank parsed table shows top dependency errors listed part speech tags training set 
table displays source word correct target incorrect target proposed parser table displays tag source word tag correct target tag incorrect proposed target 
pos tags correct target proposed target identical table correspond di erent words sentence 
surprisingly words ago repeatedly involved errors commas prepositions words part speech tag consistently parser 
portability portability domains important concern corpus methods su er accuracy tested domain unrelated count source word correct target proposed target share ago year year ago said said root says said root root said said share rose said year earlier earlier year said table top dependency errors training set word count tag source word tag correct target tag proposed target nn nn nnp nnp nnp nn nn vbd nn vb nn nn nnp nn nn vbd vbd nn nn nnp nn nnp nn vbd vbd nn vbd vbd nn nnp nnp nn vbd vbd root vbd root vbd vbd nn nnp nnp nn nn nn table top dependency errors training set part speech tag parser precision recall cb cb maximum entropy maximum entropy magerman collins goodman charniak collins table results sentences section words length wsj treebank 
evaluations marked ignore quotation marks 
evaluations marked collapse distinction advp prt ignore punctuation 
accuracy precision recall sample original training set performance section function training data size 
axis represents random samples di erent sizes sections wall st journal corpus 
name description category wsj train sections wsj corpus financial news train sentences section brown corpus magazine journal articles test remaining sentences section brown corpus magazine journal articles train sentences section brown corpus general fiction test remaining sentences section brown corpus general fiction train sentences section brown corpus adventure fiction test remaining sentences section brown corpus adventure fiction table description training test sets training strategy strategy train wsj train test test strategy train wsj train train test test strategy train train test test test corpus precision recall avg 
precision recall table portability experiments brown corpus 
see table training test sets 
trained see sekine 
treebank construction time consuming expensive process near treebanks exist domain conceivably want parse 
important potential loss accuracy training domain wall st journal testing new domain 
experiments address practical questions accuracy lost parser trained wall st journal domain tested domain compared parser trained tested wall st journal small amount training material sentences new domain help parser accuracy new domain new domains magazine journal articles general fiction ad venture fiction brown corpus francis kucera collection english text brown university represents wide variety di erent domains 
domains annotated convention similar text wall st journal treebank 
table describes results di erent training schemes table describes training test corpora 
feature sets parser changed way training brown corpus domains 
table training schemes parsing new domain ranked order best worst 
strategy train mixture lot wsj little 
strategy train lot wsj 
strategy train little experiments particular new domain controlled test set additional training sets train train train consist sentences respective domain 
compared accuracy achieved training testing wall st journal precision recall shown table conclude average lose precision recall training wall st journal testing brown corpus strategy average lose precision recall training wall st journal domain interest testing domain strategy 
discussion far omitted possibility lower brown corpus performance strategies due inherent di culty parsing brown corpus text mismatch training test data 
quick glance gure table possibility training roughly sentences wall st journal yields precision recall slightly higher results brown corpus identical circumstances roughly precision recall 
di erence accuracy due inherent parsing di culty loss accuracy su er strategies training domain test domain mismatch account accuracy loss 
reranking top advantageous produce top parses just top addi tional information secondary model re orders top hopefully improves quality top ranked parse 
see ratnaparkhi probability model output jelinek 
suppose exists perfect reranking scheme sentence magically picks best parse top parses produced maximum entropy parser best parse high est average precision recall compared treebank parse 
performance perfect scheme upper bound performance reranking scheme reorder top parses 
shows perfect scheme achieve roughly precision recall dramatic increase top accuracy precision recall 
shows exact match counts percentage times proposed parse identical excluding pos tags treebank parse rises substantially perfect scheme applied 
surprising accuracy improves looking top parses thousands partial derivations explored discarded accuracy improves drastically looking top completed parses 
reason research reranking schemes appears promising practical step goal improving parsing accuracy 
comparison previous compared parsers accuracy maximum entropy parser state art 
performs slightly better equal systems compared table performs slightly worse collins 
di erences accuracy fairly small unclear di erences matter perfor mance applications require parsed input 
main advantage maximum entropy parser accuracy achieves accuracy simple facts data derived linguistically obvious intuitions parsing 
result evidence needs speci ed concisely method re accuracy precision recall sentence precision recall perfect reranking scheme top parses section wsj treebank function evaluation ignores quotation marks 
accuracy exact match sentence exact match perfect reranking scheme top parses section wsj treebank function evaluation ignores quotation marks 
tasks resulting minimum amount ort part experimenter 
maximum entropy parser di ers statistical parsers repre sents words generality method uses learn parsing actions 
example parsers black jelinek magerman general learning technique decision trees learn parsing actions need represent words bitstrings derived statistical word clustering technique 
maximum entropy parser uses general learning technique require typically expensive clustering procedure allows experimenters natural linguistic repre sentations words constituents 
parsers collins goodman charniak collins natural linguistic representations words constituents general machine learning techniques 
custom built statistical models combine evidence clever ways achieve high parsing accuracies 
possible tune methods maximize accuracy methods speci pars ing problem require non trivial research ort develop 
contrast maximum entropy parser uses existing modeling framework essentially independent parsing task saves experimenter designing new parsing speci statistical model 
general supervision typically leads higher accuracy 
example collins uses semantic tags penn treebank slightly ac parsers table discard information 
mooney hand constructed knowledge base subcategorization table report la precision recall di erent test set evaluation method 
currently maximum entropy parser additional information theory implemented features parser appropriate probability model 
portability parsers discussed limited availability banks 
currently treebanks exist constructing new treebank requires amount ort 
current corpus parsers parse text accurately domain text similar domain treebank train parser 
maximum entropy parser achieves state art parsing accuracy minimizes human ort necessary construction general learning technique simple representation derived intuitions parsing 
results exceed parser require human ort form additional resources annotation 
practice parses test sentence linear time respect sentence length 
trained domains modi cation learning technique representation 
lastly clearly demonstrates schemes reranking top parses deserve research ort yield vastly better accuracy results 
high accuracy maximum entropy parser interesting implications applications general machine learning techniques parsing 
shows procedures actions parser builds trees designed independently learning technique learning technique utilize exactly sorts information words tags constituent labels normally traditional non statistical natural language parser 
implies feasible maximum entropy models general learning techniques drive actions kinds parsers trained linguistically sophisticated treebanks 
better combination learning technique parser treebank exceed current state art parsing accuracies 
chapter unsupervised prepositional phrase attachment prepositional phrase attachment sub task general natural language parsing prob lem task choosing attachment site preposition corresponds interpretation sentence 
example task examples decide preposition modi es preceding noun phrase head word shirt preceding verb phrase head word bought washed 

bought shirt pockets 

shirt soap 
sentence modi es noun shirt pockets describes shirt 
sentence modi es verb washed soap describes shirt washed 
form attachment ambiguity usually easy people resolve computer requires detailed knowledge words washed vs bought order successfully resolve ambiguities predict correct semantic interpretation 
previous previous successful approaches problem statistical corpus consider prepositions attachment ambiguous preceding noun phrase verb phrase 
previous framed problem classi cation task goal predict correct attachment fn vg corresponding noun verb attachment head verb head noun preposition optionally object preposition 
example tuples corresponding example sentences 
bought shirt pockets 
washed shirt soap correct classi cations examples respectively 
hindle rooth describes partially supervised approach fidditch partial parser extract tuples raw text preposition attachment ambiguous head verb head noun extracted tuples construct clas si er resolves unseen ambiguities accuracy 
ratnaparkhi brill resnik collins brooks merlo zavrel daelemans franz trains tests ples form extracted penn treebank marcus gradually improved accuracy kinds statistical learning methods yielding accuracy collins brooks 
nagao reported accuracy corpus model conjunction semantic dictionary claim match human performance task reported ratnaparkhi 
de lima uses shallow parsing techniques collect training data corpus method resolve ambiguous attachments german language 
previous corpus methods approach accuracy humans task portable require resources expensive construct simply nonexistent languages 
english portability genres serious hurdle supervised approaches chapter shows training natural language parser treebank genre testing genre leads substantial loss prediction accuracy 
unsupervised algorithm prepositional phrase attachment requires part speech tagger morphology database training phase resource intensive portable previous approaches required treebanks partial parsers 
theory algorithm easily re trained genres english languages similar word orders 
results english spanish 
unsupervised prepositional phrase attachment exact task algorithm construct classi er cl maps instance ambiguous prepositional phrase corresponding noun attachment verb attachment respectively 
full natural language parsing task just potential attachment sites limit task choosing verb noun may compare previous supervised attempts problem 
candidate attachment sites testing training procedure assumes information potential attachment sites 
generating training data raw text generate training data raw text part speech tagger simple chunker extraction heuristic morphology database 
order tools applied raw text shown table 
tagger chapter rst annotates sen tences raw text sequence part speech tags 
chunker implemented small regular expressions replaces simple noun phrases quanti er phrases head words 
extraction heuristic nds head word tuples attachments tagged chunked text 
heuristic relies observed fact english languages similar word order attachment site preposition usually located words left preposition 
num bers replaced single token text converted lower case morphology tool output raw text professional conduct lawyers jurisdictions guided american bar association rules state bar ethics codes permit non lawyers partners law rms pos tagger dt professional jj conduct nn lawyers nns jj jurisdictions nns vbz guided vbn american nnp bar nnp association nnp rules nns cc state nn bar nn ethics nns codes nns nn wdt permit vbp non lawyers nns vb partners nns law nn rms nns chunker conduct nn lawyers nns jurisdictions nns vbz guided vbn rules nns cc codes nns nn wdt permit vbp non lawyers nns vb partners nns rms nns extraction heuristic lawyers jurisdictions guided rules morphology lawyer jurisdiction guide rule table obtain training data raw text database nd base forms verbs nouns 
extracted head word tuples di er training data previous super attempts important way 
supervised case potential sites verb noun known conjunction attachment 
unsupervised case discussed extraction heuristic nds thinks unambiguous cases prepositional phrase attachment 
possible attachment site preposition verb noun exist case noun attached preposition verb attached preposition respectively 
extraction heuristic loosely resembles step bootstrapping procedure get training data classi er hindle rooth 
step unambiguous attachments fidditch parser output initially resolve ambiguous attachments resolved cases iteratively disambiguate remaining unresolved cases 
procedure di ers critically hindle rooth iterate extract unambiguous attachments unparsed input sen tences totally ignore ambiguous cases 
hypothesis approach information just unambiguous attachment events resolve ambiguous attachment events test data 
tagging chunking rst tagger chapter automatically annotate raw text part speech tags 
simple noun phrases quanti ed phrases chunked replaced head word trivial perl program input sentence line format word tag word tag stdin chunk simple noun phrases replace word dt nnp nn nns jj jjs cd nnp nn nns chunk quantifier phrases replace word cd cd print example tagged chunked sentence shown table 
heuristic extraction unambiguous cases tagged chunked sentence extraction heuristic returns head word tuples form verb noun preposition object preposition 
heuristic parameters need designed experimenter window size parameter determines maximum distance words tween preposition orn 
experiments 
functions identify prepositions nouns verbs assume existence functions identify prepositions nouns verbs tagged text 
function identify forms assume function returns true false indicate verb form verb 
actual function de nitions depend language text annotation style tagger 
tagged data description tagset trivial implement english easy port tagsets languages 
main idea extraction heuristic attachment site preposition usually words left preposition 
extract preposition rst verb occurs words left form verb noun occurs rst noun occurs words right ofp verb occurs preposition rst noun occurs words left verb occurs words left rst noun occurs words right ofp verb occurs table shows result applying extraction heuristic sample sentence 
heuristic ignores cases cases rarely ambiguous opt model deterministically noun attachments 
report accuracies section cases 
heuristic excludes examples verb training set test set unreliable sources evidence 
morphology morphology database xtag system karp reduce nouns verbs extracted tuples morphological base forms 
addition upper case characters translated lower case morphology database number percent sign replaced token num 
table shows example verb guided nouns lawyers jurisdictions rules reduced base forms 
accuracy extraction heuristic applying extraction heuristic unannotated sentences wall st journal data yields approximately unique head word tuples form 
extraction heuristic far perfect applied compared annotated wall st journal data penn treebank extracted head word tuples represent correct attachments 
extracted tuples meant noisy abundant substitute information get treebank 
data available linguistic data consortium www ldc upenn edu accuracy excludes cases 
frequency verb prep noun close num reach comment rise num compare num fall num account num value say interview compare price num table frequent head word tuples frequency noun prep noun num num num num share trading exchange num num num month share revenue num day trading yesterday share sale table frequent head word tuples tables list frequent extracted head word tuples unambiguous verb noun attachments respectively 
frequent noun attached tuples num incorrect 
prepositional phrase num usually attached verb fall wall st journal domain pro ts rose 
recall num token quanti er phrases identi ed chunker 
statistical models extracted tuples form represent unambiguous noun verb attachments verb noun known eventual goal resolve ambiguous attachments test data form noun verb known 
information unambiguous cases resolve ambiguous cases 
natural way classi er compares probability outcome cl arg max fn pr pr probability head words noun attach ment pr probability head words verb attachment 
factor pr follows pr pr pr pr pr pr jp terms pr pr independent ofthe attachment need com puted cl estimation pr pr pr jp problematic training data head words extracted raw text occur leads intuitively motivated approximations pr pr pr jp 
random variable range falseg denote presence absence preposition unambiguously attached noun verb question 
isthe conditional probability particular noun free text unambiguous phrase attachment 
true written simply true 
approximate pr follows pr pr pr pr pr pr rationale approximation tendency pair noun verb attachment related tendency noun verb occur unambiguous prepositional phrase 
term exists approximation formed probability fn vg 
approximate pr follows similarly approximate pr jp pr pr pr pr pr jp pr jp true pr jp pr jp true rationale approximations generating noun verb attachment counts involving noun verb relevant assuming noun verb attached prepositional phrase true 
approxima tions avoid counts seen extracted data 
word statistics tagged corpus set extracted head word tuples estimate probability generating true 
counts tagged corpus chunked count noun count counts extracted tuples true count noun unambiguously attached phrase heads 
true count unambiguously attached phrase heads 
extracted tuples correspond unambiguous attachments correspond instances true 
occurrences verbs nouns ex traction heuristic participate unambiguous attachments correspond instances false 
relationship kinds counts false true false true represents missing head word 
types counts derived usual ways generate true true true true true true true true quantities pr pr denote conditional probability occur unambiguously attached preposition estimated follows generate pr pr true true terms pr true pr true denote conditional probability par ticular preposition occur unambiguous attachment techniques estimate probability raw counts interpolation method maximum entropy framework 
raw counts technique uses raw counts extracted head word tuples backs uniform distribution denominator zero 
pr pr interpolation true true jpj true true jpj true wherep set possible prepositions true wherep set possible prepositions technique similar hindle rooth interpolates tendencies bigrams tendency type attachment particular preposition de ne cn number noun attached tuples cn number noun attached tuples preposition analogously de ne cv cv cn true cn true cv true cv true notation interpolate follows pr true cn cn true pr true cv cv true interpolation maximum entropy framework technique interpolate bigrams attachment tendency particular preposition implement bigrams cn cv statistics features maximum entropy framework 
note bigrams count cuto resulting probability model equivalent tothe model section uses raw counts 
notation introduced chapter de ne maximum entropy conditional model probability preposition noun verb unknown depending value attachment variable 
outcome preposition context triple model de ned follows outcomes set outcomes consists words data tagged prepositions occurred times 
count cuto throws away tagging errors 
special outcome unknown represents sition included word list obtained frequency cuto 
contextual predicates predicates capture attachment cpn true cpv true types predicates capture attachment noun verb templates true noun true verb noun verb represent noun verb respectively 
actual predicates obtained automatically matching templates instances training data 
actual contextual predicate true buy recall noun unknown verb unknown training data consists unambiguous attachments 
feature selection discard features occur times 
example feature model fp true outcome context 
estimated compute pr pj pr true noun pr true verb condition true written explicitly assumed trained unambiguous examples represent true 
generate quantities pr jp true pr jp true denote conditional probability noun occur preposition noun preposition verb attempts far quantities helped accuracy classi er omit calculation classi er 
equivalently assume uniform distribution terms factor terms comparing probabilities 
experiments english approximately unannotated sentences wall st journal processed manner identical example sentence table 
result approximately subset number events total accuracy table accuracy unsupervised classi ers english head word tuples form 
note head word tuples represent correct attachments time quantity times greater quantity data previous supervised approaches 
extracted data training material classi ers classi er constructed follows baseline classi er accuracy indicate level performance attain virtually information classi er form equation uses method section generate interpolation method section generate classi er form equation uses method section generate maximum entropy method section generate classi er form equation uses method section generate raw count method section generate table shows accuracies classi ers test set ratnaparkhi derived manually annotated attachments penn treebank wall st journal data 
penn treebank drawn wall st journal data possibility training data 
furthermore extraction heuristic developed tuned development set set annotated examples overlap test set training set 
shows ect accuracy random sample training data test set performance function training set size varying training set size performance test set shows performance improve additional data 
classi ers clearly outperform baseline approach performance best supervised approach 
surprisingly classi ers interpolate speci evidence preposition counts speci evidence bigram counts outperform classi er despite fact appear better motivated classi er 
failure outperform may due errors extracted training data supervised classi ers train clean data typically bene greatly combining speci evidence speci evidence 
experiments spanish claim approach portable languages similar word order support claim demonstrating approach spanish language 
training set unambiguous tuples extracted table tagger morphological analyzer extraction heuristic modi ed spanish 
spanish tagger morphological analyzer developed xerox research centre europe modify extraction heuristic account new pos tags exclude cases preposition de del analogous correctly identify spanish forms ser analagous chunker spanish experiments di cult port natural language tools 
approximately sentences raw text spanish news text collection spanish experiment rst sentences set aside create test set remainder extract training set 
creating test set english widely available test set ambiguous prepositional phrase attachments spanish language annotators hired create test set 
initially rst annotator scanned raw text subsequences words object attached annotator recorded head words marked corresponding noun verb attachment 
order shift focus highly ambiguous cases annotator extracted test set tuples contained preposition con con observed highly ambiguous rst test set 
test set con overlaps rst test set subset rst test set 
annotators head words extracted rst annotator test sets asked judge noun verb attachments 
judgement rst annotator withheld second third annotators 
supplied dr lauri visit penn di cult author write spanish chunker 
native spanish speaker hand probably nd di cult write chunker analagous english experiment 
test set subset number events total accuracy con total accuracy table accuracy unsupervised classi ers spanish correct incorrect correct incorrect table proportions correct incorrect spanish data prepositions test sets examples annotators agreed evaluate performance classi er spanish 
performance spanish data performance classi ers trained tested spanish language data shown table 
performance higher spanish english spanish test sets performance exceeds 
test sets spanish fairly small compared set english experiment di erence performance statistically signi cant 
signi cance test non independent proportions suggested mcnemar compares number decisions new classi er correct incorrect correct incorrect table proportions correct incorrect spanish data con attachment pr pr noun verb table key probabilities ambiguous example rise num num improves performance old classi er number decisions new classi er dis improves performance 
equally accurate expect versus sided alternative mere chance 
test null hypothesis assuming decisions changed bernoulli trials successes 
standard normal approximation bernoulli trials reject con dence level see larsen marx mcnemar details derive sig ni cance test 
experiment table experiment table 
assuming signi cance level su cient safely reject null hypothesis di erence spanish prepositional phrase attachment experiments 
discussion despite errors extracted head word tuples best performance classi ers english begins approach best performance comparable supervised classi ers literature 
example table shows erroneous noun attached head word tuple num num frequent verb attached rise num conditional probabilities lead prefer verb attachment 
comparison accurate results nagao useful stated goal cheaply replicate information treebank semantic dictionary 
spanish unsupervised classi er performs signi cantly better baseline demonstrates approach inherently portable 
results show information imperfect abundant data un ambiguous attachments shown tables su cient resolve ambiguous prepositional phrase attachments accuracies just best comparable supervised accuracy 
directions take improve prediction accuracy 
firstly may possible improve extraction heuristic way increases precision maintains simplicity portability 
secondly approach preposition previous supervised approaches helps accuracy see brill resnik 
lastly spanish experiment include chunker allow extraction cleaner head word tuples 
believe precise extraction heuristic noun chunker spanish improve accuracies unsupervised approach 
bigram model compute pr pj best classi er fully ex power maximum entropy framework features uses word bigrams homogenous diverse 
circumstances maximum entropy models implemented raw counts 
see section framework useful evaluating diverse feature sets section useful testing diverse forms information involving second noun 
unsupervised algorithm prepositional phrase attachment algorithm published literature signi cantly outperform baseline data derived treebank parser 
accuracy technique approaches accuracy best comparable supervised methods tiny fraction supervision 
small part extraction heuristic speci english part speech taggers morphology databases widely available languages approach far portable previous approaches problem 
furthermore demonstrated portability approach successfully applying prepositional phrase attachment task spanish language 
acknowledgments carmen rio rey initial annotator spanish data extra ort gave nding consistent ways annotate di cult cases advice gave dealing prepositional phrases spanish 
chapter experimental comparison feature selection decision tree learning chapter describes controlled experiments compare maximum entropy framework previous chapters alternative modeling techniques maximum entropy models incremental feature selection compare maximum entropy probability models feature set obtained incremental feature selection count cuto decision trees compare decision tree package known implementation decision tree learning algorithm 
conduct studies previously studied tasks supervised prepositional phrase attachment supervised text categorization 
maximum entropy feature selection feature selection process nd informative subset set pre de ned candidate 
applications thesis simple frequency count cuto ffj cg wheret bn training sample heuristically set threshold usually feature 
previous chapters show strategy works practice resulting set selected minimal sense exist features inf contribute modeling data redundant non informative 
sophisticated strategies literature berger della pietra incrementally attempt build minimal feature setf set candidate feature evaluated bution modeling data added tof 
informally incremental feature selection ifs algorithm works follows 
setf set setc set candidate features 

select leads improvement added 

fi 
stopping conditions met terminate loop 
repeat de ne qf set log linear models feature setf qf ajb fj fj jg de ne pf maximum likelihood model form pf arg max qf rst glance natural way carry step compute maximum likelihood model pf uses feature setf select pf pf greatest 
practice large computation pf cis time consuming 
approximate contribution cin expensive manner 
de ne model form rf set parameter models weights features xed weight candidate feature parameter rf pj ajb pf ajb maximum likelihood model form qf pf ajb qf arg max rf meant parameter approximation parameter model pf quantity qf pf meant approximate true likelihood gain pf pf 
step terminates loop certain stopping conditions met 
stopping condition important performance degrade test data feature setf small large 
iff small contain information necessary successfully model data large training data perform poorly unseen test data 
reasonable stopping condition terminate loop log likelihood held data begins decrease new features added indicator feature set starting may case held data log likelihood decreases adding noisy unreliable feature increases adding reliable feature 
case stopping condition terminate loop prematurely 
order avoid terminating loop prematurely avoid running algorithm long rst select features ifs algorithm gis nd qf technically parameter model gis algorithm requires additional correction feature 
heuristically set upper bound number features necessary accurately model data 
nal feature setf ff fng yields highest log likelihood held data 
denotes held data log likelihood model feature setf arg max ff fn pf speci de nition ifs algorithm 
set candidate features initialize maximum number features select 

select feature think increase likelihood training data added tof 
fi arg max pfi 
set 
terminate loop return arg max ff fn pf likelihood held data repeat 
discussion follows fully specify experiment ifs algorithm parameters candidate feature set setc ifs algorithm select features 
maximum number features number features ifs algorithm select evaluates resulting feature sets held data 
assume existence training set development set test set 
training set build feature fn development set evaluate feature fn test set reporting results 
key property ofthis algorithm candidate feature redundant fi non informative compared features selected approximate gain likelihood computed step negligible 
resulting set far numerous set features obtained count cuto redundant non informative features absent 
goal experiments chapter see resulting smaller feature set better prediction accuracy larger feature set obtained count cuto decision tree learning decision trees popular learning technique arti cial intelligence literature experimentally compare maximum entropy technique commercially available decision tree package successor known package quinlan 
uses recursive partitioning algorithm attempts nd informative tests attributes training data 
assume train ing event consists attribute attri takes nitely discrete values vi 
works continuous numerical valued attributes relevant experiments 
constructs tree shaped classi er root top leaves bottom 
internal nodes consist tests leaves consist classi cation decision 
test node form value attri branch leading node corresponds value vij serve answer test 
classifying test event root node trace unique path leaf tests root internal nodes package licensed research www com branches correspond answers tests 
classi cation returned classi cation leaf corresponds test event 
note di erent statistical decision tree probability model sec tion 
statistical decision tree discussed earlier returns probability distribution returns classi cation decision appears counts leaves arriving classi cation decision 
statistical decision tree discussed earlier binary branching ary branching node depends possible outcomes test node 
prepositional phrase attachment recall task prepositional phrase attachment take head words classify corresponds noun verb attachment 
past supervised approaches data ratnaparkhi extracted penn treebank marcus divided train ing set development set test set 
event training development test sets tuple appropriate head words fn vg 
experiments task controlled training set parameter estimation feature selection decision tree induction 
development set additional parameter tuning test set report results 
try experiments maximum entropy framework called default tuned ifs experiments decision trees called dt default dt tuned 
default maximum entropy framework thesis conjunction simple frequency feature selection 
experiment feature occurs times discarded 
precise model described outcomes vg contextual predicates tuple head words exist con textual predicates look patterns head word grams head word grams head word grams head word grams addition default predicate returns true context 
example gram contextual predicate looks pattern true rose feature uses true feature selection feature occurs times discarded 
tuned model outcomes contextual predicates default model uses di erent feature selection strategy 
count cuto experimentally tuned di erent types gram contextual predicates depending experiment gram features kept gram features occur times discarded gram features occur times discarded 
cuto determined semi automatically development set examples separate test set 
ifs model outcomes contextual predicates default model uses incremental feature selection count cuto candidate feature set ifs algorithm consists feature formed outcomes contextual predicates default model 
equivalent feature set default experiment count cuto applied 
figures graph likelihood accuracy accuracy accuracy pp attachment development set features added development set function number features 
table shows rst features selected ifs algorithm weights 
incremental feature selection maximum entropy framework implemented ratnaparkhi ifs experiment di ers space candidate features 
setting maximum number features selected ifs algorithm 
ultimately features chosen optimal feature set 
dt default experiment package 
training event rep resented head words answer fn vg 
terminology vg attributes verb noun sition noun attribute ranges corresponding words appeared training data 
kinds tests disposal value verb attribute value noun attribute value preposition attribute value noun attribute log likelihood log likelihood pp attachment development set features added feature cp weight default np stake npn np access np including table rst features selected ifs algorithm pp attachment outcomes questions binary ary large 
rst question number outcomes number verbs seen 
command invoke pp pp testing 
dt tuned experiment package optional parameters optimized development set 
report results pp prevents splitting node count pruning con dence level allows induce bigger complex trees 
parameters dt default experiment assumes 
dt binary experiment package representation context identical default experiment decision tree access contextual predicates default experiment 
resulting trees binary contextual predicates binary valued 
baseline performance obtain classi er cl preposition occurs frequently data attached noun 
results experiments number features resulting maximum entropy models training times maximum entropy decision tree models listed table 
maximum entropy models classi er cl cl ajb ran experiments mhz ultrasparc processor 
ran dt experiments mhz ultrasparc processor 
java implementation experiments 
experiment accuracy training time default min tuned min ifs hours dt default min dt tuned min dt binary week baseline table maximum entropy decision tree dt experiments pp attachment context 
decision trees grown package return probability distribution give classi cation 
accuracy table refers classi cation accuracy number times classi cation proposed maximum entropy decision tree models agreed actual annotated classi cation 
tuned experiment performs best default ifs dt tuned perform slightly worse dt default experiment performs worse 
dt binary experiment nish week computation compare accuracy experiments 
default tuned ifs experiments vary feature selection algorithm 
ifs experiment tests incrementally selecting features setc better default default tuned count cuto tuned setc 
ifs experiment took longer run yields feature set order magnitude numerous feature sets selected default tuned experiments 
dt default dt tuned dt binary experiments compare performance decision trees performance maximum entropy models default attempt hold factor representation xed 
representations dt default dt tuned slightly di erent questions experiments multiple valued outcomes contextual predicates maximum entropy models binary valued 
minimum splitting count dt tuned experiment resembles count cuto default experiment 
questions dt binary equivalent default experiment nish 
apparently important tune smoothing parameters dt default experiment perform better baseline task 
decision trees dt default dt tuned experiments harder task maximum entropy models experiments 
decision trees construc tively induce conjunctions questions maximum entropy models told conjunctions 
example default tell model trigram predicates form decision trees dt default dt tuned learn trigram predicates form 
give decision trees hints kinds grams useful contextual predicates default experiment experiment uses hints nish presumably number predicates large 
text categorization text categorization task examine document predict zero categories prede ned set categories topic document 
comparative experiments restrict category acq category represents documents mergers acquisitions 
task nd classi er cl ftrue falseg returns true document category acq 
implement classi er cl maximum entropy probability model follows cl true false experiments discussed 
notation probability models set consists set possible documents outcomes area ftrue falseg 
training set documents reuters collection manually annotated topic categories 
experiments assume existence training sett bn pair consists document annotation ftrue falseg indicates annotated acq category reuters collection 
reuters corpus annotated training set test set split apte experiments 
training set consists documents test set consists documents 
split original training set rst documents development training set remainder development test set 
standard convention text categorization literature words documents lower cased reduced morphological base forms database karp ltered word list lewis 
learning algorithms trained development training set tuned necessary development test set tested original test set 
experiments text categorization maximum entropy models called default ifs experiment decision trees called dt 
default default experiment uses maximum entropy framework con junction count cuto feature selection 
maximum entropy probability model described follows outcomes true falseg contextual predicates contextual predicates check presence words documents form true document contains word false note frequency word document completely ignored 
default predicate returns true context 
feature selection select features positive examples training set documents annotated true features available www research att com lewis research purposes 
accuracy accuracy text categorization development set features added check true 
features form true true true false feature occurs times training set discarded 
ifs experiment uses incremental feature selection maximum entropy framework 
outcomes contextual predicates default experiment feature set built incrementally ifs algorithm 
candidate feature set consists feature formed outcomes contextual predicates default experiment 
equivalent feature set default experiment text categorization count cuto applied 
figures graph likelihood accuracy development set function number features 
table shows rst features selected ifs algorithm weights 
predicate named null default predicate 
setting maximum number features selected ifs algorithm 
ultimately features chosen optimal feature set 
log likelihood log likelihood text categorization development set features added feature cp weight null true acquire true stake true merger true cts true rate true acquisition true year true share true true export true disclose true bid true rise true true unit true debt true true net true sale true table rst features selected ifs algorithm text categorization dt default experiment uses decision tree package 
document encoded predicates default experiment feature selection 
predicates cp cpm represent document element boolean cpm node decision tree ectively asks word document form arbitrarily complex conjunctions questions 
com mand acq 
dt tuned experiment uses representation dt experiment optional parameters optimized text categorization development set 
report results acq 
invokes splitting count pruning con dence level 
optimal setting prepositional phrase attachment task 
table shows accuracy training times default ifs dt default dt tuned experiments text categorization task acq category 
addition feature set sizes experiments included 
default ifs algorithms text categorization vary feature selection results show incremental feature selection slightly outperforms feature selection count cuto default ifs outperform dt experiments 
feature set selected ifs approximately times smaller selected 
representation dt experiments exactly default experiment dt default dt tuned contextual predicates selected feature set default experiment 
decision tree dt experiments advantage ability induce conjunctions word questions ectively test word grams 
contrast maximum entropy models induce conjunctions predicates single words 
surprising representational advantage dt experiments translate accuracy gain experiments 
ran experiments mhz ultrasparc processor 
ran dt experiments mhz ultrasparc processor 
java implementation experiments 
experiment accuracy training time default min ifs hours dt default hours dt tuned hours table text categorization performance acq category maximum entropy technique appears perform better decision tree package prepositional phrase attachment task text categorization task 
experiments controlled training development test sets 
experiments decision trees controlled representation close possible representation maximum entropy models 
surprisingly ability decision tree induce conjunctions questions give performance advantage types conjunctions pre speci ed maximum entropy model case pp attachment conjunctions questions appear improve accuracy case text categorization 
compared di erent feature selection strategies maximum entropy models count cuto feature selection incremental feature selection ifs 
strategy performed accurately prepositional phrase attachment ifs strategy performed accurately text categorization 
strategy performed far 
advantage extremely quick implement execute ifs algorithm complicated extremely time consuming 
ifs algorithm yields concise readable list features represent facts learned 
di cult ascertain exactly features important modeling data 
discussion suggests ciency main objective better choice readable features objective ifs choice 
terms accuracy feature selection strategy consistently outperforms 
chapter limitations maximum entropy framework probability models estimated maximum entropy framework perform practice certain limitations may lead poor prediction accuracy may prevent framework capturing relevant facts data 
major limitation exact maximum likelihood maximum entropy solution exist certain circumstances case probability distribution resulting gis algorithm may lead poor prediction accuracy 
somewhat minor disadvantage facts natural language may expressible binary valued features represented current implementation maxi mum entropy framework 
discuss limitations handled practice 
convergence problems order satisfy constraint feature expectations training data may require solution achieve ajb pair 
cases exact solution exist form probability model model parameters converge gis algorithm 
furthermore probability estimates resulting model typically desires learning technique 
rst give examples parameters converge diverge describe interaction leads undesirable results 
discuss smoothing methods literature dealing problem 
exact solution exists parameters converge suppose space predictions isa space contexts limited tob fxg 
task observe context inb trivially consists element predict probability seeing 
assume features fx fx fx fx assume training features overlap equa tion see maximum entropy probability model constraints yield jx jx 
trivial case represents usually encountered natural language processing tasks context ambiguous outcomes 
exact solution exist parameters diverge suppose thata fxg suppose training sample ist assume feature fx de ned 
order meet constraint case jx 
expand probability model try explicitly calculate jx get jx clearly formula achieve jx exactly nite values 
model expectation gis algorithm increase value iteration drive value jx closer reach 
reason divergence ambiguous appears 
parameter interaction unambiguous contexts yield strange results ambiguous contexts 
suppose thata andb fx features fx fy 
furthermore suppose training sett consists elements large number say 
expand probability model jy jx order meet constraint diverge order meet constraint converge nite value 
probability seeing prediction orp term diverges dominate term 
property highly non intuitive unattractive outcome context pair occurs times training data parameter occurrence takes precedence 
ect model gives nite con dence contexts ambiguous respect predictions occur regardless frequency 
natural language contexts occur infrequently reliable frequently occurring contexts 
model implemented language wherea represented binary linguistic category represented word occurrences low prediction accuracy gives nite con dence unreliable event 
count cuto feature selection strategy thesis allows ignore problem practice infrequent features occur times discarded 
remaining frequent features ambiguous su er diverging parameter values tend reliable cause loss prediction accuracy 
pair occurs times training data occurs model place nite con dence feature fy practice su er loss prediction accuracy result con dence 
smoothing relied smoothing techniques cope situation exact solution exist 
lau reports results experiments fuzzy maximum entropy frame objective maximize sum entropy function penalty function subject linear constraints feature expectations 
penalty function heuristically selected penalize deviations unreliable infrequently observed con straints deviations reliable frequently observed constraints 
ect constraints soft strict equality model feature expectation observed expectation required 
lau applies turing discounting observed feature expectation 
example model meet constraint meet constraint determined formula katz 
approach guarantee constraints consistent longer represent counts drawn training data 
techniques allow maximum entropy solution exist requiring probability model achieve ajb 
binary valued features features sets thesis consist binary valued features return depending presence absence certain contextual evidence certain outcome 
framework developed steven della pietra vincent della pietra ibm tj watson research center 
features su cient capture information word sentence level useful capturing information document level word frequency opposed word presence absence important source evidence 
word frequency information captured somewhat ad hoc manner quantized predicates 
text categorization task want feature fw count outcome document count returns frequency word document alternatively features generalized return integer values fw count limitation binary valued features implementation choice inherent property framework 
probability models maximum entropy framework thesis natural limitation model data requires ajb orp ajb 
order overcome limitation modi ed framework allow soft constraints applied smoothing techniques observed expectations 
features cause problems result limitations low count features feature selection technique discards low count features greatly reduces chance adverse modeling ects 
implementation drawback uses binary valued features represent facts data 
limitation severe integer valued features useful representing facts word sentence level 
chapter experiments thesis support claims accuracy knowledge poor features reusability 
table summarizes tasks describes probability models implemented task 
tasks table di er dramatically outcomes contextual predicates reason suspect underlying contextual predicates model pos tagging model parser build model 
modeling technique practically feature selection strategy performs accurately tasks 
provide detailed arguments describe experiments thesis ful lled claims accuracy knowledge poor features reusability 
accuracy claim maximum entropy models wide variety natural language learning tasks gives highly accurate results 
wehave maximum entropy probabil ity models sentence detection part speech tagging parsing prepositional phrase attachment perform near state art substantial task speci tuning 
maximum entropy probability models prepositional phrase text categorization outperform decision tree package trained tested identical similar conditions 
results exceed pre sented require additional linguistic resources form annotation task outcomes contextual predicates eos detection text candidate punctuation mark pos tagging tagset parsing part speech categories chunk model parsing build model parsing check model prepositional phrase attachment unsupervised prepositional phrase attachment supervised text categorization word pre su surrounding previous tags words tags words pos uni bi tri grams head words constituents punctuation feature selection count cuto count cuto count cuto count cuto head rst head count cuto bigram head possible head current constituent surrounding words cfg rule bigrams head words count cuto prepositions vg grams head words count cuto tuned count cuto words count cuto table summary maximum entropy models implemented thesis linguistic databases require additional research building task speci statistical models 
furthermore usually possible incrementally increase accuracy adding interesting features discovered 
knowledge poor feature sets claim high accuracies obtained knowledge poor feature sets 
knowledge poor mean features require linguistic expertise test simple occurrences words linguistic material context 
feature sets rely manually constructed linguistic semantic classes pre existing training corpus annotation 
feature sets knowledge poor design objective impart little knowledge possible computer force learn possible data 
conceivable knowledge rich features improve accuracy studies restricted knowledge poor features inexpensive implement ported genres languages easily knowledge rich features 
discuss feature sets tasks table represent advances require supervision knowledge preprocessing researcher competing approaches literature perform better comparable competing approaches 
section discuss count cuto works feature selection strategy 
sentence detection approach task uses word spellings optionally list approaches palmer hearst require part speech tags addition features 
part speech tags approach far portable languages previous approaches 
part speech tagging tagger thesis automatically derives features tagging unknown words training corpus uses usual features tagging known words previous tag previous tags uni ed probability model 
di ers taggers brill uses separate learners tagging known unknown words di ers weischedel needs manually pre speci ed su lists tag unknown words 
parsing features parser knowledge poor sense derived fairly shallow intuitions parsing 
parser requires pre processing linguistic information parsers built general learning algorithms 
example di ers decision tree parsers black jelinek magerman require preprocessing words statistically derived word classes 
di ers decision tree decision list parser mooney hand constructed knowledge base 
parser uses general learning algorithm features need carefully selected approaches parsing speci learning frameworks collins goodman charniak collins 
unsupervised prepositional phrase attachment features model derived data annotated part speech tags morphology information attachment information 
di ers previous approaches treebanks parsers obtain relevant statistics 
count cuto feature sets tasks discussed obtained count cuto feature selection 
illustrative know simple count cuto su ces feature selection strategy despite fact discards information 
count cuto tasks unsupervised prepositional phrase attachment 
exact count cuto determined nature feature set 
feature sets consist speci features speci generalized features 
contrast feature set uses consists speci features head word bigrams 
properties suggest conditions best select feature set speci generalized features 
features discarded speci features features tend low frequencies 
discard hypothesis features unreliable sources evidence 
speci generalized features model event speci features frequent reliable generalized features speci features corresponding event discarded 
discussion limited kinds features speci generalized practice feature set consists kinds features varying degrees generality 
non zero count cuto easy way automatically selecting levels generality included feature set 
optimal cuto usually semi automatically evaluating performance models held data model corresponds di erent value tasks thesis invested ort nding optimal task appear variety tasks 
possible careful tuning boost accuracies 
feature set consists speci features generalized features 
case throw away valuable information model fall back generalized features necessary speci features exist 
applications exist hierarchy features varying generality appropriate 
point maximum entropy modeling combine di erent kinds evidence maximum entropy models homogeneous forms evidence implemented simpler techniques classi er unsupervised prepositional phrase attachment chapter 
software re usability advantage maximum entropy framework software implementation highly reusable 
theory maximum entropy framework independent particular natural language task 
practice task speci modi cations software developed task re tasks implemented framework 
single software implementation train maximum entropy probability models thesis 
versions thesis rst written second written java increase portability platforms 
discussion claims important practical rami cations researchers natural language pro cessing 
researchers view maximum entropy framework re usable general purpose modeling tool natural language problem reformulated machine learning task 
encode data knowledge poor features experiments suggest expect high performance unseen test sets 
consequence researchers need concentrate ort discovery information necessary solve problem need spend time nding specialized models combine information discovered 
theory general purpose machine learning algorithm able ful ll claims reusability knowledge poor features accuracy tasks 
general learning algorithms computational linguistics literature demonstrated accurately tasks scale problems large parsing 
example decision trees applied extensively solving problems natural language processing technique thesis outperforms popular decision tree package prepositional phrase attachment text categorization tasks 
thesis rst application maximum entropy technique natural language study framework demonstrated consistently high accuracies di erent tasks feature selection strategy 
intend apply natural language learning techniques corpora linguistically deeper corpora linguistically annotated 
statistical approaches natural language criticized inability deal problems deeper syntax shallow semantics 
believe criticism merely re ects nature available annotated data approach 
interesting direction annotate text deeper level semantic informa tion hope learned application statistical techniques knowledge poor features 
corpora enable statistical techniques return anal natural language semantically deeper hopefully accurate returned current statistical techniques 
secondly tasks thesis excluding chapter assumed existence large annotated training set 
linguistic annotation expensive limits portability supervised natural language learning methods 
interest natural language processing community develop unsupervised methods learn linguistic information time intensive linguistic annotation natural language problems solved genres english languages 
results chapter suggest unsupervised methods inducing certain kinds grammatical relations hold promise interesting see methods accurately predict types grammatical relationships english languages 
appendix relevant proofs non overlapping features mentioned section give proof show probability estimate ajb computed closed form need iterative algorithm features overlap 
space possible contexts cp cpm set predicates cp cp true cp true cp true cp cp 
space possible predictions assume features form fcp cp true fcp denotes feature tests predicate cp prediction theorem non overlapping features 
possible predictions space possible contexts 
predicates cp cpm features fcp cp fcp closed form solution jb jb cp true cp true fcp fcp cp unique predicate cp true fcp unique feature corresponds pair proof 
fact fcp parameter active pair 
rewrite ajb fcp cp cp cp ajb ajb cp cp fcp cp equation simply re arranges summation sum fcp equation follows fact parameter computing ajb fcp 
result ajb moved sum 
follows ajb cp true maximum likelihood maximum entropy purpose section supplement chapter hopes making thesis self contained 
give proofs notions conditional maximum likelihood conditional maximum entropy equivalent circumstances discussed chapter 
proofs conditional models identical proofs joint models della pietra 
de nitions introduce notation proofs 
assume existence training set bn element consists possible prediction context history denote observed probability sett denote observed probability ofthe context int proofs follow assume discuss consequences 
de nition relative entropy training set 
relative entropy conditional probability distributions pkq ajb log ajb ajb de nition nonnegativity 
conditional probability distributions pkq equality assuming 
see cover thomas proof 
de nition set consistent probability models 
set conditional proba bility models consistent observed feature expectations denoted pfj pfj fj ajb fj de nition form log linear models 
set conditional probability models log linear form denoted ajb ky fj ky fj jg de nition entropy training set 
entropy proba bility distribution training sett de ned ajb log ajb useful note pk constant uniform condi tional distribution 
de nition log likelihood training set 
log likelihood probability distribution training sett de ned log ajb useful note pkp constant 
lemma called pythagorean property resembles pythagorean theorem represent vertices relative entropy measure replaced squared distance 
lemma pythagorean property 
pkq pkp pk proof 
term convenience pkq 
ajb log ajb rewrite ajb log fj log log log zq zq parameters written zq indicate correspond probability distribution note ep fj log log zq ep fj log log zq substitution rewrite pkp pk pkp pk pkq lemmas lemma prove properties assume exact solution exists discuss situations assumption false 
lemma 
arg max proof 
show lemma 
pkq pkp pkp pk pkp pk arg max proof 
uniform conditional distribution 
show pk pk pkp pk pk pkp theorem shows maximum likelihood model log linear form maximum entropy model set linear constraints feature expectations 
theorem maximum likelihood maximum entropy 
arg maxq arg maxp proof 
arg maxq pkp pk pkp pkp pk pk implies pk andp lemma arg maxp 
similar arguments show equivalence direction 
theorem maximum entropy maximum likelihood 
arg maxp arg maxq proof 
arg maxp uniform conditional distribution 
pk pk pk pkp pk pk implies pk andp lemma arg maxq 
theorems assume practice usually case 
show lemma maximum likelihood maximum entropy solution unique 
solution unique possible maximum likelihood estimates contexts training set di er context 
theorems assume non empty constraints de ne may require solution value ajb pair model nite parameters achieve value due log linear form empty exact solution exist leading problems discussed chapter 
della pietra deal problem theory closure show case 
practice exact solution exist inexact solution returned gis algorithm probability model data 
chapter discusses inexact solution cause adverse modeling ects 
bibliography abney abney 

parsing chunks 
berwick abney editors principle parsing 
kluwer academic publishers 
aho aho sethi ullman 

compilers principles techniques tools 
addison wesley 
apte apte damerau weiss 

automated learning decision rules text categorization 
acm transactions information systems 
baayen sproat baayen sproat 

estimating lexical priors low frequency morphologically ambiguous forms 
computational linguistics 
berger berger della pietra della pietra 

max imum entropy approach natural language processing 
computational linguistics 
black black 

procedure quantitatively comparing syntactic coverage english grammars 
proceedings february darpa speech natural language workshop pages 
black black jelinek la erty magerman mercer roukos 

history grammars richer models probabilistic parsing 
proceedings st annual meeting acl columbus ohio 
breiman breiman friedman olshen stone 

clas si cation regression trees 
wadsworth belmont 
brill brill 

corpus approach language learning 
phd thesis university 
brill brill 

transformation error driven parsing 
proceed ings st annual meeting acl columbus ohio 
brill brill 

advances transformation part speech tagging 
proceedings twelfth national conference arti cial intelligence volume pages 
brill resnik brill resnik 

rule approach phrase attachment disambiguation 
proceedings fifteenth interna tional conference computational linguistics coling 
briscoe carroll briscoe carroll 

generalized probabilistic lr parsing natural language corpora uni cation grammars 
com putational linguistics 
brown brown della pietra desouza lai mer cer 

class gram models natural language 
computational linguistics 
bruce bruce 

word sense disambiguation decomposable models 
proceedings nd annual meeting acl pages 
charniak charniak 

statistical parsing context free grammar word statistics 
fourteenth national conference onarti cial intelligence prov rhode island 
church church 

stochastic parts program noun phrase chunker unrestricted text 
proceedings second conference applied natural language processing 
collins collins 

generative lexicalised models statistical parsing 
proceedings th annual meeting acl th conference eacl madrid spain 
acl 
collins brooks collins brooks 

prepositional phrase backed model 
yarowsky church editors pro ceedings third workshop large corpora pages cambridge mas 
collins collins 

new statistical parser bigram lexical dependencies 
proceedings th annual meeting acl 
cover thomas cover thomas 

elements informa tion theory 
wiley new york 
csiszar csiszar 

divergence geometry probability distributions minimization problems 
annals probability 
csiszar csiszar 

geometric interpretation darroch ratcli generalized iterative scaling 
annals statistics 
darroch ratcli darroch ratcli 

generalized iterative scaling log linear models 
annals mathematical statistics 
de lima de lima 

assigning grammatical relations back model 
cardie weischedel editors second conference empirical methods natural language processing providence della pietra della pietra della pietra la erty 

features random fields 
ieee transactions pattern analysis machine intelligence 
eisner eisner 

new probabilistic models dependency pars ing exploration 
proceedings th international conference computa tional linguistics coling copenhagen denmark 
francis kucera francis kucera 

frequency analysis english usage lexicon grammar 
houghton mi boston 
franz franz 

independence assumptions considered harmful 
pro ceedings th annual meeting acl th conference eacl madrid spain 
acl 
gale gale church yarowsky 

method disam word senses large corpus 
computers humanities 


maximum entropy hypothesis formulation cially multidimensional contingency tables 
annals mathematical statistics 
goodman goodman 

probabilistic feature grammars 
proceedings international workshop parsing technologies 
mooney mooney 

learning parse translation decision examples rich context 
proceedings th annual meeting acl th conference eacl madrid spain 
acl 
hindle rooth hindle rooth 

structural ambiguity lexical relations 
computational linguistics 
jr 

applied logistic regression 
wiley new york 
jaynes jaynes 

information theory statistical mechanics 
phys ical review 
jelinek jelinek 

self organized language modeling speech recog nition 
waibel lee editors readings speech recognition 
morgan kaufmann 
jelinek jelinek la erty magerman mercer ratnaparkhi roukos 

decision tree parsing hidden derivational model 
proceedings human language technology workshop pages arpa 
johansson johansson 

tagged lob corpus user manual 
computing centre humanities bergen norway 
karp karp schabes 

freely avail able wide coverage morphological analyzer english 
proceedings fourteenth international conference computational linguistics coling nantes france 
katz katz 

estimation probabilities sparse data language model component recognizer 
ieee transactions acoustics speech signal processing assp 
pedersen bruce 

statistical decision making method case study prepositional phrase attachment 
compu tational natural language learning workshop madrid spain 
larsen marx larsen marx 

math statistics applications 
prentice hall 
lau lau 

adaptive statistical language modelling 
master thesis massachusetts institute technology cambridge ma 
lau lau rosenfeld roukos 

adaptive language mod eling maximum entropy principle 
proceedings human language technology workshop pages 
arpa 
lewis lewis 

representation learning information retrieval 
phd thesis university massachusetts amherst mass lewis ringuette lewis ringuette 

comparison learning algorithms text categorization 
third annual symposium document analysis information retrieval pages las vegas nevada 
liberman church liberman church 

text analysis word pronunciation text speech synthesis 
furui editors advances speech signal processing 
marcel dekker new york 
magerman magerman 

statistical decision tree models parsing 
proceedings rd annual meeting acl 
marcus marcus 

theory syntactic recognition natural language 
mit press cambridge mass marcus marcus santorini marcinkiewicz 

building large annotated corpus english penn treebank 
computational lin 
mcnemar mcnemar 

psychological statistics 
john wiley sons merialdo merialdo 

tagging english text probabilistic model 
computational linguistics 
merlo merlo crocker 

attaching multiple prepositional phrases generalized backed estimation 
cardie weischedel editors second conference empirical methods natural language processing pages providence nunberg nunberg 

linguistics punctuation 
technical report lecture notes number center study language information 
palmer hearst palmer hearst 

adaptive multilingual sentence boundary disambiguation 
computational linguistics 
pedersen pedersen bruce wiebe 

sequential model selection word sense disambiguation 
proceedings fifth conference applied natural language processing pages washington quinlan quinlan 

induction decision trees 
machine learning 
quinlan quinlan 

programs machine learning 
morgan mann 
ramshaw marcus ramshaw marcus 

text chunking transformation learning 
yarowsky church editors proceed ings third workshop large corpora cambridge massachusetts 
ratnaparkhi ratnaparkhi 

maximum entropy part speech tag ger 
brill church editors conference empirical methods natural language processing university 
ratnaparkhi ratnaparkhi reynar roukos 

maxi mum entropy model prepositional phrase attachment 
proceedings human language technology workshop pages arpa 
ratnaparkhi ratnaparkhi roukos ward 

maximum entropy model parsing 
proceedings international conference spoken language processing pages yokohama japan 
reynar ratnaparkhi reynar ratnaparkhi 

maximum entropy approach identifying sentence boundaries 
proceedings fifth con ference applied natural language processing pages washington riley riley 

applications tree modelling speech language 
proceedings darpa speech language technology workshop pages cape cod massachusetts 
darpa 
rosenfeld rosenfeld 

maximum entropy approach adaptive statis tical language modeling 
computer speech language 
sanchez leon sanchez leon 

spanish tagset crater project 
technical report de linguistica informatica universidad de madrid 
available cmp lg archive xxx lanl gov cmp lg 
sekine sekine 

domain dependence parsing 
proceedings fifth conference applied natural language processing pages washington nagao nagao 

corpus pp attach ment ambiguity resolution semantic dictionary 
zhou church editors proceedings fifth workshop large corpora pages beijing hong kong 
van halteren van halteren zavrel daelemans 

im proving data driven tagging system combination 
proceedings seventeenth international conference computational linguistics coling acl university 
weischedel weischedel meteer schwartz ramshaw 

coping ambiguity unknown words probabilistic models 
computational linguistics 
white white 

presenting punctuation 
proceedings fifth eu workshop natural language generation pages leiden lands 
yarowsky yarowsky 

machine learning algorithms lexical ambiguity resolution 
phd thesis university 
zavrel daelemans zavrel daelemans 

memory learning similarity smoothing 
proceedings th annual meet ing acl th conference eacl madrid spain 
acl 

