
clusters 
clustering method 
answers model cluster analysis chris fraley adrian raftery department statistics university washington usa email fraley stat washington edu consider problem determining structure clustered data prior knowledge number clusters information composition 
data represented mixture model component corresponds different cluster 
models varying geometric properties obtained gaussian components different parametrizations cross cluster constraints 
noise outliers modelled adding poisson process component 
partitions determined expectation maximization em algorithm maximum likelihood initial values agglomerative hierarchical clustering 
models compared approximation bayes factor bayesian information criterion bic significance tests allows comparison models time removes restriction models compared nested 
problems determining number clusters clustering method solved simultaneously choosing best model 
em result provides measure uncertainty associated classification data point 
examples showing approach give performance better standard procedures fail identify groups overlapping varying sizes shapes 
consider problem determining intrinsic structure clustered data information observed values available 
problem known cluster analysis distinguished related problem discriminant analysis known groupings observations categorize infer structure data 
probability models proposed quite time basis cluster analysis 
approach data viewed coming mixture probability distributions representing different cluster 
methods type shown promise number practical applications including character recognition murtagh raftery tissue segmentation banfield raftery seismic fault detection dasgupta raftery identification textile flaws images campbell classification astronomical data celeux 
bayes factors approximated bayesian information criterion bic applied successfully problem determining number components model deciding partitions closely matches data model 
describe clustering methodology multivariate normal mixtures bic computer journal vol 
direct comparison models may differ number components mixture underlying densities various components 
partitions determined combination hierarchical clustering expectation maximization em algorithm dempster laird rubin maximum likelihood 
approach give better performance existing methods 
em result provides measure uncertainty resulting classification 
shows example model classification able match clinical classification biomedical data set closely single link nearest neighbour standard means absence training data 
organized follows 
section give necessary background multivariate cluster analysis including discussions probability models em algorithm clustering approximate bayes factors 
basic model strategy modifications handling noise described sections respectively 
detailed analysis multivariate data set shown section followed example detection presence noise section 
information available software various procedures approach section 
final section summarizes indicates extensions method 
clinical classification glucose single link nearest neighbour error rate glucose model cluster analysis model classification error rate glucose means error rate glucose 
projection group classification diabetes data miller single link nearest neighbour standard means unconstrained model approach 
filled symbols represent misclassified observations 

model cluster analysis 
cluster analysis background cluster analysis mean partitioning data meaningful subgroups number subgroups information composition may unknown introductions include hartigan gordon murtagh mclachlan basford kaufman rousseeuw 
clustering methods range largely heuristic formal procedures statistical models 
usually follow hierarchical strategy observations relocated tentative clusters 
hierarchical methods proceed stages producing sequence partitions corresponding different number clusters 
agglomerative meaning groups merged divisive groups split stage 
hierarchical procedures subdivision practical number possible splittings restricted 
computer journal vol 
agglomerative hierarchical clustering number stages bounded number groups initial partition 
common practice observation cluster procedure initialized coarser partition groupings known 
drawback agglomerative methods practical terms time efficiency require memory usage proportional square number groups initial partition 
stage hierarchical clustering splitting merging chosen optimize criterion 
conventional agglomerative hierarchical methods heuristic criteria single link nearest neighbour complete link farthest neighbour sum squares 
modelbased methods maximum likelihood criterion merging groups 
relocation methods move observations iteratively group starting initial partition 
number groups specified advance typically change course fraley raftery iteration 
common relocation method means macqueen hartigan wong reduces group sums squares 
clustering mixture models relocation techniques usually em algorithm see section 
hierarchical relocation methods directly address issue determining number groups data 
various strategies simultaneous determination number clusters cluster membership proposed hartigan bock survey see bock 
alternative described 

probability models cluster analysis model clustering assumed data generated mixture underlying probability distributions component represents different group cluster 
observations xn fk xi density observation xi kth component corresponding parameters number components mixture 
model composite clusters usually formulated ways 
classification likelihood approach maximizes lc 

xi discrete values labelling classification xi belongs kth component 
mixture likelihood approach maximizes lm 

fk xi probability observation belongs kth component 
mainly concerned case fk xi multivariate normal gaussian model considerable success number applications 
instance parameters consist mean vector covariance matrix andthe density form fk xi exp xi xi 
clusters ellipsoidal centred means 
covariances determine geometric characteristics 
banfield raftery developed model framework clustering covariance matrix terms eigenvalue decomposition form dk ak dk orthogonal matrix eigenvectors ak diagonal matrix elements proportional eigenvalues scalar 
orientation principal components determined dk ak determines shape density contours specifies volume corresponding ellipsoid proportional ak 
characteristics orientation volume shape distributions usually estimated data allowed vary clusters constrained clusters 
approach subsumes earlier proposals gaussian mixtures gives sum squares criterion long known heuristic ward clusters spherical equal volumes dad clusters shape volume orientation friedman rubin unconstrained dk ak dt general model scott dk murtagh raftery orientations clusters may differ 
table shows geometric interpretation various parametrizations discussed 
extensive set models framework treated 
classification likelihood basis agglomerative hierarchical clustering 
stage pair clusters merged maximize resulting likelihood 
fraley developed efficient algorithms hierarchical clustering various parametrizations gaussian mixture models 

em algorithms clustering iterative relocation methods clustering mixture models possible em related techniques 
em algorithm general approach maximum likelihood presence incomplete data 
em clustering complete data considered yi xi zi zi zig xi belongs group zik constitutes missing data 
relevant assumptions density observation xi zi fk xi zik zi independent identically distributed multinomial distribution draw categories probabilities 
resulting complete data loglikelihood zik computer journal vol 
zik log fk xi 
quantity zik zik xi 
model conditional expectation zik observation xi parameter values 
value ik zik maximum conditional probability observation belongs group classification observation xi taken ij maxk ik 
conventions normalizing ak include requiring ak requiring max ak largest eigenvalue model cluster analysis table 
parametrizations covariance matrix gaussian model geometric interpretation 
models shown discussed banfield raftery 
distribution volume shape orientation spherical equal equal na spherical variable equal na dad ellipsoidal equal equal equal dk ak dk ellipsoidal variable variable variable dk ellipsoidal equal equal variable dk ellipsoidal variable equal variable initialize zik discrete classification repeat step compute maximum likelihood parameter estimates zik nk zik nk ni nk depends model see celeux step compute zik parameter estimates step zik fk xi gj xi fk form convergence criteria satisfied 
em algorithm clustering gaussian mixture models 
strategy described initializes iteration indicator variables corresponding partitions hierarchical clustering terminates relative difference successive values mixture loglikelihood falls small threshold 
em algorithm iterates step values zik computed data current parameter estimates step loglikelihood zik replaced current conditional expectation zik maximized respect parameters see 
celeux detail steps case multivariate normal mixture models parametrized eigenvalue decomposition 
certain conditions mclachlan krishnan wu method shown converge local maximum mixture likelihood 
conditions convergence proven hold practice method widely mixture modelling context results 
observation maxk ik measure uncertainty associated classification 
em algorithm clustering number limitations 
rate convergence slow 
computer journal vol 
appear problem practice mixtures started reasonable values 
second number conditional probabilities associated observation equal number components mixture em algorithm clustering may practical models large numbers components 
em breaks covariance matrix corresponding components ill conditioned singular nearly singular 
general proceed clusters contain observations observations contain nearly collinear 
em model having certain number components applied mixture fewer groups may fail due ill conditioning 
number variants em algorithm clustering studied 
include stochastic em sem algorithm celeux diebolt celeux diebolt zik simulated estimated step fraley raftery classification em cem algorithm celeux converts zik step discrete classification performing step 
standard means algorithm shown version cem algorithm corresponding uniform spherical gaussian model 

bayesian model selection clustering advantage mixture model approach clustering allows approximate bayes factors compare models 
gives systematic means selecting parametrization model clustering method number clusters 
review bayes factors emphasizing underlying concepts scientific applications see kass raftery 
bayes factor posterior odds model assuming favoured priori 
banfield raftery heuristically derived approximation twice log bayes factor called awe determine number clusters hierarchical clustering classification likelihood 
em find maximum mixture likelihood reliable approximation twice log bayes factor called bic schwarz applicable logp constant lm mm log bic integrated likelihood data model lm maximized mixture loglikelihood model mm number independent parameters estimated model 
number clusters considered independent parameter purposes computing bic 
model equally apriori proportional posterior probability data conform model accordingly larger value bic stronger evidence model 
fit mixture model data set improve likelihood increase terms added model 
likelihood directly assessment models cluster analysis 
bic term added loglikelihood penalizing complexity model may maximized parsimonious parametrizations smaller numbers groups loglikelihood 
bic compare models differing parametrizations differing numbers components 
bayesian criteria bic cluster analysis bock binder 
regularity conditions bic hold mixture models considerable kass raftery authors define bic opposite sign case smaller negative bic stronger evidence model 
chosen reverse convention order easier interpret plots bic values 
theoretical practical support context 
standard convention calibrating bic differences differences correspond weak evidence differences positive evidence differences strong evidence differences greater strong evidence kass raftery jeffreys 

model strategy clustering computer journal vol 
practice agglomerative hierarchical clustering classification likelihood gaussian terms gives suboptimal partitions 
em algorithm refine partitions started sufficiently close optimal value 
dasgupta raftery able obtain results number examples partitions produced model hierarchical agglomeration starting values em algorithm constant shape gaussian models bic determine number clusters 
approach forms basis general model strategy clustering 
determine maximum number clusters consider set candidate parametrizations gaussian model consider 
general small possible 
agglomerative hierarchical clustering unconstrained gaussian model obtain corresponding classifications groups 
em parametrization number clusters starting classification hierarchical clustering 
compute bic cluster model parametrization mixture likelihood optimal parameters em 
clusters 
gives matrix bic values corresponding possible combination parametrization number clusters 
plot bic values model 
decisive local maximum indicates strong evidence model parametrization number clusters 
important avoid applying procedure larger number components necessary 
reason minimize computational effort reasons discussed section 
heuristic works practice select number clusters corresponding decisive local maximum parametrizations considered 
may cases local maxima giving larger values bic due genuine indication better model discussion see section 
hierarchical clustering method corresponding parametrization gaussian model appears sufficient practice unconstrained model initialization 

modelling noise outliers model strategy cluster analysis described section directly applicable noisy data model modified em works reasonably initial identification noise clusters 
noise modelled constant rate poisson process resulting mixture likelihood lm 

fk xi data region fk xi multivariate normal 
observation contributes belongs noise contributes gaussian term 
basic model procedure noisy data follows 
necessary obtain initial estimate noise 
possible approaches de noising include nearest neighbour method byers raftery method fraley uses tessellations 
hierarchical clustering applied de data 
final step em augmented model applied entire data set gaussian components initialized hierarchical clustering partitions noise component initialized result de noising procedure 
bic select best model representing data 

examples 
diabetes diagnosis section illustrate model approach clustering dimensional data set involving observations diabetes diagnosis miller 
pairs plot showing clinical classification partitions data groups 
variables meanings glucose plasma glucose response oral glucose insulin plasma insulin response oral glucose degree insulin resistance 
clusters overlapping far spherical shape 
result clustering procedures application 
example shows projection cluster classifications obtained single link nearest neighbour method standard kmeans model method unconstrained gaussian mixture 
possible group assignments shown chosen minimize error rate case 
assumption classes artificial single link means model method bic determine number groups see 
standard means single link perform example 
clusters identified single link singletons nearly data assigned class 
classes resulting standard model cluster analysis computer journal vol 
means non trivial classes confined long thin extensions third class subsumes extension conjunction 
clinical classification long extensions roughly represents cluster third cluster concentrated closer origin 
clustering methods currently common clusters separated break clusters overlap intersect 
important distinguish clustering nearest neighbour discrimination 
discrimination training set data group memberships known advance clustering group memberships unknown 
nearest neighbour discrimination assigns data point group point training set nearest 
works ripley success depends entirely available training set 
gives plot bic model methods spherical models equal varying volumes constant variance unconstrained variance constant shape models equal varying volumes 
local maximum case global maximum occurs unconstrained model clusters classification assignment shown 
initial values em zik equation discrete classification agglomerative hierarchical clustering unconstrained model dk ak cases leaving model selection em phase 
note values bic spherical varying volume model clusters unconstrained model clusters 
cases covariance matrix associated mixture components ill conditioned loglikelihood bic computed 
hierarchical clustering spherical model produces cluster solution cluster singleton unconstrained model produces cluster solutions cluster contains points 
data threedimensional minimum points required estimate covariance matrix non singular 
algorithms em computing bic monitor estimate reciprocal condition number smallest largest eigenvalue ratio covariances 
quantity falls range values near zero imply ill conditioning 
computations reliable problems result ill conditioning may cause anomalies reaching point actual failure 
implementation em terminates warning estimated covariance matrices judged close singularity bic calculation produces missing value circumstances 
table shows reciprocal condition estimates different gaussian mixture models clusters 
clear em started partitions obtained hierarchical clustering continued higher numbers clusters ill conditioning encountered 
fraley raftery glucose insulin 
pairs plot showing clinical classification diabetes data 
symbols interpretation triangles normal circles chemical diabetes squares overt diabetes 
bic bic diabetes data spherical equal volume spherical varying volume constant variance unconstrained constant shape equal volume constant shape varying volume number clusters 
uncertainty model classification 
glucose 
plot left shows bayesian information criterion bic model methods applied diabetes data 
local global maximum occurs unconstrained model clusters 
plot right depicts uncertainty classification produced best model unconstrained clusters indicated bic 
symbols interpretation dots open circles filled circles 
computer journal vol 
model cluster analysis table 
minimum reciprocal condition estimates covariances model methods applied diabetes data 
rows correspond models columns numbers components 
values near zero cases small cluster points nearly collinear 
bic dk ad dk ad simulated noise bic data spherical equal volume spherical varying volume constant variance unconstrained constant shape equal volume constant shape varying volume number clusters de data model classification 
model classification simulated noise 
hierarchical clustering applied data nearest neighbour de noising 
em applied full data set noise term included model 

detection presence noise shows results model strategy noise section simulated data smith see 
data arise processing series images taken reconnaissance aircraft large number points identified representing computer journal vol 
possible mines fact false positives noise 
assumption imaged area lie completely occur area higher density identified points 
goals determine image contains give location may 
fraley raftery initial de noising carried procedure nearest neighbour de noising 
bic clearly maximized value clusters plus noise favours uniform shape model 
clusters give accurate reconstruction actual 
noted method sensitive value assumed volume data region 
clear area image banfield raftery dasgupta raftery similarly volume smallest hyperrectangle sides parallel axes contains data points 
value overestimate cases 
possibility take smallest hyperrectangle sides parallel principal components data contains data points 
implementation uses smaller alternatives default allows specification user 
better solution volume convex hull data may practical compute higher dimensions 

software software implementing state art algorithms hierarchical clustering em various parametrizations gaussian clustering models available internet details see www stat washington edu fraley soft shtml 
included functions incorporate hierarchical clustering em bic model cluster analysis strategy described 
software designed interface commercial interactive software plus 
earlier version model hierarchical clustering software included plus package function 
subscription information mailing list occasional announcements software updates web page 
plus function implementing nearest neighbour de noising method available lib stat cmu edu 

discussion described clustering methodology multivariate normal mixture models shown give better performance existing methods 
approach uses model agglomerative hierarchical clustering initialize em algorithm variety models applies bayesian model selection methods determine best clustering method number clusters 
uncertainty associated final classification assessed conditional probabilities em 
approach limitations 
computational methods hierarchical clustering seattle wa usa www 
com splus storage time requirements grow faster linear rate relative size initial partition directly applied large data sets 
way determine structure subset data strategy resulting parameters initial values em data classify remaining observations supervised classification discriminant analysis 
celeux developed method regularized discriminant analysis full range parametrizations gaussian mixtures 
alternatively fast methods determining initial rough partition reduce computational requirements 
suggested method minimum spanning tree purpose shown works practice 
second experience date suggests models multivariate normal distribution sufficiently flexible accommodate practical situations underlying assumption groups concentrated locally linear subspaces models methods may suitable instances 
section obtained results noisy data combining model methodology separate de noising procedure 
example suggests nonlinear features instances represented framework piecewise linear features groups 
alternative models classes characterized different geometries linear manifolds bock diday sp th 
features strongly curvilinear curves groups centred modelled principal curves hastie stuetzle 
clustering principal curves successfully applied automatic identification ice contours tracking ice modelling ice leads :10.1.1.49.9676
initial estimation ice outlines accomplished means mathematical morphology 
principal curve clustering presence noise bic discussed stanford raftery 
situations bic definitive computationally intensive bayesian analysis may provide solution 
showed exact bayesian inference gibbs sampling calculations bayes factors laplace metropolis estimator works real simulated examples 
approaches clustering classification likelihood known classification maximum likelihood methods mclachlan bryant williamson fixed classification methods bock 
alternatives classification mixture likelihoods section classification likelihood lc xi computer journal vol 
posterior likelihood anderson lp zik xi zik complete data likelihood em algorithm zik restricted indicator variables form complete data likelihood em algorithm includes zik parameters estimated 
fuzzy clustering methods bezdek model provide degrees membership observations 
means algorithm applied classical sum squares criterion modelbased clustering criteria bock diday diday sp th celeux 
model clustering methodologies include cheeseman stutz implemented autoclass software mclachlan implemented software 
autoclass handles discrete data continuous data data discrete continuous variables 
autoclass continuous data rely em algorithm multivariate normal distribution allows choice equal unconstrained diagonal covariance matrices autoclass covariances assumed diagonal 
approach autoclass uses approximate bayes factors choose number clusters see chickering heckerman approximation differs bic 
determines number clusters resampling option modelling outliers fitting mixtures multivariate distributions mclachlan peel 
autoclass em initialized random starting values number trials determined specification limit running time 
options initializing em include common heuristic hierarchical clustering methods means model hierarchical clustering solution initial value 
funded office naval research contract nos 
go simon byers providing de noising procedure referees suggesting number improvements 
murtagh raftery 
fitting straight lines point patterns 
pattern recognition 
banfield raftery 
model gaussian non gaussian clustering 
biometrics 
dasgupta raftery 
detecting features spatial point processes clutter model clustering 
amer 
stat 
assoc 
model cluster analysis computer journal vol 
campbell fraley murtagh raftery 
linear flaw detection woven textiles modelbased clustering 
pattern recognition lett 
celeux 
gaussian parsimonious clustering models 
pattern recognition 
babu murtagh fraley raftery 
types gamma ray bursts 


dempster laird rubin 
maximum likelihood incomplete data em algorithm 
stat 
soc 
miller 
attempt define nature chemical diabetes multidimensional analysis 

hartigan 
clustering algorithms 
wiley new york 
gordon 
classification methods exploratory analysis multivariate data 
chapman hall london 
murtagh 
multidimensional clustering algorithms 
lectures 
physica verlag 
mclachlan basford 
mixture models inference applications clustering 
marcel dekker new york 
kaufman rousseeuw 
finding groups data 
wiley new york 
macqueen 
methods classification analysis multivariate observations 
cam neyman 
eds proc 
th berkeley symp 
mathematical statistics probability vol 
pp 

university california press 
hartigan wong 
algorithm means clustering algorithm 
appl 
stat 
hartigan 
percentage points test clusters 
amer 
stat 
assoc 
bock 

ruprecht 

choosing number component clusters mixture model new informational complexity criterion inverse fisher information matrix 
lausen eds information classification pp 

springer verlag berlin 
bock 
probability models hypothesis testing partitioning cluster analysis 
arabie hubert 
eds clustering classification pp 

world science publishers singapore 
ward 
hierarchical groupings optimize objective function 
amer 
stat 
assoc 
friedman rubin 
invariant criteria grouping data 
amer 
stat 
assoc 
scott 
clustering methods likelihood ratio criteria 
biometrics 
fraley 
algorithms model gaussian hierarchical clustering 
siam sci 
comput 
mclachlan krishnan 
em algorithm extensions 
wiley new york 

convergence em algorithm 
stat 
soc 
wu 
convergence properties em algorithm 
annals 
stat 
fraley raftery celeux raftery robert 
inference model cluster analysis 
stat 
comput 
celeux diebolt 
reconnaissance de de par un algorithme apprentissage 
diday pag 

eds data analysis informatics iii elsevier science pp 

celeux diebolt 
sem algorithm probabilistic teacher algorithm derived em algorithm mixture problem 
comput 
stat 
quart 
celeux 
classification em algorithm clustering stochastic versions 
comput 
stat 
data anal 
kass raftery 
bayes factors 
amer 
stat 
assoc 
schwarz 
estimating dimension model 
ann 
stat 
binder 
bayesian cluster analysis 
biometrika 

consistent estimation mixing distribution 
annals 
stat 
roeder wasserman 
practical bayesian density estimation mixtures normals 
amer 
stat 
assoc 
jeffreys 
theory probability rd edn 
clarendon oxford 
byers raftery 
nearest neighbor clutter removal estimating features spatial point processes 
amer 
stat 
assoc 
fraley 
nonparametric maximum likelihood estimation features spatial point processes tessellation 
amer 
stat 
assoc 
ripley 
neural networks related methods classification 
stat 
soc 
golub van loan 
matrix computations rd edn 
johns hopkins university press baltimore md smith 
nonparametric detection localization 
technical report css tm coastal systems station panama city florida 
celeux 
regularized gaussian discriminant analysis eigenvalue decomposition 
amer 
stat 
assoc 

hierarchical model clustering large data sets 
technical report university minnesota school statistics 
diday 
optimisation en classification automatique 
inria france 
sp th 
cluster dissection analysis theory fortran programs examples 
ellis horwood new york 
hastie stuetzle 
principal curves 
amer 
stat 
assoc 
banfield raftery 
ice identification satellite images mathematical morphology computer journal vol 
clustering principal curves 
amer 
stat 
assoc 
banfield raftery 
identifying ice satellite images 
naval research rev 
banfield :10.1.1.49.9676
automated tracking ice statistical approach 
ieee trans 
geoscience remote sensing 
banfield 
skeletal modeling ice leads 
ieee trans 
geoscience remote sensing 

mathematical morphology modern approach image processing algebra geometry 
siam rev 
stanford raftery 
principal curve clustering noise 
technical report university washington department statistics 
mclachlan 
classification mixture maximum likelihood approaches cluster analysis 
krishnaiah kanal 
eds handbook statistics vol 
pp 

north holland amsterdam 
bryant williamson 
maximum likelihood classification comparison approaches 

eds classification tool research pp 

elsevier science amsterdam 
bock 
probability models partitional cluster analysis 
comput 
stat 
data anal 
bock 
probability models partitional cluster analysis 

eds developments data analysis pp 

fdv ljubljana slovenia 

clustering criteria multivariate normal mixtures 
biometrics 
anderson 
normal mixtures number clusters problem 
comput 
stat 
quart 
bezdek 
pattern recognition fuzzy objective function algorithms 
plenum new york 
diday 
classification avec distances 
comptes acad 
sci 
paris 
cheeseman stutz 
bayesian classification autoclass theory results 
fayyad 
eds advances knowledge discovery data mining pp 

aaai press 
stutz cheeseman 
autoclass bayesian approach classification 
skilling 
eds maximum entropy bayesian methods cambridge 
kluwer dordrecht 
mclachlan peel 
algorithm automatic fitting testing normal mixtures 
proc 
th int 
conf 
pattern recognition vol 
pp 

ieee computer society 
peel mclachlan 
user guide version 
university queensland australia 
chickering heckerman 
efficient approximations marginal likelihood bayesian networks hidden variables 
machine learning 
mclachlan peel 
robust cluster analysis mixtures multivariate distributions 
amin 
eds lecture notes computer science vol 
pp 

springer berlin 
