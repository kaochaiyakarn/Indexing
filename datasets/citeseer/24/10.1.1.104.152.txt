sprint scalable parallel classifier data mining john shafer agrawal mehta classification important data mining problem 
classification studied problem current classi fication algorithms require por tion entire dataset remain memory 
limits suitability mining large databases 
new decision tree classification algo rithm called sprint removes memory restrictions fast scalable 
algorithm designed easily parallelized allowing processors build single consistent model 
parallelization exhibits excellent scalability 
combination characteristics proposed algorithm ideal tool data min ing 
classification identified important problem emerging field data mining 
classification studied problem see excellent overviews focus algorithms handle large databases 
intuition classifying larger datasets department computer science university wis madison 
permission copy fee part material granted provided copies distributed direct commercial advantage vldb copyright notice title publication date appear notice copying permission large data base endowment copy republish requires fee special permission endowment 
proceedings nd vldb conference mumbai bombay india ibm almaden research center harry road san jose ca able improve accuracy classifi cation model 
hypothesis studied confirmed 
classification set example records called training set record con sists fields attributes 
attributes ei ther continuous coming ordered domain categorical coming unordered domain 
attributes called attribute class example belongs 
objective classification build model classifying attribute attributes 
shows sample training set record represents car insurance applicant 
interested building model applicant high low insurance risk 
model built determine class fu ture unclassified records 
applications classification arise diverse fields retail target market ing customer retention fraud detection medical diagnosis 
classification models proposed years neural networks statistical models linear quadratic discriminants deci sion trees genetic models 
models decision trees particularly suited data mining 
trees constructed rel fast compared methods 
ad vantage decision tree models simple easy understand 
trees eas ily converted sql statements access databases efficiently 
decision tree classifiers obtain similar better accu racy compared classification methods 
focused building scalable parallelizable decision tree classifier 
decision tree class discriminator recur partitions training set partition consists entirely examples class 
non leaf node tree contains split point test attributes rid age training set high low decision tree car insurance example determines data partitioned 
shows sample decision tree classifier training set shown la 
age sports split points partition records high low risk classes 
decision tree screen insurance applicants classifying high low risk categories 
random sampling handle large datasets building classifier 
previous building tree classifiers large datasets includes catlett study methods improv ing time taken develop classifier 
method data sampling node de cision tree second discretized continuous tributes 
catlett considered datasets fit memory largest training data examples 
chan considered partitioning data subsets fit memory developing classifier subset parallel 
output multiple classifiers combined various algorithms reach final classification 
studies showed approach reduces running time significantly mul tiple classifiers achieve accuracy single classifier built data 
incremental learning methods data classified batches studied 
cumulative cost classifying data incrementally exceed cost classifying entire training set 
classifier built database consider ations size training set overlooked 
stead focus building classifier database indices improve retrieval efficiency classifying test data 
examined parallelizing decision tree classifier id serial classifier 
id assumes entire dataset fit real memory address issues disk algorithms re quire processor communication evaluate split point limiting number possible partition ing schemes algorithms efficiently consider leaf 
darwin toolkit thinking ma chines contained parallel implementation decision tree classifier cart details parallelization available published liter 
proposed sliq classification algorithm addressed issues building fast scalable classifier 
sliq gracefully handles disk resident data large fit memory 
small memory sized datasets obtained sampling partitioning builds single decision tree entire training set 
sliq require data record stay memory resident time 
size memory data struc ture grows direct proportion number input records limits amount data classified sliq 
decision tree clas algorithm called removes memory restrictions fast scalable 
algorithm designed easily par 
measurements parallel implementa tion shared ibm system sp show sprint excellent scaleup speedup properties 
combination characteristics sprint ideal tool data mining 
rest organized follows section discuss issues building decision trees serial sprint algorithm 
section describes parallelization sprint approaches parallelizing sliq 
section give performance evaluation serial parallel gorithms measurements implementa tion sp 
conclude summary sec tion 
expanded version available 
serial algorithm decision tree classifier built phases growth phase prune phase 
growth phase tree built recursively partitioning sprint stands scalable parallelizable decision trees 
data partition pure mem bers belong class sufficiently small parameter set user 
process shown 
form split partition data depends type attribute split 
splits continuous attribute form value value domain splits categorical attribute form value domain 
con sider binary splits usually lead accurate trees techniques extended handle multi way splits 
tree fully grown pruned second phase generalize tree removing dependence sta tistical noise variation may particular training set 
tree growth phase computationally expensive pruning data scanned multiple times part computation 
ing requires access fully grown decision tree 
experience previous sliq pruning phase typically takes total time needed build clas focus tree growth phase 
pruning algorithm sliq minimum description length principle 
partition data points class return attribute evaluate splits attribute best split partition sz partition partition initial call partition general tree growth algorithm major issues critical formance implications tree growth phase 
find split points define node tests 

having chosen split point partition data 
known cart classifiers example grow trees depth repeatedly sort data node tree arrive best splits numeric attributes 
sliq hand replaces repeated sorting time sort separate lists attribute see details 
sliq uses data structure called class list remain memory resident times 
size structure proportional number example attribute lists input records limits number input records sliq handle 
sprint addresses issues differently previous algorithms restriction size input fast algorithm 
shares sliq advantage time sort uses differ ent data structures 
particular structure class list grows size input needs memory resident 
discuss dif ferences sliq sprint section described sprint 
data structures attribute lists sprint initially creates attribute list tribute data see 
entries lists call attribute records consist tribute value class label index record tid value obtained 
initial lists continuous attributes sorted attribute value created 
entire data fit memory attribute lists maintained disk 
initial lists created training set associated root classification tree 
tree grown nodes split create new children attribute lists belonging node partitioned associated children 
list partitioned order records list preserved partitioned lists require resorting 
shows process pictorially 
histograms continuous attributes histograms associ ated decision tree node con splitting 
histograms denoted capture class distribution attribute records node 
see low maintains distribution attribute records processed maintains 
categorical attributes histogram asso ciated node 
histogram needed contains class distribution value attribute 
call histogram count nat 
lists node attribute lisa node high lists node class tid sports high family low buck low splitting node attribute lists attribute lists processed time memory required set histograms 
figures show example histograms 
finding split points growing tree goal node de termine split point best divides train ing records belonging leaf 
value split point depends separates classes 
splitting indices proposed past evaluate goodness split 
gini index originally proposed experience sliq 
data set con taining examples classes gini defined gini cp pj relative frequency class split divides subsets sr sz index divided data gini ut gini ut si gini 
advantage index calculation requires distribution class values partitions 
find best split point node scan node attribute lists evaluate splits attribute 
attribute containing split point lowest value gini index split node 
discuss split points evaluated attribute list 
continuous attributes continuous attributes candidate split points mid points consecutive tribute values training data 
determining split attribute node histogram low initialized zeros attribute list position cursor scan age class high tid position high high low position high il low position cursor position cursor low position cursor pos state class histograms io evaluating continuous split points ized class distribution records node 
root node distribution ob tained time sorting 
nodes distribution obtained node created dis cussed section 
attribute records read time low updated record read 
shows schematic histogram update 
record read split values tribute records seen eval 
note neces sary information compute gini index 
ice lists continuous attributes kept order candidate split points attribute evaluated single sequential scan correspond ing attribute list 
winning split point scan saved low histograms deallocated processing attribute 
categorical attributes categorical split points single scan attribute list collecting counts count matrix combination class label tribute value data 
sample count matrix data scan shown 
finished scan consider subsets attribute values possible split points com pute corresponding gini index 
cardinality attribute certain threshold greedy gorithm initially proposed ind subsetting 
important point infor mation required computing gini index subset splitting available count matrix 
memory allocated count matrix re claimed splits corresponding attribute evaluated 
attribute list car type class tid family high sports high sports high family low truck low family high count matrix evaluating categorical split points performing split best split point node execute split creating child nodes ing attribute records 
requires splitting node lists attribute see illustration 
partitioning tribute list winning attribute attribute winning split point age exam ple straightforward 
scan list apply split test move records new attribute lists new child 
unfortunately remaining attribute lists node example test apply attribute values decide divide records 
rids 
partition list splitting tribute age insert rids record probe structure hash table noting child record moved 
collected rids scan lists remaining attributes probe hash table rid record 
retrieved information tells child place record 
hash table large memory splitting done step 
attribute list splitting attribute partitioned upto attribute record hash table fit memory portions attribute lists non splitting attributes partitioned process repeated re attribute list splitting attribute 
hash table fit memory quite nodes lower levels tree simple optimiza tion possible 
build hash table rids smaller children 
relative sizes children determined time split point evaluated 
splitting operation build class file creation usually expensive operation solution require creation new files new attribute list 
details optimization 
attribute class lists sliq histograms new leaf 
stated earlier histograms initialize histograms evaluating continuous split points pass 
comparison sliq technique creating separate attribute lists original data proposed sliq gorithm 
sliq entry attribute list consists attribute value rid class labels kept separate data structure called class list indexed rid 
addition class label entry class list contains pointer node classification tree node corresponding data record currently belongs 
list attribute 
illustrates data struc tures 
advantage having separate sets tribute lists node sliq rewrite lists split 
reassignment records new nodes done 
simply changing tree pointer field corresponding class list entry 
class list randomly accessed frequently updated stay memory time suffer severe performance degradations 
size list grows direct proportion training set size 
ultimately limits size training set sliq handle 
goal designing sprint form sliq datasets class list fit memory 
purpose algorithm develop accurate classifier datasets sim ply large algorithm able develop classifier efficiently 
sprint designed easily parallelizable see section 
parallelizing classification turn problem building classification trees parallel 
focus growth phase due da intensive nature 
pruning phase easily done line serial processor computationally inexpensive requires access decision tree grown training phase 
parallel tree growth primary problems re main finding split points partitioning data discovered split points 
par algorithm issues data place ment workload balancing consid ered 
fortunately issues easily resolved sprint algorithm 
sprint specifically de signed remove dependence data structures centralized memory resident cause design goals sprint quite naturally efficiently 
section parallelize sprint 
comparison discuss sliq 
algorithms assume shared paral lel environment processors private memory disks 
processors connected communication network communicate passing messages 
examples parallel machines include gamma teradata ibm sp 
data placement workload balancing recall main data structures sprint attribute lists class histograms 
sprint achieves uniform data placement load balancing distributing attribute lists evenly processors shared machine 
allows processor total data 
partitioning achieved distributing training set examples equally processors 
processor generates attribute list partitions parallel projecting attribute training set examples assigned 
lists categorical attributes evenly partitioned require processing 
contin uous attribute lists sorted tioned contiguous sorted sections 
parallel sorting algorithm 
result sorting operation processor gets fairly equal sized sorted sections attribute list 
shows example initial distribution lists processor configuration 
finding split points finding split points parallel sprint simi lar serial algorithm 
serial version pro cessors scan attribute lists evaluating split points continuous attributes collecting distri bution counts categorical attributes 
change parallel algorithm extra communication required processor scanning attribute list partitions 
get full advantage having processors simultaneously processor car type class rid family high sports high sports high processor car type class rid family low truck low family high parallel data placement independently processing total data 
differences serial parallel algorithms appear attribute list parti tions scanned 
continuous attributes continuous attributes parallel version sprint differs serial version ini low bove class histograms 
parallel environment processor separate contiguous section global attribute list 
processor histograms initialized reflect fact sections attribute list processors 
specifically initially reflect class distribution sections attribute list assigned processors lower rank 
histograms likewise initially reflect class distribution local sec tion sections assigned processors higher rank 
serial version statistics gathered attribute lists new leaves created 
collecting statistics information exchanged processors stored leaf initialize leaf cb vr class histograms 
attribute list sections leaf processed processor con best split leaf 
processors communicate determine split points lowest cost 
categorical attributes categorical attributes difference serial parallel versions arises attribute list section scanned build count matrix leaf 
count matrix built processor local information exchange matrices get global counts 
done choosing coordinator collect count matrices processor 
coordinator process sums local matrices get global count matrix 
serial algorithm global matrix find best split categorical attribute 
performing splits having determined winning split points splitting attribute lists leaf nearly identical serial algorithm processor responsible splitting attribute list partitions 
additional step building probe struc ture need collect rids pro cessors 
recall processor attribute records belonging leaf 
partition ing list leaf splitting attribute rids col lected scan exchanged processors 
exchange processor contin ues independently constructing probe structure rids split leaf remaining attribute lists 
needed parallelize sprint algorithm 
design sprint require complex see section scales quite nicely 
parallelizing sliq attribute lists sliq partitioned evenly multiple processors done parallel sprint 
parallelization sliq com centralized memory resident data structure class list 
class list requires random access frequent updating parallel algorithms sliq require class list kept memory resident 
leads primary approaches parallelizing sliq class list replicated memory processor distributed pro cessor memory holds portion entire list 
replicated class list approach call sliq class list entire training set replicated lo cal memory processor 
split points eval manner parallel sprint exchanging count matrices properly ing class histograms 
partitioning attribute lists chosen split point different 
performing splits requires updating class list training example 
processor maintain consistent copy entire class list class list update communicated applied processor 
time part tree growth increase size training set amount data node remains fixed 
sliq split point evalua tion class list updates suffers drawback sliq size training set lim memory size single processor 
processor full copy class list sliq efficiently process training set class list entire database fit memory processor 
true regardless number processors 
distributed class list second approach parallelizing sliq called sliq helps relieve sliq memory constraints partitioning class list multiprocessor 
processor contains nth class list 
note partitioning class list correlation partitioning continu ous attribute lists class label corresponding attribute value reside different processor 
implies communication required look non local class label 
class list cre ated original partitioned training set perfectly correlated categorical attribute lists 
communication required continuous attributes 
scenario sliq high communica tion costs evaluating continuous split points 
attribute list scanned need look corresponding class label tree pointer tribute value 
implies processor require communication data 
processor service lookup requests processors middle scanning attribute lists 
sliq implementation reduces communication costs batching look ups class lists extra computation processor performs requesting servicing remote look ups class list high 
sliq incurs similar communication costs class list updated partitioning data best splits 
performance evaluation primary metric evaluating classifier perfor mance class cation accuracy percentage test samples correctly classified 
important metrics class cation time size decision tree 
ideal goal decision tree classifier produce compact accurate trees short classification time 
data structures tree grown different sprint sliq consider types splits node identical splitting index gini index 
gorithms produce identical trees dataset provided sliq handle dataset 
sprint uses sliq pruning method final trees obtained algorithms identical 
accuracy tree characteristics sprint identical sliq 
detailed son sliq accuracy execution time tree sire cart predecessor available 
performance evalua tion shows compared classifiers sliq achieves comparable better classification accuracy produces small decision trees small exe cution times 
focus classi fication time metric performance evaluation 
datasets benchmark classification statlog lg largest dataset contains training examples 
due lack classification benchmark containing large datasets synthetic database proposed experiments 
record synthetic database consists attributes shown table 
classification functions pro posed produce databases distributions varying complexities 
results function 
function results fairly small decision trees function pro duces large trees 
functions divide database classes group group shows predicates group shown function 
function group age salary look age salary age salary function group disposable disposable salary commission loan classification functions synthetic data table description attributes synthetic data uniformly distributed examples millions response times serial algorithms serial performance serial analysis compare response times serial sprint sliq training sets vari ous sizes 
compare algorithm sliq shown sliq cases outperforms popular decision tree classi 
disk resident datasets exploring sliq viable algo rithm 
experiments conducted ibm rs workstation running aix level 
cpu clock rate mhz mb main memory 
apart standard unix daemons system processes experiments run idle system 
training sets ranging size records records 
range selected examine sprint performs operating regions sliq run 
results shown databases generated function 
results encouraging 
expected data sizes class list fit memory sprint somewhat slower sliq 
oper ating region sprint rewriting dataset sliq memory updates class list 
surprising region sprint comes quite close sliq 
soon cross input size threshold records system configuration sliq starts thrashing sprint continues exhibit nearly linear scaleup 
parallel performance examine sprint algorithm performs parallel environments implemented paral ibm sp standard mpi communication primitives lo 
mpi allows implementation completely portable processors response times parallel algorithms shared parallel architectures including station clusters 
experiments conducted node ibm sp model 
node mul node consisting power pro cessor running mhz mb real mem ory 
attached node disk stored datasets 
processors run aix level communicate high performance switch hps tb adaptors 
see sp hardware details 
due available disk space smaller available memory prevented running experiments attribute lists forced disk 
results costs scale linearly sprint smaller fraction execution time 
costs may scale exaggerated 
comparison parallel algorithms compare parallel sprint paral sliq 
experiments proces sor contained training examples num ber processors varied 
total training set size ranges records records 
response times gorithm shown 
get de tailed understanding algorithm performance show breakdown total response time time spent discovering split points time spent partitioning data split points 
immediately obvious poorly sliq forms relative sliq sprint 
com munication costs distributed class list time spent servicing class list requests pro cessors extremely high sliq probably attractive algorithm despite time total real time measured start program termination 
ability handle training sets large sliq sliq shown sliq pays high penalty components tree growth split point discovery data partitioning scales quite poorly 
sprint performs better sliq terms response times scalability 
algorithms finding best split points takes roughly constant time amount data processor remains fixed problem size increased 
increase response times time spent partitioning data 
sprint shows slight increase cost building rid hashtables split attribute lists 
hash tables may potentially contain rids tuples belonging particular leaf node cost increases data size 
sliq performs worse sprint processor sliq communicate apply class list updates training example 
problem size increase number updates processor perform 
sprint may perform communication sliq requires processors update local records 
rest section examines scalability speedup characteristics greater detail 
sprint scaleup set sensitivity experiments pro cessor fixed number training examples examined sprint performance configuration changed processors 
studied scaleup experiments thou sand examples processor 
results runs shown 
amount data processor change experiment response times ideally remain constant configuration size increased 
results show nice scaleup 
drop scaleup due time needed build sprint rid hash tables 
amount local data proces sor remains constant size hash tables 
rid hash tables grow direct proportion total training set size 
conclude parallel sprint 
classify large datasets 
speedup examined speedup characteristics sprint 
kept total training set constant changed processor configuration 
training set sizes ex amples 
results speedup experiments total time spent discovering splits total time spent partitioning data 
processors processors total response time processors total response time examples examples breakdown response times relative response time scaleup sprint processors relative response time ideal examples processors speedup sprint total response time relative response time iij examples processor thousands examples processor thousands shown 
due limited disk space processor configuration create dataset containing examples 
ex pected speedup performance improves larger datasets 
small datasets communication significant factor response time 
especially true configuration sizes point tens examples processor 
factor limiting speedup performance rid hash tables 
hash tables size regardless processor configuration 
building hash tables requires constant amount time processors 
experiments show get nice speed sprint results improving larger datasets 
experiments examine sprint forms fixed processor configuration crease size dataset 
shows different processor configurations processor training set size increased thou sand examples 
sprint exhibits results better ideal processing twice data require twice process ing time 
reason communication costs exchanging split points count matrices change training set size increased 
doubling training set size doubles response costs remain unaffected 
result superior performance 
emergence field data min ing great need algorithms building classifiers handle large databases 
sprint proposed sliq algorithm ad dress concerns 
unfortunately due memory resident data structure scales size training set sliq upper limit number records process 
new classification gorithm called sprint removes memory re limit existing decision tree algorithms exhibits excellent scaling behavior sliq 
need centralized memory resident data structures sprint efficiently allows classification virtually sized dataset 
design goals included requirement algorithm easily efficiently parallelizable 
sprint efficient parallelization re quires additions serial algorithm 
measurements actual implementations algorithms showed sprint algorithm serial parallel environ ments 
uniprocessor sprint exhibits execu tion times compete favorably sliq 
showed sprint handles datasets large sliq handle 
sprint scales nicely size dataset large problem regions decision tree classifier compete 
implementation sp shared mul showed sprint paral efficiently 
outperforms parallel im sliq terms execution time scalability parallel sprint efficiency improves problem size increases 
excellent scaleup speedup characteristics 
sliq somewhat superior performance problem regions class list fit mem ory envision hybrid algorithm combining sprint sliq algorithm initially run sprint point reached class list constructed kept real memory 
point algorithm switch sprint sliq exploiting advantages algorithm operating regions intended 
amount memory needed build class list easily calculated switch point difficult determine 
plan build hybrid algorithm 
pi pi breiman friedman olshen stone 
classification regression trees 
wadsworth belmont 
pi philip chan salvatore stolfo 
experiments multistrategy learning met learning 
proc 
second intl 
conference info 
knowledge mgmt pages 
pi pi pi po rakesh agrawal ghosh tomasz imielinski bala iyer arun swami 
interval classifier database mining applications 

vldb conference pages vancouver british columbia canada august 
rakesh agrawal tomasz imielinski arun swami 
database mining performance perspective 
ieee transactions knowledge data engineering december 
jason catlett 
machine learning large databases 
phd thesis university sydney 
philip chan salvatore stolfo 
meta learning multistrategy parallel learning 

second intl 
workshop multistrategy learning pages 
dewitt ghandeharizadeh schneider 
hsiao rasmussen 
gamma database machine project 
ieee transactions knowledge data engineering pages march 
dewitt naughton schneider 
parallel sorting shared architecture probabilistic splitting 
proc 
st int conf 
parallel distributed information systems pages december 

distributed tree construction large datasets 
bachelor thesis australian national university 
message passing interface forum 
mpi message passing interface standard may 
wi james 
algorithms 
wiley 
mehta rakesh agrawal rissanen 
sliq fast scalable classifier data mining 

fifth int conference extending database technology edbt avignon france march 
wi wi wi wi wi goldberg 
genetic algorithms search optimization machine learning 
morgan kaufmann 
int business machines 
scalable po systems ga edition february 
lippmann 
computing neural nets 
ieee 
assp magazine april michie spiegelhalter taylor 
machine learning neural statistical classi fication 
ellis horwood 
nasa ames research center 
ind version ga edition 
quinlan 
induction large databases 
technical report stan cs university 
ross quinlan 
induction decision trees 
ma chine learning 
ross quinlan 
programs machine learning 
morgan kaufman 
rissanen 
stochastic complexity statistical inquiry 
world scientific publ 

john shafer rakesh agrawal mehta 
sprint scalable parallel classifier data mining 
research report ibm almaden research center san jose california 
available www almaden ibm com cs quest 
teradata dbc data base computer system manual clo release edition november 
weiss kulikowski 
computer systems learn classification prediction methods statistics neural nets machine learning expert systems 
morgan kaufman 
wirth catlett 
experiments costs benefits windowing id 
th int conference machine learning 
