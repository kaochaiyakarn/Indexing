fast incremental maintenance approximate histograms phillip gibbons yossi matias viswanath poosala bell laboratories mountain avenue murray hill nj gibbons matias poosala research bell commercial database systems maintain summarize contents large re lations permit efficient estimation query result sizes query optimizers 
delay ing propagation database updates histogram introduces errors estima tion 
presents new sampling ap proaches incremental maintenance approx histograms 
scheduling updates histogram updates database techniques maintain histograms effectively date times avoid com puting overheads unnecessary 
tech niques provide highly accurate approximate belonging equi depth com pressed classes 
experimental results show new approaches provide orders magni tude accurate estimation previous ap proaches 
important aspect employed new ap proaches backing sample date ran dom sample tuples currently relation 
provide efficient solutions maintaining uniformly random sample relation pres ence updates relation 
backing sam ple techniques applica tion relies random samples data 
dbmss maintain variety statistics contents database relations order estimate various quantities selectivities cost query optimizers 
statistics typically approximate permission copy part material granted provided copies direct commercial advantage copyright notice title af publication date appear notice copying permission large data base endowment 
copy republish requires fee special permission endowment 
proceedings rd vldb conference athens greece distribution data attributes various database relations 
established validity optimizer decisions may critically affected quality approximations 
particularly evident context increasingly complex queries data analysis queries 
probably common technique practice selectivity estimation maintaining histograms frequency distribution attribute 
histogram groups attribute values buckets subsets approximates true attribute values frequencies summary statistics maintained bucket real world databases exist histograms produce low error estimates occupying reasonably small space order bytes catalog 
histograms db informix ingres oracle microsoft sql server sybase teradata 
areas parallel join load balancing estimates 
histograms usually precomputed underlying data additional overheads inside query optimizer 
drawback precomputed histograms may get outdated data database modified introduce significant errors estimations 
hand clearly impractical compute new histogram update database 
fortunately necessary keep histograms perfectly date times provide reasonably accurate estimates typically 
needs appropriate schedules algorithms propagating updates histograms database performance affected 
despite popularity histograms issues related maintenance studied literature 
far focused proper values order enhance accuracy histograms assumed database modified 
earlier introduced classes histograms offer high accuracy various estimation problems 
provided efficient methods construct various histograms ignored problem maintaining histograms 
general context view histograms materialized views different aspects 
utilization typically maintained main memory im plies constraints space 
second need maintained approximately con sidered cached approximate materialized views 
aware prior approximate materialized views 
approach date histogram updates followed nearly commercial systems recompute histograms periodically night 
approach disadvantages 
significant dates data recomputations cause poor estimations optimizer 
second recomputed scratch discarding old histograms recomputation phase entire database computationally intensive 
fast effective procedures maintaining histogram classes extensively dbmss equi depth histograms dbmss compressed histograms db 
key component approach introduce notion backing sample random sample tuples relation kept date presence updates 
demonstrate important advantages gained backing sample algorithms maintenance 
conducted extensive set experi ments studying techniques confirm theoret ical findings show small amount additional storage cpu resources techniques maintain nearly date times 
due limited space proofs theorems technical details omitted full version 
histograms maintenance domain attribute set possible values value set relation set values vi vj frequency fi vi number tuples data distribution set pairs vi 
fpl attribute constructed partitioning mutually disjoint subsets called buckets approximating values frequencies bucket common fashion 
typically bucket assumed contain values smallest largest values bucket bucket range just re equi distant values range actual number values bucket 
known continuous value assumption known uniform spread assumption 
fn number tuples value bucket frequencies values bucket approximated averages different classes histograms obtained different rules partitioning values buckets 
focus important classes histograms equi depth compressed simply called compressed classes 
equi depth equi height histogram contiguous ranges attribute val ues grouped buckets number tuples fb bucket 
compressed highest frequencies stored separately singleton buckets rest partitioned equi depth histogram 
target compressed histogram value adapts data distribution ensure singleton bucket fit equi depth bucket single value spans equi depth bucket 
shown earlier compressed histograms effective approximating distributions low high skew 
equi depth histograms form nearly commercial systems db uses accurate compressed histograms 
histogram storage 
equi depth compressed histograms store bucket largest value bucket maxval count equals approximates fb 
histograms estimate range selectivities apply uniform spread assumption singleton buckets continuous value assumption equi depth buckets 
approximate histograms class histogram histogram attribute relation modified accuracy affected ways may longer cor rect histogram updated data class error may contain inaccurate information distribution error 
attribute approximate histogram pro vides approximation actual histogram relation 
quality approximation evaluated various error metrics defined class distribution errors 
error metric 
example distribution error metric relevant histogram types reflects accuracy counts associated bucket 
example modified histogram may buckets count fb difference fb count approximation error consider error metric defined follows bi count number tuples number buckets 
standard deviation bucket counts actual number elements bucket normalized respect mean bucket count 
incremental histogram maintenance components incremental approach maintaining backing sample ii framework maintaining approximate histogram performs program instructions response update database detects histogram need adjustment bucket boundaries 
adjustments backing sample 
fundamental distinction backing sample histogram supports histogram accessed far frequently sample uses memory stored main memory sample stored disk 
shows typical sizes various entities relevant discussion 
typical sizes various entities incremental histogram maintenance previously studied important case high biased compressed histogram buck ets devoted frequent values bucket devoted remaining values 
algorithm approach described example backing sample maintained 
backing sample backing sample uniform random sample tuples relation kept date presence updates relation 
argue maintaining backing sample useful histogram computation selectivity estimation sampling estimation techniques sample size needed entire relation scanned extract sample random disk blocks read 
case tuples disk block may highly correlated obtain truly random sample disk blocks read 
contrast backing sample stored consecutive disk blocks scanned reading sequential disk blocks 
tuple sample unique row id reduce overhead approach program instructions performed random sample database updates discussed section 
attribute interest retained 
entire sample stored disk blocks faster retrieval 
indexing structure sample held enable quick access sample values certain range 
time backing sample relation needs equivalent random sample size extracted time 
section techniques maintaining provably random backing sample sequence updates accessing infrequently accessed update sequence deletes half tuples 
backing sample maintained relation consider insertions technique maintaining simple random sample presence inserts reservoir sampling technique due vitter lo 
algorithm proceeds inserting tuples reservoir random number records skipped tuple replaces randomly selected tuple reservoir 
random number records skipped forth record scanned 
distribution function length random skip depends explicitly number tuples scanned far chosen tuple relation equally reservoir tuple scanned 
treating tuple inserted relation tuple scan relation essentially obtain sample data presence insertions 
extensions handle modify delete operations 
extend vitter algorithm handle modify delete operations follows 
modify operations handled updating value field element sample 
delete operations handled removing element sample sample 
deletions decrease size sample target size known subsequent insertions obtain provably random sample size sample dropped maintain sample size initially prespecified upper bound allow decrease result deletions sample items prespecified lower bound sample size drops relation re populate random sample 
full show high probability occur databases infrequent deletions 
worst case deletions frequent cost amortized cost large number deletions high probability occur necessary 
optimizations 
overheads algorithm lowered hash table row ids sample test id presence batching deletions possible data warehouses batch dis card oldest data prior loading newest data 
algorithm maintains random sample independent order updates database rearrange order date sample required ap plication sample 
lazy processing modify delete operations operations simply placed buffer processed batch buffer full date sample needed 
likewise postpone processing modify delete operations insert selected show procedure maintains random sample overheads clear algorithm best suited insert databases data warehousing environments 
fast maintenance approximate equi depth histograms standard algorithm constructing exact equi depth histogram sorts tuples relation tribute value selects ln pj th tuple 
large relations algorithm quite slow sorting may involve multiple scans relation 
approximate equi depth histogram approximates exact histogram relaxing requirement number tuples bucket accuracy counts 
histograms evaluated close buckets tuples close counts actual number tuples respective buckets 
class error metric equi depth histograms 
con sider approximate equi depth histogram buckets relation tuples 
consider error metric reflects extent histogram bucket boundaries succeed evenly dividing tuples re lation standard deviation buckets sizes mean bucket size normalized respect mean bucket size 
computing approximate equi depth histograms random sample 
random sample approximate equi depth histogram computed constructing equi depth histogram sample setting bucket counts 
denote sample compute algorithm 
incremental algorithm oc uses 
accuracy approximate histogram maintained incremental gorithm depends accuracy resulting pro cedure stated theorem 
statement theorem terms sample size theorem 
ln 
random sample size values drawn set size replacement 
sample compute computes approx equi depth histogram probability fi ped cx 
maintaining equi depth histograms back ing sample devise algorithm monitors accuracy histogram recomputes histogram backing sample approximation error exceeds pre specified tolerance parameter 
denote algorithm equi algorithm 
assume backing sample maintained algorithm section set sample size theorems 
algorithm works phases 
phase maintain threshold number tuples relation phase tunable performance parameter 
threshold set phase 
number tuples bucket maintained threshold 
recall ideal target number bucket size new tuples added relation increment counts appropriate buckets 
count exceeds threshold entire equidepth histogram recomputed backing sample sample compute new phase started 
performance analysis 
consider accuracy algorithm show high proba bility guaranteed approximation equi depth histogram 
theorem shows error parameter remains unchanged error parameter may grow additive factor tolerance parameter 
theorem 
ln forsome 
random sample size values drawn set size replacement 
ln 
equi computes approximate equi de th histogram lemma bounds total number calls sample compute function final relation size tolerance parameter computation approximate histograms ran dom sample fixed relation considered past aware similar analysis 
lemma rf total tuples inserted number calls sam ple compute min lg 
split merge algorithm section modify previous algorithm order reduce number calls sample compute trying balance buckets local inexpensive procedure resorting sample compute 
bucket count reaches threshold split bucket half recomputing entire histogram backing sample 
order maintain number buckets fixed merge adjacent buckets total count exceed pair buckets 
merge possible recompute backing sample 
define phase sequence operations consecutive recomputations 
denote equi algorithm 
operation merging adjacent buckets quite simple merely involves adding counts buckets boundary quantile 
splitting bucket straightforward approximate median value bucket selected serve bucket boundary new buckets backing sample 
split merge operation illustrated 
note split merge occur 
mean split merge operation equi depth histogram maintenance tolerance parameter determines re computation occurs 
consider extreme case 
equi recomputes histogram database update lri phases 
consider extreme setting ri 
algorithm simply sticks original buckets equivalent trivial algorithm employ balancing operation 
setting performance parameter gives spectrum algorithms efficient provides poor ac performance relatively accurate algorithm poor efficiency performance 
se suitable intermediate value get algorithm performance accuracy efficiency instance setting result algorithm imbalance factor bounded number phases lg 
lemma establishes bound number splits phase function 
prove range particularly interested 
lemma 
number splits occur phase 
number phases bounded follows lemma iy 
total tuples number recomputations min lg 
conclude theorem consider equi buckets performance parameter applied sequence inserts 
total number phases ig total number splits ig cy extensions handle modify delete operations consider equi algorithm 
handle deletions database extend follows 
deletions decrease number elements bucket relative buckets additional threshold te serves lower bound count bucket 
recomputation set te iri tunable parameter 
set 
consider deletion tuple bucket histogram interval contains decrement te recompute fi backing sample update te 
modify operations observe modify change value attribute changes value old value bucket new value remains unchanged 
update treating modify delete followed insert 
note presence deletions affect accuracy histogram computed backing sample 
upper lower thresholds control imbalance buckets recomputations histograms remain quite accurate 
hand number recomputations quite large worst case 
repeatedly inserting items bucket reached deleting items force algorithm perform recomputations 
sequence updates relation ri increases steady rate number recomputes bounded constant factor times bound lemma constant depends rate increase 
consider equi algorithm 
extensions handle delete operations identical outlined additions handle split merge operations 
count te merge adjacent buckets split bucket largest count long count 
note may may newly merged bucket 
exists recompute backing sample 
modify operations handled outlined 
fast maintenance approximate com pressed histograms section consider important histogram type compressed histogram 
split merge algorithm maintaining compressed histogram presence database insertions show extend algorithm handle database mod deletes 
assume backing sample maintained 
definitions 
consider relation priori unknown size equi depth histogram values high cies span number buckets waste buckets sequence spanned buckets value replaced single bucket single count 
com pressed histogram set singleton buckets equi depth histogram values singleton buck ets 
target compressed histogram buckets equi depth buckets singleton high biased buckets re hold rl equi depth bucket tuples total number tuples equi depth buckets single value spans equi depth bucket set bucket boundaries distinct conversely value singleton bucket frequency 
associated bucket maximum value maxval singleton value bucket boundary count count 
approximate compressed histogram approximates exact histogram relaxing requirements accuracy counts 
class error metrics 
consider approximate com pressed histogram equi depth buckets bi 
bpl singleton buckets bp 
bp 
recall defined number tuples bucket number tuples equi depth buckets 
define class error metrics phb defined section applied equi depth buckets set values violate requirement 
metric penalizes mistakes choice high biased buckets proportion true frequencies deviate target threshold respect threshold 
split merge algorithm compressed show equi extended handle compressed histograms 
denote algorithm compressed algorithm 
insert tuple value relation singleton equi depth bucket zi determined count incremented 
equi depth bucket equi check see count equals threshold splitting bucket update bucket boundaries 
steps updating compressed histogram similar equi address additional concerns 

new values added relation may skewed values warrant singleton buckets fore may belong singleton buckets 
threshold singleton buckets grows number tuples equi depth buckets 
values singleton buckets smaller may longer belong singleton buckets increases 
concerns number equi depth buckets grows shrinks adjust equi depth buckets accordingly 
likewise number tuples equi depth buckets grows shrinks dramatically sets tuples re moved added singleton buckets 
ideal maintain fl tuples equi depth bucket growing shrinking 
algorithm addresses concerns fol lows 
address concern fact large number updates value suitably crease count equi depth bucket containing cause bucket split 
bucket split doing creates adjacent bucket boundaries value know create new singleton bucket address concern allow singleton buckets rela tively small counts merged back equi depth buckets 
concerns procedures splitting merging buckets grow shrink number buckets maintaining approximate equi depth buckets recompute histogram 
imbalance equi depth buckets controlled thresholds te depend tunable performance parameters yl equi 
convert equi depth bucket singleton bucket vice versa ensure time bucket constant factor average number tuples equidepth bucket additional splits merges required 
average roughly maintained equi depth buckets added subtracted 
requirements bucket split buckets merged involved bucket candidate split bucket equi depth bucket count te singleton bucket tl 
pair buckets bi bj candidate merge pair adjacent equi depth buckets singleton bucket equi depth bucket singleton value belongs bi count bj count candidate split bucket candidate merge pair algorithm selects largest smallest combined resp 
bucket count 
due space limitations exact details algorithm including procedure computing approximate compressed histogram backing sample 
lemma algorithm maintains invariants 
buckets count te 
equi depth buckets count 
bucket boundaries maxval distinct 
value belongs singleton bucket equi depth bucket adjacent equi depth buckets case subsequent inserts deletes targeted adjacent buckets 
set equi depth buckets counts factor small constant indepen dent jr details full 
extensions handle modify delete operations discuss extend handle deletions database 
deletions decrease number tuples bucket relative buckets resulting singleton bucket converted equi depth bucket vice versa 
deletion drop bucket count lower threshold te 
consider deletion tuple bucket histogram interval contains decrement count 
count te 
part candidate merge pair merge pair smallest combined count split candidate split bucket largest count 
note may may newly merged bucket 
exists recompute backing sample 
likewise part candidate merge pair recompute backing sample 
insertion case conversion buckets singleton equi depth vice versa primarily handled detecting need conversions splitting merging buckets 
modify operations observe modify change value attribute changes value old value bucket new value remains unchanged 
update treating modify delete followed insert 
invariants lemma hold version algorithm incorporates extensions modify delete operations 
experimental evaluation section experimentally study effectiveness histogram maintenance techniques efficiency 
describe experiment testbed 
database 
model base data database update data independent extensive set zipf ian data distributions 
zipf distribution chosen supposedly models skew real life data quite closely 
value varied vary skew corresponds uniform distribution 
number tuples relation ook start number distinct values varied 
exact attribute values affect relative quality techniques chose integer value domain 
frequencies mapped values different orders decreasing deer increasing incr random random generating large collection data distributions 
refer zipf distribution parameter order zipf distribution 
histograms 
equi depth compressed histograms consisted buckets computed sample tuples size backing sample 
updates 
classes updates described mix inserts deletes 
case update data taken zipf distribution 
varying parameter vary skew updates 
number updates increased times relation size 

insert class updates consists just insert operations algorithms efficient environment studied detail 
warehouse class contains alternating se quence set inserts followed set deletes 
pattern common data warehouses keeping transactional information sliding time windows loading fresh data discarding old data loaded close capacity 
mixed class contains uniform mixture deletes occurring random order 
results insert class 
results remaining classes 
techniques 
studied variants old new techniques described terms op erations single insert operations delete similar principle 



fixed histogram sum frequencies bucket incremented total sum frequencies increases 
essentially technique currently nearly systems treat insertions values simply keep track number tuples 
periodic sample compute expensive tech nique requires recomputing histogram backing sample insertion sample total sum frequencies incremented technique 
class techniques correspond ing algorithms proposed 
recompute technique differs split merge performing recomputations sim ply increasing split threshold merge performed 
fixed buckets technique differs split merge attempting split bucket 
fixed histogram algorithm size bucket containing inserted value correctly incre mented 
error metrics 
error metrics es 
ped eq 
pm 
addition new metric defined captures accuracy histograms estimating result sizes range predicates form 
query set contains range predicates possible values joint value domain 
query find error percentage result size 
defined average errors query set 
effects recomputation depicts errors equi depth histogram obtained insertions function recompute techniques 
base data distribution case uniform date distribution zipf decr 
clear outperforms technique recomputations 
errors due techniques lowest low values increase rapidly increases 
low values histogram recomputed bucket sizes exceed low threshold keeping small 
hand small values result larger number disk accesses backing sample 
shows effect number recomputations 
clear small values result large number recomputations 
similar sets experiments conducted entire set data distributions con reasonable value limiting number computations decreasing errors setting remaining experiments 
update sampling nearly experiments conducted considering insertion database 
update intensive databases result intolerable performance degradation 
propose uniformly sampling updates certain probability mod histograms sampled updates 
experiment study effect update sampling probability histogram performance 
base date distributions chosen zipf incr zipf random respectively histogram compressed 
depicts errors due technique various sampling probabilities 
axis represents average number updates skipped axis represents errors incurred histogram re inputs estimating result sizes range queries range 
clear fig ure accuracy depends number updates sampled long updates skipped say experiment errors reasonably small 
approximation equi depth histograms compare effectiveness various techniques ap equi depth histograms insertions database 
results uniform base data zipf incr update data fairly consistent combinations 
figures contain various error measures axes number insertions axes 
experiment split merge technique performed just recomputations periodic sample compute performed 
cm recomputes recomputes gamma effect recom putation errors 
fixed histogram fixed buckets periodic sample camp 
errors equi depth histograms effect num ber recomputations fixed buckets periodic sample camp 
errors equi depth histograms clear technique nearly identical expensive periodic sample compute technique maintaining histogram close equi depth 
periodic sample compute technique maintain perfectly equi depth histogram recomputed backing sample may reflect insertions 
techniques clearly result poor equi depth histogram perform splits populated buckets 
shows fixed buckets techniques accurate reflecting accurate counts bucket sizes correctly updated insertion 
techniques size bucket equal nt measures identical 
clear technique offers best performance estimating range query result sizes 
approximation compressed histograms compare effectiveness various techniques maintaining approximating compressed histograms 
base data distribution zipf incr skewed distribution chosen compressed histogram contain high biased buckets update distribution zipf random introduces skew different points relation distribution 
figures depict phb errors axes respectively update period effect update sam pling 

fixed histogram fixed buckets periodic sample comp 
jb number updates errors equi depth histograms number insertions axes 
results metrics similar equi depth case consistently demonstrate accuracy technique 
split merge technique performed just recomputations sample periodic sample compute performed recomputations 
seen periodic sample compute techniques result zero errors capturing high frequency values updated relation values frequent base relation 
updates create new high frequency value techniques perform 
new value frequent clear techniques fail characterize incur high errors 
shows errors range size estimation follow similar pattern equi depth case 
expected earlier compressed histograms observed incur smaller errors equi depth histograms 
effect skew updates high skews update data vary distribution base relation data drastically require effec tive histogram maintenance techniques 
depict performance various compressed histograms phb errors compressed histograms resulting techniques insertions database 
axis contains parameter values axis contains errors estimating range query result sizes 
fixed histogram technique fails quickly assumes updates uniform update high biased part correctly 
clear technique forms consistently levels skew better techniques approximates equi depth part splits recomputations approximates high biased part dynamically detecting high frequency values 
proposed novel approach maintaining samples date presence updates database 
algorithms proposed widely equi depth histograms highly accurate class compressed histograms novel notions split merge techniques backing samples 
cpu storage requirements techniques insert databases data warehousing environments 
analytical experimental studies follows new techniques effective ing equi depth compressed histograms 
equally effective relations orders magnitude larger 
fact relation size grows relative overhead maintaining backing sample necessary performance smaller 
recomputations backing sample incurred large number updates proving split merge techniques quite effective min overheads due recomputation 
experiments clearly show histograms main tained techniques remain highly effective result size estimation current approaches 
results recommend techniques dbmss effective incremental nance approximate histograms 
fixed histogram fixed buckets periodic sample camp 
cmm number errors com pressed histograms fixed histogram fixed buckets periodic sample camp 
effect skew updates acknowledgments 
acknowledge contributions andy algorithm maintaining ap proximate equi depth histograms 
nabil sridhar rajagopalan discussions related 
christodoulakis 
implications certain assumptions database performance evaluation 
acm tods june 
gibbons matias 
space efficient maintenance top sellers lists large databases 
manuscript july 
gibbons matias poosala 
fast incremental maintenance approximate histograms 
technical report bell laboratories murray hill nj june 
ioannidis christodoulakis 
propagation errors size join results 
proc 
acm sigmod pages 

optimization queries relational databases 
phd thesis case western university sept 
poosala 
histogram estimation techniques database systems 
phd thesis univ wisconsin madison feb 
poosala ioannidis 
estimation query result dis tribution application parallel join load balancing 
proc 
nd int 
conf large databases pages sept 
poosala ioannidis haas shekita 
improved histograms selectivity estimation range predicates 
proc 
acm sigmod conj pages june 
selinger astrahan chamberlin lorie price 
access path selection relational database man agement system 
proc 
conf pages 
vitter 
random sampling reservoir 
acm trans 
math 

zipf 
human behaviour principle ort 
addison wesley reading ma 
