evaluation dimension reduction techniques class classification santiago cunningham university college dublin technical report ucd csi august th dimension reduction dr important processing data domains multimedia bioinformatics data high dimension 
dimension reduction supervised learning context posed problem clear objective discovering reduced representation data classes separated 
contrast dr unsupervised context ill posed objective clear 
successful unsupervised dr techniques principal component analysis pca exist pca pragmatic objective transforming data reduced number dimensions captures variation data 
class classification falls supervised unsupervised learning categories supervised dr techniques appear applicable class classification absence second class label training data 
evaluate number date unsupervised dr techniques class classification show techniques cluster coherence locality preservation effective 
years traditional distinction machine learning supervised unsupervised techniques blurred due emergence real world problems sit extremes 
supervised classification problems discriminating classifiers trained positive negative examples 
number practical problems counter examples rare entirely unavailable statistically unrepresentative 
problems include industrial process control text classification analysis chemical spectra 
class classifiers emerged set techniques situations labelled data exists classes classification problem 
instance industrial inspection tasks abundant data may exist describing process operating correctly 
difficult gather training data describing myriad ways system operate incorrectly 
related problem negative examples exist distribution characterised 
example reasonable provide characteristic examples family pictures impossible provide examples pictures typical non family pictures 
class classifiers emerging solution characterises target class distinguish classes 
practice class problems typically high dimension dr important pre processing step 
evaluation shows class support vector machine svm performance quite sensitive number features 
contrasts class svms generally considered robust high data dimensionality 
provides additional justification dr class classifier construction 
absence counter examples means difficult identify feature subset encodes discriminating description concept 
review range unsupervised dr techniques evaluate performance number 
find dr locality preservation cluster coherence principles particularly promising occ 
locality preservation effective irrelevant features full feature set locality original space meaningful 
section provide overview occ describe occ techniques included evaluation 
section describe dr techniques considered evaluation evaluation section 
concludes summary proposals research 
class classifiers traditionally machine learning tasks divided supervised unsupervised categories 
roughly speaking unsupervised learning provided dataset set examples describing real world concept objective uncover structure data 
supervised learning provided dataset information modeled explicitly stated form label class label case called classification problems task predict label new unseen examples 
class classification referred novelty outlier detection thought weaker form supervised classification information training examples belong class arbitrarily called positive target 
task accept reject unseen examples depending similarity known positive examples 
occ approaches consequently operate negative training examples 
words class learning handles counter example imbalanced data problems considering positive examples 
unlabeled examples small large amounts negative examples available training occ techniques fine tune performance 
current study choose different class classifiers support vector data description svdd nearest neighbours approach means clustering approach gaussian model available data description toolbox open source matlab software library class classification tools 
support vector data description svdd learns hypersphere defined center radius encloses training set covering little volume possible 
employ kernel trick learning flexible boundaries solution solving convex quadratic optimization problem analogous support vector machines 
clustering means approach class classification learning clusters modeling target class reduced set cluster prototypes centers new examples projected 
examples clustering methods self organizing map learning vector quantization means choose 
new example classified distance nearest prototype score extent outlier 
lazy learning nearest neighbours nearest neighbour approach constructing class classifiers 
training data stored criterion calculated new examples nearest neighbours position relative seen examples 
criteria proposed measure example 
average distances nearest neighbours 
density estimation gaussian model gaussian model simple parametric class classifier models training data assumption comes unimodal multivariate normal distribution 
assumptions fit lot natural processes violated model introduces large bias 
mean covariance matrix estimated expectation maximization approach mahalanobis distance resemblance criterion 
selected occ strategies conceptual simplicity established properties wish explore hypothesis local learners particularly suited class problems 
bar svdd local learners 
dimension reduction techniques research dimension reduction dimensions 
design decision select subset existing features transform new reduced set features 
dimension dr strategies differ question learning process supervised unsupervised see 
occ problems feature selection feature transformation strategies relevant 
labelled data available class supervised dr techniques directly applied occ problems 
supervised learning objective dr optimize performance final system minimize classification error 
class classification performance estimation difficult absence counterexamples estimation false positive rate hard 
difficult tune bias unsupervised supervised unsupervised dr best done find representation maximises variance data 
data labelled representation sought improves separation data 
classifier best strategy address problem depends specifics data available 
sensible approach try synchronize assumptions dr classification 
evaluation consider dr techniques principal component analysis algorithm wolf shashua 
final principle locality preservation described section 
believe locality preservation particular relevance dr occ domain usually class classifiers rely local neighbourhood relationships see section 
principal component analysis pca pca commonly technique unsupervised dimensionality reduction 
aims finding linear projections best capture variability data 
study common approach keeping directions explains variance 
shown retaining high variance dimensions optimal class classification minor components analysis smallest variance directions better circumstances 
algorithm motivated criterion cluster quality cluster coherence graph theoretic terms expressed notion objects clusters connected individual clusters weakly linked 
area spectral clustering captures ideas founded family clustering algorithms idea minimising graph cut clusters 
principles spectral clustering extended wolf shashua produce algorithm simultaneously performs feature subset selection discovers partition data 
spectral clustering fundamental data structure affinity matrix entry aij captures similarity typically dot product data points order facilitate feature selection affinity matrix expressed mi th feature vector data matrix normalised centered unit norm set values data set feature 
mim outer product mi 
weight vector features ultimately objective weight terms set 
spectral clustering matrix composed eigenvectors corresponding largest eigenvalues 
wolf shashua show relevance feature subset defined weight vector quantified rel trace show feature selection clustering performed single process optimising max trace qt subject wolf shashua show solved solving inter linked eigenvalue problems produce solutions show process iteratively solving fixing solving converge 
show process convenient property weights biased positive sparse zero 
algorithm performs feature selection spirit spectral clustering discovers feature subset support partitioning data clusters separated graph cut criterion 
locality preservation locality preservation dimensionality reduction techniques refers aim keeping neighbourhood properties objects close input space close reduced space 
linear nonlinear techniques exploiting criterion proposed 
occ problem rational think locality preserving dimension reduction technique practical cases global 
locality density frequently occ literature locality preservation bias 
locality preserving projections lpp idea lpp finding subspaces preserve local structure data 
matrix symmetric positive invertible usually sparse captures information relationships data points example similarity neighbourhood lpp finds optimal linear embedding respects structure matrix 
lpp preserves cluster structures clustering locality means algorithm renders attractive quality cluster analysis 
details lpp described algorithm 
embedding defined bottom eigenvectors solution equation 
construction weighted graph second steps algorithm accomplished criterion 
permits utilitarian approaches including priori information problem 
laplacian score feature selection ls criterion locality preservation lpp applied feature selection context merit feature measured locality preservation power 
explicit enumeration feature subsets 
nearest neighbour graph constructed training set spectrum analysed rank variable 
steps algorithm identical lpp algorithm 
ranking feature laplacian score computed 
th feature define laplacian score lsi th feature evaluation mi mi mti lsi mt mi mi study biomedical datasets summarized table 
difficulty assessing performance combination occ dr techniques datasets parameter optimisation model selection required different techniques 
followed simple approach fixing values parameters sensible values number clusters means neighbours nn rest left dd tools default values 
computing threshold resemblance function set training objects accepted consider training examples outliers 
parameters dimension reduction techniques applicable follow principle parameters classifiers 
example value nearest neighbour constructing adjacency graph lpp ls value means algorithm set target number clusters 
lpp ls simple minded weighting approach followed 
model selection done fix rest parameters priori defined default values 
choice target dimensionality important issue 
case just explored possibilities dimensionality original dimensionality feature selection maximum defined feature transformation embedding 
addition techniques described section add dimension reduction methods random feature selection process provide baseline supervised ranking features information gain original multiclass datasets cheating 
way try establish supervised feature selection criterion provides upper bound accuracy dimension reduction system class tasks 
algorithm lpp computation construct adjacency graph training set denote graph nodes 
put edge nodes xi xj close 
variations neighbourhoods parameter 
nodes connected xi xj norm usual euclidean norm dx nearest neighbours parameter 
nodes connected nearest neighbours viceversa 
choose weights graph edges variations weighting edges 
sparse symmetric matrix aij having weights edge joining vertices edge 
heat kernel parameter 
nodes connected put aij xi xj simple minded parameter 
nodes connected put aij 
eigenmaps compute eigenvectors eigenvalues generalized eigenvector problem sls sds diagonal matrix entries column sums dii aji 
laplacian matrix 
table summary datasets evaluation 
dataset examples features target class source day arrhythmia normal tis tis table evaluation dr occ combinations 
balanced accuracy rate estimations winning dimensionality brackets classifier dimension reduction technique pair 
italics cheating supervised feature selection 
boldface unsupervised winning dimension reduction techniques classifier 
clear beneficial dr case 
dr ig random ls lpp pca gauss kmeans knn svdd arrhythmia dr ig random ls lpp pca gauss kmeans knn svdd tis dr ig random ls lpp pca gauss kmeans knn svdd results shown table 
balanced accuracy rate defined average true positive rate sensitivity true negative rate specificity estimated fold cross validation 
figures shown obtained winning target dimensionality case parenthesis 
table dimension reduction beneficial cases 
surprising chose datasets require dimension reduction achieve results supervised setting 
cases supervised dr technique provide upper bound performance 
promising gives high scores relevant features yields consistent improvements 
case arrhythmia especially tis dataset locality preserving principle lpp competitive rest unsupervised criteria 
due fact presence lot irrelevant features full feature set renders locality space inappropriate 
non local learner svdd lpp compares adversely random feature selection criterion 
show evolution sensitivity specificity tradeoff dataset classifier combinations means tis nn 
locality evolution sensitivity vs specificity bottom tradeoff increasing target dimensionality 
left applying nn dataset 
right applying means tis dataset dimensionality varying 
best viewed colour 
classifiers usually holds dimensionality reduced better sensitivity worse specificity 
due fact computing description objects low dimensional spaces easier discriminative power lost descriptions capture outliers 
dimensionality high descriptions inaccurate classifiers accept reject machines 
phenomenon significant case lpp 
reports progress research applicability dr techniques specifically techniques unsupervised learning occ problems 
stage feel key findings demonstrated potential improvements applying carefully selected dr techniques prior class classification 
circumstances techniques cluster coherence locality preservation particularly effective 
believe lp techniques appropriate irrelevant features data set order locality meaningful original feature space necessary features relevant 
stated locality preservation appropriate criterion occ tasks contains implication input features irrelevant may just redundant 
key issue meaningful distance functions 
encounter paradoxical situation reducing dimensionality data needs rely distance measures probably meaningful original high dimensional space 
learn proper metric data imposing pre specified active research field areas classification 
class classification current techniques directly applicable information sides classification boundary 
related techniques lead useful class metric learning techniques 
class svms document classification 
journal machine learning research tax data description toolbox matlab april tax class classification 
concept learning absence counterexamples 
phd thesis delft university technology tax duin support vector data description 
mach 
learn 
january learning recognise study class classification active learning 
phd thesis delft university technology tax muller outliers prototypes ordering data 
neurocomputing august language models detection unknown attacks network traffic 
journal computer february wolf shashua feature selection unsupervised supervised inference emergence sparsity weight approach 
journal machine learning research jolliffe principal component analysis 
springer october tax muller feature extraction class classification 
icann iconip springer berlin heidelberg ng jordan weiss spectral clustering analysis algorithm 
dietterich becker ghahramani eds advances neural information processing systems 
volume 
niyogi locality preserving projections 
nips advances neural information processing systems 
locality preserving projections 
phd thesis university chicago cai niyogi laplacian score feature selection 
nips advances neural information processing systems 
doyle knowledge light mechanism explanation case reasoning 
phd thesis university dublin trinity college bay uci kdd archive kdd ics uci edu liu wong data mining tools biological sequences 
comput biol april 
