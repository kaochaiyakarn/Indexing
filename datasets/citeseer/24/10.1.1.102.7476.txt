feature selection svms weston mukherjee chapelle pontil poggio vapnik bioinformatics com georgia usa 
cbcl mit cambridge massachusetts usa 
research laboratories red bank usa 
royal holloway university london egham surrey uk 
weston chapelle research att com introduce method feature selection support vector machines 
method finding features minimize bounds leave error 
search efficiently performed gradient descent 
resulting algorithms shown superior standard feature selection algorithms toy data real life problems face recognition pedestrian detection analyzing dna microarray data 
supervised learning problems feature selection important variety reasons generalization performance running time requirements constraints issues imposed problem 
classification problems data points labeled drawn probability distribution select subset features preserving improving discriminative ability classifier 
brute force search possible features combinatorial problem needs take account quality solution computational expense algorithm 
support vector machines svms extensively classification tool great deal success variety areas object recognition classification cancer morphologies :10.1.1.41.4971
article introduce feature selection algorithms svms 
methods minimizing generalization bounds gradient descent feasible compute 
allows new possibilities speed time critical applications object recognition perform feature discovery cancer diagnosis 
show svms perform badly situation irrelevant examples problem remedied feature selection approach 
article organized follows 
section describe feature selection problem section review svms generalization bounds section article restrict case pattern recognition 
reasoning applies domains 
introduce new svm feature selection method 
section describes results toy real life data indicating usefulness approach 
feature selection problem feature selection problem addressed ways fixed find features give smallest expected generalization error maximum allowable generalization error find smallest problems expected generalization error course unknown estimated 
article consider problem 
note problem formulated dual problem 
problem formulated follows 
fixed set functions wish find preprocessing data parameters function give minimum value subject unknown loss functional norm 
literature distinguishes types method solve problem called filter wrapper methods 
filter methods defined preprocessing step induction remove irrelevant attributes induction occurs wish valid set functions example popular filter method pearson correlation coefficients 
wrapper method hand defined search space feature subsets estimated accuracy induction algorithm measure goodness particular feature subset 
approximates minimizing subject learning algorithm trained data preprocessed fixed wrapper methods provide accurate solutions filter methods general computationally expensive algorithm induction evaluated feature vector set considered typically performance hold set measure goodness fit :10.1.1.30.525
article introduce feature selection algorithm svms takes advantage performance increase wrapper methods whilst avoiding computational complexity 
note previous feature selection svms exist limited linear kernels linear probabilistic models :10.1.1.52.5153
approach applied nonlinear problems 
order describe algorithm review svm method properties 
support vector learning support vector machines realize idea map high possibly infinite dimensional space construct optimal hyperplane space 
different mappings construct different svms 
mapping performed function kernel defines inner product decision function svm optimal hyperplane maximal distance space closest image training data called maximal margin 
reduces maximizing optimization problem constraints quadratically penalize errors kernel modified identity matrix constant penalizing training errors 
non separable case suppose size maximal margin equal images training vectors sphere radius theorem holds true 
theorem images training data size belonging sphere size separable corresponding margin expectation error probability bound expectation taken sets training data size theorem justifies idea performance depends ratio function simply large margin controlled mapping bounds exist particular vapnik chapelle derived estimate concept span support vectors 
theorem assumption set support vectors change removing example function step matrix dot products support vectors probability test error machine trained sample size expectations taken random choice sample 
feature selection svms problem feature selection wish minimize equation support vector method attempts find function set minimizes generalization error 
enlarge set functions considered algorithm note mapping represented choosing kernel function equations kernels bounds theorems hold 
minimize minimize wrapper functional equation equations choosing fixed value implemented kernel 
equation minimizes radius kernel computed optimization problem subject defined maximum functional kernel 
similar way minimize span bound equation 
finding minimum requires searching possible subsets features combinatorial problem 
avoid problem classical methods search include greedily adding removing features forward backward selection hill climbing 
methods expensive compute large 
alternative approaches suggest method vector approximate binary valued vector real valued find optimum value minimize differentiable criterion gradient descent 
explained derivative criterion estimate minimum minimizing equation space gradients extra constraint approximates integer programming subject 
large elements nonzero approximating opti problem mization simplify computations considering stepwise approximation procedure find features 
minimize unconstrained 
sets smallest values zero repeats minimization nonzero elements remain 
experiments toy data compared standard svms feature selection algorithms classical filter methods select features followed svm training 
filter methods chose largest features pearson correlation coefficients fisher criterion score kolmogorov smirnov test 
pearson coefficients fisher criterion model nonlinear dependencies 
mean value th feature positive negative classes standard deviation sup ks denotes th feature training example corresponding empirical distribution 
artificial datasets objective assess ability algorithm select small number target features presence irrelevant redundant features 
linear problem dimensions relevant 
probability equal 
features drawn second features drawn probability drawn second remaining features noise nonlinear problem dimensions relevant 
probability equal 
data drawn drawn equal probability drawn normal distributions equal probability features noise 
rest linear problem features redundancy rest features irrelevant 
nonlinear problem features irrelevant 
linear svm linear problem second order polynomial kernel nonlinear problem 
filter methods svm feature selection selected best features 
results shown various training set sizes average test error samples runs training set size 
fisher score shown graphs due space constraints performed identically correlation coefficients 
problems standard svms perform poorly linear example points obtains test error svms compared test error methods 
svm feature selection methods outperformed filter methods forward selection marginally better gradient descent 
nonlinear problem filter methods kolmogorov smirnov test improved performance standard svms 
span bound forward selection rw bound gradient standard svms correlation coefficients kolmogorov smirnov test span bound forward selection rw bound gradient standard svms correlation coefficients kolmogorov smirnov test comparison feature selection methods linear problem nonlinear problem irrelevant features 
axis number training points axis test error fraction test points 
real life data problems compared minimizing fisher criterion score 
gradient descent face detection face detection experiments described section system introduced 
training set consisted positive images frontal faces size positive images negative images containing faces 
test set consisted images resulted negative images 
wavelet representation coefficients image 
performance system coefficients coefficients coefficients shown roc curve 
best results achieved features fisher score 
pedestrian detection pedestrian detection experiments described section system introduced :10.1.1.41.4971
training set consisted positive images people size negative images containing pedestrians 
test set consisted positive images negative images 
wavelet representation images resulted coefficients image 
performance system coefficients coefficients shown roc curve 
results showed trends observed face recognition problem 
detection rate detection rate false positive rate false positive rate detection rate false positive rate solid line features solid line circle feature selection method minimizing gradient descent dotted line fisher score 
top roc curves features bottom features face detection 
roc curves features features pedestrian detection 
cancer morphology classification dna microarray data analysis needs determine relevant genes discrimination discriminate accurately 
look leukemia discrimination problems colon cancer problem :10.1.1.41.4971
problem classifying ex pression genes 
training set consists examples test set examples 
genes linear svm error test set 
errors errors fisher genes score 
genes error errors fisher score 
second problem discriminating versus cells cells 
standard linear svms error problem 
genes errors errors fisher score 
colon cancer problem tissue samples probed oligonucleotide arrays contain normal colon cancer tissues discriminated expression genes 
splitting data training set test set separate trials obtained test error standard linear svms 
genes feature selection method obtained coefficients fisher score kolmogorov smirnov test 
method worse best filter method trials 
pearson correlation article introduced method perform feature selection svms 
method computationally feasible high dimensional datasets compared existing wrapper methods experiments variety toy real datasets show superior performance filter methods tried 
method applications speeds svms time critical applications pedestrian detection possible feature discovery gene discovery 
secondly simple experiments showed svms suffer high dimensional spaces features irrelevant 
method provides way circumvent naturally occuring complex problem 
alon gish mack levine 
broad patterns gene expression revealed clustering analysis tumor normal colon cancer tissues probed oligonucleotide arrays 
cell biology 
blum langley 
selection relevant features examples machine learning 
artificial intelligence 
bradley mangasarian :10.1.1.52.5153
feature selection concave minimization support vector machines 
proc 
th international conference machine learning pages san francisco ca 
chapelle vapnik bousquet mukherjee 
choosing kernel parameters support vector machines 
machine learning 
submitted 
pontil papageorgiou poggio 
image representations object detection kernel classifiers 
asian conference computer vision 
golub slonim tamayo mesirov loh downing bloomfield lander 
molecular classification cancer class discovery class prediction gene expression monitoring 
science 
jebara jaakkola 
feature selection dualities maximum entropy discrimination 
artificial 
kohavi :10.1.1.30.525
wrappers feature subset selection 
aij issue relevance 
mukherjee tamayo slonim verri golub mesirov poggio :10.1.1.41.4971
support vector machine classification microarray data 
ai memo massachusetts institute technology 
oren papageorgiou sinha osuna poggio :10.1.1.41.4971
pedestrian detection wavelet templates 
proc 
computer vision pattern recognition pages puerto rico june 
papageorgiou oren poggio 
general framework object detection 
international conference computer vision bombay india january 
vapnik 
statistical learning theory 
john wiley sons new york 
