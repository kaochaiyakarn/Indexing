ieee transactions parallel distributed systems vol 
july scheduling dynamic voltage speed adjustment slack reclamation multiprocessor real time systems zhu student member ieee rami melhem fellow ieee bruce high power consumption modern processors major concern leads decreased mission duration battery operated systems increased heat dissipation decreased reliability 
techniques proposed reduce power consumption uniprocessor systems considerably multiprocessor systems 
concept slack sharing processors propose novel power aware scheduling algorithms task sets precedence constraints executing multiprocessor systems 
scheduling techniques reclaim time unused task reduce execution speed tasks reduce total energy consumption system 
study effect discrete voltage speed levels energy savings multiprocessor systems propose new scheme slack reservation incorporate voltage speed adjustment overhead scheduling algorithms 
simulation trace results indicate algorithms achieve substantial energy savings systems variable voltage processors 
processors discrete voltage speed levels obtain nearly energy savings processors continuous voltage speed effect voltage speed adjustment overhead energy savings relatively small 
index terms real time systems multiprocessor scheduling slack sharing 
years processor performance increased expense drastically increased power consumption 
hand increased power consumption decreases lifetime battery operated systems hand held mobile systems remote solar explorers 
hand increased power consumption generates heat causes heat dissipation problem requires expensive packaging cooling technology decreases reliability especially systems processors 
reduce processor power consumption hardware techniques proposed shutting unused parts reducing power level utilized functional units 
processors multiple supply voltages multiple power levels available years making power management processor level possible 
feature software techniques proposed adjust supply voltage especially mobile uniprocessor systems :10.1.1.12.4451
done real time multiprocessing applications parallel signal processing automated target recognition atr real time mpeg encoding 
satellite parallel signal processing satellite may multiple processing units authors computer science department university pittsburgh pittsburgh pa 
mail melhem cs pitt edu 
manuscript received nov revised june accepted jan 
information obtaining reprints article please send mail computer org ieeecs log number 
ieee published ieee computer society need process signals board real time 
atr uses multiple processors detect targets comparing regions interest roi templates parallel 
mobile military systems missiles atr widely usually requires real time processing 
systems battery operated power consumption needs managed achieve maximum duration minimal energy 
applications cable television video conferencing real time performance mpeg encoding necessary processing units may achieve real time performance 
systems power management reduce energy consumption associated costs 
multiprocessor real time systems power management adjusts processor voltage speed changes task execution time affects scheduling tasks processors 
change may cause violation timing requirements 
describes novel techniques dynamically adjust processor voltage speed meeting timing requirements 
propose scheduling algorithms shared slack reclamation variable voltage speed processors task sets precedence constraints independent tasks task sets precedence constraints dependent tasks 
algorithms proven meet timing constraints 
discuss incorporate discrete voltage speed levels algorithms propose scheme incorporate voltage speed adjustment overheads scheduling algorithms slack reservation 
simulation trace real applications results show techniques save substantial energy compared static power management 
zhu scheduling dynamic voltage speed adjustment slack reclamation multiprocessor real 
related uniprocessor systems yao describe optimal preemptive scheduling algorithm independent tasks running variable speed 
deciding processor speed supply voltage consider requirement completing set tasks fixed interval different switch activities task 
assigning lower voltage tasks switch activities higher voltage tasks switch activities scheme reduce energy consumption percent 
krishna lee proposed power aware scheduling technique slack reclamation context systems voltage levels 
hsu described performance model determine processor slow factor compiler control 
super scalar architecture similar power dissipation transmeta crusoe tm simulation results show potential technique 
moss proposed analyzed techniques dynamically adjust processor speed slack reclamation 
best scheme adaptive takes aggressive approach providing safeguards avoid violating application deadlines :10.1.1.12.4451
periodic tasks executing uniprocessor systems voltage speed levels sufficient achieve energy saving infinite voltage speed levels 
studied effect dynamic voltage speed adjustment overhead choosing granularity inserting power management points program 
multiprocessor systems fixed applications predictable execution time static power management spm accomplished deciding best supply voltage speed processor 
idea spm proposed system level designs architectures variable voltage processors 
simulation results show approaches save percent energy deadline relaxed percent 
system chip designs processors running different fixed voltage levels yang proposed phase scheduling scheme minimizes energy consumption time constraints choosing different scheduling options determined compile time 
power aware multiprocessor architecture shriver proposed power management scheme satellite parallel signal processing different rate signals 
reported focused dynamic power management shared memory multiprocessor systems different static power management selection predetermined scheduling options architecture 
organized follows task model energy model power management schemes described section 
power aware scheduling dynamic processor voltage speed adjustment shared slack reclamation independent tasks addressed section 
section algorithm dependent tasks proposed proven meet timing requirements 
section discusses incorporate voltage speed adjustment overhead discrete voltage speed levels scheduling algorithms 
simulation trace results analyzed section section concludes 
models power management energy model processors cmos technology power consumption dominated dynamic power dissipation pd pd cef dd vdd supply voltage cef effective switched capacitance processor clock frequency processor speed 
processor speed linearly vdd related supply voltage vt vdd constant vt threshold voltage 
pd related pd cef time needed specific task time number cycles execute task energy consumption task ise pd time cef decreasing processor speed reduce supply voltage 
reduces processor power energy quadratically expense linearly increasing task latency 
example consider task maximum speed smax needs time units finish execution 
time units allocated task reduce processor speed supply voltage half finishing task time 
new power executing task cef vdd smax pd new energy consumption cef vdd smax cef dd smax pd power energy consumption maximum processor speed 
task model assume frame real time system frame length executed repeatedly 
set tasks ft execute frame complete frame 
precedence constraints tasks represented graph schedule periodicity consider problem scheduling single frame deadline assume multiprocessor system homogeneous processors sharing common memory 
goal develop scheduling algorithm minimizes energy consumption tasks meeting deadline 
specifying execution task ti tuple estimated worst case execution time wcet actual execution time aet 
maximal processor speed 
assume task ti value known execution determined runtime 
precedence constraints represented set edges 
edge ti tj ti direct ieee transactions parallel distributed systems vol 
july predecessor tj means tj ready execute ti finishes execution 
power management schemes consider worst case scenario tasks worst case execution time referred canonical execution 
case tasks finish maximal processor speed smax reduce processor supply voltage speed finish tasks just time reduce energy consumption 
basic idea static power management calculate minimum processor speed ensure canonical execution tasks finishes just time 
tasks run reduced supply voltage speed save energy :10.1.1.12.4451
minimal processor speed ensure tasks finish justin time referred sjit 
addition static power management may reduce energy dynamic voltage speed adjustment 
simplify discussion assume processor supply voltage speed adjusted setting maximum speed certain supply voltage 
tasks exhibit large variation actual execution time cases consume small fraction worst case execution time unused time considered slack reused remaining tasks run slower finishing :10.1.1.12.4451
case power energy consumption reduced 
get maximal energy savings combine static power management dynamic voltage speed adjustment 
algorithms assume canonical execution checked see task set finish 
task set rejected sjit calculated canonical execution finish time algorithms apply dynamic voltage speed adjustment 
rest normalize worst case execution time actual case execution time task ti ci smax sjit ai smax sjit task ti characterized ci ai 
initially simplify problem discussion assume processor supply voltage frequency changed continuously ignore overhead voltage speed adjustment 
section discuss effect discrete speeds overhead 
power aware scheduling independent tasks precedence constraints tasks available time ready execute 
major strategies scheduling independent tasks multiprocessor systems global partition scheduling 
global scheduling tasks global queue processor selects queue task highest priority execution 
partition scheduling task assigned specific processor processor fetches tasks execution queue 
consider nonpreemptive scheduling scheme fig 

global scheduling processor systems 
priority assignment 
optimal priority assignment 
task run completion begins execute 
global scheduling task priority assignment affects task goes workload processor total time needed finish execution tasks 
general optimal solution assigning task priority get minimal execution time np hard 
furthermore show section priority assignment minimizes execution time may lead minimal energy consumption 
expecting longer tasks generate dynamic slack execution longest task heuristic task wcet determining task priority 
difference total execution time optimal priority assignment longest task priority assignment small 
specific priority assignment tasks inserted global queue order priority highest priority task front 
examples number tasks order global queue longest task priority assignment 
kth task global queue identified tk 
emphasize importance task priority scheduling consider simple example task set executing dual processor system shown fig 

ft 
figures axis represents time axis represents processor speed cycles time unit area task box defines number cpu cycles needed execute task 
considering canonical execution fig 
see longest task priority assignment meets deadline optimal priority assignment fig 
results time 
easy see order deadline 
follows extend greedy slack reclamation scheme global scheduling show scheme may fail meet deadline 
propose novel slack reclamation scheme global scheduling shared slack reclamation 
global scheduling greedy slack reclamation scheme extension dynamic power management scheme uniprocessor systems moss 
scheme greedy slack reclamation slack processor reduce speed task running processor 
zhu scheduling dynamic voltage speed adjustment slack reclamation multiprocessor real 
fig 

global scheduling power management 
canonical execution 
actual execution npm 
consider task set ft 
fig 
shows canonical execution meet deadline fig 
shows power management slack reclamation actual execution finish fig 
shows actual execution finishes time slack time units 
greedy slack reclamation slack task runs 
execute units time processor speed reduced sjit accordingly 
uses time misses deadline shown fig 

canonical execution finishes global scheduling greedy slack reclamation guarantee tasks finish global scheduling shared slack reclamation example section greedy slack reclamation gives slack 
means start execution time speed sjit time units finish execution time 
time unit left misses deadline time unit 
case better share units slack splitting parts give units unit 
slack sharing starts time executes time units speed sjit ends time 
starts time executes time units speed sjit ends time 
meet deadline 
figs 
demonstrate operations scheme 
finishes time finds units slack 
time units expected finish time wcet 
fetching gives units amount slack expected finish time shares remaining slack 
different point view sharing slack may looked allocated time units allocated time units 
fig 

global scheduling greedy slack reclamation 
fig 

global scheduling shared slack reclamation 
units slack unit slack 
sense situation similar assigned assigned tasks assigned canonical execution assigned vice versa 
processor systems idea described propose algorithm processor systems 
formally presenting algorithm define estimated time eet task executing processor time task expected finish execution consumes time allocated 
start time task processor time task estimated execution processor 
tasks execute processor current frame defined finish time task executed processor 
algorithm algorithm 
processor invokes algorithm execution processor finishes executing task 
shared memory holds control information updated critical section shown algorithm 
shared memory common queue ready contains ready tasks array record processor pp 
initially tasks put ready order priorities processors set 
algorithm pid represents current processor current time sid speed pid 
algorithm algorithm invoked pid ready tk dequeue ready find pr eetk ck eetk sid sjit ck eetk execute tk speed sid wait execution pid finishes task time tasks ready pid stall sleep awakened frame 
ieee transactions parallel distributed systems vol 
july function wait put processor sleep line 
pid fetch task tk ready line 
tk starts smallest canonical execution exchange minimum lines 
try emulate timing canonical execution 
pid calculates speed sid execute tk timing information begins execution 
exchanging pid shares part slack specifically pr 
reconsider example fig 
suppose task uses actual execution time 
assuming power consumption equal cef slack reclaimed dynamically energy consumption computed cef global scheduling shared slack reclamation longest task priority assignment energy consumption computed cef note optimal priority assignment fig 
optimizes execution time energy consumption computed cef optimal priority assignment terms execution time optimal energy consumption considering dynamic behavior tasks 
algorithm notice time ready empty values processors equal biggest values eet tasks running processors 
tasks started task line 
task starts follow smallest 
properties prove correctness sense shared slack reclamation extend finish time task set execution shared slack reclamation time canonical execution shown section 
analysis algorithm canonical execution define canonical estimated time task tk 
definition know eetk latest time tk finish execution 
eetk task canonical execution finish time execution finish prove eetk tk define xng set containing largest elements set fx xng 
define history set set tasks started possibly finished execution time lemma 
time started task eetk proof 
result trivial 
consider case proof induction tk base case initially tasks start execution finish time ng 
remaining values taken zero 
induction step assuming time tk started task ft tk eetk loss generality assume gg tk started tk starts time tk started task 
ft lines algorithm eetk ck gg ck ck eetk new values theorem 
fixed independent task set common deadline executing processor systems canonical execution priority assignment global scheduling completes time execution priority assignment complete time proof 
specific priority assignment canonical execution global scheduling tasks identified orders ready queue canonical execution 
prove theorem showing execution eeti eet 
trivial 
consider case proof induction tk base case initially sets eeti execution consideration actual execution time ti 
eeti eet induction step assume eeti eet 
time tk starts tk started task 
loss generality assume eetk eetk gg 
lemma eetk eetk ut zhu scheduling dynamic voltage speed adjustment slack reclamation multiprocessor real 
fig 

list scheduling dual processor systems 
precedence graph 
canonical execution finish 
tk begins run lines algorithm noncanonical canonical execution respectively eetk min ck min eetk eetk eetk ck ck eet min ck min eet ck eet ck notice eeti 
eetk eeti eet tu section discuss scheduling shared slack reclamation dependent tasks 
idea slack sharing independent tasks 
new concern maintain execution order implied canonical execution dependent tasks 
power aware scheduling dependent tasks list scheduling standard technique schedule tasks precedence constraints 
task ready execution predecessors finish execution 
root tasks predecessors ready time 
list scheduling puts tasks ready queue soon ready dispatches tasks front ready queue processors 
task ready time finding optimal task order minimizes execution time np hard 
fig 

list scheduling slack reclamation 
shared slack reclamation 

section heuristic global scheduling 
put ready queue longest task wcet tasks ready simultaneously 
tasks numbered order added ready queue canonical execution 
kth task entering ready queue canonical execution identified tk 
consider dependent task set ft 
precedence graph shown fig 
canonical execution shown fig 

task nodes labeled tuple ci ai 
canonical execution see root tasks ready time 
ready time predecessor finishes execution 
ready time ready time 
due dependencies tasks task readiness noncanonical execution depends actual execution predecessors 
discussion independent tasks know greedy slack reclamation guarantee completion completion time canonical execution 
show straightforward application shared slack reclamation list scheduling guarantee timing constraints met 
list scheduling shared slack reclamation consider example fig 
assume task uses actual execution time 
fig 
task ready put queue 
clear list scheduling shared slack reclamation finish execution time completion time canonical execution 
reason list scheduling shared slack reclamation takes longer canonical execution tasks ieee transactions parallel distributed systems vol 
july ready time change 
order tasks added queue different canonical execution order 
example ready leads assigned 
turn leads late completion tasks deadline missed 
fixed order list scheduling shared slack reclamation schedule fig 
need prevent executing guarantee execution take longer canonical execution need maintain task execution order canonical execution 
discussed section step shown algorithm canonical execution emulated sjit calculated 
emulation tasks canonical execution order collected ready time task ti calculated rt jtk ti tasks run sjit 
determine readiness tasks define number unfinished immediate predecessors uip task 
decrease predecessor task ti finishes execution 
task ti ready 
processor free check task head global see ready 
task ready processor fetch execute processor goes sleep 
details algorithm described 
processor systems independent tasks assume shared memory holds control information 
algorithm shows algorithm 
processor pid invokes algorithm execution task finishes execution pid pid sleeping signaled processor 
function wait put idle processor sleep function signal wake processor initially tasks put global canonical execution order line important algorithm keep canonical execution order maintain temporal correctness 
set number predecessors task ti set shown algorithm 
algorithm invoked signal processor waiting signal point line 
algorithm invoked pid finishes task begins line 
head global ready pid picks task tk head global line 
claim slack pid calculates eetk tk starts time canonical execution rt whichever bigger claims difference tk start time canonical execution slack line notice rt 
pid calculates speed sid execute tk signals pw pw sleeping new head global ready lines 
pid runs tk speed sid line 
algorithm algorithm invoked pid put tasks global order canonical execution 
head global ready tk dequeue global find pr eetk tg ck eetk sid sjit ck eetk head global ready pw sleep signal pw execute tk speed sid ti tk ti wait reconsider example shown fig 

execution dual processors shown fig 

order wait readiness wastes part slack 
maintaining execution order canonical schedule tasks finish time 
analysis algorithm similar time global empty values equal biggest values eet tasks running processors 
tasks started task 
task starts follow minimum 
lemma 
time started task eetk proof 
proof induction tk similar proof lemma 
base case initially ti start execution finish time ft tmg induction step assume tk started execution tk started task 
time ft tk 
means tasks ready time number ready tasks greater equal number processors 
zhu scheduling dynamic voltage speed adjustment slack reclamation multiprocessor real 
eetk loss generality assume gg tk starts tasks finish tk started task time ft 
lines algorithm eetk rt tg ck rt tg ck notice tk starts rt 
eetk eetk rt tg ck eetk eetk rt tg ck eet rt ck rt ck tk starts rt rt notice eeti eet eetk eetk rt tg rtc new values ut theorem 
fixed dependent task set common deadline executing processor systems canonical execution priority assignment list scheduling completes time execution priority assignment complete time proof 
tasks wcet canonical execution list scheduling 
specific priority assignment tasks numbered order entered global canonical execution 
prove theorem showing execution eeti 
proof induction tk base case initially sets eeti execution consideration actual execution time ti 
eeti 
induction step assume eeti 
time tk starts tk started task 
loss generality assume gg eetk eetk 
lemma eetk eetk tk starts time noncanonical execution eetk eet canonical execution lines algorithm eeti eet tu discussion assumed continuous voltage speed ignored speed adjustment overhead 
current variable voltage processors discrete voltage speed levels 
time energy overhead associated voltage speed adjustment 
section discuss incorporate issues scheduling algorithms 
accounting overhead discrete voltage speed levels voltage speed adjustment overhead kinds overhead considered changing processor voltage speed time energy 
time overhead affects feasibility algorithms timing constraints met 
focus time overhead discuss energy overhead 
time overhead considered need model calculate overhead scheme incorporate algorithms 
propose new scheme slack reservation incorporate time overhead dynamic speed adjustment algorithms 
time overhead model time overhead consisting parts constant part set time variable part proportional degree voltage speed adjustment 
js constants processor speed adjustment processor speed adjustment 
choice results constant time overhead 
simulations section set different values see affect energy savings 
conservative way incorporate time overhead adding maximum time overhead voltage speed adjustment smax smin worst case execution time tasks 
case time change speed task 
ieee transactions parallel distributed systems vol 
july fig 

slack reservation overhead 
propose idea slack reservation incorporate time overhead 
specifically try slack slow processor speed reserve slack processor change voltage speed back appropriate level 
way ensure tasks executed appropriate speed meet deadline 
idea illustrated fig 

ti finishes early slack li portion li change voltage speed ti 
reserve slack changing processor voltage speed back sjit ti uses allocated time 
rest slack slow speed ti 
suppose current speed ti si assume speed ti si computed 
overhead oi change speed si si overhead ri change speed si back sjit oi sij ri sjit si si calculated giving additional time li oi ri task ti si sjit ci ci li oi ri assuming si si equation quadratic equation si ci li sjit si si sjit ci solution obtained si si equation assumption wrong si si 
itis possible set si si slack li ri ti reduce speed sjit si ci sjit si set si si 
possible ci li ri set si si si si si solved ci si sjit ci li sjit si si computed equation larger sjit set si sjit 
cases reserved slack ri part reclaimed slack li 
cases ti finishes useful slack li ri ti 
cases ri fig 

slack ti 
change speed back sjit ti run sjit see fig 

considering time overhead slack sharing processors needs modified 
referring fig 
suppose processor pi runs si finishes early 
described section share slack processor pj running sjit 
slack sharing time pi change speed back sjit share slack 
processor pi needs change speed sjit share slack possible 
energy overhead time overhead voltage speed adjustment energy overhead associated speed change 
suppose energy overhead changing speed si sj si sj 
assuming energy consumption ti ei sjit si efficient change speed si si ti si si si sjit ei si sjit words timing constraints met time overhead may decide run ti lower speed energy overhead larger energy saved speed change 
setting processor idle speed voltage speed adjustment overhead considered processor run slowest speed idle executing task 
speed achieves energy consumption idle state 
overhead considered independent tasks idle state appears execution set processor idle time adjust voltage speed 
dependent tasks idle state may appear middle execution 
ensure tasks finish time idle state processor needs run speed sjit processor fig 

slack sharing overhead considered 
zhu scheduling dynamic voltage speed adjustment slack reclamation multiprocessor real 
predict exactly task available 
scheme deal idle states appearing middle execution 
may put processor sleep idle wake task ready predicting ready time task task canonical ready time 
scheme require watchdog timer specify task ready 
possible task arrives timer expires case processor needs activated timer deactivated 
scheme possibly achieve additional energy savings implementation complex purpose considered 
way deal idle state dependent tasks conservative add maximum overhead task worst case execution time 
case put processor sleep idle guarantee time speed processor task ready execute 
discrete voltage speed levels currently available variable voltage processors working voltage speed settings 
algorithms easily adapted discrete voltage speed levels 
specifically calculating processor speed ifs falls speed levels sl sl setting sl guarantee tasks finish time deadline met 
higher discrete speed slack task available tasks 
experimental results show sharing slack tasks scheduling discrete voltage speed levels better performance terms energy savings continuous voltage speeds 
performance analysis section empirically demonstrate slack reclamation reduces energy consumption 
synthetic data sets trace data actual real time multiprocessor applications simulation 
compare energy consumed combination static power management dynamic supply voltage speed adjustments energy consumed static power management 
idea minimal energy scheduling technique uniprocessor systems consider algorithm uses tasks actual runtime information generate schedule compute single voltage speed tasks idle state may schedule 
consider absolute lower bound alb scheme assumes application fully parallel obtained averaging total actual workload processors speed uniformly reduced idleness case preemption needed generate schedule fairness scheduling 
alb achievable analysis impractical require knowledge 
experiments describe simulation experiments 
synthetic data get actual execution time task define average worst case ratio ti execution time actual execution time ti generated normal distribution ci 
task sets specify lower cmin upper cmax bounds task wcet average tasks reflects amount dynamic slack system 
higher value dynamic slack 
task wcet generated randomly cmin cmax generated uniform distribution simplicity power consumption assumed proportional experiments energy normalized energy consumed static power management 
assume continuous voltage speed scaling penalty changing voltage speed specified 
effects discrete voltage speed scaling voltage speed adjustment overhead reported sections 
overhead considered processor speed idle state set sjit overhead considered idle state appearing schedule processor speed set sjit idle state middle execution processor speed set sjit discussed earlier 
partition scheduling greedy slack reclamation versus spm results section obtained running synthetic independent task set tasks results average runs 
wcet tasks generated setting cmin cmax 
fig 
number processors varied 
compare global scheduling shared slack reclamation partition scheduling greedy slack reclamation 
longest task partitioning divide tasks processors apply greedy slack reclamation processor 
see global scheduling shared slack reclamation consumes energy partition scheduling greedy slack reclamation 
reason slack sharing scheme gives slack longer tasks shorter tasks 
balances speed task reduces energy consumption 
average worst case ratio average time dynamic slack global scheduling shared slack reclamation results energy saving percent versus static power management 
increases dynamic slack compared spm energy saving decreases 
note independent tasks little idle state appears schedule gets energy savings alb 
compared lower bounds performance algorithm percent difference 
see shared slack reclamation scheme performance systems different number processors run synthetic independent task set changing number processors setting 
results shown fig 

compared spm energy savings number processors equal 
number processors energy savings decreases sharply 
reason task processor executed sjit slack processor wasted 
tasks task set processors number tasks running sjit total amount slack wasted increases ieee transactions parallel distributed systems vol 
july fig 

energy savings independent tasks 
varying 
varying number processors 
quickly 
compared algorithm better 
number processors equal algorithm percent alb 
processors alb performs better 
reason alb assumes actual workload evenly balanced processors 
versus spm section consider dependent task sets compare energy consumption versus spm 
consider example synthetic tasks 
tasks wcet generated randomly assume processor system 
fig 
vary 
energy saving fixed order list scheduling shared slack reclamation compared static power management spm varies percent percent 
increases dynamic slack compared spm energy savings decreases 
average energy savings approximately percent 
idle time dependent tasks compared alb performance algorithm percent difference 
fig 

energy savings dependent tasks 
varying 
varying number processors 
consider matrix operations matrix multiplication gaussian elimination assuming matrix submatrices measure effectiveness techniques benchmarks 
worst case execution time task determined operations involved 
conduct experiments achieving similar energy savings fixed order list scheduling shared slack reclamation 
results shown fig 

gaussian elimination considered matrix submatrices allow parallelism 
vary number processors shown fig 

application number processors larger energy consumption increases sharply compared spm 
reason similar happen number tasks running sjit amount slack wasted increases 
reason idleness processors due dependence tasks 
compared algorithm percent difference 
note alb assumes fully parallel application possible gaussian elimination large number processors 
zhu scheduling dynamic voltage speed adjustment slack reclamation multiprocessor real 
fig 

dependence graph atr process frame execution time tasks atr 
assuming detections frame templates 
dependence graph atr 
execution time tasks atr 
trace data section sets trace data different parallel applications show effectiveness algorithms 
trace data gathered instrumenting applications record execution time parallel section 
applications run pentium iii mhz mb memory 
application considered automated target recognition atr 
atr searches regions interest roi frame tries match specific templates roi 
dependence graph atr shown fig 

fig 
shows runtime information tasks atr processing consecutive frames platform 
assume atr process frame roi compared different templates 
number actual runtime tasks corresponding undetected set 
second consider berkeley real time mpeg encoder 
setting group pictures gop pattern forcing encode frame dependence graph process frames gop decoded frame shown fig 

different frames dependence graph 
frame encoded single image past frames 
frame forward predicted frame encoded relative past frame 
frame bidirectional predicted frames encoded relative past frames 
frame frame 
flower garden tennis movies having frames fig 
shows runtime information processing different frames time encoding include 
trace data vary number processors run applications note maximum parallelism berkeley mpeg encoder gop simulator 
results energy savings shown table 
energy savings tennis flower garden mpeg encoder encoding time tennis varies flower garden see fig 

gets percent energy savings alb gets percent 
alb assumes fully parallel application preemption evenly balanced actual workload 
impractical tight lower bound 
results consistent earlier results synthetic data 
considering overhead observe time overhead affects algorithms performance terms energy savings set experiments constant part overhead different values relative smallest task worst case execution time 
experiment setting coefficient different values 
maximum variable part time overhead changing speed smax smin equals times smallest task worst case execution time 
recall range task worst case execution time smallest task worst case execution time 
fig 
shows independent task set tasks fig 
shows synthetic dependent task set tasks 
results reported include energy overhead optimization discussed section 
expect better results optimization considered 
fig 

dependence graph execution time process different frames mpeg encoder assuming encoding sequence force encode frame decoded frame 
dependence graph mpeg encoder 
execution time different frames 
ieee transactions parallel distributed systems vol 
july figures constant part overhead affects algorithms performance 
maximum overhead considered independent tasks percent difference energy consumption case overhead 
dependent tasks difference percent 
big jump case overhead minimal overhead 
reason overhead idle state runs sjit overhead idle state runs sjit smax load percent ensure tasks finish time see section 
note dependent specific processor hardware tasks running processor 
suppose minimum task worst case execution time ms transmeta processor takes ms change voltage speed 
percent 
similarly amd measured overhead ms change voltage sto change frequency 
amd percent 
processor needs change voltage percent 
effect discrete voltage speed levels see discrete voltage speed levels affect algorithms performance terms energy savings set different levels mhz mhz speed transmeta tm corresponding supply voltage 
levels uniformly distributed increment discrete speed levels 
idle state runs minimum speed consumes corresponding energy 
run task set tasks run synthetic task set tasks 
set number processors table energy savings versus spm trace data fix 
energy consumption versus spm different number voltage speed levels shown fig 
means continuous voltage speed adjustment 
workload percent static slack sjit smax 
static power management processors runs mhz mhz speed configurations energy consumption 
fig 
see energy consumption algorithms continuous adjustment discrete voltage speed levels levels guarantee energy consumption 
reason discrete voltage speed levels processors set speed higher discrete level saves slack tasks 
sharing slack tasks energy consumption algorithms discrete voltage speed levels may continuous adjustment levels may better levels 
case levels sufficient achieve effect continuous adjustment observation reported uniprocessor periodic tasks 
summary introduce concept slack sharing multiprocessor systems reduce energy consumption 
concept propose novel power aware scheduling algorithms independent dependent tasks 
cases prove scheduling slack reclamation cause execution tasks finish completion time canonical execution fig 

energy savings varied time overhead voltage speed adjustment 
versus spm 
versus spm 
zhu scheduling dynamic voltage speed adjustment slack reclamation multiprocessor real 
fig 

energy consumption versus spm different number voltage speed levels 
task uses worst case execution time 
specifically canonical execution task set finish time proposed algorithms global scheduling shared slack reclamation list scheduling shared slack reclamation finish execution tasks compared static power management spm simulation results show achieve considerable energy saving task execution time smaller worst case execution time true real applications 
trace data real applications automated target recognition berkeley mpeg encoder results show schemes save percent energy compared spm 
effect discrete voltage speed performance algorithms studied 
simulation results show discrete voltage speed levels sufficient achieve better energy savings continuous voltage speed 
propose scheme incorporate voltage speed adjustment overhead scheduling algorithms slack reservation 
assumption takes milliseconds adjust processor supply voltage speed simulation results show effect overhead energy saving ranges percent percent 
acknowledgments supported defense advanced research projects agency parts project contract 
authors dr daniel moss useful discussion overhead section 
referees criticisms suggestions helped rewriting better form 
moss melhem placement power management points real time applications proc 
workshop compilers operating systems low power 
melhem moss alvarez dynamic aggressive scheduling techniques power aware real time systems proc :10.1.1.12.4451
nd ieee real time systems symp dec 
baruah cohen plaxton proportionate progress notion fairness resource allocation algorithmica vol 
pp 

brodersen energy efficient cmos microprocessor design proc 
hawaii int conf 
system sciences pp jan 
pering brodersen dynamic voltage scaled microprocessor system ieee solid state circuits vol 
pp 

chandrakasan data driven signal processing approach energy efficient computing proc 
int symp 
low power electronic devices 
chandrakasan sheng brodersen low power cmos digital design ieee solid state circuit vol 
pp 

cooper subramanian experimental evaluation list scheduling technical report dept computer science rice univ sept 
cosnard parallel algorithms architectures 
int thomson computer press 
mok multiprocessor line scheduling hard real time tasks ieee trans 
software eng vol 
pp 

ernst ye embedded program timing analysis path clustering architecture classification proc 
int conf 
computer aided design pp 
nov 
gong rowe parallel mpeg video encoding proc 
picture coding symp sept 
system level design methods low energy architectures containing variable voltage processors proc 
workshop power aware computing systems nov 
hsu kremer hsiao compiler directed dynamic frequency voltage scheduling proc 
workshop power aware computing systems nov 
www microprocessor ru 
www transmeta com 
voltage scheduling problem dynamically variable voltage processors proc 
int symp 
low power electronics design pp 
aug 
krishna lee voltage clock scaling adaptive scheduling techniques low power hard real time systems proc 
sixth ieee real time technology applications symp may 
melhem moss fault tolerant real time global scheduling multiprocessors proc 
th ieee euromicro workshop real time systems june 
moss melhem compiler assisted dynamic power aware scheduling real time applications proc 
workshop compiler os low power oct 
yu meng high efficiency variable voltage cmos dynamic dc dc switching regulator proc 
ieee int solid state circuit conf pp 
feb 
shin real time dynamic voltage scaling low power embedded operating systems proc 
th acm symp 
operating systems principles oct 
walters aided automatic target recognition sensory inputs image forming systems ieee trans 
pattern analysis machine intelligence vol 
pp 

shriver gokhale kang cai mccabe suh power aware satellite parallel signal processing scheme 
chapter power aware computing plenum kluwer publishers pp 

yang wong marchal energy aware runtime scheduling embedded multiprocessor socs ieee design test computers vol 
pp 

yao demers shenker scheduling model reduced cpu energy proc 
th ann 
symp 
foundations computer science pp 
oct 
ieee transactions parallel distributed systems vol 
july member ieee 
zhu received degree computer science technology xi university degree computer science technology tsinghua university ms degree computer science university pittsburgh 
currently phd student university pittsburgh researching power fault tolerance management parallel real time systems 
student rami melhem received degree electrical engineering cairo university ma degree mathematics ms degree computer science university pittsburgh phd degree computer science university pittsburgh 
assistant professor purdue university prior joining faculty university pittsburgh currently professor computer science electrical engineering chair computer science department 
research interests include real time fault tolerant systems optical interconnection networks high performance computing parallel computer architectures 
dr melhem served program committees numerous conferences workshops general chair third international conference massively parallel processing optical interconnections 
editorial board ieee transactions computers ieee transactions parallel distributed systems 
serving advisory boards ieee technical committees parallel processing computer architecture 
editor kluwer plenum book series computer science editorial board computer architecture letters 
fellow ieee ieee computer society member acm 
bruce received bs degree computer science college william mary phd degree computer science university virginia 
assistant professor department computer science university pittsburgh 
research interests include computer architecture compilers software development tools embedded systems 
currently researching continuous compilation computer architecture small portable systems compiler optimization embedded systems 
information computing topic please visit digital library computer org publications dlib 
