shape quantization recognition randomized trees amit donald geman august department statistics university chicago chicago il email amit galton uchicago edu 
supported part daal 
department mathematics statistics university massachusetts amherst ma email geman math umass edu 
supported part nsf dms onr contract arpa contract mda 
explore new approach shape recognition virtually nite family binary features queries image data designed accommodate prior formation shape invariance regularity 
query corresponds spatial arrangement local topographic codes tags primitive common informative shape 
discriminating power derives relative angles distances tags 
important attributes queries natural partial ordering corresponding increasing structure complexity ii semi invariance meaning shapes class answer way queries successive ordering iii stability queries distinguished points substructures 
classi er full feature set evaluated impossible determine priori arrangements informative 
approach informative features build tree classi ers time inductive learning 
ect tree provides approximation full posterior features chosen depend branch 
due number nature queries standard decision tree construction xed length feature vector feasible 
entertain small random sample queries node constrain complexity increase tree depth grow multiple trees 
terminal nodes labeled estimates corresponding posterior distribution shape classes 
image classi ed sending tree aggregating resulting distributions 
method applied classifying handwritten digits synthetic linear nonlinear deformations latex symbols 
state art error rates achieved nist database digits 
principal goal experiments latex symbols analyze invariance generalization error related issues comparison ann methods context 
contents invariant recognition shape queries tags 
tag arrangements 
posterior distribution tree approximations tree structured shape quantization exploring shape space 
randomization 
structural description 
semi invariance 
unsupervised learning 
multiple trees aggregation 
dependence training set 
relative error rates 
parameter estimation 
performance bounds individual trees questions 
multiple trees weak dependence 
generalization interpolation 
extrapolation 
note 
incremental learning universal trees fast indexing handwritten digit recognition comparison ann posterior distribution generalization error 
invariance visual system 
explore new approach shape recognition joint induction shape features tree classi ers 
data binary images dimensional shapes varying sizes 
number shape classes may reach hundreds see may considerable class variation handwritten digits 
fundamental problem design practical classi cation algorithm incorporates prior knowledge shape classes remain invariant certain transformations 
proposed framework analyzed context invariance generalization error methods inductive learning principally arti cial neural networks ann 
classi cation large fact virtually nite family binary features image data constructed local topographic codes tags 
large sample small subimages xed size recursively partitioned individual pixel values 
tags simply labels cells successive partition pixel image assigned labels subimage centered 
result tags involve detecting distinguished points curves special topological structures complex attributes de nition problematic due locally ambiguous data 
fact tags primitive classify shapes 
mere existence tag conveys little information discriminating shape classes investigating just spatial relationships tags example asking tag type north tag type 
relationships speci ed coarse constraints angles vectors connecting pairs tags relative distances triples tags 
absolute location scale constraints involved 
image may contain instances arrangement signi cant variations location distances angles binary feature query spatial arrangement response positive collection tags consistent associated constraints image 
query involves extensive disjunction oring operation 
images answer query similar shapes 
fact reasonable assume shape class determined full feature set theoretical bayes error rate zero 
classi er full feature set evaluated impossible determine priori arrangements informative 
approach select informative features build tree classi ers breiman friedman olshen stone casey nagy quinlan time inductive learning 
ect approximation full posterior features chosen depend branch traversed 
natural partial ordering queries results regarding tag arrangement labeled graph vertex labels correspond tag types edge labels angle distance constraints see figures 
ways features ordered increasing structure complexity 
related attribute semi invariance means large fraction images class answer way query answer way query immediately succeeding ordering 
leads nearly invariant classi cation respect ofthe transformations scaling translation skew small non linear deformations type shown 
due partial ordering tree construction nite dimensional feature set computationally cient 
training multiple trees breiman dietterich bakiri grown form randomization reduce statistical dependence tree tree weak dependence veri ed experimentally 
simple queries top trees complexity queries increases tree depth 
way semi invariance exploited space shapes systematically explored calculating tiny fraction answers 
tree regarded random variable image space values termi nal nodes 
order recognize shapes terminal node tree labeled estimate conditional distribution shape classes image reaches terminal node 
estimates simply relative frequencies training data require optimization 
new data point classi ed dropping trees averaging resulting terminal distributions mode aggregate distribution 
due averaging weak dependence considerable errors estimates tolerated 
tree growing question selection parameter esti mation separated estimates re ned inde nitely reconstructing trees simply updating counter tree new data point 
separation tree making parameter estimation possibility di erent training samples phase opens way selecting queries unlabeled samples unsupervised learning samples shape classes 
perform surprisingly compared ordinary supervised learning 
recognition strategy di ers true invariants algebraic di tial structural features holes endings 
methods certainly introduce prior knowledge shape structure share emphasis 
invariant features usually require image normalization boundary extraction generally sensitive shape distortion image degradation 
similarly structural features di cult express de ned functions image opposed model data 
contrast queries stable primitive precisely truly invariant distinguished points sub structures 
popular approach class learning problems pattern recognition anns feedforward multilayer perceptrons dietterich bakiri fukushima miyake personnaz dreyfus martin pitman 
exam ple best rates handwritten digits reported lecun boser denker henderson howard hubbard jackel 
classi cation trees neural networks certainly aspects common example rely training data fast line require little storage see brown gelfand delp 
approach generalization comparison direct certain properties acquired hardwiring depending learning image normal ization 
anns emphasis parallel local processing limited degree disjunction large part due assumptions regarding operation visual system 
limited degree invariance achieved models 
contrast features involve extensive disjunction global processing achieving greater degree invariance 
comparison pursued 
organized follows 
approaches invariant shape recognition reviewed synthesized random deformations basic latex symbols figures provide controlled experimental setting empirical analysis invariance high dimensional shape space 
basic building blocks algorithm tags tag arrangements described 
address fundamental question exploit discriminating power feature set attempt motivate multiple decision trees context ideal bayes classi er tradeo approximation error estimation error 
explain roles partial ordering randomization supervised unsupervised tree construction discuss quantify semi invariance 
multiple decision trees full classi cation algorithm analysis dependence training set 
calculate rough performance bounds individual multiple trees 
gen experiments training test samples represent di erent populations incremental learning addressed 
fast indexing possible role shape quantization considered 
apply method real problem classifying handwritten digits nist database training testing achieving state art error rates 
develop comparison anns terms invariance generalization error connections observed functions visual system 
conclude assessing extensions visual recognition problems 
invariant recognition invariance fundamental issue shape recognition isolated shapes 
basic approaches reviewed framework 
denote space digital images denote set shape classes 
assume image true class kg 
course directly addition probability goal construct classi er thatp 
literature statistical pattern recognition common address variation preprocessing normalization 
estimating shape class estimates transformation represents standardized image 
finding involves sequence procedures brings images size corrects translation slant rotation methods 
may morphological operations standardize stroke thickness bottou cortes denker drucker guyon jackel lecun muller sackinger simard vapnik hastie buja tibshirani 
resulting image classi ed standard procedures discriminant analysis multilayer neural network nearest neighbors cases essentially ignoring global spatial properties shape classes 
di culties generalization encountered normalization robust accommodate nonlinear deformations 
de ciency ameliorated large training sets see discussions jain simard lecun denker werbos context neural networks 
clear robust normalization methods reduce variability preserve information performance classi er shall see example regard slant correction handwritten digits 
template matching approach 
estimates transformation prototypes library 
classi cation collection estimated transformations 
requires explicit modeling prototypes extensive computation estimation stage usually involving relaxation methods appears impractical large numbers shape classes 
third approach closer spirit search functions mean ing thatp cjy constants discriminating power depends extent values distinct 
invariants planar objects single views non planar objects multiple views discovered proposed recognition see reiss refer ences 
invariants fourier descriptors image moments example magnitude zernike moments lu invariant tation 
invariants require computing tangents estimates shape boundaries forsyth mundy zisserman coelho heller rothwell sabourin 
examples invariants include discontinuities curvature 
general mathematical level advanced borrowing ideas projective algebraic latex symbols 
di erential geometry mundy zisserman 
successful treatments invariance include geometric hashing lamdan schwartz wolfson nearest neighbor classi ers ne invariant metrics simard 

similarly structural features involving topological shape attributes junctions endings loops distinguished boundary points points high curvature invariance properties authors lee srihari report better results features standardized raw data 
view true invariant features form su ciently stable intensity recognition data structures crude analyze continuum methods 
particular features invariant non linear deformations depend heavily preprocessing steps normalization boundary extraction 
data high quality steps may result lack robustness distortions shapes due example digitization noise blur degrading factors see discussion reiss 
structural features di cult model extract data stable fashion 
may di cult recognize hole recognize 
similar doubts hand crafted features distinguished points expressed jung nagy 
addition recognize components objects recognizing objects choice classi er secondary 
features invariant 
semi invariant appropriate sense regarded coarse substitutes true geometric point invariants literature cited 
sense share outlook expressed model quasi invariants binford levitt burns weiss riseman strict invariance relaxed functionals compute entirely di erent 
invariance properties queries related partial ordering manner selected recursive partitioning 
roughly speaking queries proportional depth tree number questions asked 
elementary queries bottom ordering wewould expect jy orp jy collection elementary queries low discriminatory power 
statements ampli ed 
queries higher ordering higher discriminatory power maintain semi invariance relative subpopulations determined queries preceding ordering 
query immediately ordering orp 
de ned precisely ed empirically 
experiments invariant recognition scattered 
involve real data handwritten digits 
employ synthetic data case data model involves prototype shape see space image image transformations 
assume class label prototype top perturbed latex symbols 
bottom training data symbol 
preserved transformation distinct prototypes transformed image 
transformations broad sense referring ne maps alter pose shapes nonlinear maps deform shapes 
shall degradation noise blur basically consists perturbations identity 
particular considering entire pose space perturbations pose corresponding identity 
probability derived probability measure onthe space transformations follows dg prior distribution take uniform 
concentrated space images needless say situation complex actual visual recognition problems example unrestricted object recognition standard projection models 
invariance challenging context 
important emphasize model explicitly classi cation algorithm 
knowledge prototypes assumed estimated template approaches 
purpose model generate samples training testing 
images random sampling particular distribution space containing linear scale rotation skew nonlinear transformations 
speci cally log scale drawn uniformly rotation angle drawn uniformly degrees log ratio axes skew drawn uniformly 
nonlinear part smooth random deformation eld constructed creating independent random horizontal vertical displacements generated random trigonometric series low frequency terms gaussian coe cients 
images actual size object image varies signi cantly symbol symbol symbol classes due random scaling 
shape queries rst illustrate shape query context curves tangents idealized continuum setting 
example purely motivational 
practice dealing dimensional curves continuum nite pixel lattice strokes variable width corrupted data types queries described 
observe versions digit left obtained spline interpolation center points segments shown middle away segments represent direction tangent points 
segment arrangements satisfy geometric relations indicated right tangent northeast horizontal tangent turn south hor 
hor 
hor 
nw ver 
ver 
ne hor 
hor 
nw ver 
ver 
ne hor 
left curves corresponding digit 
middle tangent con determining shapes spline interpolation 
right graphical description relations locations derivatives consistent con gurations 
horizontal tangent forth 
notice directional relations points satis ed coarse tolerances 
curves contain points tangents satisfy relations 
put di erently answer query vertical tangent northeast 
substantial transformations versions answer 
answer possible choose small number alternative arrangements away entire space covered 
tags employ primitive local features called tags provide coarse description local topography intensity surface neighborhood pixel 
trying manually characterize local con gurations interest example trying de ne local operators identify gradients various directions adopt information theoretic approach code microworld subimages process similar tree structured vector quantization 
way sidestep issues boundary detection gradients discrete world allow forms local 
approach extended grey level images 
basic idea re assign symbolic values pixel examining pixels immediate vicinity symbolic values tag types represent labels local topography 
neighborhood choose subimage containing pixel upper left corner 
cluster subimages binary splits corresponding adaptively choosing informative locations sixteen sites subimage 
note size subimages depend resolution shapes imaged 
subimages appropriate certain range resolutions roughly experience 
size adjusted higher resolution data ultimate performance classi er su er resolution test data approximately training data 
best approach multi resolution done preliminary experiments carried context grey level images objects 
large sample subimages randomly extracted training data 
corresponding shape classes irrelevant retained 
reason purpose sample provide representative database micro images discover biases scale statistics world largely independent global image attributes labels 
family subimages recursively partitioned binary splits 
possible questions site black 
criterion choosing question dividing subimages ut node equally possible groups 
corresponds reducing possible entropy empirical distribution possible binary con gurations sample ut tag type node resulting tree root 
questions tags questions asked tags 
depth tags correspond detailed description local topography depth tags eleven sixteen pixels remain unexamined 
observe tags corresponding internal nodes tree represent unions associated deeper ones 
assign tags encountered corresponding subimage proceeds tree 
stated experiments tags 
rst level site splits population nearly frequencies 
second level sites informative level tag levels common con gurations 
usually site partitions remaining subpopulation better 
way world micro images ciently coded 
ciency population restricted subimages containing black white site center obviously concentrates processing neighborhood boundaries 
grey level context useful consider general tags allowing example variations concept local homogeneity 
rst levels tree shown common con guration level nodes 
notice level tag rst bit code determines original image transform invertible redundant 
show bit tags bit tags appearing image 
tag arrangements queries involve geometric arrangements tags 
asks speci geometric arrangement tags certain types qa qa image 
shows latex symbols contain speci top instances bit tags 
bottom instances bit tags 
geometric arrangement tags tag northeast tag northwest tag 
notice xed locations description tags speci image carry locations 
image means set tags prescribed types locations satisfy indicated relationships 
notice example di erent instances digit contain arrangement 
tag depth tag corresponding questions subimage indicated mask corresponds background object asked 
neighborhoods loosely described background lower left object upper right 
similar interpretations tags 
restricted rst symbol classes digits conditional distribution classes existence arrangement image table 
simple query contains signi cant information shape 
complete construction feature set need de ne set allowable rela tionships image locations 
binary functions pairs triples planar points depend relative coordinates 
arrangement labeled hyper graph 
vertex labeled type tag edge labeled type relation 
graph example binary rela top instances geometric arrangement 
bottom stances geometric arrangement table conditional distribution digit classes arrangement 
tions 
fact experiments latex symbols restricted setting 
experiments handwritten digits ternary relationship metric type see 
binary relations compass headings north northeast east example north ofv angle 
generally points satisfy angle ofk 
denote set possible arrangements fqa ag feature set 
binary ternary relations discriminating power 
example entire family metric relationships directional relationships completely scale translation invariant 
example ternary relation ku ku tow 
points ask ku kw posterior distribution tree approx simplicity order facilitate comparisons methods restrict bounded complexity 
example consider arrangements tags relations limit exceeded experiments 
enumerating arrangements fashion qm corresponding feature vector assuming values generates bit string length contains information isavailable 
enormous 
evident determine priori features informative manageable size 
evidently bit strings partition images generate bit string atom need identical 
due invariance properties queries corresponding symbols may vary considerably scale location skew ne equivalent general 
images similar shapes 
result reasonable expect jq isvery small case principle obtain high classi cation rates simplify things conceptually assume jq indicated unreasonable assumption 
equivalent assumption shape determined error rate bayes classi er yb arg maxp zero 
needless say perfect classi cation realized due size full posterior computed classi er yb hypothetical 
suppose examine features constructing single binary entropy driven recursive partitioning randomization uniformly depth features examined 
exact procedure described section details important moment 
su ce say assigned interior node oft set features branch root leaf chosen sequentially current information content observed values previously chosen features 
classi er ont yt arg maxp arg max yt bayes classi er 
values order hundreds thousands expect shall refer di erence distributions appropriate norm approximation error ae 
sources error replacing subset features 
course compute tree depth features needed achieve classi cation shall return point shortly 
regardless reality know posterior distribution 
estimated training set xm xm xm random sample 
training set estimate entropy values recursive partitioning 
denote estimated dis tribution obtained simply counting number training images land terminal node oft ifl su ciently large call di erence estimation error ae jlj 
purpose multiple trees solve approximation error problem estimation error problem time 
compute store deep tree probabilities speci estimate practical training set approach tn modest depth 
way tree construction practical total number features examined su ciently large control approxi mation error 
classi er propose ys arg max nx explanation particular way aggregating information multiple trees provided 
principle better way trees classify mode ofp tn 
impractical reasonably sized training sets reasons single deep tree impractical see numerical experiments 
tradeo ae ee related tradeo bias variance discussed relative error rates classi ers analyzed detail context parameter estimation 
tree structured shape quantization standard decision tree construction breiman 
quinlan scalar valued feature attribute vector zk generally 
course pattern recognition raw data images nding right attributes widely regarded main issue 
standard splitting rules func tions vector usually involving single applying threshold occasionally involving multivariate functions trans generated features friedman gelfand delp guo gelfand sethi 
case queries candidates splitting rules 
describe manner queries construct tree 
exploring shape space set queries indexed graphs natural partial ordering graph precedes extensions 
partial ordering corresponds hierarchy structure 
small arrangements tags produce coarse splits shape space 
arrangements increase size say number tags plus relations contain information images contain 
fewer fewer images contain instance complex arrangement 
straightforward way exploit hierarchy build decision tree collection candidates splitting rules complexity queries increasing tree depth distance root 
order computationally feasible de ne minimal extension arrangement mean addition exactly relation existing tags addition exactly tag relation binding new tag existing 
binary arrangement mean tags relation collection associated queries denoted build tree follows 
root search choose leads greatest reduction mean uncertainty 
standard criterion recursive partitioning machine learning elds 
denote chosen 
data points child node search data points child node instances pending arrangement 
search minimal extensions choose leads greatest reduction uncertainty existence ofa 
digits taken depth node tree 
measure uncertainty shannon entropy 
expected uncertainty random jz log de neh jz way thatp replaced conditional probability jb 
tree history fqa qk meaning second query chosen answer rst qa third query chosen answers rst andq forth 
pending arrangement say aj deepest arrangement path root 
bt minimal extensions aj 
continue fashion examples node 
images lie node pending arrangement vertices 
separated asking presence new tag separated asking question relative angle existing vertices 
particular tags associated vertices indicated 
stopping criterion satis ed number data points terminal node falls threshold 
tree may regarded discrete random terminal node corresponds di erent value oft practice compute expected entropies estimate training set replaced empirical distribution fx computing entropy values 
randomization despite growth restrictions procedure practical number bi nary arrangements large minimal extensions complex arrangements 
addition tree fresh sample data points tree little di erence trees 
solution simple searching admissible queries node restrict search small random subset 
structural description notice connected arrangements selected meaning tags neighbors participate relation connected sequence neighboring tags 
result training complex standard recursive partitioning 
node list assigned data point consisting instances pending arrangement including coordinates participating tag 
data point passes child instances incremented maintained updated rest deleted 
data points bookkeeping 
far simpler possibility sampling exclusively binary arrangements vertices relation listed order 
fact imagine evaluating queries data point 
vector variety standard classi ers including decision trees built standard fashion 
case pending arrangements unions binary graphs disconnected 
approach simpler faster implement preserves semi invariance 
price dear lose common global characterization shape terms large connected graph 
referring pending arrangements terminal nodes branch de nition graph shapes node 
description 
di erence connected graph union binary graphs illustrated follows 
relative entire population random selection quite carry information measured say jq 
hand random choice queries say tags information nearly data points answer words sense start binary arrangements 
assume restricted subset fqa determined moderate complexity 
general subsets nodes determined answers situation virtually 
small subset randomly sampled binary arrangement yield signi cant drop uncer tainty randomly sampled query minimal extensions observations samples data sets 
top spot noise 
middle duplication 
bottom severe perturbations 
veri ed experimentally omit details 
distinction pronounced images noisy top panel contain structured backgrounds bottom panel false positives arrangements tags 
chance nding complex arrangements utilizing noise tags background tags smaller 
put di erently structural description robust list attributes 
situation complex shapes see example middle panel shapes created duplicating symbol times shifts 
random choice minimal extensions carries information random choice semi invariance bene structural description refer semi invariance 
history aj pending arrangement 
minimal extension aj shape wewant max qa jy bt qa jy bt words images class answer way 
terms entropy semi invariance equivalent relatively small values bt 
averaging classes turn equivalent small values ofh bt 
order verify property created trees depth data set described samples symbol class 
non terminal tree average value ofh bt calculated randomly sampled minimal exten sions 
nodes mean entropy entropy distribution 
standard deviation nodes queries 
clear decrease average entropy increase degree invariance depth node increases 
estimated entropy severe deformations 
variable data set approximately double range rotations log scale log skew relative tothe values non linear deformations corresponding numbers 
rotations sampled degrees log scale log skew doubling variance random non linear deformation see bottom panel corresponding mean entropy corresponding split 
words average percent images shape class answer way new query 
notice invariance property independent power query extent qa peaked distribu 
due symmetry mutual information bt bt means seek question maximizes reduction conditional entropy ofy assume second term right small due semi invariance need nd query 
class variable points possibility unsupervised learning discussed section 
unsupervised learning outline ways construct trees unsupervised mode class xj samples xj clearly decreases uncertainty determined generally ift tree components contain considerable information shape class 
recall supervised mode chosen qm random sample admissible queries event inx corresponding answers previous queries 
notice typically equivalent simply maximizing information qm terms depend onm 
light discussion preceding section semi invariance rst term ignored focus maximizing second term 
way motivate criterion inwhich case qm rst term query choice maximizing 
recall entropy values estimated training data binary 
follows growing tree aimed reducing nding node query best splits data node equal parts 
results fact log log reduces minimizing jp 
way generate shape quantiles clusters ignoring class labels 
tree highly correlated class tree grown samples representing shape classes 
words clustering trees produce generic quantization shape space 
fact trees classify new shapes see 
experimented trees splitting criterion described unsupervised question metric dq mx qm qm statement true 
leads toy sense divide data child homogeneous possible respect omit details 
clustering methods lead classi cation rates course inferior obtained splits determined separating classes surprisingly high experiment reported 
multiple trees seen small random subsets admissible queries contain query informative shape class 
happens trees constructed training set 
family queries large di erent queries tag arrangements address di erent aspects shape separate trees provide separate structural descriptions characterizing shapes di erent points view 
visually illustrated image shown instance pending graph terminal node di erent trees 
aggregating information provided family trees see yield accurate robust classi cation 
demonstrated experiments remainder 
generating multiple trees randomization proposed geman amit wilder 
previously authors advanced methods generating multiple trees 
earliest weighted voting trees casey uses di erent splitting criteria breiman uses bootstrap replicates dietterich bakiri introduce novel idea replacing multiclass learning problem family class problems dedicating tree 
papers deal xed size feature vectors coordinate questions 
authors report gains accuracy stability 
graphs image terminal nodes di erent trees aggregation suppose family tn 
best classi er ya arg max tn feasible see 
option regard trees high dimensional inputs standard classi ers 
tried cart linear nonlinear discriminant analysis means clustering nearest neighbors improvement simple averaging see amount training data 
averaging mean 
denote posterior distribution denotes terminal node 
write tn random variable tn 
probabilities parameters system problem estimating discussed 
de ne nx tn arithmetic average distributions leaves reached mode class assigned data point ys arg max training database samples symbol distribution described trees average tested performance test set samples symbol 
classi cation rate 
experiment repeated times similar results 
hand growing unsupervised trees average depth labeled data estimate terminal distributions classi cation rate achieved 
dependence training set performance classi ers constructed training samples adversely ected dependence particular sample 
way measure consider population training sets particular size compute data point le denotes error classi er 
averages may averaged 
average error decomposes terms corresponding bias variance geman bienenstock doursat 
roughly speaking bias term captures systematic errors classi er design variance term measures error component uctuations generally parsimonious designs relatively unknown parameters yield low variance highly biased decision boundaries complex nonparametric classi ers neural networks parameters su er high variance enormous training sets 
generalization requires striking balance 
see geman 
comprehensive treatment bias variance dilemma see discussions breiman kong dietterich jain 
simple carried measure dependence classi er ys training sample systematically explore decomposition mentioned 
sets trees di erent training sets consisting samples symbol 
average classi cation rate standard deviation 
table shows number images test set correctly labeled classi ers 
example see test points correctly labeled times 
plurality classi ers improves classi cation rate pointwise variability classi ers 
decision boundaries performance fairly stable respect 
correct classi ers 
points table number points function number correct classi ers attribute relatively small variance component aggregation weakly dependent trees turn results randomization 
bias issue com plex de nitely noticed certain types structural errors experiments handwritten digits nist database example certain styles writing systematically misclassi ed despite randomization ects 
relative error rates due estimation error favor trees modest depth deep ones expense theoretically higher error rates perfect estimation possible 
section analyze error rates alternative classi ers discussed asymptotic case nite data assuming total number features examined held xed presumably large guarantee low approximation error 
implications nite data outlined 
tn just cases questions 
course practical values mentioned illuminating compare hypothetical performance methods 
suppose criterion minimize error rate trees arg max maxp maximum trees 
error rate corresponding classi er arg 
notice require solution global optimization problem generally intractable accounting nearly universal adoption greedy tree growing algorithms entropy reduction 
notice minimizing jt error amounts basically thing 
lete ya ande ys error rates ya ys respectively 
ya ys rst inequality results observation trees combined tree simply terminal node oft terminal node new tree forth 
error rate tree constructed ya 
error rate oft minimal trees lower ya 
ys function oft tn second inequality follows standard argument ys tn arg max tn jt tn ya parameter estimation terms tree depth limiting factor parameter estimation computation storage 
tn andp unknown estimated training data 
cases ya nd parameters estimate number shape classes ys kn parameters 
number data points available parameter klk nd rst cases klk aggregation 
example consider family ofn trees described classify latex symbols 
average depth isd approximately parameters nearly zero 
reported largest elements estimated rest set zero 
emphasized parameter estimates re ned inde nitely additional samples form incremental learning 
ya arg tn estimation problem overwhelming assuming conditional independence model dependence 
illustrate tried compare magnitudes ofe ya withe ys simple case 
trees classify just symbols digits 
trees constructed training set samples symbol 
ys error rate just test set samples symbol error rate 
unfortunately large estimate full posterior trees 
consequently tried samples symbol esti mation 
trees error rate consistent samples symbol slightly lower ys 
trees signi cant gap estimated ya onl samples symbol estimated value ofe ya compared fore ys 
trees samples symbol estimate ofe ya itwas better ys required samples symbol 
go samples symbol 
ultimately ya better amount data needed demonstrate prohibitive trees 
evidently problems encountered trying estimate error rate deep tree 
performance bounds divide cases individual trees multiple trees 
analysis individual trees concerns ideal case questions shape classes atomic natural metric shape classes obtain bounds expected uncertainty number queries terms metric initial distribution classes 
key issue multiple trees weak dependence analysis focused dependence structure trees 
individual trees questions suppose rst shape class atomic consists single atom de ned 
words hypothesis unique codeword denote qm determined byy setting corresponds exactly mathematical version questions game 
initial distribution 
binary sequence qm qm determines subset hypotheses answer query qm 
codewords distinct asking questions eventually determine mathematical problem nd ordering queries minimizes mean number queries needed mean uncertainty xed number queries 
best known example query subset kg optimal strategy hu man code case mean number queries required lies interval see cover thomas 
suppose represent indices queries 
mean residual uncer tainty queries jq jq jq consequently stage query divides active hypotheses groups mass smaller group jq kh 
mean decision time 
unsupervised trees produced jq greater corresponding nodes 
assumptions degree separation codewords obtain bounds mean decision times expected uncertainty queries terms prior distribution types calculations easier hellinger measure uncertainty shannon entropy 
probability vector pj de ne de neg andg qm way entropy 
andh similar properties example minimized point mass uniform distribution follows jensen log 
initial amount uncertainty subset fm mkg mg bayes rule fact thatp obtain ky suppose subsets fm mkg allowing tion 
average mk ky mk dq consequently better average subset queries satis es dq minc dq residual uncertainty 
order hypotheses uniform starting distribution need log log queries ork logk small 
clear general result eliminate fraction remaining hypotheses new query 
value ofk large practical realistic values due storage express divide conquer nature recursive partitioning logarithmic dependence number hypotheses 
needless say compound case realistic number atoms shape class measure complexity 
example expect atoms handwritten digit class printed font class 
compound case obtain results similar mentioned considering degree homogeneity classes degree separation classes 
example index replaced maximum codewords class minimum codewords di erent classes 
bounds obtained call trees deep deeper empirically demonstrated obtain discrimination 
achieve practice due semi invariance guaranteeing small extraordinary richness world spatial relationships guaranteeing large 
multiple trees weak dependence statistical perspective randomization leads weak conditional dependence trees 
example correlation andt small 
words class image knowing leaf oft aid predicting leaf reached int 
section analyze dependence structure trees obtain crude lower bound performance classi er ys xed family tn constructed xed training set investigating asymptotic formance ys jlj 
nite training data tree arbitrarily deep leading arbitrarily high classi cation rates nonparametric classi ers generally strongly consistent 
ec ec denote mean conditioned ec ni tn jy 
assumptions mean vector turn true practice 
arg ec ec validity rst clear table 
assumption says amount aggregate distribution true class tends uniformly distributed classes 
denote dimensional simplex probability vectors arg maxd cg subset 
de ne euclidean distance uc boundary 
clearly ec implies arg maxd euclidean norm 
bound misclassi cation rate 
need compute clearly uc cf uc symmetry arguments point uc achieves minimum distance lie sets union 
straightforward computation involving orthogonal projections yields ck 
chebyshev inequality crude upper bound misclassi cation rate class obtained follows 
ys cjy arg max cjy ec cjy ek ec cn kx nx var tn jy cov tn tm jy denote sum conditional variances sum conditional covariances averaged trees nx kx see var tn jy ys cjy kx cov tn tm jy ck small compared parameters inequality yields coarse bounds 
clear assumptions high class ec table estimates classes classi cation rates feasible long su ciently small su ciently large estimates tn poor 
observe trees form simple random sample large population trees suitable distribution due randomization aspect tree construction 
recall node splitting rule chosen small random sample queries 
sum variances sample means functionals sum covariances form statistic 
trees drawn independently range corresponding variables small typically standard statistical arguments imply sample means close corresponding population means moderate trees say 
pk words ce tex jy jy conditions translate conditions corresponding expectations performance variability trees ignored 
table shows estimates resulting misclassi cation ys cjy 
pairs random trees classes estimate bounds crude re ned considering higher order joint moments trees 
generalization convenience consider types generalization referred interpolation extrapolation 
terms may standard decidedly ad hoc 
interpolation easier case training testing samples randomly drawn number training samples su ciently large cover space consequently test points classi er asked interpolate nearby training points 
extrapolation mean situations training samples represent space test samples drawn 
examples training small number samples symbol ii di erent perturbation models generate training test sets adding severe scaling skewing iii degrading test images correlated noise lowering resolution 
example occurred rst nist competition wilkinson geist janet hammond hull larsen wilson hand printed digits test set written di erent population distributed training set 
surprisingly distinguishing feature winning algorithm size diversity actual samples train classi er 
way situations mixture ipi correspond writer populations perturbation models levels degradation complex visual recognition problems number terms large training samples represent sample order gauge di culty problem shall consider performance classi ers nearest neighbor classi cation optimal setting 
nearest neighbors benchmark common see geman 
lu 
nn raw refer nearest neighbor classi cation hamming distance binary image space bitmaps 
clearly wrong metric helps calibrate di culty problem 
course metric entirely blind invariance entirely unreasonable symbols nearly ll bounding box degree perturbation limited 
nn refer nearest neighbor classi cation binary tag arrange ments 
images compared andq computing hamming distance corresponding binary se quences 
chosen subset binary tag arrangements split 
queries total binary tag arrangements 
sample size nn raw table classi cation rates percent various training sample sizes compared nearest neighbor methods 
due invariance properties expect metric better hamming distance image space course see 
interpolation randomized trees constructed training data set samples symbols 
average classi cation rate tree test set consisting samples symbol 
performance classi er ys trees 
clearly demonstrates weak dependence random ized trees discriminating power queries 
classi er classi cation rate raw rate see table 
rates test set 
random perturbations non linear scaling rotation skew standardization done raw image see 
samples symbol nn raw climbs trees reach 
extrapolation grew trees original prototypes recursively dividing group pure leaves obtained 
course trees relatively shallow 
case half symbols recognized see table 
trees grown samples symbol tested samples exhibit latex symbols perturbed non linear deformations top 
middle 
bottom clutter 
nn raw original spot noise clutter table classi cation rates percent perturbations greater level distortion variability described point 
results appear table 
resp 
refers uniform sampling original scale twice resp 
half original scale top resp 
middle panel figures spot noise refers adding correlated noise top panel 
clutter bottom panel refers addition pieces symbols image 
distortions came addition random nonlinear deformations skew rotations 
creates confusions due extreme thinning stroke 
notice nn classi er falls apart spot noise 
reason number false positives tags due noise induce random occurrences simple arrangements 
contrast complex arrangements far image pure chance chance occurrences deeper tree 
note purpose experiments illustrate various attributes recognition strategy 
ort optimize classi cation rates 
particular tags tree making protocol experiment 
experiments repeated times variability negligible 
direction appears promising explicitly introducing di erent protocols tree tree order decrease dependence 
small experiment carried direction 
images subsampled half resolution images 
tag tree subimages subsampled data set trees grown subsampled training set 
output trees combined output original trees test data 
change classi cation rate observed original test set 
test set spot noise sets trees classi cation rate 
combined yield rate 
clearly signi cant potential improvement direction 
incremental learning universal trees mentioned earlier parameters incrementally updated new training samples 
set trees actual counts training set normalized distributions kept terminal nodes new labeled sample obtained dropped trees corresponding counters incremented 
need keep image 
separation tree construction parameter estimation crucial 
pro vides mechanism gradually learning recognize increasing number shapes 
trees originally constructed training samples small number classes eventually updated accommodate new classes parameters re estimated 
addition data points observed estimates terminal distributions perpetually re ned 
trees deepened data avail able 
terminal node assigned randomly chosen list minimal extensions pending arrangement 
answers queries calculated stored new labeled sample reaches node need keep sample 
su ciently samples accumulated best query list determined simple calculation stored information node split 
adaptivity additional classes illustrated experiment 
set trees grown training samples classes randomly chosen full set classes 
trees grown depth just 
original training set samples class classes terminal distributions estimated recorded tree 
aggregate classi cation rate classes compared full training set quantization parameter estimation 
clearly fty shapes su cient produce reasonably sharp quantization entire shape space 
improving parameter estimates recall trees grown pure symbols reached test set 
terminal distributions trees updated original training set samples symbol 
classi cation rate test set climbed 
fast indexing problem recognition paradigms suchas hypothesize test determining particular hypothesis test 
indexing shape library central issue especially methods matching image data model data involving large numbers shape classes 
standard approach model vision ag ble interpretations searching key features discriminating parts hierarchical representations 
indexing ciency inversely related stability respect image dation 
deformable templates highly robust provide global interpretation image data 
deal searching may necessary nd right template 
method invariant features lies extreme axis indexing shot tolerance distortions data 
attempted formulate trade manner susceptible tation 
noticed multiple trees appear er reliable mechanism fast indexing framework terms narrowing number possible classes 
example original experiment classi cation rate highest ranking classes aggregate distribution contained true class images test set size samples class 
example true label top cases 
experiments suggest high recognition rates obtained nal tests dedicated ambiguous cases determined example mode handwritten digit recognition ocr problem variations literature immense survey mori suen yamamoto 
area handwritten character recognition di cult problem recognition unconstrained script zip codes hand drawn checks formidable challenge 
problem consider line recognition isolated binary digits 
special case attracted enormous attention including competition sponsored national institute standards technology nist wilkinson 
solution matches human performance commercially viable restricted situations 
comparisons methods see bottou 
lucid discussion brown 

best reported rates obtained research group training testing composites nist training test sets bottou 

brief summary results experiments tree shape quantization method nist data base 
detailed description see geman 

experiments portions nist database consists approximately binary images isolated digits written writers 
images vary widely dimensions ranging rows vary stroke thickness attributes 
training testing 
random sample test set shown 
results reported literature utilize sophisticated methods preprocessing thinning slant correction size normalization 
sake comparison experiments crude form slant correction scaling thinning 
trees 
stopped splitting number data points second largest class fell 
depth terminal nodes number questions asked tree varied widely trees 
average number terminal nodes average classi cation rate determined mode terminal distribution 
best error rate achieved single tree 
classi er tested ways 
preprocessed scaled slant corrected random sample test images top bottom pre processing 
classi cation rate vs number trees 
test set manner training set 
resulting classi cation rate rejection 
shows classi cation rate grows number trees 
recall estimated class image mode aggregate distribution 
measure con dence estimate value mode call itm 
provides natural mechanism rejection classifying images rejection corresponds tom 
example classi cation rate percent rejection percent rejection 
doubling number trees classi cation rates zero percent rejection respectively 
performed second experiment inwhich test data preprocessed manner training data fact test images classi ed utilizing size bounding box 
especially important presence noise clutter essentially impossible determine size bounding box 
test image classi ed set trees resolutions original halved xed 
highest resulting modes determines classi cation 
classi cation rate 
classify approximately digits second single processor sun sparcstation special orts optimize code time approximately equally divided transforming tags answering questions 
test data dropped trees parallel case classi cation approximately times faster 
comparison ann comparison anns natural view widespread pattern recog nition werbos common attributes 
particular approach model sense utilizing explicit shape models 
addition compu cient comparison model methods memory methods nearest neighbors 
cases performance diminished dedication training data tting problems result de cient approximation capacity parameter estimation 
key di erences described subsections 
posterior distribution generalization error clear feature vector accommodated ann framework 
direct successful anns high dimensional input su er poor generalization baum haussler large training sets necessary order approximate complex decision boundaries jain 
role posterior explicit approach leading somewhat di erent explanation sources error 
case approx imation error results replacing entire feature set adaptive subset really subsets tree 
di erence full posterior tree approximations thought analog approximation error analysis learning capacity anns see niyogi girosi 
case interested set functions generated family networks xed architecture 
centered approximation particular function 
example lee 
consider squares approximation bayes rule conditional independence assumptions features error vanishes number hidden units goes nity 
estimation error case results inadequate amount data estimate conditional distributions leaves trees 
analogous situation anns known network unlimited capacity problem estimating weights limited data niyogi girosi 
classi ers anns address classi cation nonparametric fashion 
contrast approach classi er explicit functional input output repre sentation feature vector course great parameters may need estimated order achieve low approximation error complex classi ca tion problems shape recognition 
turn leads relatively large variance component bias variance decomposition 
view di culties small er ror rates achieved anns handwritten digits shape recognition problems noteworthy 
invariance visual system structure anns shape classi cation partially motivated observations processing visual cortex 
emphasis parallel processing local integration information layer 
local integration done spatially stationary manner translation invariance achieved 
successful example context handwritten digit convolution neural net lenet lecun 

scale deformation forms invariance achieved lesser degree partially gray level data soft thresholds mainly utilizing large training sets sophisticated image normalization procedures 
neocognitron fukushima miyake fukushima wake inter mechanism achieve degree robustness shape deformations scale changes addition translation invariance 
done hidden layers carry local oring operation mimicking operation complex neurons 
layers referred type layers fukushima miyake dual resolution version aimed achieving scale invariance larger range 
spatial stationarity connection weights layers multiple res forms oring aimed achieving invariance 
complex cell basic evidence operation visual system 
additional evidence disjunction global scale cells inferotemporal cortex large receptive elds 
respond certain shapes locations receptive eld variety scales variety non linear deformations proportions certain parts see ito tamura fujita tanaka ito fujita tamura tanaka 
extremely ine cient obtain invariance oring responses shapes 
preferable carry global oring level primitive generic features serve identify discriminate large class shapes 
demonstrated global features consisting geometric arrangements local responses tags just 
detection tag arrangements described preceding sections viewed extensive global oring operation 
ideal arrangement vertical edge northwest horizontal edge tested locations scales large range angles northwest vector 
positive response produces positive answer associated query 
leads property wehave called semi invariance allows method perform preprocessing large training sets 
performance extends transformations perturbations encountered training scale changes spot noise clutter 
local responses employ similar ones employed rst level convolution neural nets example fukushima wake 
level geometric arrangements de ned explicitly designed accommodate priori assumptions shape regularity variability 
features convolution neural nets implicitly derived local inputs layer slack obtained local oring soft thresholding carried layer layer 
clear su cient obtain required level invariance clear portable problem 
evidence correlated activities neurons distant receptive elds accumulating lgn retina singer 
gilbert das ito kapadia report increasing evidence extensive horizontal connections 
large optical point spreads observed subthreshold neural activation appears area covering size receptive elds 
orientation sensitive cell res response stimulus receptive eld subthreshold activation observed cells orientation preference wide area outside receptive eld 
appears integration mechanisms global de nitely complex previously thought 
features described contribute bridging gap existing computational models new experimental ndings regarding visual system 
wehave studied new approach shape classi cation illustrated performance high dimensions respect number shape classes degree variability classes 
basic premise shapes distinguished su ciently robust invariant form recursive partitioning 
quantization shape space growing binary classi cation trees geometric arrangements local topographic codes splitting rules 
arrangements semi invariant linear nonlinear image transformations 
result method generalizes samples encountered training 
addition due separation quantization estimation framework accommodates unsupervised incremental learning 
codes primitive redundant arrangements involve simple spa tial relationships relative angles distances 
necessary isolate dis points shape boundaries special di erential topological structures perform form grouping matching functional optimization 
con virtually nite collection discriminating shape features generated elementary computations pixel level 
classi er full feature set realizable impossible know apriori features informative selected features built trees time inductive learning 
data driven non parametric method shape classi cation ann comparison drawn terms invariance generalization error leading prior information plays relatively greater role approach 
wehave experimented nist database handwritten digits synthetic database constructed linear nonlinear deformations latex symbols 
despite large degree class variability setting evidently simpli ed images binary shapes isolated 
looking ahead central question recognition strategy extended unconstrained scenarios involving example multiple solid objects general poses variety image formation models 
aware approach di ers standard computer vision emphasizes viewing object models matching advanced geometry 
currently modifying strategy order recognize objects structured backgrounds cluttered desktops grey level images acquired ordinary video cameras see 
looking applications document processing omni font ocr 
acknowledgment 
authors ken wilder valuable sug carrying experiments handwritten digit recognition 
baum haussler 
size net gives valid generalization neural comp 

binford levitt 
quasi invariants theory exploitation proc 
image understanding workshop washington pp 

bottou cortes denker drucker guyon jackel lecun muller sackinger simard vapnik 
comparison classi er methods case study handwritten digit recognition proc 
ieee inter 
conf 
pattern recognition pp 

breiman 
bagging predictors technical report department statistics university california berkeley 
breiman friedman olshen stone 
classi cation regression trees wadsworth belmont ca 
brown 
comparison decision tree classi ers backpropagation neural networks multimodal classi cation problems pattern recognition 
burns weiss riseman 
view set line segment features ieee trans 
pami 
casey 
processor ocr system ibm journal research development 
casey nagy 
decision tree design probabilistic model ieee trans 
information theory 
cover thomas 
elements information theory john wiley new york 
dietterich bakiri 
solving multiclass learning problems error correcting output codes arti cial intell 
res 

forsyth mundy zisserman coelho heller rothwell 
invariant descriptors object recognition pose ieee trans 
pami 
friedman 
recursive partitioning decision rule nonparametric classi ca tion ieee trans 
comput 

fukushima miyake 
neocognitron new algorithm pattern recognition tolerant deformations shifts position pattern recognition 
fukushima wake 
handwritten alphanumeric character recognition neocognitron ieee trans 
neural networks 
gelfand delp 
tree structured classi ers sethi jain eds arti cial neural networks statistical pattern recognition north holland amsterdam pp 

geman amit wilder 
joint induction shape features tree clas si ers technical report university massachusetts amherst 
geman bienenstock doursat 
neural networks bias variance dilemma neural computation 
gilbert das ito kapadia 
spatial integration cortical dynamics proc 
natl 
acad 
sci 


properties simulated neurons model primate inferior temporal cortex cerebral cortex 
guo gelfand 
classi cation trees neural network feature extraction ieee trans 
neural networks 
hastie buja tibshirani 
penalized discriminant analysis annals statistics 

novel feature recognition neural network application character recognition ieee trans 
pami 
ito fujita tamura tanaka 
processing contrast polarity visual images inferotemporal cortex macaque monkey cerebral cortex 
ito tamura fujita tanaka 
size position invariance neuronal response monkey inferotemporal cortex jour 
neuroscience 

object recognition geometric queries proc 
image com bordeaux france 
jung 
nagy 
joint feature classi er design ocr proc 
third inter 
conf 
document analysis processing montreal pp 

lu 
shape texture recognition neural network sethi jain eds arti cial neural networks statistical pattern recognition north holland amsterdam 
personnaz dreyfus 
handwritten digit recognition neural networks single layer training ieee trans 
neural networks 
kong dietterich 
error correcting output coding corrects bias variance technical report 
lamdan schwartz wolfson 
object recognition matching ieee int 
conf 
computer vision pattern rec pp 

lecun boser denker henderson howard hubbard jackel 
handwritten digit recognition back propagation network ed advances neural information vol 
morgan kau man denver 
lee srihari 
bayesian neural network pattern recognition theoretical connection empirical results handwritten characters sethi jain eds arti cial neural networks statistical pattern recognition north holland amsterdam 
martin pitman 
recognizing hand printed letters digits backpropagation learning neural computation 
mori suen yamamoto 
historical review ocr research devel opment proc 
ieee vol 
pp 

mundy zisserman 
geometric invariance computer vision mit press cambridge 
singer 
long range synchronization oscillatory light responses cat retina lateral geniculate nucleus nature 
niyogi girosi 
relationship generalization error hypothesis complexity sample complexity radial basis functions neural comp 

quinlan 
induction decision trees machinelearning 
jain 
small sample size problems designing arti cial neural networks sethi jain eds arti cial neural networks statistical pattern recognition north holland amsterdam 
reiss 
recognizing planar objects invariant image features lecture notes computer science springer verlag berlin 
sabourin 
optical character recognition neural network neural networks 
sethi 
decision tree performance enhancement arti cial neural network implementation sethi jain eds arti cial neural networks statistical pattern recognition north holland amsterdam 

multiple binary decision tree classi ers pattern recognition 
simard lecun denker 
memory character recogni tion transformation invariant metric ieee inter 
conf 
pattern recog pp 

werbos 
links arti cial neural networks ann statistical pattern recognition sethi jain eds arti cial neural networks statistical pattern recognition north holland amsterdam 
wilkinson geist janet hammond hull larsen wilson 
rst census optical character recognition system conference technical report nistir nat 
inst 
standards technol gaithersburg md 
