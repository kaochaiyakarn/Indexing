originally published proceedings usenix annual technical conference san diego california january lmbench portable tools performance analysis larry mcvoy silicon graphics carl staelin hewlett packard laboratories information usenix association contact 
phone 
fax 
email office usenix org 
www url www usenix org lmbench portable tools performance analysis lmbench micro benchmark suite designed focus attention basic building blocks common system applications databases simulations software development networking 
cases individual tests result analysis isolation customer actual performance problem 
tools currently compare different system implementations different vendors 
cases benchmarks uncovered previously unknown bugs design flaws 
results shown strong correlation memory system performance performance 
lmbench includes extensible database results systems current late 

lmbench provides suite benchmarks attempt measure commonly performance bottlenecks wide range system applications 
bottlenecks identified isolated reproduced set small microbenchmarks measure system latency bandwidth data movement processor memory network file system disk 
intent produce numbers real applications reproduce frequently quoted somewhat reproducible marketing performance numbers 
benchmarks focus latency bandwidth performance issues usually caused latency problems bandwidth problems combination 
benchmark exists captures unique performance problem important applications 
example tcp latency benchmark accurate predictor oracle distributed lock manager performance memory latency benchmark gives strong indication verilog simulation performance file system latency benchmark models critical path software development 
lmbench dev identify evaluate system performance bottlenecks machines 
entirely possible computer architectures changed advanced years render parts larry mcvoy silicon graphics carl staelin hewlett packard laboratories benchmark suite obsolete irrelevant 
lmbench widespread sites users system designers 
cases lmbench provided data necessary discover correct critical performance problems gone unnoticed 
lmbench uncovered problem sun memory management software pages map location cache effectively turning kilobyte cache cache 
lmbench measures system ability transfer data processor cache memory network disk 
measure parts system graphics subsystem mips mflops throughput saturation stress graphics multiprocessor test suite 
frequently run multiprocessor mp systems compare performance uniprocessor systems take advantage multiprocessor features 
benchmarks written standard portable system interfaces facilities commonly applications lmbench portable comparable wide set unix systems 
lmbench run aix hp ux irix linux freebsd netbsd osf solaris sunos 
part suite run windows nt 
lmbench freely distributed free software foundation general public license stallman additional restriction results may reported benchmarks unmodified 

prior benchmarking performance analysis new endeavor 
benchmark suites list 
set similar benchmarks 
disk benchmarks iostone park wants benchmark measures memory subsystem tests fit easily cache 
wolman systematic file system disk benchmark complicated unwieldy 
mcvoy reviewed benchmarks lacking took long run complex solution fairly simple problem 
wrote small simple benchmark measures sequential random far faster iostone 
part mcvoy results checked sun internal benchmarks 
proved accurate benchmarks 
disk vendor routinely performance testing disk drives 
chen patterson chen chen measure performance variety workloads automatically varied test range system performance 
efforts differ interested cpu overhead single request capacity system 
berkeley software distribution suite bsd effort generated extensive set test benchmarks regression testing quality performance bsd releases 
basis ideas reasons missing tests memory latency tests results tended obscured mountain numbers wrong copyright wanted free software foundation general public license 
ousterhout operating system benchmark ousterhout proposes system benchmarks measure system call latency context switch time file system performance 
ideas basis trying go farther 
measured complete set primitives including hardware measurements went greater depth tests context switching went great lengths benchmark portable extensible 
networking benchmarks netperf measures networking bandwidth latency written rick jones hewlett packard 
lmbench includes smaller complex benchmark produces similar results 
ttcp widely benchmark internet community 
version benchmark routinely delivers bandwidth numbers numbers quoted 
stream benchmark memory bandwidth measurements results large number high systems 
discovered results versions 
probably include benchmarks 
summary rolled wanted simple portable benchmarks accurately measured wide variety operations consider crucial performance today systems 
portions benchmark suites include similar includes portable far complex 
filling tastes great 

benchmarking notes 
sizing benchmarks proper sizing various benchmark parameters crucial ensure benchmark measuring right component system performance 
example memory memory copy speeds dramatically affected location data size parameter small data cache performance may times faster data memory 
hand memory size parameter big data paged disk performance may slowed extent benchmark finish lmbench takes approach cache memory size issues benchmarks affected cache size run loop increasing sizes typically powers maximum size reached 
results may plotted see benchmark longer fits cache 
benchmark verifies sufficient memory run benchmarks main memory 
small test program allocates memory clears memory strides memory page time timing 
takes microseconds page longer memory 
test program starts small works forward memory seen memory limit reached 

compile time issues gnu compiler gcc compiler chose gav reproducible results platforms 
gcc vendor supplied cc 
benchmarks compiled optimization benchmarks calculate clock speed context switch times compiled optimization order produce correct results 
optimization flags enabled wanted results commonly seen application writers 
benchmarks linked default manner target system 
systems binaries linked shared libraries 

multiprocessor issues multiprocessor systems ran benchmarks way uniprocessor systems 
systems allow users pin processes particular cpu results better cache reuse 
pin processes defeats mp scheduler 
certain cases decision yields interesting results discussed 

timing issues clock resolution benchmarks measure elapsed time reading system clock gettimeofday interface 
systems interface resolution milliseconds long time relative benchmarks results measured tens hundreds microseconds 
compensate coarse clock resolution benchmarks hand tuned measure operations single time interval lasting clock ticks 
typically done executing operation small loop unrolled operation exceedingly fast dividing loop time loop count 
caching benchmark expects data cache benchmark typically run times result recorded 
benchmark want measure cache performance sets size parameter larger cache 
example benchmark default copies megabytes megabytes largely defeats second level cache today 
note benchmarks trying defeat file process page cache hardware caches 
variability results benchmarks notably context switch benchmark tendency vary quite bit 
suspect operating system set physical pages time process created seeing effects collisions external caches 
compensate running benchmark loop minimum result 
users interested accurate data advised verify results platforms 
results included database donated users created authors 
benchmarking practice suggests name multi operating spec list model uni system cpu mhz year int price ibm powerpc ibm uni aix 
mpc ibm power ibm uni aix 
power freebsd tp xe uni freebsd pentium hp hp mp hp ux pa sgi challenge sgi challenge mp irix sgi indigo sgi indigo uni irix linux alpha dec uni linux alpha linux ram uni linux pentium linux intel uni linux pentium pro dec alpha dec uni osf alpha dec alpha dec mp osf alpha sun ultra sun ultra uni sunos ultrasparc sun sc sun sc mp sunos solaris intel uni sunos pentium pro intel aurora uni pentium pro table 
system descriptions 
run benchmarks user machine resource intensive unpredictable processes daemons 

database lmbench includes database results useful comparison purposes 
quite easy build source run benchmark produce table results includes run 
tables produced database included lmbench 
included lmbench may reproduced incorporating new results 
information consult file lmbench howto distribution 

systems tested lmbench run wide variety platforms 
includes results representative subset machines operating systems 
comparisons similar hardware running different operating systems illuminating included examples results 
systems briefly characterized table 
please note list prices approximate year 
specint numbers little suspect vendors optimizing certain parts spec 
try quote original specint numbers 

reading result tables rest tables results benchmarks 
tables sorted best worst 
tables multiple columns results tables sorted columns 
sorted column heading bold 

bandwidth benchmarks bandwidth mean rate particular facility move data 
attempt measure data movement ability number different facilities library bcopy hand unrolled bcopy direct memory read write copying pipes tcp sockets interface interface 

memory bandwidth data movement fundamental operating system 
past performance frequently measured mflops floating point units slow microprocessor systems rarely limited memory bandwidth 
today floating point units usually faster memory bandwidth current mflop ratings maintained memory resident data cache ratings 
measure ability copy read write data varying set sizes 
results report concentrate large memory transfers 
measure copy bandwidth ways 
user level library bcopy interface 
second hand unrolled loop loads stores aligned byte words 
cases took care ensure source destination locations map lines caches direct mapped 
order test memory bandwidth cache bandwidth benchmarks copy area area 
secondary caches reach benchmarks resized reduce caching effects 
copy results represent half third memory bandwidth obtain results reading writing memory 
cache line size larger word stored written cache line typically read written 
actual amount memory bandwidth varies architectures special instructions specifically designed bcopy function 
architectures move twice memory reported benchmark advanced architectures move times memory memory read memory read overwritten memory written 
bcopy results reported table may correlated john stream benchmark results manner stream benchmark reports memory moved benchmark reports bytes copied 
numbers approximately half third numbers 
memory reading measured unrolled loop sums series integers 
systems measured integer size bytes 
loop unrolled compilers generate code uses constant offset load resulting pcs available memory machines copied 
load add word memory 
add integer add completes cycle processors 
today processor typically cycles fewer nanoseconds ns memory typically ns cache line results reported dominated memory subsystem processor add unit 
memory contents added compilers optimize loop optimization turned generate far instructions optimization 
solution add data pass result unused argument finish timing function 
memory reads represent third bcopy expect pure reads run roughly twice speed 
exceptions rule studied exceptions indicate bug benchmarks problem bcopy unusual hardware 
bcopy memory system unrolled libc read write ibm power sun ultra dec alpha hp solaris dec alpha linux freebsd linux alpha linux sgi challenge sgi indigo ibm powerpc sun sc table 
memory bandwidth mb memory writing measured unrolled loop stores value integer typically byte integer increments pointer 
processor cost memory operation approximately cost read case 
numbers reported table raw hardware speed cases 
power capable sec read rates hp pa risc prefetching systems better higher levels code optimization code hand tuned 
sun libc bcopy table better hardware specific bcopy routine uses instructions new sparc added specifically memory movement 
pentium pro read rate table higher write rate intel described machine processor memory subsystem 
write transaction turns read followed write maintain cache consistency mp systems 

ipc bandwidth interprocess communication bandwidth frequently performance issue 
unix applications composed processes communicating pipes tcp sockets 
examples include groff documentation system prepared window system remote file access wide web servers 
unix pipes interprocess communication mechanism implemented way byte stream 
stream associated file descriptor write descriptor read descriptor 
tcp sockets similar pipes bidirectional cross machine boundaries 
pipe bandwidth measured creating processes writer reader transfer data transfers 
transfer size chosen overhead system calls context switching dominate benchmark time 
reader prints timing results guarantees data moved timing finished 
tcp bandwidth measured similarly data transferred page aligned transfers transfers 
tcp implementation supports send receive socket buffers enlarged default 
hav setting transfer size equal socket buffer size produces greatest throughput implementations 
system libc bcopy pipe tcp hp linux ibm power linux alpha sun ultra dec alpha solaris dec alpha sgi indigo linux ibm powerpc freebsd sgi challenge sun sc table 
pipe local tcp bandwidth mb bcopy important test pipe write read typically implemented bcopy kernel writer kernel reader 
ideally results approximately half bcopy results 
possible kernel bcopy faster library bcopy kernel may access bcopy hardware unavailable library 
interesting compare pipes tcp tcp benchmark identical pipe benchmark transport mechanism 
ideally tcp bandwidth pipe bandwidth 
widely known majority tcp cost bcopy checksum network interface driver 
checksum driver may safely eliminated loopback case costs eliminated tcp just fast pipes 
pipe tcp results table easy see solaris hp ux done optimization 
bcopy rates table lower pipe rates pipe transfers done buffers size frequently fits caches bcopy typically copy fit cache 
table sgi indigo uniprocessor better sgi mp pipe bandwidth caching effects case processes share cache mp process communicating different cache 
tcp results table loopback mode ends socket machine 
impossible get remote networking results machines included 
interested receiving results identical machines dedicated network connecting 
results wire tcp bandwidth shown 
system network tcp bandwidth sgi sun ultra hp fddi freebsd sgi indigo hp linux mhz table 
remote tcp bandwidth mb sgi mb far fastest table 
sgi interface hardware support tcp checksums irix operating system uses virtual memory tricks avoid copying data possible 
larger transfers sgi reached mb tcp 
looking quite competitive compared fddi table fddi packets times larger 
wonder long see gigabit ethernet interfaces 

cached bandwidth experience shown reusing data file system page cache performance issue 
section measures operation interfaces read mmap 
benchmark benchmark disk activity involved 
wanted measure overhead reusing data overhead cpu intensive disk intensive 
read interface copies data kernel file system page cache process buffer buffers 
transfer size chosen minimize kernel entry overhead remaining realistically sized 
difference bcopy read benchmarks cost file virtual memory system overhead 
systems bcopy speed faster speed 
exceptions usually hardware specifically designed bcopy function hardware may available operating system 
read benchmark implemented file typically buffers 
buffer summed series integers user process 
summing done reasons apples comparison memory mapped benchmark needs touch data file system transfer data memory faster processor read data 
example sgi xfs move data memory rates excess second move data cache second 
intent measure performance delivered application dma performance memory 
libc file memory file system bcopy read read mmap ibm power hp sun ultra dec alpha solaris dec alpha linux ibm powerpc sgi challenge sgi indigo freebsd linux alpha linux sun sc table 
file vs memory bandwidth mb mmap interface provides way access kernel file cache copying data 
mmap benchmark implemented mapping entire file typically process address space 
file summed force data cache 
table system file read fast faster libc bcopy file system overhead goes zero file reread case virtually case 
er file reread faster kernel may access assist hardware available library 
ideally file mmap performance approach memory read performance mmap dramatically worse 
judging results looks potential area operating system improvements 
table power better file reread bcopy takes full advantage memory subsystem inside kernel 
mmap reread probably slower lower clock rate page faults start show significant cost 
surprising sun ultra able bcopy high rates shown table show rates file reread table 
hp opposite problem get file reread faster bcopy kernel bcopy access hardware support 
system outstanding mmap reread rates better systems substantially higher cost 
linux needs mmap code 

latency measurements latency overlooked area performance problems possibly resolving latency issues frequently harder resolving bandwidth issues 
example memory bandwidth may increased making wider cache lines increasing memory width interleave memory latency improved shortening paths increasing successful prefetching 
step improving latency understanding current latencies system 
latency measurements included suite memory latency basic operating system entry cost signal handling cost process creation times context switching interprocess communication file system latency disk latency 

memory read latency background section expend considerable effort define different memory latencies explain justify benchmark 
background bit tedious important believe memory latency measurements useful measurements 
basic latency measurement memory latency latency measurements expressed terms memory latency 
example context switches require saving current process state loading state process 
memory latency rarely accurately measured frequently misunderstood 
memory read latency definitions common increasing time order memory chip cycle time processor pins memory back time load vacuum time back back load time 
memory chip cycle latency memory chips rated nanoseconds typical speeds ns 
general overview dram architecture may hennessy 
specific information describe toshiba pertains thm module tc ajs dram sgi workstations 
ns time time ras assertion data available dram pins assuming cas access time requirements met 
possible get data dram ns time involved 
precharge time occur access 
toshiba quotes ns random read write cycle time time representative cycle time 
pin pin latency number represents time needed memory request travel processor pins memory subsystem back 
vendors pin pin definition memory latency reports 
example describing dec quotes memory latencies ns careful reading shows pin pin numbers 
spite historical precedent vendor reports definition memory latency misleading ignores actual delays seen load instruction immediately followed data loaded 
number additional cycles inside processor significant grows significant today highly pipelined architectures 
worth noting pin pin numbers include amount time takes charge lines going time increases potential number system 
mean capacitance requires longer charge times 
reason personal computers frequently better memory latencies workstations pcs typically memory capacity 
load vacuum latency load vacuum time processor wait load fetched main memory cache 
vacuum means activity system bus including loads 
number frequently memory latency useful 
basically exceed number important marketing reasons 
architects point processors implement nonblocking loads load cause stall data perceived load latency may real latency 
pressed admit cache misses occur bursts resulting perceived latencies load vacuum latency 
back back load latency back back load latency time load takes assuming instructions loads 
back back loads may take longer loads vacuum reason systems implement known critical word means subblock cache line contains word loaded delivered processor entire cache line brought cache 
load occurs quickly processor gets restarted current load second load may stall cache busy filling cache line previous load 
systems current implementation ultrasparc difference back back load vacuum 
lmbench measures back back load latency measurement may easily measured software feel software developers consider memory latency 
consider code fragment head dec alpha loop part turns instructions including load 
mhz processor ns cycle time loop execute slightly ns 
load takes ns mhz dec 
words instructions cost ns load stalls 
way look instructions load needed hide load latency 
superscalar processors typically execute multiple operations clock cycle need useful operations cache misses keep processor stalling 
benchmark illuminates tradeoffs processor cache design 
architects large cache lines bytes prefetch effect gathering line increases hit rate reasonable spatial locality 
small stride sizes high spatial locality higher performance large stride sizes poor spatial locality causing system prefetch useless data 
benchmark provides insight negative effects large line prefetch multi cycle fill operations typically atomic ev ents caches block cache accesses complete 
caches typically single ported 
having large line prefetch unused data causes extra bandwidth demands cache cause increased access latency normal cache accesses 
summary believe processors fast average load latency cache misses closer back back load number load vacuum number 
hopeful industry standardize definition memory latency 

memory read latency entire memory hierarchy measured including board data cache latency size external data cache latency size main memory latency 
instruction caches measured 
tlb latency measured saavedra stopped main memory 
measuring tlb time problematic different systems map different amounts memory tlb hardware 
benchmark varies parameters array size array stride 
size list pointers created different strides 
list walked mov code time loads list wraps measured reported 
time reported pure latency time may zero load instruction execute zero time 
zero defined clock cycle words time reported memory latency time include instruction execution time 
assumed processors load instruction processor cycle counting stalls 
words processor cache load time ns ns processor load latency reported ns additional ns load instruction 
processors manage get load address address pins load cycle get free time benchmark don know processors 
benchmark validated logic analyzer measurements sgi indy ron maryland supercomputer research center 
results memory latency benchmark plotted series data sets shown 
data set represents stride size array size varying bytes 
curves contain series horizontal plateaus plateau represents level memory hierarchy 
point plateau ends line rises marks portion memory hierarchy external cache 
machines similar memory hierarchies board cache external cache main memory main memory plus tlb costs 
variations processors retrospect bad idea calculate clock rate get instruction execution time 
clock rate load time 
dec alpha mhz memory latencies kb cache mb cache main mem log array size 
memory latency missing cache add cache hierarchy 
example alpha onboard caches 
cache line size derived comparing curves noticing strides faster main memory times 
smallest stride main memory speed cache line size strides faster memory getting hit cache line 
shows memory latencies nicely machine dec alpha 
machine example shows latencies sizes chip level motherboard level caches numbers especially considering support level cache 
board cache bytes external cache bytes 
table shows cache size cache latency main memory latency extracted memory latency graphs 
graphs tools extracting data included lmbench 
worthwhile plot graphs examine table missing details dec alpha processor second chip cache 
sorted table level cache latency think applications fit level cache 
hp ibm systems level cache count level level 
systems remarkable cache performance caches size 
cases cache delivers data clock cycle load instruction 
hp systems usually focus large caches close possible processor 
older hp multiprocessor system split way level level cache cache memory system clk 
lat 
size lat 
size latency hp ibm power linux sun ultra linux alpha solaris sgi indigo sgi challenge dec alpha dec alpha freebsd linux sun sc ibm powerpc table 
cache memory latency ns set associative cache accessible clock ns 
system primarily database server 
ibm focus low latency high bandwidth memory 
ibm memory subsystem memory close processor weakness extremely difficult evolve design multiprocessor system 
powerpc quite poor second level caches caches substantially better main memory 
pentium pro sun ultra second level caches medium speed clocks latency 
clocks fast compared hp ibm cycle latency caches similar size 
tight integration pentium pro level cache surprising high latencies 
mhz dec alpha high clock latency second level cache probably reasons needed level cache 
sgi dec large second level caches hide long latency main memory 

operating system entry entry operating system required system facilities 
calculating cost facility useful know expensive perform nontrivial entry operating system 
measure nontrivial entry system repeatedly writing word dev null pseudo device driver discard data 
particular entry point chosen optimized system measured 
entry points gettimeofday heavily heavily optimized implemented user level library routines system calls 
write dev null driver go system call table write verify user area readable look file descriptor get vnode call vnode write function return 
system system call linux alpha linux linux sun ultra freebsd solaris dec alpha sun sc hp sgi indigo dec alpha ibm powerpc ibm power sgi challenge table 
simple system call time microseconds linux clear winner system call time 
reasons twofold linux uniprocessor operating system mp overhead linux small operating system features accumulated commercial offers 
solaris doing quite fairly large commercially oriented operating systems large accumulation features 
signal handling cost signals unix way tell process handle event 
processes interrupts cpu 
signal handling critical layered systems 
applications databases software development environments threading libraries provide operating system layer top operating system making signal handling critical path applications 
lmbench measure signal installation signal dispatching separate loops context process 
measures signal handling installing signal handler repeatedly sending signal 
table shows signal handling costs 
note context switches benchmark signal goes process generated signal 
real applications signals usually go process implies true cost sending signal signal overhead plus context switch overhead 
wanted measure signal context switch overheads separately context switch times vary widely operating systems 
sgi signal processing especially hardware older generation system sigaction sig handler sgi indigo sgi challenge hp freebsd linux ibm power solaris ibm powerpc linux dec alpha linux alpha table 
signal times microseconds 
linux alpha signal handling numbers poor suspect bug especially linux numbers quite reasonable 

process creation costs process benchmarks measure basic process primitives creating new process running different program context switching 
process creation benchmarks particular interest distributed systems remote operations include creation remote process shepherd remote operation completion 
context switching important reasons 
simple process creation 
unix process creation primitive fork creates virtually exact copy calling process 
vms operating systems unix starts new process fork 
consequently fork execve fast light facts ignoring time 
lmbench measures simple process creation creating process immediately exiting child process 
parent process waits child process exit 
benchmark intended measure overhead creating new thread control includes time 
benchmark includes wait system call parent context switches parent child back 
context switches sort order microseconds system call order microseconds entire benchmark time order millisecond extra overhead insignificant 
note relatively simple task expensive measured milliseconds operations consider measured microseconds 
new process creation 
preceding benchmark create new application created copy old application 
benchmark measures cost creating new process changing process new application 
forms basis unix command line interface shell 
lmbench measures facility forking new child having child execute new program case tiny program prints hello world exits 
startup cost especially noticeable systems shared libraries 
shared libraries introduce substantial tens milliseconds startup cost 
fork fork exec fork exec system exit exit sh exit linux alpha linux linux dec alpha ibm powerpc sgi indigo ibm power freebsd hp dec alpha sgi challenge sun ultra solaris sun sc table 
process creation time milliseconds complicated new process creation 
programs start programs frequently standard interfaces system 
interfaces start new process invoking standard command interpreter bin sh start process 
starting programs way guarantees shell look requested application places user look words shell uses user path variable list places find application 
library routine looks program user path variable 
common way starting applications felt useful show costs generality 
measure starting bin sh start tiny program ran case 
table cost asking shell go look program quite large frequently times expensive just creating new process times expensive explicitly naming location new program 
results stand table poor sun ultra results 
processor fastest problem software 
room substantial improvement solaris process creation code 

context switching context switch time defined time needed save state process restore state process 
context switches frequently critical performance path distributed applications 
example multiprocessor versions irix operating system processes move data networking stack 
means processing time new packet arriving idle system includes time needed switch networking process 
typical context switch benchmarks measure just minimal context switch time time switch processes doing context switching 
feel misleading frequently active processes usually larger working set cache footprint benchmark processes 
benchmarks frequently include cost system calls needed force context switches 
example ousterhout context switch benchmark measures context switch time plus read write pipe 
systems measured lmbench pipe overhead varies context switch time careful factor pipe overhead 
number processes 
context switch benchmark implemented ring processes connected unix pipes 
token passed process process forcing context switches 
benchmark measures time needed pass token times process process 
transfer token costs context switch overhead passing token 
order calculate just context switching time benchmark measures cost passing token ring pipes single process 
overhead time defined cost passing token included reported context switch time 
size processes 
order measure realistic context switch times add artificial variable size cache footprint switching processes 
cost context switch includes cost restoring user level state cache footprint 
cache footprint implemented having process allocate array data sum array series integers receiving token passing token process 
systems cache data context switches working set benchmark slightly larger number processes times array size 
worthwhile point overhead mentioned includes cost accessing data way actual benchmark 
arrays virtual address processes 
overhead measured single process cost typically cost hot caches 
size plotted line context switch times axis number processes axis process size data set 
process size hot cache overhead costs pipe read writes data access labeled size kb overhead 
size kilobytes overhead microseconds 
context switch time include context switch provided benchmark processes fit cache 
total size benchmark processes larger cache size cost context switch include cache misses 
trying show realistic context switch times function size number processes 
context switches linux mhz size kb overhead size kb overhead size kb overhead size kb overhead size kb overhead 
context switch times processes results intel pentium pro system running linux mhz shown 
data points labeled working set due sum data processes 
actual working set larger includes process kernel overhead 
expect context switch times stay constant working set approximately size second level cache 
intel system second level cache context switch times stay constant marked graph 
cache issues context switch benchmark deliberate measurement effectiveness caches process context switches 
cache include process identifier pid called address space identifier part address cache flushed ev ery context switch 
cache map virtual addresses different processes different cache lines cache appear flushed context switch 
caches cache context switches grouping lower left corner graph appear series straight horizontal parallel lines 
number processes matter process case just bad process case cache useful context switches 
processes processes system kb kb kb kb linux linux linux alpha ibm power sun ultra dec alpha ibm powerpc hp freebsd solaris sgi indigo dec alpha sgi challenge sun sc table 
context switch time microseconds picked points graph extracted values table 
complete set values tools graph included lmbench 
note multiprocessor context switch times frequently expensive uniprocessor context switch times 
multiprocessor operating systems tend complicated scheduling code 
believe multiprocessor context switch times uniprocessor times 
linux quite context switching especially architectures 
comparing linux processes linux processes apparent wrong linux case 
look back table find part cause 
second level cache latency substantially worse alpha 
poor second level cache behavior powerpc surprising context switches especially larger sized cases 
sun ultra context switches quite part enhancements register window handling sparc 

interprocess communication latencies interprocess communication latency important operations control messages process frequently system 
time tell remote process pure overhead frequently critical path important functions distributed applications databases network servers 
interprocess communication latency benchmarks typically form pass small message byte back forth processes 
reported results microseconds needed round trip 
way timing half round trip right 
er cpu cycles tend somewhat asymmetric trip receiving typically expensive sending 
pipe latency 
unix pipes interprocess communication mechanism implemented way byte stream 
stream associated file descriptor write descriptor read descriptor 
pipes frequently local ipc mechanism 
simplicity pipes frequently fastest portable communication mechanism 
pipe latency measured creating pair pipes forking child process passing word back forth 
benchmark identical process zero sized context switch benchmark includes context switching time pipe overhead results 
table shows round trip latency process process back process system pipe latency linux linux linux alpha sun ultra ibm powerpc dec alpha hp ibm power solaris freebsd sgi indigo dec alpha sgi challenge sun sc table 
pipe latency microseconds time broken context switches plus system calls plus pipe overhead 
context switch component small processes table 
benchmark identical context switch benchmark ousterhout 
tcp rpc tcp latency 
tcp sockets may viewed interprocess communication mechanism similar pipes added feature tcp sockets machine boundaries 
tcp rpc tcp connections frequently low bandwidth latency sensitive applications 
default oracle distributed lock manager uses tcp sockets locks second available service accurately modeled tcp latency test 
system tcp rpc tcp linux sun ultra dec alpha freebsd solaris linux alpha hp sgi indigo ibm power ibm powerpc linux dec alpha sgi challenge sun sc table 
tcp latency microseconds sun rpc layered tcp udp 
rpc layer responsible managing connections port mapper managing different byte orders word sizes xdr implementing remote procedure call abstraction 
table shows benchmark rpc layer show cost rpc implementation 
tcp latency measured having server process waits connections client process connects server 
processes exchange word loop 
latency reported round trip time 
measurements table local loopback measurements intent show overhead software 
benchmark may frequently measure host host latency 
note rpc layer frequently adds hundreds microseconds additional latency 
problem external data representation xdr layer data passed back forth byte xdr done 
justification extra cost simply expensive implementation 
dce rpc worse 
udp rpc udp latency 
udp sockets alternative tcp sockets 
differ udp sockets unreliable messages leave retransmission issues application 
udp sockets system udp rpc udp linux sun ultra linux alpha dec alpha linux freebsd solaris ibm power ibm powerpc hp sgi indigo dec alpha sgi challenge sun sc table 
udp latency microseconds advantages 
preserve message boundaries tcp single udp socket may send messages number sockets tcp sends data place 
udp rpc udp messages commonly client server applications 
nfs probably widely rpc udp application world 
tcp latency udp latency measured having server process waits connections client process connects server 
processes exchange word loop 
latency reported round trip time 
measurements table local loopback measurements intent show overhead software 
note rpc library add hundreds microseconds extra latency 
tcp udp system network latency latency sun ultra freebsd hp fddi sgi indigo hp sgi linux mhz table 
remote latencies microseconds network latency 
hav results wire latency included table 
expected heavily network interfaces ethernet lowest latencies 
times shown include time wire microseconds mbit ethernet microseconds mbit ethernet fddi microseconds 
tcp connection latency 
tcp reliable byte stream oriented protocol 
part reliability connection established data transferred 
connection accomplished way handshake exchange packets client attempts connect server 
udp connection established tcp sends packets startup time 
application creates tcp connection send message startup time substantial fraction total connection transfer costs 
benchmark shows connection cost approximately half cost 
connection cost measured having server registered port mapper waiting connections 
client figures server registered repeatedly times connect system call server 
socket closed connect 
connects completed fastest result 
time measured include packets way tcp handshake cost greater times listed 
system tcp connection hp linux ibm power freebsd linux sgi indigo sgi challenge sun ultra solaris sun sc table 
tcp connect latency microseconds table shows need send quick message process packets get udp message cost send reply positive acknowledgments needed order apples apples comparison tcp 
transmission medium mbit ethernet time wire approximately microseconds way microseconds total 
thing short lived tcp connection cost microseconds wire time 
comparison meant tcp tcp useful protocol 
point suggest messages udp 
cases difference microseconds microseconds insignificant compared aspects application performance 
application latency sensitive transmission medium slow serial link message routers udp message may prove cheaper 

file system latency file system latency defined time required create delete zero length file 
define way file systems bsd fast file system directory operations done synchronously order maintain disk integrity 
file data typically cached sent disk date file creation deletion bottleneck seen application 
bottleneck substantial synchronous update disk matter tens milliseconds 
cases bottleneck perceived performance issue processor speed 
benchmark creates zero sized files deletes 
files created directory names short 
aa ab 
system fs create delete linux ext fs hp hfs linux ext fs linux alpha ext fs ufs sgi challenge xfs dec alpha advfs solaris ufs sun ultra ufs sun sc ufs freebsd ufs sgi indigo efs dec alpha ibm powerpc jfs ibm power jfs table 
file system latency microseconds create delete latencies shown table 
notice linux extremely orders magnitude faster slowest systems 
linux guarantee disk integrity directory operations done memory 
fast systems sgi xfs log guarantee file system integrity 
slower systems millisecond file latencies synchronous writes guarantee file system integrity 
modified ufs substantially running unsafe mode freebsd ufs slower file systems basically bsd fast file system 

disk latency included lmbench small benchmarking program useful measuring disk file patterned unix utility dd measures sequential random optionally generates patterns output checks input supports flushing data buffer cache systems support flexible user interface 
benchmarks trivially replaced perl script wrapped 
generated sequential random results part benchmarks heavily influenced performance disk drives test 
intentionally measure system overhead scsi command overhead may bottleneck large database configurations 
important applications transaction processing limited random disk io latency 
administrators increase number disk operations second buying disks processor overhead bottleneck 
benchmark measures processor overhead associated disk operation provide upper bound number disk operations processor support 
designed scsi disks assumes disks read ahead buffers read ahead faster processor request chunks data 
benchmark simulates large number disks reading byte transfers sequentially raw disk device raw disks unbuffered read ahead unix 
disk read ahead faster system request data benchmark doing small transfers data disk track buffer 
way look benchmark doing memory memory transfers scsi channel 
possible generate loads scsi operations second single scsi disk 
comparison disks database load typically run operations second 
system disk latency sgi challenge sgi indigo hp dec alpha sun sc sun ultra table 
scsi overhead microseconds resulting overhead number represents lower bound overhead disk real overhead numbers higher scsi systems scsi controllers disconnect request satisfied immediately 
benchmark processor simply sends request transfers data normal operation processor send request disconnect get may true processor fast requests faster rotating disk 
take second disk speed divide minimum transfer size ios second microseconds io 
don know processor os io controller combinations io microseconds 
interrupted reconnect transfer data 
technique discover drives system support system cpu limited produce overhead load fully configured system just disks 

known improvements extensions 
memory latency 
current benchmark measures clean read latency 
clean mean cache lines replaced highly unmodified associated write back cost 
extend benchmark measure latency write latency 
changes include making benchmark sequential prefetching measuring tlb cost 
mp benchmarks 
benchmarks lmbench designed measure multiprocessor features directly 
minimum measure cache cache latency cache cache bandwidth 
static vs dynamic processes 
process creation section cost starting processes shared libraries 
create statically linked processes systems quantify costs exactly 
stream benchmark 
probably incorporate part benchmark lmbench 
automatic sizing 
hav technology determine size external cache memory external cache effect 
detailed papers 
areas yield interesting papers 
memory latency section depth treatment context switching section turn interesting discussion caching technology 

lmbench useful portable micro benchmark suite designed measure important aspects system performance 
hav memory subsystem important processor speed 
processors get faster faster system design effort need move cache memory subsystems 

acknowledgments people provided invaluable help insight benchmarks 
usenix reviewers especially helpful 
especially ken sun kevin sun sun greg sgi john sgi neal sgi john univ delaware ron sarnoff chris ruemmler hp tom hp john 
people run benchmark contributed results possible assistance 
free software community tools project 
lmbench currently developed linux unix written linus torvalds band happy hackers 
lmbench documentation produced groff suite tools written james clark 
data processing results done perl written larry wall 
sun microsystems particular paul supported initial development project 
silicon graphics supported ongoing development turned far time imagined 
grateful companies financial support 

obtaining benchmarks benchmarks available reality sgi com employees lm engr lmbench mail server 
may request latest version lmbench sending email archives engr sgi com lmbench current subject 
chen chen patterson new approach performance evaluation benchmarks predicted performance tr computer systems pp 
november 
chen peter chen david patterson storage performance metrics benchmarks proceedings ieee pp 
august 
david denis foley william gist stephen alphaserver series high server platform development digital technical journal pp 
august 
hennessy john hennessy david patterson computer architecture quantitative approach nd edition morgan kaufman 
john memory bandwidth machine balance current high performance computers ieee technical committee computer architecture newsletter appear december 
mcvoy mcvoy kleiman performance unix file system pp 
proceedings usenix winter conference january 
ousterhout john ousterhout aren operating systems getting faster fast hardware pp 
proceedings usenix summer conference june 
park park becker iostone synthetic file system benchmark computer architecture news pp 
june 
saavedra saavedra smith measuring cache tlb performance effect benchmark runtimes ieee transactions computers pp 
october 
stallman free software foundation richard stallman general public license 
included 
toshiba toshiba dram components modules pp 
toshiba america electronic components 
wolman barry wolman thomas olson system independent io benchmark computer architecture news pp 
september 
biographical information larry mcvoy currently works silicon graphics networking systems division high performance networked file systems networking architecture 
computer interests include hardware architecture software implementation architecture performance issues free software issues 
previously sun architect sparc cluster product line redesigned wrote entire source management system implemented ufs clustering implemented posix support sunos 
concurrent sun stanford university operating systems 
sun worked eta systems supercomputer unix port 
may reached electronically lm sgi com phone 
carl staelin works hewlett packard laboratories external research program 
research interests include network information infrastructures high performance storage systems 
worked hp berkeley bsd lfs port highlight hierarchical storage file system mariposa distributed database project 
received phd computer science princeton university high performance file system design 
may reached electronically staelin hpl hp com 
