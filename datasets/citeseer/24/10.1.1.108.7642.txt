electronic workshops computing series edited professor van rijsbergen david turner ed functional programming glasgow proceedings glasgow workshop functional programming scotland july granularity divide conquer parallelism hans wolfgang loidl kevin hammond copyright belongs author published collaboration british computer society bcs granularity divide conquer parallelism hans wolfgang loidl department computing science university glasgow glasgow scotland mail dcs gla ac uk kevin hammond division computer science university st andrews st andrews scotland mail kh dcs st ac uk studies runtime behaviour various parallel divide conquer algorithms written non strict functional language common granularity control mechanisms simple cut priority thread creation priority scheduling mechanism 
mechanisms granularity information currently provided annotations improve performance parallel programs 
programs examine variants generic divide conquer program unbalanced conquer algorithm parallel determinant computation 
results indicate balanced computation trees simple low overhead mechanism performs complex mechanisms offer improvements unbalanced computation trees 
goal research improve efficiency parallel functional programs improving granularity threads produced execution 
granularity thread time required perform computations including overhead creating thread overheads imposed parallel execution communication costs 
non strict purely functional language haskell evaluate die mechanism computation 
model possible dynamically create new subsidiary threads evaluate sub expressions needed entirely avoid creating threads absorbing done parent thread 
optimal granularity threads compromise minimal parallel overhead maximal processor utilisation 
result minimum possible runtime parallel program parallel machine 
obtaining optimal thread granularity program hard problem affected details architecture communications latency number processors algorithmic details communications patterns generally unpredictable 
order threads scheduled dramatic impact granularity 
chosen concentrate divide conquer algorithms exhibit interesting parallel behaviour simple dynamic partitioning sub division potentially serious bottlenecks combination stage 
furthermore widely algorithms divide conquer matrix operations determinant computation multiplication quicksort alpha beta search study considerable practical relevance 
chosen focus granularity issues impact time performance 
previous studies shown strong correlation space usage time performance thread level 
supported research fellowship royal society edinburgh epsrc parade 
functional programming glasgow granularity divide conquer parallelism space usage minimised maximising granularity space leaks mean case 
mcburney sleep studied issue functional context 
granularity simulator objective obtain results apply parallel systems possible chosen simulator study granularity effects 
simulator delivers realistic results calibrated real architectures 
interested reader referred description construction simulator validation results studies various test programs 
prefer simulation theoretical approach gives controllable realistic result 
nature common analytical approaches ignore important costs communication fail deal complex significant interactions exact scheduling algorithm precise time communications occur 
parallelism control basic parallel construct sparks closure 
sparks similar lazy futures potentially turned parallel threads :10.1.1.17.101
compute result terminate having notify parent thread 
important note evaluate die mechanism dynamically increases granularity threads parent process may subsume computation child thread 
prevent system producing small threads workload low 
granularity control mechanisms aim increasing thread size 
spark created placed spark queue local processor 
idle processors look spark queue belonging processors 
case sparks chosen start queue 
basic difference lazy task creation model maintain explicit spark pool 
order create parallelism stolen certain position stack 
lazy futures basically indicate positions 
existence spark pool easier attach granularity information sparks 
creation spark cheap putting pointer closure queue willing pay overhead order improve granularity 
set annotations control parallelism spark created 
evaluation continues expression contains granularity information explained 
non spark created 
spark created processor owning 
evaluated sequence 
granularity control information provided annotations studied granularity control mechanisms cut mechanism compares value fixed cut value parameter runtime system decide spark created 
priority mechanism uses value priority deciding spark turn thread 
priority scheduling mechanism retains priorities threads produced uses deciding thread run 
functional programming glasgow granularity divide conquer parallelism mechanisms progressively allow precise control granularity impose increasing overheads 
comparing priority threshold spark creation time cheap 
eliminating low priority sparks regardless processor load may cause starvation 
priority mechanisms avoid problem starvation generally expensive maintain priority queues sparks threads perform simple threshold comparison needed cut mechanism 
objective assess overhead worthwhile 
divide conquer parallelism section discuss results obtained simple generic divide conquer algorithms 
consider algorithm generates unbalanced computation tree 
study larger program parallel determinant computation 
generic divide conquer algorithm primary components divide conquer algorithm computation sub divided split function performs actual computation solve results combined join 
generic divide conquer skeleton constructed components plus predicate determine computation sub divided divisible 
divisible split join solve divisible join map split solve create parallel divide conquer programs template simply replace sequential map parallel version parmap 
divisible split join solve divisible join parmap split solve parmap parmap xs gx gx fx fx fx gx parmap xs extra parameter parmap function generate granularity information element list 
avoid significantly affecting time performance obviously cheaper function worker function sections study applications generic divide conquer algorithm differ relative computational costs main steps 
variants create balanced computation tree total associated node decreases tree deeper 
common pattern divide conquer algorithms 
parallel determinant computation described section example real program exhibits behaviour 
functional programming glasgow expensive split granularity divide conquer parallelism function expensive split function involving factorial computation cheap join maximum solve identity functions 
variant small threads dominate computation threads runtime machine cycles threads created levels divide conquer tree hardly left done 
approximately number sparks created levels result evaluate die model causes tiny sparks subsumed parent thread 
expensive solve second expensive solve function sum factorials cheap split enum join maximum functions 
program coarsest granularity generic algorithms 
average runtime threads cycles compared cycles cycles 
small threads medium large threads threads runtime cycles significant variants 
total relatively large threads created computation done leaves tree threads runtime greater cycles compared 
main reason variant shows highest average parallelism generic algorithms 
expensive join xjoin cheap split enum solve identity functions expensive join expensive sum function 
xjoin variant highest percentage tiny threads threads runtime smaller cycles 
due fast creation tree structure caused cheap split phase 
results early creation fine grained leaves subsumed parents 
high degree parallelism creates runnable blocked threads maximum compared 
threads exist long time explains small number total threads compared 
granularity control speedup processors latency latency cut speedup processors latency latency cut speedup varying cut values measure defined precisely basic cost measure 
functional programming glasgow granularity divide conquer parallelism order reduce number small threads programs cut mechanism depth recursion represents size computation 
shows speedup variants varies cut value changed 
graph shows results different communication latencies 
obvious cut effective 
produces small threads 
root cause small improvement speedup fact sparks created breadth fashion 
means coarser grained threads near root balanced tree created early computation 
threads picked smaller threads leaves rarely executed anyway pruned automatically evaluate die strategy 
unbalanced divide conquer algorithm contrast programs previous section function produces unbalanced computation tree shown 
leaf case maximum list node case list parmap rem diverge diverge function split join solve phases cheap 
fifth node tree performs recursive call leaves levels tree 
unbalanced divide conquer tree generated shows speedup program changes cut values varied 
improvement greater balanced algorithms fifth spark large 
default spark selection strategy choose earlier inconsequential sparks execution threads 
compares granularity graph optimal cut cut optimal cut eliminates leaf computations 
threads similar lengths grouped 
height dominant bar second graph just tenth graph note logarithmic scale 
comparison shows small threads eliminated cut 
note granularity function approximates actual granularity small threads discarded 
long cut accurately discards small threads preserves large threads significantly affect performance comparison priority mechanisms implemented shown 
measures speedup communications latency processor machine 
main reason functional programming glasgow number threads speedup granularity divide conquer parallelism processors latency latency latency latency cut speedup varying cut values processors granularity pure exec 
time number threads processors granularity pure exec 
time granularity optimal cut poor improvement speedup spark thread queues tend quite short programs 
obviously priority scheme minimal effect items choose 
latency cycles average spark queue length average thread queue length 
higher latencies averages quickly approach 
behaviour reflected speedup graph priority schemes cease yield significant improvement soon latency exceeds approximately cycles 
contrast program average sparks spark queue latencies cycles proportional decrease sparks latency cycles 
average thread queue length greater latency cycles quickly approaches 
interesting observe speedup better low latencies zero latency 
explanation apparently counter intuitive result low latencies sparks stolen turned threads instantaneously 
value sparked closure soon needed parent parent thread functional programming glasgow speedup processors priorities pri 
pri 
scheduling latency granularity divide conquer parallelism speedup processors priorities pri 
pri 
scheduling latency speedups priority scheduling block search new normally simply evaluated closure 
overhead incurred threads created 
latency increases probability junk threads absorbed parents consequent increase granularity speedup 
interesting ask priority mechanisms effective overhead reduced 
completely eliminated overhead costs simulator measured execution time improvements latencies generic divide conquer programs 
improvements usually small compared cut mechanism 
unbalanced programs priority mechanisms outperform simple cut mechanism latencies 
parallel determinant computation parallel determinant computation central part parallel linear system solver described 
order compute determinant matrix split sub matrices 
done choosing row pivot row 
element row sub matrix constructed cancelling original matrix column row pivot element belongs 
determinant weighted sum determinants sub matrices 
weights pivot elements 
table shows average runtimes threads generated various spark sites different input matrices dense matrix size left column sparse matrix matrix entries size right column 
spark site number threads average runtime dense sparse dense sparse zero entries sign cancel row column construct sub matrix sub det total functional programming glasgow speedup determinant processors latency latency cut granularity divide conquer parallelism number threads determinant processors granularity pure exec 
time speedup granularity varying cut values pivot row generates leaf thread computation tree sparse matrix generates unbalanced computation tree small threads zero entries table 
contrast dense matrix generate balanced computation tree 
dense matrices fine grained threads compute sign pivot element 
smallest spark sites involved splitting computation cancelling elements pivot row column constructing sub matrices 
interesting spark sites compute determinants sub matrices sub det table 
generate significantly coarse grained threads threads execution time cycles 
shows effect cut offs parallel determinant computation sparse input matrix 
best results obtained groups small threads eliminated compute sign cancel pivot element 
remaining threads needed compute summands 
small threads needed avoid starvation computation main reason low speedups cut set high 
granularity graph shows optimal cut small threads successfully eliminated 
dense matrix behaviour parallel determinant program closer generic conquer algorithms 
cut mechanism speedup achieved far fewer leaf nodes large threads created early computation 
speedup hardly varies cut value changed ranging 
related authors suggested techniques aimed controlling granularity specifically divide conquer programs 
example goldberg discusses heuristics improving granularity lazy functional programs 
techniques apply models parallelism fewer detailed results 
section considers results related 
roe highly idealised simulator study effect evaluate die model divide conquer programs 
broadly confirmed evaluate die sufficient achieve high performance additional granularity control mechanisms needed 
roe best results obtained manual cut mechanism similar described 
attempts cut offs usually manual programming techniques 
typical performance functional programming glasgow granularity divide conquer parallelism improvements order 
example larus aiken report improvements order program shared memory implementation sml 
interesting system red implicitly bases cut recursion depth 
achieves improvement programs 
developed variant lazy task creation reduces overhead program running sequentially :10.1.1.17.101
closures execution stack searched top potential threads thread needed 
clearly successful strategy balanced divide andconquer programs shown give results arbitrary computation structures unbalanced divide conquer programs 
dynamic technique suggested feitelson barak involves spawning threads available thread perform cost spawning thread 
divide conquer algorithms normally prune leaf threads low cost sub trees 
general danger approach losing parallelism slowing behave quite unbalanced computation trees 
programmer control analysis improve granularity concentrates scheduling strategies fork join parallel setting 
techniques aim optimising joins preventing thread migration computation 
problem severe evaluate die mechanism lower overhead obtaining results child threads 
studied different mechanisms controlling granularity divide conquer programs cut priority priority scheduling mechanism 
divide conquer programs studied simple cut mechanism yields better results complex mechanisms higher overheads 
closer examination shows average thread length depends balanced computation tree tree seriously unbalanced granularity control mechanisms achieve larger improvements runtime 
tree balanced default ordering sparks possible achieve relatively small improvements 
plan examine combination granularity control mechanisms considered extend measurements broader class algorithms 
results apply albeit different way lazy task creation approach 
lazy task creation tries improve granularity provisionally inlining potentially parallel threads 
minimises overhead parallelism expense increased overheads thread creation 
important possess granularity information creating thread 
results show information discarded thread created 
suggests approach tagging inlined potentially parallel threads relative execution costs 
approach trade small increase overhead costs order reduce total costs thread creation 
results confirm reasonable ignore communication costs studying parallel behaviour 
realistic cost model essential understanding runtime behaviour parallel program granularity generated threads especially evaluation sophisticated parallel haskell 
ultimate objective research implementation practical static analysis determine thread granularity 
analysis produce information effectively control runtime behaviour parallel program 
demonstrated simple cut mechanism relative thread sizes gives results examples studied 
strengthens belief straightforward analysis sufficient provide information effectively exploited parallel runtime system 
jim mattson phil trinder worked hard gum system partly 
partain sterling keeping glasgow haskell compiler running spite repeated attempts break 
simon peyton jones provided important insights valuable criticisms reported 
functional programming glasgow granularity divide conquer parallelism feitelson barak 
run time algorithm managing granularity parallel functional programs 
journal functional programming 
lck held experience implementation concurrent graph reduction system ncube platform 
proc 
linz austria lncs pp 

goldberg 
multiprocessor execution functional programs 
phd thesis dept comp 
sci yale univ apr 
hammond 
loidl partridge 
visualising granularity parallel programs graphical winnowing system haskell 
proc 
pp 
denver apr 

scheduling grain size control 
phd thesis univ amsterdam 
larus aiken 
run time list sizes guide parallel thread creation 
proc 
acm fp pp 
orlando fl june 

loidl hammond partridge 
solving systems linear equations functionally case study parallelisation 
tech 
rep dept comp 
sci univ glasgow 
mcburney sleep 
transputer experiments architecture 
proc 
parle lncs pp 
eindhoven netherlands june 
mohr kranz halstead jr lazy task creation technique increasing granularity parallel programs :10.1.1.17.101
proc 
acm fp pp 
nice france june 
peyton jones clack 
high performance parallel graph reduction 
proc 
parle lncs pp 

roe 
parallel programming functional languages 
phd thesis dept comp 
sci univ glasgow feb 

task exposure parallel implementation functional programming languages 
phd thesis dept comp 
sci univ manchester 
functional programming glasgow 
