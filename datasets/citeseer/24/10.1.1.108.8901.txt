efficient algorithms minimizing cross validation error andrew moore carnegie mellon university school computer science pittsburgh pa cs cmu edu model selection important areas supervised learning 
dataset set models predicting dataset choose model expected best predict data 
situations online learning control robots factories data cheap human expertise costly 
cross validation highly effective method automatic model selection 
large scale cross validation search computationally expensive 
introduces new algorithms reduce computational burden searches 
show experimental design methods achieve technique similar bayesian version kaelbling interval estimation 
improvements including blocking quickly spot near identical models schemata search new method quickly finding families relevant features 
experiments robot data noisy synthetic datasets 
new algorithms speed computation sacrificing reliability cases reliable conventional techniques 
model selection important aspect areas supervised learning 
computer dataset set models predicting dataset determine best model 
models collection rival learning algorithms different sets high level learning parameters collection alternative architectures learning algorithm different architectures 
best model expected give accurate predictions data 
cross validation powerful method model selection 
general purpose prior mary lee howe street pittsburgh pa mary sp cs cmu edu sumptions rarely tied particular internal features algorithm 
prices paid generality cross validation 
data penalty 
firstly leave cross validation data reserved test set 
secondly naive intensive cross validation thousands models may produce deceptively lowest error model manner similar overfitting data 
deciding data required justify searching number models extremely difficult knowledge general purpose decision procedure exists 
absence assumptions danger searching insufficient data guarded leaving portion data check basic cross validation search 
computational penalty 
medium large datasets computing cross validation error model take long 
instance computing leave error nearest neighbor algorithm datapoint input dataset takes seconds sparc 
performing search thousands models may take hours impractical applications 
large datasets may sufficient data theoretically support search millions models computational cost cross validation search prohibitive 
interested applying cross validation control tasks robotics manufacturing 
involve decisions streams information sensors actuators data plentiful 
cases cross validation helpful selecting relevant sets visual features moore relevance weightings robot joint torques atkeson smoothing kernel sizes schaal atkeson moore determining neural network architectures moody 
describe new algorithms speeding cross validation decisions 
cross validation uses leave cross validation loocv applied memory instance learning 
models models dataset datapoints training points define loocv error th model point difference true output th point predicted output model trained members dataset th point 
call error output dataset th loocv error mean point predict model datapoints search problem find minimizes describe earlier maron moore statistical tests decide partially completed cross validation produced clear favorite rendering rest computation unnecessary 
appeal statistical techniques known experimental design methods box 
knowledge accelerating model selection feature selection function approximation problems 
number applications similar techniques 
examples ai literature kaelbling interval estimation method kaelbling uses confidence testing alleviate exploration versus exploitation dilemma reinforcement learning 
greiner similar technique decision making knowledgebased systems 
gratch uses related method choosing appropriate search rules large scheduling domains 
extends ideas new algorithms demonstrates cross validation search 
new algorithms applicable machine learning 
racing cross validation errors maron moore hoeffding bounds accelerate cross validation 
hoeffding bounds nonparametric method sequence independent observations random variable permit place confidence interval underlying mean value random variable 
probability true underlying mean lies distance sample mean ln known priori maximum range random variables number samples 
ad vantage hoeffding bounds lack assumption distribution random variable 
disadvantage resulting conservatism 
stronger assumptions confidence intervals able shrink faster 
start considering variant call race algorithm hoeffding bounds replaced bayesian approach 
estimates mean loocv error model built parallel kind race 
race predict model highly eventually lowest loocv error win model eliminated 
race algorithm randomizes order datapoints 
strong assumption leave errors model distributed normally 
mean priori ance unknown vari remember true mean loocv datapoints dataset goal computation find model minimizes model value 
evidence accumulates uncertainty decreases 
loocv errors th iteration surviving models evaluated datapoints 
sample mean variance model remembering leave error predict th datapoint 
model bayesian statistics values put probability distribution model relatively elementary process described example schmitt summarized appendix 
gives example 
distributions compute welch approximation welch models pair probable lower fairly probable lower race algorithm proceeds eliminating exists model certainly better 
statement model needs priors mean variance uninformative 
posterior distributions loocv errors models involved race 
lower loocv error better turn better race 
eliminated point 
clarification 
wish eliminate model confident worse model 
confidence required denoted 
example means willing risk chance making error test 
wish race models believe high confidence extremely similar 
achieved defining threshold small positive number eliminating model confident mean value distance model 
rules combined rule eliminate model exists model 
statistics gathered models elim prob model remains run dat select model lowest blocking see race algorithm greatly reduce computational effort compared fully computing loocv errors models 
behavior models produce nearly identical predictions 
near identical models run race point dataset 
problem reduced statistical technique called blocking box 
datapoint independent different models 
instance noisy dataset imagine datapoint particularly noisy larger output value neighbors 
models large leave error 
spread distributions due factors independent models 
spread eliminated blocking 
trick estimate values defined race maintain statistics bayesian statistics maintain probability distributions time dependencies models removed 
large variation errors fractionally smaller blocking statistics reveal quickly basic race 
model eliminated small detected testing prob prob simplest example blocking having beneficial effect case models race identical 
race algorithm race long time 
mean step race race samples confidence intervals measures mean error model close indifference parameter 
contrast brace blocking race algorithm maintain step difference leave errors datapoint 
models difference zero require small number statistics depending parameters models eliminated 
example identical models extreme common case near identical models large reduction time elimination 
results racing blocking table shows results comparing race algorithm brace algorithm standard algorithm computes models exhaustively 
results evals denotes number individual loocv computations needed search correct asks model selected search true minimum loocv relative evals column gives fraction cost search relative exhaustive search 
algorithms tested datasets robot inputs output points juggling robot inputs output points 
experiments 
value means sure eliminating model race 
value means prepared eliminate models slightly better factor race winner 
set models race 
models local weighted averaging known kernel regression models locally weighted regression cleveland 
group models different kernel smoothing parameters set experiments race improved exhaustive method brace turn improved race 
proportional improvement greater juggling example mainly juggling dataset larger 
larger dataset means exhaustive method perform proportionally extra loocv evaluations racing methods datapoints evaluated 
races table model left race 
race algorithm dataset models survived 
comments race brace briefly discuss issues regarding race brace 
concerns decision bayesian statistics looser non parametric hoeffding bounds greiner maron moore 
value defining confidence level prepared cut competitor race bayesian approach cuts far earlier hoeffding approach 
bayesian approach achieves superiority making stronger assumptions distribution errors expected robust hoeffding approach 
experiments performed date bayesian method converge wrong model easily hoeffding approach empirical observation may true datasets 
issue concerns choice assume normal distribution errors 
sensible different distribution exponential chi squared 
normal distribution take account possibility occasional highly datapoints outliers dataset errors times greater root mean square error 
worth considering computational penalty maintaining statistics making cut decisions race 
cost models th step models race models remain 
examples cost quite small comparison cost computing leave errors 
case memorybased methods locally weighted regression model cost computing leave error datapoint datapoints number input variables dataset 
step race requires computation leave error models models 
provided models small compared models requiring datapoints datapoints models computational penalty small 
furthermore need reduce computational penalty difficult invent racing schemes require computation models step means testing carefully chosen subset pairs models 
searching sets features particularly promising cross validation automatically choose relevant inputs wider set possible inputs 
obvious benefit accelerating learning rate algorithms suffer face irrelevant inputs 
benefits include helping select relevant visual features visually controlled robot tasks moore selecting sets time windows time series predictions 
problem known subset selection known problem statistics surveyed thoroughly miller rapidly gaining attention machine learning community 
proceedings papers addressing subset selection various machine learning algorithms caruana freitag john skalak 
inputs possible input sets performing exhaustive cross validation search soon impractical rises assuming adequate data support justify searching models 
hill climbing clearly sensible alternative 
section provide hill climbing versions brace aim speed computation reduce danger trapped local maxima 
sets inputs represented binary strings 
possible inputs denote ignore inputs inputs standard non racing hill climbing algorithm begins start string possible feature changes exhaustively finds minimizes loocv error 
uses best string say new base point generates feature successors determines best 
continues way single feature change improves 
special case starting zeroes termed forward selection sel starting ones termed backward elimination back el 
forward selection better features expected relevant backward elimination better features expected irrelevant 
unfortunately prior knowledge may available start search 
table experiments described text juggling method evals correct 
relative evals correct 
relative evals evals exh exh exh race brace racing counterparts algorithms straightforward base string generate feature changes race 
proceed winner race improve base 
experiments described versions tested brace back brace start zeroes ones respectively 
objection simple application brace 
imagine inputs relevant independently provide reduction loocv error 
start string successors shame run separate hill climbing iterations switch 
motivates algorithm gauss seidel version hill climbing predefined start string race current string current string bit flipped versus select winner race new current string race current string current string second bit flipped versus bits raced 
return bit proceed entire pass current string fails produce improvement 
versions algorithm gs brace back gs brace tested 
occasions help performance poor 
new algorithm gauss seidel job better solves problem 
schemata search schemata search new algorithm aims solve problem gauss seidel method addresses problem forward selection long time features relevant similarly backward elimination long time features irrelevant 
help second problem 
suppose family features simultaneously reduction loocv error 
family member ignored loocv error just bad family members ignored 
happen quite easily example features distributed function learned product 
forward selection family converge suboptimal 
backward elimination problem features irrelevant stuck early stages hill climbing removal irrelevant attribute improve loocv error 
schemata search searches space schemata strings 
denotes percent chance attribute ignored percent chance 
loocv error string expected loocv error binary string generated schemata string random rules example simple algorithm stars find racing better field race 
having finished race determine second field race entire string filled practice better 
racing field race fields parallel races races races races races inputs races occurring parallel races race produces winner confidence level 
step race random binary string generated loocv error randomly chosen datapoint computed binary string 
statistic added statistics strings sets races match binary string 
continues pairs significant believe probability member significant pair beats competitor 
iteration race begins new set winning field previous race switched 
race significantly better iteration races races races races may preferable hill climbing reasons feature detected quickly having wait entire iteration hill climbing take place 
features independently quickly selected having wait determine precisely best weakness brace back brace 
small mutually dependent families features missed hill may 
features gain benefit schemata string eventually win race strings generated features 
performed experiments shown test phenomena noisy binary optimization problems mutually dependent families size schemata search algorithm finds correct family 
experiments run algorithms randomly generated synthetic datasets 
task find set features minimized leave cross validation error nearest neighbor function approximator 
datasets inputs realvalued output noisy multivariate function random subset inputs 
inputs randomly generated uniformly range 
multivariate function syntax table 
number terms dataset randomly decided varied 
datasets trivial output complex output max corrupt product mean corrupt product corrupt corrupt corrupt max corrupt interesting note searchers managed identify precise set relevant inputs complex dataset datapoints experiments 
shows performance forward searchers schemata random datasets 
measures performance 
accuracy 
searchers suboptimal solutions 
shown columns percentage datasets searcher produced imperfect result 
result imperfect search produced feature set lower loocv error 
percentage datasets result fairly wrong loocv error greater minimum search 
give number meaning minimum loocv errors typically range depending dataset similar magnitude variation 
percentage datasets result wrong loocv error greater minimum search 
search time 
number individual evaluations loocv errors 
mean shown dominated hard datasets required tens thousands evaluations methods 
datasets required thousands cases hundreds evaluations 
reason shown scatterplots distributions ratio number samples needed compared number samples needed conventional forward selection method 
seen distribution highly skewed especially schemata searches 
schemata searches took quarter time conventional search 
schemata searches took twice long 
similar table comparing various backward methods schemata searches forward backward biases 
forward backward racing methods usually faster conventional methods little loss accuracy 
gauss seidel races similar performance 
schemata search roughly equal accuracy slowest method sel needing evaluations fastest schemata described shortly needed evaluations table syntax multivariate functions experiments expr th input expr expr product subexpressions mean expr expr mean subexpressions max expr expr maximum subexpressions set experiments achieved distinction wrong errors 
twelve schemata searches twice long conventional forward method due conventional method quickly stuck inferior solution considerably computation schemata search better result 
eventually equally solutions solution 
interestingly schemata searches frequently relevant features fast tenth total time search 
produced strings 
spend long time convincing justified putting 
initial ugly attempt address tried additional schemata algorithm give replace eagerly 
iterations races produced significant winners forced stars zero race statistics choose input schemata relevant 
fastest algorithm converging reliable schemata 
discussion extend research ways 
firstly investigate formal relationship probability cutoff tolerance factor probability race produce satisfactory answer 
secondly explore relationship searches techniques gradient descent genetic optimization 
gradient descent combination cross validation memory methods atkeson identify irrelevant features 
select models parameterized smoothly varying continuous parameters 
gradient descent particularly useful locally refining near optimal solutions 
cases suspect computationally expensive prone local maxima discrete searches additional investigation 
genetic optimization evident similarity gaussian random noise added value subexpression corrupt expr function non differentiable expr 
schemata search earlier moore saw inferior conventional hill climbing producing poorer solutions requiring computation time 
invention schemata search intended yield widely reported benefits genetic optimization statistical bookkeeping minimize binary string evaluations 
aha aha method permit nearest neighbor learners incrementally improve estimates relevance input features 
advantage technique operates locally features relevant parts input space may ignored 
interesting try combining schemata search kind local model selection 
introduced notion racing bayesian statistics accelerate model selection 
extended algorithms new ways 
new search methods may applications machine learning merits investigation 
appendix appendix concerns inferences means normal distributions random sample bayesian statistics 
analysis directly schmitt 
assume priori know distribution normal 
mean value variance value ignorance turned uninformative priors constant constant denotes probability density function observe sample define note priors legitimate probability density function 
approximation harmless see schmitt details 
method mean mean median evals relative relative evals evals sel race gs race schemata distribution relative evals relative sel ratios greater ratios greater ratios greater schemata comparing conventional forward selection algorithm racing counterparts schemata search forwards backwards bias 
method mean mean median evals relative relative evals evals back el back race back gs race schemata distribution relative evals relative back el ratios greater ratios greater schemata comparing conventional backward elimination algorithm racing counterparts schemata search results 
sample size sample mean sample variance marginal posterior distribution mean student distribution variance degrees freedom 
cumulative density function distribution compute probabil mean ity true mean lies interval 
implementation brace computation prob equation achieved manner 
sample size sample mean sample variance equations 
noted statistics updated incre defined terms 
mentally implementation race need compute probability means distributions differ certain amount equation 
achieved welch approximation fisher problem welch 
samples assumptions sample size mean sample sam ple variance corresponding values second sample signed difference population means approximately student mean variance degrees freedom justin boyan oded maron reviewers helpful comments 
supported research gift andrew moore 
aha aha kibler albert 
instance learning algorithms 
machine learning 
aha aha 
study instance algorithms supervised learning tasks mathematical empirical psychological evaluations 
phd 
thesis technical report university california irvine november 
atkeson atkeson 
memory approaches approximating continuous functions 
proceedings workshop nonlinear modeling forecasting santa fe new mexico 
box box hunter hunter 
statistics experimenters 
wiley 
caruana freitag caruana freitag 
greedy attribute selection 
cohen hirsh editors machine learning proceedings eleventh international conference 
morgan kaufmann 
cleveland cleveland 
locally weighted regression approach regression analysis local fitting 
journal american statistical association september 
gratch gratch chien dejong 
learning search control knowledge deep space network scheduling 
proceedings th international conference machine learning 
morgan kaufman june 
greiner greiner 
statistical approach solving ebl utility problem 
proceedings tenth international conference artificial intelligence aaai 
mit press 
john john kohavi pfleger 
irrelevant features subset selection problem 
cohen hirsh editors machine learning proceedings eleventh international conference 
morgan kaufmann 
kaelbling kaelbling 
learning embedded systems 
phd 
thesis technical report 
tr stanford university department computer science june 
maron moore maron moore 
hoeffding races accelerating model selection search classification function approximation 
advances neural information processing systems 
morgan kaufmann april 
miller miller 
subset selection regression 
chapman hall 
moody moody 
principled architecture selection neural networks application corporate bond rating prediction 
moody hanson lippman editors advances neural information processing systems 
morgan kaufmann april 
moore moore hill johnson 
empirical investigation brute force choose features smoothers function approximators 
hanson judd petsche editors computational learning theory natural learning systems volume 
mit press 
moore moore 
fast robust adaptive control learning forward models 
moody hanson lippman editors advances neural systems 
morgan kaufmann april 
schaal atkeson schaal atkeson 
assessing quality local linear models 
advances neural information processing systems 
morgan kaufmann april 
schmitt schmitt 
measuring uncertainty elementary bayesian statistics 
addison wesley 
skalak skalak 
prototype feature selection sampling random mutation hill climbing algorithms 
cohen hirsh editors machine learning proceedings eleventh international conference 
morgan kaufmann 
welch welch 
significance difference means population variances unequal 
biometrika 
