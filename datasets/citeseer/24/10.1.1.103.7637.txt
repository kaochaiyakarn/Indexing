maximum entropy approach natural language processing adam berger stephen della pietra vincent della pietra ibm watson research center box yorktown heights ny concept maximum entropy traced back multiple threads times 
computers powerful permit application concept real world problems statistical estimation pattern recognition 
describe method statistical modeling maximum entropy 
maximum likelihood approach automatically constructing maximum entropy models describe implement approach ciently examples problems natural language processing 

statistical modeling addresses problem constructing stochastic model predict behavior random process 
constructing model typically disposal sample output process 
sample representing incomplete state knowledge process modeling problem knowledge representation process 
representation predictions behavior process 
baseball managers rank better paid statistical modelers employ averages compiled history bats gauge likelihood player succeed appearance plate 
informed manipulate accordingly 
wall street rank best paid statistical modelers build models past stock price movements predict tomorrow uctuations alter portfolios capitalize predicted 
pay scale reside natural language researchers design language acoustic models speech recognition systems related applications 
past decades witnessed signi cant progress increasing predictive capacity statistical models natural language 
language modeling instance bahl decision tree models della pietra automatically inferred link grammars model long range correlations language 
parsing black described extract grammatical rules columbia university computer science department renaissance technologies stony brook ny research supported part arpa onr association computational linguistics computational linguistics volume number annotated text automatically incorporate rules statistical models grammar 
speech recognition lucassen mercer introduced technique automatically discovering relevant features translation word spelling word pronunciation 
orts varied speci cs confront essential tasks statistical modeling 
rst task determine set statistics capture behavior random process 
set statistics second task corral facts accurate model process model capable predicting output process 
rst task feature selection second model selection 
pages uni ed approach tasks maximum entropy philosophy 
discussion proceed follows 
section maximum entropy philosophy motivating example 
section describe mathematical structure maximum entropy models give cient algorithm estimating parameters models 
section discuss feature selection automatic method discovering facts process sample output process 
series re nements method practical implement 
section describe application maximum entropy ideas tasks stochastic language processing bilingual sense disambiguation word reordering sentence segmentation 

maximum entropy overview introduce concept maximum entropy simple example 
suppose wish model expert translator decisions concerning proper french rendering english word 
model expert decisions assigns french word phrase estimate probability expert choose translation 
guide developing collect large sample instances expert decisions 
goal extract set facts decision making process sample rst task modeling aid constructing model process second task 
obvious clue glean sample list allowed translations 
example discover expert translator chooses french phrases en au cours de 
information hand impose rst constraint dans en au cours de pendant equation represents rst statistic process proceed search suitable model obeys equation 
course nite number models identity holds 
model satis es equation dans words model predicts dans 
model obeys constraint predicts pendant probability probability 
models knowing expert chose french phrases justify probability distributions 
making bold assumptions empirical justi cation 
put way models assume know expert decision making process 
know expert chose exclusively french phrases intuitively appealing model berger della pietra della pietra maximum entropy approach nlp dans en au cours de pendant model allocates total probability evenly possible phrases uniform model subject knowledge 
uniform model equal probability possible french phrase 
hope glean clues expert decisions sample 
suppose notice expert chose dans en time 
apply knowledge update model translation process requiring satisfy constraints dans en dans en au cours de pendant probability distributions consistent constraints 
absence knowledge reasonable choice uniform distribution allocates probability possible subject constraints dans en au cours de pendant say inspect data time notice interesting fact half cases expert chose dans incorporate information model third constraint dans en dans en au cours de pendant dans look uniform satisfying constraints choice obvious 
added complexity wehave encountered di culties 
exactly meant uniform measure uniformity model 
second having determined suitable answer questions go nding uniform model subject set constraints described 
maximum entropy method answers questions demonstrate pages 
intuitively principle simple model known computational linguistics volume number assume unknown 
words collection facts choose model consistent facts uniform possible 
precisely approach took selecting model step example 
maximum entropy concept long history 
adopting complex hypothesis possible embodied occam razor est sine appears earlier bible writings jaynes 
laplace considered father maximum entropy having enunciated underlying theme years ago principle insu cient reason information distinguish probability oftwo events best strategy consider equally 
jaynes pioneer maximum entropy put jaynes fact certain probability distribution maximizes entropy subject certain constraints representing incomplete information fundamental property justi es distribution inference agrees known carefully avoids assuming known 
transcription mathematics ancient principle wisdom 

maximum entropy modeling consider random process nite set translation example just considered process generates translation word output word set en au cours de 
generating process may uenced contextual information member nite set example information include words english sentence surrounding 
task construct stochastic model accurately represents behavior random process 
model method estimating conditional probability process output denote yjx probability model assigns context slight abuse notation yjx denote entire conditional probability distribution provided model interpretation placeholders speci instantiations 
proper interpretation clear context 
denote set conditional probability distributions 
model yjx de nition just element ofp 
training data study process observe random process time collecting large number samples xn yn 
example considering sample consist phrase containing words surrounding translation process produced 
imagine training samples generated expert number random phrases containing asked choose translation 
discuss real world applications section show samples automatically extracted bilingual corpus 
summarize training sample terms empirical probability distri berger della pietra della pietra maximum entropy approach nlp bution de ned number times occurs sample typically particular pair occur sample occur times 
statistics features constraints goal construct statistical model process generated training sample 
building blocks model set statistics training sample 
current example employed statistics frequency translated dans en frequency translated dans au cours de 
particular statistics independent context consider statistics depend conditioning information instance notice training sample april word translation en frequency 
express event translates en april word introduce indicator function en april follows expected value respect empirical distribution exactly statistic interested 
denote expected value express statistic sample expected value appropriate indicator function feature function feature short 
probability distributions abuse notation denote value particular pair aswell entire function 
discover statistic feel useful importance requiring model accord 
constraining expected value model assigns corresponding feature function expected value respect model yjx yjx empirical distribution training sample 
constrain expected value expected value training sample 
require combining yields explicit equation yjx computational linguistics volume number call requirement constraint equation simply constraint 
restricting attention models yjx holds eliminating consideration models agree training sample output process exhibit feature sum far means representing statistical phenomena inherent sample data means requiring model process exhibit phenomena 
nal note features constraints bears repeating words feature constraint interchangeably discussions maximum entropy distinguish urge reader likewise feature binary valued function constraint equation expected value feature function model expected value training data 
maximum entropy principle suppose feature functions fi determine statistics feel important modeling process 
model accord statistics 
lie subset de ned jp fi fi provides geometric interpretation setup 
space unconditional probability distributions points called simplex 
ifwe impose constraints depicted probability 
imposing linear constraint restricts lie region de ned 
second linear constraint determine exactly constraints satis able case intersection non empty 
alternatively second linear constraint inconsistent rst instance rst require probability rst point second probability third point shown 
setting linear constraints extracted training sample construction inconsistent 
furthermore linear constraints applications come close determining set cn allowable models nite 
models maximum entropy philosophy dictates select distribution uniform 
face question left open section uniform mean 
mathematical measure uniformity conditional distribution yjx provided conditional entropy yjx log yjx entropy bounded zero entropy model uncertainty entropy uniform distribution possible common notation conditional entropy ish random variables joint distribution yjx 
emphasize dependence entropy onthe probability distribution wehave adopted alternate notation 
berger della pietra della pietra maximum entropy approach nlp di erent scenarios constrained optimization 
represents space probability distributions 
constraints applied allowable 
constraint narrows set allowable models lie line de ned linear constraint 
consistent constraints de ne single model 
constraints inconsistent satisfy 
computational linguistics volume number jyj values de nition hand ready principle maximum entropy 
select model set allowed probability distributions choose model 
maximum entropy 
shown 
de ned unique model 
maximum entropy constrained set parametric form maximum entropy principle presents problem constrained optimization nd 
maximizes 
simple cases nd solution problem analytically 
true example section imposed rst constraints unfortunately solution general maximum entropy written explicitly need indirect approach 
reader invited try calculate solution example third constraint imposed 
address general problem apply method lagrange multipliers theory constrained optimization 
relevant steps outlined reader referred della pietra thorough discussion constrained optimization applied maximum entropy 
refer original constrained optimization problem primal problem 
nd 
feature fi introduce parameter lagrange multiplier 
de ne lagrangian fi fi holding xed compute unconstrained maximum lagrangian denote achieves maximum thevalue maximum argmax call function 
functions calculated explicitly simple calculus 
nd yjx exp ifi logz fi berger della pietra della pietra maximum entropy approach nlp normalizing constant determined requirement yjx exp ifi pose unconstrained dual optimization problem find argmax rst glance clear achieve 
fundamental principle theory lagrange multipliers called generically kuhn tucker theorem asserts suitable assumptions primal dual problems fact closely related 
case situation 
detailed account relationship scope easy state nal result suppose solution dual problem 
solution primal problem 
words maximum entropy model subject constraints parametric form parameter values determined maximizing dual function 
important practical consequence result algorithm nding maximum nd maximum 
forp 
relation maximum likelihood log likelihood empirical distribution predicted de ned log yjx log yjx easy check dual function previous section fact just log likelihood exponential model interpretation result previous section rephrased model 
maximum entropy model parametric family yjx maximizes likelihood training sample dual function maximum nite 
case maximum entropy model form limit models form indicated result proof omit suppose sequence converges maximum 
converges 
henceforth abbreviate empirical distribution clear context 
computational linguistics volume number primal dual problem description maximum entropy argmax maximum likelihood type constrained optimization unconstrained optimization search domain real valued vectors solution 
kuhn tucker theorem 
table duality maximum entropy maximum likelihood example general phenomenon duality constrained optimization 
result provides added justi cation maximum entropy principle notion selecting model 
basis maximum entropy isn compelling happens 
model models parametric form best account training sample 
table summarizes primal dual framework established 
computing parameters simple problems maximize analytically 
resort numerical methods 
perspective optimization function behaved smooth convex consequently avariety methods calculate 
simple method coordinate wise ascent computed iteratively maximizing coordinate time 
applied maximum entropy problem technique yields popular brown algorithm brown 
general purpose methods maximize include gradient ascent conjugate gradient 
optimization method speci cally tailored maximum entropy problem iterative scaling algorithm darroch ratcli darroch 
version algorithm speci cally designed problem hand proof monotonicity convergence algorithm della pietra 
algorithm applicable feature functions fi non negative fi andy course true binary valued feature functions considering 
algorithm generalizes darroch ratcli procedure requires addition non negativity feature functions satisfy fi algorithm improved iterative scaling input feature functions fn empirical distribution output optimal parameter values optimal model 
start ng 
ng solution yjx fi exp fi berger della pietra della pietra maximum entropy approach nlp nx update value 
go step converged fi key step algorithm step computation increments solve 
constant say explicitly fi log fi computed numerically 
simple ective way doing newton method 
method computes solution equation iteratively recurrence appropriate choice suitable attention paid domain 
feature selection earlier divided statistical modeling problem steps nding appropriate facts data second incorporate facts model 
point wehave proceeded assuming rst task performed 
simple example section explicitly state selected particular constraints 
fact dans chosen expert translator time important countless facts contained data 
fact principle maximum entropy directly concern issue feature selection merely provides recipe combining constraints model 
feature selection problem critical universe possible constraints typically thousands millions 
section introduce method automatically selecting features included maximum entropy model er series re nements ease computational burden 
motivation specifying large collection candidate features 
require priori features relevant useful 
pool large practically possible 
small subset collection features eventually employed nal model 
training sample nite size determine true expected value candidate feature computing fraction events sample 
real life applications provided small sample events trusted represent process fully accurately 
speci cally expect feature estimate derive sample close value limit grows large 
employing larger just di erent sample data process result di erent estimates candidate features 
computational linguistics volume number short include model subset full set candidate features call set active features 
choice capture information random process possible include features expected values reliably estimated 
nds adopt incremental approach feature selection similar strategy growing decision trees bahl 
idea build successively adding features 
choice feature add step determined training data 
denote set models determined feature set 
adding feature shorthand requiring set allowable models satisfy equality 
members satisfy equality ones denote 
time candidate feature adjoined linear constraint imposed space features result shrinks model 
greatest entropy re ects increasing knowledge hopefully accurate representation process 
narrowing space permissible models represented gure series intersecting lines hyperplanes general probability simplex 
intuitively represent series nested subsets gure 
nested sequence subsets corresponding increasingly large sets features basic feature selection basic incremental growth procedure may outlined follows 
stage process characterized set active features determine space models fp jp sg optimal model space denoted model greatest entropy adding feature obtain new set active features set features determines set models fp jp fg berger della pietra della pietra maximum entropy approach nlp optimal model space models argmax adding feature allows model better account training sample results gain log likelihood training data stage model construction process goal select candidate feature maximizes gain select candidate feature adjoined set active features produces greatest increase likelihood training sample 
strategy implemented algorithm basic feature selection input collection candidate features empirical distribution output set active features model incorporating features 
start uniform 
candidate feature compute model algorithm compute gain log likelihood adding feature 
check termination condition 
select feature maximal gain 
adjoin 
compute algorithm 
go step issue left unaddressed algorithm termination condition 
obviously condition applies exactly useful features selected 
reasonable stopping criterion subject proposed feature crossvalidation held sample data 
feature lead increase likelihood held sample data feature discarded 
say stopping criterion section 
approximate gains algorithm practical method incremental feature selection 
candidate feature considered step compute maximum entropy model task computationally costly cient iterative scaling algorithm introduced earlier 
introduce modi cation algorithm making greedy feasible 
replace computation gain ofa feature approximation denote 
recall model set parameters feature model contains set parameters plus single new parameter corresponding way think models number parameters computational linguistics volume number structure hope optimal values change feature adjoined case imposing additional constraint require optimizing single parameter maximize likelihood 
unfortunately new constraint imposed optimal values parameters change 
feature ranking computation tractable approximation addition feature ects leaving values associated features unchanged 
determining gain model pretend best model containing features form yjx real valued yjx parameter distinguishes models form models interested maximizes approximate gain gs denote gain model optimal model log ps despite unwieldy notation idea simple 
computing approximate gain likelihood adding feature reduced simple onedimensional optimization problem single parameter solved popular line search technique newton method 
yields great savings computational complexity computing exact gain dimensional optimization problem requiring sophisticated methods conjugate gradient 
savings comes price particular feature probably underestimating gain reasonable chance select feature approximate gain highest pass feature maximal gain 
graphical representation approximation provided gure 
log likelihood represented arbitrary convex function parameters corresponds old parameter new parameter 
holding xed adjusting maximize log likelihood involves search darkened line search entire space 
actual algorithms appropriate mathematical framework appendix 

case studies pages discuss applications maximum entropy modeling candide fully automatic french english machine translation system berger della pietra della pietra maximum entropy approach nlp likelihood function parameters 
start constraint model optimal parameter value consider increase adjoining second constraint parameter exact answer requires search 
simplify task holding constant performing line search possible values new parameter darkened line represents search space restrict attention 
show reduced problem line search computational linguistics volume number development ibm 
past years candide test bed exploring cacy various techniques modeling problems arising machine translation 
section review general theory statistical translation describing detail models employed candide 
section describe wehave applied maximum entropy modeling predict french translation english 
section describe maximum entropy models predict di erences french word order english word order 
section describe maximum entropy model predicts divide french sentence short segments translated sequentially 
review statistical translation french sentence candide task nd english sentence bayes theorem equivalent nding je candide estimates probability string english english sentence parametric model english language commonly referred language model 
system estimates je probability thata french sentence translation parametric model process english french translation known translation model 
models plus search strategy nding maximizes comprise engine translation system 
brie describe translation model probability je amore thorough account brown 
imagine english sentence generates french sentence steps 
word independently generates zero french words 
words ordered give sentence word ei jth word yj 
employ yj intuitive fj avoid confusion feature function notation 
words sentence jej number words sentence jf generative process yields french sentence association words words call association alignment denote alignment parametrized sequence jf numbers aj ai jej 
word position aj word position english word generates yj 
depicts typical alignment 
probability je translation expressed sum possible alignments probability je jf sum equation computationally unwieldy involves sum jej possible alignments words sentences 
reason berger della pietra della pietra maximum entropy approach nlp dog ate homework le chien mange mes alignment english sentence pair 
subscripts give position word sentence 
anda 
simplifying assumption exists extremely probable alignment called viterbi alignment je alignment viterbi probability jej ei jf ei denotes words aligned ei 
expression probability english word generates french words probability english word generates french word probability particular order french words 
call model described equations basic translation model 
take probabilities andp fundamental parameters model parametrize distortion probability terms simpler distributions 
brown describe method estimating parameters maximize likelihood large bilingual corpus english french sentences 
method estimation maximization em algorithm known iterative technique maximum likelihood training model involving hidden statistics 
basic translation model hidden information alignment employed em algorithm estimate parameters basic translation model maximize likelihood bilingual corpus obtained proceedings canadian parliament 
historical reasons proceedings called hansards 
hansard corpus contains english french sentence pairs total little words language 
table shows parameter estimates translation probabilities 
basic translation model worked bilingual corpus additional knowledge languages relation uncovered highly plausible translations 
basic translation model major shortcoming take english context account 
model account surrounding english words predicting appropriate french rendering english word 
pointed section successful translation works 
best french translation function surrounding english words month time computational linguistics volume number translation probability dans de en pour au cours de sur par pendant table frequent french translations estimated em training 
represents catch classi er french phrase listed probability exceeding 
berger della pietra della pietra maximum entropy approach nlp je eme que les chances sont 
say odds superior 
il que bank boston ait de 
appears bank boston completed 
typical errors encountered em model brown french english translation system subsequent words pendant scal year follows dans 
basic model blind context assigning probability dans pendant 
yield errors candide called translate french sentence 
examples errors shown 
rst example system chosen english sentence french word rendered superior greater higher preferable translation 
knowledge context expert translator quite select superior english word generates 
expert fact words inclined select greater higher 
similarly second example incorrect rendering il translation model fact word appears 
context models hope rectifying errors consider problem context sensitive modeling word translation 
envision practice separate maximum entropy model pe yjx english word yjx represents probability expert translator choose french rendering surrounding english context just slightly recast version longstanding problem computational linguistics sense disambiguation determination word sense context 
training sample english french sentence pairs randomly extracted hansard corpus contains english word 
sentence pair basic translation model compute viterbi alignment alignment construct training event 
event consists context containing words surrounding equal french word viterbi alignment aligned 
actual examples events depicted table 
de ne set candidate features 
application employ features indicator functions simply described sets 
speci cally consider functions particular french word context contains english word zero 
employ notation represent features computational linguistics volume number translation dans committee stated letter required respect au cours de scal year dans government postal reported canada de notice ordinary way table actual training events maximum entropy translation model extracted transcribed proceedings canadian parliament 
berger della pietra della pietra maximum entropy approach nlp number template actual features table feature templates word translation modeling 
size english vocabulary size french vocabulary 
en april pendant weeks april follows en translation words pendant translation 
set features consideration vast may expressed abbreviated form table 
table symbol placeholder possible french word symbol placeholder possible english word 
feature mentioned derived template en april feature derived template pendant weeks 
total english words total french words template features features templates 
template features give rise constraints enforce equality probability translation model probability translation empirical distribution 
examples constraints dans dans de de en en maximum entropy model uses template features predicts french computational linguistics volume number translation probability determined empirical data 
exactly distribution employed basic translation model 
template features independent ofx maximum entropy model employs constraints derived template features takes account information assigning probability toy 
include constraints derived template features take rst step context dependent model 
simply constraining expected probability word equal empirical probability constraints require expected joint probability ofthe english word immediately french rendering equal empirical probability 
example template constraint pendant pendant maximum entropy model incorporates constraint predict translations manner consistent word 
particular empirical sample presence led greater probability pendant re ected maximum entropy model incorporating constraint 
taken rst step context sensitive translation modeling 
templates consider di erent way various parts context 
instance template constraints expert translator biased appearance word words word translating 
house appears words phrases house red house dans translation 
hand year appears window year year au cours de 
constraint templates allow model condition assignment probabilities window words word question 
constructed maximum entropy yjx iterative model growing method described section 
automatic feature selection algorithm rst selected template constraint translations seen sample constraining model expected probability ofeach translations empirical probabilities 
constraints selected algorithm shown table 
rst column gives identity feature expected value constrained second column gives approximate increase model log likelihood data result imposing constraint third column gives log likelihood adjoining feature recomputing model 
consider fth row table 
constraint requires model expected probability words right word speech equal empirical sample 
imposing constraint onthe model iterative model growing process log likelihood current model empirical sample bits 
feature selection algorithm described section calculated constraint imposed model log likelihood rise approximately bits value higher constraint considered constraint selected 
applying iterative scaling recompute parameters new model likelihood empirical sample rose bits increase bits 
table lists rst selected features model translating english word run 
hansard avor speci domain parliamentary discourse related canadian airs easy detect features table 
hard incorporate maximum entropy word translation models translation model je french sentence english sentence 
merely berger della pietra della pietra maximum entropy approach nlp feature canada house en pour order dans speech dans area de increase verb marker dans case au cours de year table maximum entropy model predict french translation 
features shown rst non template features selected 
verb marker denotes morphological marker inserted indicate presence verb word 
computational linguistics volume number feature time nous counter table maximum entropy model predict french translation run top ranked features template berger della pietra della pietra maximum entropy approach nlp je eme que les chances sont 
say odds greater 
il que bank boston ait son de 
appears bank boston completed improved french english translations resulting maximum entropy system replace simple context independent basic translation model general context dependent models pe yjx jej ei jf pea denotes context english word illustrates improved translation model candide system led improved translations sample sentences earlier 
segmentation ideal machine translation system input sentences unrestricted length typical stochastic system cut french sentence polite lengths 
processing time exponential length input passage case candide system splitting french sentence reasonably sized segments result exponential slowdown translation 
common task machine translation nd safe positions split input sentences order speed translation process 
safe vague term instance reasonably de ne safe segmentation results coherent blocks words 
purposes safe segmentation dependent viterbi alignment input french sentence english translation de ne rift position ak aj ak aj 
words words left french word yj generated words left english word right generated words right alignment gure example positions french sentence 
visual method determining rift occurs french word try trace line letter yj letter line drawn intersecting alignment lines position rift 
de nition rede ne safe segmentation segment boundaries located 
illustrates unsafe segmentation segment boundary denoted symbol lies mange rift 
hand illustrates safe segmentation 
reader notice safe segmentation necessarily result semantically coherent segments mes certainly part logical unit computational linguistics volume number dog ate homework le chien mange mes example unsafe segmentation 
word translated sentence aligned words erent segments input sentence 
separated safe segmentation 
safe segmentation applied french sentence assumption searching appropriate english translation word translated english sentence account french words located multiple segments 
disallowing alignments dramatically reduces scale computation involved generating translation particularly large sentences 
consider segment sequentially generating translation working left right french sentence 
example safe segmentation dog ate homework le chien mange mes describe maximum entropy assigns location french sentence score measure safety cutting sentence location 
word translation problem training sample english french sentence pairs randomly extracted hansard corpus 
sentence pair basic translation model compute viterbi alignment stochastic part speech tagger described merialdo label word part speech 
position construct training event 
value rift rift belongs position rift 
context information reminiscent employed word translation application described earlier 
includes word window french words left yj right 
includes part speech tags words classes words derived mutual information clustering scheme described brown 
complete pair illustrated 
creating principle modeling decisions expert french segmenter 
sample training sample rift 
ea ea tag ea tag ea sentence segmentation class ea class ea berger della pietra della pietra maximum entropy approach nlp log likelihood training held number features change log likelihood segmenting model growing 
overtraining begins occur features measure worth model log likelihood 
iterative model growing procedure algorithm selects constraints basis increase objective function 
algorithm proceeds constraints imposed model bringing stricter compliance empirical data 
useful point insofar empirical data embodies expert knowledge french segmenter incorporate knowledge model 
data contains expert knowledge algorithm terminate extracted knowledge 
model yjx quirks empirical data 
standard approach statistical modeling avoid problem tting training data employ cross validation techniques 
separate training data training portion pr heldout portion ph 
pr model growing process select features increase likelihood pr 
algorithm progresses pr increases monotonically 
long new constraint imposed allows better account random process generated pr ph quantity ph increases 
point tting begins new constraints longer help model random process require model noise sample pr 
point pr continues rise ph longer 
point algorithm terminate 
illustrates change log likelihood training data pr held data ph 
algorithm terminated log likelihood held data stopped increasing nal model contain slightly features 
employed segmenting model component english machine translation system manner 
model assigns position french sentence score rift measure appropriate split location 
dynamic programming algorithm selects appropriateness score position requirement segment words optimal reasonable splitting sentence 
computational linguistics volume number poser une question au des transports 
date le il en 
les utilises pour evaluation de ces 
nous que si nous contr la dans du canada en un nous notre en de de dollars 
maximum entropy segmenter behavior sentences selected random hansard data shows system segmentation sentences selected random hansard data 
remind reader keep mind evaluating segmenter task produce logically coherent blocks words divide sentence blocks translated sequentially left right 
word reordering translating french sentence english involves selecting appropriate english renderings words french sentence selecting ordering english words 
order di erent french word order 
way candide captures word order di erences languages allow alignments crossing lines 
addition candide performs pre processing stage reordering step shu es words input french sentence order closely resembling english word order 
component reordering step deals french phrases noun de noun form 
noun de noun phrases best english translation nearly word word con inter example rendered con ict interest 
phrases best translation obtained interchanging nouns dropping de 
phrase inter example best rendered interest rate 
table gives examples noun de noun phrases appropriate english translations 
section describe model french noun de noun phrase estimates probability best english translation involves interchange nouns 
sample english french sentence pairs randomly extracted hansard corpus thatf contains de phrase 
sentence pair basic translation model compute viterbi alignment words construct training event follows 
context pair french nouns 
berger della pietra della pietra maximum entropy approach nlp table word word phrases sum money pays origin country origin question de privilege question privilege con inter con ict interest interchanged phrases bureau de post ce inter interest rate assurance insurance de prison prison guard noun de noun phrases english equivalents computational linguistics volume number template number actual features table template features noun de noun model interchange english translation word word translation french phrase interchange order nouns english french phrases interchanged 
de ne candidate features template features shown table 
table symbol placeholder interchange interchange symbols placeholders possible french words 
total french words possible features templates features template 
template features consider left noun 
expect features relevant decision interchange nouns uenced identity left noun 
example including template feature interchange systeme gives model sensitivity fact nouns french noun de noun phrases systeme de surveillance systeme de quota interchanged english translation 
similarly including template feature interchange gives model sensitivity fact french noun de noun phrases de mai month may translated word word 
template features useful dealing translating noun de noun phrases interchange decision uenced nouns 
example noun de noun phrases inter translated word word con inter con ict interest interchanged inter interest rate 
feature selection algorithm section construct maximum entropy model candidate features derived templates 
model grown training events randomly selected hansard corpus 
nal model contained constraints 
test model constructed noun de noun word reordering module interchanges order nouns interchange order 
table compares performance suite test data baseline noun de noun reordering module swaps word order 
table shows randomly chosen noun de noun phrases extracted test suite probability model assigned inversion 
right phrases model strongly berger della pietra della pietra maximum entropy approach nlp table test data simple model maximum entropy accuracy model accuracy interchanged interchanged total noun de noun model performance simple approach vs maximum entropy de commerce de mai acquisition de niveau inflation construction inflation cout de transport systeme de quota bureau de plan de base smaller interchange larger predictions noun de noun interchange model phrases selected corpus unseen training process computational linguistics volume number predicted inversion 
left phrases model strongly prefers interchange abus de privilege 
intriguing phrases lie middle ation translate ation rate rate ation 

began building blocks maximum entropy modeling real valued features constraints built features 
discussed maximum entropy principle 
principle instructs choose models consistent constraints model greatest entropy 
observed model member exponential family adjustable parameter constraint 
optimal values parameters obtained maximizing likelihood training data 
di erent philosophical approaches maximum entropy maximum likelihood yield result model greatest entropy consistent constraints exponential model best predicts sample data 
discussed algorithms constructing maximum entropy models concentrating attention main problems facing modelers selecting set features include model computing parameters model contains features 
general feature selection slow practice techniques making algorithm feasible 
second part described applications algorithms concerning modeling tasks arising candide automatic machine translation system development ibm 
applications demonstrate cacy maximum entropy techniques performing context sensitive modeling 
acknowledgments authors wish harry john la erty suggestions comments preliminary draft jerome bellegarda providing expert french knowledge 
bahl brown de souza mercer 
tree statistical language model natural language speech recognition 
ieee transactions acoustics speech signal processing vol 

berger brown della pietra della pietra la erty ures 
candide system machine translation 
proceedings arpa conference technology new jersey 
black jelinek la erty magerman mercer roukos 
history grammars richer models probabilistic parsing 
proceedings darpa speech natural language workshop arden house new york 
brown 
note approximations discrete probability distributions 
information control vol 

brown della pietra della pietra mercer 
mathematics statistical machine translation parameter estimation 
computational linguistics vol 

brown cocke della pietra della pietra jelinek la erty mercer 
statistical approach translation 
computational linguistics vol 

brown della pietra de souza mercer 
class gram models natural language 
proceedings ibm natural language itl 
berger della pietra della pietra maximum entropy approach nlp brown della pietra della pietra mercer 
statistical approach sense disambiguation machine translation 
darpa workshop speech natural language 
cover thomas 
elements information theory 
john wiley sons 
csiszar 
divergence geometry probability distributions minimization problems annals probability vol 

ibid 
geometric interpretation darroch ratcli generalized iterative scaling 
annals statistics vol 

csiszar 
information geometry alternating minimization procedures 
statistics decisions supplemental issue 
darroch ratcli 
generalized iterative scaling log linear models 
annals mathematical statistics 
della pietra della pietra la erty ures 
inference estimation long range trigram model 
second international symposium grammatical inference spain 
della pietra della pietra la erty inducing features random elds cmu technical report cmu cs 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society vol 


principle maximum entropy 
mathematical vol 

jaynes 
notes status prospects 
maximum entropy bayesian methods 
kluwer 

jelinek mercer 
interpolated estimation markov source parameters sparse data 
proceedings workshop pattern recognition practice amsterdam netherlands 
lucassen mercer 
information theoretic approach automatic determination phonemic baseforms 
proceedings ieee international conference acoustics speech signal processing san diego ca 
merialdo 
tagging text probabilistic model 
proceedings ibm natural language itl paris france 
nadas mercer bahl cohen cole jelinek lewis 
continuous speech recognition automatically selected acoustic prototypes obtained bootstrapping clustering 
proceedings ieee international conference speech signal processing atlanta ga 
er 
mathematics physics modern engineering second edition mcgraw hill book 
appendix cient algorithms feature selection computing approximate gain feature section picks section left describing detail set algorithms implement feature selection process ciently 
rst describe iterative algorithm computing max gs candidate feature algorithm fact maximum gs occurs rare cases unique value derivative zero 
nd zero apply newton iterative root nding method 
important twist updates obtained applying newton method directly variable guarantee gs increases monotonically updates 
updates derived applying newton method variables convexity updates sequence gs converges monotonically maximum approximate gain gs increases monotonically 
value maximizes gs solving equation 
sequence converges monotonically computational linguistics volume number gs increase monotonically 
consequence convexity solve equation newton method produces sequence recurrence repeated convenience start su ciently close sequence converge converge zero 
general monotonic 
shown sequence monotonic important cases decreasing convex increasing convex 
function convex convex function shown derivatives decreasing convex increasing convex 
wecan apply newton method obtain sequence increases monotonically zero 
similarly wecan apply newton method obtain sequence decreases monotonically zero 
case gs increases monotonically maximum gs 
updates resulting newton method applied variable forr easily computed log order solve recurrence need compute zeroth rst second derivatives gs log fjx ps ps fjx jx ps hjx ps yjx place ready enumerate algorithm computing gain single feature input empirical distribution initial model candidate feature output approximate gain feature 

set ps berger della pietra della pietra maximum entropy approach nlp 
repeat gs converged compute compute gs 
set gs computing approximate gains parallel purpose incremental model growing outlined algorithm need compute maximum approximate gain candidate feature 
obvious approach cycle candidate features apply algorithm sequentially 
algorithm requires pass event inthe training sample iteration entail millions passes training sample 
signi cant cost exists reading training data data stored memory accessed disk example algorithm passes minimal number times data may utility 
give parallel algorithm speci cally tailored scenario 
algorithm computing approximate gains collection features input collection candidate features empirical distribution initial model output approximate gain candidate feature 
calculate expected value training data 
determine set active 
ff jf yjx yg 
initialize ps 
repeat converges set update fjx fjx jx ps fjx ps yjx computational linguistics volume number update log 
substitute determine 
convergence algorithm guaranteed just algorithm iteration step value candidate feature closer optimal value 
importantly gain gs closer maximal gain 

