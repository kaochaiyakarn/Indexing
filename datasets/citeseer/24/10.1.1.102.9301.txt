breadth search crawling yields high quality pages marc najork compaq systems research center lytton avenue palo alto ca usa marc najork compaq com examines average page quality time pages downloaded web crawl unique pages 
connectivity metric pagerank measure quality page 
show traversing web graph breadth search order crawling strategy tends discover high quality pages early crawl 
keywords crawling crawl order breadth search page quality metric pagerank 
study released october directly accessible surface web consists pages deep web dynamically generated web pages consists pages publicly accessible 
comparison google index released june contained full text indexed pages 
words google measurement greatest coverage search engines covers publicly accessible web major search engines worse 
increasing coverage existing search engines orders magnitude pose number technical challenges respect ability discover download index web pages ability serve queries index size 
query engines inverted lists cost serving query linear size index 
search engines attempt download best pages include index 
cho garcia molina page suggested connectivity document quality metrics direct crawler high quality pages 
performed series crawls pages stanford edu domain copyright held author owner 
www may hong kong 
acm 
janet wiener compaq systems research center lytton avenue palo alto ca usa janet wiener compaq com different ordering metrics breadth backlink count pagerank random direct different crawls 
breath ordering pages crawled order discovered 
backlink ordering pages highest number known links crawled 
pagerank ordering pages highest pagerank page quality metric described crawled 
random ordering crawler selects page download random set pages 
repeatability crawls virtual performed cached copy pages 
cho evaluated effectiveness ordering metric examining fast led crawler hot pages 
context hot page page high number links pointing page high pagerank 
pagerank metric direct crawler works extremely 
discovered performing crawl breadth order works particular hot pages defined pages high pagerank 
extends results cho regarding effectiveness crawling breadth search order larger diverse data set 
cho crawl pages stanford edu domain performed crawl pages entire web covering distinct hosts 
connectivity page quality metrics brin page pagerank variations measure quality downloaded pages life crawl 
find breadth search download hot pages average quality pages decreased duration crawl 
suggest crawler modifications strict breadthfirst search increase download rate avoid overloading web server enhance retrieving important pages 
remainder structured follows section reviews pagerank metric evaluate effectiveness crawling breadth search order 
section describes tools conduct experiments 
section describes experiments performed results obtained 
section offers concluding remarks 

pagerank conceivable metrics judging quality web page analyzing content measuring popularity viewed examining connectivity determining pages link page vice versa 
metrics connectivity advantages require information easily accessible page popularity data easy compute scale large page collections 
require retrieving links page full page contents 
storing full page contents requires kilobytes page orders magnitude just storing links 
pagerank connectivity page quality measure suggested brin page 
static measure designed rank pages absence queries 
pagerank computes global worth page 
intuitively pagerank measure page similar degree possible measure importance page 
pagerank page high pages high pagerank contain links page containing outgoing links contributes weight pages links page containing outgoing links 
pagerank page expressed mathematically follows 
suppose total pages web 
choose parameter explained typical value lie range 
pages 
pk link page pi bethe pagerank pi pi number links pi 
pagerank page defined satisfy kx pi pi equation defines uniquely modulo constant scaling factor 
scale pageranks pages sum thought probability distribution pages 
pagerank distribution simple interpretation terms random walk 
imagine web surfer wanders web 
surfer visits page random walk state step web surfer jumps page web chosen uniformly random web surfer follows link chosen uniformly random current page 
occurs probability probability equilibrium probability surfer page simply 
alternative way say average fraction steps walk spends page sufficiently long walks 
means pages high pagerank visited pages low pagerank 
experiments set 
modi fied pagerank slightly pages outgoing links contribute weight equally pages 
random surfer equally jump page page outgoing links 
ran experiments original pagerank algorithm distinguish links pages versus different hosts variant pagerank considers links different hosts 

tools tools conducting research mercator connectivity server developed lab 
mercator crawl web connectivity server provide fast access link information downloaded crawl 
mercator extensible multithreaded high performance web crawler 
written java highly configurable 
default download strategy perform breadth search web modifications 
downloads multiple pages typically parallel 
modification allows download pages day download pages day 

single connection opened web server time 
modification necessary due prevalence relative urls web links average web page refer host leads high degree host locality crawler download queue 
download pages host parallel overload crash web server 

took seconds download document web server mercator wait seconds contacting web server 
modification strictly necessary eases load crawler places individual servers web 
policy reduces rate complaints receive crawling 
experiments described configured mercator extract links downloaded page save disk disk space reasons retain pages 
conducted crawl attempted download pages course days refer days 
download attempts returned valid unique html pages resulted tcp dns errors non return codes non html documents duplicates 
mercator download rate decreased course crawl due increasing access times disk data structures keeps track urls seen 
median download day mean download day 
extracted links data loaded connectivity server cs database urls links 
build cs takes web crawl input creates database representation web graph induced pages crawl 
cs database consists urls crawled extended urls referenced times crawled pages 
incorporating urls multiple links pointing ensured ignore popular urls 
setting threshold incoming links reduced set urls enabled fit database gb ram available 
cs database contains links urls host information url 
maps url outgoing incoming links 
possible get incoming links url just links different hosts 
cs stores links directions average bytes link compared bytes link original connectivity server cs described 
cs average pagerank day crawl average pagerank score day crawl cs designed give high performance access run machine ram store database memory 
mhz compaq alphaserver es gb ram experiments takes ms convert url internal id vice versa ms link retrieve incoming outgoing link internal id database crawl pages contained urls links 
iteration pagerank ran minutes 

average page quality long crawl section report experiments 
implemented pagerank variants cs interface ran algorithm iterations link database 
experiments pagerank computation converged iterations 
pagerank scores conventionally normalized sum making easier think probability distribution normalized sum number nodes graph 
way average page pagerank independent number pages 
shows average pagerank pages downloaded day crawl 
average score pages crawled day times average score pages crawled second day 
average score tapers week second week fourth week 
clearly downloaded high quality pages pages high pagerank early crawl 
decided examine specifically crawled highest ranked pages 
examined pages top pageranks increasing values pages downloaded 
graphs average day crawled pages highest scores 
note horizontal axis shows values log scale 
top top pages crawled day 
anomalies graph equals average day fluctuates second third days crawl 
anomalies caused pages top downloaded week 
pages lot local links links pages host remote links 
words average day top pages crawled top average day top pages crawled pages host endorse hosts endorse 
address phenomenon experiment shown 
equals curve steadily increases day mean download day entire crawl 
experiment checks pages high page rank ranked high crawled early 
example page outgoing links point pages links back artificially high pagerank outgoing links crawled pages 
experiment ran pagerank algorithm graph induced days crawl 
graph contains urls links 
compared top ranked pages data sets 
top scoring pages downloaded weeks ranked top pages day data set 
clear pages important crawl finished 
generalizes statistics value plot percentage overlap top scoring pages day crawl versus day crawl 
top pages different top ranked pages overlap 
overlap continues range extent entire day data set 
suggests breadth search crawling fairly immune type self endorsement described size graph induced full crawl larger graph induced day crawl longer crawl replaced hot pages discovered days irrespective size hot set 
connectivity metrics kleinberg algorithm consider remote links links pages different hosts 
noticed anomalies due lot local links decided experiment variant pagerank algorithm propagates weights remote links 
modification pagerank counts links different hosts proper endorsements page links host viewed improper self endorsement counted 
shows results average pagerank pages downloaded day higher links considered 
average pagerank day second day overlap data sets top pages percent overlap top ranked pages vs days crawl fourth day 
average pagerank declines gradually day 
notice average page rank day crawling higher curve falls sharply 
drop indicates crawling strategy biased hosts crawler standard version pagerank 
believe lack bias due part crawler policies impose rate limit accesses particular host 
flaws metric remote links 
example www yahoo com high pagerank score 
local outlinks weight gets evenly distributed pages graph just pages yahoo 
points 
transitively pages hosts yahoo 
links benefit high score www yahoo com 
section outline ideas problem 

experiments described demonstrate crawler downloads pages breadth search order discovers highest quality pages early stages crawl 
crawl progresses quality downloaded pages deteriorates 
speculate breadthfirst search crawling strategy important pages links numerous hosts links early regardless host page crawl originates 
discovering high quality pages early crawl desirable public web search engines altavista google search engines able crawl index fraction web 
results practical implications search engine companies 
breadth search crawling natural crawling strategy crawlers familiar employ 
example internet archive crawler described perform breadthfirst search entire web picks hosts time crawls hosts parallel 
host crawled exhaustively links point hosts saved seed subsequent crawls hosts 
crawling strategy bias high quality pages hosts crawled picked random order quality downloaded pages uniform crawl 
average pagerank remote links day crawl average pagerank remote links considered similarly web crawler altavista downloaded pages essentially random order 
point altavista mercator 
approach easier provide guarantees essentially spread load imposed crawler evenly web servers result quality discovered pages uniform life crawl 
statements large scale web crawlers 
search engine companies treat crawling strategy trade secret described literature 
cho showed connectivity ordering metric downloads pagerank steer crawler higher quality pages breadth search 
computing pagerank values pages extremely expensive computation 
took day compute pageranks graph pages despite fact hardware resources hold entire graph memory 
pagerank steer crawler require multiple computations larger larger graphs order take newly discovered pages account essentially infeasible real time 
hand crawling breadth search order provides fairly bias high quality pages computational cost 
believe crawling breadthfirst search order provides better tradeoff 

directions extend 
direction try variant pagerank weighs links pages remote hosts differently links pages host 
experiment generated learned remote links count local links weights propagated local links distribute weight www yahoo com pages yahoo 
recommends 
suspect search engines different weights links formal study divide weights links division static remote links get total weight proportional number total links remote link gets times weight local link 
direction try different connectivity metrics 
pagerank connectivity measure know aimed ranking pages world wide web kleinberg algorithm known connectivity analysis algorithm targeted computing quality scores pages 
algorithm computes scores document hub score authority score 
pages high authority scores expected high quality content authority scores similar intent page ranks 
kleinberg algorithm designed rank results query search engine considers small set pages computes authority scores 
believe extend algorithm consider entire graph web 

bharat broder henzinger kumar venkatasubramanian 
connectivity server fast access linkage information web 
proceedings th international world wide web conference pages brisbane australia april 
elsevier science 
brin page 
anatomy large scale hypertextual web search engine 
proceedings th international world wide web conference pages brisbane australia april 
elsevier science 
burner 
crawling eternity building archive world wide web 
web techniques magazine may 
cho garcia molina page 
efficient crawling url ordering 
proceedings th international world wide web conference pages brisbane australia april 
elsevier science 
google press release google launches world largest search engine june 
available www google com press html henzinger heydon mitzenmacher najork 
near uniform url sampling 
proceedings th international world wide web conference pages amsterdam netherlands may 
elsevier science 
heydon najork 
mercator scalable extensible web crawler 
world wide web dec 
kleinberg 
authoritative sources hyperlinked environment 
proceedings th acm siam symposium discrete algorithms pages san francisco ca jan 
lyman varian dunn swearingen 
information 
school information management systems univ california berkeley 
available www sims berkeley edu info mercator home page 
www research digital com src mercator wiener burrows randall stata 
better link compression 
manuscript progress 
compaq systems research center 
marc najork senior member research staff compaq computer systems research center 
current research focuses high performance web crawling web characterization 
principal contributor mercator web crawler altavista 
past worked tools web surfing animation information visualization algorithm animation visual programming languages 
received ph computer science university illinois urbana champaign 
janet wiener member research staff compaq computer systems research center 
currently focuses developing algorithms characterize web tools connectivity server support algorithms 
prior joining compaq research scientist stanford university working data warehousing heterogeneous data integration semi structured data 
received ph university wisconsin madison williams college 
