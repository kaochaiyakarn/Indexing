cmp lg mar nymble high performance learning name finder scott miller bbn fawcett street cambridge ma bbn com presents statistical learned approach finding names nonrecursive entities text muc definition ne task variant standard hidden markov model 
justification problem approach detailed discussion model successful results new approach 

past decade speech recognition community huge successes applying hidden markov models hmm problems 
natural language processing community effectively employed models part ofspeech tagging seminal church efforts weischedel 
propose hmm successfully applied problem name finding 
built named entity ne recognition system slightly modified version hmm call system nymble 
knowledge nymble performs best published results learning name finder 
furthermore performs accuracy level considered near human performance 
system arose ne task specified message understanding conference muc organization names person names location names times dates percentages money amounts delimited text sgml markup 
describe various models employed methods training models method daniel bikel bbn fawcett street cambridge ma bbn com richard schwartz bbn fawcett street cambridge ma schwartz bbn com ralph weischedel bbn fawcett street cambridge ma bbn com decoding test data term decoding borrowed speech recognition community goal traversing hmm recover hidden state sequence 
date successfully trained model english spanish met multi lingual entity task 

background name finding informationtheoretic problem basic premise approach consider raw text encountered decoding passed noisy channel originally marked named entities 
job generative model model original process generated name class annotated words went noisy channel 
formally find sequence name classes nc sequence words pr nc order treat generative model generates original name class annotated words bayes rule pr nc pr nc pr priori probability word sequence denominator constant sentence maximize equation maximizing numerator 
see cover thomas ch 
excellent overview principles information theory 
previous approaches previous approaches typically manually constructed finite state patterns weischedel appelt 
new language new class new information spot write new set rules cover new language cover new class information 
finite state pattern rule attempts match sequence tokens words way general regular expression matcher 
addition finitestate pattern approaches variant brill rules applied problem outlined aberdeen 
interest problem potential applications atomic elements information extraction language considered sentence 
name finder performs known surface lightweight parsing delimiting sequences tokens answer important questions 
step chain processors level processing relate named entities give semantics relationship verb 
way processing discover sentence body text 
furthermore name finding useful right internet query system construct appropriately formed queries bill gates born yield query bill gates born 
name finding directly employed link analysis information retrieval problems 

model model twice conceptual informal overview formal description type hmm 
model bears resemblance scott miller novel air traffic information system atis task documented miller 
person start sentence sentence organization name classes name pictorial representation conceptual model 
conceptual model pictorial overview model 
informally ergodic hmm internal states name classes including name class special states start sentence states 
name class states statistical bigram language model usual word state emission 
means number states name class states equal vocabulary size generation words name classes proceeds steps 
select name class nc conditioning previous name class previous word 

generate word inside name class conditioning current previous 

generate subsequent words inside current name class subsequent word conditioned immediate predecessor 
steps repeated entire observed word sequence generated 
viterbi algorithm efficiently search entire space possible name class assignments maximizing numerator equation pr nc 
informally construction model manner indicates view type name language separate bigram probabilities generating words 
number word states name class equal word feature example text intuition digit year digit year product code date date monetary amount monetary amount percentage number bbn organization person name initial word sentence useful capitalization information sally capitalized word lowercase word punctuation marks words table word features examples intuition interior bigram language model ergodic probability associated transitions 
parameterized trained model transition observed model backs powerful model described 
words word features model consider words ordered pairs element vectors composed word word feature denoted word feature simple deterministic computation performed word added looked vocabulary 
produces fourteen values table 
values computed order listed case non disjoint feature classes take precedence 
features arise need distinguish annotate monetary amounts percentages times dates 
rest features distinguish types capitalization words punctuation marks separate tokens 
particular feature arises fact word capitalized word sentence information capitalized note computed take precedence 
word feature part model language dependent 
fortunately word feature computation extremely small part implementation roughly lines code 
word features distinguish types numbers language independent 
rationale having features clear roman languages capitalization gives evidence names 
formal model section describes model formally discussing transition probabilities generate words name class 
top level model trained probabilistic models accurate powerful model back powerful model insufficient training ultimately back unigram probabilities 
order generate word transition name class calculate likelihood word 
intuition word preceding start name class president titles preceding person name class word name class strong indicators subsequent preceding name classes respectively 
accordingly probability generating word name class factored parts pr nc nc pr nc nc 
non english languages tend comma period reverse way english comma decimal point separates groups digits large numbers 
re ordering precedence relevant word features little effect decoding spanish left spanish lower case words organization names 
see details 
top level model generating word name class pr nc 
magical word probability may computed current word final word name class pr nc 
final imagine useless factor equation conditioned word probability conditioned previous real word previous name class compute pr nc nc nc start sentence observed word note probability conditioned word feature intuition cases previous word help model predict name class word feature capitalization particular important indicator word person name class regardless capitalization especially seen 
calculation probabilities calculation probabilities straightforward events sample size nc nc pr nc nc nc pr nc nc pr nc nc nc nc nc nc nc represents number times events occurred training data count 
back models smoothing ideally sufficient training observation 
event conditional probability wish calculate 
ideally sufficient samples conditional probability conditioned pr nc nc seen sufficient numbers nc 
unfortunately rarely training data compute accurate probabilities decoding new data 
unknown words vocabulary system built trains 
necessarily system knows words stores bigram counts order compute probabilities equations 
question arises system deal unknown words ways appear bigram current word previous word 
answer train separate unknown word model held data gather statistics unknown words occurring midst known words 
typically holds training smoothing unknown word training 
order overcome limitations small amount training data particularly spanish hold data train unknown word model vocabulary built save counts training data file hold concatenate bigram counts unknown word training file 
way gather likelihoods unknown word appearing bigram available training data 
approach perfectly valid trying estimate legitimately seen training 
decoding word bigram unknown model estimate probabilities equations unknown word model model normal training 
unknown word model viewed level back backup model unknown word encountered necessarily accurate bigram model formed actual training 
back models smoothing bigram contains unknown word possible model may seen bigram case model backs powerful descriptive model 
table shows graphic illustration back scheme name class bigrams pr nc nc pr word bigrams nc nc non word bigrams pr nc pr nc nc pr nc number name classes pr nc pr nc pr nc pr nc pr nc pr nc pr nc number word features number word features weight back model computed onthe fly formula computing pr assign weight direct computation formulae weight back model old unique outcomes old sample size model backing 
simple method smoothing tends levels back 
method overcomes problem back model roughly amount training current model factor equation essentially ignores back model puts weight primary model equi trained situation 
example disregarding factor saw bigram come training saw come times see word come name class computing pr come name back unigram probability pr name weight number unique outcomes word state come levels back require sophisticated smoothing technique deleted interpolation 
matter smoothing technique remember smoothing art estimating probability unknown seen training 
table back strategy total number times come preceding word bigram weight bigram probability weight back model 
comparison traditional hmm traditional hmm probability generating particular word word state inside name class states 
alternative traditional model small number states name class having semantic significance states person name class representing middle name states probability associated emitting word vocabulary 
chose bigram language model semantically appealing gram language models remarkably practice 
research attempt gram model captures general significance words name class specifics structure names la person name class example 
important approach mathematically valid long transitions state sum 
decoding modeling existence efficient algorithm finding optimal state sequence decoding original sequence name classes 
number possible state sequences states ergodic model sentence words dynamic programming appropriate merging multiple theories converge particular state viterbi decoding algorithm sentence decoded time linear number tokens sentence viterbi 
interested recovering name class state sequence pursue theories step algorithm 

implementation results development history initially word feature model system relied third level back partof speech tag turn computed stochastic part speech tagger 
tags taken face value best tags system treated part speech tagger black box 
part speech tagger capitalization help determine proper noun tags feature implicit model levels back 
capitalization word submerged part speech tags capitalization probability mass tags 
capitalization name predicting feature appear earlier model eliminated reliance part speech altogether opted direct word feature model described 
originally small number features indicating word number word sentence uppercase initial capitalized lower case 
expanded feature set current state order capture subtleties related numbers due increased performance entirely dramatic test kept enlarged feature set 
contrary expectations experience english spanish contained examples lower case words organization location names 
example departamento department start organization name adjectival place names korean appear locations convention capitalized 
current implementation entire system implemented atop home general purpose class library providing rapid code compile train test cycle 
fact nlp systems suffer lack software computer science engineering effort runtime efficiency key performing numerous experiments turn key improving performance 
system may excellent performance task takes long compile run test data rate improvement system compared run efficiently 
sparc sgi indy appropriate amount ram nymble compile minutes train minutes run mb hr 
days reduction error rate borrow performance measure speech community error rate fmeasure 
see definition measure 
results evaluation section report results evaluating final version learning software 
report results english spanish results set experiments determine impact training set size algorithm performance english spanish 
language held development test set held blind test set 
report results blind test set respective language 
measure scoring program measures precision recall terms borrowed information retrieval community number correct responses number responses number correct responses 
number correct key put informally recall measures number hits vs number possible correct answers specified key file precision measures answers correct ones compared number answers delivered 
measures performance combine form measure performance measure computed weighted harmonic mean precision recall rp represents relative weight recall precision typically value 
knowledge learned name finding system achieved higher measure learned system compared state art manual rule systems similar data 
english spanish results test set english data reporting results muc test set collection wsj documents different test set development 
spanish test set met comprised articles news agency afp 
table illustrates nymble performance compared best reported scores category 
case language best reported score nymble mixed english upper english mixed spanish table measure scores amount training data required learning technique important questions training data required get acceptable performance 
generally performance vary training set size increased decreased 
ran sequence experiments english spanish try answer question final model implemented 
english words training data 
mean text document including headlines including sgml tags words long 
maximum size training available successfully divided training material half eighth original training set size training set words smallest experiment 
give sense size words roughly half length edition wall street journal 
results shown histogram 
positive outcome experiment half training data equivalent performance 
quarter data approximately words performance degraded slightly percent 
reducing training set size words significant decrease performance system performance impressive small training set 
training training training training impact various training set sizes performance english 
learning algorithm performs remarkable nearly comparable handcrafted systems little words training data 
hand result shows merely annotating data yield dramatic improvement performance 
increased training data possible detailed models require data achieve significantly improved system performance detailed models 
spanish words training data 
measured performance system half training data slightly words text 
shows results 
change performance little words training data 
results languages comparable 
little words training data produces performance nearly comparable handcrafted systems 
training training impact training set size performance spanish 
initial results quite favorable done potentially improve performance completely close gap learned rule name finding systems 
incorporate current model lists organizations person names locations aliasing algorithm dynamically updates model ibm alias international business machines longer distance information find names captured bigram model 
shown fairly simple probabilistic model finding names numerical entities specified muc tasks performed near human performance likened 
shown system trained efficiently appropriately consistently marked answer keys trained languages foreign trainer system example speak spanish trained nymble answer keys marked native speakers 
formalisms techniques new approach task model lies novelty 
incredibly difficult nature nlp tasks example learned stochastic approach name finding lends credence argument nlp community ought push approaches find limit phenomena may captured probabilistic finite state methods 

aberdeen burger day hirschman robinson vilain 
proceedings sixth message understanding conference muc morgan kaufmann publishers columbia maryland pp 

appelt jerry hobbs bear israel kameyama kehler martin myers tyson 
proceedings sixth message understanding conference muc morgan kaufmann publishers columbia maryland pp 

church 
second conference applied natural language processing austin texas 
cover thomas 
elements information theory john wiley sons new york 
miller bobrow schwartz ingria 
human language technology workshop morgan kaufmann publishers new jersey pp 

viterbi 
ieee transactions information theory 
weischedel 
proceedings sixth message understanding conference muc morgan kaufmann publishers columbia maryland pp 

weischedel meteer schwartz ramshaw 
computational linguistics 

reported supported part defense advanced research projects agency technical agent part fort contract number dabt 
views contained document authors interpreted necessarily representing official policies expressed implied defense advanced research projects agency united states government 
give special stuart shieber mckay professor computer science harvard university endorsed helped foster completion phase nymble development 
