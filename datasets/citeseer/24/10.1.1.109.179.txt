practical part speech tagger doug cutting julian kupiec jan pedersen penelope sibun implementation part speech tagger hidden markov model 
methodology enables robust accurate tagging resource requirements 
lexicon unlabeled training text required 
accuracy exceeds 
describe implementation strategies optimizations result high speed operation 
applications tagging described phrase recognition word sense disambiguation grammatical function assignment 
desiderata words ambiguous part speech 
example tag noun verb 
word appears context words ambiguity reduced tag part speech label word tag noun 
part speech tagger system uses context assign parts speech words 
automatic text tagging important rst step discovering linguistic structure large text corpora 
part speech information facilitates higher level analysis recognizing noun phrases patterns text 
tagger function practical component ina language processing system believe tagger xerox palo alto research center coyote hill road palo alto ca usa robust text corpora contain ungrammatical constructions isolated phrases titles nonlinguistic data tables 
corpora contain words unknown tagger 
desirable tagger deal gracefully situations 
cient tagger analyze arbitrarily large corpora cient performing time linear number words tagged 
training required fast enabling rapid turnaround new corpora new text genres 
accurate tagger attempt assign correct part speech tag word encountered 
tunable tagger able take advantage linguistic insights 
able correct systematic errors supplying appropriate priori hints 
possible give di erent hints di erent corpora 
reusable ort required retarget tagger new corpora new tagsets new languages minimal 
methodology background di erent approaches building text taggers 
greene rubin rule approach program greene rubin aid tagging brown corpus francis kucera 
disambiguated corpus rest done manually period years 
koskenniemi rule approach implemented nite state machines koskenniemi 
statistical methods derose garside 
provide capability resolving ambiguity basis interpretation 
form markov model widely assumes word depends probabilistically just part speech category turn depends solely categories preceding words 
types training parameter estimation model 
rst tagged training corpus 
merialdo bootstrap method training merialdo 
rst relatively small amount text manually tagged train partially accurate model 
model tag text tags manually corrected retrain model 
church uses tagged brown corpus training church 
models involve probabilities word lexicon large tagged corpora required reliable estimation 
second method training require tagged training corpus 
situation baum welch algorithm known forward backward algorithm baum 
regime model called hidden markov model hmm state transitions part speech categories assumed unobservable 
jelinek method training text tagger jelinek 
parameter smoothing conveniently achieved method deleted interpolation weighted estimates taken rst order models uniform probability distribution jelinek mercer 
kupiec word equivalence classes referred ambiguity classes parts speech pool data individual words kupiec 
common words represented individually su cient data exist robust estimation 
words represented set possible categories assume 
manner vocabulary words brown corpus reduced approximately distinct kupiec 
reduce number parameters rst order model employed assumes word category depends immediately preceding word category 
kupiec networks selectively augment context basic rstorder model uniformly second order dependencies 
approach describe techniques satis es criteria listed section 
hmm permits complete exibility choice training corpora 
text desired domain tagger tailored particular text database training portion database 
lexicons containing alternative tag sets easily accommodated need re labeling training corpus exibility specialized tags 
resources required simply lexicon suitably large sample ordinary text taggers built minimal ort languages kupiec 
ambiguity classes rst order model reduces number parameters estimated signi cant reduction accuracy discussed section 
enables tagger reliably trained moderate amounts text 
wehave produced reasonable results training sentences 
fewer parameters reduce time required training 
relatively ambiguity classes su cient wide coverage adding new words lexicon requires retraining ambiguity classes accommodated 
vocabulary independence achieved predicting categories words lexicon context su information 
probabilities corresponding category sequences occurred training data assigned small non zero values ensuring model accept sequence tokens providing tagging 
fact words typically associated part ofspeech categories carefully ordering computation algorithms linear complexity section 
hidden markov modeling hidden markov modeling component tagger implemented independent module speci cation levinson special attention space time ciency issues 
rst order modeling addressed presumed remainder discussion 
formalism brief hmm doubly stochastic process generates sequence symbols fs st si nite set possible symbols composing underlying markov process state dependent symbol generator markov process noise 
markov process captures notion sequence dependency described set ofn states matrix transition jn probability anda vector initial probabilities ig probability starting statei 
symbol generator state dependent measure described matrix symbol jn km wherem jw probability generating markov process 
part speech tagging model word order dependency underlying markov process operates terms lexical tags able observe sets tags ambiguity classes possible individual words 
ambiguity class word set permitted parts speech correct context 
hidden markov modeling allows compute probable sequence state transitions sequence lexical tags corresponding sequence ambiguity classes 
identi ed number possible tags andw set ambiguity classes 
applying hmm consists tasks estimating model band training set computing sequence underlying state transitions new observations 
maximum likelihood estimates estimates maximize probability training set application alternating expectation procedure known baum welch forward backward algorithm baum 
proceeds recursively de ning sets probabilities forward probabilities nx aij bj st tt ibi alli probabilities nx st 
forward probability joint probability sequence time fs stg event markov process timet 
similarly backward probability probability seeing sequence fst st stg markov process timet 
follows probability ofthe entire sequence nx nx st range tt 
hidden markov modeling see rabiner juang 
write bj st st sk 
conveniently evaluated case initial choice expected number transitions ij conditioned observation computed follows ij aij ij ij st st pt similarly bjk estimated follows bjk st sk pt summary nd maximum likelihood estimates baum welch algorithm chooses starting values applies equations compute new values iterates convergence 
shown algorithm converge possibly non global maximum baum 
model estimated selecting underlying sequence state transitions corresponding thought maximization sequences generates 
cient dynamic programming procedure known viterbi algorithm viterbi arranges computation proceed time proportional tot suppose fv tt state sequence generates probability ty st nd probable sequence de ning ibi perform recursion max aij bj st max tt jn 
crucial observation statei need consider probable sequence arriving state timet 
probability probable sequence max sequence reconstructed max qt 
numerical stability baum welch algorithm equations viterbi algorithm equation involve operations products numbers constrained 
products easily ow measures taken rescale 
approach probabilities accumulating product depending ont levinson 
de ne ct nx tt de ne ct place equation de ne iteration nx aij bj st tt note tt similarly de ne ct nx st scaled backward forward probabilities exchanged unscaled probabilities equations ecting value ratios 
see note ct ct terms scaled probabilities equation example seen unchanged jy ct pn ct slight di culty occurs equation cured addition new term ct product upper sum st ct aij numerical instability viterbi algorithm ameliorated operating logarithmic scale levinson 
maximizes log probability sequence state transitions log log log tx equation replaced log log st max log aij st care taken zero probabilities 
elegantly handled ieee negative nity 
reducing time complexity seen equations time cost training iso tn 
similarly equation viterbi algorithm tn 
part speech tagging problem structure dictates matrix symbol sparsely populated 
bij ambiguity class corresponding includes part speech tag associated statei 
practice degree overlap ambiguity classes relatively low tokens assigned unique tags non zero symbol probability 
sparseness ofb leads consider restructuring equations check zero symbol probability obviate need computation 
equation conveniently factored dependence bj st outside inner sum 
average number non zero entries cost computing equation reduced 
equations similarly reduced switching order iteration 
example equation time accumulate terms parallel 
net ect rewriting place abj st check outside innermost iteration 
equations submit similar approach 
equation 
cost training reduced experience amounts order magnitude speedup 
time complexity viterbi algorithm reduced noting st factored maximization equation 
controlling space complexity adding sizes probability easy see storage cost directly representing model proportional ton 
running baum welch algorithm requires storage sequence observations probabilities vector copies matrices originals overwritten iteration 
grand total space required training proportional tot 
andm xed model parameter varied reduce storage costs ist adequate training requires processing tens thousands hundreds thousands tokens kupiec 
training set considered long sequence large broken number smaller sequences convenient boundaries 
rst order hidden markov modeling stochastic process ectively restarts unambiguous tokens sentence paragraph markers tokens convenient points break training set 
baum welch algorithm run separately starting point piece resulting trained models recombined way 
approach simply average 
fails equivalent approach maintains mapping states non zero symbol probabilities simply avoids inner iteration computing products zero kupiec 
states indistinguishable sense transition probabilities symbol probabilities start states matched trained models 
important state distinguished role relatively easy achieve part speech tagging 
implementation baum welch algorithm breaks input xed sized pieces training text 
baum welch algorithm run separately piece results averaged 
running viterbi algorithm requires storage sequence observations vector current scratch array size matrix indices total proportional tot grand total including model oft 
andm xed 
need longer single sentence observed hmm viterbi algorithm restarts sentence boundaries 
model tuning hmm part speech tagging tuned variety 
choice tagset lexicon determines initial model 
second empirical apriori information uence choice starting values baum welch algorithm 
example counting instances ambiguity classes running text allows assign non uniform starting probabilities particular tag realization particular ambiguity class 
alternatively state priori particular ambiguity class re ection subset component tags 
example ambiguity class consisting open class tags unknown words may encode fact unknown words nouns proper nouns biasing initial probabilities inb 
biasing starting values arises noting tags followed 
example lexical item maps ambiguity class containing tags nitive marker occurs ambiguity class 
stated hmm states indistinguishable 
remedied setting initial transition probabilities nitive marker strongly favor transitions states verb ected adverb 
implementation allows sorts biasing starting values ambiguity classes annotated favored tags states annotated favored transitions 
biases may speci ed sets set complements 
biases implemented replacing probabilities small constant machine epsilon redistributing mass possibilities 
ect indicated outcomes disallowing su cient converse data values 
architecture support developed system architecture text access cutting 
architecture de nes components systems search analysis corpus index stem tag ambiguity class stem tag corpus provides text generic manner analysis extracts terms text index stores term occurrence statistics search utilizes statistics resolve queries 
part speech tagger described implemented analysis module 
illustrates architecture showing tagger analysis implementation detail 
tagger modular architecture isolating standard protocols elements may vary enabling easy substitution alternate implementations 
illustrated data types tagger components 
analysis implementation tagger generate terms text 
context term word stem annotated part speech 
text enters analysis sub system rst processing module encounters tokenizer duty convert text sequence characters sequence tokens 
sentence boundaries identi ed tokenizer passed reserved tokens 
tokenizer subsequently passes tokens lexicon 
tokens converted set stems annotated part speech tag 
set tags identi es ambiguity class 
identi cation classes responsibility lexicon 
lexicon delivers set stems paired tags ambiguity class 
training module takes long sequences ambiguity classes input 
uses baum welch algorithm produce trained hmm input tagging module 
training typically performed sample corpus hand trained hmm saved subsequent corpus large 
tagging module bu ers sequences ambiguity analysis tagging token character trained hmm lexicon tokenizer tagger modules system context training ambiguity class classes sentence boundaries 
sequences disambiguated computing maximal path hmm viterbi algorithm 
operating sentence granularity provides fast throughput loss accuracy sentence boundaries unambiguous 
resulting sequence tags select appropriate stems 
pairs stems tags subsequently emitted 
function complete analysis component providing tagged text search indexing components sub system elaborate analysis phrase recognition 
tokenizer implementation problem tokenization addressed compilation programming languages 
accepted approach specify token classes regular expressions 
may compiled single deterministic nite state automaton partitions character streams labeled tokens aho lesk 
context tagging require token classes sentence boundary word 
classes may include numbers paragraph boundaries various sorts punctuation braces various types commas 
simplicity henceforth assume words sentence boundaries extracted 
just programming languages text possible unambiguously specify required token classes regular expressions 
addition simple lookahead mechanism allows speci cation right context aho lesk 
example sentence boundary english text identi ed period followed whitespace followed uppercase letter 
letter consumed rst component token 
lookahead mechanism allows specify sentence boundary regular expression nal character matched considered part token 
method meets stated goals system 
cient requiring character examined modulo lookahead 
easily parameterizable providing expressive power concisely de ne accurate robust token classes 
lexicon implementation lexicon module responsible enumerating parts speech associated stems word 
english word lexicon return verb doe plural noun 
responsible identifying ambiguity classes sets tags 
employed stage implementation consult manually constructed lexicon nd stems parts speech 
exhaustive expensive impossible produce 
fortunately small set words accounts vast occurences 
high coverage obtained prohibitive ort 
words manually constructed lexicon generally open class regularly ected 
second stage language speci method employed guess ambiguity classes unknown words 
languages english french word su xes provide strong cues words possible categories 
predictions word category analyzing su xes untagged text kupiec meteer 
nal stage word manually constructed lexicon su recognized default ambiguity class 
class typically contains open class categories language 
dictionaries su tables ciently implementable letter trees tries knuth require character word examined lookup 
performance section detail tagger meets desiderata outlined section 
cient system implemented common lisp steele 
timings reported sun sparcstation 
english lexicon contains tags ambiguity classes 
training performed words articles selected randomly encyclopedia 
iterations training performed total time cpu seconds 
time breakdown component training average seconds token tokenizer lexicon iteration iterations total tagging performed words collection articles journalist dave barry 
required total cpu seconds 
time breakdown follows tagging average seconds token tokenizer lexicon viterbi total seen gures training new corpus may accomplished matter minutes tens megabytes text may tagged hour 
accurate robust lexicon tagset built tagged text brown corpus francis kucera training half corpus words tagging word instances assigned correct tag 
iterations training 
level accuracy comparable best achieved taggers church merialdo 
brown corpus contains fragments providing demonstration robustness 
tunable reusable tagger tunable systematic tagging errors anomalies addressed 
similarly important fast easy target tagger new genres languages experiment di erent tagsets re ecting di erent insights linguistic phenomena text 
section describe hmm implementation supports tuning 
addition implementation supports number explicit parameters facilitate tuning reuse including speci cation lexicon training corpus 
support exible tagset 
example want collapse distinctions lexicon positive comparative superlative adjectives small change mapping lexicon tagset 
similarly ifwe wish ner grain distinctions available lexicon case marking pronouns simple way note exceptions 
applications tagger number applications 
describe applications phrase recognition word sense disambiguation grammatical function assignment 
projects part research ort shallow analysis techniques extract content text 
phrase recognition constructed system recognizes simple phrases input sequence tags sentence 
recognizers noun phrases verb groups adverbial phrases prepositional phrases 
phrases comprises contiguous sequence tags satises simple grammar 
example noun phrase unary sequence containing pronoun tag arbitrarily long sequence noun adjective tags possibly preceded determiner tag possibly embedded possessive marker 
longest possible sequence program committee program 
conjunctions recognized part phrase example fragment cats dogs cats dogs recognized noun phrases 
prepositional phrase attachment performed stage processing 
approach phrase recognition cases captures parts phrases approach minimizes false positives rely recognizers results 
word sense disambiguation part speech tagging useful tool lexical disambiguation example knowing dig noun verb indicates word appropriate meaning 
words multiple meanings occupying part speech 
tagger implementation experimental noun homograph disambiguation algorithm hearst 
algorithm known catch word performs supervised training large text corpus gathering lexical orthographic simple syntactic evidence sense ambiguous noun 
period training classi es new instances noun checking context previously observed instances choosing sense evidence 
sense distinctions coarse disambiguation accomplished expense knowledge bases inference mechanisms 
initial tests resulted accuracies nouns strongly distinct senses 
algorithm uses tagger ways determine part speech target word ltering non noun usages ii step phrase recognition analysis context surrounding noun 
grammatical function assignment phrase recognizers provide input system sibun recognizes nominal arguments verbs speci cally subject object predicative arguments 
rely information arity voice speci particular verbs involved 
rst step assigning grammatical functions partition tag sequence sentence phrases 
phrase types include mentioned section additional types account conjunctions indicators sentence boundaries unknown type 
sentence partitioned simple noun phrase examined context phrase left phrase right 
basis local context set rules noun phrase marked subject object predicative marked 
label predicative assigned determined governing verb group form verb form 
determined labeled objects 
noun phrase labeled annotated governing verb closest verb group right left 
algorithm accuracy approximately assigning grammatical functions 
acknowledgments marti hearst contributions lauri karttunen annie zaenen lexicons kris supporting project 
aho aho sethi ullman 
compilers principles techniques tools 
addison wesley 
baum baum 
inequality associated maximization technique statistical estimation probabilistic functions markov process 
inequalities 
church church 
stochastic parts program noun phrase parser unrestricted text 
proceedings second conference applied natural language processing acl pages 
cutting cutting pedersen 
object oriented architecture text retrieval 
conference proceedings riao intelligent text image handling barcelona spain pages april 
available xerox parc technical report ssl 
derose derose 
grammatical category disambiguation statistical optimization 
computational linguistics 
merialdo merialdo 
natural language modeling text transcription 
ieee transactions pattern analysis machine intelligence pami 
francis kucera francis kucera 
frequency analysis english usage 
houghton mi 
garside garside leech sampson 
computational analysis english 
longman 
greene rubin greene rubin 
automatic grammatical tagging english 
technical report department linguistics brown university providence rhode island 
hearst hearst 
noun homograph disambiguation local context large text corpora 
proceedings th new oed conference corpora pages oxford 
jelinek mercer jelinek mercer 
interpolated estimation markov source parameters sparse data 
proceedings workshop pattern recognition practice pages amsterdam 
north holland 
jelinek jelinek 
markov source modeling text generation 
editor impact processing techniques communication 
dordrecht 
knuth knuth 
art computer programming volume sorting searching 
addison wesley 
koskenniemi koskenniemi 
state parsing disambiguation 
karlgren editor coling pages helsinki university 
kupiec kupiec 
augmenting hidden markov model phrase dependent word tagging 
proceedings darpa speech natural language workshop pages cape cod ma 
morgan kaufmann 
kupiec kupiec 
probabilistic models short long distance word dependencies running text 
proceedings darpa speech natural language workshop pages philadelphia 
morgan kaufmann 
kupiec kupiec 
robust part speech tagging hidden markov model 
submitted computer speech language 
lesk lesk 
lex lexical analyzer generator 
computing science technical report bell laboratories murray hill new jersey 
levinson levinson rabiner sondhi 
application theory probabilistic functions markov process automatic speech recognition 
bell system technical journal 
merialdo merialdo 
tagging text probablistic model 
proceedings icassp pages toronto canada 
meteer meteer schwartz weischedel 
post probabilities language processing 
proceedings th international joint conference onarti cial intelligence pages 
ieee task 
proposed standard binary oating point arithmetic 
computer march 
rabiner juang rabiner juang 
hidden markov models 
ieee assp magazine january 
sibun sibun 
grammatical function assignment unrestricted text 
internal report xerox palo alto research center 
steele steele jr common lisp language 
digital press second edition 
viterbi viterbi 
error bounds convolutional codes asymptotically optimal decoding algorithm 
ieee transactions information theory pages april 
