infinite gamma poisson feature model school computer science university manchester uk cs man ac uk probability distribution non negative integer valued matrices possibly infinite number columns 
derive stochastic process reproduces distribution equivalence classes 
model play role prior nonparametric bayesian learning scenarios multiple latent features associated observed data feature multiple appearances occurrences data point 
data arise naturally learning visual object recognition systems unlabelled images 
nonparametric prior consider likelihood model explains visual appearance location local image patches 
inference model carried markov chain monte carlo algorithm 
unsupervised learning mixture models assumes latent cause associated data point 
assumption quite restrictive useful generalization consider factorial representations assume multiple causes generated data 
factorial models widely modern unsupervised learning algorithms see algorithms model text data :10.1.1.110.4050
algorithms learning factorial models deal problem specifying size representation 
bayesian learning especially nonparametric methods indian process useful solving problem 
factorial models usually assume feature occurs data point 
inefficient model precise generation mechanism data images 
image contain views multiple object classes cars humans class may multiple occurrences image 
deal features having multiple occurrences introduce probability distribution sparse non negative integer valued matrices possibly unbounded number columns 
matrix row corresponds data point column feature similarly binary matrix indian process 
element matrix zero positive integer expresses number times feature occurs specific data point 
model derived considering finite gamma poisson distribution infinite limit equivalence classes non negative integer valued matrices 
stochastic process reproduces infinite model 
process uses distribution integer partitions introduced population genetics literature equivalent distribution partitions objects induced dirichlet process 
infinite gamma poisson model play role prior nonparametric bayesian learning scenario latent features number occurrences unknown 
prior consider likelihood model suitable explaining visual appearance location local image patches 
introducing prior parameters likelihood model apply bayesian learning markov chain monte carlo inference algorithm show results image data 
finite gamma poisson model xn data data point xn set attributes 
section specify xn collection local image patches 
assume data point associated set latent features feature multiple occurrences 
znk denote number times feature occurs data point xn 
features znk non negative integer valued matrix collects znk values row corresponds data point column feature 
znk drawn poisson feature specific parameter follows distribution znk exp znk 
mk exp znk 
mk znk 
assume parameter follows gamma distribution favors sparsity sense explained shortly exp 
hyperparameter vague gamma prior 
equations easily integrate parameters follows znk mk mk znk 
shows hyperparameter columns independent 
note distribution exchangeable reordering rows alter probability 
increases distribution favors sparsity 
shown expectation sum elements columns independent expectation znk znk znk znk nb znk denotes negative binomial distribution positive integers znk nb znk znk pr znk mean equal equation expectation sum independent number features 
increases sparser controls sparsity matrix 
alternative way deriving joint distribution generative process ln ln poisson zn znk znk zn znk denotes symmetric dirichlet 
marginalizing gives rise distribution 
process generates gamma random variable multinomial parameters samples rows independently poisson multinomial pair 
connection dirichlet multinomial pair implies infinite limit gamma poisson model related dirichlet process 
section see connection revealed distribution 
models combine gamma poisson distributions widely applied statistics 
point finite model shares similarities techniques model text data 
infinite limit stochastic process express probability distribution infinite features need consider equivalence classes matrices similarly 
association columns features defines arbitrary labelling features 
likelihood affected relabelling features equivalence class matrices reduced standard form column reordering 
define left ordered form non negative integer valued matrices follows 
assume possible znk holds znk sufficiently large integer 
define znk integer number associated column expressed numeral system basis left ordered form defined columns appear left right decreasing order magnitude numbers 
starting equation wish define probability distribution matrices constrained left ordered standard form 
kh multiplicity column number example 
number zero columns 
equivalence class consists pcn different kh 
ces generated distribution equal probabilities reduced left ordered form 
probability 
kh 
mk mk znk 
assume features represented mk rest features unrepresented mk 
infinite limit derived similar strategy expressing distribution partitions objects limit dirichlet multinomial pair 
limit takes form cn kh 
mk 
znk 
mk 
expression defines exchangeable joint distribution non negative integer valued matrices infinite columns left ordered form 
sequential stochastic process reproduces distribution 
stochastic process distribution equation derived simple stochastic process constructs matrix sequentially data arrive time fixed order 
steps stochastic process discussed 
data point arrives features currently unrepresented 
sample feature occurrences set unrepresented features follows 
firstly draw integer number negative binomial nb mean value equal 
total number feature occurrences data point 
randomly select partition integer parts 
drawing distribution integer partitions 

multiplicity integer partition 
distribution equivalent distribution partitions objects induced dirichlet process chinese restaurant process derive simple combinatorics arguments 
difference distribution integer partitions distribution partitions objects 
kn number represented features nth data point arrives 
feature kn choose znk popularity feature previous data partition positive integer way writing integer sum positive integers order matter partitions 
points 
popularity expressed total number occurrences feature mk zik 
particularly draw znk nb znk mk mean value equal mk sampled represented features need consider sample set unrepresented features 
similarly data point draw integer gn nb gn subsequently select partition integer drawing formula 
process produces distribution 
gn mk 
znk 
integer multiplicities nth data point arise draw distribution 
note expression exactly form distribution equation exchangeable depends order data arrive 
consider left ordered class matrices generated stochastic process obtain exchangeable distribution equation 
note similar situation arises indian process 
conditional distributions combine prior likelihood model wish inference gibbs type sampling need express conditionals form znk nk nk znk 
derive conditionals limits conditionals finite model stochastic process 
suppose current value exist represented features mk 
en 
conditional znk 
different cases need special conditional samples new features accounts 
conditional draws integer number nb gn determines occurrences new features choosing partition integer gn distribution 
conditional directly expressed equation prior nb znk 
typically likelihood model depend quantity posterior conditional data likelihood model images image contain multiple objects different classes 
object class occurrences multiple instances class may appear simultaneously image 
unsupervised learning deal unknown number object classes images unknown number occurrences class image separately 
object classes latent features wish infer underlying feature occurrence matrix consider observation model combination latent dirichlet allocation gaussian mixture models 
combination 
image represented dn local patches detected image xn yn wn wni dn 
dimensional location patch wni indicator vector binary satisfies ni points set possible visual appearances 
denote data locations appearances respectively 
describe probabilistic model starting joint distribution variables joint dn zn mn zn sni wni sni sni mn 
features kind unrepresented features unique features occur data point znk 
wni sni mn dn graphical model joint distribution equation 
graphical representation distribution depicted 
explain pieces joint distribution causal structure graphical model 
firstly generate prior draw feature occurrence matrix infinite gamma poisson prior 
matrix defines structure remaining part model 
parameter vector kl describes appearance local patches feature object class generated symmetric dirichlet set vectors drawn hyperparameter symmetric dirichlet common features 
note feature appearance parameters depend number represented features obtained counting non zero columns parameter vector defines image specific mixing proportions mixture model associated image see mixture model arises notice local patch image belongs certain occurrence feature 
double index kj denote occurrence feature znk 
mixture model mn znk components total number feature occurrences image assignment variable sni kj ni takes mn values indicates feature occurrence patch drawn symmetric dirichlet zn mn zn denotes nth row hyperparameter shared images 
notice depends nth row parameters mn determine image specific distribution locations local patches image assume occurrence feature forms gaussian cluster patch locations 
follows image specific gaussian mixture mn components 
assume component kj mean covariance 
describes object location object shape 
mn collect means covariances clusters image object image arbitrary scale orientation drawn quite vague prior 
conjugate normal wishart prior pair mn zn znk znk hyperparameters shared features images 
assignment sni determines allocation local patch certain feature occurrence follows multinomial sni znk skj znk ni 
similarly observed data pair wni local image patch generated wni sni ni skj ni sni mn znk znk skj ni hyperparameters take fixed values give vague priors depicted graphical model shown 
chosen conjugate priors analytically marginalize joint distribution parameters mn obtain 
marginalizing assignments generally intractable mcmc algorithm discussed produces samples posterior 
mcmc inference inference model involves expressing posterior feature occurrences assignments parameter 
note joint factorizes sn zn yn sn zn sn denotes assignments associated image algorithm uses mainly gibbs type sampling conditional posterior distributions 
due space limitations briefly discuss main points algorithm 
mcmc algorithm processes rows iteratively updates values 
single step change element nk nk 
initially mn znk means mixture component explains data image 
proposal distribution changing ensures constraint satisfied 
suppose wish sample new value znk joint model 
simply znk nk useful znk changes number states assignments sn take changes 
clear znk structural variable affects number components mn znk mixture model associated image assignments sn 
hand dimensionality assignments sn images affected znk changes 
deal marginalize sn sample znk marginalized posterior conditional znk nk computed znk nk znk nk yn sn zn sn zn znk infinite case computed described section computing sum requires approximation 
sum marginal likelihood apply importance sampling importance distribution posterior conditional sn yn 
sampling sn yn carried applying local gibbs sampling moves global metropolis moves allow occurrences different features exchange data clusters 
implementation consider single sample drawn posterior distribution sum approximated yn zn sample accepted burn period 
additionally scans update add metropolis hastings steps update hyperparameter posterior conditional equation 
experiments experiment set artificial images 
consider features regular shapes shown 
discrete patch appearances correspond pixels take possible grayscale values 
feature multinomial distribution appearances 
generate image decide include feature probability 
included feature randomly select number occurrences range 
feature occurrence select pixels appearance multinomial place respective feature shape random location feature occurrences occlude 
row shows training image left locations pixels middle discrete appearances right 
mcmc algorithm initialized zn 
third row shows left sum right evolve mcmc iterations 
algorithm iterations sn training image locations yn appearances wn row shows training image left locations pixels middle discrete appearances right 
second row shows feature occurrences images 
image corresponding row shown 
third row shows left sum right evolve mcmc iterations 
left plot row shows locations detected patches bounding boxes annotated images 
remaining plots show examples detections dominant features including car category non annotated images 
visited matrix generate data stabilizes 
samples equal 
state frequently visited second row shows different feature occurrences images 
ellipse drawn posterior mean values pair illustrates predicted location shape feature occurrence 
note ellipses color correspond different occurrences feature 
second experiment consider real images uiuc cars database 
patch detection method constructed dictionary visual appearances clustering sift descriptors patches means 
locations detected patches shown row left 
partially labelled images 
particularly images annotated car views bounding boxes 
allows specify elements column matrix feature correspond car category 
values plus assignments patches inside boxes change sampling 
patches lie outside boxes annotated images allowed part car occurrences 
achieved applying partial gibbs sampling updates metropolis moves sampling assignments algorithm initialized iterations stabilizes fluctuates twelve features 
keep plots uncluttered shows detections dominant features including car category non annotated images 
red ellipses correspond different occurrences car feature green ones tree feature blue ones street feature 
discussion infinite gamma poisson model nonparametric prior non negative integer valued matrices infinite number columns 
discussed prior unsupervised learning multiple features associated data feature multiple occurrences data point 
infinite gamma poisson prior purposes 
example interesting application bayesian matrix factorization matrix observations decomposed product matrices non negative integer valued matrix 
antoniak 
mixture dirichlet processes application bayesian nonparametric problems 
annals statistics 
blei ng jordan 
latent dirichlet allocation 
jmlr 

applying discrete pca data analysis 
uai 
canny 
gap factor model discrete data 
sigir pages 
acm press 

sampling theory selectively neutral alleles 
theoretical population biology 
green richardson 
modelling heterogeneity dirichlet process 
scandinavian journal statistics 
griffiths ghahramani 
infinite latent feature models indian process 
nips 
lowe 
distinctive image features scale invariant keypoints 
international journal computer vision 
neal 
bayesian mixture modeling 
th international workshop maximum entropy bayesian methods statistical analysis pages 
newton raftery 
approximate bayesian inference weighted likelihood bootstrap 
journal royal statistical society series 
saund 
multiple cause mixture model unsupervised learning 
neural computation 
sudderth torralba freeman willsky 
describing visual scenes transformed dirichlet processes 
nips 
available fromhttp cs uiuc edu data car 
