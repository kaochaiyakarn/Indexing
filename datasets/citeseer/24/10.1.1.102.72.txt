role identification free text hidden markov models georgios georgios paliouras software knowledge engineering laboratory institute informatics telecommunications tel fax mail iit gr 
explore hidden markov models task role identification free text 
role identification important stage information extraction process assigning roles particular types entities respect particular event 
hidden markov models hmms shown achieve performance applied information extraction tasks semistructured free text 
main contribution analysis linguistic processing textual data improve extraction performance hmms 
emphasis minimal computationally expensive linguistic analysis 
performance hmms worse equivalent manually constructed system 
clear paths improvement method shown aiming method easily adaptable new domains 

role identification subtask information extraction dealing assignment event specific roles various entities mentioned piece text describes event 
information extraction process defined message understanding conferences role identification part scenario template filling task ultimate goal information extraction process 
role identification hard task requiring significant computationally expensive linguistic processing methods 
investigate problem role identification hidden markov models hmms 
hidden markov modeling powerful statistical learning technique widespread application area speech recognition 
hmms applied successfully language related tasks including part speech tagging named entity recognition text segmentation 
main advantage hmms language modeling fact suited modeling sequential data spoken written language 
serious motivation hmms text tasks strong statistical foundations provide sound theoretical basis constructed models 
hand important concern hmms large amount training data required acquire estimates model parameters 
research suggested hmms task role identification semistructured free text 
leek designed hmms extract gene locations biomedical texts 
freitag mccallum hmms information extraction newsgroups collection reuter articles 
focus techniques improve estimation model parameters learn model structure training data 
hmms role identification free text largely unexplored territory important issues examined 
examine time hmms role identification greek texts 
purpose collection greek financial articles describing acquisitions project 
previous hmms role identification pay particular attention linguistic processing textual data improve extraction performance hmms 
difficult issue initial intuition linguistic analysis help extracting information natural language face reality high computationally costs linguistic analysis tools 
important identify minimum necessary linguistic processing improving performance information extraction maintaining computational efficiency process 
line thought performed various types linguistic preprocessing dataset considered different data representations linguistic information represented part text sequential form 
motivation sequential representation suitability hmms modeling sequential data 
rest structured follows 
section review basic theory hmms discuss hmms role identification 
section experimental results dataset varying linguistic processing 
conclude section discussing potential improvements method 

hmms role identification basic theory hidden markov model extension markov process observation probabilistic function state 
elements characterize hmm individual states interconnected way state reached state ergodic model 
discrete vocabulary observation symbols vm 
nxn state transition probability matrix ij indicating probability transitioning state state deal order hmms means transitioning state time depends current state time ij observation symbol probability matrix indicating probability observing symbol state nx initial state matrix si indicating probability state time 
hmm probabilistic generative model sequence symbols denoted produced starting initial state probability emitting output symbol probability transitioning new state probability ij emitting new symbol reaching final state time emitting output symbol deal discrete output hmms meaning sequence discrete symbols chosen vocabulary classic issues hmms 
parameters hmm sequence symbols efficiently compute probability observation sequence produced hmm 
evaluation problem allows choose model best matches sequence 

parameters hmm sequence symbols efficiently compute state sequence associated symbol sequence 
state sequence hidden observed sequence issue relates uncovering hidden state sequence 

efficiently estimate parameters maximize difficult problems dealing training hmm set observation sequences 
problems solved forward backward viterbi baum welch algorithms respectively described 
key insight hmms language related tasks state transitions modeled bigram model emitting label types length discrete vocabulary just traditional markov models state label specific unigram language model emitting tokens length discrete vocabulary 
hmms role identification order train hmms role identification task assumptions inspired related 
hmm models entire document requiring segmentation document sentences pieces text 
training document modeled sequence lexical units tokens 
discrete tokens training sequences constitute discrete alphabet hmm 
separate hmm constructed role event 
deal collection greek articles describing acquisitions 
event interested roles buyer acquired acquisition rate acquisition amount 
construct different hmms role 
structure hmm set hand follows basic form different roles 
state hmm associated specific label type 
set label types involve start label type models token document type models token eof file symbol target types model tokens hand tagged target roles prefix label types model tokens target tokens background type models tokens document particular interest 
set label types attribute particular meaning state hmm confused token vocabulary model 
typical hmm structure label types shown 
hmm fully connected 
constraint allowable transitions encodes prior knowledge task aiming improve extraction performance 
example self transition state indicates role instance buyer may consist tokens 
similarly transition state state indicates role instance may consist single token 
fig 

example hmm structure 
label types associated states model start background prefix prefix target target suffix suffix 
sequence labels associated training sequence 
encodes hand tagged information roles document elements take values vocabulary label types depicted 
example label sequence 
training hmm specific role buyer tokens hand tagged role associated target tokens 
mapping states labels state sequence longer hidden baum welch algorithm needed train hmms 
state transition token emission probabilities acquired directly training data associated label sequences simply calculating ratios counts maximum likelihood estimation follows ij counts transitions state state andc counts occurrence frequency token state token emission probabilities need smoothed order avoid zero probabilities vocabulary tokens observed smoothing technique described 
state transition probabilities require smoothing due small size low connectivity model 
training phase hmms evaluated articles seen training process 
set test sequences denoted objective find state sequence label sequence extract target tokens 
uncovering hidden label sequence corresponds second issue concerning hmms described subsection achieved viterbi algorithm 
issue arises modeling approach deal unknown tokens test collection tokens exist training vocabulary deal problem added special token unknown vocabulary hmms training phase 

experiments data preprocessing purposes experiments collection greek financial articles describing acquisition events 
texts roles buyer acquired rate amount hand tagged 
mentioned buyer indicates acts buyer acquired indicates bought acquisition rate percentage bought amount amount spent buyer 
text describes single acquisition event 
text corpus preprocessed text 
engineering platform linguistic tools tokenizer part ofspeech tagger stemmer 
tokenizer identifies text tokens words symbols characterizes token type tag set encodes information token comprises upper case greek characters 
part tag set shown table 
part speech pos tagger identifies pos word token pos tag set 
addition part speech tag set includes morphological features number gender case 
part tag set shown table 
pos tagger rule constructed transformation learning method 
performance tagger greek financial texts approximately 
stemmer converts word tokens lowercase removes inflectional suffixes greek nouns adjectives 
table 
part token type subset tokenizer 
part part speech tag set pos tagger glw greek lowercase word greek uppercase word greek word capital english uppercase word english word capital period int integer nnf noun feminine singular number definite article vbd verb past tense wp relative pronoun fw foreign word result linguistic processing step new collection articles linguistic information represented part text various ways 
due sequential modeling nature traditional hmms represented linguistic features token sequence document text 
example result tokenizer new collection extra token added just token indicating type token tag set 
table shows sample sentence various data representations examined 
table 
different representations sample sentence incorporating linguistic information 
collection baseline collection type token collection pos token collection type pos token itc computer logic 
glw glw glw itc glw glw computer logic period nnf nnf fw itc vbd fw computer fw logic 
glw nnf glw nnf glw fw itc glw vbd glw computer fw fw logic period 
collection token type collection token pos collection token type pos collection type pos glw glw glw itc glw glw computer logic period nnf nnf itc fw vbd computer fw logic fw glw nnf glw glw nnf itc fw glw vbd glw computer fw logic fw period glw nnf glw glw nnf fw glw vbd glw fw fw period collection stems itc computer logic results conducted groups experiments 
group uses collections table represent linguistic information similar manner 
group contains experiments baseline collection table linguistic information 
second group contains experiments collections linguistic information token type pos represented extra tokens added just token text 
third group contains experiments collections linguistic information represented tokens attached token text underscore character depicted table 
fourth group comprises collection token text substituted corresponding type pos connected underscore character type pos 
fifth group contains collection token baseline collection substituted corresponding stem 
experiment collection involves training hmms role domain 
experimented various structures hmms collection 
model structure achieved best results majority collections shown 
conducted experiments prefix suffix target states expecting complex hmm structures capture content collections new tokens introduced achieve better results 
results justify additional complexity 
evaluation hmms done fold cross validation method 
evaluation method collection split equally sized subsets learning algorithm run times 
time pieces training tenth kept unseen data evaluation algorithm 
pieces acts evaluation set runs final result average runs 
extraction performance hmms evaluated measures hmm role recall precision accuracy 
recall measures number items certain role buyer correctly identified divided total number items specific role data 
precision measures number items certain role correctly identified divided total number items assigned role hmm 
accuracy measures number tokens certain role correctly identified divided total number tokens assigned role 
total measures experiments recall precision accuracy roles buyer acquired rate amount acquisition domain 
best results group experiments collections achieved results tables 
table 
best performance hmms groups experiments buyer acquired rate amount recall precision accuracy performance collection baseline collection buyer acquired rate amount best collection recall precision accuracy best performance collections buyer acquired rate amount best collection recall precision accuracy best performance collections buyer acquired rate amount recall precision accuracy performance collection buyer acquired rate amount recall precision accuracy performance collection comparing results table baseline results table note significant increase recall accompanied smaller decrease precision accuracy 
justified follows capitalization character token usually provides evidence name 
type token representation collection hmms learn rules form emitting high probability token target token 
number items assigned buyer acquired roles increases causing equivalent increase recall followed smaller decrease measures 
hand rate amount roles involve numerical entities 
number items assigned roles increases presence numeric token type int added just number 
learned rules case form emitting integer decimal number high probability token target token 
comparing results table results table note improvement buyer acquired amount roles performance rate role remains unaffected 
means additional part speech information included representation token type pos improves performance hmms 
true collection type pos token encoding linguistic information extra tokens causes significant deterioration precision additional part speech information beneficial 
removing information token collection result significant increase recall comparing tables significant decrease measures 
indication overgeneralization expected due generality simplicity linguistic information part speech token type 
results table show stemming improves recall hurts precision buyer acquired roles 
means reduction vocabulary stemming causes higher level generalization increases recall reduces precision 
performance rate role improves slightly measures justified emergence clearer contextual patterns role stemmed words 
clear experiments performance buyer acquired roles worse rate role experiments 
lesser extend true amount role 
reasonable explanations 
rate amount roles involve numerical entities easier detect text detecting named entities companies 
justifies high recall roles 
second difficult discriminate roles entities type companies 
result buyer companies collection detected hmms acquired vice versa 
hand rate amount different roles aren similar roles domain rate amount 
justifies low precision buyer acquired roles 
verify second explanation conducted set experiments buyer acquired tagged concept buyer acquired 
experiments conducted collections best results previous experiments table 
new results depicted table 
table 
performance hmms role buyer acquired collections best results groups experiments 
collection collection collection collection collection baseline type token token type pos type pos stems recall precision accuracy expected table consistent improvement measures recall precision accuracy obtained results buyer acquired roles table 
improvement substantial expect 
happens names collections particular role acquisition event hmms erroneously detect entities buyer acquired 
ultimate question remains unanswered representation leads best performance hmms 
results table conclude best representation buyer acquired roles collection token type pos leads significant increase measures comparison baseline collection representation collection type pos achieve best performance amount role 
rate role best representation collection stems 
note rate role deviation performance measures experiments 
happens rate role involves exclusively numerical entities percent symbol little affected different representations experiments 
hand amount role may involve alphabetic characters 

performance amount role easier influenced various representations table 
discussion examined effect linguistic pre processing training data performance hidden markov models role identification 
evaluation measures recall precision accuracy fold cross validation method order gain unbiased estimate performance 
data consisted greek articles announcing acquisition events 
data processed simple efficient linguistic analysis tools translated training data hmms various representations linguistic information represented part text sequential form 
size hmms small structure simple model parameters easily estimated training data straightforward manner 
experiments showed certain representations simple linguistic analysis improves extraction performance hmms role identification 
performance high simpler roles rate amount lower roles buyer acquired 
improvement performance gained linguistic information clearer harder roles 
difficulty identifying instances buyer acquired roles stems mainly fact correspond type entity organization insufficient linguistic information distinguishing roles 
richer linguistic processing involving syntactic analysis improve results 
supported higher performance equivalent handcrafted system 
indicative results system shown table 
manual system performs badly rate amount roles due weak performance detection numerical entities text 
better roles extensive albeit computationally expensive linguistic analysis 
results comparable reported 
table 
performance handcrafted system acquisition domain buyer acquired rate amount recall precision extraction performance hmms improved ways 
freitag mccallum implemented statistical technique called shrinkage improves token emission probabilities hmm presence sparse training data 
furthermore learning probabilistic model hmm involves learning structure model 
assumed fixed model structure carefully designed particular dataset domain 
certain structures may capture better content documents straightforwardly affecting extraction performance 
machine learning techniques learning structure hmms training examples 
acknowledgments performed context project funded ec contract ist data collected project funded greek secretariat research technology contract neo 

bikel miller schwartz weischedel 

nymble high performance learning name finder 
proceedings anlp 

brill 

transformation error driven learning natural language processing case study part speech tagging computational linguistics vol 


freitag mccallum 

information extraction hmms shrinkage 
aaai workshop machine learning information extraction pp 

aaai technical report ws 

freitag mccallum 

information extraction hmm structures learned stochastic optimization aaai pp 


androutsopoulos paliouras spyropoulos 

information extraction greek texts information management system 
internal technical report institute informatics telecommunications 

kupiec 

robust part speech tagging hidden markov model 
computer speech language 

leek 

information extraction hidden markov models master thesis uc san diego 

muc 
proceedings sixth message understanding conference morgan kaufman defense advanced research projects agency 

paliouras spyropoulos androutsopoulos 

machine learning techniques part speech tagging greek language proceedings th conference informatics ioannina greece 

paliouras androutsopoulos 
text engineering platform 
internal technical report institute informatics telecommunications 

rabiner juang 

hidden markov models 
ieee acoustics speech signal processing magazine 

rabiner 

tutorial hidden markov models selected application speech recognition 
proceedings ieee 

seymore mccallum rosenfeld 

learning hidden markov model structure information extraction 
aaai workshop machine learning information extraction pp 


stolcke omohundro 

hidden markov model induction bayesian model merging 
advances neural information processing systems volume 
morgan kaufmann 

yamron carp gillick lowe 

hidden markov model approach text segmentation event tracking 
proceedings ieee icassp 

witten bell 

zero frequency problem estimating probabilities novel events adaptive text compression 
ieee transactions information theory 
