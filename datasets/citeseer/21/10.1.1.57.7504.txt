multi layer sparse coding network learns contour coding natural images patrik hoyer hyvarinen neural networks research centre helsinki university technology box fin hut finland patrik hoyer hut fi vision research july important approach visual neuroscience considers function early visual system relates statistics natural input 
previous studies shown basic properties primary visual cortex receptive fields simple complex cells spatial organization topography cells understood efficient coding natural images 
extend framework considering responses complex cells sparsely represented higher order neural layer 
leads contour coding stopped receptive fields 
addition contour integration interpreted top inference model 
keywords natural images neural networks contours cortex independent component analysis build statistical models natural images 
hubel wiesel showed neurons mammalian primary visual cortex optimally stimulated bars edges large part visual neuroscience concerned exploring response characteristics neurons higher visual areas 
studies directly answer question neurons respond way 
sense filter incoming visual signals receptive fields simple cells 
goal neural code 
questions interesting right finding answers give deeper understanding information processing visual system give predictions neuronal receptive fields higher visual areas 
important approach answering questions consider function visual system relates properties natural images 
long hypothesized early visual system adapted input statistics attneave barlow 
adaptation thought result combined forces evolution neural learning development 
hypothesis lately gaining ground efficient coding strategies explain early processing visual sensory data including response properties neurons retina srinivasan atick redlich lateral geniculate nucleus dong atick dan olshausen field bell sejnowski van hateren van der schaaf van hateren ruderman rao ballard simoncelli schwartz krieger hoyer hyvarinen hyvarinen hoyer tailor hyvarinen hoyer :10.1.1.15.1105:10.1.1.57.3911
review see simoncelli olshausen 
previously title non negative sparse coding network learns contour coding integration natural images 
title changed revision 
consensus information theoretic arguments relevant investigating earliest parts visual system general agreement far arguments taken 
information theory understand neuronal processing higher processing hierarchy say areas inferotemporal cortex 
instance inclined think higher processes visual system expected concerned specific tasks estimation shape heading simply representing information efficiently 
early levels argument goes goal oriented number tasks need information simply concentrate representing information faithfully efficiently 
suggest emphasis placed hypothesis cortex simply seeks represent sensory data efficiently notion builds probabilistic internal model data 
change emphasis original redundancy reduction hypothesis argued barlow 
framework natural think neural networks simply transforming input signals coded representations having desired properties sparseness modeling structure sensory data 
viewed light data driven probabilistic models sense higher levels processing hierarchy earliest stages visual system 
focus large majority sensory coding reviewed simoncelli olshausen point view emphasised number researchers see 
mumford dayan hinton hinton ghahramani olshausen field rao ballard approach take 
modeling receptive fields sparse coding influential olshausen field showed classical receptive fields simple cells understood framework sparse coding 
basic idea model observed data random variables weighted sum hidden latent random variables gaussian noise added ji 
expressed compactly vector form bold letters indicating vector quantities 
words observed data pattern approximately expressed linear combination basis patterns hidden variables give mixing proportions stochastic differ observed crucial assumption sparse coding framework hidden random variables mutually independent exhibit sparseness 
sparseness property independent scale variance implies probability densities highly peaked zero heavy tails 
essentially idea single typical input pattern accurately described active significantly non zero units basis patterns needed represent data set active units changes input input 
model represented simple neural network see observed data represented activities lower input layer linear function activities hidden variables higher layer contaminated additive gaussian noise olshausen field 
observing input network calculates optimal representation sense configuration hidden variables caused observed data 
inferring latent variables short timescale goal network 
prior probability sparse boils finding maximally sparse configuration approximately generates long run goal network learn adapt generative weights basis patterns probability data maximized 
sparse latent variables course information theory tells better model data efficiently compactly potentially represent 
case brain may model important forming compact representation 
sparse coding model called noisy independent component analysis ica model hyvarinen :10.1.1.107.3613
image simple cells ij linear sparse coding neural network 
units depicted filled circles arrows represent conditional dependencies generative model units 
observing data hidden neuron activities calculated probable latent variable values generated data 
longer timescale generative weights adapted allow typical data represented sparsely 
achieved weights higher layer neurons need significantly active represent typical input patterns 
network trained data consisting patches natural images elements representing pixel gray scale values learned basis patterns resemble gabor functions simple cell classical receptive fields corresponding interpreted activations cells olshausen field bell sejnowski van hateren van der schaaf :10.1.1.57.3911
basic result extended explain spatiotemporal van hateren ruderman chromatic hoyer hyvarinen tailor binocular hoyer hyvarinen properties simple cells 
done simply training network input data consisting image sequences colour images stereo images respectively 
basic sparse coding model quite successful explaining receptive fields simple cells difficult see account behaviour complex cells 
cells just simple cells sensitive orientation spatial frequency stimulus simple cells sensitive phase stimulus 
invariance impossible describe strictly linear model 
consider instance reversing contrast polarity stimulus 
operation flip sign linear representation response typical complex cell change significant degree 
model stimulus invariance hyvarinen hoyer modified network include higher order cells pooled energies squared outputs groups simple cells 
pooling interpreted generative model higher order cells determine variances simple cells hyvarinen :10.1.1.107.3613
model depicted 
note simply adding second linear layer top useful linear transform followed linear transform linear transform gained multi layer network 
trained natural image patches adaptable weights converged simple cell receptive fields behaviour higher order units qualitatively similar complex cell responses 
behaviour due fact simple cells single group learned receptive fields similar orientation spatial frequency differing spatial phase 
extension introduced hyvarinen hoyer hyvarinen shown topography additionally emerge :10.1.1.107.3613
multi layer model important question extend models account response properties neurons higher processing hierarchy 
straightforward approach add linear layer top complex cell model 
amount assuming model activities model complex cells independent described linear combination higher order independent units 
contribution study simplified version model lower layers fixed responses model complex cells straightforward function image input 
situation depicted lower layers model estimated hyvarinen hoyer simplified version noise number hidden variables equal dimensionality data :10.1.1.15.1105
simple cells image complex cells ij extended sparse coding model hyvarinen hoyer hyvarinen :10.1.1.107.3613:10.1.1.15.1105
complex cell determines variance group simple cells 
image complex cells contour cells simple cells ij simplified hierarchical model investigated 
model complex cell responses calculated feedforward manner responses subsequently analyzed higher order sparse coding layer network 
emphasise lower layers fixed learned layers grayed 
grayed illustrate layers active learned component simplified model 
choice investigate simplified model driven factors 
model computationally significantly simpler learn set weights needs adapted 
experiments performed larger scale 
fact operation model complex cells completely specified interpretation results straightforward full model fixing lower layer structure allows simple visualization analysis results compared unrestricted model 
chosen feedforward complex cell response model allows results compared kruger sigman geisler analyzing dependencies complex cell responses 
believe analysis provided viewed preliminary investigation complex cell responses represented unrestricted multi layer model 
brief model complex cell outputs classic complex cell energy model responses input estimate linear model eq 
assuming sparse non negative show network learns contour coding natural images unsupervised fashion discuss contour integration viewed resulting top inference model 
stimulus complex cell model 
response complex cell obtained linearly filtering quadrature gabor filters squares summing 
model complex cell responses natural image patches 
patches set natural images 
responses model complex cells patches 
ellipses show orientation approximate extent individual complex cells 
brightness different ellipses indicate response strengths 
response distribution single complex cell 
solid line shows normalized histogram response single complex cell measured image patches 
comparison dotted line density absolute value gaussian random variable 
distributions normalized scale measured expected squared value random variable 
methods model complex cells modeled complex cell responses simple widely energy model pollen adelson bergen morrone burr detecting spatially localized oriented fourier energy static monocular achromatic images 
response model complex cell calculated summing squared responses pair quadrature gabor filters 
details see appendix 
depicted 
simplicity interpretation computational reasons restricted analysis single spatial scale cells placed rectangular grid differently oriented cells location 
illustrate behaviour model complex cells showing responses natural image patches 
shows response distribution ensemble image patches single cell 
distribution exhibits high peak zero heavy tail consistent notion complex cell responses natural images sparse hyvarinen hoyer :10.1.1.15.1105
sparse coding complex cell responses 
complex cell activity pattern represented linear combination basis patterns goal find basis patterns coefficients sparse possible meaning input patterns needed represent pattern accurately 
cf 
equation 
sparse coding complex cell responses having sampled large number activation patterns shown trained linear sparse coding network data 
words estimated parameters model represented equation depicted network 
details learning procedure appendix 
complex cell activation pattern gives data vector element representing firing rate neuron 
represents response higher order neuron receptive field closely related corresponding goal find basis patterns typical input patterns described accurately significantly active higher order neurons 
kind sparse coding complex cell responses illustrated 
input data complex cell responses go negative natural require generative representation non negative 
restricted non negative values 
arguments non negative representations previously lee seung 
contrast approach emphasize importance sparseness addition reconstruction 
emphasis sparseness previously forcefully argued barlow field 
combine sparse coding constraint non negativity single model 
note assumption arbitary follow properties complex cell responses sparse non negative 
results properties learned representation simulated complex cell responses natural images input data see estimated nonnegative sparse coding model obtaining basis activity patterns 
representative subset estimated basis patterns shown 
note basis patterns consist variable number active complex cells arranged 
intuitive sense collinearity strong feature visual world kruger sigman geisler 
addition analyzing images terms smooth contours supported evidence psychophysics field sagi physiology kapadia kapadia incorporated models contour integration see 
grossberg li neumann sepp 
knowledge model learn type representation statistics natural images 
easy understand basis patterns consist collinear complex cell activity patterns patterns typical data set sparsely coded long contour represented higher level units 
necessity different length basis patterns comes fact long basis patterns simply code short curved contours short basis patterns inefficient representing long straight contours 
kind contour coding illustrated 
allowing take negative values imply model assign non zero probability density negative data non negative sense require basis coefficients 
experiments noted non negativity constraint strictly necessary tended positive constraint 
constraint crucial 
representative set basis functions learned basis 
majority units code simultaneous activation collinear complex cells indicating smooth contour image 
contour coding model 
hypothetical contour image left transformed complex cell responses middle 
responses sparsely represented higher order units right types shown 
characterizing population basis patterns 
locations basis patterns 
dot indicates central position basis pattern sampling window 
histogram basis pattern orientation 
distribution pattern lengths units relative width sampling window 
obvious higher order representation necessarily code contours 
multi layer mechanisms similar proposed context texture segregation malik perona 
priori expected texture boundary detectors emerge model 
results indicate contour coding sparse coding sense basic texture segregation 
note single spatial scale texture segregation efficient spatial frequencies 
characterize population basis patterns described pattern terms parameters location orientation length width see appendix details 
investigated basis vectors distributed parameter space 
main results shown 
note positions basis pattern distributed relatively evenly inside window fig 

fig 
shows cardinal orientations represented slightly better oblique ones 
due partly similar bias natural images coppola 
possibility rectangular complex cell sampling array biases results distances collinear complex cells longer oblique cardinal orientations 
interesting result distribution pattern lengths fig 

histogram clear short basis patterns abundant longer ones progressively scarce 
reminiscent spatial frequency distribution wavelet filter set number filters needs increase quadratically spatial frequency order yield scale invariant representation olshausen field 
important question results explained simply resulting linear correlations complex cell outputs opposed higher order statistics 
show covariance structure data 
top pattern shows covariance unit brightest ellipse cardinal orientation units 
bottom patterns gives corresponding pattern complex cell oblique orientation 
strongest correlations collinear units parallel units show relatively strong correlations 
observations compatible results previous studies kruger sigman geisler 
note clear circularity effect sigman geisler data 
fact learned basis patterns show stronger collinearity compared parallel structure data covariance indication covariance structure 
stronger evidence comes fact learned patterns highly varying lengths 
non trivial finding explained simply linear correlations data 
fundamental principle feature learned decomposition sparseness 
fact omit sparseness objective simply optimize reconstruction non negativity constraint perform non negative matrix factorization squared error objective algorithm lee seung get collinearity significant variation basis pattern lengths 
illustration basis patterns shown width parameter vary practically basis patterns consisted row active complex cells 
length parameter practical purposes equal pattern elongation length width 
various additional experiments 
covariance structure complex cell responses 
brightness ellipse shows covariance complex cell represented brightest ellipse 
top covariance units complex cell cardinal orientation 
bottom covariance units cell exhibiting oblique orientation 
representative basis patterns resulting applying non negative matrix factorization data 
equivalent model sparseness objective 
see main text discussion 
typical basis patterns model learned image patches consisting white noise 

return length distribution section 
second question dependencies observed model complex cells significant degree due particular choice forward transform complex cell model chosen sampling grid natural image statistics 
investigate fed network image patches consisting gaussian white noise examined learned basis patterns small subset shown 
basis patterns exhibited low degree collinearity localized inside patch 
shows clearly results part consequence input statistics simply due particular complex cell transform chosen 
nonlinear responses higher order neurons network linear latent variables data inferred nonlinear function data due noise basis olshausen field 
words contour coding neurons respond complex cell activity patterns nonlinear fashion 
particular competition neurons olshausen field respond better competing units representing stimulus 
prominent feature representation existence different length patterns leads units selective contour length addition tuned position orientation 
words units representing long contours respond short ones units coding short contours exhibit stopping hubel wiesel hubel wiesel 
illustrate nonlinear transform complex cell activities higher order activities linear approximation details see appendix 
optimal approximating linear filters shown units basis patterns depicted 
note units representing short contour segments tend inhibitory regions ends receptive fields illustrating stopping effect 
hand units code longer contours inhibitory weights complex cells positioned contour wrong orientation 
enhances selectivity units don respond contours partly overlap receptive field 
nonlinear effects seen directly showing length tuning curves 
plot shows response corresponding higher order neuron varies length stimulus stimulus parameters held optimal values 
length stimulus relative length sampling window horizontal axis note logarithmic scale corresponding response plotted bit difficult see negative weights dark ellipses partly masked strong positive weights bright ellipses 
basis patterns estimated basis 
optimal approximating linear filters units 
filters minimize mean squared error linear response followed optimal activations 
length tuning curves units 
horizontal axis gives length stimulus logarithmic scale relative size sampling window vertical axis denotes response strength normalized maximum 
vertical axis 
notice response stopped units starts decrease stimulus length increases past optimal value eventually falling zero 
hand response unit coding long contours decline significant degree 
results show model higher level units extra classical properties clearly distinct standard complex cells 
noted higher order neurons represent long contours bear similarities collector units proposed psychophysical levi 
units thought integrate responses smaller collinear filters give robust estimate global orientation achieved elongated linear mechanisms 
contour integration viewed top inference model central question visual neuroscience concerns computational role feedback connections 
suggested purpose feedback information higher order units modulate lower level outputs selectively enhance responses consistent broader visual context 
hierarchical generative models naturally understood part inference process finding configuration network requires integrating bottom evidence top priors layer network mumford hinton ghahramani 
kind feedback inference useful 
cases multiple conflicting interpretations stimulus lowest level top feedback needed resolve conflicts 
essence feedback inference computes interpretation scene knill richards combining bottom sensory information top priors 
principle long contours represented long basis vectors level simple cells 
representation higher order contour coding cells advantage sensitive small curvature departures strict collinearity 
small curvature completely change response elongated linear filter simple cell change representation higher level assuming curvature small line stays inside receptive fields complex cells 
higher order contour cells give robust representation contours 
noise reduction contour integration 
image patches containing random locations orientations 
top patch collinear set bottom patch random orientations 
response model complex cells images 
response complex cells feedback noise reduction learned network model 
note reduction noise left activations collinear stimuli suppressed activity fit learned sparse coding model 
particular suggest contour integration viewed natural consequence inference full hierarchical model 
basically argument goes follows collinear complex cells active activate higher order contour coding unit 
activation unit evidence contour location evidence strengthen responses complex cells lying contour especially bottom input relatively weak 
simplified network model responses complex cells straightforward function image input energy model 
simulate full network inference process described 
unrestricted network top connections contour coding units complex cells seek adjust complex cell responses predicted contour units note essentially equation noise term dropped 
simplified model seen reduction noise bottom signals 
note noise context refers activity consistent learned statistical model neural dark noise 
hyvarinen lewicki olshausen simoncelli adelson essentially suppresses responses typical training data retaining responses fit learned statistical model :10.1.1.29.5390:10.1.1.50.4731
show basic example feedback noise reduction model results emphasis smooth contours 
generated image patches placing gabor filters random locations orientations 
case collinear alignment consecutive random orientations 
image patches shown 
processed model complex cells processed natural image patches main experiments 
result shown 
calculated contour coding unit activities plotted noise reduced complex cell activity 
note noise reduction step responses spurious edges emphasizing responses part collinear arrangement 
response enhancement contours defining characteristic proposed computational models contour integration see example grossberg li neumann sepp 
comparing denoised responses observe collinear contextual interactions model 
response central gabor stronger collinear upper row random orientations bottom row fall outside receptive field central neuron 
type contextual interactions subject study see 
kapadia kapadia hypothesized related contour integration relation certain 
experiment exceedingly simple suggests contour integration natural result inference noise reduction network 
experiments needed examine noise reduction mechanism perform contours natural images 
left 
discussion related authors kruger sigman geisler studied mutual dependencies complex cell responses natural images 
main result studies complex cell responses mutually independent particular correlated complex cells arranged parallel common circle kruger sigman geisler 
investigations considered pairwise statistics responses higher order statistics 
previous area natural image statistics know higher order statistics fundamental importance example explaining oriented receptive fields field olshausen field krieger 
probably important consider linear correlations analyzing dependencies complex cell outputs 
explained section finding sparse code complex cell responses employs basis patterns varying lengths clear indication covariance structure learnt 
difference previous generative model collinearity dominant property parallel circular basis patterns 
lack circular basis vectors course caused limited sampling space orientation model complex cells represent curves equally 
preliminary larger scale experiments tighter complex cell sampling space orientation indicate case 
possible collinearity special sense straight contours coded units curved contours best represented standard complex cells possibly bound horizontal connections 
curved contours really represented hierarchical layer representation curved contours described activation suitable combination stopped units study 
approach related predictive coding model rao ballard 
proposed multi layer network higher level units provide predictive feedback lower units 
lower level neurons send forward residuals observed data stimulus prediction 
fixed weights network resembles full network model discussed section generative model different due difference form nonlinearities 
main advantage approach results interpreted intuitively terms contour coding units 
major difference lies interpreted extra classical effects stopping 
proposed stopping due cells sending residuals top predictions bottom input stopping cell stopped responding stimulus length increased predicted residual 
interpret stopping property arising competition represent stimulus neurons representing different length stimuli 
study related park 

show receptive fields sensitive translation contraction expansion rotation observed area emerge learning sparse code optical flow fields 
inputs model vector fields simulated neuron outputs shows quite complex neuronal response properties learned visual input statistics completely unsupervised manner 
possible neural implementation far said hypothesize kind representation implemented biological visual system 
issues need considered 
note nonlinear optimization technique calculate optimal higher level activities input data said real neural network perform operation 
basis arguments relating processing speed thorpe expected feedforward processing tuned give fairly estimate optimal responses 
processing complemented influences recurrent connections 
concrete suggestion inference performed network see olshausen field 
note contour integration model explicitly handled feedback connections higher order units rule role horizontal connections complex cells 
contrary neural implementation probably quite useful implement part inference noise reduction horizontal connections 
contour integration performed connections viewed inference noise reduction possible general purpose horizontal connections visual cortex 
issue left untouched question contour coding neurons reside cortex 
nave guess area 
strongly reciprocally connected important property contour units prevalent particularly pale stripes hubel livingstone 
stopping neurons hubel wiesel quite possible part hypothesized contour coding neurons situated 
hand possible units exist horizontal connections complex cells utilized perform relevant computations 
turned case contribution mainly illustrate existing statistical dependencies hint dependencies related contour integration 
extended sparse coding framework outputs model complex cells natural image input 
provides way predicting understanding neuronal response properties higher visual processing hierarchy 
framework suggests view feedback pathways essential part inference process performing noise reduction lower level activities 
specifically shown contour coding stopped receptive fields understood sparse coding framework 
furthermore contour integration framework interpreted natural result inference hierarchical network 
appendix facilitate reproduction results provide complete matlab code experiments www cis hut fi tar gz 
sample image patches pixels sampled natural images available www www cis hut fi projects ica data images 
output model complex cell fed image patch 
odd symmetric respectively gabor filters centered orientation gaussian envelope aspect ratio spatial frequency sinusoid chosen give main excitatory inhibitory regions accordance measurements receptive fields 
total complex cells dimensionality data model eq 
estimated maximizing joint log posterior latent variables model parameters data 
equivalent minimizing 
denotes matrix containing columns represents th data vector vector containing latent variables corresponding th data vector 
linear activation penalty term comes assuming latent variables exponentially distributed exp quadratic term derives gaussian prior noise 
constant defines tradeoff representation error sparseness obtained directly noise level selected manually assumed model 
objective minimized standard gradient descent restricted non negative respect short run respect longer timescale olshausen field 
additional constraint latent variable assumed contribute equally data basis patterns parametrized parameters location orientation length width 
patterns approximated gaussian cluster activity complex cells orientation specified position having standard deviations length width 
parameters chosen minimize summed squared difference basis patterns parametrized patterns 
yielded excellent fit cases 
weights shown obtained finding optimal data points minimizing objective eq 
iteratively seeking weights minimize rect rect denotes half rectification 
noise reduction done data vector finding latent variable configuration minimizing respect denoised data vector 
adelson bergen 

spatiotemporal energy models perception motion 
opt 
soc 
am 

atick redlich 

retina know natural scenes 
neural computation 
attneave 

informational aspects visual perception 
psychological review 
barlow 

possible principles underlying transformations sensory messages 
editor sensory communication pages 
mit press 
barlow 

single units sensation neuron doctrine perceptual psychology 
perception 
barlow 

exploitation regularities environment brain 
behavioral brain sciences 
barlow 

redundancy reduction revisited 
network computation neural systems 
bell sejnowski 

independent components natural scenes edge filters 
vision research 
coppola mccoy 

distribution oriented contours real world 
proceedings national academy science usa 
dan atick reid 

efficient coding natural lateral geniculate nucleus experimental test computational theory 
journal neuroscience 
dayan hinton neal zemel 

helmholtz machine 
neural computation 
freeman 

spatiotemporal organization simple cell receptive fields cat striate cortex 
general characteristics development 
journal neurophysiology 
viewpoint model estimation non negativity constraint important allows accurate model prior densities main feature density sharp peak zero 
ica estimation methods assume sharp peaks coincides mean distribution case due strong asymmetry nonnegative distribution 
conventional ica algorithms hyvarinen able successfully estimate components real life noisy data characterized nonnegativity :10.1.1.50.4731
note parra 
considered constraint non negativity constraint basis vectors latent variables dong atick 

temporal decorrelation theory lagged nonlagged responses lateral geniculate nucleus 
network computation neural systems 
field 

goal sensory coding 
neural computation 
field hayes hess 

contour integration human visual system evidence local association field 
vision research 
geisler perry super 

edge occurence natural images predicts contour grouping performance 
vision research 
grossberg 

neural dynamics perceptual grouping textures boundaries emergent segmentations 
perception psychophysics 
hinton dayan frey neal 

wake sleep algorithm unsupervised neural networks 
science 
hinton ghahramani 

generative models discovering sparse distributed representations 
phil 
trans 
soc 
lond 

hoyer hyvarinen 

independent component analysis applied feature extraction colour stereo images 
network computation neural systems 
hubel wiesel 

receptive fields binocular interaction functional architecture cat visual cortex 
journal physiology 
hubel wiesel 

receptive fields functional architecture non striate visual areas cat 
journal neurophysiology 
hubel wiesel 

receptive fields functional architecture monkey striate cortex 
journal physiology 
hubel livingstone 

segregation form color stereopsis primate area 
journal neuroscience 
james payne girard 

cortical feedback improves discrimination background neurons 
nature 
hyvarinen 

fast robust fixed point algorithms independent component analysis 
ieee trans 
neural networks 
hyvarinen 

sparse code shrinkage denoising nongaussian data maximum likelihood estimation 
neural computation 
hyvarinen hoyer 

emergence phase shift invariant features decomposition natural images independent feature subspaces 
neural computation 
hyvarinen hoyer 

layer sparse coding model learns simple complex cell receptive fields topography natural images 
vision research 
hyvarinen hoyer 

topographic independent component analysis 
neural computation 
hyvarinen karhunen oja 

independent component analysis 
wiley interscience 
kapadia ito gilbert 

improvement visual sensitivity changes local context parallel studies human observers alert monkeys 
neuron 
kapadia gilbert 

spatial distribution contextual interactions primary visual cortex visual perception 
journal neurophysiology 
knill richards editors 
perception bayesian inference 
cambridge university press 
kruger 

collinearity parallelism statistically significant second order relations complex cell responses 
neural processing letters 


neurophysiology ground segregation primary visual cortex 
journal neuroscience 
lee seung 

learning parts objects non negative matrix factorization 
nature 
lee seung 

algorithms non negative matrix factorization 
advances neural information processing proc 
nips 
mit press 
lewicki olshausen 

probabilistic framework adaption comparison image codes 
opt 
soc 
am 
optics image science vision 
li 

pre attentive segmentation primary visual cortex 
spatial vision 
malik perona 

preattentive texture discrimination early vision mechanisms 
journal optical society america 
morrone burr 

feature detection human vision phase dependent energy model 
proc 
royal soc 
london ser 



units second stage orientational filters 
higher order processing visual system 
foundation symposium pages 
john wiley chichester mumford 

neuronal architectures pattern theoretic problems 
koch davis editors large scale neuronal theories brain 
mit press cambridge ma 
levi 

spatial properties filters underlying vernier acuity revealed masking evidence mechanisms 
vision research 
neumann sepp 

recurrent interaction early visual boundary processing 
biological cybernetics 
olshausen field 

emergence simple cell receptive field properties learning sparse code natural images 
nature 
olshausen field 

sparse coding overcomplete basis set strategy employed 
vision research 


positive matrix factorization non negative factor model optimal utilization error estimates data values 

park jabri lee sejnowski 

independent components optical flows receptive fields 
proc 
int 
workshop independent component analysis blind signal separation ica pages 
parra spence muller 

unmixing hyperspectral data 
advances neural information processing proc 
nips pages 
mit press 


collinear stimuli regulate visual responses depending cell contrast threshold 
nature 
sagi 

lateral interactions spatial channels suppression facilitation revealed lateral masking experiments 
vision research 
pollen 

visual cortical neurons localized spatial frequency filters 
ieee trans 
systems man cybernetics 
rao ballard 

predictive coding visual cortex functional interpretation extra classical receptive field effects 
nature neuroscience 
sigman gilbert 

common circle natural scenes gestalt rules 
proceedings national academy science usa 
simoncelli adelson 

noise removal bayesian wavelet coring 
proc 
third ieee int 
conf 
image processing pages lausanne switzerland 
simoncelli olshausen 

natural image statistics neural representation 
annual review neuroscience 
simoncelli schwartz 

modeling surround suppression neurons normalization model 
advances neural information processing systems pages 
mit press 


stages systems visual processing 
spatial vision 
srinivasan 

predictive coding fresh view inhibition retina 
proc 
royal society ser 

tailor finkel buchsbaum 

color opponent receptive fields derived independent component analysis natural images 
vision research 
thorpe 

speed processing human visual system 
nature 
van hateren ruderman 

independent component analysis natural image sequences yields spatiotemporal filters similar simple cells primary visual cortex 
proc 
royal society ser 

van hateren van der schaaf 

independent component filters natural images compared simple cells primary visual cortex 
proc 
royal society ser 

lee sejnowski 

chromatic structure natural scenes 
journal optical society america 
krieger 

nonlinear neurons higher order statistics new approaches human vision electronic image processing 
proc 
spie 
authors wish bruno olshausen helpful discussions providing source code code 
addition due mika li valpola useful suggestions valuable comments manuscript jaakko performing additional experiments erkki oja stimulating environment 
grateful anonymous referees pointed weaknesses draft manuscript markedly helped improve quality 
funded helsinki graduate school computer science engineering 
funded academy finland 

