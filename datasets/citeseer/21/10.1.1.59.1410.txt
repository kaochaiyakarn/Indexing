exploiting belief bounds practical pomdps personal assistant agents pradeep rajiv maheswaran milind tambe department computer science university southern california los angeles ca tambe usc edu agents agent teams deployed assist humans face challenges monitoring state key processes environment including state human users making periodic decisions monitoring 
pomdps appear suited enable agents address challenges uncertain environment cost actions optimal policy generation pomdps computationally expensive 
introduces key techniques speedup pomdp policy generation exploit notion progress dynamics personal assistant domains 
policy computation restricted belief space polytope remains reachable progress structure domain 
introduce new algorithms particularly applying lagrangian methods compute bounded belief space support polynomial time 
techniques complementary existing exact approximate pomdp policy generation algorithms 
illustrate enhancing fastest existing algorithms exact pomdp policy generation 
order magnitude speedups demonstrate utility techniques facilitating deployment pomdps agents assisting human users 
categories subject descriptors artificial intelligence distributed artificial intelligence multi agent systems general terms algorithms keywords task allocation meeting rescheduling partially observable markov decision process pomdp 
research focused individual agents agent teams assist humans offices home medical care permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage copies bear notice full citation page 
copy republish post servers redistribute lists requires prior specific permission fee 
aamas july utrecht netherlands 
copyright acm 
spheres daily activities 
agents monitor evolution process state time including human agents deployed assist periodic decisions monitoring 
example office environments agent assistants may monitor location users transit decisions delaying canceling meetings asking users information 
similarly assisting caring elderly therapy planning agents may monitor users states plans periodic decisions sending reminders 
unfortunately agents henceforth referred personal assistant agents monitor decisions despite significant uncertainty observations true state world may known explicitly actions outcome agents actions may non deterministic 
furthermore actions costs delaying meeting repercussions attendees 
researchers turned decision theoretic frameworks reason costs benefits uncertainty 
research focused markov decision processes mdps ignoring observational uncertainty domains potentially degrading agent performance significantly requiring unrealistic assumptions observational abilities 
pomdps partially observable markov decision processes address uncertainty long run times generating optimal policies pomdps remains significant hurdle 
recognizing run time barrier pomdp usage previous pomdps encouraging progress approaches 
exact approach finds optimal solution 
despite advances exact algorithms remain computationally expensive currently scale problems interest paa domains 
second approximate approach solution quality sacrificed speed :10.1.1.29.1347
unfortunately approximate algorithms provide loose quality guarantees solutions guarantees crucial inhabit human environments 
aims practically apply pomdps paa domains introducing novel speedup techniques particularly suitable settings 
key insight monitoring users processes time large shifting parts belief space pomdps regions uncertainty remain unreachable 
focus policy computation reachable polytope changes dynamically 
instance consider paa monitoring user driving meeting 
knowledge user current location reachable belief region bounded maximum probability user different locations time step defined transition function 
similarly pomdp decisions minutes agent exploit fact zero probability going world state ime pm world state ime pm 
current pomdp algorithms typically fail exploit belief region reachability properties 
pomdp algorithms restrict belief regions fail dynamically 
techniques exploiting belief region reachability exploit key domain characteristics states reachable decision epoch limitations physical processes progression time ii observations obtainable states reachable iii maximum probability reaching specific states tightly bounded 
introduce polynomial time techniques lagrangian analysis compute tight bounds belief state probabilities 
techniques complementary existing exact approximate pomdp algorithms 
enhance state art exact pomdp algorithms delivering order magnitude speedup different paa domains 

motivating personal assistant agent paa domains motivating examples teams software assist human users office setting 
meeting rescheduling problem mrp implemented electric system 
large scale operationalized system agents monitored location users decisions delaying meeting user projected late ii asking user information plans attend meeting iii canceling meeting iv waiting 
agent relied mdps arrive decisions actions asking non deterministic outcomes user may may respond decisions delaying costs 
mdp state represented user location meeting location time meeting user home meeting usc minutes policy mapped states actions 
unfortunately observational uncertainty user location ignored computing policy 
second key example task management problem tmp domain 
domain set dependent tasks performed human users users deadline 
agents monitor progress humans reallocation decisions 
lines connecting agents users indicate communication links 
illustration reallocation scenario suppose assigned respectively capabilities 
observed progressing slowly may may need reallocate ensure tasks finish deadline 
may reallocate original task nearing completion known capable 
progressing slowly may reallocated despite potential loss capability 
pomdps provide framework analyze obtain policies domains mrp tmp 
tmp pomdp policy take account possibly uneven progress different users users may progress deadline bulk closer deadline 
contrast instantaneous decision maker take account dynamics progress 
instance consider tmp scenario levels task progress decision points comm 
structure task dependency deadline 
observations levels task progress time moves forward single steps 
transition uncertainty implies irregular task progress observation uncertainty implies agent may observe progress instance 
despite uncertainty observing task progress paa needs choose waiting asking user info reallocate 
pomdp policy tree takes account uncertainty observations costs decisions maps observations actions scenario shown nodes actions links observations 
complex domains additional actions delaying deadlines cascading effects actions require careful planning afforded pomdp policy generation 
scenarios tmp investigated section 




partial sample policy tmp 
pomdps generalized incre mental pruning pomdp represented tuple finite set states finite set actions finite set observations provides probability transitioning state action probability observing action reaching reward function 
belief state probability distribution set states value function belief state defined maxa bt 
currently efficient exact algorithms pomdps value iteration algorithms specifically gip 
dynamic programming algorithms iteration value function represented minimal set dominant vectors called parsimonious set 
parsimonious set time vt generate parsimonious set time vt follows notation similar 
sp vi vt 
rune 
rune rune va va 
vt rune va rune call executes linear program lp recognized computationally expensive phase generation parsimonious sets 
approach effectively translates obtaining speedups reducing quantity calls 

dynamic belief supports approach consists key techniques dynamic state spaces ds ii dynamic observation sets iii dynamic belief supports db 
ideas may enhance existing pomdp algorithms gip 
key intuition personal assistant domains progress implies dynamically changing polytope belief states remains reachable time policy computation speeded computing parsimonious set just polytope 
speedups due elimination policies dominant regions outside polytope 
ds provides initial bound polytope exploits bound limit space possible vectors iteration 
db captures ds provides tighter bounds reachable belief states polynomial time technique obtained lagrangian analysis 
techniques alter relevant parsimonious set reachable belief states yield optimal solution reachable belief states 
resulting algorithms ds db applied enhance gip shown algorithm functions get bound gip main additions significant updates gip functions gip descriptions follows :10.1.1.10.4797
discuss key enhancements algorithm subsection 
enhancements exploit dynamics transitions time applicable finite horizon problems 
dynamic state spaces ds natural method represent user state tmp consisting spatial element tmp capturing progress task temporal element capturing stage decision 
transition matrix static function state 
approach adjustable autonomy problem addressed mdps 
note kinds domains reach states state 
example scenario section limits tasks progress advance progress level time step know know 
implies state space point time represented compactly dynamic fashion 
algorithm gip func pomdp solve st ot max vt gip vt dp update vt func dp update ot st st st rt st ot st stp st st st rune va rune vt rune va return vt func point dominate st st st st return true return false func lp dominate lp vars st st st lp max subject st stb st st max st st return return nil func best max inf max max lex max return func prune element point dominate true lp dominate nil best return func gip st set starting states st st max st st add st reachable states get relevant obs st get constraints st max st maxc get bound st return st max func get bound st constraint ymin mins constraint constraint ymax maxs constraint constraint int get intersect sorted constraint ymin ymax int sort constraint constraint st numer denom ascending order find corresponding state bound st bound st numer bound st constraint st denom bound st constraint st bound st numer constraint st denom constraint st break numer denom numer denom max return numer denom require transition matrix reward function dynamic 
knowledge initial belief space possible levels task progress show obtain dynamic state spaces representation affect optimality pomdp solution 
length finite horizon decision process 
set possible states occupied process 
time st denote set possible states occur time 
reachable belief state bt st 
obtain st 
st st tively know set follows st st tt belief probability particular state time starting belief vector time bt action observation expressed follows ot bt tt st bt st st st tt st st bt st ot st st st implies belief vector bt support st st bt bt support st st generated 
model process migrates dynamic state spaces st indexed time accurately stage decision process opposed transitioning static global state set proposition 
replace static state space dynamic state spaces st generated dynamic transition matrices dynamic reward functions finite horizon pomdp affecting optimality solution obtained value function methods 
proof 
pt denote set policies available time denote value policy time denote value optimal policy time bl maxp bl si 
rl rl re ward function time action prescribed policy bl sl bl maxp bl bl sl vl vl si sl 
calculating value function time bl maxp pl bl vl si rl tl pl policy subtree policy tree pl observing initial action 
bl sl vl bl maxp bl bl sl vl vl si sl 
applying reasoning inductively show need st st st furthermore st st st rt st tt st st st st 
need st st st 
value functions expressed beliefs dynamic state spaces st identical expected rewards advantage method generating set value vectors dominant underlying belief point parsimonious set particular iteration eliminate vectors dominant belief supports reachable 
reduces set possible policies need considered iteration 
line gip function dp update function algorithm provide algorithm finding dynamic states 
dynamic observation spaces note domains certain observations obtained certain states 
consequently dynamic state spaces imply observations capable obtained particular time dynamic 
instance consider tmp scenario section conditions transition probabilities subsection 
dynamic state space limits progress levels able get observations regardless action take time 
show obtain dynamic observation sets prove affect value iteration process 
proposition 
dynamic state spaces st replace static observation space dynamic observation spaces st finite horizon pomdp affecting optimality solution obtained value function methods 
proof 
subsection rewrite st rt st tt st st st st st st set complement 
dynamic observations second part sum goes zero 
implies observations relevant value strategy time creating policy trees subtrees necessary reduces set policies vectors generated pruning reduces number lp calls pruning 
consistency index observation probability matrix time depends dynamic state 
line gip function dp update function algorithm provide procedure utilizing 
dynamic belief spaces db introducing dynamic state spaces attempting accurately model support reachable beliefs occur 
process precise information initial belief distribution transition observation probabilities bound belief dimensions positive support 
example know initial belief regarding task progress probability rest probability mass find maximum probability stage dynamic transition matrix 
outline polynomial time procedure obtain bounds belief support 
bt st space bt bt 
exists initial belief vector action observation sequence length applying standard belief update rule get belief vector bt captured set bt 
bt st min st bt ot bt bt min st bt st max st bt ot bt bt max st st bt ot st st st ot st st tt st st bt st st st tt st st bt st bt min max min max bt bt 
show max st similarly min st generated polynomial time procedure deduced lagrangian methods 
action observation express problem max bt bt st st bt bt ot st tt st st ot st tt st st 
rewrite problem terms new variables follows min xi xi max si xi bmax si ensure existence feasible solution 
expressing problem lagrangian xi xi xi ixi kkt conditions imply xk xk xk xk dk ck dk ck xk dk ck 
identical conditions non negative state lowest value dk ck receive maximal allocation assuming xk entire allocation 
reasoning recursively see extremal point candidate solution values components xk constructed giving weight possible components order prescribed zk ck 
value construct solution iteratively giving weight possible violating equality constraint component bound lowest zk 
example illustrate method introduced 
size state space values expression dk ck different states 
xk identical values need equal deciding allocations xk 
xk case values reduced reduction happens third equation xk subtraction non negative variable 
imperative smaller values increase 
observed equations particular values increased case xk xk non negative variable equation full allocation smaller values dk ck value state increased assigning 
value state increase gets full assignment 
value state remain value state decrease gets zero allocation 
reasoning making values equal states allocation bounds done 
question finding maximum value yields consistent solution 
note value attempting maximize bound ymax maxi ci di ymin mini ci di 
note component zk describes line support ymin ymax 
find set points set lines described zk intersect 
intersections points 
partition support ymin ymax disjoint intervals intersection points yielding regions 
region consistent ordering zk obtained polynomial time 
illustration seen 
region furthest right real line create candidate solution implied ordering zk region calculate value candidate solution 
obtained value fall region solution inconsistent move region immediately left 
obtained value fall region candidate extremal point yields highest possible value solution problem 
technique dynamically propagate forward bounds feasible belief states 
line gip function algorithm provide procedure db 
get constraints function line gives set vectors state time action observation 
min max partition procedure solving belief maximization lagrangian lagrangian method introduced works general case special cases simpler methods feasible obtain belief bounds 
instance belief maximization equation max si equal states si easily proved maximum value equal maxk ck dk 
special case require complexity lagrangian method solved log 
maximum possible value belief probability previous stage equal maxk ck dk serve bound exact maximum 
simple improvement method assigning xk maximum value sum order ck dk 
observed example method doesn yield maximum 
max xi example bounds allocated order ratios coefficients value expression exists allocations gives value 
dynamic beliefs increase costs pruning adding constraints 
gain looking dominant vectors smaller support reduces cardinality parsimonious set leaving fewer vectors consider iteration 

experimental results experiments conducted explained section 
agent uses pomdp decision making domains 
enhancements ds dynamic states dynamic observations db dynamic beliefs implemented gip enhancement gip 
experiments compare performance run time gip enhancements gip 
domains ran problems methods gip ds gip enhancements implemented anthony cassandra pomdp solver pomdp org pomdp code index shtml code available implemented anthony cassandra pomdp solver machine specs experiments intel xeon ghz processor gb ram linux redhat gip gip ds 
problem pre specified run time upper limit seconds 
experiments sizes problem instances tested follows 
tmp domain number states actions observations smallest problem respectively largest problem numbers respectively 
mrp domain number states actions observations smallest problem respectively largest problem numbers respectively 
results tmp domain 
experimental setup consisted problems increasing complexity 
graphs axis denotes problem name axis denotes run time problem 
gip finished time limit problem shown 
ds gip provides fold speedup problem fold speedup problems actual speedup expect larger seen due cutoff 
gip gip run time ds provides comparisons enhancements gip 
problems complex dominates enhancements providing approximately fold speedup ds 
gip terminate time limit shown 
key point show ds gip provides fold speedup cut ds faster gip 
reason providing results enhancements gip 
pomdp policy dominates mdp policy presents results mrp domain 
experimental setup mrp consisted set problems 
show results gip finish cutoff problems 
gip run times identical ds gip 
gip provides approximately fold speedup ds 
ds comparable methods fails finish cutoff 
domains provide similar gip dominates techniques fold speedup gip cases dominance significant larger problems 
provides quantitative reason pomdps mdps policy generation paa domains 
mdps obtaining policies suffer handling observational uncertainty 
emphasizes point quantitatively comparing expected values optimal comparison gip gip ds gip comparison ds ds gip gip gip ds ds gip comparison mrp results ds ds gip gip tmp ds gip gives orders magnitude speedup gip gip dominates enhancements gip ds gip dominates ds mrp gip dominates policies generated mdp pomdp small tmp problem 
mdp policy evaluated presence uncertainty assuming state point policy execution highest belief probability belief state point 
modified mdp policy mdp policy modified take safe action ask user high uncertainty belief state 
axis denotes belief points states axis denotes expected value policy 
expected value pomdp policy dominates mdp policy approximately problem expected values policies vary value choosing wait action highest reward pomdp policy 
significant quality loss incurred mdp policy 

related discussed related section 
discussed techniques solving pomdps categorized exact approximate 
gip exact algorithms enhanced 
ways compactly representing dynamics domain 
compact representations advantages terms speedups 
exact algorithms attempt exploit domain specific properties speedup pomdps 
instance presents hybrid framework combines mdps pomdps take advantage perfectly partially observable components model 
focus reachable belief spaces analysis capture dynamic changes belief space reachability ii analysis limited factored pomdps iii speedup measurements shown 
contrasts focuses dynamic changes belief space reachability application flat factored state pomdps 
approximate algorithms faster exact algorithms cost solution quality 
significant amount area point grid policy search approaches dominate algorithms 
approaches solve larger problems provide loose quality guarantees solution 
critical quality guarantees paa domains agent gain trust human user 
point value iteration pbvi provides quality guarantees obtain results needs increase sampling consequently increasing run time 
approach works big problems dimensionality reduction technique 
applies pca improvement principal component analysis set belief vectors obtain low dimensional representation original state space 
provides reduction dimension state space doesn provide guarantees quality solutions 
techniques benefit approximate algorithms pbvi benefit technique 

summary provides techniques application pomdps personal assistant agents reality 
particular provide key techniques speedup pomdp policy generation exploit key properties paa domains 
key insight initial possibly uncertain starting set states agent needs generate policy limited range dynamically shifting belief states 
techniques propose complementary existing exact approximate pomdp policy generation algorithms 
illustrate technique enhancing gip efficient exact algorithms pomdp policy generation obtain orders magnitude speedup policy generation 
provide detailed algorithm illustrating enhancements algorithm proofs correctness techniques 
techniques facilitate agents utilizing pomdps policies assisting human users 

material supported darpa department interior nbc acquisition services division contract 

eric hansen nicholas roy pineau valuable comments 

littman cassandra zhang 
incremental pruning simple fast exact method partially observable markov decision processes 
uai 
boutilier poole 
computing optimal policies partially observable decision processes compact representations 
aaai 
craig boutilier 
stochastic local search pomdp controllers 
aaai 
feng zilberstein 
region incremental pruning pomdps 
uai 
hauskrecht 
value function approximations pomdps 
jair 
www ai sri com project calo calo sri com 
calo cognitive agent learns organizes 
gordon pineau thrun 
pbvi anytime algorithm pomdps 
ijcai 
littman kaelbling cassandra 
planning acting partially observable stochastic domains 
ai journal 
leong cao 
modeling medical decisions new general framework dynamic decision analysis 
world congress medical informatics medinfo pages 
fraser hauskrecht 
planning treatment heart disease partially observable markov decision processes 
ai medicine 
kaelbling kim cassandra 
solving pomdps searching space finite policies 
uai 
magni 
uncertainty management techniques medical therapy planning decision theoretic approach 
applications uncertainty formalisms pages 
pollack brown mccarthy ramakrishnan 
intelligent cognitive system people memory impairment 
robotics autonomous systems 
roy gordon 
exponential family pca belief compression pomdps 
nips 
scerri pynadath tambe 
adjustable autonomy real world 
jair 
martin bonasso kortenkamp 
supporting group interaction humans autonomous agents 
aaai 
zhang zhang 
speeding convergence value iteration partially observable markov decision processes 
jair 
zhou hansen 
improved grid approximation algorithm pomdps 
ijcai 
