unified locally linear embedding linear discriminant analysis algorithm face recognition zhang shen zhi hua zhou intelligent information processing laboratory university shanghai china edu cn school software university shanghai china edu cn national laboratory novel software technology university china edu cn 
manifold learning approaches locally linear embedding algorithm lle isometric mapping isomap algorithm aimed discover low dimensional variables high dimensional nonlinear data 
order achieve effective recognition tasks manifold learning problems remain solved 
propose unified algorithm lle linear discriminant analysis remained problems 
training samples mapped low dimensional embedding space lda algorithm project samples discriminant space enlarging class distances decreasing class distance 
second unknown samples directly mapped discriminant space computation corresponding low dimensional embedding space 
experiments face databases show advantages proposed algorithm 
faces varying intrinsic features illumination pose expression thought constitute highly nonlinear manifolds high dimensional observation space 
visualization exploration high dimensional nonlinear manifolds focus current machine learning research 
recognition systems linear method bound ignore subtleties manifolds concavities protrusions bottleneck achieving highly accurate recognition 
problem solved build high performance recognition system 
years progresses modelling nonlinear subspaces manifolds 
rich literature exists manifold learning 
basis different representations manifold learning roughly divided major classes projection methods generative methods embedding methods mutual information methods 

find principal surfaces passing middle data principal curves 
geometrically intuitive difficulty generalize global variable arc length parameter higherdimensional surface 

second adopts generative topology models hypothesizes observed data generated evenly spaced low dimensional latent nodes 
mapping relationship observation space latent space modelled 
resulting inherent insufficiency adopted em expectation maximization algorithms generative models fall local minimum easily slow convergence rates 

third generally divided global local embedding algorithms 
isomap global algorithm presumes isometric properties preserved observation space intrinsic embedding space affine sense 
hand locally linear embedding lle laplacian focus preservation local neighbor structure 

fourth category assumed mutual information measurement differences probability distribution observed space embedded space stochastic nearest neighborhood henceforth sne manifold charting 
impressive results discover features manifold reports published practical applications manifold learning especially face recognition 
possible explanation practical data includes large number intrinsic features high curvature observation space embedded space manifold learning methods strongly depends selection parameters 
assuming data drawn independently identically distributed underlying unknown distribution propose unified locally linear embedding linear discriminant analysis algorithm face recognition 
training samples projected intrinsic low dimensional space 
improve classification ability lda introduced enhancing class distances decreasing class distances mapping sample discriminant space 
assumption neighborhood unknown sample high dimensional space sample low dimensional discriminant space unknown sample directly mapped discriminant space proposed algorithm 
experiments face databases show advantages proposed recognition approaches 
final section discuss potential problems researches 
unified locally linear embedding linear discriminant analysis algorithm locally linear embedding establish mapping relationship observed data corresponding low dimensional locally linear embedding lle algorithm obtain corresponding low dimensional data training set rn 
data set modelling subsequently mapping relationship 
main principle lle algorithm preserve local neighborhood relation data embedding space intrinsic 
sample observation space linearly weighted average neighbors 
basic lle algorithm described follows step define xi samples xij neighbors xi 
considering constraint term wij xi xij neighbor wij compute weighted matrix square 
step define yi arg minw 
considering constraint yi number local covering set 
calculate arg miny 
step algorithm approximate nonlinear manifold sample xi linear hyperplane passes neighbors xi 
xik 
considering objective invariant translation constraint term yi added step 
term avoid degenerate solution 
step transformed solution eigenvector decomposition seen follows arg min yi arg min arg min optimal solution formula smallest eigenvectors matrix 
respect constraint conditions eigenvalue zero need removed 
need compute bottom eigenvectors matrix discard smallest eigenvector considering constraint term 
linear discriminant analysis assuming data different classes similar categories instance facial images sampled difference persons generally thought owning cognitive concept 
data different classes reduced subspace manifold learning approaches 
manifold learning capable recovering intrinsic low dimensional space may optimal recognition 
highly nonlinear manifolds mapped low dimensional subspace reason believe optimal classification hyperplane exists manifolds 
principal axes low dimensional mapping classes manifolds acute angle example classification ability may impaired 
linear discriminant analysis lda introduced maximize separability data different classes 
suppose class equal probability event class scatter matrix defined sw ni yj mi yj mi ni samples class class means mi 
mean samples classes class scatter matrix defined sb mi mi 
maximize class distances minimizing class distances manifolds column vectors discriminant matrix eigenvectors sw associated largest eigenvalues 
projection matrix play role projects vector low dimensional face subspace discriminatory feature space formulated follows proposed algorithm difficult see mentioned procedure comprises steps 
data mapped intrinsic low dimensional space lle mapped discriminant space lda 
expect unify algorithm step computational effectiveness improved 
considering nearest neighbor unknown sample weighted values unknown data training data calculated idea lle idea 
basis formula written follows wi jxi xi means ith unknown sample xi corresponding training samples values 
soon weighted values neighbor samples unknown sample obtained presume data high dimensional space neighborhood relationship low dimensional discriminant space 
unified mapping formulate seen follows zi corresponding unknown samples discriminant space zi closely training samples obtained mentioned steps neighbor indices sample original highdimensional space 
recognition carried discriminant spaces 
proposed approach advantages data directly mapped discriminant space computation intrinsic low dimensional space 
classification neighbor relationship unknown sample implicitly embodies capacity discriminant analysis 
experiments fig 

examples orl face database fig 

examples umist face database verify proposed approach face databases orl database umist database jaffe database investigated 
examples seen 
training samples test samples randomly fig 

examples jaffe face database separated overlapping detail seen table 
intensity pixel regarded dimension 
example pixel equal dimensions 
samples standardized range 
results average runs 
table 
number training samples tr test samples te classes tr te classes dimensions orl umist jaffe expression comparing effectiveness dimensionality reduction proposed approach experiments performed lle pca respectively 
lle dimensionality reduction instance test samples mapped intrinsic low dimensional space mapping approaches ul 
dimensions lle reduced data set jaffe dimension 
nd mapping lda reduction reduced dimension generally 
eigenvalues eigenvectors complex values 
remain real value part complex values th reduced dimensions higher 
addition neighbor factor lle algorithm need predefined 
broad experiments selection neighbor factor little impact result final recognition 
loss generality set orl umist jaffe expression database jaffe face database 
classification algorithms nearest neighbor algorithm nearest feature line nfl adopted final recognition 
corresponding combinational algorithm example combination nfl abbreviated nfl 
due adopted dimensionality reduction approaches different experimental result algorithm listed table lowest error rate corresponding reduced dimensions 
example error rate nfl orl obtained dimensions 
table 
error rates standard deviation proposed algorithm algorithms orl umist jaffe express lle nfl lle nn pca nfl pca nn nfl nn pca lda nfl pca lda nn better analysis recognition tasks conducted 
see proposed algorithm better performance nfl nn approaches 
experimental results longer version 
clear see proposed approach obtains improvements compared lle algorithm isolated pca 
lda introduced lle pca recognition performances approaches comparable 
observation summarize parts proposed algorithm combine lle lda step structure information may loss lead decrease recognition rate 
implement different mapping approach manifold learning algorithm face recognition experimental results better proposed algorithm 
detail seen 
high dimensional data nonlinearity inherent intrinsic variables generated nonlinear mapping intrinsic variables 
condition lda may unsuitable recognition 
error rates orl face recognition nfl nn pca lda nfl pca lda nn reduced dimensions fig 

orl face recognition propose algorithm face recognition 
training set projected intrinsic low dimensional space lda adopted enhance class distances decrease class distances 
second unknown samples projected discriminant space directly computation corresponding counterpart low dimensional space 
experiments show proposed algorithm better lle algorithm pca algorithm recognition comparable pca lda approach 
problems worthy making researches 
observe selection neighbor factor relate error rates recognition 
study feasibility ensemble approach solve problems 
second proposed algorithm lower recognition rate databases compared proposed mla approach lda 

possible reason may projection procedure test sample neighbor factor consider distribution data set global structure information embodied effectively 
compare proposed dimensionality reduction approach state art approaches 
supported ht fd national outstanding youth foundation china 

haw lu robert hecht image manifolds proc 
spie vol 
pp 

hastie stuetzle curves journal american statistical association pp 


gl linder zeger learning design principal curves ieee transactions pattern analysis machine intelligence vol 
pp 


bishop williams gtm generative topographic mapping neural computation pp 


chang ghosh unified model probabilistic principal surfaces ieee transactions pattern analysis machine intelligence pp 


tenenbaum de silva langford global geometric framework nonlinear dimensionality reduction science pp 


roweis nonlinear dimensionality reduction locally linear embedding science pp 


mikhail belkin niyogi laplacian eigenmaps dimensionality reduction data representation 

hinton roweis stochastic neighbor embedding neural information proceeding systems natural synthetic vancouver canada december 

brand merl charting manifold neural information proceeding systems natural synthetic vancouver canada december 

daniel swets john weng discriminant eigenfeatures image retrieval ieee transactions pattern analysis machine vol 
pp 

face recognition hidden markov models phd thesis university cambridge 

wechsler phillips bruce fogelman soulie huang eds em characterizing virtual general purpose face recognition daniel graham nigel 
face recognition theory applications nato asi series computer systems sciences vol 
pp 


michael lyons julien shigeru akamatsu automatic classification single facial images ieee transactions pattern analysis machine intelligence vol pp 


turk pentland eigenfaces recognition journal cognitive neuroscience vol pp 

stan li chan wang 
performance evaluation nearest feature line method image classification retrieval 
ieee transactions pattern analysis machine intelligence 
november 

zhang stan li wang manifold learning applications recognition 
intelligent multimedia processing soft computing 
yap peng tan kim hui yap wang ed springer verlag heidelberg 

opitz maclin popular ensemble methods empirical study 
art 
intell 
research 

