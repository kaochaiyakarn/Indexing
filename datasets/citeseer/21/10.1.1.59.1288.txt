practical pomdps personal assistant domains pradeep rajiv maheswaran milind tambe department computer science university southern california los angeles ca tambe usc edu agents agent teams deployed assist humans face challenge monitoring state key processes environment including state human users making periodic decisions monitoring 
challenge particularly difficult significant observational uncertainty uncertainty outcome agent actions 
pomdps partially observable markov decision problems appear suited enable agents address uncertainties costs slow run times generating optimal pomdp policies presents significant hurdle 
slowness attributed cautious planning possible belief states uncertainty monitored process assumed range possible states times 
introduces key techniques speedup pomdp policy generation exploit notion progress dynamics personal assistant domains 
key insight initial possibly uncertain starting set states agent needs prepared act limited range belief states belief states simply unreachable dynamics monitored process policy needs generated belief states 
techniques propose complementary existing exact approximate pomdp policy generation algorithms 
illustrate technique enhancing generalized incremental pruning gip efficient exact algorithms pomdp policy generation illustrate orders magnitude speedup policy generation 
speedup facilitate agents deploying pomdps assisting human users 

research projects focussing individual agents teams agents assist humans offices home medical care spheres daily human activities 
major area responsibility agents monitor evolution process state time including human agents deployed assist periodic decisions monitoring 
agents may accomplish monitoring decision making fully autonomously adjustable autonomy dynamically reducing autonomy seeking human input 
example office environments agent assistants deployed monitor location individual users transit periodic decisions delaying canceling meeting seeking information users 
similarly caring elderly agent assistants monitor progress plan user decisions regular intervals activities performed 
monitoring periodic decision making seen therapy planning 
unfortunately agents henceforth referred personal assistant agents monitor decisions despite significant uncertainty observations true state world may known uncertainty actions outcome agent decisions actions may uncertain 
furthermore actions costs delaying meeting may cost meeting attendees 
researchers naturally turned decision theoretic frameworks reason costs benefits uncertainty 
research traditionally focused markov decision problems mdps variants decision making ignoring observational uncertainty domains potentially significantly degrading agent performance requiring unrealistic assumptions agent ability monitor state world 
pomdps partially observable markov decision problems suggest natural candidates address observational uncertainty recognized time natural step evolution soft ware assistants 
long run times generating optimal policies pomdps remains significant hurdle agent assistants 
recognizing hurdle pomdp usage practical domains previous pomdps encouraging progress types approaches 
approach exact algorithms try find optimal solution 
exact algorithms remain computationally expensive currently scale large scale problems interest personal assistant domains 
second approach approximate algorithms tradeoff solution quality speed :10.1.1.29.1347:10.1.1.12.7112:10.1.1.126.4744
unfortunately approximate algorithms achieve significant speedups exact algorithms provide loose quality guarantees solutions quality guarantees crucial personal assistants inhabit human environments 
aims practically apply pomdps personal assistant domains introducing novel speedup techniques particularly suitable domains 
key insight monitoring users processes time large parts belief states pomdps regions states uncertainty fundamentally unreachable 
unreachable belief states change time dynamically 
current pomdp algorithms exploit unreachable belief state regions plan entire set belief states point time 
instance personal assistant monitoring user may travelling meeting user travel certain distance time uncertainty user may time step limited locations 
current pomdp algorithms create policies assuming uncertainty belief states spread locations infeasible 
similarly state world move forward minutes meeting fifteen minutes past meeting time passing state actual meeting time 
information dynamics world currently ignored pomdp algorithms 
introduces key techniques exploit sparseness reachable belief space personal assistant domains 
enhancements provide ways simply avoiding policy generation unreachable regions belief space 
characteristics domains provide insight enhancements 
states reachable decision epoch increasing progress time 

observations observable reduced number states 
belief probability state tightly bounded just 
enhancements apart tailored kinds personal assistant domains complementary existing algorithms 
improvements suggest applied variety algorithms see related demonstrate improvements context generalized incremental pruning reasons 
incremental pruning provides public domain implementation documented understood algorithm 
second incremental pruning provides extremely efficient baseline algorithm report improves section algorithm recognized efficient exact algorithm compare 
third improvements orthogonal key improvements discussed change basic framework incremental pruning 
improvements add speedups demonstrated demonstrate orders magnitude improvements performance 
critical note set ideas applied algorithms 

personal assistant domains research personal assistant domains involved agents monitoring state user process environment making periodic decisions status user monitored process 
multiple personal assistants may collaborate order assist collaborative user tasks 
discuss specific example domains personal assistants deployed office environments illustrating challenges domains 
example actual system implemented previously going system development 
key challenges outlined arise personal assistant domains 
key example meeting rescheduling domain implemented electric system 
large scale operationalized system agents monitored location users decisions delaying meeting user projected late ii asking user information plans attend meeting iii canceling meeting iv waiting 
agent relied mdps arrive decisions actions asking nondeterministic outcomes user may may respond decisions delaying costs 
mdp state represented user location meeting location time meeting user home meeting usc minutes policy mapped states actions 
unfortunately observational uncertainty user location ignored computing policy 
second key example task management problem tmp domain 
domain set dependent tasks performed human users 
users 
agents monitor progress humans reallocation decisions 
lines connecting agents users indicate lines communication 
illustration reallocation scenario suppose assigned respectively initial capabilities 
observed progressing slowly may may need reallocate ensure tasks finish deadline 
may reallocate original task nearing completion known capable 
progressing slowly may reallocated despite potential loss capability 
agents monitoring progress dependent tasks important 
reason seen situation 
progressing needs reason compromise allocating instantly waiting capable user finish 
unfortunately task monitoring involves lot transitional progress users time steps observational uncertainty may difficult monitor exact progress time step 
agents ask human decision lot uncertainty progress 
human assistance hand involves cost disturbing user 
user problem solved just transferring control user 
requires sequence decisions 
model problem capture sequential decision making presence observational uncertainty 
necessity sequence decisions seen tmp model users progress task may uniform 
users may progress deadline may bulk closer deadline 
instantaneous assessment may take account dynamics progress 
example consider action space agent observe progress np progress take actions wait ask user reallocate 
sequential decision model yield policy tree shown 
policy takes account uncertainty observation includes costs decisions stages management problem analysis 
complex domains additional actions delaying task deadline cost choosing appropriate user task reallocated cascading affects actions require planning time 
pomdps provide framework analyze obtain policies domains simple rule strategies fail 

sample pomdp tmp policy tmp domain state pomdp represents progress various tasks monitoring task dependent tasks time deadline 
policy provides mapping observed progress task action 
employing pomdp solve tmp problem helpful 
problem modelled mdp observational uncertainties disastrous circumstances 
situations example problem clear gets noisy information complete ignores possible reallocation decision prudent 
gets noisy information having started task prematurely 

overview pomdps pomdp represented tuple finite set states finite set actions finite set observations provides probability transitioning state action probability observing action reaching reward function 
belief state probability distribution set states 
communication structure task dependency diagram value function belief state defined maxa bt 
currently efficient exact algorithms pomdps value iteration algorithms specifically gip 
dynamic programming algorithms iteration value function represented minimal set dominant vectors called parsimonious set 
parsimonious set time vt generate set time vt follows 
sp vi vi vt 
rune 
rune rune 
rune va rune call executes linear program lp recognized computationally expensive phase generation parsimonious sets exact algorithm 
approach obtain speedups reducing quantity calls 

approaches approach named belief support value iteration consists enhancements gip 
include introducing ideas algorithms enable dynamic state ds spaces dynamic observation sets dynamic belief db supports 
key realization domains personal assistant agents interacting users tasks idea progress implies properties state transitions observation generations restrict states time 
provide algorithms extract states corresponding viable algorithms 
ideas better calculate accurate belief support apply gip 
theoretical bases proofs validity provided algorithm details implementation 

dynamic state spaces ds natural method personal assistant agents represent user state tmp consisting spatial element tmp capturing progress task temporal element capturing stage decision 
transition matrix static function state 
approach adjustable autonomy problem addressed mdps 
note domain due nature task progress time evolve reach states state 
consider scenario levels task progress decision points deadline 
static state space model states 
domain time move forward single steps 
dynamic state model time states furthermore limits tasks progress advance progress level time step know know 
implies state space point time represented compactly dynamic fashion 
require transition matrix reward function dynamic 
dynamic state spaces transition matrices reward functions affect dynamic programming process finite horizon problem value functions generated particular stage depend transition reward functions states value function previous stage known 
knowledge initial belief space possible levels task progress show obtain dynamic state spaces representation affect optimality pomdp solution 
line gen pomdp function dp update function algorithm provides algorithm ds 
part algorithm describe information transition matrix extract appropriate compact dynamic state spaces 
length finite horizon decision process 
set possible states occupied process 
time st denote set possible states occur time 
reachable belief state algorithm func pomdp solve st ot max gen pomdp vt vt dp update vt func dp update ot st st st rt st ot st stp st st st rune va rune vt rune return vt func point dominate st st st st return true return false func lp dominate lp vars st st st lp max subject st stb st st max st st return return nil func best max inf max max lex max return func prune element point dominate true lp dominate nil best return func get bound st func gen ob st set starting states st st max st st add st reachable states ot get relevant obs st ob max st get bound st return st ot max st st bt st 
obtain st 
inductively know set follows st st tt belief probability particular state time starting belief vector time bt action observation expressed follows bt ot st st ot st st st tt st bt st st st tt st st bt st implies belief vector bt support st st bt bt support st st generated 
model process migrates dynamic state spaces st indexed time accurately stage decision process opposed transitioning static global state set modeling process manner effect optimality solution obtained value function methods 
vt bt value optimal policy time equivalently th stage dynamic programming 
pt denote set policies available time denote value policy time denote value opti mal policy time bl maxp pl vl si rl rl reward function time bl action prescribed policy bl sl bl maxp pl bl bl sl vl sl si sl 
calculating value function time bl maxp pl bl si rl tl pl policy subtree policy tree pl observing initial action 
bl sl vl bl bl sl maxp pl bl vl sl si sl 
applying reasoning inductively show need st furthermore st rt st tt 
need st 
value functions expressed beliefs dynamic state spaces st identical expected rewards advantage method generating set value vectors dominant underlying belief point parsimonious set particular iteration eliminate vectors dominant belief supports reachable reduces set possible policies previous time 

dynamic state observation spaces ds note domains certain observations obtained certain states 
consequently dynamic state spaces imply observations capable obtained particular type dynamic 
consider situation progress levels observations 
furthermore assume move progress level stage go step second reached state action viable observations progress levels current level progress get observations 
assume dynamic state space particular stage limits progress levels able get observations regardless action take time 
show obtain dynamic observation sets affect value iteration process 
line gen pomdp function dp update function algorithm provides algorithm ds 
set possible observations 
define st 
rewrite st rt st st st tt st st st st st st set complement 
dynamic observations second part sum goes zero 
implies observations relevant value strategy time creating policy trees subtrees necessary reduces set policies generated pruning 
improves performance reducing number vectors need considered linear program 
consistency index observation probability matrix time depends dynamic state 

dynamic belief spaces db introducing dynamic state spaces attempting accurately model support reachable beliefs occur 
process precise incorporating information initial belief distribution transition observation probabilities 
example know initial belief regarding task progress probability quarter done rest probability mass started highest probability quarter half done stage dynamic transition matrix 
outline polynomial time procedure obtain bounds belief support answer question 
line gen function algorithm provides algorithm db 
bt st space bt bt 
exists initial belief vector action observation sequence length applying standard belief update rule get belief vector bt captured set bt 
bt st min st bt ot bt bt min st bt st max ot bt bt st bt ot st st bt max st st st tt st st bt st st st ot st st st tt st st bt st bt min max min st max st bt bt 
show max st similarly min st generated linear programming 
action observation express problem max bt bt ot st tt st st st st ot st tt st st 
shown maxi upper bound expression 
proof done mathematical induction number vari ables obtain bmax st optimize st ot combination 
set chosen modeled set linear constraints 
sets bt bmax dynamic beliefs increase costs lp adding constraints 
gain looking dominant vectors smaller support reduces cardinality parsimonious set leaving fewer vectors consider iteration 
represented linear constraints min 
experimental results experiments conducted tmp domain explained section 
explained earlier agent uses pomdp reason reallocation transfer control humans 
complexity pomdps increased decreased increasing decreasing number progress steps tasks monitoring steps deadline 
criterion comparing various approaches gip ds ds ds db time taken vectors pruning 
vectors pruning important criterion experiments involving ds main contribution ds reducing number vectors pruning iteration 
number vectors pruning serves indicator planned belief space 
approach gives number vectors pruning approach finding equal quality solution problem implies planning unreachable belief space 
ds ds fully implemented db partially implemented initial results ds db 
experimental setup consisted problems order increasing complexity 
table provides time results gip ds ds 
indicates results obtained 
seen table ds provides orders magnitude experiment gip ds ds table 
comparing time taken seconds 
comparison ds ds algorithm time taken pruning pruning ds db ds ds table 
illustrating advantages db speedup gip 
seen ds takes time compared ds difference apparent problem complexity increases 
fig compares number maximum number vectors generated pruning ds ds 
seen number vectors pruning ds far number ds 
table provides results comparing performance ds db ds db specific problem 
seen criterion ds db outperforms ds ds 
number vectors pruning ds db gives indication unreachable belief region removed putting better bounds belief probabilities states 

related techniques solving pomdps categorized exact approximate 
gip existing exact algorithms complementary 
exact algorithms attempt exploit domain specific properties speedup pomdps 
instance presents hybrid framework combines mdps pomdps take advantage perfectly observable components model 
focus reachable belief spaces analysis capture dynamic changes belief space reachability ii analysis limited factored tate pomdps iii extent speedup provided measured 
contrasts focuses dynamic changes belief space reachability application flat factored state pomdps 
approximate algorithms faster exact algorithms cost solution quality 
significant amount area point grid dominate algorithms :10.1.1.12.7112
approaches solve larger problems provide loose quality guarantees solution 
critical quality guarantees paa domains agent gain trust human user 
developed technique uses state space dimensionality reduction pca provide guarantee quality solution 
point value iteration pbvi provides best quality guarantees obtain results needs increase sampling consequently increasing run time 
techniques benefit approximate algorithms pbvi benefit technique 
provides techniques application pomdps personal assistant agents reality 
particular provide key techniques speedup pomdp policy generation exploit notion progress dynamics paa domains 
key insight initial possibly uncertain starting set states agent needs prepared act limited range belief states 
belief states unreachable dynamics monitored process policy needs generated belief states 
techniques propose complementary existing exact approximate pomdp policy generation algorithms 
illustrate technique enhancing generalized incremental pruning gip efficient exact algorithms pomdp policy generation obtain orders magnitude speedup policy generation 
detailed algorithm illustrating enhance ments algorithm proofs correctness techniques 
techniques facilitate agents utilizing pomdps assisting humans 

research supported sub contract sri international 
littman cassandra zhang 
incremental pruning simple fast exact method partially observable markov decision processes 
uai 
arens pynadath tambe chalupsky 
electric agent organization human organization 
aaai fall symposium socially intelligent agents human loop 
feng zilberstein 
region incremental pruning pomdps 
uai 
hauskrecht 
value function approximations partially observable markov decision processes 
jair 
www ai sri com project calo calo sri com 
calo cognitive agent learns organizes 
gordon pineau thrun 
point value iteration anytime algorithm pomdps 
ijcai 
leong cao 
modeling medical decisions new general framework dynamic decision analysis 
medinfo pages 
lovejoy 
computationally feasible bounds partially observable markov processes 
operations research 
mccarthy ramakrishnan pollack brown 
intelligent cognitive system people memory impairment 
robotics autonomous systems 
fraser hauskrecht 
planning treatment heart disease partially observable markov decision processes 
ai medicine 
magni 
uncertainty management techniques medical therapy planning decision theoretic approach 
applications uncertainty formalisms pages 
pynadath scerri tambe 
adjustable autonomy real world 
jair 
roy gordon 
exponential family pca belief compression pomdps 
nips pages 
zhang zhang 
speeding convergence value iteration partially observable markov decision processes 
jair 
zhou hansen 
improved grid approximation algorithm pomdps 
ijcai pages 
