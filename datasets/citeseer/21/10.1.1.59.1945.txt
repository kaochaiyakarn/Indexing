discriminative clustering samuel kaski sinkkonen neural networks research centre helsinki university technology box fi hut finland department computer science box fi university helsinki finland distributional clustering model continuous data reviewed new methods optimizing regularizing introduced compared 
samples discrete valued auxiliary data associated samples continuous primary data continuous data space partitioned voronoi regions maxi mally homogeneous terms discrete data 
variation primary data associated variation discrete data affects clustering discrete data supervises clustering 
continuous space partitioned new samples easily clustered continuous part data 
experiments approach shown produce homogeneous clusters alternative methods 
regularization methods demonstrated im prove results entropy type penalty unequal cluster sizes inclusion model marginal density primary data 
inter special kind joint distribution modeling tunable emphasis preprint submitted neurocomputing november discrimination marginal density 
key words discriminative clustering information metric learning metrics regularization vector quantization models exist discovering components underlying occurrences nom inal variables joint distribution continuous discrete data 
consider related task clustering continuous primary data conditional modelling clusters relevant informative discrete auxiliary data capable predicting 
discriminative approach expected result clusters informative obtained modeling joint distribution 
continuity distinguishes setting classic distributional clustering :10.1.1.39.9882
task coined discriminative clustering dc different classification number clusters constrained equal number classes clustering purposes may high low 
dc derived cluster structure space primary outcome degree distributional parameters predicting cluster integrated 
main application area dc data exploration mining 
alterna authors contributed equally 
corresponding author 
tel fax 
email address samuel kaski hut fi samuel kaski 
tively interpreted existing probabilistic partitioning dc alter coarseness partitioning 
prototypical application grouping existing customers com basis continuous covariates including instance coor residence age clusters informative buying behavior customers product categories 
new real potential customers clustered purchases 
potential applications include finding prototypical gene expression patterns refine existing functional classifications genes clustering financial statements discover different ways descend bankruptcy partitional clustering general variable automatically guide feature selection :10.1.1.28.7256
earlier model discriminative clustering reviewed extended :10.1.1.28.7256
line implementation earlier model simple interesting connections neural computation practical data analysis shortcoming formulated distributions data finite data sets implies take uncertainty caused finiteness sample rigorously account 
formulate discriminative clustering bayesian terms additional benefit parameters earlier method modeling auxiliary data integrated cost function 
model optimized directly gradient algorithms show complementing conjugate gradient algorithm smoothing partitions gives comparable results time consuming sim annealing 
improve performance model additionally regularized alternative ways penalizing unequal cluster sizes alternatively adding cost function term modeling primary data 
equivalent generative modeling full joint distri bution primary auxiliary data interpretable tunable compromise modeling 
experiments proposed models outperform alternative mixture models task regularization methods outperform pure dc 
cases new bayesian optimization method performs better older stochastic line algorithm requires time opti mization 
discriminative clustering model start reviewing basic discriminative clustering model simultaneously clarifying relationship maximum likelihood estimation :10.1.1.28.7256
different original derivation perspective bayesian extension possible 
goal discriminative clustering partition primary data space clusters local primary space ii homogeneous predictive terms auxiliary data 
connection homogeneity clusters detailed 
locality enforced defining clusters voronoi regions primary data space belongs cluster vj mj mk voronoi regions uniquely determined parameters mj 
homogeneity enforced assigning distributional prototype denoted vj voronoi region searching partitionings capable predicting auxiliary data prototypes 
resulting model piecewise constant generative model conditioned log likelihood vj log 
probability class jth voronoi region vj predicted ji ci vj denotes class sample summary assumed data generating mechanism simple primary data covariates determine cluster membership relation ship deterministic parameters clusters 
auxiliary data generated cluster specific multinomial having parameters ji 
generative mechanism assumed 
asymptotically large data sets vj dkl dx const 
dkl kullback leibler divergence prototype observed distribution auxiliary data 
cost function means clustering vector quantization vq distortion measured dkl 
sense maximizing likelihood model maximizes distributional homogeneity clusters 
shown maximizing equivalent maximizing mutual information auxiliary variable partitioning connection models empirical mutual information clustering criterion :10.1.1.28.7256
asymptotically dc performs vector quantization fisher metrics restriction voronoi regions defined original usually euclidean metric 
optimization discriminative clustering introduced current name line stochastic algorithm optimizing cost function derived :10.1.1.28.7256
gradient cost non zero borders voronoi regions overcome difficulty regions smoothed 
resulting algorithm briefly reviewed 
smoothing performed introducing membership functions yj mj 
values membership functions vary yj 
smoothed cost function kl yj mj dkl dx 
possible form memberships normalized gaussian yj normalizes sum unity value parameter controlling smoothness chosen validation set 
smoothed cost minimized algorithm 
denote data pair line step index discrete value ci 
draw clusters independently probabilities values membership functions yk keep distributional parameters summed unity soft max log ji ji log exp jm 
adapt prototypes mj mj mj log li ji jm jm jm mi mi kronecker delta 
due symmetry possible evidently beneficial adapt parameters twice swapping second adaptation 
note updating takes place mj mj 
learning parameter decreases gradually zero schedule guarantee convergence fulfill conditions stochastic approximation theory 
finite data algorithm maximizes conditional likelihood heuristically smoothing clusters get computable gradient 
map estimation clusters dc earlier discriminative clustering algorithm motivated maxi mization empirical mutual information :10.1.1.28.7256
asymptotically large amounts data mutual information justified measure homogeneity depen dency 
maximization mutual information re interpreted previous section maximum likelihood estimation uses smoothed cluster mem optimization trick 
new interpretation opens pos bayesian extensions 
small data sets alternative potentially better behaving form discriminative clustering obtained ing likelihood introduced 
turns distributional prototypes analytically integrated posterior distri bution mj data leave parameters mj voronoi regions 
convenient improve results account uncertainty associated 
goal partition primary space predicting classes predictions needed 
auxiliary data denoted primary data wish find set clusters mj maximizes marginalized posterior integration map dc mj mj 
improper prior mj factors ji dirichlet priors pa rameters common dirichlet distribution conjugate prior multinomial distribution convenient 
bayes rule marginalization posterior map dc mj nji ji nji nj 
nji number samples class cluster nj nji final objective function posterior probability cluster centroids data 
assuming dc task objective meaningful comparing various alternative methods optimization marginalized maximum likelihood prior mj im proper 
criterion searching maximum posterior map estimate 
practice logarithm posterior log mj ij log nji log nj const 
computational simplicity 
notice connection mutual infor mation retained marginalization process 
maximizing posterior asymptotically maximizes mutual information clusters aux data appendix 
optimization plain marginalized dc unsuitable gradient optimization reason infinite data cost gradient affected samples typically zero probability border clusters 
prob lem earlier avoided smoothing approach similar smoothing possible marginalized dc 
smoothed number samples nji yj class yj smoothed cluster membership function defined 
experiments smoothing optimization smoothing evaluating clus tering results 
value smoothing parameter selected validation 
smoothed map objective function log mj log ij yj log yj const 
normalized gaussian membership functions gradient objec tive function respect jth model vector appendix mj log mj mj yl yj lj ll nji nj digamma function derivative logarithm 
standard gradient optimization algorithm maximize conjugate gradients 
alternatively objective function optimized directly simu lated annealing sa 
described smoothed optimization method compared sa experimental section 
iteration sa candidate step generated making small random displacements prototype vectors 
step accepted increases value objective function 
decreases objective function accepted probability decreasing function change objective function 
gaussian displacements covariance matrix identity matrix temperature parameter decreased linearly 
parameter chosen preliminary experi ments validation set 
displacement step decreases objective function accepted probability exp 
dc produces optimal contingency tables large number methods analyzing statistical dependencies discrete valued nominal categorical random variables basis occurrence frequencies contingency tables exist classi cal see example 
old example due fisher measure order adding milk tea affects taste 
variable indicates order adding ingredients second taste better worse 
medicine variable indicate health status demographic groups 
resulting contingency table tested dependency row column variables 
discrete valued auxiliary data result clustering method analyzed contingency table possible values auxiliary vari able correspond columns clusters rows dimensional table 
clustering compresses potentially large number multivariate continuous valued observations manageable number categories contin table tested dependency 
note difference traditional contingency tables row categories fixed clustering method tries find suitable categorization 
question discriminative clustering way constructing contingency tables 
answer optimal sense introduced 
consider problem finite sample sizes 
large sample sizes sampling variation cell frequencies table negligible 
empirical mutual information approaching real mutual information data available natural measure dependency margins contingency table 
various ways take account effects small sample sizes small cell frequencies contingency tables subject re search 
bayesian methods cope small data sets derive connection simple version bayesian approach discriminative clustering method 
classical results derived contingency tables fixed margin categories optimize categories 
type bayesian test dependency contingency tables com puting bayes factor hypothesis statistical independence row column categories nji nji negation alternative hypothesis margins dependent 
practice hypotheses formulated dirichlet priors product marginal priors independency cells dependency 
special case fixed margin auxiliary data contingency table prior defined section bayes factor proportional appendix 
map estimation discriminative clusters equivalent constructing dependency table results maximal bayes factor constraints model 
regularization problem pure dc categories may overfit apparent de small data set 
regularization methods marginal ized dc introduced section reduce overfitting 
straightforward attempt improve optimization joint distribution modeling 
explicit modeling covariates may improve discrimination especially small data sets cf 

emphasizing equal cluster sizes non parametric regularization method equal distribution data clusters favored useful avoiding dead clusters bad initialization 
equalized penalized objective function ceq mj log nji eq log nj ij eq parameter governing amount regularization 
number data samples increases divided approaches mutual information plus eq times entropy clusters plus term depend parameters see appendix 
larger eq solutions roughly equal numbers samples clusters favored 
alternative equalization prior place effect similar sense second part cost function important 
prior inconsistent viewpoint derivation section prior wholly justified bayes factor interpretation section 
modeling marginal density primary data discriminative methods model conditional probability may benefit regularizing effects modeling marginal 
case dc complemented full joint distribution model mj mj mj generative gaussian mixture type model 
note fac tors parameterized centroids mj 
explicit special kind parameterization possi ble interpret adjustable compromise modeling 
standard mixture gaussians model isotropic gaussians covariances centers mj 
map estimation clusters joint model improper prior mj posterior gets extra factor exp mog mj mog weights gaussians 
parameter mog mog variances gaussians better illustrate regularizing nature term mog means regularization 
correspondingly log posterior joint model log mj log ij nji log nj log exp mog mj model interpreted additive regularization term cost function 
change value mog focus clus tering shift dc traditional mixture clustering 
practice value mog chosen validation set maximize un regularized cost 
means regularization interpretation suggests simpler partly heuristic regularization ing log cost mixture gaussians negative cost function euclidean means clustering ev vj vq mj vq similar role mog mixture gaussians 
means mixture gaussians cally rigorous intuitively meaningful think making means clustering derived probabilistically called classifi compromise kullback leibler divergence euclidean dis tance computationally simple 
tunable compromise dc means clustering apparent cost written related methods cv mj map dc 
connections related problems approaches data analysis including feature selection various clustering criteria briefly discussed 
automatic feature extraction proper manual feature selection extraction indispensable step data analysis 
automated methods developed complementing especially pattern recognition applications 
feature extraction change topology input space general operation change metric dc asymptotically interpreted change metric 
may advisable automatic feature extraction methods preprocessing dc reasons clusters dc defined euclidean voronoi regions data space 
shape principle tuned transforming feature space 
ii dimensionality reduction reduces number parameters cation mixture final log posterior having extra term proportional log mj mj sum gaussian integrals voronoi regions 
computing mj infeasible 
solution 
additionally desired changes topology discontinuous transformations included preprocessing steps dc 
discriminative clustering pre estimating densities possible alternative approach discriminative clustering find density estimate data apply standard clustering algorithms metric estimated densities see :10.1.1.26.6383
straightforward proximity measure points dkl 
keep clusters local primary data space 
metric locally equivalent kl divergence generated help fisher information matrix close points 
problem approach involves unrelated criteria density estimation clustering 
hard see costs principled way 
approach works practical data engineering tool applied self organizing maps :10.1.1.26.6383
generative occurrence models term occurrence model refers model joint occurrences nominal variables 
example document clustering nominal vari ables documents words documents clustered comparing occurrences words documents 
statistical point view straightforward method mod eling occurrence data postulate parameterized probabilistic model estimate parameters con criterion maximum likelihood 
approach hofmann introduced class mixture models marginals joint distribution 
field text document analysis coined joint distribution model probabilistic latent semantic indexing plsi 
conceptually discriminative clustering seen occurrence model exactly distributional clustering model conditional distri butions continuous margin clusters restricted local 
interpretation extends original occurrence distributional clustering paradigm introducing continuous variable 
concept local clusters asymptotic connection metrics described meaningful discrete occurrence setup 
practical applications continuity parameterization partitions necessary imple mentation discriminative clustering different classic occurrence models 
classic distributional clustering information bottleneck distributional clustering term clustering margin occur rence data introduced pereira 
information bottleneck ib principle gives deeper justification classic distributional clus tering 
information bottleneck originally introduced categorical variables principle commonalities theory native clustering 
discuss bottleneck length 
tishby get motivation rate distortion theory shan non kolmogorov see textbook account 
rate distortion theory framework finds optimal representation conventionally codebook set discrete symbols cost form dis function describing effects transmission line 
notation authors consider problem building optimal representation discrete random variable optimality rep resentation measured capability represent random variable possibly distorted noisy transmission channel lossy compression 
representation input sample deter case function general relationship stochastic described density 
frequency codes described marginal density 
rate distortion theory real valued distortion function sumed known mutual information minimized re spect representation subject constraint ex intuitive 
optimum conditional distribu tions defining codebook vl vl exp vl vj exp vj constant depends information bottleneck negative mutual information average distortion ex 
intuitive terms equivalently procedure mutual information earlier negative distortion maximized representation informative possible limited value interpretable kind resource limitation representation functional minimized varia tional optimization respect conditional densities leads vj dkl vj 
result self referential constitute algorithm finding 
explicit solution ob tained iterative algorithm resembles blahut algorithm cf 

order clarify connection dc consider continuous data space bottleneck principle defining partitions informally extended case continuous asymptotic case large de regularization parameter bottleneck clusters voronoi regions kullback leibler distortion categorical 
kullback leibler voronoi regions non local space 
practice ib continuous data require additional parameterization clusters 
dc clusters parameterized voronoi regions space giving additional bonus local clusters 
locality eases interpretation may important applications 
cost functions information bottleneck asymptotical tive clustering common term mutual information 
bottleneck additional term keeping complexity represen tation low complexity discriminative clusters restricted number parameterization practice regularization 
original mutual information kl distortion cost dc cost ib defined distributions data sets 
straightforward way applying cost function finite data sets approximate densities empirical distributions see section 
limit crisp clusters uniform distribution ib equivalent finding maximum likelihood solution certain multinomial mixture model 
knowledge marginalization procedures similar marginalized dc proposed 
generative models joint density mixed type variables popular finite mixture models called model clustering 
models data sample generated finite number generators identified clusters density mixture densities 
models fitted data maximizing likelihood em algorithm 
model paired data assumed generator generates discrete continuous multinomial gaussian distribution respectively 
model joint density called mixture discriminant analysis mda 
dc models conditional density 
conditional densities derived models joint density bayes rule possible conditional models perform better focus resources directly conditional density 
section provides empirical support hypothesis 
hand regularization joint modeling section turns dc traditional joint density models believe making compromise extremes provides better generalization ability 
experiments experiments divided parts 
dc demonstrated simple toy data set illustrate discriminative properties effect regularization 
second new finite data marginalized variant compared older stochastic algorithm standard machine learning data sets 
optimization algorithms section marginalized dc additionally compared 
regularization principles section tested find help little training data 
closest alternative mixture methods included comparisons demonstrate dc solves problem addressed standard clustering methods 
toy demonstration voronoi region centers mj vq regularized model shown different values regularization parameter vq 
data samples isotropic gaussian vertically fig 

vq regularized discriminative clustering dc model com promise plain dc ordinary means vq 
viewpoint plain dc vq left vertical dimension relevant distribution binary auxiliary data change monotonically direction 
compromise representation data vq middle 
algorithm turns ordinary vq vq right 
circles denote voronoi region parameters mj gray shades density 
varying 
samples come classes samples increases monotonically bottom top 
naturally increases top bottom 
small values vq original cost function discriminative clustering minimized clusters represent vertical direction space conditional distribution changes 
vq increases clusters gradually start represent variation converging means solution large vq 
similar compromise mixture gaussians regularization 
comparison stochastic dc performance older stochastic line algorithm sec tion optimization algorithms maximizing marginalized posterior section compared real life data sets 
standard mixture models mixture gaussians modeling mda modeling see section included 
comparisons show discriminative modeling outperforms ordinary clustering methods task 
materials methods algorithms compared data sets landsat satellite data dimensions classes samples letter recognition data dimensions classes samples uci machine learning repository speech data timit collection 
altogether samples picked timit material classified groups phones phonetic sounds encoded cepstral components 
best values smoothing parameter sought series preliminary runs 
data sets partitioned clusters class indicators auxiliary data 
number clusters solutions computed logarithmically spaced values smoothing parameter dc similar values spread gaussians mixture models 
set logarithmically spaced values tried width jumping kernel simulated annealing 
cluster prototypes centers models initialized random draws data 
conservatively large number iterations chosen em iterations mixture models times number clusters stochastic iterations non marginalized dc simulated anneal ing marginalized dc 
maximal number iterations marginal ized dc optimized conjugate gradients set algorithm converged cases 
prior parameters set unity 
adaptation coefficient old stochastic algorithm decreased piecewise linearly zero coefficient times larger 
performance methods compared posterior proba bility cluster prototypes computed held data 
note auxiliary parts held data way computing cluster identities function primary data 
posterior probability justified measure goodness native clusters irrespective clusters generated 
somewhat problematic cost function meth ods 
main drawn comparisons discriminative clustering promises 
alternative goodness mea sure empirical mutual information cluster identities nominal auxiliary data labels 
produces practically identical results shown 
results significance performance differences tested tailed tests fold cross validation runs table 
combination model cluster count smoothing parameter model fixed best value preliminary phase experiments 
best dc variant significantly better non dc methods 
landsat timit data sets marginalized map variant best regardless number clusters 
interesting cases cluster solutions best result obtained gate gradient algorithm 
letter recognition data old stochastic algorithm produces best results difference marginalized dc optimized conjugate gradients insignificant 
drawn 
dc algorithms perform clearly better task standard clustering algorithms expected solve problem directly addressed standard clustering 
ii new marginalized version gives results better data sets comparable old stochastic version 
iii optimization algorithms marginalized dc comparable conjugate gradient algorithm having clear edge cluster solutions 
clearly faster computation preferred choice cases 
table average cost negative log posterior algorithms fold cross validation trials 
best performance cluster number nc shown bold results value pairwise test doubly underlined 
single un denotes value 
cg map map estimation smoothed dc conjugate gradient algorithm sa map map estimation simulated anneal ing sdc dc old stochastic algorithm mog mixture gaussians mda mixture model joint probabilities 
data nc cg map sa map sdc mog mda landsat landsat landsat timit timit timit letter letter letter effect regularization methods compared plain marginalized discriminative clustering model regularized variants data sets final performance models measured pure dc objective function regularization 
closest alternative mixture models included dc optimized stochastic line algorithm 
keep experiment set manageable regularization methods applied stochastic algorithm 
time classical euclidean vector quantization means included completeness regularization models 
marginalized dc models optimized conjugate gradient algorithm results previous section 
effects regularization expected apparent small data sets data split number smaller subsets set independent tests 
landsat data left samples test setup 
letter recognition data split subsets 
fold modeling testing subset gave total repetitions cluster solutions 
width parameter mixture components smoothing regularization parameters selected fold cross validation learning set 
parameters mj initialized random set training samples 
results means initialization appearing table experiments described 
larger subset samples timit collection allowed subsets resulting repetitions parameters repetition selected fold cross validation 
results best regularized methods significantly better plain marginal ized discriminative clustering turn produced better discriminative clusters methods 
results columns letter rand timit rand table clear timit data old tic algorithm significantly worse regularized marginalized dc 
letter recognition data old stochastic algorithm best previous set experiments section difference best marginalized dc insignificant 
note dc regularized mix ture gaussians significantly better plain marginalized dc letter recognition data difference visible table 
combined results show regularization helps marginalized dc performance compared old stochastic algorithm depends data 
heuristic regularization means slightly lower performance compared probabilistically justified mixture gaussians difference significant 
effect tuning compromise means dc vq regularization demonstrated 
expected increasing vq shifts solution optimizing posterior probability optimizing means error 
new finding slanted form slight regularization improves predictive power clusters test set 
replacing means mixture gaussians gives similar curve 
studied repeating cluster experiments table er replacing random initialization means improve results reduce variation data sets 
results dc variants table comparison marginalized discriminative clustering dc regularized ver sions dc vq dc mog dc eq data sets letter recog nition timit 
line sdc comes old stochastic algorithm 
mixture gaussians mog plain means vq joint density model mda included 
results random means vq initialization 
key see table note values computed columns rows 
method letter rand letter vq timit rand timit vq sdc dc dc vq dc mog dc eq mog vq mda improved significantly columns letter vq timit vq table 
exception old stochastic algorithm letter recogni tion data expectation performance decreased 
regularized versions best relative goodness varied 
results regularization experiments summarized vq dc mda mog vq map dc fig 

effect tuning vq regularization timit data 
curves show components cost change amount regularization tuned 
components means cost predictive power 
small dots curves vq regularized dc varying parameter vq large dots left right plain dc mda mixture gaussians mog plain means vq 
solid line test set dashed line learning set 
results aver ages cross validation runs computational reasons parameter dc runs cross validated kept constant 
main points 
regularization improves performance marginalized dc small data sets 
ii relative performance regularization principles depends data 
iii initialization means significantly improves performance random initialization 
discussion algorithm distributional clustering continuous data interpreted covariates discrete data reviewed extended 
prototype distri butions discrete variable associated voronoi regions continuous data space regions optimized predict discrete data 
experiments method produced better discriminating clusters common methods 
core dc model close models proposed earlier classification rbf 
dc main outcome clusters enabled marginalize parameters producing pre new cost function takes account finite amount data maximization equivalent maximizing bayesian measure statistical dependency contingency tables 
optimization new cost function leads clustering results comparable better produced previously stochastic line algorithm 
augmented new cost function regularization methods 
similar kinds regularization approaches applicable old infinite data cost function 
addition new cost function contains new empirical results 
fast optimization smoothed voronoi regions conjugate gradients produces clusters comparable obtained considerably time consuming simulated annealing 
ii regularization meth ods equalization cluster sizes shifting joint distribution model improve results compared plain dc 
drawn relative goodness methods 
iii tion important means superior initialization random data 
regularization joint distribution modeling interpretable sion term modeling primary data cost function 
number parameters regularized models independent regularization parameter sense model complexity fixed 
regularized model compromise tunable representing variation associated changes dc task representing variation classical clustering task 
experiments regularization performance learning data impaired test set performance improves significantly 
reason allocating resources modeling improves generalization respect 
adjustable combination mixture models proposed joint modeling terms hyperlinks text documents 
similar combination improved discriminative conditional density model 
joint distribution modeling approach possible treat primary data samples lacking corresponding auxiliary part partially missing data lines semisupervised learning proposed classification tasks 
improvement obtained means initialization hints prac tical optimization strategy starts standard clustering tunes gradually dc 
acknowledgments 
supported part academy finland ist programme european community pascal network excellence ist 
publication reflects authors views 
authors insights contingency tables acknowledge access rights materials produced project restricted due commitments 
connection marginalized likelihood mutual informa tion consider objective function penalized clustering algorithm ceq mj log nji log nj ij constant reduces 
stirling approximation log log log applied yields ceq mj ij nji kji log nji kji nj kj log nj kj log kji kj constants depend prior 
note nj nji 
zeroth order taylor expansion log log gives ceq mj nji log nji nj log nj log division gives ceq mj ij nji ij log nji nj nj log nj log log nji approaches probability class cluster nj approaches pj number data samples increases 
ceq mj ij log pi log pi pj log pj log term mutual information second term constant respect mj third term times entropy pj term depend parameters 
result equal mutual information added constant 
gradient marginalized likelihood denote brevity tji nji tj tji 
gradient respect mj mj log mj il tli yl mj tl yl mj tl tl yl mj straightforward show normalized gaussian membership func tions mj substituting gradient gives mj log mj yl mj lj yl yj mj lj yl yj tl tl final form gradient results applying identity 
lj yl lj ll connection marginalized likelihood contingency tables connection posterior probability bayesian measure statistical dependency contingency tables derived 
note contrast connection mutual information principle measure statistical dependency connection non asymptotic 
denote number samples ith auxiliary category number entries ith column margin contingency table ci 
application contingency tables margin fixed consists discriminative clusters depends cluster centroids 
explained section evidence dependence margin variables quantified bayes factor relative strength evidence dependence see advanced treatment infinite mixtures 
computing bayes factor assumptions dependence independence encoded joint prior distribution data cells hypothesis independence rows columns prior probability data cell product margin dirichlet prior probabilities hypothesis dependence prior simply dirichlet distribution cells 
dirichlet distribution sharpness parameter may inter amount prior data hypothesis dependence dirichlet prior amount prior data denoted cell contingency table 
hypothesis dependence need dirichlet prior columns rows 
row margin dirichlet distribution equal amount prior data row 
contrast assume total amount prior data hypotheses 
prior sample size rows prior marginalized 
prior column margin follows similarly consistency detailed 
bayes factor conditioned column margin denominator nji nji nj ci nji ci 
nji ci nji nj ci nj ci factor frequencies data table margins follows hypergeometric distribution second factor nj nj nj dirichlet prior parameters distribution 
multinomial data nj nominal values dirichlet prior equal amount prior data values formula nj kn 
nj kn nj 
get similar expression third factor ci inde pendent cluster solution depends column margin ci fixed viewpoint dc omitted derivation 
consistency columns amount kn prior data 
numerator nji ci nji ci nji similar products go cells table prior data cell 
nji nji irrelevant factors omitted 
margin ci hand identical ci constant dc 
considerations bayes factor written nji ci nji ci constant depends nj nji 
nji nj const 
mj const 
agresti survey exact inference contingency tables statistical science 
becker mutual information maximization models cortical self organization network computation neural systems 
blake merz uci repository machine learning databases www ics uci edu mlearn mlrepository html 
blei ng jordan latent dirichlet allocation journal machine learning research 
buntine variational extensions em multinomial pca elomaa mannila toivonen eds proceedings ecml th european conference machine learning springer berlin 
celeux classification em algorithm clustering stochastic versions computational statistics data analysis 
cohn hofmann missing link probabilistic model document content hypertext connectivity leen dietterich tresp eds advances neural information processing systems mit press cambridge ma 
cover thomas elements information theory wiley new york 
fisher interpretation contingency tables calculation journal royal statistical society 
freeman note exact treatment contingency goodness fit problems significance biometrika 
application symmetric dirichlet distributions mixtures contingency tables annals statistics 
hastie tibshirani discriminant analysis gaussian mixtures journal royal statistical society 
hastie tibshirani buja flexible discriminant mixture models kay titterington eds neural networks statistics oxford university press oxford 
hofmann unsupervised learning probabilistic latent semantic analysis machine learning 
kaski sinkkonen principle learning metrics data analysis journal vlsi signal processing systems signal image video technology special issue machine learning signal processing 
kaski sinkkonen bankruptcy analysis self organizing maps learning metrics ieee transactions neural networks :10.1.1.26.6383
kohonen self organizing maps rd edition springer berlin 
miller uyar mixture experts classifier learning labelled unlabelled data mozer jordan petsche eds advances neural information processing systems mit press cambridge ma 
ng jordan discriminative vs generative classifiers comparison logistic regression naive bayes dietterich becker ghahramani eds advances neural information processing systems mit press cambridge ma 
pereira tishby lee distributional clustering english words proceedings th annual meeting association computational linguistics acl columbus oh 
sinkkonen kaski clustering conditional distributions auxiliary space neural computation :10.1.1.28.7256
slonim tishby agglomerative information bottleneck solla leen 
ller eds advances neural information processing systems mit press cambridge ma 
slonim weiss maximum likelihood information bottleneck becker thrun obermayer eds advances neural information processing systems mit press cambridge ma 
szummer jaakkola kernel expansions unlabeled examples leen dietterich tresp eds advances neural information processing systems mit press cambridge ma 
timit cd rom prototype version darpa timit acoustic phonetic speech database 
tishby pereira bialek information bottleneck method th annual allerton conference communication control computing urbana illinois 

