fast light boosting adaptive mining data streams fang chu carlo zaniolo university california los angeles ca usa zaniolo cs ucla edu 
supporting continuous mining queries data streams requires algorithms fast ii light demands memory resources iii easily adapt concept drift 
propose novel boosting ensemble method achieves objectives 
technique dynamic sample weight assignment scheme achieves accuracy traditional boosting requiring multiple passes data 
technique assures faster learning competitive accuracy simpler base models 
scheme extended handle concept drift change detection 
change detection approach aims significant data changes cause serious deterioration ensemble performance replaces obsolete ensemble built scratch 
experimental results confirm advantages adaptive boosting scheme previous approaches 
keywords stream data mining adaptive boosting ensembles change detection substantial amount focused continuous mining data streams :10.1.1.119.3124:10.1.1.14.4071
typical applications include network traffic monitoring credit card fraud detection sensor network management systems 
challenges posed data increasing amount speed constantly evolving concepts underlying data 
fundamental issues addressed continuous mining attempt 
performance issue 
constrained requirement line response limited computation memory resources continuous data stream mining conform criteria learning done fast preferably pass data algorithms light demands memory resources storage intermediate results final decision models 
fast light requirements exclude high cost algorithms support vector machines decision trees nodes preferably replaced fewer nodes base decision models 
adaptation issue 
traditional learning tasks data stationary 
underlying concept maps features class labels unchanging 
context data streams concept may drift due gradual sudden changes external environment increases network traffic failures sensors 
fact mining changes considered core issues data stream mining 
dai srikant zhang eds pakdd lnai pp 

springer verlag berlin heidelberg fast light boosting adaptive mining data streams focus continuous learning tasks propose novel adaptive boosting ensemble method solve problems 
general ensemble methods combine predictions multiple base models learned learning algorithm called base learner 
method propose simple base models decision trees nodes achieve fast light learning 
simple models weak predictive models exploit boosting technique improve ensemble performance 
traditional boosting modified handle data streams retaining essential idea dynamic sample weight assignment eliminating requirement multiple passes data 
extended handle concept drift change detection 
change detection aims significant changes cause serious deterioration ensemble performance 
awareness changes possible build active learning system adapts changes promptly 

ensemble methods hardly approach continuous learning 
domingos devised novel decision tree algorithm hoeffding tree performs asymptotically better batched version :10.1.1.119.3124
extended attempt handle concept drift 
hoeffding tree algorithms need large training set order reach fair performance unsuitable situations featuring frequent changes 
designed incremental support vector machine algorithm continuous learning 
related boosting ensembles data streams 
fern proposed online boosting ensembles studied online bagging online boosting 
frank boosting scheme similar boosting scheme 
took concept drift consideration 
previous ensemble methods drifting data streams primarily relied bagging style techniques :10.1.1.14.4071
street gave ensemble algorithm builds classifier data block independently 
adaptability relies solely retiring old classifiers time 
wang similar ensemble building method 
algorithm tries adapt changes assigning weights classifiers proportional accuracy data block 
algorithms related call bagging weighted bagging respectively experimental comparison 
organized follows 
adaptive boosting ensemble method section followed change detection technique section 
section contains experimental design evaluation results conclude section 
adaptive boosting ensembles boosting ensemble method learning procedure provides number formal guarantees 
freund schapire proved number positive results generalization performance 
importantly friedman showed boosting particularly effective base models simple 
desirable fast light ensemble learning steam data 
name bagging derives analogy traditional bagging ensembles 
chu zaniolo algorithm adaptive boosting ensemble algorithm ensure maintaining boosting ensemble eb classifiers cm new block bj xn yn yi compute ensemble prediction sample eb xi round ck xi change detection eb change detected 
eb compute error rate eb bj ej set new sample weight wi ej ej eb xi yi wi set wi learn new classifier cm weighted block bj weights wi update eb add cm retire original form boosting algorithm assumes static training set 
earlier classifiers increase weights misclassified samples classifiers focus 
typical boosting ensemble usually contains hundreds classifiers 
lengthy learning procedure apply data streams limited storage continuous incoming data 
past data stay long making place new data 
light boosting algorithm requires passes data 
time designed retain essential idea boosting dynamic sample weights modification 
algorithm summary boosting process 
data continuously flows broken blocks equal size 
block bj scanned twice 
pass assign sample weights way corresponding adaboost 
ensemble error rate ej weight misclassified sample xi adjusted wi ej ej 
weight correctly classified sample left unchanged 
weights normalized valid distribution 
second pass classifier constructed weighted training block 
system keeps classifiers traditional scheme combine predictions base models averaging probability predictions selecting class highest probability 
algorithm binary classification easily extended multi class problems 
adaptability note step called change detection line algorithm 
distinguished feature boosting ensemble guarantees ensemble adapt promptly changes 
change detection conducted block 
details detect changes section 
ensemble scheme achieves adaptability actively detecting changes discarding old ensemble alarm change raised 
previous learning algorithm scheme 
argument old classifiers tuned new concept assigning different weights 
hypothesis borne experiment obsolete classifiers bad effects ensemble performance weighed 
propose learn new ensemble fast light boosting adaptive mining data streams scratch changes occur 
slow learning concern base learner fast light boosting ensures high accuracy 
main challenge detect changes low false alarm rate 
change detection section propose technique change detection framework statistical decision theory 
objective detect changes cause significant deterioration ensemble performance tolerating minor changes due random noise 
view ensemble performance random variable 
data stationary fairly uniform ensemble performance fluctuations caused random noise normally assumed follow gaussian distribution data changes obsolete classifiers kept ensemble performance undergo types decreases 
case abrupt change distribution change gaussian 
shown 
situation underlying concept constant small shifts 
cause ensemble performance deteriorate gradually shown 
goal detect types significant changes 
fig 

types significant changes 
type abrupt changes type ii gradual changes period time 
changes aim detect 
change detection algorithm certain form hypothesis test 
decision change occurred choose competing hypotheses null hypothesis alternative hypothesis corresponding decision change change respectively 
suppose ensemble accuracy block conditional probability density function pdf null hypothesis alternative hypothesis known decision likelihood ratio test 
ratio compared threshold 
accepted rejected 
chosen ensure upper bound false alarm rate 
chu zaniolo consider detect possible type change 
null hypothesis change true conditional pdf assumed gaussian exp mean variance easily estimated just remember sequence alternative hypothesis true possible estimate sufficient information collected 
means long delay change detected 
order time fashion perform significance test uses 
significant test assess null hypothesis explains observed 
general likelihood ratio test equation reduced 
likelihood null hypothesis accepted rejected 
significant tests effective capturing large abrupt changes 
type ii changes perform typical hypothesis test follows 
split history sequence halves 
gaussian pdf estimated half denoted 
likelihood ratio test equation conducted 
far described techniques aiming types changes 
integrated stage method follows 
step significant test performed 
change detected hypothesis test performed second step 
stage detection method shown effective experimentally 
experimental evaluation section perform controlled study synthetic data set apply method real life application 
synthetic data set sample vector independent features xi xi 
geometrically samples points dimension unit cube 
class boundary sphere defined xi ci center sphere radius 
labelled class class 
learning task easy feature space continuous class boundary non linear 
evaluate boosting scheme extended change detection named adaptive boosting compare weighted bagging bagging 
experiments decision trees base model boosting technique principle traditional learning model 
standard algorithm modified generate small decision trees base models number terminal nodes ranging 
full grown decision trees generated comparison marked table 
average accuracy fast light boosting adaptive mining data streams decision tree terminal nodes adaptive boosting bagging fig 

performance comparison adaptive boosting vs bagging stationary data 
weighted bagging omitted performs bagging 
evaluation boosting scheme boosting scheme compared bagging ensembles stationary data 
samples randomly generated unit cube 
noise introduced training data randomly flipping class labels probability data block samples blocks total 
testing data set contains noiseless samples uniformly distributed unit cube 
ensemble classifiers maintained 
updated block evaluated test data set 
performance measured generalization accuracy averaged ensembles 
shows generalization performance 
weighted bagging omitted predictions bagging surprising result stationary data 
shows boosting scheme clearly outperforms bagging 
importantly boosting ensembles simple trees performs 
fact boosted level trees terminal nodes performance comparable bagging full size trees 
supports theoretical study boosting improves weak learners 
higher accuracy boosted weak learners observed block size ensemble size noise level 
learning gradual shifts gradual concept shifts introduced moving center class boundary adjacent blocks 
movement dimension step 
value controls level shifts small moderate sign randomly assigned 
percentage positive samples blocks ranges 
noise level set multiple runs 
average accuracies shown small shifts moderate shifts 
results settings shown table 
experiments conducted block size 
similar results obtained block sizes 
results summarized chu zaniolo average accuracy decision tree terminal nodes adaptive boosting weighted bagging bagging fig 

performance comparison ensembles data small gradual concept shifts 
average accuracy decision tree terminal nodes adaptive boosting weighted bagging bagging fig 

performance comparison ensembles data moderate gradual concept shifts 
table 
performance comparison ensembles data varying levels concept shifts 
top accuracies shown bold fonts 
adaptive boosting weighted bagging bagging adaptive boosting outperforms bagging methods time demonstrating benefits change detection technique boosting especially effective simple trees terminal nodes achieving performance compatible better bagging ensembles large trees 
learning abrupt shifts study learning abrupt shifts sets experiments 
abrupt concept shifts introduced blocks abrupt shifts occur block 
fast light boosting adaptive mining data streams table 
performance comparison ensembles data abrupt shifts mixed shifts 
top accuracies shown bold fonts 
adaptive boosting weighted bagging bagging accuracy adaptive boosting weighted bagging bagging data blocks fig 

performance comparison ensembles data abrupt shifts 
base decision trees terminal nodes 
set experiments data stays stationary blocks 
set small shifts mixed adjacent blocks 
concept drift parameters set abrupt shifts small shifts 
show experiments base decision trees terminal nodes 
clearly bagging ensembles empirical weighting scheme seriously impaired changing points 
hypothesis obsolete classifiers detrimental performance weighed proved experimentally 
adaptive boosting ensemble hand able respond promptly abrupt changes explicit change detection efforts 
base models different sizes show results table 
accuracy averaged blocks run 
experiments real life data subsection verify algorithm real life data containing credit card transactions 
data features including transaction amount time transaction task predict fraudulent transactions 
detailed data description 
part data contains transaction chu zaniolo accuracy adaptive boosting weighted bagging bagging data blocks fig 

performance comparison ensembles data abrupt small shifts 
base decision trees terminal nodes 
transaction amount 
concept drift simulated sorting transactions changes transaction amount 
accuracy adaptive boosting weighted bagging bagging data blocks fig 

performance comparison ensembles credit card data 
concept shifts simulated sorting transactions transaction amount 
study ensemble performance varying block sizes different base models decision trees terminal nodes full size trees 
show experiment block size base models terminal nodes 
curve shows dramatic drops accuracy bagging weighted bagging small adaptive boosting 
drops occur transaction amount jumps 
boosting fast light boosting adaptive mining data streams ensemble better 
true experiments details omitted due space limit 
boosting scheme fastest 
training time affected size base models 
due fact base models tend simple structures just decision stumps level decision trees 
hand training time bagging methods increases dramatically base decision trees grow larger 
example base decision tree weighted bagging takes times longer training produces tree times larger average 
comparison conducted mhz pentium processor 
details shown 
summarize real application experiment confirms advantages boosting ensemble methods fast light adaptability 
fig 

comparison adaptive boosting weighted bagging terms building time average decision tree size 
total amount data fixed different block sizes 
summary propose adaptive boosting ensemble method different previous aspects boost simple base models build effective ensembles competitive accuracy propose change detection technique actively adapt changes underlying concept 
compare adaptive boosting ensemble methods bagging ensemble methods extensive experiments 
results synthetic real life data set show method faster demands memory adaptive accurate 
current method improved aspects 
example study trend underlying concept limited detection significant changes 
changes detected finer scale new classifiers need built changes trivial training time saved loss accuracy 
plan study classifier weighting scheme improve ensemble accuracy 
chu zaniolo 
breiman 
bagging predictors 
icml 

dietterich 
ensemble methods machine learning 
multiple classifier systems 

gunopulos 
incremental support vector machine construction 
icdm 

domingos hulten 
mining high speed data streams 
acm sigkdd 

dong jiawei han laks lakshmanan jian pei wang philip yu 
online mining changes data streams research problems preliminary results 
acm sigmod 

fern givan 
online ensemble learning empirical study 
icml 

frank holmes hall 
racing committees large datasets 
discovery science 

freund schapire 
experiments new boosting algorithm 
icml 

friedman hastie tibshirani 
additive logistic regression statistical view boosting 
annals statistics 

ganti gehrke ramakrishnan andw 
loh 
mining data streams block evolution 
sigkdd explorations 

hulten spencer domingos 
mining time changing data streams 
acm sigkdd 

russell 
experimental comparisons online batch versions bagging boosting 
acm sigkdd 

schapire freund bartlett 
boosting margin new explanation effectiveness voting methods 
icml 

stolfo fan lee prodromidis chan 
credit card fraud detection meta learning issues initial results 
aaai workshop fraud detection risk management 

street kim 
streaming ensemble algorithm sea large scale classification 
acm sigkdd 

wang fan yu han 
mining concept drifting data streams ensemble classifiers 
acm sigkdd 

widmer kubat 
learning presence concept drift hidden contexts 
machine learning 
