valid generalization size weights important size network peter bartlett department systems engineering research school information sciences engineering australian national university canberra australia peter bartlett anu edu au shows large neural network pattern classification problem learning algorithm finds network small weights small squared error training patterns generalization performance depends size weights number weights 
specifically consider layer feed forward network sigmoid units sum magnitudes weights associated unit bounded misclassification probability converges error estimate closely related squared error training set rate ca log ignoring log factors number training patterns input dimension constant 
may explain generalization performance neural networks particularly number training examples considerably smaller number weights 
supports heuristics weight decay early stopping attempt keep weights small training 
results statistical learning theory give bounds number training examples necessary satisfactory generalization performance classification problems terms vapnik chervonenkis dimension class functions learning system see example 
baum haussler results give sample size bounds multi layer threshold networks grow quickly number weights see 
pattern classification applications vc bounds loose neural networks perform successfully training sets considerably smaller number weights 
shows classification problems neural networks perform weights big size weights determines generalization performance 
contrast function classes algorithms considered vc theory neural networks binary classification problems real valued outputs learning algorithms typically attempt minimize squared error network output training set 
encouraging correct classification tends push output away zero target values gamma 
easy see total squared error hypothesis examples gamma ff examples hypothesis incorrect sign magnitude ff 
section gives misclassification probability bounds hypotheses distinctly correct way examples 
bounds terms scale sensitive version vc dimension called fat shattering dimension 
section gives bounds dimension feedforward sigmoid networks imply main results 
proofs sketched section 
full proofs full version 
notation bounds misclassification probability denote space input patterns space labels gamma 
assume probability distribution product space theta gamma reflects relative frequency different input patterns relative frequency expert classification patterns 
learning algorithm uses class real valued functions called hypothesis class hypothesis correct example sgn sgn ff 
gamma takes value iff ff misclassification probability error defined er theta gamma sgn yg crucial quantity determining misclassification probability fat shattering dimension hypothesis class say sequence points shattered functions give classifications sequence 
gamma satisfying sgn vc dimension defined size largest shattered sequence 
scale parameter fl say sequence points fl shattered sequence real values gamma satisfying gamma fl 
fat shattering dimension fl denoted fat fl size largest fl shattered sequence 
dimension reflects complexity functions class examined scale fl 
notice fat fl nonincreasing function fl 
theorem gives generalization error bounds terms fat fl 
related result applies case errors training set appear 
theorem define input space hypothesis class probability distribution theta gamma 
ffi fl 
probability gamma ffi training sequence xm labelled fact usual definition vc dimension class thresholded versions functions examples hypothesis satisfies er jfi jh fl sgn gj ffl fl ffi ffl fl ffi ln em log ln ffi fat fl 
comments informative compare result standard vc bound 
case bound misclassification probability er jfi sgn gj log log ffi vcdim constant 
shall see section function classes vcdim infinite fat fl finite fl example class functions computed layer neural network arbitrary number parameters constraints size parameters 
known learning algorithm error estimates constrained sample considering proportion training examples hypotheses misclassify distributions second term vc bound improved log factors 
theorem shows improved learning algorithm sample considering proportion training examples correctly classified jh fl 
possible give lower bound see full function classes considered shows theorem improved log factors 
idea magnitudes values give precise estimate generalization performance proposed vapnik developed vapnik workers 
case linear hypothesis classes 
results give bounds misclassification probability test sample terms values training test data 
result extended give bounds misclassification probability unseen data terms values training examples 
extended general function classes give error bounds applicable hypothesis errors training examples 
lugosi pint er obtained bounds misclassification probability terms similar properties class functions containing true regression function conditional expectation 
results extend case true regression function class real valued functions estimator 
unnatural quantity fl specified advance theorem depends examples 
full gives similar result statement uniform values quantity 
fat shattering dimension neural networks bounds vc dimension various neural network classes established see review linear number parameters 
section give bounds fat shattering dimension neural network classes 
assume input space subset define sigmoid unit function parametrized vector weights unit computes 
oe delta oe fixed bounded function satisfying condition 
simplicity ignore offset parameter 
equivalent including extra input constant value 
multi layer feed forward sigmoid network depth network sigmoid units single output unit arranged layered structure layers output unit passes inputs units layers 
consider networks weights bounded 
relevant norm norm vector define kwk jw result gives bound dimension bounded linear combination real valued functions terms fat shattering dimension basis function class 
apply result recursive fashion give bounds single output feed forward networks 
theorem class functions map gammam gammaf define class weight bounded linear combinations functions kwk suppose fl fat fl 
fat fl cm fl log mad fl constant koiran shown fat shattering dimension class layer networks bounded output weights linear threshold hidden units gamma fl log fl delta special case theorem improves result 
notice fat shattering dimension function class changed constant factor compose functions fixed function satisfying lipschitz condition standard sigmoid function 
fx 
fat fl log fl 
fx 
delta fat fl fl 
observations theorem give corollary 
delta notation suppresses log factors 
formally ff ff corollary class layer sigmoid networks weights output unit satisfying kwk fat fl gamma fl delta fx kxk bg hidden unit weights bounded fat fl gamma log fl delta applying theorem result gives result deeper networks 
notice constraint number hidden units layer total magnitude weights associated processing unit 
corollary constant class depth sigmoid networks weight vector associated unit layer satisfies kwk fat fl gamma ca gamma fl gamma delta fx kxk bg weights layer units satisfy kwk fat fl gamma ca fl log delta part corollary network fat shattering dimension similar vc dimension linear network 
formalizes intuition weights small network operates linear part sigmoid behaves linear network 
comments consider depth sigmoid network bounded weights 
corollary theorem imply training sample size grows roughly ffl misclassification probability network ffl proportion training examples network classifies distinctly correct 
results give plausible explanation generalization performance neural networks 
applications networks units small weights small squared error training examples vc dimension number parameters important magnitude weights generalization performance 
possible give version theorem probability bound uniform values complexity parameter indexing function classes technique mentioned section 
case sigmoid network classes indexed weight bound minimizing resulting bound misclassification probability equivalent minimizing sum sample error term penalty term involving weight bound 
supports popular heuristic techniques weight decay early stopping see example aim minimize squared error maintaining small weights 
techniques give bounds fat shattering dimension generalization performance function class expressed bounded number compositions bounded weight linear combinations scalar lipschitz functions functions class finite fat shattering dimension 
includes example radial basis function networks 
proofs proof sketch theorem ae pseudometric space set ffl cover ae ffl 
define ffl ae size smallest ffl cover xm define pseudometric set functions defined max jf gamma set functions denote max ffl ffl 
alon obtained bound terms fat shattering dimension :10.1.1.21.996
lemma class functions map ng bg fat log log nb log gamma delta provided log gamma delta fl define fl 
gammafl fl piecewise linear squashing function satisfying fl ff fl ff fl fl ff gammafl ff gammafl fl ff ff 
class real valued functions define fl set compositions fl functions lemma ffi fl theorem er ln fl fl ffi jfi jh fl sgn gj ffi proof lemma relies observation ae er ffl jfi jh fl sgn gj oe ae fl gamma fl ffl jfi fl fly gj oe standard symmetrization argument permutation argument introduced vapnik chervonenkis bound probability probability random permutation double length sample related property holds 
fixed sample pollard approach approximating hypothesis class fl cover case appropriate cover respect pseudometric 
applying hoeffding inequality gives lemma 
prove theorem need bound covering numbers terms dimension 
easy apply lemma quantized version function class get bound advantage range constraint imposed squashing function fl proof sketch theorem xm define pseudometric class functions defined jf gamma similarly define gamma gamma delta set functions defined denote max fl fl similarly fl 
idea proof theorem derive general upper bound covering number class apply result implicit proof theorem give bound fat shattering dimension 
lemma class valued functions satisfying fat fl log fl 
derive upper bound fl start bound lemma implies covering number fl class hidden unit functions 
implies bound covering number provided satisfies condition required lemma turns theorem trivial 
log fl log dfl log mm fl result approximation barron attributes 
lemma suppose hilbert space kfk element convex closure gamma kfk functions ff fl fl flf gamma fl fl fl implies element approximated particular accuracy respect fixed linear combination small number elements follows construct cover cover lemma inequality shows log fl fl log emma dfl log mm fl jensen inequality implies gives bound fl 
comparing lower bound lemma solving gives result 
refined analysis neural network case involves bounding successive layers solving give bound fat shattering dimension network 
andrew barron jonathan baxter mike jordan adam kowalczyk wee sun lee phil long john shawe taylor robert helpful discussions comments 
alon ben david cesa bianchi haussler :10.1.1.21.996
scale sensitive dimensions uniform convergence learnability 
proceedings ieee symposium foundations computer science 
ieee press 
bartlett 
sample complexity pattern classification neural networks size weights important size network 
technical report department systems engineering australian national university 
available anonymous ftp anu edu au pub peter tr ps 
bartlett kulkarni posner 
covering numbers realvalued function classes 
technical report australian national university princeton university 
baum haussler 
size net gives valid generalization 
neural computation 
blumer ehrenfeucht haussler warmuth 
learnability vapnik chervonenkis dimension 
acm 
koiran 
approximation learning convex superpositions 
computational learning theory eurocolt 
haussler 
decision theoretic generalizations pac model neural net learning applications 
inform 
comput 
hertz krogh palmer 
theory neural computation 
addison wesley 
lugosi pint er 
data dependent skeleton estimate learning 
proc 
th annu 
conference comput 
learning theory 
acm press new york ny 
maass 
vapnik chervonenkis dimension neural nets 
arbib editor handbook brain theory neural networks pages 
mit press cambridge 
shawe taylor bartlett williamson anthony 
framework structural risk minimisation 
proc 
th annu 
conference comput 
learning theory 
acm press new york ny 
shawe taylor bartlett williamson anthony 
structural risk minimization data dependent hierarchies 
technical report 
vapnik 
estimation dependences empirical data 
springerverlag new york 
