maximum margin hierarchical multilabel classification rousu department computer science royal holloway university london tw ex united kingdom cs rhul ac uk electronics computer science university southampton bj united kingdom ss ecs soton ac uk craig saunders electronics computer science university southampton bj united kingdom cjs ecs soton ac uk john shawe taylor electronics computer science university southampton bj united kingdom jst ecs soton ac uk progress maximum margin hierarchical classification objects allowed belong category time 
classification hierarchy represented markov network equipped exponential family defined edges 
variation maximum margin multilabel learning framework suited hierarchical classification task allows efficient implementation gradient methods 
compare behaviour proposed method introduced hierarchical regularized squares classifier svm variants reuter news article classification 
hierarchical classification object classified assumed belong exactly leaf node hierarchy 
consider general case single object classified categories hierarchy specific multilabel union partial paths hierarchy 
example news article david victoria belong partial paths sport football entertainment music belong leaf categories champions league jazz 
setting training data xi xi consists pairs vector rn multilabel consisting 
model class exponential family exp ye exp defined edges markov network node corresponds th component multilabel edges correspond corresponding author 
classification hierarchy input 
ye yj denote restriction multilabel yk edge 
vector turn concatenation class sensitive feature vectors ye ye denotes indicator function weight vector edge vector bag words experiments reported feature representation document note feature vector duplicated edge edge labeling weight vector ue ue separate weights represent importance differences feature different contexts :10.1.1.7.8542
ways define loss functions hierarchical classification setting 
zero loss suited task ignores severity discrepancy symmetric difference loss yi ui suffer deficiency 
fails take dependency structure account 
choice hierarchical loss function 
penalizes mistake path path cj yj uj yk uk anc coefficients cj cpa weight mistakes deeper hierarchy 
denoted anc ancestor pa immediate parent set siblings node consider simplified version path edge cj yj uj upa penalizes mistake child label parent correct 
choice lets loss function capture hierarchical dependencies parent child allows define loss terms edges crucial efficiency learning algorithm 
achieved dividing loss node edges adjacent 
goal learn weight vector maximizes minimum margin training data correct multilabel xi incorrect xi :10.1.1.129.8439:10.1.1.151.3243
margin scale function loss 
single slack variable training example results soft margin optimization problem min xi yi xi xi yi xi 
optimization problem suffers possible high dimensionality feature vectors 
dual problem max xi kernel matrix xi loss vector allows circumvent problem feature vectors :10.1.1.7.8542:10.1.1.129.8439
number dual variables xi exponential number due exponential number constraints polynomial number dual variables suffices approximate solution :10.1.1.151.3243
loss functions edge marginalization trick obtain polynomial sized dual optimization problem dual variables xi ye ue ye xi seen edge marginals original dual variables 
arithmetic manipulation optimization problem takes form max xi ue xi ue ue xi ue ke xi ue xj xj ue xi ue xi ue 
ke edge kernel defined ke xi ue xj xi ue xj 
formulation closely related described differences pointed 
firstly assign loss edges able richer loss functions simple 
secondly single node marginal dual variables redundant box constraints constraint set marginal consistency constraints constraint sets terms edges 
utilize fact cross edge kernel values zero feature vectors described 
considerably smaller optimization problem challenging solve 
efficient reformulation problem able arrive tractable problem 
notice box constraints marginal consistency constraints defined separately suggest possible decomposition 
unfortunately objective decompose similarly certainly exists non zero kernel values ye gradient objective ke decomposed training examples 
gives possibility conduct iterative constrained gradient ascent subspaces examples gradient update single relatively cheap expensive gradient update needs performed moving example 
working set approach chunk worst predicted training added current working set immediately suggests 
algorithms implemented relatively low memory footprint account special structure feature vectors repeat times 
omit details due lack space 
report initial experiment hierarchical classification reuter newswire articles 
reuters corpus volume rcv 
documents training testing 
document corpus associated topic codes arranged forest trees 
experiments tree corresponding family categories total nodes compared performance learning approach denoted algorithms svm denotes svm trained separately svm denotes case svm trained examples ancestor labels positive rls algorithm described 
training notation meant indicate ensure consistency suffices pair edge pairs need considered symmetric difference loss edge loss edge 
training svm svm losses produce learned model 
value experiments 
noted due time constraints models trained convergence duality gap objective value 
took hours cpu time high pc running matlab 
svm rls obtain reasonable prediction accuracy necessary postprocess predicted labeling follows start root traverse tree breadth fashion 
label node predicted descendants node labelled negatively reasoning algorithm coarse grained decisions punish inability fine ones 
algorithms postprocessing minor effect results shown postprocessing 
results comparison shown table 
flat svm inferior competing algorithms utilize dependencies way 
terms zero loss performs best rls lowest slightly lower path loss competitors 
experiment edge path equal cases 
due levels hierarchy 
algorithm path edge svm svm rls edge initial experiments conclude method promising prediction accuracies differ algorithms 
plan extend experimental studies time workshop board document 
choice loss function far clear experiments 
example scaling path edge losses done ways examined 
altun hofmann tsochantaridis :10.1.1.7.8542
hidden markov support vector machines 
icml pages 
cai hofmann 
hierarchical document categorization support vector machines 
acm cikm 
cesa bianchi gentile 
incremental algorithms hierarchical classification 
neural information processing systems 
dekel keshet singer 
large margin hierarchical classification 
icml pages 
hofmann cai ciaramita 
learning taxonomies classifying documents words 
nips workshop syntax semantics statistics 
david lewis yang tony rose fan li 
rcv new benchmark collection text categorization research 
jmlr apr 
taskar guestrin koller 
max margin markov networks 
neural information processing systems 
tsochantaridis hofmann joachims altun :10.1.1.151.3243
support vector machine learning interdependent structured output spaces 
icml pages 
