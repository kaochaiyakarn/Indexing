convexity surrogate functions iterative optimization multi class logistic regression models zhang james kwok dit yan yeung gang wang department computer science hong kong university science technology clear water bay kowloon hong kong cs ust hk 
propose family surrogate maximization sm algorithms multi class logistic regression models called conditional exponential models 
sm algorithm aims turning intractable maximization problem tractable iterating steps 
step computes tractable surrogate function substitute original objective function step seeks maximize surrogate function 
apply sm algorithms logistic regression models leading standard sm generalized sm gradient sm quadratic sm algorithms 
compared newton method sm algorithms dramatically save computational costs dimensionality number data samples huge 
demonstrate efficacy sm algorithms compare empirical performance text categorization 
machine learning statistics problems involve maximization minimization optimization techniques important solving 
objective function widely optimization process log likelihood function 
closely related convex concave functions convexity plays central role 
example known expectation maximization em algorithm derived jensen inequality concavity logarithm function 
example optimization transfer algorithm known surrogate maximization sm algorithm optimizes function serving surrogate original objective function 
surrogate function constructed invoking convexity arguments 
sm algorithm effective intractable complicated optimization problem tractable significantly simpler 
example decouple correlation parameters estimate parameters parallel avoid computational burden inverting large matrices associated newton method 
sm algorithm enjoys local convergence properties standard em algorithm 
attempt demonstrate power potential sm algorithms machine learning logistic regression models specific example 
particular address major issues devising sm algorithms surrogate function defined resultant surrogate function maximized 
problem exist main approaches jensen inequality order taylor approximation low quadratic bound principle 
approaches follow readily properties convex functions third uses quadratic function approximate original objective function 
second problem general different maximization methods required different surrogate functions 
leads standard sm generalized sm gradient sm quadratic sm algorithms detailed sequel 
logistic regression models called conditional exponential models machine learning community closely related maximum entropy principle :10.1.1.43.7345
successfully applied data mining information retrieval problems 
appealing characteristic logistic regression models predict class labels combining evidences correlated input features 
certain extent resembles boosting improves accuracy learning algorithm combining weak learners form powerful committee ensemble 
fact pioneering established relationship boosting logistic regression models developed 
training data set consider problem finding optimal logistic regression model different sm algorithms 
popular solution generalized iterative scaling gis variants improved iterative scaling iis faster iterative scaling fis algorithms 
basic idea methods approximate intractable log likelihood function conditional exponential model auxiliary function model parameters decoupled 
consequently tractable optimize auxiliary function log likelihood function 
see surrogate functions play role auxiliary functions iterative scaling 
difference lies fact auxiliary function approximate difference log likelihood function values computed parameters consecutive iterations 
optimization methods bregman distance proposed 
bregman distance optimization algorithms order taylor expansion convex function argument function 
algorithms share common principles sm algorithms 
rest organized follows 
sections give brief overviews sm algorithms logistic regression models respectively 
section devise different sm algorithms logistic regression models 
section presents experimental results section gives concluding remarks 
surrogate maximization algorithm consider general problem maximizing arbitrary function parameter 
estimate tth iteration sm algorithm consists steps surrogate step step substitute surrogate function equality holding 
maximization step step obtain parameter estimate maximizing surrogate function arg max 
clearly construction surrogate function key sm algorithm turning intractable optimization problem tractable 
hand closer surrogate function efficient sm algorithm 
hand surrogate function preferably closed form solution step 
invoking convexity arguments general principle providing guidelines constructing surrogate functions specific examples special cases discussed 
depending context relies important techniques jensen inequality order taylor approximation low quadratic bound principle 
combinations methods 
depending surrogate functions obtained different sm algorithms devised accordingly 
standard sm algorithm closed form solution step exists 
standard sm algorithm enjoys local convergence properties standard em algorithm 
possible obtain closed form solution step 
spirit generalized em algorithm devise generalized sm algorithm 
maximizing attempt find 
obviously generalized sm algorithm locally convergent 
alternatively spirit gradient em algorithm may devise gradient sm algorithm 
application low quadratic bound principle mentioned leads quadratic sm algorithm 
logistic regression models logistic regression model conditional likelihood usually exp jfj fj stands jth feature extracted input output class label real valued weight measuring importance feature fj technically considered lagrange multiplier corresponding fj certain constrained optimization problem 
normalization term enforces sum different class labels unity 
words exp jfj focus multi class problem 

xn yn finite set training examples instance xk comes domain instance space yk 
corresponding class label possible labels 
assume set real valued feature functions 
fm line assume classes share set features fj xk leading simplified parametric model yk xk xk xk exp xk ij 
sequel write matrix form ij training procedure aims finding set weights ij maximizes log likelihood training data 
empirical joint density estimated training data log likelihood expressed xk ln xk xk xk xk ln xk xk xk xk xk ln xk empirical density input 
im ti xk 

fm denote lk xk xk ln xk lk 
intractable directly optimize proposed improved iterative scaling iis algorithm modified version generalized iterative scaling algorithm 
faster iterative scaling fis algorithm proposed problem 
ij ij parameter values consecutive iterations 
iis fis proceed replacing lower bound auxiliary function optimized ij ij ij ij 
typically auxiliary function decouples correlation parameters ij optimization problem tractable 
general exist auxiliary functions reasonable criterion successfully fis choosing auxiliary function measuring closeness original objective function 
sm algorithms logistic regression models section employ surrogate function construction methods discussed section devise sm algorithms logistic regression models 
loss generality assume fi xk fi xk 
standard sm algorithm section combination jensen inequality taylor approximation construct surrogate function 
order taylor approximation concave function ln ln xk ln xk xk xk 
follows jensen inequality convex function exp xk fj xk ij ij fj xk 
substituting inequalities back leads surrogate function qm xk xk xk ln ij fj xk xk xk fj xk ij ij 
alternatively may apply taylor approximation jensen inequality reverse order 
easy show log sum exp function ln convex 
jensen inequality function ln xk ln fj xk ij ij xk fj xk xk fj xk ln ij ij xk fj xk ln xk 
features fi xk prespecified conditions easily satisfied transformation fi xk fi xk fj xk 
apply order taylor approximation concave function ln obtain ln ij ij xk ln xk xk ij ij 
substituting back leads surrogate function 
consider maximizing qm required step 
setting derivative qm xk fj xk xk xk fj xk ij ij ij zero obtain closed form solution step update ij ij ln generalized sm algorithm xk xk fj xk xk xk 
fj xk note leads surrogate functions qj xk xk xk fj xk ln xk xk fj xk ln ij ij xk qt xk xk xk ln xk xk xk xk 
closed form solutions arg max qj arg max qt 
result easy show qj qm qt qm 
qj qm qm qj qt qm qm qt 
iterative algorithm generalized sm algorithm qj qt 
gradient sm algorithms mentioned section generalized sm algorithm surrogate functions qt qj devise gradient sm algorithm 
see qm completely decouples correlation ij qt qj decouple correlation ij column row directions respectively 
qt xk xk xk xk fi qt xk xk xk xk hi iterative procedure resultant gradient sm algorithm qt hi fi 

similarly denoting 
cj obtain qj qj xk fj xk qk qk xk fj xk diag qk qk qk xk 
xk qk xk 
xk gradient sm algorithm qj updates current parameter estimate 

note matrices hi sizes respectively 
order update gradient step needs invert matrices needs invert matrices 
quadratic sm algorithm objective function fisher score vector second derivative hessian matrix lindsay proposed quadratic lower bound algorithm assumption negative definite matrix 
define means positive semi definite 
surrogate function 
clear attains minimum 
quadratic function convexity shows maximum 
sm algorithm seeks approximate maximum iterative procedure 
tth estimate denoted maximizing yields th estimate 
shows quadratic lower bound algorithm amounts maximizing newton method hessian matrix replaced 


cm lk xk xk fj xk ij gives rise score vector lk qk qk xk symbol stands kronecker product arbitrary matrices 
addition second partial derivative lk computed lk ij leading hessian matrix xk xk fj xk fj xk xk xk fj xk fj xk lk diag qk xk xk 
xk lk xk diag qk xk xk 
leads newton method 
inequality diag qk matrix column vector ones obtain xk xk xk xk xk xk df fj xk diag 
xn 
iterative procedure resolving xk qk qk xk 
see algorithm described just standard sm algorithm surrogate function qq 
refer quadratic sm algorithm derivation low quadratic bound principle 
note quadratic sm newton method necessary require fi xk fi xk 
dft superscript stands moore penrose inverse 
hand easy obtain xk qk qk xk vec vec qt xk xk qk qk notation vec denotes st vector formed stacking columns matrix xk qt xk properties vec operation vec df vec qt vec df qt rewrite iterative procedure matrix form df qt 
note cm cm inversion cm cm matrix required iterations 
hand inverses matrix matrix required 
clearly efficient especially cm large 
note newton method defined efficient iterative equation derived 
discussions experimental results see surrogate function objective function unique 
section different approaches outlined section different sm algorithms corresponding different surrogate functions devised 
approaches separately useful 
hand see standard sm algorithm surrogate function time generalized sm algorithm surrogate function 
interested criteria guide design surrogate functions 
intuitively criterion closeness surrogate function original objective function 
example closer surrogate function objective function better 
possible criterion tractability step 
example closed form update equation desirable 
words want surrogate function efficient effective 
practice tradeoff criteria 
demonstrate tradeoff sm algorithms text categorization task webkb data set 
data set contains web pages gathered computer science departments universities 
pages divided categories 
experiments populous entity representing categories student faculty course project resulting total pages 
information gain features selected 
satisfy condition define feature fj xk nj xk xk nj xk number occurrences feature document xk xk total number occurrences features document xk 
experiments data randomly sampled training remaining testing 
training example xk xk set yk 
initial values ij set zero 
sm algorithms sm standard sm algorithm sm gradient sm algorithm sm gradient sm algorithm sm quadratic sm algorithm newton method compared 
implementations matlab experiments run sun microsystems ultra sparc iii mhz cpu mb cache gb ram 
shows results algorithms newton method 
note axis log scale plots 
see sm sm newton method take iterations converge maximum loglikelihood value sm sm need tens iterations 
results test set classification accuracy show similar trends 
table gives cpu time methods 
recall sm required invert matrix matrix iterations 
workstation takes zero time invert matrix takes seconds invert matrix 
newton method needs invert matrix iteration 
computational cost huge 
needs lot memory matrix 
newton method ineffective problem 
see sm sm reduce computational storage demands newton method 
sm sm sm newton sm newton sm sm sm sm fig 

log likelihood loss vs number iterations testing accuracy vs number iterations 
table 
cpu time seconds required running iterations 
sm sm sm sm newton concluding remarks multi class logistic regression models specific example illustrate general principles devising sm algorithms include construction surrogate function iterative maximization 
recall sm sm essentially newton method works surrogate function qt qj 
problems large amount data data high dimensionality gene expression arrays typically samples genes newton method intractable inverse high dimensional hessian matrix required iteration 
certain extent sm sm deal problem 

berger 
improved iterative scaling algorithm gentle 
technical report 
available www cs cmu edu www ps scaling ps 

berger della pietra della pietra 
maximum entropy approach natural language processing 
computational linguistics 


multinomial logistic regression algorithm 
annals institute statistical mathematics 

lindsay 
monotonicity quadratic approximation algorithms 
annals institute statistical mathematics 

collins schapire singer 
logistic regression adaboost bregman distances 
machine learning 

craven freitag mccallum mitchell nigam slattery 
learning extract symbolic knowledge world web wide 
national conference artificial intelligence 

darroch ratcliff 
generalized iterative scaling log linear models 
annals mathematical statistics 

della pietra della pietra lafferty 
inducing features random fields 
ieee transactions pattern analysis machine intelligence 

dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society series 

friedman hastie tibshirani 
additive logistic regression statistical view boosting 
annals statistics 

horn johnson 
topics matrix analysis 
cambridge university press cambridge uk 

jin yan zhang hauptmann 
faster iterative scaling algorithm conditional exponential model 
th international conference machine learning 

lafferty 
additive models boosting inference generalized divergences 
twelfth annual conference computational learning theory pages 

lange 
gradient algorithm locally equivalent em algorithm 
journal royal statistical society series 

lange hunter yang 
optimization transfer surrogate objective functions discussion 
journal computational graphical statistics 

lebanon lafferty 
boosting maximum likelihood exponential models 
advances neural information processing systems 


meng 
discussion optimization transfer surrogate objective functions 
journal computational graphical statistics 

zhang kwok yeung 
surrogate maximization minimization algorithms adaboost logistic regression model 
th international conference machine learning 

