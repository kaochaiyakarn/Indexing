discovering problem solutions low kolmogorov complexity high generalization capability technical report jurgen schmidhuber fakultat fur informatik technische universitat munchen munchen germany informatik tu muenchen de informatik tu muenchen de html august machine learning algorithms aim finding simple rules explain training data 
expectation simpler rules better generalization test data 
occam razor 
practical implementations measures simplicity lack power universality elegance kolmogorov complexity solomonoff algorithmic probability 
likewise previous approaches especially bayesian kind suffer problem choosing appropriate priors 
addresses issues 
reviews basic concepts algorithmic complexity theory relevant machine learning solomonoff levin distribution universal prior deals prior problem 
universal prior leads probabilistic method finding algorithmically simple problem solutions high generalization capability 
method levin complexity time bounded generalization kolmogorov complexity inspired levin optimal universal search algorithm 
problem solution candidates computed efficient self sizing programs influence runtime storage size 
probabilistic search algorithm finds programs ones quickly computing algorithmically probable solutions fitting training data 
simulations focus task discovering algorithmically simple neural networks low kolmogorov complexity high generalization capability 
demonstrated method certain toy problems computationally feasible lead generalization results previous neural net algorithms 
remains done large scale applications incremental learning feasible 
keywords generalized kolmogorov complexity solomonoff levin distribution generalization universal search self sizing programs neural networks 
number 
second number 
third number 
fourth number 
fifth number 
answer 
reason law 
nth number gamma gamma iq test requires answer 

reasons simple solutions preferred complex ones 
idea referred occam razor 
assumed simpler rules better generalization test data 
makers iq test assume everybody agrees simple means 
similarly researchers agree learning algorithms ought extract simple rules explain training data 
exactly simple mean 
theory providing convincing objective criterion simplicity theory kolmogorov complexity algorithmic complexity 
contrary popular myth kolmogorov complexity due halting problem prevent machine learning applications tractable general extensions kolmogorov complexity 
machine learning researchers powerful tools provided theory 
purpose 
experiments intended demonstrate basic concepts theory kolmogorov complexity interest machine learning purposes encourage machine learning researchers study theory point limitations current state art important open problems 
outline 
section briefly reviews basic concepts algorithmic complexity theory relevant machine learning kolmogorov complexity universal prior solomonoff levin distribution probability computable object solution problem essentially equal probability guessing shortest program universal computer solomonoff theory inductive inference justifies occam razor principle minimal description length mdl general form levin complexity generalization kolmogorov complexity levin universal optimal search algorithm 
computable solution problem consider negative log probability guessing program computes plus log runtime 
levin complexity solution minimal possible value 
levin universal search algorithm essentially generates tests solution candidates set possible computable candidates order levin complexity solution 
broad class problems universal search shown optimal respect total expected search time leaving aside constant factor depend problem 
knowledge section presents general implementation probabilistic variant universal search conventional digital machine 
efficient self sizing programs influence runtime storage size 
simulations section focus task finding simple neural nets high generalization capability 
experiments toy problems demonstrate method certain cases computationally feasible lead generalization results impossible obtain traditional neural net algorithms briefly reviewed section 
promising outlook section presents preliminary experiments extensions designed incremental learning 
best solution candidate far serves basis additional improvements 
theoretical foundations developed incremental extensions appear promising improving neural net algorithms evolutionary genetic algorithms learning paradigms 
section concludes general remarks problem solving occam razor 
basic concepts relevant learning section briefly reviews basic concepts theories algorithmic probability kolmogorov complexity algorithmic complexity 
selected brief incom final section provides remarks don agree 
plete history subject section 
algorithmic complexity theory provides various different closely related measures complexity simplicity objects 
focus appears useful machine learning 
informally speaking complexity computable object length shortest program computes halts set possible programs forms prefix code 
precise consider turing machine tm tapes program tape tape output tape 
finite may grow indefinitely 
simplicity loss generality focus binary tape alphabets 
initially tape output tape consist single square filled zero 
program tape consists finitely squares filled zero 
tape scanning head 
initially scanning head tape points square 
program tape read output tape write scanning heads may shifted right square time 
tape read write scanning head may move directions 
scanning heads tape output tape shift current tape boundary additional square appended filled zero 
case program tape scanning head shifting program tape boundary considered 
moment assume happen 
tm finite number internal states initial state 
behavior specified function implemented look table 
maps current state contents square scanning head tape new state action 
actions shift left right write write output tape shift scanning head right copy contents scanning head program tape square scanning head tape shift program tape scanning head right halt 
self delimiting programs 
denote number bits bitstring consider nonempty bitstring written program tape scanning head points bit program tm iff reads bits halts 
words eventually terminating computation process head program tape incrementally moves start position 
may say carries information length 
obviously program prefix 
compiler theorem 
tm mapping bitstrings written program tape outputs written output tape computes partial function undefined halt 
known universal tm property tm exists constant prefix fc bitstrings compiler compiles programs equivalent programs follows denote self delimiting program 
kolmogorov complexity 
kolmogorov complexity algorithmic complexity algorithmic information occasionally kolmogorov chaitin complexity arbitrary bitstring denoted ku defined length shortest program producing ku min fj sg ku noncomputable halting problem solved 
comparing number possible programs bits 
number possible bitstrings greater bits 
observes strings complex random incompressible sense computed program shorter invariance theorem 
due compiler theorem ku ku universal machines may choose particular universal machine henceforth write ku 
machine learning mdl prior problem 
machine learning applications concerned problem training data select probable hypothesis generating data 
bayes formula yields select maximal 
equivalent minimizing gamma logp gamma logp logp interprete equations 
may viewed normalizing constant 
djh usually measured approximated 
classical information theory gammalog djh optimal minimal efficient code length description length gammalog minimal code length leads minimum description length mdl principle best hypothesis explaining data minimizes sum description length hypothesis description length data encoded hypothesis 
prior come 
define priori probability distribution set possible hypotheses introducing arbitrariness 
perceived prior problem bayesian approaches 
theory algorithmic probability provides solution 
universal prior 
define pu priori probability bitstring probability guessing halting program computes way guessing defined procedure initially program tape consists single square 
scanning head program tape shifts right append new square 
probability fill probability fill 
obtain pu fu jpj clearly sum probabilities halting programs exceed halting program prefix 
certain program tape contents may lead non halting computations 
different universal priors different universal machines probabilities string differ constant factor independent string size due invariance theorem constant factor corresponds probability guessing compiler 
may drop index write pu justifies name universal prior known solomonoff levin distribution 
universal priors appear convincing method assigning probabilities hypotheses computable objects advance 
algorithmic entropy 
denoted algorithmic entropy dominance shortest programs 
shown proof non trivial implies gammao gammak gammak probability guessing programs computing string probability guessing shortest program essentially equal 
probability string dominated probabilities shortest programs 
inductive inference occam razor 
occam razor prefers solutions minimal descriptions short solutions minimal descriptions longer 
modern prefix version solomonoff theory inductive inference justifies occam razor way 
suppose problem extrapolate sequence symbols bits loss generality 
observed bitstring predict bit 
si denote event followed symbol 
bayes tells going predict bit vice versa 
si si continuation lower kolmogorov complexity general 
kolmogorov complexity general universal prior 
popular myth states fact renders useless concepts kolmogorov complexity far practical machine learning concerned 
seen 
focus natural computable general extension kolmogorov complexity 
levin complexity 
slightly extend notion program 
follows program string program tape scanned completely program may lead different results depending runtime 
computable string consider log probability guessing program computes plus log corresponding runtime 
levin complexity string minimal possible value 
formally scan program program extended sense written program tape finishes printing tape 
number steps taken printed 
kt min fj log invariance theorem similar holds kt 
universal search 
suppose got problem solution represented bit string 
levin universal optimal search algorithm essentially generates evaluates strings solution candidates order kt complexity solution 
essentially equivalent enumerating programs order decreasing probabilities divided runtimes 
program computes string tested see solution problem 
search stopped 
get intuition universal search denote probability computing solution halting program program tape 
runtime 
suppose gambling scenario 
bet success bid 
minimize expected losses going bet maximal fail hit solution may bet bet 
continuing procedure going list halting programs order decreasing course usually known advance 
broad class problems including np problems time limited optimization problems universal search shown optimal respect total expected search time leaving aside constant factor independent problem size string computed time steps program probability guessing time steps systematic enumeration levin generate run time steps output experiments probabilistic algorithm strongly inspired universal search 
conditional algorithmic complexity 
complexity measures simplifications slightly general measures 
suppose universal machine starts computation process nonempty string written tape 
conditional kolmogorov complexity defined min fj computes kt defined 
variants conditional complexity determine information string conveys 
context machine learning conditional complexity measures additional information acquired known 
history selected kolmogorov founder modern axiomatic probability theory kolmogorov introduce variant complexity measure sake kolmogorov 
levin cites announcements kolmogorov lectures subject dating back 
independent earlier solomonoff come measure product algorithmic probability inductive inference preliminary version dated 
solomonoff kolmogorov observed machine independence 
today solomonoff refers kolmogorov complexity 
solomonoff 
chaitin independently published essential concepts chaitin hints provided 
important related early described martin lof acs schnorr 
apparently levin introduce analyze today standard form kolmogorov complexity halting programs prefix codes levin see acs levin levin levin 
levin proved equation 
importance prefix codes independently seen chaitin proved attributes part argument pippenger 
levin introduced kt complexity universal optimal search algorithm see 
levin levin related ideas attributed adleman see adleman 
generalizations kolmogorov complexity proposed 
hartmanis see contributions watanabe 
easily computable approximations mdl principle formulated wallace boulton rissanen 
approximations build basis current machine learning applications 
quinlan rivest gao li pednault 
barzdin referred levin related kolmogorov complexity variant godel incompleteness theorem subject central theme chaitin research chaitin :10.1.1.48.3094
theory kolmogorov complexity split subfields 
excellent overview additional details history li vitanyi book 
see cover 
see schmidhuber application fine arts 
presentation partly inspired presentations chaitin li vit anyi solomonoff :10.1.1.48.3094
probabilistic search useful self sizing programs low levin complexity levin universal search algorithm considered interest theoretical purposes see 
allender li vit anyi 
implemented experimental applications fear constant factor may large 
knowledge general universal search implemented time project led solomonoff apparently implemented restricted versions 
follows focus implementation slightly different probabilistic algorithm levin complexity strongly inspired universal search 
experimental results obtained probabilistic algorithm see section similar obtained original universal search procedure schmidhuber 
overview 
method described section searches finds algorithms compute solutions problem specified possibly limited training data 
goal discover solutions high generalization performance test data unavailable search phase 
purpose probabilistic search algorithm randomly generates programs written general assembler programming language sequences integers 
programs may influence storage size runtime 
program computes solution candidate tested training data 
probability generating program upper bound max runtime essentially equals quotient probability guessing max implies candidates low levin complexity preferred candidates high levin complexity 
measure generalization performance candidates fitting training data evaluated test data 
experiments section solution candidates weight matrices neural net supposed solve certain generalization tasks difficult impossible solve conventional neural net algorithms 
universal programming language 
need universal set primitive instructions called primitives may composed form arbitrary algorithms computing arbitrary partial recursive functions 
limitation willing accept storage size machine 
easy devise universal sets primitives 
ones prefer 
informally general constraint obey computable hardware computable just efficiently small constant factor program composed primitives 
instance typical serial digital machine primitives exploiting fast storage addressing mechanisms 
want limit simulation say slow tape turing machine 
likewise machine parallel processors set primitives allowing programs maximal parallelism 
follows focus conventional digital machines 
description set experiments described storage 
programs sequences integers 
stored storage consisting single array cells 
cell integer address interval gammas 
positive integers 
program tape set cells addresses 
tape set cells addresses gammas gamma 
cells non negative addresses belong program tape 
cells negative addresses belong tape 
contents cell address denoted maxint type integer implemented version maxint equals 
execution program portion program tape may increase 
portion tape may increase decrease 
time step variable max gamma max denotes topmost address storage 
variable min gammas min denotes smallest address 
time legal addresses dynamic range min max definition 
time integer sequence program tape address max called current program 
max gamma implies empty program 
instructions 
time variable may equal address cells contents may interpretable instruction 
ops different possible instructions implemented version ops 
instruction uniquely represented instruction number set ops gamma 
instruction may arguments type integer 
arguments stored addresses address instruction 
argument instruction legal argument range set integer values argument allowed take 
certain limits legal argument ranges dynamically modified programs seen shortly 
initialization time limits time probability 
execution program run variables min set zero 
variable define upper bound runtime current program 
obtain probabilistic variant universal search chosen randomly follows elements set drawn equal probability drawn 
denote number trials 
set theta equals time steps program allowed execute instructions may choose halt earlier 
exceeds replaced 
time probability current program defined max 
short runtimes long runtimes 
instruction cycle oracles 
single step program interpreter works follows equals max interpreted request oracle 
primitive corresponding arguments chosen randomly set legal options described 
sequentially written program tape starting 
max increased accordingly reflect growth program tape 
new primitive gets executed growth halts program 
oracle request equals content ops gamma corresponding number arguments corresponding legal argument ranges looked checked contents addresses current address 
instruction syntactically correct gets executed 
current program halted 
executed primitive change value causing jump set point address address argument current instruction 
instruction executed incremented 
reached program halted 
runs programs space probability 
initialization instruction cycle repeated halt situation encountered 
space probability program defined product probabilities parameters primitives requested executed runtime 
essentially space probability probability guessing executed content program tape 
probabilistic search 
programs generated randomly executed described results evaluated problem specific performance criterion met 
obviously results low levin complexity preferred results high levin complexity 
alternative implementation original universal search algorithm systematically generate solution candidates order levin complexities 
primitives 
instruction numbers semantics primitives experiments listed 
expression form denotes value interpreted address ith cell containing current instruction indirect addressing 
list assumes syntactical correctness instructions 
rules legal argument ranges syntactical correctness shortly 
address address address 
contents address equal contents address set equal address 
output 
primitive interaction external environment 
corresponds tm action writing output tape see section 
experiments output called 
generate weights neural network 
variants specified needed 
jump address 
set equal address 

halt current program 
add address address address 
contents address added contents address result written address 
address address 
primitive interaction external environment 
requires separate input fields may modified environment experiments equal 
reads current value ith input field address value address 
conjunction primitives changing environmental state provides opportunity exploiting computing resources outside world 
applications pretty useless input fields remain zero time 
move address address 
contents address copied address 
allocate address 
size tape increased value address new cells initialized zeros 
min updated accordingly growth gammas halts program 
variable written space allocated tape 
explained allocate essential programs 
increment address 
contents address incremented 
decrement address 
contents address decremented 
subtract address address address 
contents address subtracted contents address result written address 
multiply address address address 
contents address multiplied contents address result written address 
free address 
size tape decreased value address 
min updated accordingly 
primitive complements allocate 
rules legal argument ranges syntactical correctness 
jumps may lead address dynamic range min max recall max equals current value 
operations read contents certain cells add move may read addresses min max 
operations change contents certain cells may write tape addresses min gamma 
program tape read execute random writes requested moves 
easy seen 
tape read write execute 
results arithmetic operations leading underflow overflow replaced maxint respectively 
tape cells may allocated freed time 
comments 
universality 
difficult show primitives form universal set sense composed form programs writing computable integer sequence tape size range limitations 
note primitives easy create programs handling stacks recursion program may create executable code sub programs tape 
executable code represented sequence numbers code may modify 
scheme allows general sequential interaction environment appropriate problem specific actions translating storage contents output actions environmental changes 

self sizing programs 
set adaptation turing machine section 
designed efficient sense programs may exploit conventional digital machines fast storage addressing jumping single array cells negative addresses tape positive addresses program tape considerably simplifies primitives 
glance price pay general addressing capabilities random jumps contribute successful programs possible legal addresses jump 
programs may influence size number available legal addresses potential remain small seen 
programs influence size 
keep small avoiding requests new oracles avoiding jumps current allocate free balanced way 
oracle requests allocate free provide ways influencing number visible legal addresses available storage 
oracle requests source randomness 
current program request oracles space probability tend remain low program may perform extensive computations 
bigger storage smaller probability guessing particular visible address arguments instructions generated oracle requests 
small beautiful 


program tape read execute rerun randomly chosen halting program written program tape erasing tape making equal starting execution loop 
contents program tape completely determine contents tape time environment reacts non deterministic way 
desire able rerun programs reason way oracle requests handled 
introduced extra primitive calling random instruction appear location say program tape 
different led different oracles general 
way done oracle requests generated operation moving top program tape oracle executed immediately 
legal instruction appearing program tape blue executed 
say randomness wasted handled efficiently 
moves provoke oracle requests 
point guessing program isn 
absolute determinism environment reacts non deterministic way course 

probabilistic setting 
probabilistic search algorithm original universal search procedure 
create time limits programs randomly systematically enumerating 
reason avoid unintended bias 
instance unintended bias may introduced imposing systematic say alphabetic order programs equal quotients probability runtime 
drawback probabilistic version programs low levin complexity general tested 
speed issue prefer systematic enumeration slightly complicated probabilistic variant expected search time equals systematic enumeration 
variants systematic universal search primitives implemented collaboration norbert schmidhuber 
examples total principle possible run variant universal search neural net architecture conventional digital machine 
earlier shown different context neural nets may talk weights terms activations modify weight matrix schmidhuber schmidhuber 
self modifying capabilities form basis universal set primitives 
search time main issue simulations section probabilistic search intended highlight generalization performance speed 
similar results obtained systematic search 
application finding simple neural nets neural networks particularly studied instances generalizers see 
maass baum haussler amari murata wolpert moody pearlmutter rosenfeld barron mozer smolensky numerous :10.1.1.56.1756
reason simulations section focus task finding algorithmically simple neural networks high generalization capability 
briefly look definitions simplicity supervised neural net training algorithms 
follows solutions weight vectors neural nets 
previous algorithms making nets simple ffl weight decay 
special error term addition standard term enforcing matches desired outputs actual outputs encourages weights close zero 
idea zero weight cost bits specified simple hinton van camp 
pearlmutter hinton probably propose weight decay rumelhart suggest reducing overfitting 
variants weight decay successfully applied weigend 
krogh hertz 
ffl soft weight sharing 
nowlan hinton introduce additional objective function encouraging groups weights nearly equal values 
weights taken generated mixtures gaussians 
fewer number gaussians closer weight center gaussian higher probability fewer bits needed encode classical information theory shannon 
ffl bayesian strategies backprop nets 
mackay evaluates hyper parameters weight decay rates respect probabilities generating observed data mackay 
estimates probabilities computed basis gaussian assumptions 
ffl optimal brain surgeon 
hassibi second order information obtain simple nets pruning weights influence error minimal changing weights compensate 
see lecun related approach 
see vapnik guyon theoretical analysis 
ffl non algorithmic mdl methods gaussian priors 
minimize sum description lengths neural net errors hinton van camp assume gaussian weight priors gaussian error distributions minimize asymmetric divergence kullback distance prior posterior training 
ffl methods finding flat minima 
hochreiter schmidhuber efficient second order methods search large connected regions acceptable error minima 
corresponds simple networks low description length low expected overfitting 
ffl mutual information networks 
deco 
measure network complexity respect data measuring mutual information inputs internal representations extracted hidden units 
ffl methods removing redundant information input data 
input data compressed networks processing data smaller simpler general 
standpoint classical information theory optimal compression algorithm builds factorial code input data code statistically independent components 
barlow 
various neural methods compressing input data known 
see schmidhuber neural method designed generate factorial codes 
see atick focus visual inputs 
see schmidhuber loss free sequence compression 
see becker numerous additional 
numerous heuristic constructive methods network size grows case underfitting training data 
mdl approaches areas machine learning include quinlan rivest gao li pednault 
implemented methods neural net approaches ones general sense solomonoff kolmogorov levin 
previous implementations measures simplicity lack universality elegance kolmogorov complexity algorithmic information theory 
previous approaches ad hoc usually gaussian priors 
remainder devoted simulations general method universal prior self sizing programs probabilistic search algorithm preferring candidates low levin complexity candidates high levin complexity 
certain seemingly trivial non trivial toy problems demonstrated approach lead generalization results traditional neural net algorithms 
mentioned say applicability method real world tasks 
generalization tasks simulations experiments values maximal program tape size tape size 
current implementation optimized speed tests programs second sun sparc elc 
average program runs time steps halting halted 
programs running millions time steps course 
perceptron counting inputs pattern association task may trivial difficult traditional approaches providing training examples 
task 
linear perceptron network input units output unit weights fed dimensional binary input vectors 
denotes th input vector 
denotes ith component ranges 
input vector exactly bits set bits set zero 
obviously gamma delta possible inputs 
network output response th weight 
weight may take integer values 
task find weights equals number bits possible number solution candidates search space possible weight vectors huge exhaustive search 
solution 
solution problem equal 
kolmogorov complexity solution small short program computes 
levin complexity small logical depth runtime shortest program bennett time steps 
difficulty 
training set small just training examples conventional perceptron algorithms solve apparently simple problem 
achieve generalization unseen test data 
reason connections units won changed conventional gradient descent algorithms 
werbos lecun parker rumelhart 
note scaling inputs differently addresses contents interpretation table program counting perceptron 
going improve matters 
weight decay 
weight decay encourages weight matrices zero entries 
current task bad strategy 
training data 
illustrate generalization capability search solution candidates low levin complexity training examples 
randomly chosen possible inputs 
training example binary vector bits positions bits 
second bits positions 
third bits positions 
cases desired output target 
generalization results described obtained particular training set similar obtained different sets training examples created randomly permuting input units 
search procedure follows probabilistic search algorithm described section lists executes programs computing solution candidates weight vectors 
primitive replacing output see section writing network weights 
argument uses variable values set 
run weights initialized 
instruction number semantics follows compare list primitives section address 
set equal contents address variable incremented 
halt range 
solution candidate fits training data exactly solution tested test data 
note reward goal task measure success binary network fits training data doesn 
teacher providing informative error signal distance desired outputs 
results 
programs fitting training exemplars runs 
lead perfect generalization gamma delta gamma unseen test examples 
weight vector fitting training data runs 
corresponding program wild allocating lot space executing useless instructions leading perfect generalization unseen test data 
halting program allocated time steps 
time probability gamma recall unit time time steps 
space probability gamma weight vector fitting training data computed th run 
corresponding program table 
readable interpretation program instruction preceded address write contents address weight pointed increment weight pointer 
halt range 
contents address equal contents address goto address 
condition tested second instruction true little program write correct solution time 
requires time steps 
case got time randomly chosen time limit 
allow real valued weights set equal contents address divided say 
addresses contents interpretation table faster program counting perceptron 
runs system came faster program 
see table 
program write contents address happens due code write weight pointed increment 
halt range 
write contents address weight pointed increment 
halt range 
contents address happens equal contents address true goto address 
program writes times jumping reducing runtime time steps recall execution instruction including jumps takes time step 
space probability gamma successful programs exactly runtime runs 
faster programs 
comment 
example probabilistic search self sizing programs leads excellent generalization performance 
theory possible appropriate variant nowlan hinton approach achieve generalization performance task 
recall section nowlan hinton encourage groups weights equal values strategy case 
reason task requires weights equal values 
kolmogorov complexity solution low 
perceptron adding input positions task 
perceptron network input data 
goal different 
task find weights equals sum positions bits gamma delta possible task difficult providing limited training data 
solution 
solution problem equal example short fast programs computing solution 
training data 
training inputs previous task 
target values different 
obviously target input vector 
target input vector 
target input vector 
success binary solution candidate fits training examples exactly solution evaluated test data 
note conventional perceptron algorithms solve generalization problem 
results 
programs fitting training data runs total search time time steps 
successful runs lead perfect generalization unseen test examples 
weight vector fitting training data runs 
corresponding program pretty wild 
led perfect generalization test data 
halting program allocated time steps 
time probability gamma space probability gamma table shows part storage execution 
program addresses contents table storage execution program adding perceptron 
addresses contents table storage execution elegant program adding perceptron 
allocate cells tape 
initialize zero 
set min min 
get contents input field see list instructions section position write address 
write contents address weight pointed increment 
halt range 
contents address equal contents address goto address 
goto address 
increment contents address 
goto address 
contents address equal contents address true goto address 
instructions addresses useless 
catastrophic 
essentially program allocates space variable initially zero tape recall program tape read execute variables 
executes loop incrementing writing variable contents network weight vector 
runs faster elegant nearly minimal program 
table shows storage execution 
program ran allocated time steps 
space probability gamma inspection reveal operation program 
different sets training examples obtained randomly permuting input units led similar generalization results 
indexing write operations clearly choice primitives affects probabilities solutions 
algorithmic information theory tells delays optimal search constant factor 
long primitives form universal set primitives set composed 
constant factors may large 
subsection repeats experiment section slightly different set primitives increasing constant factor 
primitive redefined gets additional argument 
primitive redefined gets new name 
separate automatic increment mechanism position 
new primitives may directly address read write network weights 
primitives remain 
new ones instruction numbers compare section address address 
set equal contents address value address 
addresses 
contents 
table storage execution program adding perceptron different primitive 
address address 
written address location address value address 
appropriate syntax checks halt programs attempt impossible writing non existent weight 
new primitive additional argument guessed correctly successful programs tend 
results 
runs total search time time steps runs generated weight vectors fitting training data 
allowed perfect generalization test data 
execution filled storage seen table 
program ran allocated time steps 
space probability gamma allocate cell tape 
initialize zero 
set min min 
increment contents address 
wc gamma equal contents address 
jump address 
repeated execution instruction address unnecessarily allocates cells tape damage slightly slowing program 
different sets training examples obtained randomly permuting input units led similar generalization results 
incremental search seen probabilistic search algorithm inspired universal search lead excellent generalization performance 
tasks typical sense learning system receive feedback progress 
success criterion binary system solves task doesn 
environments providing limited evaluative feedback done 
cases levin algorithm optimal 
typical learning situations real world informative feedback 
instance supervised gradient neural net algorithms back prop werbos lecun parker rumelhart information provided error signals distances actual network outputs target values 
universal search algorithms incrementally adjust network weights iterative manner solution candidates previous trials serve basis additional improvements 
reinforcement learning algorithms watkins dayan sejnowski barto williams schmidhuber see barto overview receive informative environmental feedback supervised learning algorithms designed incremental fashion 
instance tend information provided magnitude rewards amount time rewarding events 
solutions build basis better solutions 
true simple hill climbing evolutionary genetic algorithms gas rechenberg schwefel holland back see 
dickmanns schmidhuber koza applications ga paradigm evolution computer programs 
original universal search procedure formulated levin designed incremental learning situations 
current theory incremental learning developed 
appears reasonable way appropriately extending universal search 
possibilities solomonoff paul see solomonoff solomonoff paul solomonoff 
apparently implemented incremental extensions universal search far solomonoff paul emphasize importance experiments 
depth study extensions designed incremental learning scope 
promising outlook initial experimental results certain probabilistic variants implemented incremental extensions reported 
experiments incremental extensions basic set experiment network described section adding perceptron 
allow incremental learning modifications introduced 
generation mutations low levin complexity 
provide informative feedback weight vectors evaluated non binary fashion 
fitness weight vector defined number correct weights 
recall solution requires ith weight equal weights initially zero re initialized run 
improvement run leads weight vector correct weights best far weights modified weight vector stored 
runs try generate improvements modified weight vector 
words run leads mutation best weight vector far 
essentially weight vector mutations listed order levin complexity improvement 
improved weight vector goes new round mutations 
mutations mutation algorithms 
problem section modification lead significant reduction total search time 
typical improvements led additional correct weight run runs improvement 
introducing modification search time reduced dramatically 
define fitness improvement difference fitness newly generated weight vector best far 
additional modification fitness improvement exceeds fitness best improvement far corresponding mutation program kept program tape 
tape erased 
trials start equal address successful program min 
means new mutation programs may build earlier successful mutation programs 
approach similar spirit approaches proposed solomonoff solomonoff paul solomonoff 
successful programs represent short descriptions mutations different solution components 
probability new successful mutation may higher may mutate successful mutation algorithms just mutating mutation results 
additional improvements may 
think programmers prefer rewriting programs high level language rewriting microcode 
results 
test runs led step improvements 
means runs led better weight vector number correct weights exceeded best far exactly 
period network weight vector correct weights 
run number dramatic improvement leading correct weights 
described corresponding mutation program left program tape 
space probability gamma briefly event run number system generated additional dramatic improvement 
additional code just jump useful position old code 
old code new program led correct weights 
space probability additional code high gamma reason quickly 
missing correct weight generated shortly run number 
runs dramatic improvement solution completed 
corresponds small fraction second additional cpu time 
second test series step improvements 
time lasted network weight vector correct weights 
run number half hour cpu time apparently minor improvement leading correct weights 
minor improvement major breakthrough 
corresponding mutation program building block flurry additional improvements 
nearly immediately run number correct weights 
run number correct weights 
run number correct weights 
run number correct weights 
runs second cpu time apparently minor improvement solution completed 
additional experiments weight vector reinitialized zeros run 
programs leading improvements erased program tape just described 
average led equal better performance version keeping best weight vector far 
similar observations variants incremental search 
comments problems 
learning speed 
incremental extensions turned faster non incremental search compare section 
obviously useful program may serve useful subprogram 
may dramatically increase probability improvements reduce search time 
way system may learn learn faster 

improving evolutionary algorithms 
theory strategy listing parameter mutations order levin complexity appears smarter mutation strategy trivial mutation strategies employed conventional hill climbing evolutionary genetic algorithms 
rechenberg schwefel holland back dickmanns koza 
general expected come reasonable time nontrivial changes require simultaneous correlated mutations quite different positions th position say 
universal search soon find useful correlated mutations levin complexities low 
incremental extensions universal search appear promising candidates learning complex tasks replacing sophisticated strategies typically traditional algorithms 

code explosion 
theoretical investigations complex strategies handling subprograms proposed 
solomonoff described methods giving new names successful programs complex primitives solomonoff solomonoff 
approach suffers obvious problem methods tested code continues expand material form new programs referred code explosion problem 
code explosion may negative influence probability additional successful code 
reasons paul solomonoff address theoretical advantages grouping related programs directories subprograms 

compressing successful programs 
degree code explosion problem searching programs compressing old successful code 
solomonoff proposed spend half total search time trying compress previous useful programs idea pursued focus combining concepts algorithmic probability theory traditional approaches assigning modified probabilities subprograms appearing successful programs 
program compression algorithms help deal related problem observed simulations general programs incremental search short efficient elegant ones non incremental search 
reason incremental search tends generate programs incorporate non optimal code previous runs 

research 
remains done clear mutual advantages disadvantages different incremental extensions universal search 
moment knows best general algorithm learning previous experiences 
strategy incremental learning optimal sense universal search optimal broad class non incremental situations 
may important questions machine learning 
concluding remarks shown basic concepts theory algorithmic complexity interest machine learning purposes 
certain toy problems computationally feasible search preference solutions computable short fast programs may lead excellent generalization performance traditional algorithms 
focus experiments perceptron neural nets methods general applied wide variety problems 
instance done collaboration norbert variants universal search successfully applied path finding problems mazes schmidhuber 
incremental learning real world applications remains done 
bias algorithmic simplicity general 
weaker kinds problem specific inductive bias 
utgoff haussler 
solution simple bias justified require know way solution simple 
solution simple bias algorithmic simplicity won damage case algorithmically complex solutions lose focus simple candidates looking complex candidates 
general complex candidates greatly outnumber simple ones 
simple ones don significantly affect total search time optimal search algorithm 
general bias algorithmic simplicity cause harm useful problem solving 
solutions simple 
paragraph appears support answer hardly 
final part section argues expression hardly refers worst case atypical real world problems 
general generalization impossible 
specific task learn relation finite bitstrings finite bitstrings 
training set chosen randomly 
cases shortest algorithm computing non overlapping test set essentially size test set recall section computable objects incompressible 
shortest algorithm computing test set training set won shorter 
words mutual algorithmic information 
chaitin test set training set zero cases ignoring additive constant independent problem :10.1.1.48.3094
general case knowledge training set provide clues test set hope generalization obviously reason simple kind solution preferred priori complex ones related observations discussed length dietterich schaffer wolpert 
may viewed reason certain worst case results pac learning theory initiated valiant appear discouraging 
similarly problem solving general problem usually defined search space solution candidates computable criterion solution 
solutions problems set possible welldefined problems algorithmically complex random incompressible 
problems efficiently solved efficient means faster exhaustive search levin universal search algorithm hypothetical optimal incremental learning scheme method 
apparently typical problems confronted real world simple 
simple sense solutions require information specified solution candidates 
problems humans consider typical atypical compared general set defined problems see li vit anyi 
interesting problems bias algorithmic simplicity justified 
may 
consequence possibility universe run short algorithm electron behaves way 
cases just consequence fact select problems solve exist survive doing argument 
anyway learning machines try enormous amount algorithmic redundancy friendly universe 
general way doing appears tools provided theory algorithmic probability kolmogorov complexity 
martin sepp hochreiter daniel mark ring jan gerhard wei especially ray solomonoff useful comments earlier drafts 
adleman 

time space randomness 
technical report mit lcs tm laboratory computer science mit 
allender 

application time bounded kolmogorov complexity complexity theory 
watanabe editor kolmogorov complexity computational complexity pages 
eatcs monographs theoretical computer science springer 
amari murata 

statistical theory learning curves entropic loss criterion 
neural computation 
atick li redlich 

understanding retinal color coding 
neural computation 
barlow 

unsupervised learning 
neural computation 
barron 

complexity regularization application artificial neural networks 
nonparametric functional estimation related topics pages 
kluwer academic publishers 
barto 

connectionist approaches control 
technical report coins technical report university massachusetts amherst ma 
barto sutton anderson 

neuronlike adaptive elements solve difficult learning control problems 
ieee transactions systems man cybernetics smc 
barzdin 

algorithmic information theory 
reidel editor encyclopaedia mathematics volume pages 
kluwer academic publishers 
baum haussler 

size net gives valid generalization 
neural computation 
becker 

unsupervised learning procedures neural networks 
international journal neural systems 
bennett 

logical depth physical complexity 
universal turing machine half century survey volume pages 
oxford university press oxford hamburg 
blumer ehrenfeucht haussler warmuth 

occam razor 
information processing letters 
chaitin 

length programs computing finite binary sequences 
journal acm 
chaitin 

length programs computing finite binary sequences statistical considerations 
journal acm 
chaitin 

theory program size formally identical information theory 
journal acm 
chaitin 

algorithmic information theory 
cambridge university press cambridge 
cover acs gray 

kolmogorov contributions information theory algorithmic complexity 
annals probability theory 
dayan sejnowski 

td convergence probability 
machine learning 
press 
deco zimmermann 

elimination overtraining mutual information network 
proceedings international conference artificial neural networks amsterdam pages 
springer 
dickmanns schmidhuber 

der eine implementierung prolog 
institut fur informatik lehrstuhl prof technische universitat munchen 
dietterich 

limitations inductive learning 
proceedings sixth international workshop machine learning ithaca ny pages 
san francisco ca morgan kaufmann 
acs 

symmetry algorithmic information 
soviet math 
dokl 
gao li 

minimum description length principle application online learning characters 
proc 
th ieee international joint conference artificial intelligence detroit mi pages 
guyon vapnik boser bottou solla 

structural risk minimization character recognition 
lippman moody touretzky editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 
hartmanis 

generalized kolmogorov complexity structure feasible computations 
proc 
th ieee symposium foundations computer science pages 
hassibi stork 

second order derivatives network pruning optimal brain surgeon 
lippman moody touretzky editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 
haussler 

quantifying inductive bias ai learning algorithms valiant learning framework 
artificial intelligence 
hinton van camp 

keeping neural networks simple 
proceedings international conference artificial neural networks amsterdam pages 
springer 
hochreiter schmidhuber 

simplifying networks discovering flat minima 
technical report fakultat fur informatik technische universitat munchen 
nips 
back 

genetic algorithms evolution strategies similarities differences 
manner schwefel editors proc 
st international conference parallel problem solving nature berlin 
springer 
holland 

adaptation natural artificial systems 
university michigan press ann arbor 
kolmogorov 

approaches quantitative definition information 
problems information transmission 
kolmogorov 

der 
springer berlin 
koza 

genetic evolution evolution computer programs 
langton taylor farmer rasmussen editors artificial life ii pages 
addison wesley publishing 
krogh hertz 

simple weight decay improve generalization 
lippman moody touretzky editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 
lecun 

une proc apprentissage pour eseau asym 
proceedings paris pages 
lecun solla 

second order properties error surfaces learning time generalization 
lippman moody touretzky editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 
levin 

notion random sequence 
soviet math 
dokl 
levin 

universal sequential search problems 
problems information transmission 
levin 

laws information aspects foundation probability theory 
problems information transmission 
levin 

various measures complexity finite objects axiomatic description 
soviet math 
dokl 
levin 

randomness conservation inequalities information independence mathematical theories 
information control 
li vit anyi 

theory learning simple concepts simple distributions average case complexity universal distribution 
proc 
th american ieee symposium foundations computer science pages 
li vit anyi 

kolmogorov complexity applications 
springer 
linsker 

self organization perceptual network 
ieee computer 
maass 

perspectives current research complexity learning neural nets 
roychowdhury siu editors theoretical advances neural computation learning 
kluwer academic publishers 
mackay 

practical bayesian framework backprop networks 
neural computation 
martin lof 

definition random sequences 
information control 


discovery minimal length encoding case study molecular evolution 
machine learning 
moody 

effective number parameters analysis generalization regularization nonlinear learning systems 
lippman moody touretzky editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 
mozer smolensky 

skeletonization technique trimming fat network relevance assessment 
touretzky editor advances neural information processing systems pages 
san mateo ca morgan kaufmann 
nowlan hinton 

simplifying neural networks soft weight sharing 
neural computation 
parker 

learning logic 
technical report tr center comp 
research economics management sci mit 
paul solomonoff 

autonomous theory building systems 
manuscript revised 
pearlmutter rosenfeld 

chaitin kolmogorov complexity generalization neural networks 
lippman moody touretzky editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 
pednault 

experiments applying inductive inference principles surface reconstruction 
th ijcai pages 
san mateo ca morgan kaufmann 
quinlan rivest 

inferring decision trees minimum description length principle 
information computation 
rechenberg 

evolutionsstrategie optimierung technischer systeme nach prinzipien der biologischen evolution 
dissertation 
published holzboog 
rissanen 

modeling shortest data description 
automatica 
rissanen 

universal prior integers estimation minimum description length 
annals statistics 
rissanen 

stochastic complexity modeling 
annals statistics 
rumelhart hinton williams 

learning internal representations error propagation 
parallel distributed processing volume pages 
mit press 
schaffer 

overfitting avoidance bias 
machine learning 
schmidhuber 

evolutionary principles self referential learning learning learn meta meta hook 
report institut fur informatik technische universitat munchen 
schmidhuber 

local learning algorithm dynamic feedforward recurrent networks 
connection science 
schmidhuber 

learning complex extended sequences principle history compression 
neural computation 
schmidhuber 

learning factorial codes predictability minimization 
neural computation 
schmidhuber 

decreasing ratio learning complexity number timevarying variables fully recurrent nets 
proceedings international conference artificial neural networks amsterdam pages 
springer 
schmidhuber 

self referential weight matrix 
proceedings international conference artificial neural networks amsterdam pages 
springer 
schmidhuber 

algorithmic art 
technical report fakultat fur informatik technische universitat munchen 
schmidhuber 

applications levin optimal universal search algorithm 
technical report fakultat fur informatik technische universitat munchen 
preparation 
schnorr 

unified approach definition random sequences 
mathematical systems theory 
schwefel 

numerische optimierung von computer modellen 
dissertation 
published birkhauser basel 
shannon 

mathematical theory communication parts ii 
bell system technical journal 
solomonoff 

formal theory inductive inference 
part information control 
solomonoff 

application algorithmic probability problems artificial intelligence 
kanal lemmer editors uncertainty artificial intelligence pages 
elsevier science publishers 
solomonoff 

system incremental learning algorithmic probability 
pednault editor theory application minimal length encoding preprint symposium papers aaai spring symposium 
utgoff 

shift bias inductive concept learning 
machine learning volume 
morgan kaufmann los altos ca 
valiant 

theory learnable 
communications acm 
vapnik 

principles risk minimization learning theory 
lippman moody touretzky editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 
wallace boulton 

information theoretic measure classification 
computer journal 
watanabe 

kolmogorov complexity computational complexity 
eatcs monographs theoretical computer science springer 
watkins 

learning delayed rewards 
phd thesis king college 
weigend huberman rumelhart 

predicting connectionist approach 
international journal neural systems 
werbos 

regression new tools prediction analysis behavioral sciences 
phd thesis harvard university 
williams 

theory reinforcement learning connectionist systems 
technical report nu ccs college comp 
sci northeastern university boston ma 
wolpert 

overfitting avoidance bias 
technical report sfi tr santa fe institute nm 
levin 

complexity finite objects algorithmic concepts information randomness 
russian math 
surveys 
