pattern recognition www elsevier com locate feature extraction dimensionality reduction algorithms applications vowel recognition wang paliwal school engineering nathan campus gri th university brisbane qld australia received march accepted december feature extraction important component pattern recognition system 
performs tasks transforming input parameter vector feature vector reducing dimensionality 
de ned feature extraction algorithm classi cation process ective cient 
popular methods feature extraction linear discriminant analysis lda component analysis pca 
minimum classi cation error mce training algorithm originally optimizing classi ers feature extraction 
gmce training algorithm shortcomings mce training algorithm 
lda pca algorithms extract features linear transformation 
support vector machine svm developed pattern classi cation algorithm uses non linear kernel functions achieve non linear decision boundaries parametric space 
svm linear feature extraction algorithms 
pattern recognition society 
published elsevier rights reserved 
keywords feature extraction dimensionality reduction mce svm 
aim pattern recognition systems classify input data classes 
conventional pattern recognition systems components feature analysis pattern classi cation shown fig 

feature analysis steps parameter extraction step extraction step 
parameter extraction step information relevant pattern classi cation input data form dimensional parameter vector feature extraction step parameter vector feature vector dimensionality 
parameter extractor properly designed corresponding author 
tel ext 
mail addresses wang gu edu au telecom ca wang paliwal gu edu au paliwal 
current address telecommunication place de la west level suite montreal quebec canada 
parameter vector pattern classi er dimensionality low necessity feature extraction step 
practice parameter vectors suitable pattern classi ers 
example parameter vectors decorrelated applying classi er basedon gaussian mixture models diagonal variance matrices 
furthermore dimensionality parameter vectors normally high needs reduced sake computational cost complexity 
due reasons feature extraction important problem pattern recognition tasks 
feature extraction conducted independently jointly parameter extraction classi cation 
lda pca popular independent feature extraction methods 
extract features projecting original parameter vectors new feature space linear transformation matrix 
optimize transformation matrix di erent intentions 
pca optimizes transformation matrix nding largest pattern recognition society 
published elsevier rights reserved 
doi wang paliwal pattern recognition variations original feature space 
lda pursues largest ratio class variation class variation projecting original feature subspace 
drawback independent feature extraction algorithms optimization criteria di erent classi er minimum classi cation error criterion may cause inconsistency feature extraction classi cation stages pattern recognizer degrade performance classi ers 
direct way overcome problem conduct feature extraction cation jointly consistent criterion 
mce training algorithm provides 
type discriminant analysis achieves minimum classi cation error directly extracting features 
direct relationship mce training algorithm widely popular number pattern recognition applications dynamic time wrapping recognition hidden markov model hmm recognition 
lda pca training algorithm linear feature extraction algorithms 
advantage linear algorithms ability reduce feature dimensionalities 
limitation decision boundaries linear little computational exibility 
svm developed pattern classi cation algorithm non linear formulation 
basedon idea classication ords dot products computed ciently higher dimensional feature spaces 
classes linearly separable original parametric space linearly higher dimensional feature space 
svm advantage handle classes complex non linear decision boundaries 
di erent conventional systems shown fig 
svm highly recognition system shown fig 

svm active area research 
investigates lda pca algorithms feature extraction 
gmce training algorithm shortcomings mce training algorithms 
performances mce gmce training algorithms lda databases 
svm fig 

conventional pattern recognition system 
fig 

svm recognition system 
classi cation system 
performance lda pca mce algorithms 
rest follows section gives brief lda pca 
section introduces framework mce training algorithm 
alternative mce training algorithm uses ratio model misclassi cation measure proposed 
mce training algorithm applied dimensionality reduction tasks gmce training algorithm proposed 
section introduces formulation svm section compare lda pca mce gmce training algorithms vowel recognition task timit database 
results 

standard feature extraction methods 
linear discriminant analysis goal linear discriminant analysis separate classes projecting classes samples dimensional space nely 
class problem min di erent lines involved 
projection dimensional space dimensional space 
suppose classes xk 
ith observation vector xj xji nj 
number classes nj number observations class class covariance matrix sw class covariance matrix sb de ned nj sw sj xji xji sb nj wang paliwal pattern recognition nj nj nj xji mean class xi global mean 
projection observation space feature space linear transformation matrix corresponding class class covariance matrices feature space sw sb nj yji yji nj nj nj yji show sw swt sb sbt linear discriminant de ned linear functions objective function sb sw sbt swt maximum 
shown solution eq 
ith column optimal generalized eigenvector corresponding ith largest eigenvalue matrix sb 

principal component analysis pca feature extraction dimensionality reduction 
basedon assumption information classes directions variations largest 
common derivation pca terms projection maximises variance 
dimensional data set principal axes tm orthonormal axes maximum 
generally tm leading eigenvectors sample covariance matrix xi xi xi sample mean number samples sti iti ith largest eigenvalue principal components observation vector ym mx principal components decorrelated 
multi class problems variations data determined global basis principal axes derived global covariance matrix nj xji xji global mean samples number classes nj number samples class nj xji represents ith observation class principal axes tm leading eigenvectors sti iti ith largest eigenvalue assumption feature extraction dimensionality reduction pca information observation vectors subspace rst principal axes original data vector principal component vector dimensionality 
minimum classi cation error training algorithm 
derivation mce criterion consider input vector classi er decision decision rule class gk max gi gi discriminant function class parameter set number classes 
negative gk gi measure misclassi cation form di erentiable needs modi cation 
modi ed version introduced misclassi cation measure 
kth class dk gk gi positive number gk discriminant observation known class reduces approaches dk gk gj class largest discriminant value classes class obviously dk wang paliwal pattern recognition implies misclassi cation dk means correct classication dk suggests sits boundary 
loss function de ned monotonic function misclassi cation measure 
chosen function suitable gradient descent algorithm 
loss function lk dk 
training set empirical loss de ned lk lk nk number samples class clearly minimizing empirical loss function minimization classi cation error 
result eq 
mce criterion 
class parameter set minimizing loss function steepest gradient descent algorithm 
iterative algorithm andthe iteration rules denotes tth iteration class parameters adaption constant 
gradient follows nj gk class gj class case mahalanobis distance measure discriminant functions class mean covariance matrix 
di erentiation discriminant functions respect gm gm alternative de nition misclassi cation measure enhance control joint behaviour discriminant functions gk gj 
alternative misclassi cation de ned follows gi dk gk extreme case eq 
dk gj gk class parameters matrix adaption rules shown eq 

gradients respect nj class gj gk gk gk gj class di erentiation discriminant functions eq 


mce training algorithms dimensionality reduction feature extraction methods mce reduces feature dimensionality projecting input vector lower dimensional feature space linear transformation tm class parameter set feature space 
accordingly loss function dk tx tx empirical loss data set dk tx lk eq 
function elements parameter set gradient descent procedure 
adaption rule denotes tth iteration adaption constant learning rate row indicators transformation matrix gradient respect conventional mce gk tx gj tx alternative mce gj tx gk tx gk tx gj tx gk tx mahalanobis distance discriminant functions gm tx tx tx 
generalized mce training algorithm major concerns mce training dimensionality reduction initialization parameters 
gradient descent method mce training algorithm guarantee global minimum value 
optimality mce training process largely dependent initialization parameter sets parameters transformation matrix crucial success mce training lters class information brought decision space 
paliwal give initialization mce training algorithm taken unity matrix 
cases convenient way initialization ective way classi cation criterion considered initialization 
order increase generalization mce training algorithm necessary classi cation criteria initialization process 
searching point view training sequential search procedures general rough search initialization parameters andthe local thorough search optimization parameters 
search procedure provide global class parameters andthe thorough search relevant local minimum 
fig 
compares normal mce training process training process 
far criterion general searching process proposed 
employ current feature extraction methods process 
practice employ lda general searching process initialization class parameters 

support vector machine 
constructing svm considering class case suppose classes set training data wang paliwal pattern recognition xn training data labeled rule xi yi xi basic idea svm estimation project input observation vectors non linearly high dimensional feature space andthen compute linear function functions take form denotes dot product 
ideally data classes satisfy constraint yi xi considering points xi inf equality eq 
holds points lie hyper planes xi xi 
hyper planes parallel training points fall 
margin 
pair hyper planes maximum margin minimizing subject eq 

problem written convex optimization problem minimize subject yi xi rst function primal objective function andthe corresponding constraints 
eq 
constructing lagrange function primal function andthe corresponding constraints 
introduce positive lagrange multipliers constraint eq 

lagrange function lp iyi xi lp respect requires gradient lp vanish respect gradients lp ws iyi xis ws lp iyi dimension space combine conditions constraints primal functions multipliers obtain karush kuhn tucker kkt conditions lp ws iyi xis ws wang paliwal pattern recognition lp fig 

comparison normal mce training process andthe training process 
iyi yi xi yi xi variables solved 
kkt condition eq 
obtain iyi xi iyi iyi xi xi xi xi xj kernel function uses dot product feature space 
substitute eq 
eq 

leads maximization dual function ld ld writing dual function incorporating constraints obtain dual optimization problem maximize subject iyi primal problem lp eq 
andthe dual problem ld eq 
objective function di erent constraints 
optimization primal dual problem type convex optimization problem interior point algorithm 
discussion interior point algorithm scope 
algorithm vanderbei 

multi class svm classi ers svm class classi cation algorithm 
multi class classi er constructed 
far best method constructing multi class svm classi er clear 
scholkopf vs type classi er 
clarkson vs type classi er 
structures shown fig 

types classi ers fact combinations class sub classi ers 
input data vector enters classi er dimensional value vector dimension class generated 
classi er classi es classi cation criteria class max 
classi cation experiments experiments focus vowel recognition tasks 
databases 
start vowels database 
advantage starting computational burden small 
database evaluate di erent types gmce training algorithms svm classi ers 
feature extraction cation algorithms timit database 
feature extraction cation algorithms experiments table 
order evaluate performance linear feature extraction algorithms pca lda mce minimum distance classi er 
feature vector classi jth class distance dj wang paliwal pattern recognition fig 

types multi class svm classi er structure vs multi class svm classi er structure vs multi class svm classi er 
table feature extraction cation algorithms experiments parameter dimension feature classi er lar pca minimum distance mahalanobis lda mce gmce mfcc timit pca lda mce gmce lar svm vs lar svm vs mfcc timit svm vs distances di weuse mahalanobis distance measure compute distance feature vector class 
distance di follows di mean vector class covariance matrix 
experiments full covariance matrix 
types svm kernel function database 
formulation kernel functions follows linear kernel polynomial kernel rbf kernel 
database experiments vowels database vowel classes shown table 
database past number researchers pattern recognition applications 
vowels uttered times di erent speakers 
gives total vowel tokens 
central frame speech signal vowel tokens 
th order linear prediction analysis frame andthe resulting linear prediction coe cients log area lar parameters 
frames speakers train models frames rest speakers test models 
table compares results lda pca conventional form andthe alternative form mce training algorithm 
results show alternative mce training algorithm best performance 
alternative mce experiments 
types gmce training algorithm investigated database experiments 
uses lda general search andthe uses pca 
figs 
show experiment results 
alternative mce training algorithm chosen mce training denote types gmce training algorithms gmce lda gmce pca respectively 
normal alternative mce wang paliwal pattern recognition table vowels words database vowel word hod hid hoard hard hud table comparison various feature extractors database conventional mce alternative mce lda pca vowels train vowels test recognition rate mce unit gmce lda lda training data dimension recognition rate mce unit gmce lda lda testing data dimension fig 

results mce unit gmce lda lda database 
recognition rate recognition rate mce alt unit mce alt pca pca mce alt unit mce alt pca pca training data dimension testing data dimension fig 

results mce unit gmce pca pca database 
table vowel data set classi cation results kernel classi er training testing linear vs linear vs polynomial vs polynomial vs rbf vs rbf vs training algorithm denoted mce unit 
observations results follows gmce training algorithm lda general search initial transformation matrix lda demonstrates best performance mce unit gmce pca lda 
performance gmce training algorithm pca general searching process 
performances gmce lda pca testing data show best classi cation results usually obtained dimensionality reduced 
table shows classi cation results di erent svm classi ers 
order polynomial kernel function 
classi cation result show performance rbf kernel function best types kernels 
performance vs multi class classi er better vs multi class classi er 
types svm classi ers vs multi class classi er rbf kernel function table number training data set wang paliwal pattern recognition phonemes aa ae ah ao aw ax ay eh oy uh training testing phonemes el en er ey ih ix iy ow uw total training testing best performance experiments 

timit database experiments order provide results bigger database timit database vowel recognition 
database contains total sentences sentences spoken speakers 
training part database training vowel recognizer andthe test part testing 
vowels classi cation tasks selected vowels semi vowels timit database 
altogether vowels semi vowel nasal vowel classi cation experiments 
timit database comes phonemic transcription phonemic boundaries 
center msec segments sentence 
spectral analysis segments segment dimension mel frequency cepstral coe cients mfccs feature vectors 
vector contains energy coe cient mfccs 
mahalanobis distance minimum distance classi er pattern classi er 
table shows number segments vowel experiment 

comparison separate integrated pattern recognition systems fig 
shows results separate pattern recognition systems pca plus classi er mce feature extraction cation tasks 
dimensionalities experiments full dimension 
horizontal axis gure dimension axis 
vertical axis represents recognition rates 
svm suitable dimensionality reduction applied classi cation tasks results svm appears gure single points 
observations fig 
follows lda fairly performance curve 
performs best low dimensional feature spaces dimension lda pca training algorithm training data 
testing data lda performs better pca mce low dimensional spaces dimension 
recognition rate recognition rate pca lda mce svm training data pca lda mce svm dimension testing data dimension fig 

results lda pca mce timit database 
mce training algorithm performs better lda pca high dimensional feature spaces dimension training data 
testing data mce training algorithm performs better pca high dimensional spaces dimension 
performances svm training data lda pca training algorithm 
svm performs better lda pca training algorithm testing data 

analysis gmce training algorithm section investigate performance gmce 
types gmce 
employs lda general search denote gmce lda 
employs pca general search denote gmce pca 
experiments results shown figs 

observations gures follows gmce uses lda general search tool performances gmce better lda mce dimensions 
gmce uses pca general search process general performances gmce signi cantly improved 
high dimensional feature spaces dimension performances gmce lda close mce training algorithm better lda 
medium dimensional dimension lowdimensional dimension feature spaces gmce wang paliwal pattern recognition recognition rate recognition rate mce lda lda mce training data dimension mce lda lda mce testing data dimension fig 

results gmce lda mce timit database 
mce pca pca mce training data dimension recognition rate mce pca pca mce testing data dimension recognition rate fig 

results gmce pca mce timit database 
lda signi cantly better performances lda 

investigate major feature extraction classi cation algorithms 
algorithms investigation 
lda pca mce training algorithm training algorithm 
observation experimental results drawn pattern classi cation systems integrate feature extraction cation mce training algorithms show better performance systems independent feature extraction algorithm lda pca 
gmce training algorithm mce training algorithm 
signi cant improvements low dimensional feature spaces 
performance svm interesting 
poorer training data lda pca training algorithms testing data better linear feature extraction algorithms 
implies linearly models better tness training data svm better generalization properties 

summary conventional pattern recognition systems components feature analysis classi cation 
feature analysis steps parameter extraction step extraction step 
lda popular independent feature extraction methods 
drawback independent feature extraction algorithms optimization criteria di erent classi er minimum classi cation error criterion may cause inconsistency feature extraction andthe classi cation stages pattern recognizer 
direct way overcome problem conduct feature extraction classi cation jointly consistent criterion 
mce training algorithm provides integrated framework 
lda pca training algorithm linear feature extraction algorithms 
advantage linear algorithms ability reduce feature dimensionalities 
limitation decision boundaries generated linear 
svm developed pattern classi cation algorithm non linear formulation 
basedon idea classi cation ords dot products ciently higher dimensional feature spaces 
svm advantage handle classes complex non linear decision boundaries 
investigates lda pca algorithms feature extraction 
gmce training algorithm proposed shortcomings mce training algorithms 
svm classi cation system 
hotelling analysis complex statistical variables principal components educational psychol 

principal component analysis springer verlag new york 
rao principal component analysis 
discriminative analysis feature reduction automatic speech recognition proceedings ieee international conference acoustics speech signal processing vol 
pp 

poston recursive dimensionality reduction fisher linear discriminant pattern recognition 
sun feature dimension reduction reduced rank maximum likelihood estimation hidden markov model proceedings international conference spoken language processing philadelphia usa pp 

juang discriminative learning minimum error classi cation ieee trans 
signal process 

lee juang descent method proceedings acoustic society japan fall meeting pp 

paliwal sagisaka simultaneous design feature extractor classi er minimum classi cation error training algorithm proceedings ieee workshop neural networks signal processing boston usa september pp 

chang chen juang discriminative analysis distortion sequences speech recognition proceedings ieee international conference acoustics speech signal processing vol 
pp 

gpd training dynamic programming recognizer acoust 
soc 
japan 
chou juang lee segmental gpd training hmm recognizer proceedings ieee international conference acoustics speech processing vol 
pp 

liu lee chou juang study minimum error discriminative training speaker recognition acoust 
soc 
amer 

minimum error classi cation training hmms implementation details experimental results acoust 
soc 
japan 
boser guyon vapnik training algorithm optimal margin classi ers haussler ed th annual acm workshop colt pittsburgh pa pp 

roth steinhage nonlinear discriminant analysis kernel functions technical report nr iai tr issn university bonn 
vapnik nature statistical learning theory springer ny 
joachims making large scale svm learning practical scholkopf burges smola eds advances kernel methods support vector learning mit press cambridge usa pp 

scholkopf vapnik incorporating invariances support vector learning machines international conference wang paliwal pattern recognition arti cial neural networks icann berlin pp 

scholkopf bartlett smola williamson support vector regression automatic accuracy control proceedings th international conference arti cial neural networks perspectives neural computing berlin pp 

smola scholkopf tutorial support vector regression neurocolt technical report series nc tr esprit working group neural computational learning theory neurocolt 
duda hart pattern classi cation scene analysis john wiley new york 
cottrell principal components analysis images back propagation spie proceedings visual communication processing vol 
pp 

burges tutorial support vector machines pattern recognition data mining discovery 

vanderbei loqo interior point code quadratic programming optimization methods software 

clarkson moreno support vector machines phonetic classi cation proceedings international conference acoustics speech signal processing pp 

scholkopf vapnik extracting support data task proceedings international conference knowledge discovery data mining menlo park pp 

robinson dynamic error propagation networks ph thesis cambridge university engineering department february 
fisher doddington darpa speech recognition research database speci cations status proceedings darpa speech recognition workshop pp 

sankar neural tree networks eds neural networks theory applications academic press new york pp 

tsoi pearson comparison classi cation techniques cart layer perceptrons lippman moody touretzky eds advances neural information processing systems vol 
morgan kaufmann san mateo ca pp 

author wang degree mechanical engineering university china andm degree signal processing university science china 
ph candidate signal processing lab gri th university brisbane australia 
current research interests include discriminant learning speech recognition 
author paliwal received degree university india degree university india ph degree bombay university india 
professor gri th university brisbane australia 
worked number organizations including tata institute fundamental research bombay india norwegian institute technology trondheim norway university keele bell laboratories murray hill new jersey research laboratories kyoto japan 
books speech coding synthesis elsevier speech speaker recognition advanced topics kluwer 
papers international journals 
recipient ieee signal processing society senior award 
associate editor ieee transactions speech processing signal processing letters 
current research interests include speech processing image coding neural networks 
