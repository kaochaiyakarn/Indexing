hybrid access model storage area networks singh sandeep david pease ling liu hybrid storage area network uses band nfs band virtualization san fs access models 
hybrid servers serve metadata nas servers intelligently decides access model request characteristics requested data 
hybrid model implemented low overhead cache admission cache replacement schemes aims improve response times wide variety workloads 
preliminary analysis hybrid model indicates performance improvements models 
currently prevalent access models storage area networks sans 
band virtualization model henceforth called direct access model clients access file metadata dedicated metadata servers mds access data directly storage controllers 
contrast band nas access model clients access data intermediate nas server 
access models advantages disadvantages 
compare characteristics 
scalability direct access model scalable nas model 
centralized mds direct access model serve greater number clients hosts access data directly intermediate server data path 
metadata transactions shorter metadata caching client improves scalability 

caching nas model access provides great opportunity exploit access locality multiple clients 
nas servers typically maintain caches objects actively accessed hot objects 
additional caching layer storage controller cache client cache significantly reduce response times 
direct access model caches available client cache storage controller cache 
argued storage controller cache suffice hot objects workloads single controller hit hot objects controller cache swapping desired objects 
addition causes additional load controllers 
client cache fails exploit similar accesses different clients 

workload specific easily seen done visiting ibm almaden georgia tech cc gatech edu ibm almaden research center ibm com sandeep pease almaden ibm com models better suited certain kinds workloads 
example accessing files heavy read sharing multiple clients nas model perform better 
small sized files costs establishing connections metadata server storage direct access model relatively expensive 
hand large files little sharing multiple clients direct access model faster zero hops data access 

infrastructure currently sans fibre channel infrastructure expensive ip 
advent iscsi cost difference longer issue 
seen models strengths weaknesses 
provide hybrid framework called provide benefits approaches offer single better solution variety workloads 
aim reduce client response times selecting access model appropriate desired objects 
unique characteristic design allow granularity single data request words decision choose particular mode access request 
implement intelligence cache admission cache replacement policies hybrid servers 
policies utilize information existing mds nas servers 
discuss centralized direct access models dedicated mds easy extend proposed solutions decentralized approaches 
important note solution requires hybrid servers act metadata nas servers 
require mds understand nas protocols initiatives parallel nfs nas servers able serve metadata 
believe small change prospective benefits 
rest organized follows 
describe proposed design section provide brief preliminary analysis section 
section discuss related including caching related areas databases 
conclude section note 
design possible approach design nas direct access models choose describe additions direct access design 
choice gives underlying infrastructure support host connectivity storage 
put differently describe adding caching data access mds direct access design primary motivation reduce response times seen clients 
design mds maintain data cache case cache hit mds immediately sends data response metadata request saving costs delays parsing metadata client connecting appropriate storage controllers retrieving data 
modified mds called hybrid server hs 
case cache penalty check existence data object cache operation proposed scheme 
addition maintenance statistics required making decision mode access cheap add considerable overheads 
note client acquired metadata desired object subsequent accesses example cached metadata directly storage controller direct access model 
discuss condition potentially relaxed save operations 
mentioned earlier implement hybrid scheme decide model access cache admission cache replacement policies 
cache admission policy determines conditions necessary object fulfill admitted cache 
cache replacement policies determine selection object typically called victim object replaced space required incoming object 
scheme works follows 
modify metadata information object contain pointer data object cache case data cache pointer set null 
client requests metadata certain object hs checks requested data object cache 
case cache hit data directly served client 
case hs checks requested object pass cache admission test replace existing data object cache exists space cache 
accessed nas model hs making access storage forwarding data client 
fails cache admission test replace existing cached object cache replacement policy accessed direct access model hs providing metadata client client making access storage 
scheme problem dynamically deciding access model reduces design cache admission cache replacement policies 
workflow 
decision factors main challenge design policies identifying factors influence decision object cached 
addition evaluation factors expensive prevent caching overheads prohibitive 
storage scenario believe factors critical decision making rate access object accessed frequently greater incentive keeping object direction require host storage connectivity nas servers able serve metadata client request metadata object cache object cache passes admission space available existing object replaced hit cache sends data hs cache access metadata 
storage required model selected direct just sends metadata hs accesses data client caching 
storage 
model selected nas retrieves data hs forwards storage metadata client caches data 
workflow transaction cache 
frequent accesses result cache hits improved response times 
describe metric needs measured conjunction data sharing locking mechanisms order obtain estimate caching policy 
cost obtaining object object expensive obtain storage example slower heavily loaded storage controller greater incentive cached 
case cache hits objects response time improvement significant 
size object size objects plays critical component 
object smaller size valuable cached takes lesser cache space case cache hit provides maximum response time improvement ratios compared direct access models 
load hs hs metadata transactions important prevent queuing delays hs due various operations 
heavily loaded hs perform operations promote direct access 
parameters define value data object 
greater value indicates greater incentive cache object turn means access nas model 
value oi ci fourth parameter load hs set admission threshold described 
value metric favor objects accessed frequently expensive obtain case smaller size 
parameter set depending amount favor desired smaller objects especially order favor metadata hs 
discuss issue interaction data metadata objects 
important note rate access rate requests metadata hs rate access storage 
objects accessed directly storage cached metadata lesser incentives cached hs accessed fast direct access model 
parameter evaluation mentioned earlier evaluation parameters needs low overhead operation 
achieve goal way computing si size object available mds added overhead 
evaluate rate access moving average inter arrival times request oi parameter determining amount history considered typically set 
precisely defined tk current time tk time th oi 
metadata requests come hs parameter efficiently computed 
ci measure cost obtaining data storage controller means average access times 
direct access model clients directly access storage parameter obtained clients 
achieve manner 
object accessed storage controller client maintains average access time requests lock metadata object hs shares statistic mds 
mds averages access time clients data sharing calculation scheme described till take account kind locks held data objects 
locking mechanisms potential impact object values 
example consider case object accessed frequently exclusive mode 
attempts access object held exclusive lock satisfied 
caching object incentive due inability serve clients object 
modify evaluation manner 
access attempts satisfied object cache client holds exclusive lock counted rate access 
scheme automatically prefers object read shared provide maximum benefits caching 
cache admission control mentioned object considered caching accessing nas model passes cache admission test cat 
motivation having test ensure object valued dedicate hs resources operations 
simple policy admit object value greater minimum value cached objects 
cat value min value oi policy insufficient 
example consider workload scenario objects cache accessed frequently rest possible client specific parameters averaging clients 
example value oi defined ij cij ij cij parameters client 
case computed clients objects rarely accessed replaced low contention cache space 
minimum value objects cache decrease time potentially small number 
scenario prefer avoid bringing new objects low values workload hs 
modify policy follows cat value max min value oi threshold parameter dynamically adjusts workload seen far 
achieved setting avg min value oi computed periodically average minimum value cached objects intervals 
determines amount history taken account statically set 
computation period set terms number transactions hs 
example computed metadata transactions 
addition extended incorporate load hs 
example heavily loaded hs factor added threshold value raises bar accessing data nas model reducing load queuing delays hs 
cache replacement policy incoming object passes test try evaluate space cache accommodate object 
case space try evict existing objects cache replacement policy 
algorithm 
arrange cache objects list increasing value order 
sorted list 

minimal prefix size size size om size 
value om value evict om replacement done cached 
step ensures replace higher value object expense lower value object 
clause turn mean object accessed direct access model 
cache admission cache replacement policies efficiently implemented low overhead priority queue heap 
issue discussed detailed version 
cache consistency caching solution important efficient mechanisms maintaining cache consistency 
mechanism achieving strong consistency 
object required accessed writes exclusive mode access hs serves initial metadata request invalidates object cache 
efficient mechanism object held exclusive manner shared cache anyway 
accesses object treated similar new object accessed 
design avoids explicit cache consistency mechanisms preventing high caching overheads 
investigating mechanisms maintaining weaker forms consistency lazy writes hs 
memory model argued mds meant provide metadata information fine tuned workload characteristics large number small requests 
data caching mds competes main memory metadata objects influencing core task mds 
propose approaches counter depending workload characteristics 
partitioned memory model model statically assigned distinct spaces metadata caching data caching overlap 
ensures data caching component effect regular metadata caching 
dependent availability memory hs 
model best workloads size cached metadata fluctuate 

shared memory model strict priority metadata model uses shared space metadata data 
metadata objects strict priority data objects 
data object replace metadata object similar setting metadata object values metadata object replaces valued data objects 
metadata objects regular cache replacement policies 
model best workloads metadata cache size vary significantly metadata performance critical application 

shared memory model appropriate scenarios reasonable swap metadata objects rarely accessed order cache valued data objects 
scenario shared memory model appropriately set value value function depending priority smaller objects 
notice adds value small objects necessarily metadata objects 
desired modify value parameter metadata objects ici giving head start admission threshold parameter discussed earlier 
analysis section preliminary analysis hybrid model compared nas direct access models 
part initial analysis simple models describe certain empirical behavior 
example linear model determine queue delays servers number clients serving different parameters metadata operations 
important emphasize model designed predict accurately response times comparatively analyze models 
detailed version include experimental analysis 
time sending data metadata request 
characteristics metadata response mds hs similar short messages assume response request 
time send request storage retrieve data particular file 
time host get data storage direct access model nas hs server access data storage nas hybrid model 
time send request obtain data server memory nas hs server cache 
delay nas mds hs server due contention simultaneous accesses 
set linear function number connections server slope linear function lower metadata connections compared data connections 
give short example analysis accessing single file space available caches 
assume file accessed clients client accesses times reads issued 
consider client data caching models 
metadata cached direct hybrid model 
nas model accesses results storage 
subsequent accesses served cache nas server 
total response time nas model nas nas term includes data request fetching data storage delay nas server forwarding content client fetched served memory request data object 
second term includes subsequent requests data request delay serving cache 
total response time nc nas direct access model access client results metadata request metadata response fetch 
subsequent access client requires fetch metadata cached considering data caching client controllers scenario nas model 
total response time dir dir hybrid model determine value object considered valued cached hs 
hs return metadata client cached clients subsequent accesses 
hyb sum delays due concurrent metadata data transactions 
response time hyb hyb hyb glance analysis looks favor nas model important note nas higher due costs model 
addition workload read workload 
indicates hybrid model strategy obtain data storage hs cache 
plan explore extension 
consider delays due implementation caching policies models detailed version 
experimental results model evaluated performance nas direct access hybrid models number assuming additional cost including metadata response average response times nas direct hybrid number clients varying number clients average response times nas direct hybrid load client load scenarios 
file data file sizes distributed poisson distribution mean kb 
accesses files zipf law 
values assumed poisson distributed mean ms 
nas cache implemented lru cache 
hs cache implemented partitioned memory model 
plots average response times models varying number clients ratio nas cache size hybrid data cache size mb 
seen increasing number clients queuing delays nas servers increase significantly response times increase 
direct access model queuing delays increase smaller rate metadata transactions 
hybrid model presents interesting analysis 
jump number clients response time decreases slightly 
due initial increase data locality hybrid cache multiple clients 
lesser data transactions hybrid server queuing delays increasing number clients able offset 
transactions delays dominant 
plots models increasing read percentage workload clients 
expected nas hybrid models perform better due caching effects 
plots models varying cache size hybrid cache nas cache fixed mb 
expected performs better bigger caches 
encouraging note hybrid model able outperform models due ability intelligent choices request 
continue evaluate models criteria real benchmarks 
related data management file systems storage area networks best knowledge prior describing san existing band band access models ability switch models request 
general caching important performance enhancing mechanism application wide range problems 
number caching algorithms lru targeted simplicity implementation complex algorithms extensively variety scenarios databases web caching approaches follow similar principle cost aware caching 
caching policy similar algorithm differ cache admission parameter evaluation 
especially policy closely aligned data sharing mechanisms works focus environments 
percentage reads varying read write ratio average response times nas direct hybrid hybrid cache size varying hybrid cache size hybrid access model storage area networks 
best knowledge attempt designing sans exploit strengths band nas band direct access models unified solution 
important characteristic design intelligently choose access models request granularity low overhead cache admission cache replacement policies 
initial analysis indicates hybrid model outperforms nas direct access models variety workloads 
evaluate hybrid model real benchmarks design extensions model 
anderson dahlin neefe patterson roselli wang 
serverless network file systems 
proc 
sosp pages 
burns 
data management distributed file system storage area networks 
phd thesis ucsc 
cao irani 
cost aware www proxy caching algorithms 
proc 
usits 
gibson corbett 
problem statement 
ietf internet draft 
jin bestavros iyengar 
accelerating internet streaming media delivery network aware partial caching 
proc 
icdcs page 
johnson shasha 
low overhead high performance buffer management replacement algorithm 
proc 
vldb pages 
megiddo modha 
arc self tuning low overhead replacement cache 
proc 
fast 
menon pease rees 
ibm storage tank heterogeneous scalable san file system 
ibm systems journal 
sandberg goldberg kleiman walsh lyon 
design implementation sun network filesystem 
proc 
usenix 
scheuermann shim 
watch man data warehouse intelligent cache manager 
vldb journal pages 
singh trivedi ramamritham shenoy 
ptc proxies cache heterogeneous web client environments 
world wide web journal 
