published proceedings fifth international conference microelectronics neural networks fuzzy systems lausanne switzerland february 
hardware friendly learning algorithms neural networks overview idiap cp ch martigny switzerland mail perry idiap ch hardware implementation artificial neural networks learning algorithms fascinating area research far reaching applications 
mapping ideal mathematical model compact reliable hardware far evident 
presents overview various methods simplify hardware implementation neural network models 
adaptations proper specific learning rules network architectures discussed 
range perturbation multilayer feedforward networks local learning algorithms quantization effects self organizing feature maps 
general terms problems inaccuracy limited precision robustness treated 
decade demonstrated neural networks capable providing solutions problems areas pattern recognition signal processing time series analysis 
software simulations useful investigating capabilities neural network models fulfill need real time processing necessary successful application neural networks real world problems 
fully profit inherent parallelism neural networks hardware implementations essential 
mapping existing neural network algorithms resultant networks fast compact reliable hardware difficult task 
learning rules better suited hardware implementation proposed 
hardware friendly learning algorithms divided subclasses 
adaptations existing neural network learning rules facilitate hardware implementation lead better exploitation chip area parallelism 

learning algorithms conception suitable hardware implementation 
example class perturbation algorithms eliminate calculation derivative activation function need separate circuitry backward pass widely backpropagation algorithm 
example second class cellular neural networks represent general class networks original definition terms analog circuitry due local connectivity suited vlsi implementation 
overview hardware friendly algorithms various neural network models 
presenting remedies typical problems encountered hardware implementation neural networks outlined 
hardware implementations neural networks problems constraints kind implementation neural networks analog electronic digital electronic optical hybrid brings various constraints accuracy compared ideal neural network models hardware implementations offer limited accuracy 
examples phenomenon representation weight values small number bits opposed realvalued weights model ii non uniformity circuit components ideally supposed identical iii non linearity effects components multipliers 
area design hardware implementations requires constant interplay accuracy required chip area available degree parallelism 
reliable elements available incorporation comes price area penalty reduction degree parallelism 
envisage different approaches solve hardware related problems 
firstly improvement hardware required implementation neural networks crucial importance 
example pulse modulation techniques combine advantages digital analog electronics design compact reliable components 
second approach try overcome adapting existing learning algorithms designing new hardware friendly algorithms focus 
hardware friendly learning algorithms multilayer feedforward networks popular algorithm training multilayer feedforward networks backpropagation algorithm popularized rumelhart 
realization analog hardware poses serious problems need accurate derivative activation function calculation gradients error surface 
disadvantage need separate circuitry backward pass algorithm 
perturbation algorithms general idea perturbation algorithms obtain direct estimate gradients slightly perturbing parameters network forward path measure change network error implies circuitry backward pass dispensed 
advantage priori knowledge non linearity implementation robust hardware non linearities 
perturbation algorithm rule neuron perturbation gradient estimated perturbing input de perturbation 
schematic weight perturbation algorithm value neuron 
node perturbations done serially computational complexity increases considerably compared standard backpropagation 
requires extra circuitry addressing selection neurons 
perturbation weights see eliminates extra circuitry needed implement node perturbation 
performs better limited precision weights 
comes price higher computational complexity stems fact weights weights output layer perturbed serially 
complexity weight perturbation algorithm addressed reduced viewing perturbation weights incoming neuron summed weight perturbation neuron 
result weight perturbation method improves iii rule require access hidden neurons computational complexity 
scheme implemented hardware shows behaviour small benchmarks 
loss parallelism weight perturbation scheme overcome perturbing weights simultaneously resulting error update weights 
reliable estimate gradient multiple perturbations performed number normally quite small compared total number weights network 
similar approach fact pursued studying detail convergence properties 
chain rule perturbation addresses complexity standard weight perturbation employs chain rule approximation gradient enables weights going neuron perturbed parallel 
improves iii rule summed weight perturbation assumptions multiplication allowing non linear synapses typical analog implementations 
local learning algorithms anti hebbian local learning algorithm described weight update certain layer depends input output layer global error signal 
local learning rule circumvents backpropagation error signals complicates hardware implementation backpropagation algorithm 
gradient descent rule guaranteed synaptic weights updated error descent direction 
brandt lin developed algorithm requires explicit backpropagation errors uses information locally available neuron importantly rates change outgoing weights 
local algorithms equivalent standard backpropagation algorithm 
algorithm especially measuring rates change weights hard implement 
training derivatives necessity derivative activation function backpropagation algorithm circumvented approximation needs non linearity backward pass 
example established chosen taylor expansion offers close approximation original algorithm 
completely different approach exclude derivatives learning algorithms taken battiti 
reactive tabu search heuristic method solve combinatorial optimization problems 
applied training neural networks transforming continuous space weights discrete gray encoding weight values 
heuristic obtain new set weights choose configuration smallest error value differs bit current configuration gray encoding method performs fact discretized form steepest descent 
order avoid cycles changing weight configuration confined limited part search space additional constraints included heuristic 
advantage reactive tabu search limited precision weights needed 
characteristics suitable hardware implementation illustrated totem chip 
complex backpropagation applications hardware implementation nn accepts sinusoidal complex valued signals 
backpropagation extended complex domain allowing complex valued inputs weights activation functions outputs 
carefully solves problem design suitable complex activation function bounded non linear differentiable easily implementable 
properties exclude example complex extension standard sigmoid function unbounded 
threshold networks design compact digital neural network simplified considerably hard limiting threshold gates activation functions differentiable sigmoidal nonlinearity 
training algorithms layer threshold networks perceptrons abound limited solving linearly separable problems 
constraint resolved allowing layers neurons algorithms training multilayer networks gradient descent require differentiable activation function 
development training algorithms multilayer networks threshold activation function important issue nn hardware implementation 
madaline ii rule closely related neuron perturbation madaline iii 
discontinuities threshold network exclude direct estimation gradient 
error minimized way small neuron input second layer perturbed see inversion activation value neuron reduces hamming error output neurons 
case incoming weights neuron adapted perceptron algorithm reinforce inversion 
procedure repeated output layer reached weights directly updated perceptron algorithm 
approaches trying standard backpropagation obtain threshold networks 
steepness sigmoidal non linearity gradually increased training network obtain final network thresholds fact sigmoid function approaches threshold steepness parameter approaches infinity 
algorithm useful line training network appropriate 
host called constructive algorithms gradually building threshold network adding neurons weights 
example geometrical approach construct multilayer threshold network 
goal find set separating hyperplanes hidden layer neurons input pattern space property inputs located neighbouring hyperplanes target output 
advantage constructive algorithms number neurons hidden layer need specified priori 
course constructive algorithms suited implementation hardware resulting compact threshold networks 
kohonen self organizing maps basic elements kohonen algorithm selforganizing maps reproduce input probability distribution compact way time selection neuron weight vector closest input pattern winner neuron minimal distance kx gamma ii update weights ff gamma ff adaptation gain neighbourhood function depends winner neuron neuron consideration 
description clear original algorithm demanding terms computing resources calculation euclidean distance multiplication weight storage adaptation functions 
various adaptations algorithm proposed assist hardware implementation 
crucial issue hardware implementations influence quantization weights inputs behaviour learning algorithm 
studies quantization effects kohonen network demonstrates consequences greatly reduced having neighbourhood function decreases distance winner neuron neighbours 
experiments show bits sufficient guarantee convergence solution close solution obtained quantization 
key features digital design easily calculable manhattan distance jx gamma stead euclidean distance see ii quantization adaptation gain powers rectangular neighbourhood function iii binary input values quantization weights bits 
way implementation obtained uses multipliers high degree parallelism price slightly bigger map size shows results comparable original algorithm 
demands nn implementation systolic arrays effective processor resources 
general batch processing appropriate means obtain better parallelisation 
kohonen original algorithm line winner selection line weight update 
possible variants batch winner selection batch weight update ii batch winner selection line weight update 
shown convergence properties variants comparable original line algorithm second variant identical performance original algorithm 
recurrent networks class recurrent networks exhibits complex dynamical behaviour needs reliable method training recall 
widely paradigms training recurrent networks boltzmann machine mean field theory mft learning 
boltzmann machine boltzmann machine stochastic learning rule uses locally available information reason suited hardware implementation 
parallelism potential hardware implementation severely constrained required asynchronous update neurons 
analog neurocomputer synchronous version boltzmann machine 
peculiarity boltzmann machine simulated annealing gradual increase gain activation function 
bellcore implementation boltzmann machine annealing schedule replaced gradual decrease additive noise efficiently implemented analog hardware 
mean field theory method idea simulated annealing process stochastic boltzmann machine time consuming replaced deterministic mean field approximation 
optical design demonstrated mft algorithm synchronous updating neurons leads results suitable hardware implementation 
types neural networks ram networks special class neural networks random access memories fit hardware implementation 
network model easily implemented standard available components disadvantage limited learning capacity 
various generalizations original model developed extended capabilities complex realization hardware 
overview ram networks related implementation aspects 
cellular neural networks cellular neural networks particular interest vlsi implementation sparse connectivity 
unit network simple analog processor interacts directly neighbouring units small range 
range network dynamics connection complexity independent number units implementation scales bigger networks 
extensive overview chua cellular neural network paradigm 
inaccuracy robustness key issue hardware implementations neural network models required precision parameters hardware implementation liable imperfections limited numerical precision component imprecision noise stuck faults weights neurons 
inaccuracies subject investigation neural network models show fact remarkable degree robustness inaccuracies incorporated training network 
limited precision different types limited precision discerned 
firstly limitation representation values small number bits plays major role digital neural network implementation 
secondly limited precision caused component imprecision example non linear responses multipliers variations components 
problem paramount electronic optical implementations 
large range theoretical experimental studies performed investigate confine effects limited precision computation 
limited numerical precision accuracy needed representing weights neural network area consuming incompatible system noise analog implementations 
number different weight values network small possible order obtain efficient accurate implementation 
different network architectures learning algorithms effect limited weight precision investigated 
common tenor investigations certain level limitations large influence behaviour network 
example accuracy needed standard backpropagation training algorithm order deviate ideal learning trajectory generally bits 
note accuracy needed forward pass lies bits 
order reduce chip area needed weight storage overcome system noise reduction number allowed weight values desirable 
weight discretization algorithms backpropagation learning rule designed training multilayer networks limited number weight values gamma bits 
rationale weight discretization algorithms keep update weights high resolution line discrete weights forward pass methods best suited chip training 
component imprecision state art analog optical electronic hardware progressed considerably decade compared digital technology mature discipline design reliable identical components gives rise problems 
analog electronic implementations example complex efficiently construct linear multiplier sufficient operating range simple non linear multipliers preferable inevitable 
examples non linear multipliers analog electronic optical implementations 
shown backpropagation learning rule compensate non linearity multiplications incorporating non linear multiplier derivation 
problem analog hardware construction activation function close widely standard sigmoid 
incorporation accurate model hardware activation function training algorithm compensate inaccuracy 
additional difficulties arise analog optical implementation sigmoidal function intensity encoding limitation non negative values means nonlinearities shifted non negative domain gain steepness differs greatly 
analog electronic implementations easily deal non standard gain including gain stage impossible intensity optical implementation 
adapted backpropagation learning rule precise relation gain initial parameters provides compensation non standard gain 
important aspect analog implementations non uniformity elements multipliers activation functions 
exemplified accurate modeling non chip learning compensate component variations 
robustness years ago robustness neural networks mainly folk theorem investigated quite thoroughly years 
examples robustness neural networks inaccuracies 
influence faulty weights neurons noise discussed detail 
faulty weights neurons removal interconnection weights network occurrence stuck faults neurons types faults serve test bed robustness neural networks 
robustness backpropagation trained multilayer network removing weights hidden layer influence redundancy form excess hidden neurons investigated 
graceful degradation network performance weight removal observed addition hidden neurons deteriorate results 
concluded standard backpropagation training inherently fault tolerant 
augmentation technique tries introduce linear dependencies trained network adding hidden neurons leads better robustness 
effect stuck stuck neurons solutions recurrent optimization networks investigated 
type network exhibits high degree fault tolerance ability find sub optimal solutions optimization problems presence stuck faults neurons 
widely believed verify robustness neural network model posteriori incorporate robustness criterion training phase 
done changing objective function optimized training injecting expected faults training 
illustration fact adaptation backpropagation learning rule uses random subset hidden neurons iteration 
resulting network far robust destruction hidden neurons small loss accuracy noiseless case 
noise perturbation surprising effects injection random noise weights multilayer neural network training backpropagation algorithm discussed murray edwards 
analytically experimentally demonstrated synaptic noise improves network fault tolerance weight damage generalization unseen patterns training trajectory 
similar results obtained injecting additive noise weights recurrent neural networks 
theoretical study effect perturbations parameters general class feedback neural networks studied 
type network important know stable states perturbed networks close original stable states 
shown reasonable conditions linear relationship holds perturbations network parameters resulting error stable states 
summary overview variety methods developed facilitate hardware implementation neural network models 
known neural network models brings specific problems hardware implementation 
example standard backpropagation algorithm accurate derivative activation function complicates implementation realization hardware boltzmann machine hindered sequential update neurons 
hardware friendly algorithms described geared implementation chip learning 
advantages onchip learning manifold include gain speed inherent compensation component inaccuracies adaptation new training patterns 
chip learning rules described realized hardware efficacy difficult judge 
notable exceptions bellcore implementation boltzmann machine mean field theory algorithm battiti totem chip reactive tabu search 
attention learning algorithms suited hardware implementation resulting network efficiently implemented 
important example class threshold networks training constructive methods evolve network topology training 
key problem realizations neural networks hardware inaccuracy imperfections hardware components 
ranges quantization weight values component variations stuck faults weights neurons 
aspects discussed exemplified neural network models remarkably robust limited precision inaccuracies incorporated training network 
alspector 
experimental evaluation learning neural microsystem 
advances neural information processing systems nips vol 
pp 
morgan kaufmann san mateo 
alspector meir 
parallel gradient descent method learning analog vlsi neural networks 
advances neural information processing systems nips vol 
pp 
morgan kaufmann san mateo ca 
austin 
review ram neural networks 
proceedings fourth international conference microelectronics neural networks fuzzy systems pp 
turin italy september isbn 
battiti 
totem digital processor neural networks reactive tabu search 
proceedings fourth international conference microelectronics neural networks fuzzy systems pp 
turin italy september isbn 
battiti 
training neural nets reactive tabu search 
ieee transactions neural networks vol 
pp 
september 
brandt lin 
supervised learning neural networks explicit error backpropagation 
proceedings second allerton conference communication control computing pp 
monticello illinois september 
card schneider 
analog cmos neural circuits situ learning 
international journal neural systems vol 
pp 

fast stochastic error descent algorithm supervised learning optimization 
advances neural information processing systems nips vol 
morgan kaufman san mateo ca 
chua 
cnn paradigm 
ieee transactions circuits systems fundamental theory applications volume pp 
march 

iterative method training multilayer networks threshold functions 
ieee transactions neural networks vol 
pp 
may 
damper 
determining improving fault tolerance multilayer perceptrons pattern recognition application 
ieee transactions neural networks vol 
pp 
september 

universal weight discretization method multi layer neural networks 
ieee transactions systems man cybernetics ieee smc accepted publication 
see 
weight discretization paradigm optical neural networks 
proceedings international congress optical science engineering vol 
spie pp 
spie bellingham washington 
isbn 
flower jabri 
summed weight neuron perturbation improvement weight perturbation 
advances neural information processing systems nips vol 
morgan kaufmann san mateo ca 
wong 
backpropagation learning analog neural network hardware 
ieee transactions neural networks vol 
pp 
january 

complex domain backpropagation 
ieee transactions circuits systems analog digital signal processing vol 
pp 
may 
hamilton murray baxter churcher reekie tarassenko 
integrated pulse stream neural networks results issues pointers 
ieee transactions neural networks vol 
pp 
may 
hertz krogh lehmann 
nonlinear back propagation doing back propagation derivatives activation function 
preprint available march file archive cis ohio state edu pub hollis 
neural network learning algorithm tailored vlsi implementation 
ieee transactions neural networks vol 
pp 
september 
holt 
hwang 
finite error precision analysis neural network hardware implementations 
ieee transactions computers vol 
pp 
march 
jabri flower 
weight perturbation optimal architecture learning technique analog vlsi feedforward recurrent multilayer networks 
ieee transactions neural networks vol 
pp 
january 
jim giles horne 
synaptic noise dynamically driven recurrent neural networks convergence generalization 
technical report umiacs tr cs tr institute advanced computer studies university maryland college park md may efr 
theoretical investigation robustness multilayer perceptrons analysis linear case extension nonlinear networks 
ieee transactions neural networks vol 
pp 
may 
kim 
park 
geometrical learning binary neural networks 
ieee transactions neural networks vol pp 
january 
uhl 
analog cmos implementation multilayer perceptron nonlinear synapses 
ieee transactions neural networks vol 
pp 
may 
verleysen 

analog implementation kohonen map onchip learning 
ieee transactions neural networks vol 
pp 
may 
saxena 
effects optical thresholding backpropagation neural networks 
proceedings international conference artificial neural networks icann paris france october 
murray edwards 
enhanced mlp performance fault tolerance resulting synaptic weight noise training 
ieee transactions neural networks vol 
pp 
september 

high capacity neural networks hardware 
applied optics vol 
pp 

peterson keeler hartman 
architecture multilayer learning single crystal 
neural computation vol 
pp 


selection weight accuracies 
ieee transactions neural networks vol 
march 

performance fault tolerance neural networks optimization 
ieee transactions neural networks vol 
pp 
july 
qiao 
adaptive multilayer optical networks progress optics editor wolf vol 
chapter pp 
elsevier science publishers amsterdam netherlands isbn 
klein 
ra analog neurocomputer synchronous boltzmann machine 
proceedings fourth international conference microelectronics neural networks fuzzy systems pp 
turin italy september isbn 
rueckert 
chip selforganizing feature maps 
proceedings fourth international conference microelectronics neural networks fuzzy systems pp 
turin italy september isbn 
rumelhart hinton williams 
learning internal representations error propagation 
parallel distributed processing explorations microstructure cognition vol 
foundations pp 
mit press cambridge massachusetts 
isbn 
saxena 
adaptive optical neural multilayer network optical thresholding 
optical engineering issn special optics switzerland rastogi editor vol 
pp 


neural network constructive algorithms trading generalization learning efficiency circuits systems signal processing vol 
pp 

tang kwan 
multilayer feedforward neural networks single power weights 
ieee transactions signal processing vol 
pp 
august 

interchangeability learning rate gain backpropagation neural networks accepted publication vol 
neural computation 
see 
results steepness backpropagation neural networks 
proceedings workshop parallel distributed computing ed 
aguilar pages institute informatics university fribourg fribourg switzerland october 
heim 
quantization effects digitally behaving circuit implementations kohonen networks 
ieee transactions neural networks vol 
pp 
may 

modify kohonen self organizing feature maps efficient digital parallel implementation 
proceeding international conference artificial neural networks cambridge june 
wang michel 
robustness perturbation analysis class artificial neural networks 
neural networks vol 
pp 

widrow lehr 
years adaptive neural networks perceptron madaline backpropagation 
proceedings ieee vol 
september 
