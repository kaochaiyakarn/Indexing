published nonlinear structures physical systems pattern formation chaos waves lam morris springer verlag new york 
information metric crutchfield information taken primary physical entity probabilities derived 
information produced source defined class sources 
shannon entropy family formal renyi information measures space unique sources 
measures quantifies volume source recoding equivalence class 
space information sources constructed elements class recoding equivalent unique sources 
norm space source entropy 
measure distance information sources derived algebra measurements 
space information sources shown metric space logic described metric lattice 
applications information metric quantum informational uncertainty information densities multicomponent dynamical systems outlined 

brief note introduces measure distance information sources 
demonstrates space information sources topological structure heretofore utilized directly applications information theory study complex systems 
space information sources fact metric space geometric picture ascribed 
furthermore considers elemental events information source experimental measurements logic inference described metric lattice 
note restricted presentation formal details geometric interpretation motivational remarks order 
aspects information theory physics complex systems suggest need better understanding formal structure concept information 
briefly observational theory chaotic systems subjectivity information modeling complex dynamical systems multicomponent information sources 
crutchfield physics department university california berkeley california usa internet chaos berkeley edu 
explicit physical theory observing chaotic behavior necessitated chaotic process extreme exponential sensitivity extrinsic fluctuations ii impossibility infinitely precise determination system state iii impossibility employing infinitely large computational resources prediction states arbitrary times 
limitations rigorous lower bounds classical deterministic universe practical macroscopic facts 
proper framework description behavior information theory renyi noted information subjective probability 
position taken simple corollary renyi information theory provides quantitative consistent framework describe physical processes admit partial knowledge 
information framework functions 
quantifies utility observations prediction behavior conversely quantifier behavioral complexity 
second higher level abstraction measures utility observations construction models chaotic data analysis fields number information theoretic quantities referred distances 
notable quantities conditional mutual information jeffrey divergence kullback information gain noted authors communication theory mathematical statistics satisfy metric properties required distance function 
discussion alleviates informational quantity distance function 
aside clarifying problems chaotic data analysis information metric presumably information theory proper provide starting point extending information theory sources producing transmitting receiving information simultaneously 
complete discussion topics detailed discussion results sequel 

information coding step discuss sources information 
deterministic nondeterministic 
examples pseudo random number generators chaotic dynamical systems markovian general stochastic processes noisy dynamical systems case sources producing distinct discrete events considered 
information source shall denoted measurement state information source yields finite number symbols 
symbols simply measurements come finite alphabet elements events fa 
tantamount model measuring instrument observe source 
source states measuring instrument uniquely identify say continuous model measuring instrument needs specify information source states grouped measurement symbols observation process 
case association source states instrument measurements called partition information source state space space unique sources fs collection information sources 
dimension space unspecified 
sources consider done information 
acquisition processing inferring information functions observer 
observer essentially defined available observation resources 
consist measurement resolution rate storage capacity computational power 
delineating resources constitutes outline model observer 
discussion require development model useful keep mind general goal 
central problem observational theory stated follows 
arbitrarily large number measurements finite observational resources algorithmic structure information source deduced 
number related questions existence optimum partitions instruments bounds obtainable information size available statistics affect quantities subsidiary questions wait main questions addressed 
purposes dependence partition geometry ignored 
information sources considered resultant measurements produced instrument 
words source output instrument filtered observed behavior partition 
conventional developments information theory measure complexity source terms entropy 
entropy function space sources yields real numbers source characterized probability measure 
respect entropy degeneracy information sources entropy 
sources mean sources produce information merely quantity information produced sources possibly information production rates 
nature degeneracy explored existence transformations encodings sources 
recall shannon coding theorem noiseless channel 
uni directional channel capacity input source encoded output source input source reconstructed output 
say channel looses information errors transmission 
considered information source faithful reproduction proper channel encoding 
channel considered mechanism transforms source 
somewhat anticipating development applied observation process note encodings capture conditional nature measurements 
time measurement particular measurement occurrence conditioned physical process state 
clarify situation define encoding source transformation source alphabet second 
encodings describe asymmetric relationships denoted defined encoding order relation space partially orders information sources 
notion source equivalence established bi directional channel 
define recoding coding recoding sources isomorphic follows 
terms bi directional channel channel capacities direction transformation shannon theorem requires recoding 
bound complexity recoding considered transformation 
additional restrictions placed 
allow temporal translation may decorrelated considered equivalent time shift coding 
instantaneous encodings may allow finite time construction 
restricted encodings consistent shannon theorem framework number formal properties straightforwardly established 
recoding binary relation sources ae equivalently fa fb sets possible measurements 
theorem recoding equivalence relation proof 
reflexive rg ae equivalently ae follows noting identity encoding recoding 

symmetric rg follows reversibility recoding 

transitive rg ae follows composition codings 
establishes equivalence relation information sources sources equivalent exists coding transforms measurement sequences 
words measurement sequences source inferred probability finite computational resources 
means converges error probability ffi ffl resolution 
resolution refers average closeness representations 
coding called recoding emphasize observations exist code perceived processed stored 
sources discrete recoding permutation matrix continuous recoding homeomorphism invertible 
note definition equivalence notion computational difficulty computing transformation included 
quantitative aspects computational complexity missing geometric picture 
extending definition equivalent sources account appears interesting direction 
denote subset sources recoding equivalent source fs equivalence class speaks information refers entity shared sources equivalent recoding 
observation suggests definition information 
information source equivalence class symbol sequences noteworthy information generally left undefined information theory 
information theory considers amount information rates production loss transmission measured various entropies 
recoding equivalence reflects degeneracy due transformation symbols source second consequence existence encoding possible meaning source symbols symbol sequences may relevance context symbols 
leave stage consideration anthropomorphic projections context meaning return questions mention simple example 
consider situation context mathematical language recognition 
exists trajectory chaotic rossler attractor coarse measuring instrument measurement symbols symbol sequence encodes system equations motion 
meaning mathematical observer meaningless observer encodes information 
standard result binary relations gives theorem recoding partitions space unique information sources mutually disjoint subsets 
proof element subset recoding equivalent 
suggests definition information space set equivalence classes ig 
elements shall denoted 
distinguish classes sources unique sources elements shall objects interest 
necessary logical distinction class constituent sources blurred 
information source construed common properties members 
source generic source 
speak similar vein events measurements source 
information geometry preceding development information theory relies probability measure 
coincidence formal equivalence established appropriate restrictions probability derived information 
probability taken secondary concept 
quantity information entropy measure space information sources sense probability measure event space 
entropy source quantifies size volume equivalence class 
establishes entropy measure sets partially geometric picture 
starting point development definitions 

origin measurement set predictable 
norm kxk source entropy 

addition sources union measurements fall events 
product sources intersection measurements fall events common operations yield algebra measurements 
step establish entropy measure 

source 
sources independent 
follows 


sources determine distance sources requires measure difference symmetric difference set theoretic sense 
information source formally define kx yields generalized picture norm source distance origin predictable zero entropy sources kxk kx interpretation information source roughly speaking common events missing 
set theoretic terms constituent independent sources events ii corresponds entropy source measurements defines conditional entropy jy measurements determined probability 
define source union sources algebra measurements follows informational terms measure size entropy measure non commonality distance jz step follows independence continuing find xjy jx established starting entropy norm information source algebra measurements allows define conditional sources readily follows kx 
associated pseudo geometric picture shown fig 

information metric information quantity xjy jx interpreted total independent information 
establish metric properties 
proofs straightforward basic steps order elucidate central role recoding 
theorem metric metric space 
proof symmetry 
follows directly symmetry definition 
equivalence assume 
conditional entropies positive zero sum zero individually vanish 
consider zero conditional entropies xjy 
measurements knowing provide new information may inferred probability recoding may measurements measurements 
similarly jx vanishes recoding measurements source geometric interpretation space information sources 
shown distances information vectors sources 
coding measurements equivalent sources portion assume recoding measurements measurements source deduced probability visa versa 
follows conditional entropies vanish distance 
triangle inequality 
consider expansions variable joint entropy 
xjy noting additional measurements increase entropy xjy xjy xjy subtracting average independent entropy yields xjz zjx xjy jx jz zjy metric 
follows pair space recoding equivalent information sources metric space 
completes proof 
theorem indicates space information sources quite bit topological structure 
example notion ffl balls close information sources continuity functions information sources limits convergence sequences information sources developed 
numerical computations information distances follow sequel 
define normalized metric follows xjy jx note case independent sources 
information fluctuations information metric readily extended account fluctuations information renyi generalization shannon information starting point 
brevity sake shall simply quote results terms probabilities developing axiomatically 
recall definition ff order renyi information probability distribution ff ff log ff ff order conditional information ff jx ff ff log ff ff yjx ff ff renyi metric ff ff xjy ff jx standard note ff ff reduces normalized shannon metric 
spectrum information fluctuations source investigated ff dependence renyi metric ff ff provides metric say comparing fluctuations different sources 

applications preceding established geometric picture underlying information theory measurements unpredictable processes 
closing briefly mention applications conclude remarks philosophical context 

noted recoding binary relation induces partial ordering space information sources 
existence information metric follows metric space metric lattice established framework discuss relationship inferential logic underlying observations information sources quantum logic developed developed birkhoff von neumann context observing chaotic dynamical systems similarity chaos quantum mechanics 
example initial information prepared state decay necessitating measurements determine states 
additional measurements observer ignorance collapses revealing system actual state 
said simply just process observation central quantum mechanics model measurement process required chaotic physical systems 
second application metric derivation informational form quantum mechanical uncertainty 
consider wigner wave function state space associated joint probability density function position momentum example consider dimensional gaussian 
gaussian gives maximum information consistent mean standard deviation position momentum 
conditional distributions required metric readily formed pjq qjp informational distance measurements position momentum different sources pjq qjp give significant result change coordinates unit sized physical system planck constant scale 
find informational uncertainty principle log note applies time shift invariant measurement system quantum mechanics 
suggests possibility measuring effective quantization data set measuring observables mutual distance 
observable pairs metric vanish effectively conjugate 
interpretation result quantum mechanics conjugate variables quantum mechanics closer approximately bits 
bits information 
measured order characterize quantum system state 

final arena application introduce general notion information densities 
multicomponent systems wishes measure information production transport properties 
take example consider dynamical systems 
generally define information density spacetime point terms metric lim ffi ffi ffi ffi space time separation 
source asymptotic distribution reconstructed data point yields dimension density 
source asymptotic distribution measurement sequences obtained point yields entropy density 
exposition applications appear 

concluding remarks question arises development simply mutual information information metric 
aside pseudo geometric picture note measures kind informational correlation 
information metric quantifies degree recoding equivalence 
provides insight nature information 
mutual information derivative concept simply reflects properties shannon entropy 
foregoing mathematical development instantiates particular philosophical viewpoint phenomenology 
observer developing understanding world finite measurements attendant information 
intrinsic finiteness derives foremost limited computation resources available observer finite space time region 
information space developed substrate perception quantification modeling building 
structured pseudo geometry just shown 
suitable restrictions justified observations form probabilities say frequencies events 
information theory founded quantitative measure amount information 
foregoing formal definition information terms equivalence class structure sources 
meaning information 
motivation unstated point conviction understanding topological structure metric lattice inferential logic necessary developing quantitative measure meaning context 
offer immediate answer question hope progress 
shall return question 
author diaconis extended loan divergent variations lead current 
post doctoral fellowship international business machines supported academic year 
development funded part onr contract 
postscript distributed october 
appears original form minor corrections 

crutchfield noisy chaos 
phd thesis university california santa cruz 
published university microfilms intl ann arbor michigan 

shaw strange attractors chaotic behavior information flow vol 


renyi fundamental questions information theory selected papers alfred renyi vol 
budapest 

crutchfield mcnamara equations motion data series complex systems vol 
pp 


shannon weaver mathematical theory communication 
university illinois press 

jeffreys theory probability 
oxford oxford clarendon press second ed 

kullback information theory statistics 
new york dover 

jacobson symbolic dynamics phys 
rep vol 


crutchfield packard symbolic dynamics dimensional maps entropies finite precision noise intl 
theo 
phys vol 


information probability colloq 
math vol 
ix 

birkhoff lattice theory 
providence american mathematical society third ed 

birkhoff von neumann logic quantum mechanics ann 
math vol 

