attending visual motion john tsotsos liu julio martinez marc zhou dept computer science york university toronto canada centre vision research york university toronto canada dept medicine mcgill university montreal canada dept computer science university massachusetts boston ma usa correspondence tsotsos cs yorku ca novel model attentive visual motion processing 
new feedforward motion processing pyramid described motivation lies neurobiology primate motion processes 
structure selective tuning st model visual attention implemented demonstrated showing localize label simple motion patterns 
main contributions new feed forward motion processing hierarchy include multi level decomposition processing including local spatial derivatives velocity separate layer examples st operate hierarchy localize label motion patterns new solution aspects feature binding problem show sufficient task grouping motion features coherent object motion 
feature grouping binding accomplished top attentional selection mechanism depend single location saliency representation 
selective tuning model proposal explanation computational behavioral levels visual attention humans primates 
key characteristics model previously detailed tsotsos tsotsos include top fine wta selection process unique wta formulation provable convergence properties wta region point selection task relevant inhibitory bias mechanism selective inhibition spatial feature dimensions elimination signal interference leads surround attended items task specific executive controller 
characteristics lead extensive set predictions supported experiment 
bulk focus attention visual motion 
past summarized showing studied issue 
new model motion processing demonstrated st operates representation changes previously described definition 
way points weaknesses previous demonstrations st remedied second original statement st generality wide variety visual processing representations third examples feature binding solved st complex motion patterns 
suggested previous demonstrations selective tuning model biologically plausible useful 
order demonstrate st operate realistic representations motion domain chosen known motion processing enable reasonable attempt defining feed forward pyramid 
effort unique past model motion hierarchy plus attention motion simoncelli heeger beardsley poggio anderson nowlan sejnowski grossberg zemel sejnowski pack perrone stone 
layout remainder presentation follows 
section detail feed forward motion processing network 
earlier versions network appear tsotsos 
overview st provided structure imposed feed forward network 
st detailed times past brief presentation reader referred tsotsos tsotsos tsotsos tsotsos tsotsos tsotsos details 
section iv show examples operation entire including feed forward feedback components new solution feature binding problem 
concluding discussion rounds 
ii 
feed forward motion processes motion representations processes modeled informed current knowledge motion analysis primate cortex 
literature large topic selected experimental observations order simplify models 
generally accepted motion processing monkey cortex goes series stages neural representations areas mt mst providing input see van essen 
areas specializes particular kinds motions contains populations neurons specialized certain motion features generally simple complex smaller larger receptive fields higher hierarchy 
different neural properties outlined section sub section devoted area 
model aims explain hierarchical feed forward network consisting multiple neural populations cortical areas mt mst primates detects classifies different kinds motion patterns 
best order motion model elaboration left 
previous motion models listed earlier offer better sophistication level processing cover levels incorporate selective attentional processes 
ii feed forward motion pyramid component motion analysis process feed forward data driven 
goal define set processing stages areas mt mst corresponding areas motion processing hierarchy macaque monkey van essen conform basic properties observed neural populations areas siegel read andersen duffy wurtz van essen 
brief characterization processing levels follows cells striate area selective particular local speed direction motion main speed ranges cells area mt kinds 
kind tuned particular local speed direction movement similar direction speed selective cells larger receptive fields 
second kind selective particular angle local movement direction spatial velocity gradient 
cells area mst tuned complex motion patterns expand approach contract clockwise counter clockwise rotation translation larger receptive fields cells area code different types patterns translation spiral motion mst full field rotation regardless direction radial motion expansion contraction largest receptive fields 
claim necessarily neural populations area primates simply ones modeled 
model includes neurons areas referred vf vi mt referred mt mg mst referred st ss referred 
number parameters numerical constraints set guidance literature available reasonable estimations 
depicts full motion hierarchy 
emphasizes scale search problem faced visual system determine responses representations belong event 
layer described turn 

full motion hierarchy ii area area receives visual input temporal sequence images 
spatiotemporal filters model selectivity neurons speed direction local motion see 
attempt employed spatiotemporal filter approach heeger number resolution images sequence output filters noisy subsequent velocity gradient computation area mt consequently computational mechanism defined generates appropriate input mt neurons 
mechanism paragraphs 
functionality layer realized types artificial neurons performing spatiotemporal filtering referred vf integrating local filter unit activations referred vi filter units spatiotemporal rfs provide access intensity values images sub sequence 
evaluation model 
intensity value position image taken time indicates indicates image sequence 
input space rf neuron vf oriented way local motion position direction speed sv induce constant intensity rf 
consists neurons distinct speed selectivity types type low speed type medium speed type high speed 
model neurons implemented different preferred speeds set pixels frame pixel frame pixels frame 
limit computational complexity model different preferred directions realized known wider range preferred directions exist area 
preferred direction preferred speed sv pixels frame spatial offset sv line constant intensity image taken time computed follows cos sv sv sin cases function yield integer values inputs image estimate actual intensity defining line constant intensity 
visualized sequence dimensional images see inputs image necessary 

illustration spatiotemporal energy computation darker pixel larger weight computation input image line passing centrally 
neuron vf receives input temporal layer computes intensity constancy value ic decreases increasing standard deviation intensity layers 
quantitative description real valued function sv ic vf sv maximum activation neuron central rf position neuron vf visual input 
intensities ranging value set ic assumes nonnegative values 
current implementation model order compensate noise visual input filter neuron rf consists multiple linear arrangements inputs 
preferred directions speeds sn arrangements deviate slightly preferred motion filter neuron 
probability functions describe statistical distribution variables sn 
sn 
sn 

filter neuron obtains value icn linear arrangement inputs activation neuron vf vf max ic obviously filter neurons reach state high activation motion preferred orientation velocity rf 
maximum activation induced motion region homogeneous intensity image sequence 
function integration units vi reduce noise raw filter unit activations eliminate pseudo motion detected filter cells 
achieved implementing lateral inhibition units vi identical positions speed selectivity different preferred directions motion vi vf 
vf set neurons outputs converge integration unit denotes number implemented preferred directions motion model 
formula assume directions 
achieve computation model uses vf neurons pixel visual field 
comprises neuron type different preferred speeds different preferred directions motion units 
furthermore model employs evenly distributed vi units receive input local filter units 
implementation size input images pixels integration units rfs covering neighboring filter units creating substantial overlap rfs 
integration units provide input model mt neurons 
parts show filter outputs layers hierarchy input image sequence purely motion defined object pattern random motion static random field gaussian noise 
motion counterclockwise rotating square 
darker pixel value representation stronger response white means zero response 

output area computations ii area mt group cells mt tuned particular local speed direction movement similar cells 
subpopulation mt neurons selective particular angle local direction movement speed gradient andersen 
mt designed different types neurons cells selectivity identical neurons larger rfs detectors translational motion cells selective local speed direction motion angle direction motion velocity gradient detectors velocity gradients 
mt implemented array neurons translation gradient detection speeds directions direction gradient angles 
mt cell receives input field neurons direction speed tuning 
translation neuron preferred direction motion speed selectivity type activation mt gi jv eq 
equations stands set units constitute rf neuron gi denotes value dimensional gaussian function position xj yj neuron rf neuron gi represents connection weights xj yj measured unit spaces relative center rf 
seen eq 
gaussian functions centered corresponding rf standard deviation chosen half length square shaped rf 
example mt neurons rfs size peak gaussian function second third rf neurons horizontally vertically standard deviation 
activation velocity gradient detector computed product activation cells feeding rf speed direction tuning gradient response 
gradient determined oriented rfs example rf detecting upward gradient velocity gradient neuron preferred direction motion preferred angle motion gradient speed selectivity type mg threshold ck coefficients linear reconstruction absolute speed activation types speed selective neurons specifies activation increase direction receptive field neuron speed selectivity type direction orientation specific configuration weights leads maximum activation inputs increase direction 
individual weights set ri order range neuron activation values independent rf size 
example rf preferred direction motion rightward configuration region input image consistently find angle motion speed gradient directions motion signifies particular motion pattern 
angle indicates expansion indicates clockwise rotation indicates contraction indicates counterclockwise rotation 
type spiral motion represented way example angle stands expansion smaller proportion clockwise rotation 
coding andersen andersen 
color coding angles shown 
output area mt computations input image sequence shown 
output area mt processes ii area mst cells mst larger receptive fields mt cells tuned complex motion patterns expand approach contract rotation duffy wurtz 
types neurons modeled translation spiral motion clockwise counterclockwise rotation expansion contraction combinations 
reason translational motion included area full pyramid translation scales included 
mst implemented array neurons translation mt types pattern selectivity motion patterns speeds 
mst cell receives input field mt neurons tuning mst cell 
activation translation neuron preferred direction motion speed selectivity type st gi response spiral neuron selective pattern angle direction speed gradient motion described ii speed selectivity type ss mg area mt direction speed gradient angle indicates expansion indicates clockwise rotation indicates contraction indicates counterclockwise rotation angles represent combinations motion types 
output area mst neurons shown output area mst computations ii area area involve different types computations larger rfs areas siegel read translation spiral motion mst rotation clockwise counterclockwise regardless direction radial motion irrespective direction expansion contraction 
types properties st ss respectively rf size gi jst gi jss neuron art responds rotation regardless direction rotation clockwise counterclockwise 
activation art neuron speed selectivity type computed follows gi jss direction selectivity function defined dimensional gaussian function neuron ard responds radial motion regardless expansion contraction 
activation ard neuron speed selectivity type gi jss area implemented array neurons translation speeds rotation speeds radial motion speeds times patterns spiral motions 
cell receives input field mst neurons relevant tuning 
output area computations shown output area computations ii motion hierarchy summary motion pyramid novel contains separate streams processing translation generalised spiral motions 
features decomposition motion simpler components including local spatial gradients velocity 
concluding section discussion appear 
previous figures demonstrate representations result different neural populations separate full field filter representations complex non trivial 
outputs noisy effect noise gradually ameliorated signals reach higher levels pyramid 
satisfying note complexity representation peak responses right terms correct feature detection search strategy described section successfully find peaks 
important acknowledge weakness resulted original motivation research described 
just earlier research motivated valid criticism past demonstrations tsotsos simple gaussian pyramids features st mechanisms demonstrated 
choice affect demonstrations properly show characteristics st demonstrations easy targets criticism allow possibility st mechanisms expected realistic feature pyramids 
chose address criticisms visual motion domain 
task defining motion neurons motion pathway addressed coarse order fashion filter definitions motion decompositions strong 
current research attempting improve motion representation way affect st demonstration strengthen motion representations 
iii 
selective tuning model visual attention modeling effort described distinguished ways neural network learns attend model goal explain particular set quantitative observations data fitting exercise set equations numerical simulation leads output functions form similar experimental data 
contrast trying show principles qualitative form visual processing take define theory accompanying computer simulation take input digital images perform qualitatively manner human primate vision performs 
features theoretical foundation provable properties theory computational complexity tsotsos 
principles arise vision formulated search problem specific input subset neurons best represent content image 
complexity theory concerned cost achieving solutions problems 
foundation suggests specific biologically plausible architecture processing stages briefly described article detailed account tsotsos tsotsos 
clear foundations derived considering visual search problem 
considerations related dimensions attention functionality remain note tsotsos saccades peripheral targets included model active visual search added st 
st compared attention models particular points need order keep length control general comparisons appeared previous st publications cited 
iii model visual processing architecture pyramidal structure units network receiving feed forward feedback connections 
stimulus input layer pyramid activates feed forward manner units pyramid receptive fields rfs mapping stimulus location result diverging cone activity processing pyramid 
assumed response strength units network measure goodness match stimulus receptive field model determines selectivity unit 
selection relies hierarchy winner take processes 
wta parallel algorithm finding maximum value set 
wta process operates entire visual field top layer computes global winner units largest response see section iii details 
wta accept guidance favor areas stimulus qualities guidance available operates independently 
search process proceeds lower levels activating hierarchy wta processes 
global winner activates wta operates direct inputs select strongest responding region receptive field 
connections visual pyramid contribute winner pruned inhibited 
result input higher level unit changes output changes 
refinement unit responses important consequence important goals attention reduce eliminate signal interference tsotsos 
refinement process output attended units top layer attended stimulus appeared blank field 
strategy finding winners successively smaller receptive fields layer layer pyramid pruning away irrelevant connections inhibition applied recursively pyramid 
result globally strongest response cause largest response localized sensory field earliest levels 
paths remaining may considered pass zone attended stimulus pruned paths form inhibitory zone attentional beam 
wta violate biological connectivity relative timing constraints 
gives pictorial representation attentional beam 

attentional beam executive controller responsible implementing sequence operations visual search tasks 
acquire target appropriate task store working memory 
apply top biases inhibiting units compute task irrelevant quantities 
see stimulus activating feature pyramids feed forward manner 
activate top wta process top layers feature pyramids 
implement layer layer top search hierarchical wta winners top layer 
completion permit time refined stimulus computation complete second feedforward pass 
note feedforward refinement completion lowermost wta process occurs simultaneously completing wta processes step proceed downwards hierarchy 
completion lowermost wta additional time required completion feedforward refinement 

extract output top layers place working memory task verification 
inhibit pass zone connections permit salient item processed 
cycle steps times required satisfy task 
multi pass process may reflect reality biological processes fast 
claimed steps needed tasks 
different levels tasks may distinguished defined detection particular item stimulus localization detection plus accurate location recognition localization plus accurate description stimulus understanding recognition plus role stimulus context scene 
executive controller responsible choice task instruction 
detection task winner step matches target suffice remaining steps needed 
simple detection framework requires single feedforward pass argued thorpe 
localization task required steps required argued section ii top wta needed isolate stimulus remove signal interference nearby stimuli 
clearly takes time accomplish 
recognition task steps iterations procedure needed order provide complete description 
understanding task similar requirements quite scope model point 
iii 
top selection st features top selection mechanism coarse fine wta hierarchy 
purely feedforward strategy sufficient riesenhuber poggio claim 
disagreement need top mechanisms task domain knowledge considered non trivial schemes exist 
biological evidence complexity arguments suggests visual architecture consists multi layer hierarchy pyramidal abstraction 
task selective attention find value location extent salient image subset architecture 
purely feed forward scheme operating pyramid fixed size receptive fields overlap able find largest single stimulus input local wta computations receptive field location lost stimulus extent considered 
ii fixed size overlapping receptive fields suffers spreading winners problem due neural convergence largest input value signal blurred output layer location lost extent ambiguous 
iii possible rf sizes layer intractable due combinatorics tsotsos 
case useful certain computer vision detection tasks considered reasonable proposal biological vision fails localize targets 
case iii plausible intractable 
case ii reflects biologically realistic architecture fails task localizing target 
reality purely feedforward scheme insufficient describe biological vision 
top strategy successfully determine location extent selected stimulus constrained pyramidal architecture 
iii wta saliency winner take scheme st defined iterative process realized biologically plausible manner insofar time convergence connectivity requirements concerned 
roots koch ullman model see lee yuille geiger valiant provides complete redefinition proofs convergence convergence properties fully described tsotsos 
basis distinguishing characteristic comes fact implicitly creates partitioning set unit responses bins width determined task specific parameter 
partitioning arises inhibition units value single unit absolute value difference pairs unit values 
wta process restricted converging single points formulations 
winning bin partition determination described claimed represent strongest responding contiguous region image formally proved tsotsos 
wta implementation uses iterative algorithm unit response values updated iteration convergence achieved 
competition iteration depends linearly difference unit strengths way 
unit inhibit unit competition response denoted satisfies 
inhibit impact competition unit weighted sum inhibitory effects magnitude determined shown tsotsos wta guaranteed converge defined properties respect finding strongest items welldefined convergence characteristics 
time convergence contrast iterative relaxation method specified simple relationship involving maximum possible value unit responses 
reason partitioning procedure uses differences values 
larger units inhibit units smallest responses units inhibit largest valued units 
result small response units reduced zero quickly time second largest units eliminated depends values units largest units 
result unit network easy characterize 
time convergence log largest value second largest value 
quite consistent behavioral evidence closer response strength units longer takes distinguish 
second competition depends linearly topographical distance units features represent 
larger distance units greater inhibition 
strategy find largest spatially contiguous subset winning bin 
spatially large contiguous region inhibit contiguous region similar response strengths smaller spatial extent units large region apply inhibition smaller region inhibit larger region smaller 
top layer global competition lower layers takes place receptive fields 
way process require implausible connectivity lengths 
efficiency reasons currently implemented units winning bin 
respect weighted sums computed practice weights depend strongly types computations units represent 
may task specific component included weights 
needed operation ensure unit values go zero 
iterative update continues bin positive response values remaining bins contain units values fallen note winning bin positive values value greater threshold order eliminate false detections due noise 
key question root wta process hierarchy determined 
conceptual description iterative implementation wta depends process described previous paragraphs 
max function implemented iterative process just described 
set feature maps output layers particular feature maps 
values location map represented root wta computation set competition top layers pyramid depending network configuration task biases weight computation 
winning value determined 
single active feature pyramid max 
contains feature map representing mutually exclusive features max max 
contains feature map representing features exist point wta process rooted location operating different feature pyramids max 
contains subsets representing features mutually exclusive set case complementary set case winning locations determined sum strongest response set method plus strongest response set method 
combination strategies 
wta process rooted location operating different feature pyramids max mx max mx result single saliency map model models itti koch ullman wolfe 
notable exceptions models 
deco claim salience map 
similarity ends 
strategy considers attentional effects provide mechanism attentional control signals generated says contribution perception attentional modulation visual areas 
deco model implicitly codes saliency distribution modulation feature maps 
feature maps relevant task enhanced inhibited dynamics network producing winners need explicit representation salience 
selection occurs inhibitory competition complete neuronal pools 
consider realistic implementation simple saliency matrices form input 
st single wta process necessarily simultaneous wta threads 
saliency dynamic local distributed task specific determination may differ processing layers required 
known feature combinations high complexity exist higher levels cortex assume possible combinations exist 
features encoded separately predefined set maps relationships competition cooperation provide potential combinations 
types competitions select combinations explored 
flexibility allows solution part binding issues arise domain 
wta process implemented utilizing top hierarchy units 
main unit types gating control units gating units 
gating control units associated competition layer top activated executive controller order wta process 
additional network top bias units provide task specific bias available 
communicate downwards gating units form competitive gating network wta receptive field 
competition uses eqs 
depends nature inputs receptive field 
particular competition converges gating control unit associated unit sends downward signals lower competition 
process continues layers converged 
iii simulation model implemented tested labs applying goal computer vision robotics tasks 
current model structure shown 
executive controller working memory motion pathway mt mst peripheral target area po gaze wta gaze controller implemented examples performance tsotsos wai tsotsos tsotsos tsotsos 
currently underway extend implementation object pathway binocular stimuli extensions executive controller recognition layers 

st full hierarchy iii full hierarchy example shows example purely motion defined object pattern random motion static random field gaussian noise 
motion counterclockwise rotating square sequence 
illustrates separate features different locations represented different maps bound process described 

attending motion defined object iii biological behavioral predictions tsotsos description structure model appeared basic predictions 
included support appeared suppression attended items spatial maps feature dimension tsotsos mounts tsotsos 
attention top process attentional guidance control integrated visual processing hierarchy centralized external brain structure implying latency attentional modulations decreases lower higher visual areas connor mehta 
attentional modulation appears feedforward neural convergence support connor 
topographic distance attended items distractors affects amount attentional modulation tsotsos 
additional predictions supporting arguments tsotsos tsotsos 
counter intuitive predictions hints experimental evidence provide strongest possible argument biological realism theory selective tuning model 
iv 
st attend localize motion patterns computational models primate motion perception proposed concentrate feedforward classical types processing address attentional issues 
strong evidence responses motion neurons areas mt mst modulated attention maunsell martinez 
result model feedforward computations neural responses high level areas mst indicate kind motion patterns input localize spatial position patterns 
st model applied feedforward pyramid adding required feedback connections hierarchical wta processes gating networks originally defined 
model attends motion exhibits single motion combination motion types serially focuses motion sequentially order response strength 
hierarchical wta described earlier finds globally active region 
region wta processes activated described section iii 
translational spiral motion patterns exist object eq 

remaining processing proceeds described earlier winning patterns 
model includes processes detecting onset offset events start described see wai tsotsos 
usually location image contains object motion eq 
decide competing types due dynamic nature wta mechanism simultaneous motions location considered 
eq 
general form wta process permit transparent motion point compete addition motions input 
iv feature binding major contribution demonstration st operate complex motion hierarchy method grouping features known binding problem computational neuroscience 
claimed particular strategy sufficient generality solve possible issues binding problem solve limited cases occur image sequences simple motion patterns 
instance solution investigate generality 
quoting canonical example binding suggested rosenblatt sort visual feature object shape correctly associated feature location provide unified representation object rosenblatt 
explicit association binding particularly important visual object order avoid incorrect combinations features belonging different objects known illusory conjunctions treisman schmidt 
examples varieties binding problems literature appear special issue neuron edited 
authors maunsell von der malsburg suggest specialized neurons code feature combinations introduced cardinal cells barlow may assist binding 
solution include cells suffice described solve localization problem 
demonstrated localized saliency wta decision processes precisely binding problem requires neurons different representations respond different features different locations selected selection location feature space bound pass zone attention mechanism 
single neuron top pyramid represents concept wta allows multiple threads bound location definition eq 

part difficulty facing research binding confusion definitions wide variety tasks included binding discussions 
example feature integration theory treisman gelade location feature assumes faithfully represented master map locations 
true location precision changes layer layer pyramid representation 
cortex accurate euclidean sense topography qualitatively preserved van essen 
wiring pattern matters order get right image bits right neurons 
binding needs occur layer layer simply problem high level consideration 
features different representations different location coding properties converge single cells necessitate active search process 
proposal supported architecture described object recognition pathway 
suggest specific patterns input cortical circuitry may permit new complex receptive field properties cortical neurons 
appears true feed forward feedback connectivity 
projections display complicated sub modular selectivity modules non overlapping highly intermixed 
structure necessitates different view neural inputs handled different inputs dealt differently 
strategy step providing computational framework architecture 
purposes argument consider 
location feature anchor permits features bound 
location defined broadly differently visual area practice considered local coordinates visual area think array local coordinates 
grouping features coincident location considered unitary group unit represent group 
features compose group may different locations represented different visual areas long converge units represent group 
group attended wta section iii find attend parts regardless location feature map representation 
solution aspect binding attends groups finds parts groups 
applies equally object recognition faces examples grouping features 
demonstrations groups motion patterns 
components solution 
particular representations chosen motion patterns 
representation hierarchical layer defined components previous 
note constant speed rotating object exhibits constant velocity gradient location respect local motion 
neuron higher hierarchy selective regions homogeneous value easy selectivity define implement 
shown eq 
motion pattern detector layer mst simply sums responses corresponding mt units feed 
example order see sequence 
consider simple rotating textured square 
layer mt neurons sensitive local motion direction object selective gradients perpendicular direction respond shown 
directions representation sees object cut pie pieces local motion direction 
tuning properties neurons decomposed flow field distinct areas constant velocity gradient 
note partitioned depending speed 
mst layer neurons selectivity rotation particular speed band receive input mt representations 
mst neuron receptive field best centered object fire strongest receives sufficient stimulation case means sees pieces pie 
best responding neuron considered having grouped pie pieces reassembled pie bound representations mt layer incident location feature type 
feedforward part process implicit binding action 
task system simply detect presence particular motion pattern representation suffice long toplevel global wta selects region 
system task localize recognize job complete 
clear mst neurons respond 
feedback process top attention selects best responses actively sub selects particular regions mt neurons correspond best firing best fitting pattern selectivity neuron 
unique aspect receptive field mst neuron defined spatial region subset features computed spatial region feature contributing component spatial region 
wta shown eq 

shows need flexible view saliency wta computations previously shown models models definition structure koch ullman 
model currently includes distributed definition saliency 
binding complete feature location dimensions 
complex binding problem considered multiple motion patterns appear image sequence 
seen moving objects time image sequence real scene activate representations hierarchy seen representations 
rectangular object approaching camera circular rotating counter clockwise 
classic binding problem 
representation output layer complex due multiple stimuli wta able choose peak location information tuning properties feedforward connectivity winning units sub select correct components winning pattern 
full sequence shown order show sequential nature selection process attends patterns passes algorithm section iii 

grouping location motion pattern detection complex binding example objects spatially separated overlap 
example shown spatially overlapping motion patterns hexagonal disks rotating 
attentional fixations shown 

overlapping motion patterns iv motion types model model includes motion types just ones described 
methods detecting onset offset events included model described previously wai tsotsos tsotsos 
method proven effective representations 
example assume object displaying clockwise rotating motion stops changes direction 
offset rotation signifies motion clockwise rotation representation 
onset counter clockwise rotation representation denote 
representations corresponding onset offset computation representation 
clear motion processing model include methods detect initiation cessation motions 
previous examples motions generated objects change position image 
course objects motion change position camera motion induces motion motion occurs pre defined short image sequences continuously 
generate continuous output continuous input straightforward solution repeat entire process algorithm iii feed network time varying image sequences perform feedforward computation feedback attentional selection 
idea re computing pyramid beam image subset definitely efficient doubtful biological consistency due extra processing time required 
better strategy modify beam locally needed enable track changes 
local beam structure defined dynamic manner re directed new gating control signals top propagate downwards 
approach signals flow continuously pyramid 
input continuous flows upwards continuously 
wta processes complete top gating control signals downward journey implementing top wta hierarchy 
signals potentially changing occurs selections exact ones led top winner precise location 
remember wta defined guaranteed find winners receptive field 
winner top restricted single location region 
long motion speed speed signal propagation matched system blind small location changes operates correctly expected 
gating signals generated top pyramid require time propagate downwards control may exert lower levels processing reflect past 
strategy features time delay time period blindness discontinuities motion occur shorter duration propagation time missed 
tested successfully object translation 
localization simple translation pyramidal stream activated motion feature uniform object 
discussion points presents new feedforward motion processing hierarchy presents examples selective tuning model operate hierarchy localize label motion patterns shows aspects recognition require feature grouping binding may accomplished top attentional selection mechanism depend single location saliency representation 
new feed forward motion analysis hierarchy 
structure computations strongly inspired biology resulting network degree biological realism biologically accurate ways 
due incorporation functionally diverse neurons motion hierarchy output model encompasses wide variety selectivities different resolutions 
enables computer simulation model detect classify various motion patterns artificial natural image sequences showing moving objects single objects undergoing complex multiple motions 
models biological motion perception focus single cortical area 
instance models zemel simoncelli heeger beardsley biologically relevant approaches explain specific functionality mt mst neurons include embedding hierarchy motion pathway 
hand hierarchical models detection motion poggio andersen 
andersen provide computationally plausible version motion processing hierarchy 
poggio describe sophisticated biologically motivated complex hierarchy processing human movement patterns 
include attentional influences 
provide early input algorithm manually 
hand tracked body joint positions manually converted stick figures optic flow easily computed 
handle complex overlapping dense flow discontinuous motions certainly process real image sequences directly 
lu motion hierarchy attentive processes model computational 
strong biological plausibility function 
proposed human visual motion perception served separate motion systems order system responds moving luminance patterns second order system responds moving modulations feature types third order system computes motion marked locations salience map 
third order system lu similar process attending motion st computational details difficult draw close comparison 
course actively pursuing avenues 
tuning characteristics neurons coarsely model current knowledge primate vision 
model includes little cooperative competitive processing units layer 
experimental examining relationship particular structure human vision going 
important put new motion analysis framework context classic literature motion starting koenderink van doorn longuet higgins prazdny 
assumptions moving object modeled piecewise rigid planar patches fixation point surface question motion may estimated affine transformation 
affine model described basis quantities image translation image rotation divergence shear 
terms specify respectively rigid dimensional translation rotation fixated object 
third term describes isotropic expansion contraction specifies change scale pure deformation 
shear term results distortion image pattern corresponds expansion specified direction simultaneous contraction perpendicular way area pattern preserved 
perspective projection velocity vector point image computing derivative time gray value changes pixel yields 
represents translational motion 
spatial derivatives taken velocity component directions ux uy vx vy 
combinations derivatives provide definitions remaining affine motions rotation deformation divergence 
divergence represented ux vy 
deformation shear components ux vy uy vx 
rotation expressed uy vx 
extraction affine estimates essentially components identification appropriate set spatial patches represent surface scene tracking patches image sequence 
main point setting fixation point identification patches track central definition majority past uses affine estimation assumptions fixation comes 
show method determined 
importantly model explicitly local spatial velocity intermediate representation local flow vectors affine motion patterns 
evidence neurons appears andersen 
show structure biologically realistic experimentally confirming neural correlates humans layers motion processing implied affine motion definitions martinez 
second shows earlier criticisms st demonstrations simple feature pyramids computed gaussian blurring biologically realistic useful forgotten 
criticisms completely valid affect original definition st early demonstrations 
shown st operates perfectly significantly complex representation 
strength model mechanism visual attention 
knowledge computational motion models address attention motion 
earliest ones due nowlan sejnowski 
nowlan sejnowski processing spirit different form 
compute motion energy goal modeling mt neurons 
energy part hierarchy processes include softmax local velocity selection 
suggest selection permits processing focused reliable estimates velocity 
top component full processing hierarchy binding complex patterns 
attentional modulation motion neurons described experimentally maunsell appeared model course developed appear scope model 
optical flow computed motion structure 
fixated object estimate ego motion presence translation rotation observer flow log polar periphery 
computation time collision goal definition attentive motion hierarchy 
attentive fixations advantage motion processing quite specific log polar representations connection affine motion implicated need fixate recognized 
grossberg viswanathan integration segmentation model motion capture 
called bcs model goal integrate motion information image segment motion cues unified global percept 
employ models translational processing areas mt mst consider motion patterns 
competition determines local winners neural responses mst cells encoding winning direction excitatory influence mt cells tuned direction 
variety motion illusions illustrated real image sequences attempted 
model closest st goal methodology 
models breadth processing motion domain attentional selection current 
interesting comparison drawn performance st motion hierarchy population code strategy neuronal representation 
standard approach population encoding assumption exists unique unambiguous value population represents stimulus see pouget review 
contrasting view population code contain additional information multiple values uncertainty obviating need restrictive assumption zemel dayan 
relationship approaches methods 
short answer strong relationship qualitative level zemel dayan approach 
examples earlier illustrate 
set figures show representations level hierarchy arise due moving items visual field 
clutter serve illustrate representations item population encodes 
term population clarified 
population coding assumption neural responses come stimuli respond 
zemel dayan example responses mt neurons monkey clear single type neuron area mt zemel dayan scheme separate features shown computational advantages tsotsos 
review pouget shows responses behaving monkeys performing task requiring attention considered 
words population neurons zemel dayan represent result feedforward pass described 
population coding strategies pass encoding attempt remove noise inferences 
st employs attention task 
wta process operates population order estimate best response 
estimate necessarily correct response 
stage processing applying top inhibitory surround selected location removes noise permits precise representation attended stimulus 
match result task representation verifies rejects estimate 
item visual field response populations due overlap seen peaks readily visible 
importantly show population code attended stimulus removed attentive inhibition return process 
sufficiently completely cleaned second peak population correctly 
summary distributional population coding scheme zemel dayan st scheme address similar problems accomplish similar goals 
methods differ underlying assumption differs 
st includes attention time course attentional modulation process closer biological match 
interesting detailed analysis underlying mathematics see equivalences differences 
inspection patterns activation shown figures provides perspective issue location saliency map 
search neural correlate concept saliency map led interesting experimental providing evidence particular location superior colliculus robinson horwitz newsome keller lgn koch sherman koch li lee petersen posner petersen robinson petersen fef thompson parietal gottlieb 
correlate locating maxima response neural population corresponds attended location 
consider patterns response shown figures involving visual areas mst higher 
case criterion maximum corresponding attended location seen 
area feedforward feedback influences take effect 
evidence areas neural correlate saliency map 
saliency distributed computation shown attention evidence reflecting computations neural populations 
attention strategy demonstrates key aspect recognition process separate computation parts subsequent re assembly guided attention 
key solution abandonment single location saliency representation supports single point wta feature attention models 
single map attention emergent stochastic feature 
saliency local distributed deterministic phenomenon wta processes hierarchical region dynamically defined depending task neural selectivity 
entirely inappropriate claim solution classic binding problem early justify claim 
solution appear right elements solve limited aspects binding required domain 
strategy attention motion considered precursor detailed analysis order extract precise velocity direction motion 
example computer vision application determination precise velocity depends object localization elimination outliers 
attentive process provide estimate object location extent point type motion 
new algorithms velocity extraction developed take advantage reduced search space 
attention motion received sufficient study computer vision critical component general solution visual information processing tsotsos 
significant advances early attentive algorithms warrant closer look current models attention st usefully directed complex computer vision problems 
number people contributed effort important ways wish express assistance sean stefan guy albert wai liu michael 
funding research gratefully received natural sciences engineering council canada communications information technology ontario province ontario centre excellence institute robotics intelligent systems network centers excellence government canada 
holds canada research chair computational vision 


attentional interference small spatial separations vision research 
barlow 

single units cognition doctrine perceptual psychology 
perception 
beardsley 

computational modeling optic flow selectivity neurons 
network computation neural systems 


attention 
nature 


attentional selection distractor suppression vision research 
tsotsos 

attentional prototype early vision proceedings second european conference computer vision santa margherita ligure italy sandini ed lncs series vol 
springer verlag 
tsotsos 

selective tuning model visual attention testing predictions arising inhibitory surround mechanism vision research 


attentive visual motion processing computations log polar plane computing suppl 

deco 

top selective visual attention approach visual cognition 
duffy wurtz 

mst neurons respond speed patterns optic flow journal neuroscience 


receptive field properties neurons middle temporal visual area mt owl monkeys 
journal neurophysiology 
van essen 

distributed hierarchical processing primate visual cortex cerebral cortex 
xiao 

modular organization temporal pathways cortical connections visual area visual area posterior inferotemporal ventral area macaque monkeys neuroscience 
maunsell 

specialized representations review visual cortex role binding neuron 
poggio 
neural recognition complex movements actions 
nature reviews neuroscience 
gottlieb goldberg 

representation visual salience monkey posterior parietal cortex 
nature 
andersen 

tuning mst neurons spiral motions 
neurosci 

grossberg viswanathan 

neural dynamics motion integration segmentation apertures 
vision research 



dynamic model feature cues guide spatial attention 
vision research 
heeger 

optical flow spatiotemporal filters 
international journal computer vision 
horwitz newsome 

separate signals target selection movement specification superior colliculus 
science 
itti koch niebur 

model saliency visual attention rapid scene analysis ieee transactions pattern analysis machine intelligence vol 

de desimone ungerleider 

mechanisms directed attention human cortex revealed functional mri science 
koch 

theoretical analysis electrical properties cell cat lgn spine triad circuit selective visual attention 
artificial intelligence memo mit artificial intelligence laboratory february 
koch ullman 

shifts selective visual attention underlying neural circuitry hum 
neurobiology 
koenderink van doorn 

local structure movement parallax plane opt 
soc 
am 
vol 

robinson 
shared neural control attentional shifts eye movements nature vol 



speed direction selectivity macaque middle temporal neurons 
journal neurophysiology 
lee yang romero mumford 

neural activity early visual cortex reflects behavioral experience higher order perceptual saliency 
nature neuroscience 
lee itti koch braun 

attention activates winner take competition visual filters 
nature neuroscience 
li 

saliency map primary visual cortex trends cognitive sciences vol 

longuet higgins prazdny 

interpretation moving retinal image proceeding royal society london vol 


lu 

functional architecture human visual motion perception 
vision res keller 

saccade target selection superior colliculus visual search task neurophysiol 
anderson 

spiral mechanisms required account summation complex motion components 
vision research 
mehta schroeder 

selective attention monkeys 
distribution timing effects visual areas cerebral cortex 
andersen 

influence attentive fixation light sensitive neurons posterior parietal cortex neuroscience 
mounts 

attentional capture abrupt onsets feature singletons produces inhibitory surrounds 
percept 
nowlan sejnowski 

selection model motion processing area mt primates journal neuroscience 
connor 

attention modulates responses human lateral geniculate nucleus nature neuroscience kennedy 

velocity sensitivity direction sensitivity neurons areas monkey influence eccentricity 
journal neurophysiology 
ga xiao maes 

speed tuning medial superior temporal mst cell responses optic flow components 
perception 

pack grossberg 

neural model smooth pursuit control motion perception cortical area mst cognitive neuroscience 
perrone stone 

emulating visual receptive field properties mst neurons template model heading estimation 
journal neuroscience 
petersen robinson morris 

contributions visual spatial attention 
neuropsychologia 
liu martinez tsotsos 

neurally inspired model detecting localizing simple motion patterns image sequences workshop dynamic perception bochum germany nov 
posner petersen 

attention system human brain ann 
rev neurosci 

pouget dayan zemel 

inference computation population codes annual reviews neuroscience 
robinson dl petersen se 

visual salience trends neurosci 
apr 
rosenblatt 

principles neurodynamics perceptions theory brain mechanisms 
washington cd spartan books 

editor neuron vol 
september 

binding problem neuron vol 

sato thompson chi hung 

effects search efficiency surround suppression visual selection frontal eye field neurophysiol 
sherman koch 

control transmission mammalian lateral geniculate nucleus 
exp brain res 
siegel read 

analysis optic flow monkey parietal area 
cerebral cortex simoncelli heeger 

model neuronal responses visual area mt vision research 
van marchal ga 

motion responsive regions human brain 
exp brain res 

thompson 
dissociation visual discrimination saccade programming macaque frontal eye field neurophysiol 
thorpe 

ultra rapid scene categorisation wave spikes 
bulthoff eds biologically motivated computer vision lecture notes computer science pp springer verlag berlin 
tsotsos 

attention orientation results inhibitory surround orientation space th annual meeting behaviour brain cognitive science society june st john 
andersen 

neural responses velocity gradients macaque cortical area mt vis neurosci 

martinez 

feature attention influences motion processing gain macaque visual cortex 
nature 

maunsell 

attentional modulation visual motion processing cortical areas mt mst 
nature 
treisman gelade 

feature integration theory attention cognitive science 
treisman schmidt 

illusory conjunctions perception objects cognitive psychology 
tsotsos 

complexity level analysis vision proceedings international conference computer vision human machine vision workshop london england 
tsotsos 

complexity perceptual search tasks proc 
international joint conference artificial intelligence detroit 
tsotsos 

analyzing vision complexity level behavioral brain sciences 
tsotsos 

relative complexity passive vs active visual search international journal computer vision 
tsotsos 

inhibitory beam attentional selection spatial vision humans robots ed 
harris jenkin cambridge university press tsotsos wai lai davis 

modeling visual attention selective tuning artificial intelligence 
tsotsos 

computational model visual attention early vision ed 
papathomas mit press bradford books 
tsotsos 

complexity vision attention vision attention ed 
harris jenkin springer verlag new york 
tsotsos 

motion understanding task directed attention representations link perception action int 
computer vision 
tsotsos 

theoretical foundations hierarchical circuit selective attention visual attention cortical circuits ed 
braun koch davis mit press 
tsotsos liu martinez 
attending motion localizing labeling simple motion patterns image sequences conference biologically motivated computer vision tuebingen germany 
tsotsos jk martinez jc hj hopf 

human cortical specialization processing speed gradient information moving stimuli 
society neuroscience abstracts 
valiant 

parallelism comparison problems siam comput 



attention dependent suppression metabolic activity early stages macaque visual system cerebral cortex 

van essen maunsell bixby 

middle temporal visual area macaque connections functional properties topographic organization 
comp 

von der malsburg 

binding review modeler perspective neuron vol 

wai tsotsos 

directing attention onset offset image events movement control iapr conference pattern recognition jerusalem vol 

wolfe cave 

guided search alternative feature integration model visual search experimental psychology human perception performance 
yuille geiger 

winner take mechanisms handbook brain theory neural networks ed 
arbib 
tsotsos 

biologically plausible active visual search model eccv workshop attention performance computer vision prague may zemel dayan 

distributional population codes multiple motion models 
nips advances neural information processing systems 
zemel sejnowski 

model encoding multiple object motions self motion area mst primate visual cortex journal neuroscience 
captions full motion hierarchy shows set neural selectivities comprise entire pyramidal hierarchy covering visual areas mt mst 
rectangle represents single type selectivity applied full image level pyramid 
large grey arrows represent selectivity direction 
coloured rectangles area mt represent particular angles motion speed gradient 
rectangles direction represent speed selectivity ranges model 
way single sheet may considered expanded view visual area 
area example neurons integrate direction speed selectivity represented single sheet grey rectangles 
area mt sheets top representing direction speed selectivity remaining represent directions velocity gradient relative motion directions 
wheel coloured arrows represents colour coding area mt speed gradient respect local motion case larger grey arrow pointing upwards 
codes angle local motion speed gradient 
mst units respond patterns motion contract rotate 
layers represent translational motion spiral motion area mst plus radial rotation direction topmost set rectangles 

illustration spatiotemporal energy computation 
output area computations series figures gives detail filter outputs full motion hierarchy 
input square random noise rotating counter clockwise place background random noise 
output area output integrative units 
translation output area mt speed gradient output area mt summary representation different speed gradients local speed direction 
coloured dot maximum value representations 
decision process simpler visualization 
translation output area mst 
generalized spiral output area mst 
translation output area 
generalized spiral output area 
rotation radial output area 
attentional beam shows rationale suppression attended items feature stm 
stm full hierarchy full visual processing hierarchy stm operates depicted 
focuses motion pathway areas mt mst 
components demonstrated previously current research topics 

attending motion defined object shows structure attention beam localizes labels rotating square feed forward outputs shown 
beam color green signifies counter clockwise rotation 
see colour wheel 
note fact root single representation spiral neurons beam splits include components rotating object localizing components mt mst representations 
beam input image binding pieces 
top shows active beam pass zone structure bottom shows localization motion image 
layers pyramid clearly visible active representations layer shown 
figures follow pyramid tilted page input shown beam structure 
top layer occluded view part beam 

grouping location motion pattern detection example real images textured objects cluttered background rectangle approaching camera circle rotating counterclockwise 
output area mt feed forward pass summary form described 
localization winning area red colour signifying approach inhibition return attended pathways 
output area mt second feed forward pass inhibition return applied summary form described 
second attentional fixation green colour counterclockwise rotation 
inhibition return perfect pixels due approaching object eliminated additional responses remain 
reason ior set attended location 
object continues move approaching appearance original attended inhibited location 
enable inhibition location attended object 

overlapping motion patterns feature binding example overlapping textured hexagons left rotating clockwise right rotates counter clockwise 
complete representation pass feed forward outputs shown 
attentional fixations hexagons 
lower attended localized quite labeled green counter clockwise rotation upper second localized overlap region labeled red clockwise rotation 
note objects overlapping motion labels correct object reasonable overlap 
note responses area completely merged simple scheme possibly disentangle output 
top search feature binding strategy described successfully separates signals localizes labels moving objects 
time pixels line constant intensity neuron preferred motion unattended receptive field neuron top layer spatial inhibitory annulus spatial inhibition operates layer attended item layer processing pyramid stimulus ring input layer 
