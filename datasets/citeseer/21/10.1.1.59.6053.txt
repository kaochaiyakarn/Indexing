high availability dhts erasure coding vs replication high availability peer peer dhts requires data redundancy 
compares popular redundancy schemes replication erasure coding 
previous comparisons take characteristics nodes comprise overlay account conclude cases benefits coding limited may worth disadvantages 
peer peer distributed hash tables dhts propose logically centralized physically distributed hash table abstraction shared simultaneously applications 
ensuring data objects dht high availability levels nodes storing available requires form data redundancy 
peer peer dhts proposed different redundancy schemes replication erasure coding 
aims provide comprehensive discussion advantages scheme 
previous comparisons exist argue erasure coding clear victor due huge storage savings availability levels conversely huge availability gains storage levels 
somewhat different argue gains coding exist highly dependent characteristics nodes comprise overlay 
fact benefits coding limited cases easily outweighed disadvantages extra complexity erasure codes 
performing analytic comparison replication coding clearly delineates relative gains coding vs replication function server availability desired dht object availability section 
model allows understand server availability section 
measured values different traces find exact values parameters model section 
allows draw precise advantages coding replication section 
coding vs replication redundancy levels section summarizes redundancy schemes presents analytic comparison highlights main advantage coding savings terms required redundancy 
section outlines positive negative aspects schemes 
rodrigo rodrigues barbara liskov mit replication replication simplest redundancy scheme identical copies data object kept instant system members 
value set appropriately depending desired object unavailability target number nines average node availability assuming node availability independent identically distributed assuming need replicas data available order retrieve case data immutable single available copy sufficient retrieve correct object compute values 
object unavailable replicas unavailable replica unavailable solving yields erasure coding log log erasure coded redundancy scheme object divided fragments recoded fragments stored separately means effective redundancy factor kc key property erasure codes original object reconstructed fragments combined size fragments approximately equal original object size 
exhibit equivalent equation case erasure coding 
summary complete derivation 
object availability probability kc fragments available algebraic simplifications normal approximation binomial distribution see get formula erasure coding redundancy factor kc number standard deviations normal distribution required level availability 
corresponds nines availability 
replication stretch ratio server availability ratio required replication required expansion factors function server availability different object availability levels 
equation value chord implementation 
note considered deterministic coding schemes constant rate encoding reed solomon ida 
analysis extend codes consensual codes storage environment dht 
comparing redundancy previous discussion highlights main reason coding increased redundancy allows level availability achieved smaller additional storage 
exact gains depicted 
plots ratio required replication required erasure coding expansion factor ratio equations different server availability levels assuming server availability different object availability targets nines availability 
set number fragments needed reconstruct object set equation 
value chord 
erasure coding going matter store data unreliable servers lower server availability levels target better guarantees system higher number nines object availability 
redundancy gains coding range fold 
remainder discussion assumes object availability target nines 
targeting higher levels availability exaggerated aspects system keep high availability levels 
instance measurement study mit client access link host mit able reach rest internet time 
study pointed mit access link reliable links dsl line mbits link cogent 
client access link nines availability making distinction instance nines dht object availability irrelevant object availability dominated uplink quality factors considering extra dht availability noise 
question may ask redundancy savings important 
obviously lead lower disk usage 
may improve speed writes smaller amount data uploaded writer servers client upload bandwidth limits write speed coding lead faster writes 
important aspects savings bandwidth required restore redundancy levels presence changing membership 
importance due fact bandwidth spare storage limiting factor scalability peer peer storage systems 
basic model section presents simple model allows quantify bandwidth cost maintaining data redundancy presence membership changes function required redundancy understand concept server availability peer peer dht measure 
core model described sections previous publication summarize 
assumptions model assumes large dynamic collection nodes cooperatively store data 
data set partitioned subset assigned different nodes known data placement mapping function current membership set replicas block 
happens instance consistent hashing storage systems cfs :10.1.1.13.1523
number simplifying assumptions 
main simplification comes fact focus average case analysis 
considering worse case values certain parameters rate nodes leave system model underestimates required bandwidth 
assume fixed redundancy factor identical space contributions 
previous system dropped assumptions variable redundancy factor copies created initially nodes leave system redundancy levels drop 
leads biased system stable nodes donate storage drastically reducing bandwidth costs 
affects understand analysis change new design 
assume constant rate joining leaving assume join leave events independent 
assume constant steady state number nodes 
data maintenance model consider set identical hosts cooperatively provide guaranteed storage network 
nodes added set rate leave rate average system size session time membership lifetime distinction sessions lifetimes 
constant 
average node stays member queuing theory result known little law 
data model system reliably stores total bytes unique data stored redundancy factor total kd bytes contributed storage 
replication factor expansion due coding set depending desired availability target node availability specific deployment equations 
node joining system download data serve subset data mapped 
average size transfer assume identical node storage contributions 
join events happen time units average 
aggregate bandwidth deal nodes joining overlay node leaves overlay data housed copied new nodes redundancy lost 
leave event leads transfer bytes data 
leaves require aggregate bandwidth cases cost leaving system avoided instance level redundancy block sufficiently high new node join leave requiring data movement 
ignore optimization total bandwidth usage data maintenance kd node average kd bw node space node lifetime restoring redundancy coding coding creating new fragments cope nodes holding fragments leaving system trivial task 
problem coding schemes ida create new fragment access entire data object 
envision alternative approaches 
complicated alternative download fragments reconstruct object create new fragment 
costly fragment lost needs reinstated system node needs download times size fragment 
amount data needs transferred times higher amount redundancy lost 
alternative maintain full copy object nodes fragments remaining nodes share responsibility object 
practice time membership interval membership interval join leave join leave membership timeout 
corresponds increasing redundancy factors erasure coding unit 
note analysis correct mix fragments complete copies fact amount data needs moved nodes leave equal amount data departing node stored 
correct restore fragment node keeps complete copy create new fragment push new owner restore complete copy node responsible copy download fragments combined size approximately equal size object 
remainder assume system coding keeps additional complete copy object stored system 
distinguishing downtime vs departure model refer joins leaves joining system time leaving forever data movement triggered events 
words try simple distinction session times membership lifetimes authors noted 
distinction illustrated session time corresponds duration interval node reachable membership lifetime time node enters system time leaves system permanently 
distinction important avoids triggering data movement restore redundancy due temporary disconnection 
side effect doing nodes unavailable part membership lifetime 
define node availability fraction time member system reachable words sum node session times divided node membership lifetime 
detecting permanent departures problem simple model distinguishing sessions membership lifetimes requires knowledge applications means distinguish temporary departure permanent leave time node disconnection 
address problem introduce new concept membership timeout measures long system delays response failures 
words process making new hosts responsible host data host contact longer time illustrated 
main consequences increasing membership timeout higher means member lifetimes time avg leave rate fraction hr overnet farsite planetlab membership timeout hours membership dynamics function membership timeout 
longer transient failures considered leaves consequence total member count increase 
second average host availability decrease wait longer evict node system 
translating previous model implies set accordingly equations 
note decreases increase 
definition availability deduced 
consequence joins going trigger data movement re joins node retain data needs serve re joining system 
measurements minor impact data movement set long membership timeouts large hardly exist re joins ignore issue 
equation rewritten note average bandwidth system members 
point time members running application unavailable nodes contribute bandwidth usage 
may want compute average bandwidth nodes available running application 
replace left hand side equation compute 
measured dynamics availability section results measurements membership dynamics node availability change function membership timeout derive corresponding redundancy requirements maintenance bandwidth replication coding 
numbers different traces correspond distinct deployments peer peer storage system peer peer volunteer data collected bhagwan study overnet average availability overnet farsite planetlab membership timeout hours average node availability function membership timeout 
file sharing system 
tracked reachability peers gathered crawl system membership days looking node ids minutes 
corporate desktop pcs data collected bolosky study availability desktop pcs microsoft course days pinging fixed set machines hour 
server infrastructure data collected stribling reflects results pinging pair hosts planet lab testbed minutes 
data collected course days october december 
considered host reachable half nodes trace ping 
analysis looks average case behavior system 
shows increasing membership timeout decreases dynamics system 
case dynamics expressed average fraction system nodes leave system hour axis 
note leave referring having left units time referring membership dynamics session dynamics 
expected system membership dynamic membership timeout increases session terminations longer considered membership leaves node returns system units time 
mentioned second main effect increasing node availability system decrease 
effect shown 
node availability expect extremely high planetlab average slightly lower farsite average low peer topeer trace lower greater hours 
note plot varies easily deduced fact 
required replication factor overnet farsite planetlab membership timeout hours required replication factor nines object availability function membership timeout 
needed redundancy bandwidth measure bandwidth gains erasure coding vs replication deployments 
compute needed redundancy redundancy schemes function membership timeout 
availability values equations plotted corresponding redundancy factors assuming target average object availability nines 
results replication shown 
shows overnet requires redundancy expected reaching replication factor 
deployments replication factors lower order units 
note replication values rounded integer fraction copies 
shows redundancy requirements expansion factor availability values equation nines target availability 
redundancy values shown include extra copy object required create new fragments nodes leave system explained section 
shown overnet requires redundancy deployments overnet coding leads substantial storage savings fixed amount unique data stored system reduce redundancy factors half 
compare bandwidth usage schemes 
basic equation cost redundancy maintenance equation apply membership lifetimes values implied leave rates recall average membership lifetime inverse average join leave rate 
assume fixed number servers fixed amount unique data stored system 
replication factors coding redundancy factors 
shows average bandwidth different traces different values 
interesting effect observed farsite trace band required redundancy stretch overnet farsite planetlab membership timeout hours required coding redundancy factor nines object availability function membership timeout determined equation considering extra copy required restore lost fragments 
width steps hours 
correspond people turn machines night weekends respectively 
setting greater downtime periods prevent downtime generating membership change corresponding data movement 
shows equivalent case coding replication 
average bandwidth values lower due smaller redundancy coding especially overnet deployment achieve substantial redundancy savings 
discussion drawn figures 
overnet trace coding win server availability low left hand side unfortunately maintenance bandwidth scalable highly available storage system overnet membership dynamics home users kbps average modest node contribution gigabytes 
cooperative storage systems target stable environments farsite planetlab 
planetlab trace coding win server availability extremely high corresponding right hand side 
interesting deployment erasure codes farsite intermediate server availability presents visible redundancy savings 
redundancy savings coding full replication come price 
main point coding introduces complexity system 
complexity associated encoding decoding blocks entire system design complex task redundancy maintenance complicated explained section 
general principle believe complexity system design avoided proven strictly necessary 
system designers question added com maintenance bw kbps overnet farsite planetlab membership timeout hours maintenance bandwidth replication 
average bandwidth required redundancy maintenance function membership timeout 
assumes nodes cooperatively storing unique data replication data redundancy 
plexity worth benefits may limited depending deployment 
point erasure codes download latency environment internet internode latency heterogeneous 
replication data object downloaded replica closest client coding download latency bounded distance mth closest replica 
problem illustrated simulation results previous 
task downloading particular subset object sub block complicated coding entire object reconstructed 
full replicas sub blocks downloaded trivially 
similar observation erasure coding adequate system design operations done server side keyword searching 
final point analysis considered immutable data 
assumption particularly important distinction session times membership lifetimes assuming unreachable node rejoins system state valid 
true contained mutable state modified 
impact mutability redundancy choices unclear consider node determines state accurate isn study redundancy techniques presence mutability area 
jeremy stribling farsite team msr total recall team ucsd supplying data collected studies 
mike emil sit anonymous reviewers helpful comments 
andersen 
improving availability overlay networks 
phd thesis mit 
maintenance bw kbps overnet farsite planetlab membership timeout hours maintenance bandwidth erasure coding 
average bandwidth required redundancy maintenance function membership timeout 
assumes nodes cooperatively storing unique data coding data redundancy 
bertsekas gallager 
data networks 
prentice hall 
bhagwan savage voelker 
understanding availability 
proc 
iptps 
bhagwan cheng savage voelker 
total recall system support automated availability management 
proc 
nsdi 
blake rodrigues 
high availability scalable storage dynamic peer networks pick 
proc 
th hotos 
bolosky douceur ely theimer 
feasibility serverless distributed file system deployed existing set desktop pcs 
proc 
sigmetrics 
dabek li sit robertson kaashoek morris 
designing dht low latency high throughput 
proc 
nsdi 
karger lehman leighton levine lewin panigrahy :10.1.1.13.1523
consistent hashing random trees distributed caching protocols relieving hot spots world wide web 
proc 
stc 
kubiatowicz bindel chen czerwinski eaton geels gummadi rhea weatherspoon weimer wells zhao 
oceanstore architecture persistent storage 
proc 
asplos 
luby 
lt codes 
proceedings rd symposium foundations computer science focs vancouver canada nov 
rabin 
efficient dispersal information security load balancing fault tolerance 
acm 
ratnasamy francis handley karp shenker 
scalable content addressable network 
proc 
sigcomm 
reed solomon 
polynomial codes certain finite fields 
siam june 
rhea geels roscoe kubiatowicz 
handling churn dht 
proc 
usenix 
rowstron druschel 
storage management caching past large scale persistent peer peer storage utility 
proc 
sosp 
stribling 
planetlab pairs pings 
pdos lcs mit edu pl app 
weatherspoon kubiatowicz 
erasure coding vs replication quantitative comparison 
proc 
iptps 
