semi supervised learning mixture models fabio cozman usp br polit cnica university paulo av 
prof mello universit ria paulo sp brazil ira cohen ifp uiuc edu beckman institute mathews ave urbana il marcelo cesar marcelo poli usp br polit cnica university paulo analyzes performance semisupervised learning mixture models 
show unlabeled data lead increase classification error situations additional labeled data decrease classification error 
mathematical analysis degradation phenomenon show due fact bias may adversely affected unlabeled data 
discuss impact theoretical results practical situations 

semi supervised learning received considerable attention machine learning literature due potential reducing need expensive labeled data seeger 
applications text classification genetic research machine vision examples cheap unlabeled data added pool labeled samples 
literature hold optimistic view unclassified observations certainly discarded neill 
representative summary literature comes mccallum nigam declare augmenting small set labeled samples large set unlabeled data combining pools em improve parameter estimates unfortunately experiments indicate unlabeled data quite detrimental performance classifiers section 
unlabeled data added fixed set labeled samples poorer performance resulting classifier 
statement cautiously readers may find obvious may find dis incorrect 
argue numerical errors em similar algorithms natural suspects performance degradation want stress results concern performance degradation absence numerical instabilities 
object unlabeled data provably useful castelli cover degradation come incorrect analysis argue unlabeled data conceivably deleterious exceptional situations modeling assumptions clearly violated 
note unlabeled data lead performance degradation situations labeled data useful classification case modeling different effect types data 
extensive tests semi supervised learning witness complex interaction modeling assumptions classifier performance 
unlabeled data require delicate suspect researchers unaware complexities 
wish contribute better understanding semisupervised learning focusing maximum likelihood estimators generative classifiers 
sections summarize relevant facts semi supervised learning 
section show performance degradation unlabeled data depends bias 
main result theorem characterize maximum likelihood semi supervised learning convex combination supervised unsupervised learning show understand performance degradation semisupervised learning 
indicate reasons may observe labeled data improve classifier unlabeled data may degrade classifier short labeled unlabeled data contribute reduction variance unlabeled data may lead increase bias modeling assumptions incorrect 
examples il proceedings twentieth international conference machine learning icml washington dc 
circumstances semi supervised learning 
finish discussing behavior practical classifiers learned labeled unlabeled data 

semi supervised learning goal classify incoming vector observables instantiation sample 
exists class variable values classes 
simplify discussion assume binary variable values want build classifiers receive sample output assume loss objective minimize probability classification errors 
knew exactly joint distribution optimal rule choose class probability larger choose class devroye 
classification rule attains minimum possible classification error called bayes error 
take probabilities functions probabilities estimated data plugged optimal classification rule 
assume parametric model adopted 
estimate denoted adopt maximum likelihood method estimation parameters 
distribution belongs family say model correct say model incorrect model correct difference expected value called estimation bias 
estimation bias zero estimator unbiased 
model incorrect bias loosely mean difference classification error denoted difference bayes error classification bias 
assume probability models satisfy conditions adopted white essentially parameters belong compact subsets euclidean space measures measurable radon nikodym densities defined measurable spaces functions twice differentiable functions derivatives measurable dominated integrable functions 
formal list assumptions cozman cohen 
semi supervised learning classifiers built combination nl labeled nu unlabeled samples 
assume samples independent ordered nl samples labeled 
consider scenario 
sample generated value revealed sample labeled value hidden sample unlabeled 
probability sample labeled denoted fixed known independent samples 
underlying distribution models labeled unlabeled data consider possibility labeled unlabeled samples different generating mechanisms 
likelihood labeled sample likelihood unlabeled sample density mixture model mixing factor denoted assume mixtures identifiable distinct values determine distinct distributions permutations mixture components allowed 
distribution decomposed parametric model depend explicitly referred generative model 
strategy departs generative scheme focus take marginal independent 
strategy produces diagnostic model example logistic regression zhang oles 
narrow sense diagnostic models maximum likelihood process unlabeled data dataset see zhang oles discussion 
adopt maximum likelihood estimators generative models strategies object 

unlabeled data improve degrade classification performance 
reasonable expect average improvement classification performance increase number samples labeled unlabeled data processed smaller variance estimates smaller classification error 
reports literature corroborate informal reasoning 
investigations seventies quite optimistic cooper freeman jr neill 
plenty applied semi supervised learning notable successes 
workshops semi supervised learning nips nips nips ijcai 
publications meetings generally concluded unlabeled data profitably available 
important positive theoretical results concerning unlabeled data 
castelli cover venkatesh unlabeled samples es relevant baluja bruce collins singer comit goldman zhou mc nigam miller uyar nigam shahshahani landgrebe 
decision regions estimating labeled samples solely determine labels region venkatesh refer procedure algorithm 
castelli cover basically prove algorithm asymptotically optimal various assumptions asymptotically labeled data contribute exponentially faster unlabeled data reduction classification error 
authors critical assumption belongs family models model correct 
detailed analysis current empirical results reveal puzzling aspects unlabeled data 
reviewed descriptions performance degradation literature cozman cohen just mention relevant :10.1.1.16.74
results particularly interesting shahshahani landgrebe baluja describe degradation image understanding nigam 
report degradation text classification bruce describe degradation bayesian network classifiers 
shahshahani landgrebe speculate degradation may due deviations modeling assumptions outliers samples unknown classes suggest unlabeled samples labeled data produce poor classifier 
nigam 
suggest possible difficulties numerical problems em algorithm mismatches natural clusters feature space assumed classes 
intrigued results conducted extensive tests simulated problems observed pattern degradation interested reader consult cozman cohen :10.1.1.16.74
different test real data 
shows result learning naive bayes classifier different combinations labeled unlabeled datasets adult classification problem uci repository training testing datasets repository 
see adding unlabeled data improve classification labeled data set small labeled data degrade performance labeled data set larger 
shahshahani landgrebe nigam explicit stating unlabeled data degrade performance vague explaining analyze phenomenon 
possibilities numerical errors mismatches distribution labeled unlabeled data incorrect modeling assumptions 
unlabeled samples harmful numerical instabilities 
performance degradation caused increases variance bias 
performance workshop ijcai witnessed great deal discussion unlabeled data really useful communicated george forman 
probability error labeled labeled labeled number unlabeled records 
naive bayes classifiers generated adult database bars cover percentiles 
degradation occur absence bias modeling assumptions correct 
need specific types models complex structures produce performance degradation 
strategy addressing questions study asymptotic behavior exact maximum likelihood estimators semi supervised learning 
asymptotic results obtained section allows analyze semisupervised learning resorting numerical methods obtain insights uncertainties numerical optimization 
deny numerical problems happen practice see mclachlan basford section jaakkola interested fundamental phenomena 
examples section show performance degradation unlabeled data occur numerical problems removed 

asymptotics semi supervised learning section discuss asymptotic behavior maximum likelihood estimators semi supervised learning 
assume expectations log log log log exist function attains maximum value open neighborhood parameter space 
formal list assumptions cozman cohen 
assumptions eliminate important models cauchy distributions retain commonly distributional models 
state relevant results gaussian density mean variance denoted matrices defined matrices formed running indices ay log log log known result huber white 
consider parametric model properties discussed previous sections sequence maximum likelihood estimates obtained maximization log yi increasing number independent samples identically distributed open neighborhood maximizes log interior parameter space regular point ay non singular cy ay ay result require distribution belong family semi supervised learning samples realizations probability probability denote random variable assumes values plus unlabeled value 
observed samples realizations obtain equal mixture density obtained expression indicator function 
accordingly parametric model adopted equal definitions obtain main result theorem consider supervised learning samples randomly labeled probability 
adopting previous assumptions value limiting value maximum likelihood estimates argmax log log expectations respect proof 
value maximizes log expectation respect log equal log log log log ex pected value equal log log log log terms expression irrelevant maximization respect 
terms equal log log expression equal log log expectations respect obtain expression 
expression indicates objective function semisupervised learning viewed asymptotically convex combination objective functions supervised learning log unsupervised learning log 
denote value maxi expression note additional assumptions modeling densities theorem implicit function theorem prove continuous function 
shows path followed solution continuous assumed jaakkola discussion numerical methods semi supervised learning 
asymptotic variance estimating conditions theorem obtained results white 
asymptotic variance aba ax bx seen asymptotic covariance matrix positive definite asymptotically increase number labeled unlabeled samples leads reduction variance 
reduction variance true regardless model correct suppose family distributions contains distribution con dition satisfied identifiability maximum likelihood consistent bias zero classification error converges bayes error 
derivation shahshahani landgrebe unbiased estimators argue approximately expected classification error depends variance 
asymptotic covariance maximum likelihood estimator governed inverse fisher information 
fisher information sum information labeled data information unlabeled data zhang oles cozman cohen information unlabeled data positive definite unlabeled data cause reduction classification error model correct 
similar derivations mclachlan castelli 
model incorrect study scenario relevant purposes distribution belong family distributions handle difficulty unlabeled data information decide labels decision regions classification error castelli 
simplify discussion assume oracle available indicate labels decision regions 
view theorem surprising unlabeled data deleterious effect discussed section 
suppose show happen example 
observe large number labeled samples classification error approximately collect samples unlabeled eventually reach point classification error ap proaches net result started classification error close adding great number unlabeled samples classification performance degraded basic fact estimation classification biases directly affected 
necessary condition kind performance degradation sufficient condition smaller labeled dataset larger unlabeled dataset classification error dataset larger classification error labeled data 
summary labeled unlabeled data contribute reduction variance semi supervised learning maximum likelihood estimation 
model correct maximum likelihood estimator unbiased labeled unlabeled data reduce classification error reducing variance 
model incorrect may different asymptotic estimation classification biases different values asymptotic classification error may different different values increase number unlabeled samples may lead larger estimation bias larger classification error 
example performance degradation gaussian data previous discussion alluded possibility model incorrect 
understand phenomenon occur consider example obvious practical significance 
consider gaussian observations taken classes know mixing factor data sampled distribution mixing factor 
know gaussian variables mean conditional conditional variances conditional equal 
believe independent dependent conditional correlation equal fact independent conditional 
knew value obtain optimal classification boundary plane optimal classification boundary quadratic 
assume zero generating naive bayes classifier approximates incorrect assumption classification boundary linear log consequently decreasing function 
labeled data easily obtain sequence bernoulli trials classification boundary 
note linear boundary obtained labeled data best possible linear boundary 
fact find best possible linear boundary form 
classification error written function positive second derivative consequently function single minimum numerically minimizing 
consider set lines form see farther go best line larger classification error 
shows linear boundary obtained labeled data best possible linear boundary 
boundary labeled data best linear boundary 
consider computation asymptotic estimate unlabeled data 
theorem obtain arg max diag log diag diag second derivative double integral negative seen interchanging differentiation integration function concave single maximum 
search zero derivative double integral respect 
obtain value numerically 
estimate linear boundary unlabeled data 
line linear boundary labeled data previous discussion leads larger classification error boundary unlabeled data 

boundary obtained unlabeled data shown 
example suggests situation 
suppose collect large number nl labeled samples 
labeled estimates form sequence bernoulli trials probability estimates quickly approach variance decreases nl 
add large amount unlabeled data data approaches classification error increases 
changing true mixing factor correlation produce examples best linear boundary labeled unlabeled best linear labeled unlabeled 
contour plots gaussian mixture best classification boundary form linear boundary obtained labeled data middle line linear boundary obtained unlabeled data upper line 
boundaries examples unlabeled boundary 
non gaussian examples degradation easily produced including examples univariate models interested reader may consult longer version cozman cohen 
discussion obvious consequence previous results unlabeled data fact degrade performance simple situations 
degradation occur modeling errors unlabeled data beneficial absence modeling errors 
important fact understand estimation bias depends ratio labeled unlabeled data somewhat surprising bias usually taken property assumed true models dependent data 
performance obtained set labeled data better performance infinitely unlabeled samples point addition unlabeled data decrease performance 

learning classifiers practice avoid excessively pessimistic theoretical tone mention positive practical experience semi supervised learning 
observed semi supervised learning naive bayes tan classifiers friedman em algorithm handle unlabeled samples quite successful classification problems large numbers features large labeled datasets 
text classification image understanding problems typically fit pattern surprisingly best results literature exactly applications 
plausible explanation applications contain large number observables variance estimators large number available labeled samples reduction variance unlabeled data offsets increases bias 
agrees empirical findings shahshahani landgrebe unlabeled data useful observables classifiers nigam 
suggest adding observables worsen effect unlabeled data opposite expected 
experiments indicate naive bayes tan plagued performance degradation relatively common classification problems example datasets uci repository 
noticed tan classifiers edge naive bayes classifiers 
possible look performance degradation signal modeling assumptions incorrect switch initial naive bayes classifiers tan classifier performance degradation observed 
observe natural way go naive bayes tan classifiers look arbitrary bayesian networks represent relevant distributions significant success direction 
possible approaches bayesian network learning just mention interesting approach produced excellent results 
developed stochastic structure search algorithm named sss essentially performs metropolis hastings runs space bayesian networks observed method demanding huge computational effort improve tan classifiers cohen 
illustrate statements take shuttle dataset uci repository 
labeled samples naive bayes classifier classification error independent test set labeled samples 
labeled samples naive bayes classifier classification error adding unlabeled samples resulting naive bayes classifier error 
tan classifier labeled unlabeled samples leads classification error 
sss algorithm better leading classification error 
interestingly obtain classification error just selecting additional labeled samples randomly producing tan classifier em 
observation suggests active learning profitable strategy labeled unlabeled situation mccallum nigam 
feasible active learning unlabeled data clever effective manner 
combination tan em handle unlabeled data described meila 
close warning reader careful analysis unlabeled data lead better learning methods 
example insights analyze class estimators proposed nigam 

build naive bayes classifiers maximizing modified log likelihood form ll lu ll likelihood labeled data lu likelihood unlabeled data searching best possible reason procedure improve performance may gaussian example section boundary labeled data boundary unlabeled data different sides best linear boundary find best linear boundary changing improve supervised unsupervised learning situation 
case expect find best possible boundary just changing example consider shuttle dataset uci repository labeled unlabeled samples 
observed monotonic increase classification error naive bayes classifiers vary value better just available labeled data 
results obtained nigam 
attributed fact naive bayes correct model text classification fact text classification handles huge number features comments paragraph section apply 

derived studied asymptotic behavior semi supervised learning maximum likelihood estimation theorem 
detailed analysis performance degradation unlabeled data explained phenomenon terms asymptotic bias 
view results overly optimistic statements literature 
procedures algorithm reasonable presence modeling errors 
despite comments note techniques lead better semi supervised classifiers variety situations argued section 
focused modeling errors numerical instabilities 
note modeling errors performance degradation occur 
contributions connect precise way modeling errors performance degradation 
connection argued comes understanding asymptotic bias 
purpose dealt types mod authors argued labeled data weight jaakkola example shows guarantees concerning supposedly superior effect labeled data 
eling errors 
avoided possibility labeled unlabeled data sampled different distributions mclachlan pages second avoided possibility classes represented unlabeled data labeled data due scarcity labeled samples nigam 
believe constraining simpler modeling errors indicated performance degradation prevalent practice 
results extended directions 
interesting find necessary sufficient conditions model suffer performance degradation unlabeled data 
analysis bias enlarged addition finite sample results 
possible avenue look optimal estimators presence modeling errors 
important investigate performance degradation frameworks support vector machines training entropy solutions jaakkola 
conjecture approach incorporates unlabeled data improve performance model correct may suffer performance degradation model incorrect fact seen training results ghani hoovers dataset 
find universally robust semisupervised learning method method major accomplishment 
regardless approach semi supervised learning affected modeling assumptions complex ways 
helpful step understanding unlabeled data peculiarities machine learning 
received continued substantial support hp labs 
alex proposing research labeled unlabeled data suggestions comments course 
tom huang substantial support research moises goldszmidt valuable suggestions george forman telling ijcai workshop kevin murphy free bnt system marina meila muslea comments preliminary version 
coded naive bayes tan classifiers libraries system available www cs cmu edu 
baluja 

probabilistic modeling face orientation discrimination learning labeled unlabeled data 
neural information processing systems 


limiting behavior posterior distributions model incorrect 
annals math 
statistics 
bruce 

semi supervised learning prior probabilities em 
int 
joint conf 
artificial intelligence workshop text learning 
castelli 

relative value labeled unlabeled samples pattern recognition 
phd dissertation stanford university 
castelli cover 

exponential value labeled samples 
pattern recognition letters 
castelli cover 

relative value labeled unlabeled samples pattern recognition unknown mixing parameter 
ieee trans 
information theory 
cohen cozman huang 

learning bayesian network classifiers facial expression recognition labeled unlabeled data 
conf 
computer vision pattern recognition appear 
collins singer 

unsupervised models named entity classification 
int 
conf 
machine learning pp 

comit denis gilleron 

positive unlabeled examples help learning 
int 
conf 
algorithmic learning theory pp 

springer verlag 
cooper freeman 

asymptotic improvement outcome supervised learning provided additional learning 
ieee trans 
computers 
jaakkola 

continuations methods mixing heterogeneous sources 
conf 
uncertainty artificial intelligence pp 

cozman cohen 

unlabeled data degrade classification performance generative classifiers 
int 
conf 
florida artificial intelligence research society pp 

florida 
cozman cohen 

effect modeling errors semi supervised learning mixture models unlabeled data degrade performance generative classifiers athttp www poli usp br fabio cozman publications report ps gz 
devroye lugosi 

probabilistic theory pattern recognition 
new york springer verlag 
friedman geiger goldszmidt 

bayesian network classifiers 
machine learning 
mclachlan 

efficiency linear discriminant function unclassified initial samples 
biometrika 
meila 

learning mixtures trees 
phd dissertation mit 
ghani 

combining labeled unlabeled data text classification large number categories 
ieee int 
conf 
data mining 
goldman zhou 

enhancing supervised learning unlabeled data 
int 
joint conf 
machine learning 
huber 

behavior maximum likelihood estimates nonstandard conditions 
fifth berkeley symposium mathematical statistics probability 
jaakkola meila jebara 

maximum entropy discrimination 
neural information processing systems 
jr 

comparison iterative maximum likelihood estimates parameters mixture normal distributions different types sample 
biometrics 


robustness statistical pattern recognition 
academic publishers 
mccallum nigam 

employing em active learning text classification 
int 
conf 
machine learning pp 

mclachlan 

discriminant analysis statistical pattern recognition 
new york john wiley sons mclachlan basford 

mixture models inference applications clustering 
new york marcel dekker miller uyar 

mixture experts classifier learning labelled unlabelled data 
neural information processing systems 
nigam mccallum thrun mitchell 

text classification labeled unlabeled documents em 
machine learning 
nigam 

unlabeled data improve text classification technical report cmu cs 
school computer science carnegie mellon university pennsylvania 
neill 

normal discrimination unclassified observations 
american statistical assoc 
venkatesh 

learning mixture labeled unlabeled examples parametric side information 
colt pp 

seeger 

learning labeled unlabeled data technical report 
institute adaptive neural computation university edinburgh edinburgh united kingdom 
shahshahani landgrebe 

classification multi spectral data joint supervised unsupervised learning technical report tr ee 
school electrical engineering purdue university indiana 
shahshahani landgrebe 

effect unlabeled samples reducing small sample size problem mitigating hughes phenomenon 
ieee trans 
geoscience remote sensing 
white 

maximum likelihood estimation misspecified models 
econometrica 
zhang oles 

probability analysis value unlabeled data classification problems 
int 
joint conf 
machine learning pp 

