graph partition swendsen wang cuts vision tasks segmentation grouping recognition formulated graph partition problems 
literature witnessed popular graph cut algorithms ncut spectral graph analysis minimum cut maximum flow algorithm 
presents third major approach generalizing swendsen wang method celebrated algorithm statistical mechanics 
algorithm simulates ergodic reversible markov chain jumps space graph partitions sample posterior probability 
step algorithm splits merges re groups sizable subgraph achieves fast mixing low temperature enabling fast annealing procedure 
experiments show converges seconds pc image segmentation 
times faster single site update gibbs sampler times faster ddmcmc algorithm 
algorithm optimize number models works general forms posterior probabilities general existing graph cut approaches 

computer vision problems image segmentation perceptual organization object recognition require grouping image elements pixels primitives coherent visual patterns regions curves objects process optimizing grouping criteria 
problem represented adjacency graph vertices image elements edges spatial relationships subgraphs coherent visual patterns 
graph partition problem 
approaches graph partition literature 
normalized cut graph spectral analysis optimize discriminative criterion :10.1.1.160.2324
minimum cut maps energy minimization problem maximum flow algorithm 
solved polynomial time 
despite reasonable success approaches far general solu adrian song chun zhu university california los angeles departments computer science statistics ucla edu stat ucla edu tions 
firstly shown limited classes energy functions mapped maximum flow problem 
secondly graph spectral analysis discriminative clustering algorithms difficulties expressing global visual patterns shading effects perspective projection effects contour closure furthermore natural images contain diverse visual patterns coherent different ways 
requires generative bayesian formulation incorporating number diverse competing image models 
single discriminative criterion generally applicable visual patterns 
third major graph partition approach generalizing swendsen wang method celebrated algorithm statistical mechanics 
formulated bayesian framework generative image models algorithm simulates ergodic reversible markov chain jumps space possible graph partitions search global optima 
basic ideas contributions method 
adjacency graph compute local probability edge vertices image elements belong pattern 
turning edges random associated probabilities form connected components candidate coherent pattern 

step algorithm splits merges connected component includes big number vertices 
moves ergodic observe detailed balance equations 
candidate states selected proportional posterior probabilities weighted probabilities graph cuts 
acceptance probability algorithm generalized gibbs sampler 

algorithm mixes rapidly low temperature 
mcmc methods longer needs long simulated annealing procedure 
fast annealing starting lower temperature 
start initial conditions heuristics achieve short burn period 
result algorithm times faster classical gibbs sampler flips single vertex time times faster previous ddmcmc algorithm 
converges seconds ghz pc image segmentation 
central contribution mathematical theorems calculating acceptance probabilities big moves observe cancellations calculation 
theorems ensure algorithm samples general posterior probabilities provide foundation fast simulation optimization broad range vision problems 

swendsen wang basic ideas difficulty sampling graph partition space reflected ising potts models statistical mechanics exp 
adjacent spins zero 
string spins label shown highest probability achieved vertices label 
best visiting scheme gibbs sampler flips spins cracks probability po 
flip string spins expected number steps po exponential waiting 

swendsen wang algorithm flips patch spins step 
major speedup ising model equation achieved swendsen wang algorithm 
fig 
shows adjacency graph lattice edge connecting adjacent spins sw turns edge constant probability qo label 
fig 
shows component connected bold edges turned states edges neighbors state state cut turned see crosses 
denote sets edges respective cuts 
sw flips spins single step reversible jump states acceptance probability move shown 
sw flip spins string example steps 
sw algorithm achieves fast mixing critical temperature typical graphs 
unfortunately limited simple ising potts models image data information forming component 
ineffective presence external field data 
extend sw simulating general bayesian posterior probabilities bottom information form candidate components speed computation 

bayesian formulation graph partition 
graph partition go eo adjacency graph vn set vertices image elements pixels primitives eo set edges adjacent elements objective partition graph go unknown number full subgraphs gk vk ek keeping edges go connect vertices vk vk vi vj ek eo vk denote partition subgraphs 
vn gn vertices subset vk forms coherent visual pattern specified generative probability 
space possible partition denoted space partitions 
edges sets vi vj denoted cut vi vj eo vi vj fig shows typical example image segmentation 
obtain segmentation middle applying canny 
input image segmentation atomic regions vertices segmentation result 
edge detection followed edge tracing form atomic regions nearly constant intensities 
atomic regions vertices vo adjacent atomic regions connected edge form graph go 
right image result partition algorithm 

solution space markov chain design take image segmentation example denote iv observed image attributes pixel intensity edge position orientation element iv image representation set suppose classes image models various patterns color texture shading curve type model indexed cl specified parameters ci 
model space union 
inner representation segmentation cn subgraph vi partition specified model ivi ci ci type ci parameters solution space cn 
factorization solution space corresponds necessary solution steps 
partition graph go finding 

select image model subgraph vi 
fit models ci ci assume patterns mutually independent objective simulate bayesian posterior ivi ci ci 
prior image models markov random field models global spline models minimized graph cut algorithms 
markov chain ergodic space stationary probability 
short need types reversible jumps bridging subspaces different dimensions 

jumps model space cn model switching diffusion fitting parameters 
jumps partition space split merge death birth 
jumps realized metropolis hastings methods 
pair states need design proposal probabilities 
idea data driven markov chain monte carlo ddmcmc calculate bottom discriminative models summarized proposal probabilities approximate posterior proposed move accepted high probability min 
bottom data driven information go step making big moves 
algorithm reach state different step may need exponential number small moves discussed ising model example 
follow ddmcmc method jumps model space rest focused designing smart moves partition space fast convergence mixing 

sampling discriminative models adjacency graph go eo augment edge eo binary random variable representing edge turned 
contrast constant probability qo edges sw algorithm compute discriminative model qe local vector valued features texture color geometry sites 
qe indicates coherent similar vertices trained line 
turning edge go probability qe independently obtain sparse graph eo set edges turned chance 
probability qe 
qe eo consisting number connected components gk vk ek 
gk vk ek denote cp vn 
local probabilities qe trained subgraphs cp meaningful parts patterns 
way defines bottom probability partition space 

random samples cp 
shows random graph partitions cp cheetah image adjacency graph go built atomic regions fig 
middle 
column show cp sampled equation 
size components cp controlled temperature edge probabilities smaller larger size components 
clearly various parts cheetah obtained candidates big meaningful moves mcmc algorithm 

stochastic graph partition mcmc go cp 
stages graphs adjacency graph go current partition state sample discriminative models connected components cp graph partition algorithm operates graphs shown 
starts adjacency graph go eo 
current markov chain state partition vl represented graph gl gl vl el full subgraphs go fig obtained go removing edges subsets vl 
move partition states generates connected components cp fig turning edges component cp picked random candidate reassignment 
swendsen wang cuts swc input go eo discriminative probabilities qe eo generative posterior probability 
output samples 

initialize graph partition gl 

repeat current state 
repeat subgraph gl vl el 
el turn probability qe 

vl divided nl connected components gli eli nl 

collect connected components subgraphs see fig cp nl 

select component cp random probability cp usually cp see fig 

propose assign subgraph follows probability go state fig merged existing subgraph state fig new subgraph 

accept move probability theorem 
omit parallel steps model switching fitting clarity 
probability go designed simply follows gl adjacent go new subgraph go 
usually 
move states split merge operation canonical cases 
special cases birth death moves 

new subgraph move birth operation 

equal subgraph vl subgraph gl merged gl 
number subgraphs reduced death operation 
follows give simple explicit expression acceptance probability smarter choice go 
theorem notation consider candidate component selected swc 
proposed move reassign gl gl accepted probability qe vl go min qe go markov chain ergodic observes detailed balance equations 
special case proposed new subgraph vl 
cut empty vl qe 
state state state 
move partition states different set vertices 
vertices color belong subgraph 
vertices connected thick edges form connected component 
proof 
idea proof proposal probabilities complicated ratio extremely simple cancellation 
follows metropolis hastings equation 
calculate proposal probability swc assuming state subgraphs gl vl el canonical case vl vl conditional probability consists steps choosing choosing clarity discuss exception cases 
state subgraph gl broken connected components cpl turning edges el random 
denote set connected components cp nl 
example shows connected components 
cp state denote eon cp edges turned thick edges eon cp nl eki 
rest edges turned cuts connected component vertices subgraph vl cp nl cli cli vl 
note edges subgraphs turned entering state probability choosing cp depends state discriminative models cp qe 
eon cp qe cp denote cp set possible cp state interested cp contain cp cp cp 
loss generality assume component subgraph 
denote cut 
cp cp properties contain edges turned connected vertices 
words cp cp cp cp 
cp cp set picked probability cp 
ready compute proba bility selecting state cp cp cp cp cp cp qe cp qe cp able factor product qe 
eon cp qe cp cp cp 
selected assigned gl probability go cp cp 
proposal probability go 
calculate proposal probability algorithm swc 
canonical case way get state state selecting connected component re assigning gl 
state partition state ex cept belongs gl see fig 

loss generality assume component subgraph 
cp set cp contain component share common cut illustrated crosses 
similarly probability selecting state cp cp cp cp cp cp qe cp qe cp proposal probability qe eon cp go 
cp observation 
cp cp cp vice versa 
cp cp cp set edges turned eon cp eon cp set edges turned cut occurs state cut occurs state cp cp 
plug equations equations probability ratio cancellation qe 
qe qe qe go go equation obtain theorem states 
move observes detailed balance equations 
proof canonical case way go state state state state reassigning 

ways merge subgraphs state get state choose merge choose merge 
exception canonical case paths states occurs subgraph gl gl chosen state subgraphs merged state loss generality consider subgraphs state subgraph state fig 
displays 
path 
choose 
state choose merge reversely state choose split 
path 
choose 
state choose merge reversely state choose split 
proposal probability sum probabilities paths 
go similarly go go go 
state cut vl paths state cut vl paths 
previous calculation proposal probability ratio choosing path qe qe 
qe similarly probability ratio choosing path qe qe 
qe plug equations obtain ratio qe go go go go proposal probabilities designed way go go go go easily satisfied general 
qe go go general notation vl qe vl qe go go proved exception case 
prove ergodicity observe non zero probability node chosen connected component 
node assigned subgraph nonzero probability holds nodes independently get partition partition non zero probability 
proof 
shall construct go way obtain acceptance probability 
algorithm generalized gibbs sampler 
suppose markov chain partition state vn connected component vl selected swc candidate set 
choices state assigning vertex sets sl vl sn vn sn denote states bn respectively 
clearly bl state bn new subgraph 
exception case vl state bn redundant eliminated 
denote cuts sj cj sj 
theorem notation suppose candidate vertex set selected swc partition state probabilities merging vl chosen go qe bl 
proposed move accepted probability bl 
proof straightforward omit 
omit proof go satisfies condition 
practice posteriors bl involve local computation cuts cl small empty 
intuitively algorithm samples random set vertices posterior goodness fit modulated cut probability achieve detailed balance 
general original sw method gibbs sampler 

experiments performance analysis image segmentation experiment performed atomic regions obtained edge detection edge tracing 
form nodes graph 
discriminative probability qe edge vi vj qe kl pi pj kl pj pi 
pi pj bin intensity histograms atomic regions kl kullback leibler divergence 
general qe learned supervised learning 
simple image models constant linear quadratic polynomial intensity additive noise modeled bin histogram fig 
plotted energy vs time seconds runs swendsen wang cuts swc algorithm 
image segmentation input image atomic regions image elements segmentation result 

convergence comparison gibbs sampler upper curves cheetah image 
gibbs sampler start high temperature anneal slowly get minimum energy level algorithm 
right plot shows zoom view seconds 
run gibbs sampler cheetah image fig starting random partition runs starting go partition subgraph 
gibbs samplers converges seconds algorithm starting random partition seconds starting subgraph partition 
convergence faster case algorithm started lower initial energy 
observe algorithm converges times faster gibbs sampler 

convergence comparison swc discriminative models qe dotted curves 
cheetah image left airplane image right 
original swendsen wang apply case 
fig comparison algorithm discriminative models cheetah image left airplane image right starting go 
plotted energy vs time runs swc algorithm discriminative models smooth curves discriminative models dotted curves fixed edge weights constants qe 
convergence slows significantly annealing schedule slower initial temperature higher discriminative models 

curve grouping experiment input image edge map grouping result 
perceptual grouping experiment group map obtained canny edge map long smooth curves adding removing 
curve prior point histograms learned hand segmented examples 
likelihood measures difference pixels input edge map grouping result 
graph nodes discriminative probability qe point histogram gap filled 
results shown fig 
green reversible jump mcmc comput 
bayes 
model determination biometrika hofmann buhmann pairwise data clustering deterministic annealing pami 
jain dubes algorithms clustering data prentice hall englewood cliffs nj 
geman geman stochastic relaxation gibbs distrib bayesian restoration img pami vol 

kirkpatrick gelatt vecchi optimization simulated annealing science 
kolmogorov zabih energy functions min 
graph cuts eccv metropolis eqns state calculation fast computing machines chem 
phys 
roy stereo epipolar lines maximum flow formulation ijcv shi malik normalized cuts image segmentation pami pp 

tu zhu image segmentation data driven mcmc pami 
swendsen wang critical dynamics mc simulations phys 
rev lett pp wu leahy optimal graph theoretic approach data clustering pami vol 
