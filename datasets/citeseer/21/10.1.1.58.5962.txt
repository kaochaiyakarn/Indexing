logical markov decision programs convergence logical td kristian kersting luc de raedt institute computer science machine learning lab albert university georges hler geb freiburg brg germany kersting informatik uni freiburg de 
developments area relational reinforcement learning rrl resulted number new algorithms 
theory explains rrl works lacking 
provide initial results theory rrl 
realize introduce novel representation formalism called logical markov decision programs lomdps integrates markov decision processes mdps logic programs 
lomdps compactly declaratively represent complex mdps 
framework devise relational upgrade td called logical td prove convergence 
experiments validate approach 
past years lot extending probabilistic stochastic frameworks abilities handle objects relations see overview 
inductive logic programming relational learning point view approaches upgrades propositional representations relational computational logic representations 
contribution extends line research decision making 
introduce novel representation formalism called logical markov decision programs lomdps combines mdps logic programming 
result flexible expressive framework defining mdps able handle structured objects relations 
mdps framework grounded computational logic missing 
report combining mdps reiter situation calculus :10.1.1.111.9112
complex model free reinforcement techniques addressed 
lomdps share upgrades propositional representations advantages 
logical expressions form clauses rules transitions may contain variables abstraction specific grounded transitions 
allows compactly represent complex domains 
secondly abstraction number parameters rewards probabilities significantly reduced 
turn allows principle speed simplify learning learn ground level 
fascinating machine learning techniques developed name reinforcement learning rl context mdps decades cf 

increased attention dealing relational representations objects rl see 
works taken practical perspective developed systems experiments operate relational worlds 
heart systems usually function approximator logical regression tree able assign values sets states sets state action pairs 
far theory explains approach works lacking 
second contribution step direction theory 
theory notion states policies represented logical expressions 
state represents set concrete states policy function states actions 
ground states represented state essentially assigned action 
akin happens relational rl logical regression trees rrl rt leaf regression tree represents state states classified leaf obtain value action 
convergence result best knowledge context relational reinforcement learning 
proceed follows 
mathematical preliminaries reviewed section 
lomdp framework introduced section 
section defines policies section introduces general framework generalized relational policy iteration learn policies 
section devise evaluation policies 
experimentally validated section 
concluding discuss related 
preliminaries section introduce basic terminology relational logic mdps cf 

logic order alphabet set relation symbols arity set functor symbols arity 
called constant called proposition 
atom 
tm relation symbol followed bracketed tuple constants variables 
conjunction set atoms 
substitution 
vn tn tex assignment terms ti variables vi 
conjunction said subsumed conjunction denoted exists substitution term atom clause called ground contains variables vars 
substitution general unifier mgu atoms iff 
substitution exists substitution 
herbrand base denoted hb set ground atoms constructed predicate functor symbols alphabet 
rrl short hand zeroski approach 
distinguish relational reinforcement learning rrl rt short hand 
notation atoms written lower case sets atoms upper case sets sets atoms bold upper case highlight resp 
may ground write resp 

markov decision processes mdp mdp tuple 
set system states 
agent available finite set actions state cause stochastic state transitions 
transition transition denotes probability action causes transition state executed state holds 
agent gains expected reward transition 
reward function probabilistic mean value depends current state action mdp called nondeterministic deterministic 
consider mdps stationary transition probabilities stationary bounded rewards 
stationary deterministic policy set expressions form 
denotes particular course actions adopted agent action executed agent state assume infinite horizon agent accumulates rewards associated states enters 
rewards discounted 
value policy solution system linear functions 
policy optimal policies stationary nondeterministic policy maps state distribution actions 
value expectation 
logical markov decision programs logical component mdp essentially propositional representation state action symbols flat 
key idea underlying logical markov decision programs lomdps replace flat symbols symbols 
definition 
state conjunction logical atoms logical query 
case empty conjuction write 
states represent sets states 
formally state finite conjunction ground facts alphabet logical interpretation subset herbrand base 
blocks world possible state fl bl bl cl cl fl denotes object cl states clear bl denotes block fl refers floor 
state bl bl 
represents states alphabet block top block formally speaking state represents states exists substitution denote set states 
substitution previous example 
able define transitions 
definition 
transition expression form act action body head states 
assume range restricted vars vars vars vars transition relies information encoded current state 
semantics transition agent state go state probability performing action receiving expected reward illustration purposes consider transition moves block floor probability fl cl cl applied state exp mv fl cl fl fl cl cl bl bl bl transition tells mv fl leads fl fl fl cl cl cl bl bl bl probability gaining reward 
see implements kind order variant probabilistic strips operators 
lomdps typically consist set multiple transitions constraints imposed order obtain meaningful lomdps 
set bodies state transitions lomdp modulo variable renaming 
denote set actions lomdp 
require 
body act condition guarantees successor states specified executing action state probabilities sum 
secondly need way cope contradicting transitions rewards 
consider transitions state 
problem transitions transition says execute go probability state ones assign probability state state 
ensure state firing assume total order pairs actions rules bodies appear apply conflict resolution similar prolog 
forward search implicitly assume action preconditions 
mv fl mv mv fl stacks blocks floor fig 

underlying patterns blocks world stacking 
stacks height 
stack left goal state 
cuts indicate resp 
top block floor 
pairs stopping matching 
instance transition fires state transitions fire 
able formally define lomdps 
definition 
logical markov decision process lomdp tuple order alphabet set actions finite set state transitions actions discount factor holds 
giving semantics lomdps illustrate lomdps stack example blocks world goal move blocks single stack 
sake simplicity assume variables denote different objects absorb fl cl cl cl cl cl absorb absorb absorb 
mv fl cl cl 
mv cl cl 
cl 
transition probabilities sum action additional transition staying current state 
order understand lomdp stack understand states govern underlying patterns blocks world cf 

states artificial absorb state excluded order occur cover possible state action patterns blocks world furthermore stack episodic task ends reaching goal state 
rl episodic tasks encoded absorbing states transition generate zero rewards 
transition encodes absorbing achieved adding diff denote different 
assume start legal blocks world 
floor moved 
state 
transitions cover cases stacks 
transition encodes situation stack goal state stack 
cl bl describe preconditions mv floor moved 
performing action mv state exp see transition omitted transition staying state firing 
similar easily encode unstack goal 
note specified number blocks 
lomdp represents possible blocks worlds transitions probability reward parameters number parameters propositional system explodes blocks states 
theorem shows semantics lomdps uniquely specified case functors focus functor free lomdps 
case induced mdp finite 
theorem 
lomdp specifies discrete mdp 
proof sketch hb hb set ground atoms built states predicates hb hb set ground atoms built action names 
construct follows 
countable state set consists finite subsets hb 
set actions state minimal holds 
probability transition state performing action probability value associated unique transition matching normalized number transitions form transition connecting probability zero 
bounded rewards constructed similar way normalized 
theorem theorem follows lomdp exists optimal policy ground states 
lomdps generalize finite mdps finite mdp propositional lomdp relation symbols arity 
policies theorem states lomdp specifies discrete mdp 
existence optimal policy mdp guaranteed 
course policy extensional propositional sense specifies ground state separately action execute 
specifying policies lomdps large state spaces cumbersome learning require effort 
introduce policies intentionally specify action take state sets states 
definition 
policy finite set decision rules form action state vars vars assume applicable mv fl mv fl mv mv fig 

decision rules unstack stack policy 
decision rules ordered left right rule fires rule left fires 
usually consists multiple decision rules 
apply conflict resolution technique transitions 
means assume total order decision rules forward search stopping matching decision rule prolog 
consider instance unstack stack policy mv fl fl cl cl cl 
mv fl cl cl 
mv fl cl cl 
mv cl cl 
cl 
omitted absorb state front statements variables refer different blocks 
instance state exp see decision rule fire 
policy graphically depicted interesting reasons 

close unstack stack strategy known planning community 
basically strategy amounts putting blocks table building goal state stacking blocks floor single stack 
block moved twice 
number moves twice number blocks 

perfectly generalizes blocks worlds matter blocks 

learned propositional setting optimal propositional policy encode optimal number moves 
meaning decision rule agent state agent performs action uniform probability denoted 

set bodies ordered 
call abstraction level assume covers possible states lomdp 
total order guarantees forms partition states 
equivalence classes 
induced inductively defined 
generally coincide proposition holds 
generalized relational policy improvement generalized policy improvement refine greedy refinement evaluation improvement fig 

generalized relational policy iteration accounts different abstraction levels 
upgrade generalized policy iteration traditional reinforcement learning illustrated 
greedy denotes greedy policy computed current value function see 
proposition 
policy specifies nondeterministic policy level ground states 
generalized relational policy iteration crucial question learn policies 
sutton barto reinforcement learning systems follow called generalized policy iteration scheme shown 
consists interacting processes policy evaluation policy improvement 
evaluating policy refers computing value function current policy policy improvement refers computing new policy current value function 
directly applied learn policies 
different abstraction levels explored 
needs additional process call policy refinement 
resulting generalized relational policy improvement scheme illustrated 
generally speaking policy refinement traverses space possible policies 
apply ilp techniques 
instance refine unstack stack policy adding refined variant decision rule mv fl floor fl cl cl cl 
better 
model domain score different refinements measuring influence refinement state remaining state 
model domain employ experience states visited 
approach followed zeroski rrl rt 
implement policy refinement zeroski employ logical regression trees state action value function approximations 
starting initial abstraction level rrl integrates evaluation improvement refinement uses episodes build regression tree 
rrl rt seen instance 
empirically rrl rt proven wide range domains blocks world 
provide step explaining works 
precisely focus relational evaluation problem approaches prove convergence upgrade td 
logical td relational evaluation problem considers compute state value function arbitrary policy 
focus model free approaches 
model free approaches know reward transition functions advance computing value policies experiences xt yt rt furthermore contrast traditional model free approaches maintaining values states underlying mdp feasible 
basic idea come relational evaluation approach define expected reward average expected value states 
model examine state contradictory observations rewards transition probabilities 
best model average observations prior knowledge model 
unfortunately experimentally shown modelbased reinforcement learning function approximation converge general see 
fortunately hold 
prove convergence reduce evaluation problem evaluation problem state aggregation see respect 

ease explanation focus td approach see 
results general td obtained applying tsitsiklis van roy results 
algorithm sketches logical td 
experience policy updates estimate nonterminal state visited updates estimate happens visit 
updating estimate level states updates estimate abstraction level 
show convergence sufficient reduce td soft state aggregation 
basic idea soft state aggregation cluster state space map state space clusters 
ck 
state belongs cluster ci certain probability ci 
value function computed level clusters states 
logical td special case soft state aggregation 
see recall abstraction level partitions state space view states clusters state belongs cluster 
furthermore state set action set finite agent similar analysis done model approaches 
gordon showed value iteration function approximator converges bounded distance optimal value function original mdp 
policy abstraction level initialize sm arbitrarily repeat episode initialize ground state repeat step episode choose action described section select decision rule matches ii 
select induced ground actions 
take action observe successor state described section select probability transition matches select induced successor states 
state matching set terminal converged maximal number episodes exceeded algorithm logical td learning rate approximation 
nondeterministic policy 
assumptions theorem fulfilled 
theorem adopted singh corollary 
td soft state aggregation applied policy converges probability solution system equations follows lt converges applied lomdp policy abstraction level converges probability solutions system equations 
note arbitrary abstraction levels theorem shows learning find solutions error ground state space zero general 
equation basically states policy induces process 
transition probabilities rewards state averages corresponding values covered ground states see 
due process appears learner non markovian nature 
consider lomdp 
abstraction level 
assign probabilities rewards transitions ones transition 
consequently values state 
assigns differ ent values traces show 

converges level generalize unseen ground states due abstraction 
summarize theorem shows temporal difference evaluation policy converges 
different policies different errors ground state space 
context see section suggests refinement operators heuristically reduce error ground state space 
applied rrl rt reads follows 

logical regression tree induces finite abstraction level evaluation fixed regression tree converges 

relational node state splitting state sequences encountered far heuristically reduce error ground state space 
experiments task evaluate policies blocks world 
task motivated experiments relational reinforcement learning rrl fact blocks world prototypical toy domain requiring relational representations 
contrast experiments reported rrl exclusively standard predicates cl bl 
needed background knowledge predicates height stacks directives order regression tree learner 
difference approach rrl induces relevant states automatically regression tree learner 
goal system put hypotheses test converges finite abstraction levels 
policies compared 
works actions multiple outcomes 
relational policy refinement needed 
variance heuristic state splitting criterion 
implemented prolog system yap 
experiments run ghz linux machine discount factor learning rate set 
randomly generated blocks world states blocks blocks blocks procedure described 
set states constituted set start starting states experiments 
note blocks traditional mdp represent states goal states 
result experiment average runs episodes new episode randomly selected state start starting state 
run value function initialized zero 
note experiments policies value functions apply matter blocks 
value unstack stack episodes value modified unstack stack episodes fig 

learning curves td evaluation problem unstack stack policy modified unstack stack policy 
predicted values shown function number episodes 
data averages error bars show standard deviations 
experiment task evaluate unstack stack policy stack lomdp introduced 
results summarized clearly show hypothesis holds 
learning curves show values states converged converged 
note value state remained 
reason accident state blocks floor start 
furthermore values converged similar values runs 
values basically reflect nature policy 
better single stack multiple ones 
total running time episodes seconds measured yap build statistics runtime 
experiment reinforcement learning policy improvement refers computing new policy current value function 
relational setting success computing greedy policy value function depends granularity value function 
instance value function possible distinguish mv fl mv actions decision rule get expected values 
overcome refine abstraction level see experiment evaluate different policies abstraction level 
experiments evaluated modified unstack stack policy way experiment 
differed unstack stack policy perform mv fl move decision rule results summarized 
interestingly values states dropped approximately approximately 
shows unstack stack preferred policy 
furthermore total running time episodes increased seconds average length episode increased 
clearly shows hypothesis holds 
value multiple outcomes episodes value unstack episodes fig 

learning curves td evaluation problem unstack stack policy actions underlying lomdp multiple outcomes underlying lomdp encoded unstack problem 
predicted values shown function number episodes 
data averages error bars show standard deviations 
experiment reran experiment underlying lomdp encoded mv fl mv multiple outcomes 
action mv fl fl block may fall action mv block may fall floor fl 
converged shows 
shows hypotheses holds 
experiment reran experiment evaluated policy underlying lomdp encoded unstack problem 
converged shows 
running time episodes increased seconds underlying lomdp complex 
shows hypothesis holds 
experiment investigated goal 
underlying lomdp absorb fl cl cl cl 
mv fl cl cl 
mv cl cl cl cl 
mv floor floor cl cl cl 
assuming variables denote different objects absorb omitted 
transition probabilities sum additional transition staying current state 
experimental setup experiment step size evaluated different policies value restricted policy episodes value decision rules episodes fig 

learning curves td evaluation problem policy underlying lomdp encoded goal 
predicted values shown function number episodes 
data averages error bars show standard deviations state non zero value shown note finite set starting states 
policy restricted 
decision rules 
mv cl cl 
mv fl cl 
mv fl cl cl 
mv fl cl cl 
mv fl cl 
mv fl cl 
mv fl cl 
mv fl cl 
mv fl cl cl 
mv fl cl cl 
mv fl cl 
cases converged shows 
running time episodes seconds seconds 
experiments state exceptional 
obeyed higher variance states 
reason acts kind container state situations covered preceding states 
refined policy added states showed low variances 
may iterate refine 
experiments show hypotheses hold supports variance state splitting approach taken rrl rt 
related reinforcement learning rl currently significant interest rich representation languages 
kaelbling finney investigated methods relational domains deictic representations drs 
drs avoid enumerating domain variables block floor 
drs led impressive results finney results show dr may degrade learning performance relational domains 
finney relational reinforcement learning rrl rt way effectively learning domains objects relations 
function approximated relational regression tree learner 
experimental results interesting rrl rt explain theoretical terms works 
provide new insights 
furthermore complements kersting de raedt van approaches :10.1.1.14.2470
kersting de raedt report experiments relational upgrade learning 
van devised relational prioritized sweeping approach 
works consider convergence proofs 
lomdp formalism related poole independent choice logic 
independent choice logic dependencies probabilities explicit consider learning problem 
general point view approach closely related decision theoretic regression dtr 
state spaces characterized number random variables domain specified logical representations actions capture regularities effects actions 
existing dtr algorithms designed propositional representations mdps boutilier proposed order dtr probabilistic extension reiter situation calculus 
language certainly expressive lomdps 
complex 
furthermore boutilier assume model model free learning methods applied 
model approach yoon introduced method generalizing policies small large relational mdps employing description logics 
method extended approximated policy iteration 
guestrin specify relationally factored mdps probabilistic relational models reinforcement learning setting 
contrast lomdps relations change time 
assumption hold domains blocks world 
idea solving large mdp reduction equivalent smaller mdp discussed 
relational order representations investigated 
kim dean investigate modelbased rl non homogenous partitions propositional factored mdps 
furthermore great interest abstraction levels state spaces 
abstraction time primitive actions useful ways specific sub actions time 
research orthogonal applied lomdps 
baum reports solving blocks worlds blocks rl related techniques 
language domain dependent logic programming 
introduced representation integrates mdps logic programs 
framework allows compactly declaratively represent complex relationally factored mdps 
furthermore allows gain insights relational reinforcement learning approaches 
precisely introduced policies lomdps generalized relational policy iteration general scheme learning policies 
policies basically introduce state aggregation evaluated simple upgrades propositional reinforcement learning methods td learning 
convergence proven experimentally validated 
convergence guarantees important results exact relational value iteration rvi show rvi require infinite abstraction level infinite logical regression tree order converge exact values 
approximative approaches useful allow cut regression tree level estimate best values achieve abstraction level cf 
experiment 
words approximation interesting feature cases necessity successful relational reinforcement learning 
authors hope framework useful starting point theoretical developments rrl 
interest real world applications extending language negation quantification unordered transitions mdp specific relational state splitting rules applying gordon convergence results rrl rt nearest neighbour possible prove convergence logical learning 
point challenging introduces action aggregation 
initial experiments show logical learning performs 
acknowledgments authors deeply grateful bob givan valuable discussions topic 
authors martijn van fruitful discussions topic 
anonymous reviewers comments helped improve 
research supported european union ist programme contract 
fp application probabilistic inductive logic programming ii 

andre russell 
programmable reinforcement learning agents 
advances neural information processing systems pages 
mit press 

baum 
model intelligence economy agents 
machine learning 

boutilier hanks 
decision theoretic planning structural assumptions computational leverage 
jair 

boutilier reiter price 
symbolic dynamic programming firstorder mdps 
seventeenth international joint conference artificial intelligence ijcai pages seattle usa 

boyan moore 
generalization reinforcement learning safely approximating value function 
advances neural information processing systems volume 

de raedt kersting 
probabilistic logic learning 
acm sigkdd explorations special issue multi relational data mining 

dearden boutilier 
abstraction approximate decision theoretic planning 
artificial intelligence 

thomas dietterich 
hierarchical reinforcement learning maxq value function decomposition 
journal artificial intelligence research 

driessens ramon 
relational instance regression relational reinforcement learning 
proceedings twelfth international conference machine learning pages washington dc usa 

zeroski de raedt driessens 
relational reinforcement learning 
machine learning 

zeroski lavra 
relational data mining 
springer verlag 

fern yoon givan 
approximate policy iteration policy language bias 
proceedings neural information processing conference nips 

finney kaelbling oates 
thing tried didn deictic representation reinforcement learning 
proceedings eighteenth international conference uncertainty artificial intelligence uai 

flach 
simply logical intelligent reasoning example 
john wiley sons 

friedman getoor koller pfeffer 
learning probabilistic relational models 
proceedings sixteenth international joint conferences artificial intelligence ijcai pages stockholm sweden 
morgan kaufmann 

givan dean greig 
equivalence notions model minimization markov decision processes 
artificial intelligence 

gordon 
stable fitted reinforcement learning 
advances neural information processing pages 
mit press 

guestrin koller 
generalizing plans new environments relational mdps 
proceedings international joint conference artificial intelligence ijcai acapulco mexico 

kaelbling oates finney 
learning worlds objects 
working notes aaai stanford spring symposium learning grounded representations 

kersting de raedt 
logical markov decision programs 
working notes ijcai workshop learning statistical models relational data srl pages pp 


kersting van de raedt 
bellman goes relational 
proceedings international conference machine learning icml banff alberta canada july 
appear 


kim dean 
solving factored mdps non homogeneous partitions 
artificial intelligence 

mccallum 
reinforcement learning selective perception hidden states 
phd thesis department computer science university rochester 

muggleton de raedt 
inductive logic programming theory methods 
journal logic programming 

moore 
influence variance markov chain application adaptive discretization optimal control 
proceedings ieee conference decision control 

poole 
independent choice logic modelling multiple agents uncertainty 
artificial intelligence 

puterman 
markov decision processes discrete stochastic dynamic programming 
john wiley sons 

singh jaakkola jordan 
reinforcement learning soft state aggregation 
advances neural information processing pages 
mit press 

slaney thi baux 
blocks world revisited 
artificial intelligence 

sutton barto 
reinforcement learning 
mit press 

sutton precup singh 
mdps semi mdps framework temporal abstraction reinforcement learning 
artificial intelligence 

tsitsiklis van roy 
analysis temporal difference learning function approximation 
ieee transactions automatic control 

van 
reinforcement learning relational mdps 
proceedings annual machine learning conference belgium netherlands 

whitehead ballard 
learning perceive act trial error 
machine learning 

yoon fern givan 
inductive policy selection order mdps 
proceedings international conference uncertainty artificial intelligence uai 
