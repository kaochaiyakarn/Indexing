unsupervised anomaly detection network intrusion detection clusters leung christopher victoria laboratory department computer science software engineering university melbourne victoria australia email cs mu oz au current network intrusion detection systems employ signature methods data mining methods rely labelled training data 
training data typically expensive produce 
methods difficulty detecting new types attack 
unsupervised anomaly detection techniques system trained unlabelled data capable detecting previously unseen attacks 
new density grid clustering algorithm suitable unsupervised anomaly detection 
evaluated methods kdd cup data set 
evaluation shows accuracy approach close existing techniques reported literature advantages terms computational complexity 
increased usage computer networks security critical issue 
network intrusion malicious unauthorised users cause severe disruption networks 
development robust reliable network intrusion detection system ids increasingly important 
traditionally signature automatic detection methods widely intrusion detection systems 
attack discovered associated traffic pattern recorded coded signature human experts detect malicious traffic 
signature methods suffer inability detect new types attack 
furthermore database signatures growing new types attack detected may affect efficiency detection 
methods proposed machine learning algorithms train labelled network data instances preclassified attack lee stolfo 
methods classified categories misuse detection anomaly detection 
misuse detection approach machine learning algorithm trained set labelled data automatically builds detection models 
detection models similar signatures described 
detection methods weakness signature methods vulnerable new types attack 
copyright australian computer society appeared th australasian computer science conference university newcastle australia january 
conferences research practice information technology vol 

castro ed 
reproduction academic profit purposes permitted provided text included 
contrast anomaly detection approaches build models normal data attempt detect deviations normal model observed data 
consequently algorithms detect new types intrusions new intrusions assumption deviate normal network usage javitz denning 
algorithms require set purely normal data train model 
training data contains traces intrusions algorithm may detect instances attack assume normal 
circumstances labelled data purely normal data readily available time consuming expensive manually classify 
purely normal data hard obtain practice hard guarantee intrusions collecting network traffic 
address problems new type intrusion detection algorithm called unsupervised anomaly detection 
assumptions data 
assumption majority network connections normal traffic 
traffic malicious 
eskin stolfo assumption attack traffic statistically different normal traffic 
javitz denning algorithm takes input set unlabelled data attempts find intrusions buried data 
intrusions detected train misuse detection algorithm traditional anomaly detection algorithm data 
assumptions fail performance algorithm deteriorate 
example difficulties detecting bandwidth dos attack 
reason attacks instances intrusion occurs similar number normal instances 
took similar approach eskin arnold stolfo problem employed clustering method unsupervised anomaly detection 
approach chose clustering method designed dealing high dimensional data large data sets 
evaluated algorithm real network data 
training testing done kdd cup data kdd popular widely intrusion attack data set 
results show accuracy algorithm approaches previous works 
furthermore computational complexity algorithm approach promising 
able infer results requirements intrusion detection system 
structured follows 
section give general survey field anomaly detection network intrusion detection 
section describe clustering algorithm detail illustrate algorithm running example 
analyse average case worst case complexity algorithm 
section describe details experiment results graphically 
section discuss results possible implications 
section suggest possible directions investigation 
related unsupervised supervised anomaly detection applying unsupervised anomaly detection network intrusion detection new research area drawn interest academic community 
eskin 
eskin investigated effectiveness algorithms intrusion detection fixed width clustering algorithm optimised version nearest neighbour algorithm class support vector machine algorithm 

carried research clustering method eskin showed improvements accuracy clusters adaptive changing traffic patterns 
different approach support vector machine proposed schafer moderate success 
eskin mixture model explaining presence anomalies machine learning techniques estimate probability distributions mixture detect anomalies 
novel tier ids proposed 
tier uses unsupervised clustering classify packets compresses information payload second tier anomaly detection algorithm information tier intrusion detection 
lane brodley lane brodley evaluated unlabelled data looking user profiles comparing activity intrusion activity normal 
supervised anomaly detection network intrusion detection uses purely normal instances training data studied extensively academic community 
comprehensive survey various techniques kumar srivastava 
approach modelling normal traffic self organising maps gonzalez dasgupta uses principal component classifiers obtain model chen chang 
approach uses graphs modelling normal data detect irregularities graph anomalies noble cook 
approach uses normal data generate abnormal data uses input classification algorithm gonzalez dasgupta 
clustering clustering known studied problem 
exist large number clustering algorithms literature 
methods categorised partitioning methods hierarchical methods methods grid methods 
shall concentrate algorithms closely related investigation 
partitioning methods database objects partitioning method constructs partitions data partition represents cluster 
partitioning method interest study fixed width clustering algorithm 
algorithms studies stolfo 
eskin 
compare results 
main advantage fixed width algorithm scales linearly number objects data set number attributes objects 
quality clusters sensitive definition width cluster user needs repetitions algorithm choose best value particular application 
density methods density methods simple assumption clusters dense regions data space separated regions lower density 
general idea continue growing cluster long density neighbourhood exceeds threshold 
words data point cluster neighbourhood radius contain minimum number points 
methods filtering outliers discovering clusters arbitrary shapes 
examples density methods dbscan ester kriegel sander xu optics ankerst breunig kriegel sander 
grid methods grid methods divide object space finite number cells form grid structure 
clustering operations performed grid structure 
main advantage approach fast processing time typically dependent mainly number cells dimension quantised space 
examples grid methods sting wang yang muntz chatterjee zhang clique agrawal gehrke gunopulos raghavan choudhary 
builds clique algorithms 
clique clustering quest agrawal hybrid clustering method combines idea grid density approaches 
clique partitions dimensional data space non overlapping rectangular units 
attempts discover distribution patterns data set identifying sparse dense units space 
identification candidate search space monotonicity principle dimensional unit dense projections dimensional space 
optimised improved version clique 
main differences 
adaptive grid algorithm reduce total number potential dense units merging small dimensional partitions similar densities 
second operation generation population candidate dense units computer cluster 
scale exponentially dimension cluster highest dimension data set 
mining frequent itemsets focus primarily clustering techniques mining frequent itemsets intermediate steps algorithm 
shall briefly mention known methods apriori agrawal srikant fp growth han pei yin section 
apriori algorithm known method mining frequent itemsets 
employs iterative approach known level wise search 
finding level itemsets requires full scan database 
reduce search space candidate generated subsets frequent 
approach guarantees find frequent itemsets computational complexity exponential number maximally frequent itemsets may need repeatedly scan database check large set candidate itemsets pattern matching 
infeasible large databases containing millions records long patterns 
frequent pattern growth fp growth efficient method mining frequent itemsets 
avoids cost generating huge set candidate itemsets building compact prefix tree data structure frequent pattern tree fp tree 
algorithm scans database derives set frequent items support counts 
set sorted order descending support count 
denote resulting set 
construct fp tree algorithm scans database processes items record order sorted descending support count 
processed itemset represents branch tree frequent item represented node 
add branch tree exist 
prefix branch exists tree increment count node common prefix extend branch 
algorithm mines frequent itemsets fp tree nodes deep inside tree 
shown hipp fp growth algorithm order magnitude faster apriori large databases 
methodology aim discover characteristics majority connections records network traffic result classify connections 
requires data preprocessing procedures transform log records network trace program formatted set records data mining 
preprocessing step easier data set prepared mit lincoln labs kdd cup competition kdd 
widely accepted academic community shall give brief description data set section 
simplification reduce problem clustering high dimensional data large data set 
clustering algorithm aim discover set clusters large volume high dimensional input data 
main challenges efficiently process data run time algorithm scales number records high dimensionality data accurately determine boundary clusters ensure set clusters produced algorithm cover data set 
maximally frequent itemset frequent itemset superset frequent 
overview developed grid density clustering algorithm similar application 
optimised version original algorithm modification frequency pattern tree fp tree intermediate step 
implementation able run large data set records single pc terminates minutes 
general structure algorithm follows 
find frequent baskets modified adaptive grid algorithm 

transform data instances sequence frequent baskets build fp tree 

recover set candidate clusters count back method 

examine set clusters remove duplicate clusters 
obtain set clusters expect cover data set 
point falls inside clusters labelled normal 
small percentage points belong clusters labelled abnormal 
shall illustrate set data points dimensional space 
dimensions continuous range inclusive 
simplicity data points assumed integer values 
points total points represented diamond 
sample data set feature space adaptive grid adaptive grid algorithm adopted includes modifications suit application 
goal algorithm divide dimension unequal partitions data distribution 
adaptive intervals basis frequent baskets determine formation boundary clusters 
adoption parameters constants chosen set values stated 

denote set dimensions feature space 
dimension di denotes ith dimension divided small equi length partitions called fine bins bi denotes jth bin dimension 
frequent pattern vi min denote minimum value dimension di vi max denote maximum value dimension di number fine bins dimension di formula 
bi max vi max vi min algorithm repeats process dimension feature space 
process finished algorithm reads data increments frequency fine bin data point falls range fine bin 
starting form fine bin set consecutive fine bins window wi bi bi bi bi bi bi largest histogram value fine bins taken represent value window wi max histogram value bi algorithm examines adjacent windows merges windows values windows differ percentage 
chosen 
wi jm denote merged window wi denote value window wi expressing idea mathematically wi mj wi wi wi wi value merged window average combined windows 
process repeated algorithm examines windows dimensions 
windows adaptive grid bins data particular dimension uniformly distributed algorithm returns adaptive grid consisted dimension 
need examine dimension sure contribute potential clusters 
split dimension equi length partitions 
need determine adaptive grids worth investigating 
calculate threshold adaptive grid bin distinguish dense adaptive grid bins sparse ones 
dense bins expect contain data points respect sizes 
threshold defined formula 
ti ai jn di ai range bin di range dimension bin belongs total number points data set 
density factor quantifies density bin value chosen 
uniformly distributed dimensions mentioned earlier density factor lowered adoption algorithm examination 
frequency adaptive grid bins exceeds threshold algorithm reports frequent baskets hierarchy entities algorithm illustrated 
mentioned adopted adaptive grid algorithm certain limitations inherent ambiguity 
example specifically deal dimensions limited set discrete unrelated values implicitly assumes continuous dimensions fractional values 
extensions algorithm order types data sets 
frequent basket adaptive grid window fine bin dimension hierarchy entities binary discrete attributes map values integers treat continuous dimensions 
range dimension equal number discrete values attribute 
number bins dimensions equal range dimension bins adaptive grid bins directly merging 
initial analysis data set characterise numeric attributes categories 
attributes represent percentage ranges attributes integer values range hundreds 
attributes integer values ranges millions 
original algorithm process categories attributes inefficient little sense 
type attribute meaningless split dimensions fine bins fine bins significant values contain data points 
words attributes percentage type possible significant values inclusive 
majority fine bins zero frequency count defeat purpose adaptive grid bins 
dividing fine bins allocate bins significant values dimension 
bins turn windows directly windows may merge form adaptive grid bins 
second type attributes allocate number fine bins required possible number frequent baskets reported large 
size dimension exceeds predefined large value transform old values logarithmic scale formula 
vnew ln new values mapped continuous dimension algorithm process efficiently 
set algorithm 
modifications illustrated 
window fine bin dimension 
attributes range hundreds 
attributes range millions ln modification adaptive grid algorithm best illustrate algorithm sample data space earlier see works 
working small set data increase value better illustrate algorithm 
range dimensions algorithm allocates fine bins fine bins fine bin histograms fine bins fine bin histogram fine bins frequency counts illustrated 
ranges dimensions small fine bins turned windows directly 
starting windows merged count zero points 
fourth window merged fifth window difference value threshold 
sixth window merged similar fashion 
seventh window merged difference larger threshold 
process continues remaining windows dimension resulting adaptive grid bins illustrated 
adaptive grid window adaptive grid bins formula calculate threshold adaptive grid bins 
example threshold second adaptive grid bin equal 
frequency bin reported frequent baskets 
repeating procedure adaptive grid bins find total adaptive grid bins dense report frequent baskets 
shall name frequent basket dimension baskets similar fashion 
complete set frequent baskets frequency count 
frequent pattern tree mentioned section approach proposed clique similar apriori algorithm agrawal srikant mining association rules 
algorithms time complexity highest dimensionality clusters constant 
overcome problem need different approach 
fp growth proposed method mining frequent itemsets wish employ adaptive grid window adaptive grid bins fundamental idea fp growth algorithm clustering algorithm 
set frequent baskets obtained earlier thought equivalent frequent items transaction database 
value data point falls inside range frequent baskets data point thought record contains frequent item 
repeat process attributes able transform data points set frequent baskets 
transform entire data set set records various numbers frequent baskets problem finding clusters equivalent mining frequent itemsets database transactions 
dimensional cluster frequent itemset 
fp tree efficiently mine frequent itemsets 

denote data point data set 
set frequent baskets 
define function pi lies range fi pi fi pi lay range fi define transformation dimensions pi fi fi fi pi fi transformation obtain set frequent baskets 
sort set frequency frequent basket frequent frequent 
apply transformation data points obtain sets frequent baskets representing entire data set 
construct fp tree accordingly 
construct fp tree described han subtle difference maintain node link pointer node fp tree carrying frequent item 
reasons change 
node link recover clusters yield subspace projections set high dimensional clusters interested 

requirements clustering algorithm cover data set 
original method specified fp growth mine frequent itemsets guarantee meet requirement 
shall continue discussion data set illustrate construction fp tree 
obtain set frequent baskets sort set order descending frequency count 
new set 
transform data points sets frequent baskets sort order 
transformations shown table 
transformation construct fp tree description han 
final fp tree shown 
data frequent point baskets table transformation points frequent baskets root fp tree cluster recovery count back rule monotonicity stated agrawal dimensional cluster cluster dimensional subspace projection 
conceivable find frequent itemsets describe distinct clusters 
need method recover clusters clusters cover majority data set 
devise new method recover clusters fp tree 
algorithm describes method detail algorithm cluster recovery count back method nodes root node processed current node unprocessed children nodes traverse level number points node label node potential cluster number points node parent nodes mark node processed remove node tree pruning threshold percentage data set determines node tree received support 
support labelled cluster node 
threshold directly determines number data points covered clusters classification data points 
large expect lot nodes tree pruned resulting set clusters cover data set want 
similarly small lot nodes pruned away marked potential clusters 
varied data set purpose evaluation shall describe section 
algorithm processed entire fp tree determine clusters straightforward method 
traverse tree root node find nodes marked cluster node 
cluster represented set frequent baskets path root cluster node 
find nodes traverse back fp tree report cluster 
subsequent branches cluster node ignored part reported cluster 
simple method determine cluster node compute threshold node tree 
threshold function frequent basket carries depth node tree 
number points node exceeds threshold flagged cluster node 
guarantee number points resulting clusters cover 
count back method control coverage clusters data set threshold 
count back method better alternative restriction 
rationale count back method want discriminate heavily supported nodes high tree 
children nodes may receive support classified cluster node want sure eventual clusters cover points data set 
continuing demonstration fp tree constructed data set set equal data points 
iteration algorithm marks nodes cluster nodes frequency count exceeds required percentage 
nodes threshold removed 
cluster nodes marked number points inside cluster nodes parents 
resulting fp tree shown 
fp tree iteration count back method nodes left tree threshold pruned away 
reach terminating condition loop 
frequent baskets algorithm determines range clusters returns set clusters root duplicate clusters removal obtain set clusters possible set orthogonal clusters extended dimensions clusters 
method recovery clusters eliminated reported result structure fp tree 
identical branches possible different levels fp tree 
exceed threshold algorithm reports branches cluster 
eliminate sub clusters reported clusters extended dimensions clusters algorithm describes steps detail 
algorithm sub cluster elimination sort set clusters order number dimensions length set length set cluster sub cluster cluster remove cluster set clusters duplicate clusters 
shall illustrate part algorithm simple example 
fp tree shown 
corresponding set clusters 
elimination algorithm sort clusters obtain set 
starting cluster examine subsequent clusters 
cluster sub cluster remove set 
leaves clusters set algorithm terminates 
root example fp tree complexity analysis time complexity algorithm dependent stages execution discovery frequent baskets transformation data points sets frequent baskets building mining clusters fp tree elimination sub clusters 
find frequent baskets requires passes data set 
pass determines range dimensions second pass determines histogram count fine bins dimensions 
transform data points frequent baskets requires pass data set 
time complexity stage total number data points data set 
analyse complexity second stage definitions 
denote number frequent baskets nd denote total number dimensions feature space fb denote total number dimensions contain frequent baskets 
words dimensions feature space frequent baskets dimensions fb nd fb 
define average number frequent baskets contributing dimen sion 
shall consider fb average case worst case 
assume level tree contains frequent baskets dimensions feature space nd fb nd 
computational complexity operations performed tree dependent size tree 
assumptions nth level tree contains nodes 
nd levels tree total number nodes nd 
worst case analysis different average case 
assumption longer valid consider special case dimension contains frequent basket nd 
allowable combinations frequent baskets fp tree populated nodes branches fit combinations 
shall consider frequent basket ranked lowest 
set frequent baskets total number possible combi nation total number possible combinations sum combinations set frequent baskets zero number frequent baskets 
number nodes carry similarly counting possible number com second lowest ranked frequent basket know number nodes carry frequent basket extending arguments frequent baskets precisely compute size tree equal special case nd size equal nd result implies size tree exponential number dimensions data set 
consider general case 
number nodes carrying increases constant factor number possible combinations increases 
size tree increases size complexity nd constant 
operations involve fp tree computational complexity nd 
time complexity third stage nc denote number clusters discovered 
nc usually small compared number frequent baskets 
worst case complexity nd 
complexity suggests algorithm scale increase dimensions empirical results demonstrate parameter values practice quite small sum finite geometric series ri rn 
time complexity clique nk ck denote maximum dimensionality cluster 
algorithms susceptible high dimensional clusters 
need passes data number candidate dense units generated grows exponentially 
comparatively algorithm able handle clusters high dimensions gracefully extra passes data candidate dense units generated 
evaluation description darpa data set data testing kdd cup data mining competition data set kdd 
originated darpa intrusion detection evaluation program managed mit lincoln labs 
lincoln labs simulated military lan multiple attacks week period 
training data set consists weeks traffic connections testing data consists weeks traffic connections 
contains new types attack training data 
entire data set generated collecting binary tcp dump converting series connection records 
record consisted features various types class label normal attack types 
types attack include denial service unauthorized access probing attacks 
extracted features included basic features individual tcp connection content features connection suggested domain knowledge traffic features computed second time window 
experimental setup experiment data kdd cup data set 
supervised learning competition data set contains high percentage attack traffic training purposes 
need filter attacks fit assumption 
filtering done types attack original training data set equally represented possible training set 
contains records 
testing data set similar problem perform filtering 
close examination types dos attacks constitute just testing data sets 
smurf neptune attacks easily detectable means ryan generate large volumes traffic 
removing attacks testing set prevents result heavily biased enhances meaning 
result testing data set contains records 
parameter values algorithm shown table 
training phase completed obtain set clusters 
assumption points fall outside boundary clusters classified anomalies 
assumption labelled test data manner 
test data point falls outside boundary clusters labelled anomaly 
parameter value table various values parameters results pruning threshold determines resulting set clusters algorithm produced 
evaluate performance varying threshold obtaining different unique sets clusters 
clusters obtained corresponding false positive rate detection rate 
results gathered plot roc graph 
detection rate false positive rate roc curve comparison obtained set similar roc graphs eskin 
curves produced data sets different clustering outlier mining techniques 
eskin stolfo evaluated nearest outlier mining algorithm nn fixed width clustering algorithm cluster support vector machine svm 
evaluated modified clustering tv algorithm modified cluster tv 
detection rate modified cluster tv cluster nn svm false positive rate roc curves stolfo area denote area receiver operator characteristic roc curve accurately compare approaches calculated performance value area roc curves 
show area roc graphs sets figures table 
conditions roc graphs produced slightly different comparison meaningful 
algorithm area roc nn fixed width clustering svm modified clustering tv table comparison performance worth mentioning experimental setup results obtained studies stolfo 
eskin 
comparisons results table meaningful 
stolfo filter attack training set fit assumption failed describe filtering done 
primarily training data testing training algorithm 
mentioned section testing data contains new type attacks training data 
algorithms seen attacks testing phase algorithm seen attacks 
testing experiments roughly 
algorithm added advantage hand crafted feature weighting improve detection rates dos attacks 
discussion results indicate able achieve high detection rate suffered high false positive rate compared methods 
table showed performance value 
approximately worse algorithms 
analysis reveals reports cluster various values 
nature cluster remains number dimensions cluster varying different values 
correlation values 
interpreting cluster human readable form reveals majority connections share exactly values set features 
set includes large number content features number su root commands attempted number failed logins number root shells obtained basic features number wrong fragments number urgent packets number connections host port moderate number derived features percentage connections syn errors percentage connections different service 
result suggests set features important classifying nature tcp connections 
shown connection normal meets conditions described cluster 
providing data set accurate representative real world simple effective intrusion detection scheme designed particular focus features 
lack multiple clusters possible reasons inaccuracy comparison algorithms tested eskin 
studies fixed width clustering algorithm described section unsupervised anomaly detection algorithm 
compare density grid clustering algorithm fixed width clustering algorithm added advantage cluster point data set 
anomalies clustered set small clusters identification straightforward 
hand density clustering algorithm inherent inability cluster points lay sparse region data space 
anomalies small percentage data set occupants sparse regions assumption ignored 
results suggested identification precise nature anomalies important factor performance unsupervised anomaly detection algorithm network intrusion detection 
obtained roc graphs different conditions compared experiments set eskin results compare slightly 
empirical run time statistics consider algorithm reasonably promising subject research high dimensional datasets 
areas project suggest topics acquired values parameters adaptive grid algorithm 
choice parameters may affect accuracy final results 
investigate possible effects changing values independently detail 
argue pruning threshold determines number points resulting set clusters cover 
appear case data set 
correlation values obvious proportionality relationship 
examine exact effects varying number points clusters cover 
improve current directions 
possible direction substituting mining frequent itemsets mining maximally frequent itemsets 
clusters highest dimensions possible 
may contradict aim maximum coverage may able modifications algorithm achieve aims 
evaluated new approach unsupervised anomaly detection application network intrusion detection 
new approach density grid high dimensional clustering algorithm large data sets 
advantage produce clusters arbitrary shapes cover data set appropriate values parameters 
provided detailed complexity analysis showed scales linearly number records data set 
evaluated accuracy new approach showed achieves reasonable detection rate maintaining low positive rate 
results provided insights requirements unsupervised anomaly detection scheme application network intrusion detection systems 
organisers kdd cup datasets 
supported part australian research council 
ryan 
efficient filter service bandwidth attacks 
proceedings ieee globecom pp 

agrawal gehrke gunopulos raghavan 
automatic subspace clustering high dimensional data data mining applications 
proceedings sigmod pp 

agrawal srikant 
fast algorithms mining association rules 
proceedings th international conference large data bases vldb pp 

ankerst breunig kriegel 
sander 
optics ordering points identify clustering structure 
sigmod rec 

denning 
intrusion detection model 
ieee transactions software engineering 
eskin 
anomaly noisy data learned probability distributions 
proceedings international conference machine learning icml 
eskin arnold stolfo 
geometric framework unsupervised anomaly detection detecting intrusions unlabeled data 
applications data mining computer security 
ester kriegel sander xu 
density algorithm discovering clusters large spatial databases noise 
proceedings kdd pp 

gonzalez dasgupta 
neuro immune self organizing map approaches anomaly detection comparison 
proceedings st international conference artificial immune systems pp 

gonzalez dasgupta 
anomaly detection real valued negative selection 
genetic programming evolvable machines vol 
pp 

han pei yin 
mining frequent patterns candidate generation 
proceedings acm sigmod intl 
conference management data acm press pp 

hipp 
algorithms association rule mining general survey comparison 
sigkdd explorations 
javitz 
nides statistical component description justification 
technical report sri international 
kdd third international knowledge discovery data mining tools competition dataset kdd cup 
kdd ics uci edu databases kddcup html 
lane brodley 
sequence matching learning anomaly detection computer security 
proceedings aaai workshop ai approaches fraud detection risk management aaai press pp 

schafer 
intrusion detection unlabeled data support vector machines 
detection intrusions malware vulnerability assessment dortmund 
kumar srivastava 
comparative study anomaly detection schemes network intrusion detection 
proceedings third siam international conference data mining 
lee stolfo 
data mining approaches intrusion detection 
proceedings th usenix security symposium security 
choudhary 
scalable parallel subspace clustering algorithm massive data sets 
th international conference parallel processing 
noble cook 
graph anomaly detection 
proceedings kdd 

adaptive clustering network intrusion detection 
proceedings third international pacific asia conference knowledge discovery data mining pakdd 
eskin stolfo 
intrusion detection unlabeled data clustering 
proceedings acm css workshop data mining applied security 
chatterjee zhang 
multi resolution clustering approach large spatial databases 
proceedings th international conference large data bases vldb pp 

chen chang 
novel anomaly detection scheme principal component classifier 
proceedings ieee foundations new directions data mining workshop 
wang yang muntz 
sting statistical information grid approach spatial data mining 
proceedings rd international conference large data bases morgan kaufmann pp 


unsupervised learning techniques intrusion detection system 
proceedings acm symposium applied computing acm sac 
