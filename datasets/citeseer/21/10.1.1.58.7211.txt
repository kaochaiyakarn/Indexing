design crawler bounded bandwidth michelangelo diligenti dipartimento di ingegneria dell informazione universit di siena italy dii presents algorithm bound bandwidth web crawler 
crawler collects statistics transfer rate server predict expected bandwidth downloads 
prediction allows activate optimal number threads order exploit assigned bandwidth 
experimental results show effectiveness proposed technique 
categories subject descriptors information systems information storage retrieval general terms design experimentation performance keywords parallel web crawlers bandwidth optimization 
design efficient crawlers key issue design web search engines 
crawlers large search engines exploit dedicated network connections departmental search engines usually share network bandwidth services 
reason bandwidth control key point crawler design 
design bandwidth control policy achieve contrasting goals 
crawler hinder users share connection second exploit completely assigned network resources 
efficient crawling strategies studied importance documents topic pages 
focused crawlers aim maximizing quality retrieved resources consider instant bandwidth 
bandwidth control achieved optimizing number parallel downloads 
propose policy control bandwidth crawler particularly suited small focused search engines 
crawler collects statistical data transfer rates server decide optimal number parallel downloads 

algorithm bandwidth control assume url assigned score measures copyright held author owner 
www may new york new york usa 
acm 
marco dipartimento di ingegneria dell informazione universit di siena italy ing franco dipartimento di ingegneria dell informazione universit di siena italy franco dii maria dipartimento di ingegneria dell informazione universit di siena italy estimated importance related document 
crawler selects url download popping priority queue sorted score 
case focused crawlers score may represent probability document desired topic path leading relevant pages 
design crawler composed set manager 
module run separate thread pool threads created startup 
waits manager activates retrieving url 
performs request appropriate web server having transferred document returns wait state 
manager implements crawler control policy feeding appropriately 
manager selects urls retrieved priorities activates optimal number exploit assigned bandwidth 
order select url manager estimates bandwidth expected consume retrieving document download completed manager measure actual transfer rate value update estimate server speed 
manager compute expected total transfer rate crawler cb set documents currently assigned 
assuming crawler activated optimal number completes current download uc slot bandwidth available start downloads 
crawler bandwidth updated cb cb uc manager searches url queue document ud estimated bandwidth remain assigned threshold downloading cb ud search starts top queue continues maximum depth maxd order consider documents higher priority 
search successful manager activates download ud stops waits completion download 
expected bandwidth new download consume completely available slot queue searched candidates 
algorithm activate 

set 
get document ud depth url queue byte measured bandwidth continuous line predicted bandwidth dotted line seconds time window 

cb ud assign ud inactive update cb cb ud 
set 
repeat step cb maxd manager periodically checks queue avoid starvation fastest servers servers transfer rate exceeds threshold 
statistics server transfer rates stored lookup table indexed server ip address 
transfer rate measured dividing total number transferred bytes time interval issue request till completion document download 
measure considers network delivery latency server response speed 
measured transfer rate depends characteristics state network links connecting machine running crawler server speed current load server machine 
network state server load vary time decided model dependency assuming hourly distribution estimate bandwidth ip hour working days holidays 
yields total values server 
experimentally assumption guarantees sufficient accuracy estimates 
decided discrete distribution reduce algorithm complexity 
values ip updated download measured bandwidth ip 
measure obtained time minutes day estimate updated ip ip ip ke design parameters chosen accordingly current date 
formula derived assuming bandwidth measured time affects estimates parameters gaussian function 
approach bandwidth prediction feasible small departmental focused search engines repeatedly contact limited set servers 

experimental results order evaluate proposed algorithm decided force crawler download urls contained predefined queue 
approach allows compare results queue tested settings 
experiment bandwidth limit set kb actual bandwidth measured second 
parameters update estimates maximum search depth maxd set 
shows measured bandwidth maxd continuous line maxd dotted line 
byte measured bandwidth crawler simple control number 
predicted bandwidth dotted line close actual bandwidth continuous line assigned threshold 
choice maximum search depth maxd affects actual available bandwidth higher search depth find download fits available slot 
compares runs crawler maxd dotted line maxd continuous line 
experiment proves larger maxd improves average bandwidth maxd versus maxd 
maxd increases urls smaller score downloaded urls score 
compared method proposed 
case bandwidth controlled stopping actual bandwidth limit starting new limit 
shows control accurate bandwidth oscillates considerably 

technique control bandwidth crawler 
approach particularly useful small focused search engines experiments preliminary showed effectiveness 

arasu cho molina raghavan 
searching web 
acm transactions internet technologies 
cho garcia molina page 
efficient crawling url ordering 
proceedings th international conference world wide web 
diligenti coetzee lawrence giles gori 
focused crawling context graph 
proceedings th conf 
large data base 
najork 
breath search crawling yields high quality pages 
proceedings th international conference world wide web 
rennie mccallum 
reinforcement learning spider web efficiently 
international conference machine learning 
shkapenyuk suel 
design implementation high performance distributed web crawler 
proceedings th international conference data 
