cycle cutset sampling bayesian networks rina dechter information computer science university california irvine irvine ca usa ics uci edu 
presents new sampling methodology bayesian networks called cutset sampling samples subset variables applies exact inference 
show approach implemented eciently sampled variables constitute cycle cutset bayesian network exponential induced width network graph sampled variables removed 
cutset sampling instance known rao technique variance reduction investigated 
proposed scheme extends standard sampling methods non ergodic networks ergodic subspaces 
empirical results con rm expectations show cycle cutset sampling superior gibbs sampling variety benchmarks yielding simple powerful sampling scheme 
sampling methods bayesian networks commonly approximation techniques applied successfully exact inference possible due prohibitive time memory demands 
focus gibbs sampling member markov chain monte carlo sampling methods group bayesian networks 
bayesian network variables fx xn evidence gibbs sampling generates set samples fx je sample fx instantiation variables network 
known function estimated generated samples je number samples 
samples estimate converges exact value 
central query interest bayesian networks computing posterior marginals je value variable query equation reduces counting fraction occurrences supported part nsf iis muri onr award 
samples 
signi cant limitation existing sampling schemes including gibbs sampler increase statistical variance highdimensional spaces 
addition standard sampling methods fail converge target distribution network ergodic 
sampling scheme bayesian networks addresses limitations sampling subset variables 
rooted established rao methodology sampling developed past years various authors notably 
rao blackwell theorem easy show sampling subspace feasible computationally reduce variance yield faster convergence target function 
basic rao blackwellisation scheme described follows 
suppose partition space variables subsets shown eciently compute cje jc summing cases perform sampling generating approximate quantity interest je jc function posterior marginal node je je jc jc equation instantiates je jc propose scheme subspace conditioning yields sparse bayesian network exact inference polynomial cycle cutset 
proposed scheme called cutset sampling 
yields special application rao blackwellisation sampling bayesian networks ers fold bene ts regular sampling improved convergence convergence non ergodic networks 
show empirically cycle cutset sampling converges faster terms number samples dictated theory time wise cost ective benchmarks tried cpcs networks random networks coding networks 
demonstrate applicability scheme non ergodic networks hail nder network coding networks 
approach propose simple best knowledge general bayesian networks special case dynamic bayesian networks 
authors apply particle ltering iterates timeline selecting speci sampling set current extends general bayesian networks 
background section presents cutset sampling analyzes complexity section provides empirical evaluation section concludes section 
background fig 

bayesian network left moral graph center conditioned polytree right conditioned fx 
de nition belief networks 
fx xn set random variables multi valued domains xn 
belief network bn pair directed acyclic graph fp jpa ji ng set conditional probability matrices associated belief network ergodic assignment fx xn non zero probability de ned xn jx pa 
evidence instantiated subset variables moral graph belief network obtained connecting parents child eliminating arrows 
shows belief network left moral graph center 
de nition induced width 
width node ordered undirected graph number node neighbors precede ordering 
width ordering denoted width nodes 
induced width ordered graph width ordered graph obtained processing nodes rst 
node processed preceding neighbors connected 
resulting graph called induced graph triangulated graph 
de nition induced width cycle cutset 
cycle path points coincide 
cycle cutset undirected graph set vertices contains node cycle graph singly connected called polytree underlying undirected graph cycles 
called multiply connected 
loop subgraph underlying graph cycle 
vertex sink respect loop edges adjacent directed vertex sink respect loop called allowed vertex respect cycle cutset directed graph set vertices contains allowed vertex respect loop gibbs sampling gibbs sampling generates samples je converges je number samples increases long network ergodic 
bayesian network gibbs sampling generates set samples denotes sample value sample sample fx evidence variables remain xed new sample generated assigning new value variable order 
value computed sampling conditional probability distribution jx markov assignment sample markov blanket variable includes parents children parents children 
samples generated answer query samples 
particular computing posterior marginal belief je variable estimated counting samples je equals averaging conditional marginals known mixture estimator je method converge faster simple counting 
markov blanket explicitly jx pa ch jx pa generating complete new sample requires 
multiplication steps maximum family size number variables 
subsequently computing posterior marginals linear number samples 
augmentation schemes variable augmentation schemes exist allow improve convergence properties simple gibbs sampler 
main approaches blocking grouping variables sampling simultaneously rao blackwellisation integrating random variables 
bayesian network random variables schematically describe sampling schemes follows 
rao blackwellised sample xjy sample yjx integrating random variable 
blocking gibbs samples values xjy zjx 
standard gibbs samples values xjy yjx zjx shown blocking gibbs sampling scheme variables grouped sampled simultaneously expected converge faster standard gibbs sampler 
variations scheme investigated 
blocking gibbs sampler sample instantiation variables network standard gibbs sampler 
sampling scheme allows integrate random variables reducing sampling space expected converge fastest 
basic data augmentation scheme blocking rao blackwellisation generally preferred 
caveat utilization rao blackwellised sampling scheme computation probabilities xjy yjx ecient 
case bayesian networks task integrating variables equates performing exact inference network evidence nodes sampling nodes observed time complexity exponential network size 
taken priori performance sampler severely impacted variables integrated rao blackwellisation applied special cases bayesian networks 
particular applied particle filtering importance sampling method dynamic bayesian networks cases variables integrated easily conditionally independent sampled variables plus evidence probability distribution permits tractable exact inference example kalman lter 
de ne general scheme rao blackwellised sampling bayesian networks see section show rao blackwellisation done eciently sampling set cycle cutset bayesian network 
demonstrate empirically networks compute new sample faster cutset sampling scheme standard gibbs sampler 
gain easily explained 
bayesian network size jx gibbs sampler able compute individual probabilities fast repeat computation times 
rao blackwellised scheme variables integrated sampling set size jcj may take longer compute xjc repeat computation times potentially smaller 
importantly fewer samples needed convergence 
cutset sampling section presents cutset sampling method 
noted basic scheme partitions variables subsets eciently compute cje jc sample values eciently approximate quantity interest equation 
cutset sampling algorithm 
cutset sampling algorithm 
subset cutset variables fc cm generates samples subspace instantiation variables similarly gibbs sampling generate new sample sampling value probability distribution jc denote conciseness 
key idea relevant conditional distributions eq 
computed exact inference algorithms complexity tied network structure improved conditioning 
jtc generic name class variable elimination join tree clustering algorithms compute exact posterior beliefs variable evidence 
known complexity jtc time space exponential network moral graph evidence variables removed 
cutset sampling input belief network cutset fc cmg evidence output set samples tc 
initialize assign random value assign 
generate samples generate new sample compute new value variable follows 
algorithm join tree clustering jtc compute jc 
sample new value 
fig 

cutset sampling algorithm computing posterior marginals 
samples available compute posterior beliefs variables follows 
cutset variable excluding evidence variables posterior marginals computed gibbs sampling je jc record distributions computed sample generation equation quantities readily available summation 
non cutset variable sample jc computed bayesian network conditioned jtc je jc note probability distribution jc computed soon sample generated 
sucient keep running sum eq 
relative samples value variable provide proof convergence general scheme section 
computing je cutset sampling guaranteed converge exact quantities 
general cutset sampling requires fewer samples converge full sampling result rao blackwell theorem 
example 
consider belief network shown 
sampling set fx better cutset fx compute sample probabilities jx jx 
probabilities computed belief propagation singly connected network right bucket elimination linear time 
new value variables updated messages singly connected network 
desired joint computed variable normalized yield conditional distribution 
complexity cutset sampling uses adjusted induced width control size sampling set adjust trade sampling inference 
undirected graph subset removed induced width resulting graph equal called cutset adjusted induced width relative cycle cutset graph cutset 
clearly computing new sample cutset sampling complex step gibbs sampling 
ecient cutset cycle cutset bayesian network 
case jtc reduces belief propagation algorithm compute joint probability linear time normalize relative yielding equation details omitted 
cutset complexity jtc equation exponential dominate complexity generating sample 
theorem complexity sample generation 
complexity generating sample cutset sampling cutset 


size bounds variables domain size number nodes 
corollary 
cycle cutset complexity generating sample cycle cutset sampling linear size network 
computing je equation requires computing jc variable 
complexity computation jtc exponential adjusted induced width relative cutset theorem 
cutset complexity computing posterior variables cutset sampling samples 


corollary 
cycle cutset complexity computing posterior variables cycle cutset sampling linear size network 
sampling cycle cutset sampling estimating marginal posterior linear size network number samples 
convergence cutset sampling 
section show je je de ned equations converge correct probabilities je je respectively 
theorem cutset convergence 
network subset evidence variables cutset assuming je je computed equations cutset sample je je je je number samples 
result theorem implied rao blackwell theorem proof rst principles simple 
proof 
jcj jx computation je done exactly way gibbs sampling 
di erent ways prove convergence gibbs sampling repeat 
correct convergence gibbs sampling conclude je je number samples increases 
consider variable write posterior distribution variable follows je jc cje 
assume generated collection samples correct distribution cje 
number times occurs samples 
tuple cje substitute right hand side equation expression je je jc factoring get je jc clearly sum summing instantiations yielding jc jc replacing sum sum get je jc obtained expression assuming converges exact cje 
je converges je cutset sampling shown conclude je je 
ut experiments compared cycle cutset sampling full gibbs sampling cpcs networks random networks hail nder network coding networks 
generally interested accuracy achieve period time 
provide gures showing accuracy gibbs sampling function time 
comparison show accuracy iterative belief propagation algorithm ibp iterations 
ibp iterative message passing algorithm performs exact inference bayesian networks loops 
applied bayesian networks loops compute approximate posterior marginals 
advantage ibp approximate algorithm ecient 
requires linear space usually converges fast 
ibp shown perform practice considered best algorithm inference coding networks nding probable variable values equals decoding process 
bayesian network variables fx xn computed exact posterior marginals je bucket tree elimination computed mean square error mse approximate posterior marginals je approximation scheme mse jd je je averaged mse number instances tried 
networks coding networks evidence nodes selected random 
cutset selected evidence sampling nodes constitute cycle cutset network mga algorithm 
cpcs networks 
considered cpcs networks derived computer patient case simulation system 
largest network cpcs consisted nodes induced width 
evidence cycle cutset size 
results shown figures 
chart title speci es network name number nodes network size evidence set jej size cycle cutset sampling set jcj induced width network instance 
cpcs networks observed cutset sampling far better gibbs sampling 
case cpcs middle cpcs cpcs time sec mse gibbs ibp cutset cpcs time sec mse gibbs ibp cutset cpcs time min gibbs ibp cutset fig 

comparing cycle cutset sampling gibbs sampling ibp cpcs networks averaged instances 
mse function time 
cpcs time min mse gibbs ibp cutset fig 

comparing cycle cutset sampling gibbs sampling ibp cpcs network averaged instances 
mse function time 
bottom cpcs cutset sampling achieves greater accuracy ibp 
gibbs sampling converge cpcs due non ergodic properties network 
cutset sampling overcomes limitation cycle cutset selected ergodic subspace 
random networks 
generated set random networks bi valued nodes 
network contained total nodes 
rst nodes fx designated root nodes 
non root node assigned parents selected randomly list predecessors fx conditional probability table values jpa chosen randomly uniform distribution 
collected data instances top 
cutset sampling converged faster gibbs sampling 
layer networks 
generated set random layer networks bi valued nodes 
network contained root nodes rst layer total nodes 
non root node second layer assigned maximum parents selected random root nodes 
conditional probability table values jpa chosen randomly uniform distribution 
collected data instances middle 
types networks iterative belief propagation perform 
experiments show cutset sampling gibbs sampling ibp takes longer time converge ibp 
coding networks 
experimented coding networks nodes coding bits parity check bits 
results shown bottom 
networks cycle cutset size induced width 
parity check matrix randomized parity check bit parents 
computed mse coding bits averaged networks 
coding networks ergodic due deterministic parity random time sec gibbs ibp cutset layer time sec mse gibbs ibp cutset fig 

comparing cycle cutset sampling gibbs sampling ibp random networks top layer random networks middle coding networks bottom averaged instances 
mse function time 
check function 
result gibbs sampling converge 
time subspace code bits ergodic cutset sampling samples subset coding bits converges generates results comparable ibp 
practice ibp certainly preferable coding networks size cycle cutset grows linearly number code bits 
fig 

comparing cycle cutset sampling gibbs sampling hail nder network instance 
mse function time 
hail nder network 
hail nder non ergodic network deterministic relationships 
nodes cycle cutset size 
network easy solve exactly 
network compare behavior cutset sampling gibbs sampling non ergodic networks 
expected gibbs sampling fails cycle cutset sampling computes accurate marginals accuracy continues improve time 
summary empirical results demonstrate cycle cutset cost ective time wise superior gibbs sampling 
measured ratio mg mc number samples generated gibbs number samples generated cycle cutset sampling time period 
cpcs cpcs cpcs cpcs ratios correspondingly 
obtained random networks random layer networks 
cutset sampling algorithm takes time generate sample produced substantially better results due variance reduction 
cases cutset sampling compute samples faster gibbs sampler 
case improvement accuracy due large sample set variance reduction 
cutset sampling achieves better accuracy ibp cpcs random networks takes time achieve better accuracy 
layer networks coding networks cycle cutset sampling achieves ibp level accuracy quickly able substantially improve time 
related sampling scheme called cutset sampling bayesian networks samples subset variables network 
remaining nodes marginalised inference instance technique known 
showed theoretically empirically cutset sampling improves convergence rate due sampling lower dimensional space allows sampling non ergodic network ergodic subspace 
resulting scheme simple powerful extension sampling bayesian networks dominate regular sampling sampling method 
focused gibbs sampling sampling techniques better convergence characteristics implemented cutset sampling long permit exploit bayesian network structure similar manner 
previously sampling subset variables successfully applied particle sampling dynamic bayesian networks dbns 
authors demonstrated sampling subspace combined exact inference yields better approximation 
scheme ers elegant way extending combining inference sampling bayesian networks 
di erent combination sampling exact inference join trees described 
papers proposed sampling estimate probability distribution cluster compute messages sent neighboring clusters 
approximation scheme sampling performed locally cluster algorithm rely approximated messages received neighbors generating new samples 
authors attempt remedy problem iterative re nement 
cutset sampling algorithm encounter problems takes account global state network generating new sample 
cutset sampling seen approximation cycle cutset 
exact inference combination blocking gibbs sampling 
major di erences cutset sampling approach proposed rst proposed blocking gibbs sampling sample consists variables network usual cutset sampling assigns values variables integrated second exact inference perform joint sampling step group variables cutset sampling uses exact inference integrate variables 
direction investigate methods nding sampling set convergence properties 
factors strongly ect convergence mcmc methods sampling set size complexity sample generation correlations variables 
reducing sampling set size generally leads reduction sampling variance due rao blackwellisation results increased complexity exact inference generating new sample 
factor strong correlations sampled variables deterministic probabilities non ergodic networks extreme example strong correlation 
variables strongly dependent preferred integrate group sample jointly blocking gibbs sampler see 
tak ing consideration sampling set de ned minimal cutset small strongly correlated variables removed 

becker bar yehuda geiger 
random algorithms loop cutset problem 
uncertainty ai uai 

casella robert 
rao blackwellisation sampling schemes 
biometrika 

dechter 
bucket elimination unifying framework reasoning 
arti cial intelligence 

doucet de freitas murphy russell 
rao blackwellised particle ltering dynamic bayesian networks 
uncertainty ai pages 

gelfand smith 
sampling approaches calculating marginal densities 
journal american statistical association 

geman geman 
stochastic relaxations gibbs distributions bayesian restoration images 
ieee transaction pattern analysis machine intelligence pami pages 

gilks richardson spiegelhalter 
markov chain monte carlo practice 
chapman hall 

de groot 
probability statistics nd edition 
addison wesley 

rish dechter 
empirical evaluation approximation algorithms probabilistic decoding 
uncertainty ai uai 

jensen kong blocking gibbs sampling large probabilistic expert systems 
international journal human computer studies 
special issue real world applications uncertain reasoning pages 

jensen lauritzen olesen 
bayesian updating causal probabilistic networks local computation 
computational statistics quarterly 

weiss murphy jordan 
loopy belief propagation approximate inference empirical study 
uncertainty ai uai 

kj hugs combining exact inference gibbs sampling junction trees 
uncertainty ai pages 
morgan kaufmann 

koller lerner 
general algorithm approximate inference application hybrid bayes nets 
uncertainty ai pages 

lauritzen spiegelhalter 
local computation probabilities graphical structures application expert systems 
journal royal statistical society series 

wong liu kong 
covariance structure gibbs sampler applications comparison estimators augmentation schemes 
biometrika pages 

mackay 
monte carlo methods 
proceedings nato advanced study institute learning graphical models 
sept oct pages 

pearl 
probabilistic reasoning intelligent systems 
morgan kaufmann 

peot shachter 
fusion multiple observations belief networks 
arti cial intelligence pages 
