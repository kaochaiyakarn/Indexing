multitask learning knowledge source inductive bias richard caruana school computer science carnegie mellon university pittsburgh pa caruana cs cmu edu suggests may easier learn hard tasks time learn tasks separately 
effect information provided training signal task serves domain specific inductive bias tasks 
clusters related tasks learn 
straightforward create additional tasks 
domains acquiring inductive bias collecting additional teaching signal may practical traditional approach codifying domain specific biases acquired human expertise 
call approach mtl 
power inductive learner follows directly inductive bias multitask learning may yield powerful learning 
empirical example multitask connectionist learning learning improves training network related tasks time 
multitask decision tree induction outlined 
inductive bias causes inductive learner prefer hypotheses hypotheses 
learning impossible fact power inductive learner follows directly power inductive bias mitchell 
different sources inductive bias appear powerful enable machine learning develop inductive systems scale complex real world tasks 
standard methodology machine learning break large problems small reasonably independent subproblems learn subproblems separately recombine learned pieces waibel 
suggests appear proceedings tenth international conference machine learning part methodology may wrong 
reductionist method caused attempt simple isolated tasks attempting learning larger richer tasks 
adhering method may ignoring critical source inductive bias real world problems inductive bias inherent similarity related tasks drawn domain 
inductive learner related tasks time tasks valuable sources inductive bias 
may learning faster accurate may allow hard tasks learned learnable isolation 
call approach multitask learning mtl 
introduce multitask learning level explain related tasks sources mutual inductive bias 
concrete example mtl applied artificial neural networks 
demonstration discuss multitask connectionist learning detail 
briefly describe multitask learning 
discuss related notably giving hints networks transferring knowledge related tasks learned time 
summarize mtl methodology highlight research problems tackled mtl succeed complex real world tasks 
multitask learning inductive bias learn embedded world simultaneously tasks learn related things 
things obey physical laws derive human culture preprocessed sensory hardware similarity tasks learn enables learn little experience 
child trained birth single isolated complex task learn 
trained birth play tennis probably learn tennis 
learn play tennis world tasks learn things 
learn walk run jump exercise grasp throw swing recognize objects predict trajectories rest talk study practice tasks running tennis different running track related 
similarities thousands tasks learn enable learn including tennis 
artificial neural network decision tree trained tabula rasa single isolated complex task learn 
example neural network pixel input retina learn recognize complex objects real world number training patterns amount training time afford 
may better embed inductive learner environment simultaneously tasks learn related tasks 
tasks share computed subfeatures learnable subprocesses inductive learner may find easier learn learn isolation 
simultaneously train neural network recognize objects object outlines shapes edges regions subregions textures reflections highlights shadows text orientation size distance may better able learn recognize complex objects real world 
multitask learning mtl 
mtl predicated notion tasks serve mutual sources inductive bias 
inductive bias causes inductive learner prefer hypotheses 
mtl particular kind inductive bias 
mtl uses information contained training signal related tasks bias learner hypotheses benefit multiple tasks 
usually think training data bias training data contains teaching signal task easy see point view task tasks may serve bias 
multitask bias exist inductive learner biased typically simplicity bias prefer hypotheses utility multiple tasks 
categorized strength explicitness domain specificity 
strength multitask bias depends tasks related strongly inductive learner biased learned structure tasks 
multitask bias explicit information provided teaching signal explicit 
implicit simplicity bias depends inductive systems implicitly specified 
multitask bias domain specific tasks drawn domain 
knowledge inductive bias training signal tasks knowledge rich 
point view task information contained training signal tasks provide additional domain knowledge learner received working task time 
bias free learning impossible power inductive learner follows directly inductive bias mitchell 
inductive systems rely weak biases exist weak biases broad applicability easy new problems syntactic simplicity 
biases usually domain specific 
unfortunately domain specific inductive biases costly typically requiring domain theory manually encoded human expert 
mtl may provide effective source domain specific knowledge rich inductive bias requires effort domain expert 
example multitask connectionist learning simplest way introduce multitask connectionist learning example 
consider boolean functions defined bit inputs task parity task parity task parity task parity bi represents ith bit leftmost bit parity parity number bits set mod bits example input pattern task parity 
similarly task 
tasks related ways 
tasks defined inputs binary bits 
second task defined common computed parity bits 
third inputs task compute parity bits task need compute parity vice versa 
task parity task independent value parity 
task task related similarly task needs parity task backpropagation rumelhart train artificial neural networks learn tasks 
shows networks typically 
tasks independent separate network task 
single task learning stl 
possible shown single network learn tasks time 
multitask learning mtl 
graphs show generalization performance single multitask networks trained tasks 
graph contains generalization performance curves task trained isolation task trained paired task mtl tasks task tasks trained mtl tasks 
simplify presentation task multitask data curves tasks trained tasks trained 
runs fully connected feed forward networks input units hidden units output units 
networks fewer hidden units adequate 
task task task task inputs inputs inputs inputs connectionist single task learning stl functions defined inputs inputs 
task task task task connectionist multitask learning mtl related functions defined inputs tasks 
networks large minimize capacity effects different numbers tasks trained network 
experiments run suggest generalization performance tasks injured excess capacity 
networks trained backpropagation mitre aspirin learning rate momentum pattern updating 
training sets contain patterns selected randomly possible patterns 
remaining patterns cross validation hold set measure generalization performance 
training set kept small generalization challenging prevent training set including possible examples parity subfunction insure hold set large yield reliable generalization measures 
plot average runs 
carefully examine graphs 
graph shows average generalization performance training task 
curves represent performance task trained trained task trained tasks 
poorest performance occurs task trained 
single task neural network learning 
better generalization performance task achieved single network trained tasks time 
better generalization performance achieved training tasks single network 
similar results tasks task better performance achieved task trained network trained simultaneously tasks 
note better generalization performance obtained need additional training passes 
fact mtl run computationally efficient stl runs replaces 
check results graphically analysis variance anova peak generalization performances training runs performed 
analysis confirms mtl statistically outperforms stl level 
task learned better trained context network learning related tasks time 
run experiments verify improved generalization performance mtl result multitask inductive bias 
specifically done experiments rule effects depend tasks related 
effect adding noise neural net learning improves generalization performance holmstrom 
extent tasks uncorrelated contribution aggregate gradient gradient sums error fed back layer outputs appear noise tasks 
tasks improve generalization performance acting source noise 
test effect trained nets tasks random additional tasks 
example trained output net task different random tasks 
random task held constant training random tasks true functions 
pattern presentations backprop passes generalization performance task task tasks tasks pattern presentations backprop passes generalization performance task task tasks tasks pattern presentations backprop passes generalization performance task task tasks tasks pattern presentations backprop passes generalization performance task task tasks tasks generalization performance single task learning multitask learning tasks second effect tested possibility adding tasks merely changes weight updating dynamics favor nets tasks 
test trained nets replicated tasks time 
example single net trained copies task outputs received exactly training signal 
degenerate form mtl task correlates perfectly serve source knowledge inductive bias 
shows generalization performance task multitask trained random functions 
performance task trained task tasks shown comparison 
performance task trained networks simultaneously trained random functions similar performance task trained 
tasks random functions performance drops little 
conclude mtl effect due multiple tasks serving beneficial source noise 
rule possibility net capacity artificially limit performance learning random functions verified networks hidden units able train percent accuracy task random tasks 
shows performance nets trained copies task 
shows performance output selected random 
surprisingly performance better task trained isolation 
improvement gained additional information pattern presentations backprop passes generalization performance task task tasks tasks task random copies task performance mtl variants task system 
think explanations extra copies may increase effective learning rate weights input layer hidden layer backpropagated error signals add copies may provide better signal noise ratio early training error signals backpropagated initially random weights times random weights backpropagating output layer hidden layer having weights connected signal increases odds weights close value 
determine mechanism accounts observed performance increase 
fortunately replication effect large explain mtl performance increase observed tasks assume able confer magnitude advantage tasks different tasks 
final comment necessary 
mtl bias effect observe large interesting 
answers 
performance increased number related tasks increased 
domains surprisingly easy add tasks 
accumulated benefit mtl significant domains 
second parity hard function learn hard rarely generalization studies 
studies parity try learn parity training complete set patterns 
fact see improved generalization tasks compute parity internally trained explicit information parity function remarkable 
third networks told tasks share common computable 
tasks differ significantly 
represents challenging case mtl relationship tasks deeply buried 
measurable improvement occur 
multitask connectionist learning detail advantages disadvantages advantages disadvantages net learn tasks compared separate nets task 
examining possible disadvantages 
mtl net probably need additional capacity learn tasks 

mtl net doing potentially competing jobs time gradients computed error signal task may interfere cause aggregate gradient flatter directional learning slower mtl tasks 

mtl net may different tasks different times 
overtraining prevented halting training cross validation generalization performance begins drop may place training mtl net achieve peak generalization tasks 

mtl require new tasks defined training signal new tasks acquired 
places additional burden domain expert 

learning parameters learning rate momentum initial weight ranges set differently different tasks may setting appropriate tasks 
advantages mtl stem possibility tasks related information teaching signal task aid learning tasks 
benefit mutual inductive bias strong disadvantages described mitigated 
tasks share weights mtl net bigger net required individual tasks smaller individual nets combined 

mtl aggregate gradient may flatter may susceptible noise 
may point better directions early search gradients point directions beneficial tasks generally useful subfeatures 

tasks different times really problem 
mtl requires tasks trained context tasks learning produce single net tasks 
mtl produce separate nets task representing point generalization performance peaked task 
anova section 

devising new tasks acquiring training signal easy easier alternative acquiring codifying domainspecific inductive biases acquired human expertise 
conjecture mtl provides convenient pipe add domain specific inductive bias 

tasks require tweaking learning parameters may impossible train tasks time 
may limit mtl domains 
mtl improve generalization reasons may generalize better 
mtl provide data amplification effect allow tasks eavesdrop patterns discovered tasks bias network representations overlooked tasks learned separately 
examine closely 
data amplification effect best explained tasks section 
task need compute parity half patterns task needs parity patterns task vice versa 
teaching signal task provides information parity half time teaching signal task provides information parity half time 
net trained tasks receives information parity half time net trained tasks gets information parity training pattern 
nets see exactly training patterns multitask net getting information training pattern 
increase effective sample size computed 
task tasks providing inductive bias preference hypotheses accurately compute com mon subfeatures 
effect explain mtl performance observed tasks performance increases tasks trained tasks tasks require parity patterns 
eavesdropping common benefit mtl 
tasks related computed subfeatures needed tasks useful tasks 
subfeatures may learnable tasks eavesdrop trained separately prove valuable sources information learned tasks 
best mtl bias nets learn internal representations learned trained tasks isolation 
consider set tasks training patterns 
assume task benefit computing function inputs 
assume task needs training patterns task needs different pattern 
nets trained tasks time learn receive insufficient information recognize exists 
net trained tasks learn mtl may allow nets discover internal representations stl nets 
mtl may bias nets different internal representations representation benefits tasks 
multiple tasks come tasks devised related carefully chosen ways 
additional tasks mtl requires come real world problems 
real world gives related tasks learn time 
example calendar apprentice system dent inductive learning predict location time day day week duration meetings schedules 
tasks related functions data probably share common subfeatures 
calendar apprentice currently learns tasks independently 
learns tasks specified order feed output task input feature 
mtl approach induce single model tasks time 
mtl need define order task induction enable tasks feed information 
real world domains natural multitask domains avoids reductionism 
essential domain naturally multiple tasks mtl 
mtl means adding domain specific inductive bias inductive system additional teaching signal 
domains surprisingly easy devise additional related tasks 
semi modular pieces larger task tasking robot predict trajectory ball full task requires robot catch ball tasks appear require similar abilities task 
acquiring additional training signal trivial probably easier asking domain experts provide forms domain specific inductive bias domain experts excel knowledge usually find codifying knowledge difficult 
note mtl require training signals available run time 
training signals needed training outputs inputs mtl net 
free specify feature task useful operational procedure compute 
multitask decision trees mtl applied learning techniques techniques decision trees usually multiple tasks 
typically leaf decision tree provides single class assignment instances reach 
leaf nodes refer multiple classes single task assigning probabilities class separate decision trees usually built different tasks 
need case 
create multitask decision trees containing nodes assign membership tasks decision tree trained 
single decision tree leaf node corresponds class task class task example clarify 
suppose wish learn related medical diagnosis tasks 
task determine patient retinal degeneracy task determine patient diabetes task determine patient reduced peripheral circulation task determine patient pulmonary disease 
tasks strongly related relationship convoluted 
diabetes causes retinal degeneration reduced peripheral circulation increases risk pulmonary disease 
retinal degeneration reduced peripheral flow common symptoms diabetes 
pulmonary disease linked diabetes usually long term consequence diabetes useful symptom 
conditions derive occur isolation 
web relating conditions tangled 
easily imagine larger web bring dozens related symptoms 
classic decision tree induction approach quinlan induce separate decision tree 
known diagnoses benefit diagnoses learn decision trees order outputs earlier trees input features subsequent trees 
awkward number tasks complex web relates 
mtl approach grow single decision tree predicts conditions time 
devise new greedy splitting rule multitask decision trees may efficiently grown traditional top fashion 
rule average information gain entropy decrease tasks 
favor attribute splits high decision tree provide discrimination tasks 
early search decision tree biased categorizations broad utility 
mtl networks bias cause learning develop clusters represent meaningful subgroups domain 
diabetes example mtl decision tree learn class brittle class useful tasks 
related common train neural nets outputs 
usually outputs encode single task 
example classification tasks common output class output highest activation classification 
net strongly related tasks new 
classic nettalk sejnowski rosenberg application uses net learn phonemes stresses 
multitask approach natural problems nettalk goal nettalk learn control synthesizer needs phoneme stress commands time input 
nettalk example multitask learning 
experiments currently underway see stl nettalk perform worse mtl nettalk 
transferring learned structure related tasks new 
main differences previous knowledge transfer emphasis learning tasks time realization additional tasks learning substantially easier 
differences significant 
transferring learned structure neural nets trained similar tasks pratt pratt sharkey sharkey demonstrates learned task bias task 
problem nets trained tasks sequentially 
operator devise appropriate training sequence 
needs means causing net retain learned enabling flexibility learn anew modify learned 
difficulties approach scale tasks 
opportunity system trains tasks time discover representations apparent tasks viewed 
similar providing hints networks abu mostafa suddarth suddarth holden 
hints approach uses additional tasks semi modular decompositions main task 
definitely mtl 
advances hints ways recognizing multiple tasks serve mutual sources inductive bias showing tasks related diverse ways hints yield beneficial bias exploring ways tasks bias suggesting creating new related tasks may efficient way providing domain specific inductive bias learning systems 
empirical results demonstrate contributions tasks easily viewed providing hints generalization performance increases trained 
mtl decision trees related conceptual clustering techniques notably cobweb fisher 
fact simple modification indices cobweb probabilistic information metric yields metric suitable judging multitask decision tree splits 
cobweb considers features tasks predict 
mtl decision trees allow user specify signals features training signal making easier create additional tasks committing extra training information available run time 
summary acquiring domain specific inductive bias subject knowledge 
suggest domainspecific inductive bias easily acquired domains acquiring teaching signal additional tasks domain 
describe mechanisms teaching signal serve inductive bias combined bias prefers hypotheses benefit tasks 
introduce multitask learning mtl connectionism established methodology training artificial neural networks multiple outputs 
fact classic nettalk application previous supplying hints neural networks instances mtl connectionism 
empirical demonstration shows similarity multiple tasks difficult learn recognize mtl improve generalization performance 
demonstrate improvement easily attributable effects mutual inductive bias similarity tasks 
show generality mtl methodology briefly outline top induction multitask decision trees 
significant decision trees usually learn multiple tasks 
doing results system generalizes conceptual clustering techniques useful domains distinction features information available classes things want predict retained 
precisely conditions required mtl succeed known 
sufficient give multiple tasks inductive systems allow existing simplicity biases exploit relatedness 
devise new ways help inductive systems recognize common substructure coerce share representations substructure 
remains done 
need effective ways open complex networks see inside 
particular need devise methods detecting eavesdropping occurring new modified internal representation developed 
need better understand tasks related mtl succeed recognize traits real world tasks 
need empirically demonstrate mtl provide strong inductive bias useful complex tasks 
need evaluate hard build useful additional tasks domains interest vision 
conjecture complexity domain scales creating additional related tasks 
true mtl may provide practical method acquiring domain specific inductive bias scales naturally complexity task 
wray buntine chrisman jeff jackson tom mitchell herb simon sebastian thrun help refining ideas 
special reviewers chrisman rich goodwin tom mitchell diane careful review drafts 
research sponsored part avionics lab wright research development center aeronautical systems division afsc air force afb oh contract arpa order 
views contained document authors interpreted representing official policies expressed implied government 
abu mostafa 
learning hints neural networks 
journal complexity 
dent mcdermott mitchell zabowski 

personal learning apprentice 
proceedings national conference artificial intelligence 
fisher conceptual clustering learning examples inference 
proceedings th international workshop machine learning 
holmstrom 
back propagation training 
ieee transactions neural networks 

mitchell 
need biases learning generalizations 
rutgers university cbm tr 
quinlan 
induction decision trees 
machine learning 

pratt mostow kamm 
direct transfer learned information neural networks 
proceedings aaai 
pratt 
non literal transfer neural network learners 
colorado school mines mcs 
rumelhart hinton williams 
learning representations back propagating errors 
nature 

sejnowski rosenberg 
nettalk parallel network learns read aloud 
john hopkins jhu eecs 
sharkey sharkey 
adaptive generalisation transfer knowledge 
university exeter 
suddarth holden 
symbolic neural systems hints developing complex systems 
international journal max machine studies 
suddarth 
rule injection hints means improving network performance learning time 
proceedings eurasip workshop neural networks 

waibel shikano 
modularity scaling large phonemic neural networks 
ieee transactions acoustics speech signal processing 

