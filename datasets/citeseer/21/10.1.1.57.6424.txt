comparison spectral clustering algorithms deepak verma department cse university washington seattle wa deepak cs washington edu marina department statistics university washington seattle wa mmp stat washington edu spectral clustering quite popular years new algorithms published 
compare best known algorithms point view clustering quality arti cial real datasets 
implement variations existing spectral algorithms compare performance see features important 
demonstrate spectral methods show competitive performance real dataset respect existing methods 
clustering hard problem active topic research 
new approach started get lot attention spectral methods 
spectral methods clustering usually involve top eigen vectors matrix distance points properties cluster various points 
spectral clustering techniques seen explosive development proliferation past years 
promise strong competitors clustering methods 
successes registered lsa :10.1.1.160.2324
spectral methods attractive easy implement reasonably fast sparse data sets thousands 
intrinsically su er problem local optima 
depending exact algorithm local optima 
spite large number papers spectral clustering far systematic comparison existing algorithms published 
set 
intend take look spectral algorithms take apart 
generate list algorithms di erent parts di erent algorithms compare performance 
hope able nd sub components important see combination works best 
spectral clustering algorithms algorithms selected date popular published ones 
aimed representing diverse set algorithmic features 
algorithms image segmentation algorithm introduced shi malik sm variant kannan vempala kvv algorithm ng jordan weiss algorithm suggested shi multicut :10.1.1.160.2324:10.1.1.25.1824:10.1.1.25.1824
results obtained single ward linkage algorithms denoted mst ward strawman order demonstrate clustering tasks chose trivial 
describing algorithms detail introduce notation 
describe spectral grouping algorithms 
notation set data points clustered denoted ji pair points similarity ij ji 
similarities ij viewed weights undirected edges ij graph matrix ij plays role real valued adjacency matrix ij called degree node volume set ol set edges disjoint sets called edge cut short cut clustering fc ck partitioning nonempty mutually disjoint subsets ck graph theoretical paradigm clustering represents multiway cut graph algorithms just need similarity matrix points anchor 
original unmapped points 
need data similarity matrix may actual set distinct points initial domain points may come nite dimensional space 
output kernel function 
overview algorithms thought consisting stages preprocessing form normalization similarity matrix smoothing initially sure matrix ill conditioned 
results relevant papers eventually dropped smoothing 
spectral mapping eigenvectors preprocessed similarity matrix computed 
data point mapped tuple representing values component aforementioned eigenvectors 
postprocessing grouping usually simple grouping algorithm clusters data original spectral domain 
kind algorithms 
recursive spectral algorithms try split data partitions single eigenvector recursively generate partitions 
multiway spectral information multiple eigenvectors direct multiway partition data 
non spectral usually simple grouping algorithm clusters data quickly 
conjunction multiway spectral algorithms provide baseline performance 
algorithms sm kvv heuristic methods nding number clusters suggested :10.1.1.160.2324:10.1.1.160.2324:10.1.1.25.1824
provide fair comparison assumed algorithms number clusters advance 
shi malik sm algorithm algorithm introduced heuristic algorithm aimed minimize normalized cut criterion proposed authors :10.1.1.160.2324
normalized cut sets de ned ncut cut ol ol set partitioned clusters minimize ncut possible way partitions problem provably np hard authors show certain special conditions spectral algorithm exists nds optimum :10.1.1.160.2324
algorithm sm 
compute 
eigenvalues corresponding eigenvectors compute 
min ratio cut sort elements increasing order 
denote th element sorted list 
compute ncut ig fi ng partition clusters min ncut 
repeat steps recursively cluster largest clusters obtained 
min ratio cut inspired suggests line search vector produces better cuts original heuristic 
algorithm identical spectral algorithm ii :10.1.1.25.1824:10.1.1.25.1824
discussed detail 
version algorithm implemented conductance criterion min ratio cut kannan vempala algorithm kvv kvv similar sm algorithm di 
di erence step optimal cut respect cheeger conductance measure cut quality 
conductance clustering fc cg de ned cut min ol ol recursive step kvv variant decides cluster minimum conductance 
variant step called min conductance di erence normalization past rst iteration step 
sm algorithm just takes block corresponding current cluster 
variant uses blocks computed rst iteration :10.1.1.25.1824:10.1.1.25.1824
ensure row sums blocks equal diagonal elements ii current block adjusted 
variant ignores self similarity data points rst iteration 
adjust ii possibilities 
scale entries row sum add extra weight diagonal element 
call rst variant kvv mult second kvv add 
ng jordan weiss algorithm algorithm 
set diagonal elements ii 
compute matrix sd 
largest eigenvalues corresponding eigenvectors eigenvectors normalized unit length 
form matrix stacking eigenvectors columns 

form matrix renormalizing rows unit length ij ij ij 

means row point dimensions cluster means algorithm obtain nal clustering 
means algorithm initialized method described 
eigenvalues distinct pick eigenvectors dv possible matlab implementation automatically 
eigenvalues distinct choose orthogonal 
related laplacian see details :10.1.1.160.2324
proved clusters separated sense similarity matrix block diagonal sizes clusters degrees individual nodes don vary rows matrix cluster near orthonormal vectors fact suggested orthogonal initialization method 
implemented slightly di erent version algorithm think greater numerical stability 
details appendix 
meila shi algorithm algorithm suggested 
algorithm multicut 
compute stochastic matrix 

compute eigenvectors corresponding largest eigenvalues 
form matrix columns 
cluster rows points dimensional space 
anchor algorithm version anchor algorithm related minimum diameter clustering method 
algorithm anchor 
choose point random 
set 
choose anchor farthest point initial point 

construct cluster associated list points closer anchor 
list sorted decreasing distance 
test points 
jc nmin 
set 
choose anchor farthest point existing anchors 

go step 
note algorithm may produce clusters 
possible produces clusters having nmin points 
experiments performed nmin observed 
linkage algorithms hierarchical clustering algorithm distances points 
similarities need choose way mapping similarities distances 
chose inverse similarity measure small value added similarity ensure inverse zero taken 
methods single linkage ward linkage 
single linkage performing mst dissimilarities graph points ward linkage similar distance inner square distance 
details ward algorithms see 
shown sections di erent re closer initially appear 
experiments theory suggest di erences algorithms depend strongly quality postprocessing step 
experimented di erent clustering methods described 
theoretical results section theoretical results concerning various spectral algorithms main result near equivalence non recursive spectral methods multicut modi cation algorithms numerically stable conditions proof similarity 
modi cation multicut method discussed section top eigenvectors sd map data 
propose top eigen vectors generalized eigen system sx dx 
believe numerically stable method compute eigen vectors division involved contain small values 
outlier example small degree close point self similarity de ned zero 
prove gives vector consider sd premultiplying get putting get sv sv dv consider top eigen vectors un vn 
meaning previous section 
recall rows unit vectors dimensions 
diagonal matrix means th row th row scaled ii row normalized length obtained identical obtained multicut method observe generalized eigen value system sv dv mathematically equivalent numerically stable computing eigenvectors 
preliminaries perfect clusters postprocessing step spectral algorithm reduced distinct point say perfect respective algorithm 
example block diagonal similarity matrix perfect algorithms 
perfect represents ideal situation clustering algorithm 
essentially show resulting block stochastic term de ned perfect multicut give performance recursive algorithms 
de nition stochastic matrix 
say block stochastic clustering 
fc ck ck ij value points de nition vector piecewise constant pc clustering cluster 
proposition matrix orthonormal columns atmost unique rows exactly unique orthogonal rows 
proof rank independent rows means exactly unique rows 
rearrange rows identical rows 
ect result assume property 
written cn kb matrix unique rows ij th row th row just need prove orthogonal rows 
columns orthonormal implies cb easy see diagonal matrix say 
de ne gives means orthonormal orthogonal rows 
proves orthogonal rows 
premultiplying matrix diagonal matrix just scales rows 
qed 
consider algorithms multicut top eigen vectors orthonormal 
eigenvectors piecewise constant clustering 
atmost unique rows need exactly unique rows perfect 
need eigenvectors pc 
note rows orthogonal 
key result 
proposition stochastic matrix rows columns indexed independent eigenvectors 

fc ck partition pce 
corresponding non zero eigenvalues sums ik ck ij constant block stochastic matrix kk kk ij non singular easy see produces block stochastic perfect multicut algorithm 
case algorithm characterized section 
equivalence results section show kind equivalence multicut characterizes ideal case 
assume obtained symmetric non negative elements meaning section mention clustering assumed stand clustering 
fc ck recall rows unit vectors dimensions 
proposition top eigen vectors pc clustering 
rows grouped groups identical vectors yk groups distinct orthonormal 
proof proposition de ne th row th row results section jx just scaled unit sphere 
pce 
consider arbitrary arbitrarily chosen pce 

implies orthogonal distinct 
lie unit sphere orthonormal 
proposition rows equal points cluster pce 
proof reverse direction proposition somewhat tricky prove 
assume rows equal 
consider arbitrary arbitrarily chosen assumption 
implies jx jy jx jx jy jx particular rst column rst column proportional jx jx jx jx rst column rst eigenvector stochastic matrix 
vm particular implies jx jx 
rows implies pc pce 
gives theorem theorem similarity matrix block stochastic perfect multicut algorithms 
points cluster original domain mapped single point spectral domain points corresponding di erent clusters orthogonal 
words perfect algorithm perfect multicut algorithm 
experiments arti cial data focus ideal case studied theoretically robustness noise real data note spite similarity di spectral domain scale vectors important 
implement separate versions multicut theoretical comparison previous section proved block stochastic perfect multicut recursive spectral algorithms 
take case splitting potentially subset original points clusters 
top level recursive step 
points block stochastic 
case second eigen vector piece wise constant 
reorder points eigen vector points right order 
chose right point partition partitions clusters 
split 
split cluster block stochastic parts 
setting optimal stage recursive sub steps 
criterion algorithms choose ensure optimal point optimally chosen job 
fact block diagonal algorithms choose right position ncut conductance minimized zero positions 
possibility choose point maximizes gap vector 
intend explore 
datasets discussed lot spectral methods little variation needs compared 
possible visually compare needed datasets pre classi ed possible compute clustering error vi true clustering 
see section arti cial real datasets described 
arti cial datasets primarily demonstrate robustness algorithms noise 
block stochastic matrix ideal case multicut ang spectral algorithms 
constructed matrix consists clusters 
clusters size respectively 
similarity matrix slightly block diagonal 
purpose dataset demonstrate stability algorithms noise 
data le called block diagonal hard 
handwritten digits set optical handwritten digit recognition available nist site lots version available di erent preprocessing 
particular data set preprocessing mentioned description preprocessing done preprocessing programs available nist extract normalized bitmaps handwritten digits form 
total people contributed training set di erent test set 
bitmaps divided non overlapping blocks number pixels counted block 
generates input matrix element integer range 
reduces dimensionality gives invariance small distortions 
sampled dataset elements digit giving total dimensional points clusters 
call dataset digit 
digits easy distinguish particular digits lot easier distinguish 
created dataset containing instances just digits 
called data set 
just remind digit database 
gene expression data dna microarrays provide way biologists study variation genes 
plethora gene expression data generated community 
led great need data analyzed 

dataset yeast cell cycle data publically available 
shows uctuation gene expression levels genes time points 
dataset restricted genes expression level peak di erent points corresponding phases cell cycle 
objective expression levels able cluster clusters corresponding phases 
kinds pre processing suggested 
take logarithm expression level second standardize mean zero variance 
data transformations done data better gaussian model mixture models cluster data 
see details 
call rst dataset second std 
evaluating clustering performance measuring clustering performance general hard problem 
notion clustering intrinsically tied de nition cluster big research topic 
case measuring clustering performance easier datasets true clustering available 
clustering performance just measure di erent clustering produced true clustering 
kind measures clustering error variation information section true represent true clustering clustering produced clustering algorithm 
clustering error clustering error de ned number misclassi cation error induced clustering true clustering 
confusion confusion matrix clusterings 
confusion true jc true true number points cluster true true clustering cluster clustering produced 
ce true true true confusion true total number points 
subtle problem naive de nition 
take account renumbering happen clustering 
cluster true clustering assigned cluster clustering produced 
counter ce computed possible renumbering clustering produced minimum taken 
computed eciently modeling problem maximum weighted bipartite matching problem computing solution linear programming 
variation information variation information vi metric introduced compare clusterings 
measures amount information lost gained going clustering vice versa 
de ne introduce notation 
changes 
number point th cluster nk entropy associated clustering de ned log de ne true jc true true true nk true mutual information clustering true de ned true true true log true true true variation information true true true see various properties measure 
experimental setup section provide speci details experiments run 
section group vectors xn similarity matrix ij exp jjx jj parameter specify value 
refer input clustering algorithms specify number cluster generate 
algorithm run multiple number times average taken 
exact algorithms section described algorithms called classic version 
exactly distinguish ects various components spectral algorithms implemented range algorithms containing variations algorithms mentioned 
list algorithms implemented shown 
algorithm listed various components represent application particular algorithm stage 
ang mcut refer spectral mapping multicut methods domain 
mapping similarity matrix required obtained computing choice straight forward case algorithm points lie unit sphere 
value multicut 
anchor ward kmeans refer respective algorithms applied spectral mappings 
kmeans performed kmeans runs initializing orthogonal centers runs initialized random centers 
intuition spectral methods ective clustering points mapping spectral domain 
explore possibility implemented double spectral methods ang mcut ward rst map points ang spectral domain points mcut spectral domain nally grouping ward method 
linkage algorithms cluster single linkage cluster ward linkage multiway spectral algorithms ang ward ang kmeans ang anchor mcut ward mcut kmeans mcut anchor ang ang ward ang ang kmeans ang mcut ward ang mcut kmeans mcut mcut ward mcut mcut kmeans recursive spectral algorithms shi ncut kvv mult ncut kvv add ncut shi cond kvv mult cond kvv add cond table list algorithms database primarily compare robustness algorithms noise 
took original block stochastic matrix added uniform noise increasing magnitude steps exponent 
noise added preserve signal noise ratio sense noise added ij proportional degrees points precisely new ij calculated follows ij ij number chosen random 
noise levels matrices generated average performance algorithm taken 
handwritten digits dataset consisted vector dimensional space ranging 
similarity matrix computed anity matrix 
experimented various sigma value give reasonable results 
dataset digit ran algorithm iterations ranging 
iterations executed 
gene expression data datasets std similarity computed correlation coecients gene expression levels di erent genes 
plus similarity matrix positive 
similarities ranged 
runs executed varying 
implementation algorithms simple implemented able implement lines code matlab 
majority time taken eigen decomposition 
full eigen decomposition eig function matlab take time 
just needed top eigen vectors function reduce time taken 
performance graphs section graphs various algorithms datasets 
algorithms show graph 
datasets graphs 
metrics clustering error ce variation information vi shown 
rst column various versions multiway spectral algorithms 
second column recursive spectral algorithms third columns best 
best chosen follows pick best algorithm linkage recursive multiway spectral classes algorithms 
best remaining 
best method picked looking individual graphs cases lot methods similar performance just chose looked better arbitrarily hard decide 
way see various classes spectral methods compare 
note axis graphs 
di erent graphs compared just looking heights levels 
done show better contrast particular class esp performance class near identical pages contain graphs landscape mode 
ang kmeans mcut ward mcut kmeans ang ang ward ang mcut ward mcut mcut ward mcut mcut kmeans kvv mult ncut kvv add ncut kvv mult cond cluster ward linkage mcut kmeans mcut mcut ward ce spectral methods ce recursive methods ce best block diagonal hard metric vi ang kmeans ang anchor mcut kmeans ang ang ward ang ang kmeans ang mcut kmeans mcut mcut kmeans block diagonal hard metric vi shi ncut kvv add ncut shi cond kvv add cond block diagonal hard metric vi mcut ward mcut mcut ward vi spectral methods vi recursive methods vi best block diagonal hard dataset 
axis log noise added 
ang kmeans mcut ward mcut kmeans ang ang ward ang mcut ward ang mcut kmeans mcut mcut kmeans kvv mult ncut kvv add ncut kvv mult cond cluster ward linkage ang anchor mcut kmeans mcut mcut ward ce spectral methods ce recursive methods ce best digit metric vi ang kmeans ang anchor mcut kmeans mcut anchor ang ang kmeans ang mcut ward mcut mcut ward shi ncut kvv add ncut kvv add cond cluster ward linkage shi ncut ang anchor vi spectral methods vi recursive methods vi best handwritten digits digit ang ward ang anchor mcut kmeans mcut anchor ang ang kmeans ang mcut kmeans mcut mcut ward shi ncut kvv add ncut kvv mult cond kvv add cond cluster ward linkage ang ward ang ang kmeans ce spectral methods ce recursive methods ce best metric vi ang ward ang kmeans mcut ward mcut anchor ang ang ward ang mcut ward mcut mcut ward mcut mcut kmeans metric vi shi ncut kvv mult ncut shi cond kvv add cond metric vi shi ncut mcut mcut kmeans vi spectral methods vi recursive methods vi best handwritten digits digits ang ward ang anchor mcut kmeans ang ang kmeans ang mcut kmeans mcut mcut kmeans kvv mult ncut kvv add ncut kvv mult cond cluster ward linkage kvv mult ncut ang ang ward best model ce spectral methods ce recursive methods ce best ang ward ang anchor mcut ward mcut anchor ang mcut ward mcut mcut ward shi ncut kvv add ncut shi cond kvv add cond shi ncut ang ang ward ang mcut kmeans vi spectral methods vi recursive methods vi best log normalized yeast cell cycle data std metric ce ang kmeans mcut ward mcut anchor ang ang ward ang mcut ward mcut mcut ward shi ncut kvv mult ncut shi cond cluster ward linkage ang ang kmeans best model ce spectral methods ce recursive methods ce best std metric vi ang ward mcut ward mcut anchor ang mcut ward mcut mcut ward mcut mcut kmeans std metric vi shi ncut shi cond kvv add cond std metric vi mcut anchor kvv mult ncut kvv mult cond vi spectral methods vi recursive methods vi best standardized yeast cell cycle data std results discussion purpose dataset demonstrate robustness noise 
dataset error bars shown rst column methods performed nearly similar error bars 
omitted graphs clear 
see gure linkage algorithm sensitive noise infact nd correct clustering noise added block stochastic matrix 
multiway spectral methods expected perform best perfect class algorithms perform nearly mcut methods performing slightly better ang methods 
suggests block stochastic similarity matrices slightly preferable multicut base algorithms 
reason clusters unit sphere blow distances points cluster 
experimental proof conclusive 
recursive algorithms shi ncut gets perfect clusters case low noise variants perform 
interesting note conductance performs signi cantly worse 
expected conductance takes smaller cluster size account ncut cluster sizes 
handwritten digits rst real dataset tested algorithm 
complete dataset digit gure multiway spectral methods perform slightly better recursive spectral 
performance di erence signi cant case vi measure zero conclusive statement 
linkage algorithms performed lot worse 
multiway spectral algorithms mcut mcut ward clear winner 
results dataset interesting 
performance near perfect comparison done light dataset established structure 
take look gure easy see multiway spectral algorithm give nearly identical results 
strong empirical justi cation similarity multicut theoretically proved 
particular clusters obviously formed result sections independent grouping algorithm third stage 
dataset multiway spectral methods dominate recursive methods 
linkage methods expected far 
surprising thing observed ncut methods lagging performance compared conductance 
gene expression data dataset interesting real datasets 
variety reasons 
results model algorithms dataset compare spectral algorithms just 
secondly contained dataset di erent data transformation applied see section see clustering algorithms dependent preprocessing 
dataset best spectral algorithms perform slightly better model algorithms 
encouraging shows spectral methods competitive real dataset just perfect case 
recursive algorithm show similar performance multiway algorithms ncut algorithms little better conductance little worse 
ordering recursive algorithms expected clear better multiway algorithms 
possible presence noise depending eigenvectors best thing better process recursively ensures atleast rst partitions correct 
surprising performance cluster ward linkage 
simple linkage algorithm gives nearly best performance measures 
think case high error rates observing really anybody game dominant structure known data corresponds clustering algorithm 
comparison situation std completely reversed 
model algorithm performs best 
reason data transformation known better gaussian model better performance 
performance best spectral methods remains multiway methods perform better recursive ones 
conductance methods just slightly worse better 
address problem go choosing number clusters 
intend explore methods nd number clusters data 
algorithms implement lack time 
rst variant sm algorithm theoretically perform block stochastic matrix 
think sensitive noise 
second non spectral method single linkage runt analysis expect lot robust noise 
wish explore algorithm grouping algorithm spectral mapping ects performance various methods 
goal analyze comparatively features number published spectral clustering algorithms 
establishing published algorithms better aimed evaluating features spectral clustering valuable 
clustering data set goodness eye look clustering algorithms competing complementing strengths weaknesses 
second goal research see di erent various algorithms approach 
answer second question largely negative 
theory predicts perfect algorithms result strongly supported experiments 
algorithms cases perfect clear winner case 
nd multiway spectral clustering algorithm slightly better performing especially structure easily data 
recursive methods recommend ncut measure clear winner 
compared method showed spectral methods give competitive performance existing methods de nitely worth exploration 
am thankful marina meila collaborator project immense contribution possible advisor supportive thoughtful 
special thomas richardson providing support deepak verma nsf dms 
ka yee yeung providing yeast cell cycle dataset classi cations 
nally jayant madhavan provided feedback earlier version 
sanjoy dasgupta 
performance guarantees hierarchical clustering 
proceedings th annual conference computational learning theory colt pages 
springer 
ravi kannan santosh vempala adrian :10.1.1.25.1824:10.1.1.25.1824
clusterings bad spectral 
focs pages 

methods combining multiple classi ers applications handwritten digit recognition 
master thesis institute graduate studies science engineering university 
marina meila 
comparing clusterings 
technical report uw statistics department 
marina meila jianbo shi 
learning segmentation random walks 
nips pages 
marina meila jianbo shi 
random walks view spectral segmentation 
andrew moore 
anchors hierarchy triangle inequality survive high dimensional data 
proceedings twelfth conference uncertainty arti cial intelligence pages 
aaai press 
ng jordan weiss 
spectral clustering analysis algorithm 
dietterich becker ghahramani editors advances neural information processing systems pages cambridge ma 
mit press 
jianbo shi jitendra malik :10.1.1.160.2324
normalized cuts image segmentation 
ieee transactions pattern analysis machine intelligence 
daniel spielman shang hua teng 
spectral partitioning works planar graphs nite element meshes 
ieee symposium foundations computer science pages 
ward 
hierarchical grouping optimize objective function 
amer 
statist 
assoc pages 
yeung fraley raftery ruzzo 
model clustering data transformations gene expression data 
technical report uw cse dept computer science engineering university washington 
ka yee yeung 
model clustering data transformations gene expression data 
sta washington edu model 

