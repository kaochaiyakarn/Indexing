fields experts framework learning image priors stefan roth michael black department computer science brown university providence ri usa roth black cs brown edu develop framework learning generic expressive image priors capture statistics natural scenes variety machine vision tasks 
approach extends traditional markov random field mrf models learning potential functions extended pixel neighborhoods 
field potentials modeled products experts framework exploits nonlinear functions linear filter responses 
contrast previous mrf approaches parameters including linear filters learned training data 
demonstrate capabilities field experts model example applications image denoising image inpainting implemented simple approximate inference scheme 
model trained generic image database tuned specific application obtain results compete outperform specialized techniques 

need prior models image structure occurs machine vision problems including stereo optical flow denoising super resolution image rendering name 
noise uncertainty prior models images depth maps flow fields come play 
develop method learning rich markov random field mrf image priors exploiting ideas sparse image coding 
resulting field experts foe models prior probability image terms random field overlapping cliques potentials represented product experts 
show model trained standard database natural images develop diffusion scheme exploits prior approximate bayesian inference 
demonstrate modeling power foe model different applications image denoising image inpainting 
despite generic nature prior simplicity approximate inference obtain 
image reconstruction field experts 
example image additive gaussian noise psnr db 
denoised image 
psnr db 
photograph scratches 
image inpainting foe model 
state art results possible mrf approaches 
illustrates application foe model image denoising image inpainting 
provide detailed quantitative analysis performance tasks respect state art 
modeling image priors challenging due images non gaussian statistics need model correlations image structure extended image neighborhoods 
observed wide variety linear filters marginal filter responses non gaussian responses different filters usually independent :10.1.1.3.5374
sparse coding approaches attempt address issues modeling complex image structure 
particular model structural properties images terms set linear filter responses 
starting variety simple assumptions numerous authors obtained sparse representations local image structure terms statistics filters local position orientation scale :10.1.1.134.6077
methods focus image patches provide direct way modeling statistics images 
markov random fields hand widely machine vision exhibit serious tions 
particular mrf priors typically exploit handcrafted clique potentials small neighborhood systems limit expressiveness models crudely capture statistics natural images 
typical models consider simple nearest neighbor relations model derivative filter responses 
sharp contrast rich patch priors obtained sparse coding methods extremely local order priors employed mrf methods 
zhu mumford took step practical mrfs frame model allowed mrf priors learned training data 
method relies hand selected set image filters image prior built 
approach complicated discrete filter histograms reported image reconstruction results appear fall current state art 
line modeled complex spatial properties multiple non local pairwise pixel interactions 
models far exploited texture synthesis modeling generic image priors 
model complex local statistics number authors turned empirical probabilistic models captured database image patches 
freeman propose mrf model uses example image patches measure consistency model scene structure 
idea exploited prior model image rendering related example texture synthesis 
mrf models parzen window approach define field potentials 
jojic miniature version image set images called describe image 
may possible method generic image prior possibility explored 
goal current develop framework learning rich generic prior models natural images class images 
contrast example approaches develop parametric representation uses examples training rely examples part representation 
parametric model advantages example methods generalizes better training data allows elegant computational techniques 
key idea extend markov random fields frame modeling local field potentials learned filters 
exploit ideas products experts poe framework 
previous efforts model images products experts patch inappropriate learning generic priors images arbitrary size 
extend methods yielding translation invariant prior 
experts framework provides principled way learn mrfs examples greatly improved modeling power practical complex tasks 

sparse coding product experts statistics small image patches received extensive treatment literature 
particular sparse coding methods represent image patch terms linear combination learned filters bases ji rn min ai jji ai rn vectorized image patches ai sparseness prior penalizes non zero coefficients ai variations formulation lead principal components independent components specialized filters 
independent component analysis ica define probabilistic model images patches 
components ica assumption independent simply multiply marginal distributions obtain prior model 
case image patches pixels generally impossible find fully independent linear components ica model approximation 
welling went limitation model products experts framework 
idea poe framework model highdimensional probability distributions product expert distributions expert works low dimensional subspace relatively easy model 
usually experts defined linear dimensional subspaces corresponding basis vectors sparse coding models 
notice projecting image patch linear component equivalent filtering patch linear filter described ji 
observation responses linear filters applied natural images typically exhibit highly marginal distributions resemble student distribution welling propose student experts 
full product distribution pot model written ji experts form normalizing partition function 
assumed positive needed proper distributions note experts assumed normalized 
convenient rewrite probability density gibbs form exp log 

selection filters obtained training products experts model generic image database 
important property model parameters automatically learned training data image filters ji 
advantage poe model ica model number experts necessarily equal number dimensions pixels 
poe model permits fewer experts dimensions complete equally complete experts dimensions complete 
overcomplete case particularly interesting allows dependencies filters modeled consequently expressive ica 
procedure training pot model described section context generalization foe model 
shows selection filters obtained training poe model image patches 
training data contains image patches randomly cropped berkeley segmentation benchmark converted grayscale 
filters learned model kinds gabor filters obtained non parametric ica technique standard sparse coding approaches 
possible train models times complete characteristics filters remain :10.1.1.134.6077
key characteristic methods focus modeling small image patches defining prior model entire image 
despite welling suggest algorithm denoising images arbitrary size 
resulting algorithm easily generalize image reconstruction problems 
effort gone extending sparse coding models full images 
inference model requires gibbs sampling somewhat attractive machine vision applications 

fields experts 
basic model model described preceding section provides elegant powerful way learning prior distributions small image patches results generalize immediately give prior model image 
reasons simply making patches bigger viable solution number parameters learn 
selection filters obtained training fields experts model generic image database 
large model specific image size generalize image sizes model translation invariant desirable property generic image priors 
key insight overcome problems combining ideas sparse coding markov random field models 
pixels image represented nodes graph edges connecting nodes 
define neighborhood system connects nodes rectangular region 
neighborhood centered node pixel defines maximal clique graph 
hammersley clifford theorem establishes write probability density graphical model gibbs distribution exp vk image vk potential function clique 
additional assumption mrf homogeneous potential function cliques terms vk 
property gives rise translation invariance mrf model loss generality assume maximal cliques mrf square pixel patches fixed size non square neighborhoods 
defining potential function hand learn training images 
enable represent mrf potentials product experts basic form 
formally energy term define potential function 
write probability density full image foe model exp equivalently log defined 
important difference respect poe model talk translation invariance disregard fact finite size image property hold approximately 
take product neighborhoods model overcomes problems cited number parameters determined size maximal cliques mrf number filters defining potential 
furthermore model applies images arbitrary size translation invariant homogeneity potential functions 
note computing partition function intractable 
inference algorithms ones proposed section require normalization term known 
distinguishes model explicitly models overlap image patches 
overlapping patches highly correlated learned filters ji parameters account correlation 
refer resulting translation invariant product experts model field experts emphasize probability density entire image involves combination overlapping local experts 

contrastive divergence learning parameters linear filters ji learned set training images maximizing likelihood 
maximizing likelihood poe foe model equivalent minimizing kullback leibler divergence model data distribution guarantees model distribution close data distribution possible 
closed form solution parameters perform gradient ascent loglikelihood 
leads parameters updated user defined learning rate denotes average training data expectation value respect model distribution 
average training data easy compute general closed form solution expectation model distribution 
computed approximately monte carlo integration repeatedly drawing samples mcmc sampling 
implementation hybrid monte carlo hmc sampler efficient standard sampling techniques metropolis sampling 
advantage hmc sampler stems fact uses gradient log density explore space effectively 
despite efficient mcmc sampling strategies training model way practical may take long time markov chain approximately converges 
running markov chain convergence idea contrastive divergence initialize sampler data points run small fixed number steps 
denote data distribution distribution mcmc iterations pj contrastive divergence parameter update written intuition running mcmc sampler just iterations starting data distribution draw samples closer target distribution estimate parameter updates 
hinton justifies formally shows contrastive divergence learning typically approximation maximum likelihood estimation parameters 

implementation details order correctly capture spatial dependencies neighboring cliques image patches size images training data set substantially larger clique size 
hand large images required mcmc sampling inefficient 
train model randomly cropped image regions times width height maximal cliques case cliques train images 
training data taken images berkeley segmentation database natural scenes people buildings 
welling noted poe model filter learning usually benefits whitening data distribution removes potential scaling issues due non spherical covariance image patches 
avoid similar problems model apply whitening transform clique pixels computing update filters 
transform furthermore ignores changes average gray level clique reduces number dimensions filters 
enforce positivity updating logarithm 
learning algorithm works constraint 
experiments contrastive divergence single step hmc sampling 
hmc step consisted leaps leap size adjusted automatically acceptance rate near 
performed update steps 
result sensitive exact value learning rate number contrastive divergence steps 
shows selection filters learned training foe model pixel cliques 
filters respond various edge texture features multiple orientations scales demonstrated capture important structural properties images 
appear psnr lena barbara boats house peppers table 
peak signal noise ratio psnr db images denoised foe prior 
lack clearly interpretable structure filters learned standard poe model cf 

may result filters having account correlated image structure overlapping patches 
training foe model computationally intensive occurs line 
see relatively efficient algorithms approximate inference foe model practical 

applications experiments computational methods exploiting mrf models image denoising applications 
methods include gibbs sampling deterministic annealing mean field methods belief propagation non linear diffusion related pde methods 
gibbs sampler formal convergence properties computationally intensive 
derive gradient ascent method approximate inference performs practice 

image denoising currently accurate denoising methods literature fall category wavelet coring image decomposed large set wavelets different orientations scales wavelet coefficients modified prior probability image reconstructed inverting wavelet transform 
excellent review quantitative evaluation state art see 
accurate methods model fact marginal statistics wavelet coefficients non gaussian neighboring coefficients space scale independent 
portilla model dependencies gaussian scale mixture derive bayesian decoding algorithm appears accurate class methods 
pre determined set filters hand select neighboring coefficients adjacent scales intuition empirical evidence suggest statistically dependent 
contrast schemes focus bayesian formulation spatial prior term 
observed image goal find true image maximizes posterior probability 
common denoising literature experiments assume true image corrupted additive gaussian noise zero mean known standard deviation 
write likelihood exp yj xj ranges pixels image 
method generalizes kinds noise distributions long noise distribution known logarithm differentiable 
maximizing posterior probability graphical model foe generally hard 
order emphasize practicality proposed model refrain expensive inference techniques 
perform gradient ascent logarithm posterior probability 
gradient log likelihood written log 
fortunately gradient log prior simple compute log ji ji denotes convolution image filter ji 
define log denote filter obtained mirroring ji center pixel 
note log standard robust error function heavy tails proportional influence function 
introducing iteration index update rate optional weight write gradient ascent denoising algorithm ji observed zhu mumford related non linear diffusion methods 
filters xand derivative filters equation similar standard non linear diffusion filtering data term 
denoising proceeds similar ways cases prior model uses filters nonlinear diffusion 
key advantage foe model tells build richer prior models combine filters larger neighborhoods principled way 

denoising results 
original noiseless image 
image additive gaussian noise psnr db 
denoised image field experts psnr db 
denoised image approach psnr db 
denoised image standard non linear diffusion psnr db 
denoising experiments foe model trained previous section berkeley database perform number denoising experiments 
experiments conducted assume known noise distribution 
extension exposition blind denoising example robust data terms automatic stopping criteria remain subject 
foe prior filters pixels 
chose update rate depending amount noise added performed iterations 
potentially speeding convergence large update rates may result numerical instabilities experimentally disappear 
running large step sizes subsequently cleaning image iterations shows worse results performing denoising 
experimentally best results obtained additional weight likelihood term furthermore depends amount noise added 
automatically learn optimal value noise level training data set train foe model 
done choosing best value small candidate set results obtained sets images 
set consists images commonly denoising experiments 
table provides peak signal noise ratio psnr log set various levels additive gaussian noise denoised foe model cf 

portilla report accurate results test images method tuned perform dataset 
obtain signal noise ratios close results db cases surpass results db 
best knowledge mrf approach far able closely compete wavelet methods dataset 
note prior trained tuned examples 
expectation larger filters better map estimation techniques improve results 
test varied realistic images denoised second test set consisting images test section berkeley data set 
various noise levels denoised images foe model method software default settings provided simple wiener filtering matlab standard non linear diffusion scheme data term 
method employed robust huber function viewed mrf model local derivative filters 
standard non linear diffusion scheme weight prior term trained foe case stopping time selected produce optimal denoising result terms psnr 
shows performance methods wiener filter test images 
visually quantitatively foe model outperforms wiener filtering non linear diffusion nearly matches performance specialized wavelet technique 
shows performance comparison mentioned denoising techniques images test set various noise levels 
addition psnr computed perceptually similarity measure ssim 
foe model consistently outperforms wiener filtering standard non linear diffusion closely matching performance current state art image denoising 
signed rank test shows performance differences foe methods statistically significant confidence level ssim non linear diffusion highest noise level 

image inpainting image inpainting goal remove certain parts image example scratches photograph 
denoising results berkeley database 
horizontal axis psnr db noisy images 
error bars correspond standard deviation 
left psnr db models left right wiener filter standard non linear diffusion foe model variants 
right similarity index techniques 
unwanted occluding objects disturbing visual appearance 
typically user supplies mask pixels inpainted 
past approaches form diffusion fill masked pixels 
suggests diffusion technique proposed denoising may suitable task 
contrast denoising modify subset pixels specified mask 
pixels observation likelihood term 
simple inpainting algorithm propagates information foe prior ji 
update scheme mask sets gradient zero pixels outside masked region 
contrast algorithms explicit local gradient direction local structure information comes responses learned filter bank 
filter bank denoising experiments 
levin similar motivation exploit learned models image statistics inpainting 
approach relies small number features train model image inpainted 
generic prior combine information automatically determined features 
shows result applying inpainting scheme text removal application mask corresponds pixels occluded text 
color image converted color model algorithm independently applied channels 
prior trained gray scale images obviously suboptimal gives results 
order speed convergence ran iterations 
large step size may lead numerical instabilities clean image applying iterations 
inpainted result similar original qualitatively superior 
quantitatively method improves psnr db db compared db image similarity metric shows significant improvement compared higher better 
advantage rich prior seen continuity edges better preserved compared 
shows detail regions comparing method center right 
similar qualitative differences seen parts reconstructed image 

summary markov random fields popular machine vision formal properties ability model complex natural scenes limited 
practical model rich image priors extended approaches sparse coding image patches model potentials homogeneous markov random field capturing local image statistics 
resulting fields experts model rich set learned filters trained generic image database contrastive divergence 
contrast previous approaches pre determined set filters parameters model including filters learned data 
resulting probabilistic model bayesian inference method requiring spatial image prior 
demonstrated usefulness foe model applications denoising inpainting 
denoising algorithm straightforward approximately lines matlab code achieves performance close best special purpose wavelet denoising algorithms 
advantage methods lies generality prior applicability different vision problems 
believe results represent important step forward utility mrf models widely applicable 
avenues 
making mrf models richer problems revisited expectation improved results 
current efforts focused learning prior models optical flow scene depth color images object boundaries 
results applicable image super resolution image sharpening graphics applications image rendering 
avenues foe model studied detail size cliques number filters influence quality prior 
furthermore interesting explore foe model fixed filters standard derivative filters random filters expert parameters learned data 
student expert distribution replaced suitable form 
convergence related prop 
inpainting field experts 
original image overlaid text 
inpainting result diffusion foe prior 
close comparison left middle results right 
erties diffusion algorithm propose inference studied 
acknowledgments andrews gat geman hoffman simoncelli welling wood helpful discussions sapiro making inpainting examples available comparison portilla making denoising sofware available 
supported intel research nsf itr nih ns part nsf nih collaborative research computational neuroscience program 
portions performed authors intel research 
es javier denoise index html software version 
bell sejnowski 
information maximization approach blind separation blind deconvolution 
neural comp 
sapiro caselles 
image inpainting 
acm siggraph pp 

black sapiro heeger 
robust anisotropic diffusion 
ieee trans 
image proc 
efros leung 
texture synthesis non parametric sampling 
iccv pp 

fitzgibbon wexler zisserman 
image rendering image priors 
iccv pp 

freeman pasztor 
learning lowlevel vision 
ijcv 
geman reynolds 
constrained restoration recovery discontinuities 
pami 
geman geman 
stochastic relaxation gibbs distributions bayesian restoration images 
pami 

texture modeling multiple pairwise pixel interactions 
pami 
hinton 
product experts 
icann pp 

hinton 
training products experts minimizing contrastive divergence 
neural comp 
huang mumford 
statistics natural images models 
cvpr pp 

jojic frey kannan 
analysis appearance shape 
iccv pp 

levin weiss 
learning global image statistics 
iccv pp 

martin fowlkes tal malik 
database human segmented natural images application evaluating segmentation algorithms measuring ecological statistics 
iccv pp 

neal 
probabilistic inference markov chain monte carlo methods 
technical report crg tr dept computer science university toronto 
olshausen field 
sparse coding overcomplete basis set strategy employed 
vision research 
paget longstaff 
texture synthesis noncausal nonparametric multiscale markov random field 
ieee trans 
image proc 
portilla wainwright simoncelli 
image denoising scale mixtures gaussians wavelet domain 
ieee trans 
image proc 
olshausen 
learning sparse multiscale image representations 
nips pp 

wang bovik sheikh simoncelli 
image quality assessment error visibility structural similarity 
ieee trans 
image proc 
weickert 
review nonlinear diffusion filtering 
scale space theory computer vision pp 

welling hinton osindero 
learning sparse topographic representations products student distributions 
nips pp 

van gool 
compact model viewpoint dependent texture synthesis 
smile lncs pp 

zhu mumford 
prior learning gibbs 
pami 
zhu wu mumford 
filters random fields maximum entropy frame unified theory texture modeling 
ijcv 

