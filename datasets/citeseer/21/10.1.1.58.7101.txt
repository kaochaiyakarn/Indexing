text classification training positive unlabeled examples fran ois denis cmi univ fr anne laurent cmi univ fr lif umr centre de math matiques informatique cmi universit de provence rue curie marseille cedex france mi gilleron gilleron univ lille fr marc univ lille fr grappa ea projet ur inria universit de lille domaine universitaire du pont de bois bp cedex france general framework semi supervised learning labeled unlabeled data consider specific problem learning pool positive data negative data help unlabeled data 
study naive bayes algorithm pnb positive unlabeled examples 
consider case number positive examples quite small assuming training setting relevant assuming datasets natural separation features sets 
design training algorithm positive unlabeled examples 
give experimental results algorithms pnb 
show text classification naive bayes feasible positive examples unlabeled examples training algorithms significantly improve learning accuracy available set positive data small 

tedious expensive hand label large amount training data 
semisupervised learning algorithms small set labeled data help unlabeled data defined 
approaches include expectation maximization estimate maximum posteriori parameters nigam transductive inference support vector machines joachims unlabeled data define metric kernel function hofmann partition set features disjoint sets features blum mitchell nigam ghani muslea 
consider problem learning positive data help unlabeled data 
instance text learning tasks document retrieval classification goal efficient classification retrieval interests user 
positive information readily available unlabeled data easily collected 
example learning classify web pages interesting specific user 
documents pointed user bookmarks define set positive examples correspond interesting web pages negative examples available 
unlabeled examples easily available world wide web 
theoretical results show order learn positive unlabeled data sufficient consider unlabeled data negative ones denis liu 
starting point liu constrained approximation approach 
idea select function correctly classifies positive documents minimizes number unlabeled documents classified positive 
idea define new learning algorithm em built naive bayes classifier conjunction em expectation maximization algorithm 
approach statistical query learning model estimate statistical queries positive unlabeled examples denis 
fol low idea define naive bayes classifier pnb 
pnb takes input positive unlabeled data estimate possibly rough positive class probability 
practical situations positive class probability empirically estimated provided domain knowledge 
compare performance em nb pnb public domain document datasets webkb course dataset reuters collection newsgroups dataset 
consider situations small set positive data available unlabeled data 
situations building accurate classifiers may fail poverty input data 
learning possible existence different views data assumed training framework introduced blum mitchell 
instance consider retrieval bibliographic 
positive examples stored user database 
view consists bibliographic fields title author editor 
second view full content 
unlabeled examples easily available bibliographic databases accessible world wide web 
training algorithms incrementally build basic classifiers feature sets 
training methods previously train classifiers applications text classification blum mitchell word sense disambiguation named entity classification collins singer 
training learning special case multi view learning semi supervised learning algorithms defined muslea 
define training algorithm seed information small pool positive documents 
incrementally builds naive bayes classifiers positive unlabeled documents views pnb 
training steps self labeled positive examples self labeled negative examples added training sets 
propose base algorithm variant pnb able self labeled examples 
experiments webkb course dataset performed show training algorithms lead significant improvement classifiers initial seed composed positive documents 
section naive bayes algorithm positive unlabeled data pnb 
section define training algorithm 
experimental results section 
naive bayes positive unlabeled documents 
pnb algorithm naive bayes algorithm positive unlabeled examples pnb introduced denis 
briefly main ideas pnb section 
consider binary text classification problems set classes corresponds positive class 
consider bag words representations documents 
set documents word 
denote total number word occurrences number occurrences documents pnb assumes underlying generative model 
model class selected class prior probabilities 
second document length chosen length prior probability 
word document generated drawing multinomial distribution words specific class 
algorithm pnb takes input estimate positive class probability set positive documents set ud unlabeled documents 
positive naive bayes classifier pnb classifies document consisting words 
wn possibly multiple occurrences word member class pnb argmax wi 
explain class probability estimates word probability estimates wi calculated 
class probability estimates 
estimate positive class probability input learner 
estimate negative class probability set 
positive word probability estimates 
input set positive documents 
consider laplace smoothing 
positive word probability estimates calculated equation wi wi card vocabulary card cardinality 
negative word probability estimates 
set nd negative documents available neg ative word probability estimates calculated equation wi wi nd card nd framework negative word probabilities estimated negative examples 
word probabilities wi wi wi wi probability generator creates wi probability generator creates word positive document 
equation rewritten wi wi wi equation estimate negative word probabilities 
assuming set unlabeled documents generated underlying generative model probability wi estimated set unlabeled documents wi ud ud 
estimates negative word probabilities rewritten wi wi ud wi ud ud equation positive word probabilities wi calculated equation input set positive documents 
estimate probability generator creates word positive document 
assumed lengths documents independent class set directly computed inputs pnb see denis calculation smoothing negative word probability estimates 

training positive unlabeled examples 
training positive negative examples training setting introduced blum mitchell general framework learning labeled data unlabeled data 
training setting applies dataset natural division features 
blum mitchell show assumptions set features sufficient classification feature sets instance conditionally independent class guarantees learning labeled unlabeled data hold 
training algorithm see table incrementally build naive bayes classifiers views 
denote set documents described views resp 
projection resp 
second view 
documents labeled projections considered labels 
training algorithm creates pool unlabeled documents 
iterates procedure 
algorithm trains classifiers nb nb views 
second classifiers applied unlabeled examples 
examples classifiers confident predictions removed set unlabeled data added label set labeled data 
final hypothesis combine nb nb created voting scheme combines prediction classifiers learned view 
training scheme define section training algorithm positive unlabeled examples 
idea replace nb pnb 
boosting rounds positive unlabeled examples 
pnb outputs classifier label examples negative 
negative examples boosting rounds training algorithm 
aim define variant pnb able self labeled negative examples 

algorithm takes input estimate positive class probability training sets set positive documents set nd negative documents set ud unlabeled documents 
situation differs classical naive bayes labeled examples input set nd labeled examples ways ratio card card card nd estimate confident labels negative documents 
pnb key point estimating negative word probabilities equation 
negative word probabilities estimated set negative examples sets positive unlabeled examples 
mix estimates 
denote wi nd estimate obtained set negative examples equation 
denote wi ud estimate obtained sets ud section wi ud 
define estimates negative word probabilities combining estimates equation wi wi ud wi nd set parameter card nd card negative document set negative word probabilities estimated sets ud equations 
sets nd ratio positive documents union set nd equal estimate positive class probability value suppose equally confident estimates 
naive bayes algorithm takes input estimate positive class probability set positive documents set ud unlabeled documents set nd negative documents 
class probabilities positive word probabilities calculated pnb 
negative word probabilities estimated equations 

training positive unlabeled examples extend training setting case positive documents unlabeled documents learner 
training learning algorithm table 
incrementally builds classifiers views algorithm 
training process repeats iterations 
training step picks card documents set unlabeled documents form unlabeled dataset input classifiers 
large unlabeled datasets degrade performance pnb classifiers denis 
outcome cotraining process consists final hypothesis prediction obtained multiplying prediction classifiers learned view 

empirical evaluation datasets webkb course dataset collection web pages collected computer science departments universities 
web pages divided categories 
student project course faculty categories 
list html tags removed stemming performed 
reuters collection collection text classification 
formatted version reuters version called reuters prepared yang colleagues 
documents labeled belong possible categories 
consider binary classification problems defined categories acq grain 
newsgroups dataset contains different usenet discussion groups 
remove usenet headers list stemming performed 

experiments pnb preliminary experimental results denis 

show pnb robust input value compare learning accuracy varying number unlabeled examples 
apply pnb real world data sets 
compare pnb nb considering experimental results nb lower upper bounds pnb 
compare pnb em defined liu 

comparison pnb nb experiments conducted compare pnb nb varying number labeled documents 
results table 
row table repeat times procedure 
select random set labeled data nbp set labeled data set positive unlabeled data reuters dataset comes test set items train set items keep separation experiences 
case webkb dataset test set remaining data drawing train sets obtain different test sets 
estimated error averaged runs 
standard deviation estimated standard deviation accuracy estimations holdout run 
score defined pr precision recall 
naive bayes positive unlabeled examples positive examples outperforms standard naive bayes labeled examples 
obviously unlabeled documents correct label standard naive bayes outperforms naive bayes positive unlabeled examples 
noted obtain results weight positive class quite small category grain 
comparison pnb em table report error rates measure pnb em 
learning algorithms take sets input set positive documents set built negative documents positive ones 
indicated liu objective recover positive documents put mixed seen test set 
em algorithm takes input set positive documents set unlabeled documents need estimation 
pnb algorithm takes set unlabeled documents set positive documents 
considering knowledge 
results indicated table give average error rates measure draws pnb 
algorithm pnb outperforms em sets experiments 
results pnb lower variance 
table 
comparison nb pnb real world datasets 
nbp error error error reuters category acq set reuters category grain set webkb set discussion real world datasets algorithm pnb builds accurate classifiers 
em pnb give similar results pnb needs rough estimation 
worth noting liu spy documents optimize performance em 
usefulness spy documents percents positive ones twofold avoid strong bias positive documents em initialization estimate errors select classifier sequence produced em 

training experimental results run training algorithm webkb course dataset 
binary classification problem identify web pages course home pages 
example consists words occur web page full text view words occurring anchor text hyperlinks pointing page hyperlink view 
class course designed positive class setting web pages positive 
fixed seed size card experiment pick random test set documents 
remaining documents draw set labeled documents containing card positive documents positive documents define seed remaining documents left unlabeled define set ud 
estimate set 
parameter set maximal number training steps depending number available unlabeled documents 
parameters respectively set 
set experiments study evolution error rates training steps 
seed size card set 
error rates output classifiers averaged experiments 
gives plot error versus number iterations training algorithm 
training steps error rates increase 
may due fact sufficient number documents needed statistics sufficiently accurate 
training steps error rates full text classifier combined classifier decrease continuously 
output full text combined classifiers outperform classifiers built seed dataset 
hyperlink classifier helped training hyperlinks documents contain fewer words 
individual experiments seed size set obtain similar plots 
seed size lower instance consider seed positive documents rare experiments error rates initial classifiers quite poor training improve accuracy initial classifiers 
reproduce experiments blum mitchell table 
experimental results newsgroup dataset 
columns pnb em give accuracy measure evaluated averaged draws standard deviation indicated parenthesis positive negative pos pnb em error error atheism rel graphic mac guns pol med elec rel pol student course project course faculty course implementation algorithm called ct training positive negative data 
case training steps error rates increase decreases continuously 
observe phenomenon accentuated ct case 
ct algorithm robust choice initial seed 
instance positive documents negative documents ct ultimately outputs classifier error rate greater percents trials 
positive documents input ultimately outputs classifiers error rate greater percents trials 

error versus number training steps training algorithm 
seed size set error rates averaged experiments second set experiments study error rates training algorithms different seed sizes 
seed size run experiments 
table gives error rates output classifiers 
cotraining algorithm ct defined blum mitchell choose initial seed cardinality 
error versus number training steps training algorithm ct seed size set documents error rates averaged experiments close number positive documents seed corresponding row row tables positive plus negative documents ct seed positive documents seed 
experimental results webkb dataset promising 
seed positive documents unlabeled documents ultimately classifiers produced outperform naive bayes classifiers trained labeled documents see table 
note algorithm robust 
seed positive documents classifiers outperform classifiers trained seed seed labeled documents ct classifiers may quite poor draws seed 
table 
training algorithm ct blum mitchell training algorithm ct parameters input set labeled documents set ud unlabeled documents create pool ud choosing documents random ud loop iterations di train naive bayes classifier remove ud examples confidently labels positive add remove ud examples confidently labels negative add randomly choose examples ud replenish ud output combine nb nb table 
training algorithm positive unlabeled documents self labeled positive negative documents added training steps 
training algorithm parameters input set positive documents set ud unlabeled documents estimate set ud ud set nd loop iterations create pool ud learn choosing card documents random ud train input di remove udp examples confidently labels positive add remove udp examples confidently labels negative add nd output combine table 
training ct upper table 
column start gives error rate measure pnb card positive examples column gives error rate measure combined classifiers training steps 
seed steps start size error error pos neg ct pos unl 
study adaptation naive bayes allow build classifiers positive unlabeled data pnb 
main idea approximate word probabilities negative class positive unlabeled data estimation weight positive class 
presence small set examples target class reuse training scheme introduced blum mitchell pnb base classifier 
apply algorithms binary text classification problem 
experiments show starting small number documents target class estimate probability class unlabeled documents training methods build competitive classifiers 
outcomes cotraining algorithm robust choice initial seed 
lot open questions 
positive class probability estimated data 
hypothesis testing algorithm apply setting research partially supported de plan tat nord axe tact projet tic fonds europ ens feder tic de donn es traitement intelligent des obj phasing 
anonymous reviewers comments suggestions 
blum mitchell 

combining labeled unlabeled data training 
proc 
th annu 
conf 
comput 
learning theory pp 

acm press new york ny 
collins singer 

unsupervised models named entity classification 
proceedings joint sigdat conference empirical methods natural language processing large corpora pp 

denis 

pac learning positive statistical queries 
proceedings th international conference algorithmic learning theory alt pp 

berlin 
denis gilleron 

learning positive unlabeled examples 
computer science appear 
denis gilleron 

text classification positive unlabeled examples 
th international conference information processing management uncertainty knowledge systems 
hofmann 

text categorization labeled unlabeled data generative model approach 
working notes nips workshop unlabeled data supervised learning 
joachims 

transductive inference text classification support vector machines 
proceedings icml th international conference machine learning pp 

liu lee yu li 

partially supervised classification text documents 
proc 
th international conference machine learning pp 



semi supervised learning explicit misclassification modeling 
proceedings th international joint conference artificial intelligence 
appear 
muslea minton knoblock 

active semi supervised learning robust multi view learning 
proceedings icml pp 

nigam ghani 

analyzing applicability effectiveness training 
proceedings cikm ninth international conference information knowledge management pp 

nigam mccallum thrun mitchell 

text classification labeled unlabeled documents em 
machine learning 


unsupervised word sense disambiguation rivaling supervised methods 
proceedings third meeting acl pp 

