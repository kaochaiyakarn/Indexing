recognition emotions interactive voice response systems steve lin john burns hp laboratories palo alto hpl july nd mail steven lin john burns hp com speech metadata analysis emotion recognition interactive voice response systems reports emotion recognition results speech signals particular focus extracting emotion features short utterances typical interactive voice response ivr applications 
focus distinguishing anger versus neutral speech salient call center applications 
report classification types emotions sadness boredom happy cold anger 
compare results neural networks support vector machines svm nearest neighbors decision trees 
database linguistic data consortium university pennsylvania recorded actors expressing emotions 
results indicate hot anger neutral utterances distinguished accuracy 
show results recognizing emotions 
illustrate emotions clustered selected prosodic features 
internal accession date approved external publication eurospeech th european conference speech communication technology september geneva switzerland copyright hewlett packard recognition emotions interactive voice response systems steve lin john burns hewlett packard laboratories page mill rd ms palo alto ca steven lin john burns hp com reports emotion recognition results speech signals particular focus extracting emotion features short utterances typical interactive voice response ivr applications 
focus distinguishing anger versus neutral speech salient call center applications 
report classification types emotions sadness boredom happy cold anger 
compare results neural networks support vector machines svm nearest neighbors decision trees 
database linguistic data consortium university pennsylvania recorded actors expressing emotions 
results indicate hot anger neutral utterances distinguished accuracy 
show results recognizing emotions 
illustrate emotions clustered selected prosodic features 

recognition emotion human speech gained increasing attention years due wide variety applications benefit technology 
human emotions hard characterize categorize research machine understanding human emotions rapidly advancing 
emotion recognition solutions depend emotions want machine recognize purpose 
emotion recognition applications talking toys video computer games call centers 
particularly interested application emotion recognition technologies interactive voice response ivr systems specific application call centers 
obvious example automatic call routing angry customer agents customer representatives automatic quality monitoring agents performance 
systems conversational utterances usually short 
results emotion recognition ivr applications 
studies characterized properties 
speaker independence test proposed emotion classifiers unseen samples unseen speakers ensure speaker independence 
second transcription required meaning emotion recognition occurs automatic speech recognition output obtained 
third ivr specific meaning calculate emotion features short utterances typical ivr systems 
fourth prosodic features meaning addition pitch contour energy contour features features audible inaudible contour explained 

emotion recognition automatic emotion recognition speech viewed pattern recognition problem 
results produced different experiments characterized features believed correlated speaker emotional state type emotions interested database training testing classifier type classifier experiments 
compare classification results dataset agree set emotions 
purpose section compare results reported earlier research review briefly techniques emotion recognition 
dellaert compared classifiers maximum likelihood bayes classification kernel regression neighbor nn methods particular interest sadness anger happiness fear :10.1.1.153.4209
features pitch contour 
accuracy achieved 
lee linear discrimination nn classifiers support vector machines svm distinguish emotions negative non negative emotions reached maximum accuracy 
developed realtime emotion recognizer neural networks call center applications achieved classification accuracy emotions calm features chosen feature selection algorithm 
discussed techniques exploit emotional dimension prosody 
experiments showed quality features formant analysis addition prosody features pitch energy improve classification multiple emotions 
quality features speaker dependent 
yu svms emotion detection 
built classifiers emotions anger happy sadness neutral 
svms binary classifiers recognizers worked detecting emotion versus rest 
average accuracy reported 

database performance emotion classifier relies heavily quality database training testing similarity real world samples generalization 
speech data testing emotion recognition grouped categories depending way speech signal captured 
method uses actors record utterances utterance spoken multiple emotions 
actors usually time imagine specific situation speaking 
second method called wizard oz woz uses program interacts actor drives specific emotion situation records responses 
third method hard obtain actual real world recording utterances express emotions 
experiments database linguistic data consortium university pennsylvania 
original data set hours english recordings sphere format transcripts 
dataset encoded channel interleaved bit pcm 
speech file continuous recording emotions speaker 
developed splitter component takes recordings associated transcripts emits separate utterances transcripts 
utterance file represents utterance actor expressing emotion 
result obtain set utterances roughly distributed fifteen emotions neutral hot anger cold anger happy sadness disgust panic anxiety despair interest shame boredom pride 
short utterances words bit pcm khz channel 
utterances spoken actors mid females males 

feature extraction utterances spoken caller short responses specific system prompts selections available menu options 
focus utterance level features opposed word level features 
global acoustic features calculated utterance level favor studies 
perform emotion recognition signal level regardless information obtained speech recognizer 
experiments utterance split frames size samples window step samples sampling rate khz 
calculate prosody features related pitch fundamental frequency loudness energy segments audible durational follows 

fundamental frequency features obtain pitch contour input utterance pitch values frame function time excluding unvoiced frames 
calculate total pitch features 
pitch contour 
obtain minimum maximum mean standard deviation 
derivative pitch contour 
obtain minimum maximum mean standard deviation 
jitter 
jitter defined small frequency changes modulation signal calculate jitter linear filter system similar 
system sequence pitch values provided input filter normalized frame pitch value 
filter 
jitter contour calculate mean minimum maximum standard deviation 

energy features obtain energy contour input utterance energy frame utterance function time 
calculate total energy features 
energy contour 
obtain minimum maximum mean standard deviation 
derivative energy contour 
obtain minimum maximum mean standard deviation 

similar jitter energy contour pitch contour 
calculate mean minimum maximum standard deviation contour 

audible durational features obtain set features related audible segments utterance 
identify audible segments speech utterance obtain maximum frame energy utterance consider frame energy level threshold percentage maximum energy inaudible frame audible 
picked threshold 
audible inaudible contour filtered low pass filter smooth segments 
result audible inaudible segments contour produced 
inaudible segments related pauses speaker utterance 
contour obtain features minimum maximum mean standard deviation duration audible segments 
minimum maximum mean standard deviation duration inaudible segments 
ratio total duration audible segments total duration inaudible segments 
ratio duration audible segments total duration utterance 
ratio duration inaudible segments total duration utterance 
ratio average duration audible segment average duration inaudible segments 
number audible frames divided number audible segments 

classification results classifiers neural networks svms case binary classifications nearest neighbors cases decision tree 
weka toolkit www cs waikato ac nz ml weka experiments reported 
training testing data split data training validation set unseen test set 
unseen test set report accuracy measures 
ldc database produced actors 
split data unseen test set actor utterance actor training testing 
experiment actor utterances training validation eighth actor utterances testing 
cases training samples actor utterances training validation actors testing 
believe data split gives indication classifier able generalize speaker independence 
call split actor split second split actor split 
training validation 
folds cross validation technique training data randomly split sets training tenth validations picked forth 

recognizing hot anger neutral set experiments concerned distinguishing anger neutral speech 
classifiers neural networks number hidden layers equal half sum input output nodes svm nearest neighbors decision trees 
experiment types datasets actor split actor split mentioned earlier 
split training testing ratio split ratio 
table summarizes recognition accuracy features 
table illustrates precision recall classifiers 
table find neural network classifier better presence sufficient training data svm outperforms classifiers scarcity training data 
table conclude classifier outperforms classifier usually precision recall accuracy emotions 
table accuracy recognizing hot anger versus neutral speech features classifier actor split actor split neural net svm nn perform experiments identify features significant classification feature selection 
forward selection feature selection algorithm rank features training data fold cross validation 
select top significant features features included maximum minimum mean pitch contour derivative maximum minimum mean jitter maximum energy 
experiments repeated just features 
table illustrates results testing set 
neural network classifier performance deteriorated svm performance remained nn performance deteriorated 
table precision recall statistics hot anger classifier split split precision neural svm nn recall neural svm nn precision neural svm nn recall neural svm nn table accuracy recognizing hot anger versus neutral speech significant features 
neutral emotion classifier actor split neural net svm nn 
recognizing hot cold anger versus neutral sadness created emotion groups contained hot anger cold anger utterances second contained neutral sadness utterances 
actor split yields training validation testing ratio 
features table summarizes performance classifiers unseen test set 
best performance obtained svm accuracy 
interpretation high classification accuracy groups due fact cold hot anger close prosodic feature space features extracted 
confirm conducted binary classification experiment distinguish cold anger hot anger 
actor split svm 
accuracy achieved case close random 
features extracted previous section suitable distinguishing cold hot anger 
conducted binary classification experiment distinguish neutral sadness 
actor split svm 
accuracy random guess 
features extracted previous section suitable distinguishing sadness neutral emotion 
table accuracy recognizing hot cold anger versus neutral sadness speech features classifier actor split neural net svm nn forward selection algorithm order features relevance picked top features standard deviation pitch minimum standard deviation jitter ratio audible duration inaudible duration maximum energy 
table summarizes accuracy obtained just features 
table accuracy recognizing group hot cold anger versus group neutral sadness speech features classifier actor split neural net svm nn neural network nn classifiers appears features sufficient 
svm performance degradation associated features reduction 

recognizing hot anger sadness neutral happiness experiment study recognition hot anger neutral sad happy emotions class classification problem 
actor split yielded training testing ratio 
classifiers neural network nn 
neural network classifier accuracy accuracy happy hot anger neutral 
nn classifier accuracy accuracy happy hot anger neutral 
deduce happy signals introduced recognition hot anger decreases significantly drives section 

emotions close 
run experiment understand emotions close features space 
select emotions hot anger happiness sadness boredom neutral emotion 
decision tree classifier 
table illustrates confusion matrix obtained 
confusion matrix conclude sadness confused boredom boredom confused sadness happy confused hot anger hot anger confused happy neutral confused sadness reported previous experiments 
table confusion matrix emotions sadness boredom happy hot anger neutral 
sad bore happy anger neutral sad bore happy anger neutral classifying multiple emotions group happy hot anger sadness boredom separate features classifiers groups group 
similar results reported 
support results conducted experiment grouped happiness hot anger group sadness boredom group 
ran binary classification experiments svm decision trees actor split 
svm obtained accuracy accuracy sadness boredom group happy anger group 
decision trees obtain accuracy sadness boredom group happy anger group 
apparently easier mistake sadness boredom motion happy anger way 
forward selection algorithm determine features significant 
features significant distinguishing groups maximum mean pitch contour derivative maximum jitter minimum duration inaudible segments ratio audible segments duration total duration utterance 

recognizing emotions experiment considered database entirety emotions utterances 
neural network classifier actor split yielded training testing ratio 
accuracy obtained classification random 
challenge humans identify types emotions defined selected database 
indicates ivr system focus readily identified emotions anger boredom 

reported features extracted pitch contour energy contour audible segment duration contours achieve high degree accuracy distinguishing certain emotions 
focused utterance level features short utterances typical ivr applications 
database ldc consortium 
compared different classifiers applicable 
summarize findings follows able recognize hot anger versus neutral accuracy exceeding 
data classifier better accuracy notable classifier performs better terms precision recall accuracies 
hot cold anger easily distinguished prosodic features discussed earlier 
similarly sadness neutral emotions easily distinguished 
features see section sufficient distinguishing anger hot cold neutral sad emotions accuracy 
accuracy classifying multiple emotions time prosodic features discussed low random 
hierarchical classification grouping emotions desirable 
anger happiness close prosodic dimensions classifiers confuse 
applies sadness boredom 

ang dhillon shriberg stolcke prosody automatic detection annoyance frustration human computer dialog proc 
icslp denver colorado sept 
batliner fischer huber noth seeking emotions actors wizards human beings proc 
isca workshop speech emotion queen university belfast northern ireland sept 
cowie douglas cowie taylor emotion recognition human computer interaction ieee signal processing magazine pp 
jan 
dellaert polzin waibel recognizing emotion speech proc :10.1.1.153.4209
icslp philadelphia pa pp 

hess pitch determination speech signals springer verlag berlin 
lee narayanan pieraccini classifying emotions human machine spoken dialogs proc 
international conference multimedia expo lausanne switzerland august 
huber batliner prosodic speech characteristics automated detection alcohol isca tutorial research workshop prosody speech recognition understanding 
red bank nj october approaching automatic recognition emotion voice rough benchmark isca workshop speech emotion belfast 
ortony clore collins cognitive structure emotions cambridge univ press 
emotion speech recognition application call centers proc 
artificial neural networks engineering pp 
nov 
emotion recognition speech signal experimental study development application proc 
international conference spoken language processing icslp 
quinlan programs machine learning morgan kauffman 
santos pardo emotional space improves emotion recognition proc 
icslp denver colorado september 
linguistic data consortium emotional prosody speech www ldc upenn edu catalog jsp ldc 
university pennsylvania 
yu chang xu shum 
emotion detection speech enrich multimedia content second ieee pacific rim conference multimedia october beijing china 
