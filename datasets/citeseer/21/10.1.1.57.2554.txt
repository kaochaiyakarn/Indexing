blind source separation shun ichi amari fellow ieee riken frontier research program shi japan tel fax amari zoo riken go jp sp blind source separation extracts independent component signals mixtures knowing mixing coefficients probability distributions source signals 
known algorithms surprisingly 
elucidates algorithms statistical analysis 
general known asymptotic theory statistical analysis covariance extracted independent signals converges order case statistical estimation examples 
case line learning theory line dynamics shows covariances converge order learning rate fixed small constant 
contrast general properties surprising holds blind source separation certain conditions 
implies covariance decreases order uses natural gradient learning algorithm method estimating functions obtain procedures estimation line learning 
imply error variances extracted signals decrease order implies covariances 
blind source separation problem extracting independent signals mixtures observed 
simple linear model assumed original source signals independent stochastic processes 
component independently identically distributed 
probability distributions unknown assumed gaussian 
assumed observed instantaneous mixtures ia observed ia unknown nonsingular mixing matrix depend time independent signals extracted ai wx ai inverse mixing matrix equal 
need estimate past observed signals batch statistical estimation line learning neural networks type 
proposed attractive idea inspired neural learning see lot algorithms proposed 
mention independent component analysis ica entropy maximization nonlinear pca algebraic approach 
elucidated relation ica entropy approaches 
statistical efficiency algorithms analyzed point view semiparametric estimation 
natural relative gradient introduced guarantees equivariant properties see 
dynamic stability learning analyzed special cases 
succeeded give stability analysis general case 
proposed universally convergent learning algorithm 
computer simulations learning algorithms surprisingly extracting independent signals 
call 
uses statistical analysis explain 
demixing matrix estimated examples certain normalization condition eliminate indeterminacy estimation errors measured variance covariances decrease order large known asymptotic theory statistics 
implies variance covariance matrix errors extracted recovered signals decreases order 
error covariance matrix ab defined ab different components covariance ab decreases order quicker ordinary order phenomenon called independent source extraction 
condition guarantees 
uses method estimating functions analyzing problem semiparametric statistical models 
class equivalent estimating functions standardized estimating function defined explicitly calculated 
result proving batch estimation procedure 
line learning treated standardized estimating function 
learning rate fixed small constant dynamical theory line learning proves covariance ab errors recovered general order large small fluctuations order remain learning convergence speed learning large large 
implies ab order large line learning proved certain conditions standardized estimating function line learning 
standardized estimating function gives universally stable learning algorithm 
asymptotic equivalence batch line learning procedures proved extending result see 
expected hold problem multichannel blind deconvolution natural gradient studied 
error analysis recovered signals consider independent sources signals generate signals discrete times 
represented column vector denotes transposition 
assumed zero mean white random variables oe ffi ab ffi tt denotes expectation ffi denotes kronecker delta 
assumed signals observed directly instantaneous mixtures observed 
represented mixing matrix unknown 
blind source separation problem recovering original signals observed signals 
blind implies know mixing matrix probability distributions independent 
know original signals easily recovered wx problem estimate ai statistical point view obtain estimate estimate observations 
estimator derived statistical procedure recursively learning 
mixing matrix inverse identifiable 
extract independent signals know arrange order 
recovered signals permutation original absolute scale identifiable 
multiplying scalar equivalent multiplying ath column order reduce scaling introduce normalization constraint arbitrary function example determined uniquely permutation 
original independent components recovered best scales ordering 
estimator order adequately redefined observations estimation error 
error recovered signal written covariance matrix recovering errors diagonal elements ab represents recovered correlated diagonal elements aa represent magnitude errors recovered signals gives asymptotic statistical analysis error covariance matrix show fast converges tends infinity 
estimating equation estimation error problem blind source separation formulated framework semiparametric statistical model 
probability density function joint probability density written independent 
observation function probability density function det jw jr wx know product form probability model includes parameters called parameter interest want know nuisance parameter function care 
statistical model called semiparametric model estimation parameter interest general difficult problem unknown functions 
method estimating functions developed problem mathematical foundation information geometry see information geometry 
estimating function case matrix valued function ff ab including nuisance parameter satisfies denotes expectation respect probability distribution requested holds form 
order avoid trivial require non degenerate 
noted matrix matrix operator linearly maps matrix matrix 
components ab ci ci ab convenient capital indices represent pair indices 
matrix representation ab operates wb ci hw ab ci ci inverse defined inverse matrix 
estimating function estimating equation fx wg solution gives estimator derived replacing expectation empirical sum observations 
equation solved making unknown problem find estimating function learning algorithm automatically derived estimating function fx expected converge solution 
proposed number estimating functions heuristically 
mathematical theory proves estimating functions form ab ffi ab component form span effective estimating functions arbitrary functions 
implies estimating function better equivalently estimating function class 
class includes best estimator fisher efficient estimator sense estimator satisfies extended cram er rao bound asymptotically 
true distributions best choice ds log different know estimating equation gives consistent estimator estimation error converges probability order goes infinity 
estimation error define relative error matrix ab quantity convenient analysis recovering error written calculate estimation error usual method asymptotic statistical analysis expand estimating equation fx wg fx assume small higher order terms neglected 
gradient term implies ab ci cd di component form 
notation ab cd ab ci di ffx wg fx wg law large numbers guarantees convergence gradient term expectation respect true distribution ab cd operator maps matrix matrix 
expectation zero central limit theorem guarantees ffx wg converges distribution normal random variable matrix mean covariances ab cd ab cd theorem 
covariance ab cd error measured terms estimator asymptotically gk inverse operator putting ac db gcd abbreviated form ab cd ac components inverse proof 
substituting right hand side ae oe asymptotically normally distributed mean 
covariance 
covariance matrix recovered signals easily calculated covariances estimator lemma 
ab ac bc oe oe determined constraint 
proof 
fs ac gfs bd ac bd ac bd ac bc oe ac standardized estimating function arbitrary nonsingular linear operator acting matrices 
estimating function matrix estimating function matrix rf equivalent sense derived estimators exactly estimating equations fx wg ffx wg give solution family equivalent estimating functions give estimator 
equivalent estimating function matrices convenient define standardized 
say estimating function standardized identity operator 
lemma 
estimating function standardized form standardized matrix multiplication imply ab cd ab cd respectively component form 
proof 
fr ae oe identity equation immediate 
equivalent estimating functions statistical estimators derived 
line learning equation form fx give different estimators 
shown standardized gives best estimator 
form order obtain standardized estimating function explicitly calculate operator ab cd ab cd ab cd gradient hessian 
hessian calculated cases 
lemma 
true value recovered calculated ab cd ffi bd ffi ac ffi ad ffi bc denotes derivative 
proof appendix 
put aa cd know ab cd 
pairs equal gives ab ab oe ab ba summarize results 
denote pairwise component form enlarged matrix kab 
results show kab kab 
shows kab diagonalized non zero 
kab diagonalized non zero parts existing matrices form aa oe oe 
inverse diagonalized form aa part oe oe ab oe oe ab oe oe standardized form estimating function calculated follows 
theorem 
standardized estimating function matrix aa ab ab fk oe statistical analysis recovering errors covariance matrix calculated explicitly 
put ab cd ab cd lemma 
covariances ab ab cd ab cd particular ac bc ac bc oe oe oe aa ba ba oe proof appendix 
theorem 
covariance matrix aa ab ab oe ab ac bc oe theorem shows expected squared errors aa decrease proportion covariances ab recovered signals decrease order 
fact agreement ordinary asymptotic statistical analysis 
prove implying dependency recovered signals decreases order certain conditions 
proves independency easily recovered algorithms 
theorem 
holds derived true probability distribution odd functions functions 
proof 
condition holds ds log ds ds condition holds ds ab satisfy ab higher order cumulants order shown asymptotic statistical analysis 
coefficients ab order calculated explicitly method higher order asymptotics statistical inference see 
include curvature terms statistical manifold connection terms 
learning algorithms far studied statistical analysis asymptotic errors estimated batch type algorithm past data stored 
cases blind separation carried line 
natural relative gradient learning rule jf fw estimating functions form 
put put increment define ffix learning equation written ffix order analyze asymptotic behavior learning dynamics recapitulate old theory dynamics line learning 
result rediscovered 
small constant proved expectation converges optimal value exponentially provided learning algorithm stable 
large fluctuates optimal value 
variance covariance matrix theorem 
rough proof appendix 
theorem 
sufficiently small covariance matrix relative error converges jy solution kbc yb kac gab apply theorem case 
learning algorithm said ab order sufficiently large theorem 
holds natural gradient line learning algorithm condition theorem holds 
proof 
shown kbc block diagonalized 
equation splits blocks 
order calculate ab ac bc oe need calculate ac bc different splits closed blocks consisting variables ac bc ac cb ca bc ca cb equation variables oe oe oe oe oe oe oe oe ac bc ac cb ca bc ca cb ac bc ac cb ca bc ca cb similar equation holds block aa ba aa ab ba aa components ac bc right hand side calculated ac bc ac bc oe easy check components type zero shows ac bc aa ba 
large result holds learning equation 
remarked line versus batch learning long fixed constant converges environment fluctuating 
hand learning rate depends converges large satisfies certain conditions known converge stochastic approximation 
case compare asymptotic behaviors line learning batch learning statistical estimation 
general true efficiency line learning worse batch learning training example line learning observed 
proved optimal line algorithm gives asymptotically efficiency optimal batch algorithm 
true regular statistical estimation fisher information exists 
proved efficiency line learning half batch learning case noiseless binary perceptrons fisher information exist 
see 
extends result case blind separation 
theorem 
line learning rule standardized estimating function ffix gives asymptotically best performance optimal batch estimator 
condition theorem 
proof 
see appendix 
proved learning rule locally stable true solution 
statistical analysis separation errors problem blind separation independent signals 
statistical analysis batch estimation procedure framework estimating functions 
standardized estimating function calculated explicitly source separation proved hold certain conditions 
explains source separation works 
line learning analyzed 
optimal line learning algorithm explicitly terms standardized estimating functions proved cases constant time dependent learning rates 
amari theory adaptive pattern classifiers ieee trans 
electronic computers vol 
ec pp 
amari differential geometrical method statistics lecture notes statistics new york springer verlag 
amari learning statistical inference handbook brain theory neural networks ed 
arbib mit press pp 
amari natural gradient works efficiently learning neural computation appear 
amari information geometry contemporary mathematics appear 
amari cardoso blind source separation semi parametric statistical approach submitted 
amari 
chen cichocki stability analysis adaptive blind source separation neural networks accepted 
amari cichocki yang new learning algorithm blind signal separation advances neural information processing systems eds david touretzky mozer michael hasselmo mit press cambridge ma pp 

amari douglas cichocki yang multichannel blind deconvolution equalization natural gradient signal processing advance wireless communication workshop paris 
amari information geometry estimating functions semiparametric statistical models bernoulli press 
bell sejnowski information maximization approach blind separation blind deconvolution neural computation pp 

cardoso source separation higher order moments proc 
icassp 
cardoso laheld equivariant adaptive source separation ieee trans 
signal processing 
chen chen blind extraction stochastic deterministic signals neural network approach th asilomar conference signals systems computer 
common independent component analysis new concept signal processing pp 
herault jutten space time adaptive signal processing neural network models denker ed neural networks computing 
proceedings aip conference 
american institute physics new york 
heskes kappen learning process neural networks physical review pp 
jutten herault blind separation sources part adaptive algorithm neuromimetic architecture signal processing vol 

moreau self adaptive source separation part convergence analysis direct linear network controlled herault jutten algorithm submitted ieee trans 
signal processing 
parga nonlinear neurons low noise limit factorial code maximizes information transfer network pp 
oja karhunen signal separation nonlinear hebbian learning 
eds computational intelligence dynamic system perspective new york ieee press 
opper online versus line learning random examples general results phys 
rev lett pp pham blind separation instantaneous mixtures sources independent component analysis ieee trans 
signal processing 
blind separation sources part iii stability analysis signal processing vol pp 
van den unsupervised learning examples line versus line phys 
rev lett pp 
yang amari adaptive line learning algorithms blind separation maximum entropy minimal mutual information neural computation accepted 
appendix 
calculation ab cd order calculate gradient respect put df dw df denotes increment due change dw expand form df ab ab cd dx cd ab cd ab cd expectation gives ab cd ab ab ffi ab df ab dy dy dy dy wx dxy dy dx ad ffi ac dx cd ab cd ffi ac ffi bc true independent ffi ac ffi bd ffi ad proves lemma 
appendix 
calculation ab cd evaluate easily ac bc ac bc fk oe oe ac bc oe oe ac bc oe oe oe aa ba ba oe ba oe oe appendix 
derivation equation covariances errors learning extended suffix pair indices 
learning equation rewritten terms errors jf close small jff taylor expansion 
ab error covariances time ab ab je kbc kac ac higher order terms converges sufficiently large bc ac jg ab proving 
appendix 
proof theorem error term see appendix 
expansion ff case identity operator 
higher order terms solution equation asymptotically optimal batch solution 
