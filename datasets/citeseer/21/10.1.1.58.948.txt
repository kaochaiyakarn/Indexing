kernel sets vectors kondor cs columbia edu tony jebara jebara cs columbia edu computer science department columbia university amsterdam ave new york ny various application domains including image recognition natural represent example set vectors 
base kernel implicitly map vectors hilbert space gaussian distribution set kernel pca 
de ne kernel examples bhattacharyya measure anity gaussians 
resulting kernel computable closed form enjoys favorable properties including graceful behavior transformations potentially justifying vector set representation cases conventional representations exist 

kernel methods support vector machines gaussian processes proved extremely successful wide variety supervised unsupervised machine learning tasks 
whilst core algorithms eld fairly sch olkopf smola theoretical properties thoroughly investigated nding optimal ways representing real life data input algorithms largely open issue 
operating training testing examples directly input space kernel algorithms recourse value kernel function evaluated pair examples 
kernel symmetric similarity measure satisfying positive semi de niteness selection examples set coecients 
conditions ensure existence mapping 
hilbert space called feature space turns inner product traditionally examples represented vectors kernel de ned closed form positive de nite function gaussian radial basis function rbf kernel realized strengths kernel learning paradigm ability support general representations data 
input space long de ne function positive de nite plausible similarity measure examples task hand 
idea rise host novel kernels string kernels watkins lodhi leslie vishwanathan smola kernels graphs kondor la erty kernels automata cortes kernels statistical manifold jaakkola haussler la erty lebanon kernels general discrete objects haussler collins du :10.1.1.58.4737:10.1.1.21.2820:10.1.1.44.7709:10.1.1.2.7163
focus representing examples sets vectors fx bag tuples approach suit diverse domains natural way 
images tuple may correspond single pixel encoding coordinates corresponding intensity value 
time series analysis tuples may encode value time pairs 
video sequence seen collection intensity time tuples 
cases emphasis representation sake behavior confers kernel 
instance kernel sets vectors automatically invariant proceedings twentieth international conference machine learning icml washington dc 

bag tuples representation images 
tuple encodes coordinates pixel corresponding intensity value 
permutations vectors set 
point interested kernels relatively insensitive transformations 
especially smoothly varying function soft invariances match intuitive notions similarity key element design high performance kernels 
example images slowly varying function transformations kind correspond translations rotations 
achieve soft invariance property distributions sets fx fx de ne kernel bhattacharyya overlap measure intermediate step tting distributions ensures explicit invariance permutation ords degree smoothing concern nding right parametric family distributions choose suciently general capture structure objects wish represent ord controllable smoothing allow compute kernel closed form 
similar vector set representation images conjunction kernel trick proposed wolf shashua 
approaches handling sets vectors collections tuples bags pixels include applying pca data images maintaining image builtin invariance jebara 

kernels distributions estimated samples investigate case examples collections fx dimensional vectors tuples monochrome bitmap images naturally represented form letting encode coordinates pixel letting set foreground pixels 
gray scale color images representations form may encode information brightness intensity rgb color components 
set contain tuple pixel random subset pixels 
desired images described terms complex features wavelets edge features tuple code occurrence feature 
de ning kernel directly sets tuples regard samples unknown distributions parametric family proceed de ning kernel members statistical procedure estimating vector set kernel de ned kernel corresponding distributions 
simplify notation omit bold face font vector quantities understanding denote members 
bhattacharyya kernels known de nitions similarity distance distributions kullback leibler divergence fisher kernel distance 
de ne kernel bhattacharyya anity bhattacharyya dx trivially related better known hellinger distance dx 
expressing orthogonal basis functions shows automatically positive de nite 
addition satis es normalization property dx 
multivariate normal model shall restrict attention case family multivariate normal distributions probability density function 
handwritten letter resolution rst kernel principal components gaussian kernel pixels 
denotes determinant 
general applications kernels form see jebara kondor 
set maximum likelihood estimates sample mean empirical covariance matrix short computation shows bhattacharyya kernel exp 
plugging kernel computed closed form 

distributions hilbert space point representational power kernels limited 
certainly images truly hope dimensional gaussians capture sucient detail successful learning 
overcome diculty introduce second positive de nite kernel 
time de ned elementary vectors recall kernel construct hilbert space mapping repeat construction section time letting family distributions tting hilbert space points de ning kernel dz typical kernels span subspace dimensionality greater allowing simple parametric families distributions capture complex structures original sample 
choice experiments images section familiar gaussian rbf kernel kx method way limited particular kernel 
independent idea de ning kernel swarm hilbert space vectors induced kernel proposed wolf shashua context representing images sets vectors 
contrast distribution approach wolf shashua concentrate subspaces spanned de ne kernel principal angles subspaces 
discuss special case multivariate normal model computed closed form need explicitly construct images 

normal distributions facilitate discussion adopt dirac bra ket notation dirac hilbert space objects 
ket jxi denote bra denote dual analog nite dimensional vector spaces 
labeled letters denote general elements images 
inner product simply written expressions form 
reconstruction letter rst principal components pixel shading re ects gaussian tted rst principal components images elementary vectors black pixels original gure orthogonal directions uniform 
weighted sums expressions ia symmetric bilinear forms corresponding symmetric matrices nite dimensional case 
power dirac notation begins show considering corresponding linear mapping 

ia ia course just number 
denote orthogonal complement nullspace jz jz note invertible provided form orthonormal set ij inverse simply ia nite dimensional normal distribution form symmetric positive de nite bilinear form rank note proper distribution uniform directions orthogonal plugging empirical mean covariance jx jx hx lead results bhattacharyya kernel penalize lack alignment spaces related problem tting 
particular gaussian rbf kernel shown jx span subspace dimension exactly fitting dimensional normal distribution data points robust directions low covariance directions informative 
generally rst eigenvectors covariance matrix give description data carrying eigenvectors wasteful potentially misleading 
address problems time take regularized covariance form reg jv hv jv jv largest eigenvectors corresponding eigenvalues regularization constant form orthonormal basis note case nite dimensional denominator divergent 
strictly speaking normal distribution anymore gaussian process shall discuss section 
formula bhattacharyya kernel diverging normalization factors cancel conforming intuition action limited nite dimensional subset spanned data 
technique computing eigenvectors feature space known kernel principal component analysis kernel pca developed sch olkopf context unsupervised learning 
review technique show construct eigenvectors jv explicit calculations 
kernel pca key observation kernel pca eigenvectors jv lie span images jx equivalently centered images jx jx jx 
reconstruction letter gaussian rst kpca components augmented diagonal term 
plugging eigenvector equation jx jx multiplying left hx gives jx jx hx jx observe sums regular matrix multiplications disguise equivalently centered gram matrix jx nding principal components typically high possibly nite dimensional vectors jx reduces dimensional eigenvector problem shows rst kernel principal components handwritten letter mapped back original image plane induced principal components capture visually recognizable features gure 
shows reconstruction letter dimensional normal distributions regularization term 
note nonlinearity components capture appearance original letter quite 
shows reconstruction regularized gaussian model principal components 
note recovered images closer original ect tuning similar smoothing image plane 

computing bhattacharyya kernel remains put pieces compute bhattacharyya kernel recall orthogonal complement nullspace 
easy see dimensions orthogonal integrate relieving need take determinants nite dimensional forms kernel 
subscript denotes matrix corresponding restriction form subspace term dashed counterpart evaluated expansion linear combinations centered ultimately reducing weighted sum kernel evaluations hx jx 
determinant easily computed dim similarly mixed determinant mixed term exponent require explicit construction matrices fj ig orthonormal basis easiest construct basis starting basis eigenvectors jv extending vector time adding eigenvectors performing gram schmidt orthogonalization 

relationship gaussian processes far said elements 
show natural interpretation functions original space identify jxi function 
extend linearly jx corresponding continuous limit jxi dx identi ed dx 
jf interchangeably 
property hf jf hf dx jf dx particular hf jf lends construction name reproducing kernel hilbert space commonly abbreviated rkhs 
images interpretation particularly clear 
suppose dealing monochrome images unit square function 
jf image 
analog normal distribution function spaces gaussian process 
precisely set real valued random variables fy zg index set said form gaussian process marginal distribution zk multivariate normal 
natural regard distribution functions 
important property de ning mean covariance function cov marginals uniquely de ned 
gaussian process concept meshes naturally point view 
setting replacing letting jxi distribution see previously laboriously tted generalized normal distribution gaussian process mean covariance cov hf reg jf kernel pca procedure interpreted tting gaussian process sample functions 
resulting distribution functions really encodes beliefs similar image pixels bhattacharyya kernel de nes similarity integral square root similar similar machine learning literature long history gaussian processes compact bayesian function learning tool need invoke outside estimation procedure zhu mackay 
method fact gaussian process prior updated observations additive gaussian noise known variance gives rise posterior gaussian process 
question arises estimate bayesian approach 
answer methods yield gp estimators fundamentally di erent classical gp procedure regression tool kernel pca procedure density estimator 
justi cation estimation procedure essentially traditional mle estimator normal distributions 
sample functions maximum likelihood gaussian process generate functions mean covariance cov 
estimator regularized approximation gp rst components covariance form 
potentially satisfying bayesian approach estimating involve estimating lines zhu remains subject research 

experiments 
crosses squares explore robustness vector sets kernel spatial variation preliminary classi cation experiment generated monochromatic images crosses squares various positions scales pixel eld 

synthetic images data set 
crosses squares generated random translation rescaling 
trained support vector machine separate cross images square images half dataset training half testing 
baseline compare standard method treating image single vector apply conventional gaussian rbf kernel 
depicts classi cation accuracy function svm regularization parameter multiple curves shown various settings conventional rbf various settings analogous parameter gaussian base kernel novel point set kernel 
regularization keep rst principal components empirically reasonable values 
clearly provided appropriate point set kernel easily outperform traditional rbf 
severely handicapped crosses squares appearing di erent parts gure sensitive coincidence pixels unaware relative position pixels 
contrast point set kernel shape position degree 
regularization classification error 
bhattacharyya point set kernel solid lines achieve lower testing error synthetic image dataset best conventional rbf dotted lines 

handwritten digits comparing kernel common benchmarks familiar setting conducted experiments intentionally small dataset handwritten digits consisting just rst examples digits nist dataset 
test learn visual patterns sparse noisy examples original images sampled pixels foreground region image intensity greater scale coordinates pixels algorithm 
experiments performed training images testing remaining averaging performance random splits average giving positive training examples class 
pursuing simple versus strategy separate support vector machines built class testing predicted class chosen highest usual support vector coecients bias term 
results compared baseline conventional rbf dot product kernel ed images 
clearly performance point set kernel sensitive choice potential far outperform baseline 
previous experiment attempt optimize performance respectively systematic study set parameters cross validation identifying drop point eigen gap spectrum 
regularization dot product dotted rbf sigma dash rbf sigma dash rbf sigma dash 
sigma 
sigma sigma 
bhattacharyya point set kernel solid lines sensitive far outperform conventional gaussian rbf dashed dot product dotted kernels ed nist images task 

proposed novel kernel applies wide class learning problems instances represented sets vectors 
kernel de ned bhattacharyya anity gaussian models tted set 
resulting kernel powerful procedure kernelized second kernel de ned elementary vectors 
bag tuples representation instances worthy exploration 
addition explicit invariance permutation treating variables footing base kernel extend favorable smoothness properties variables 
contrast conventional gaussian rbf kernel images pixel coordinates treated indices kernel behaves gracefully intensity 
traditional kernel concept metric structure consequently behaves poorly translation rotation 
choice parametric model essentially constrained gaussians dual requirements ker existence closed form formula anity 
hand base kernel chosen freely making method quite exible 
restriction sets vectors title unnecessary come continuous discrete set meaningful kernel de ned 
possible extension apply method recursively sets sets 
possible integrate step procedure including estimating single consistent bayesian operation 
acknowledgments wolf patrick ha ner anonymous referees corrections important comments integrated 
bhattacharyya 

measure divergence statistical populations de ned probability distributions 
bull 
calcutta math soc 
collins du 

convolution kernels natural language 
advances neural information processing systems pp 

cambridge ma mit press 
cortes ha ner mohri 

rational kernels 
advances neural information processing systems 
cambridge ma mit press 
dirac 

principles quantum mechanics 
oxford university press 
haussler 

convolution kernels discrete structures technical report ucsc crl department computer science university california santa cruz 
jaakkola haussler 

exploiting generative models discriminative classi ers 
advances neural information processing systems 
cambridge ma mit press 
jebara 

convex invariance learning 
ninth international workshop arti cial intelligence statistics 
jebara kondor 

bhattacharyya expected likelihood kernels 
proceedings sixteenth annual conference learning theory seventh kernel workshop 
press 
kondor la erty 

di usion kernels graphs discrete input spaces 
machine learning proceedings nineteenth international conference icml 
la erty lebanon 

information di usion kernels 
advances neural information processing systems 
cambridge ma mit press 
leslie eskin weston noble 

mismatch string kernels svm protein classi action 
advances neural information processing systems 
cambridge ma mit press 
lodhi saunders shawe taylor cristianini watkins 

text classi cation string kernels 
journal machine learning research 
mackay 

gaussian processes replacement neural networks 
tutorial tenth annual conference neural information processing systems 
available wol ra phy cam ac uk pub mackay 
sch olkopf smola 

learning kernels support vector machines regularization optimization 
cambridge ma mit press 
sch olkopf smola uller 

nonlinear principal component analysis kernel eigenvalue problem 
neural computation 
vishwanathan smola 

fast kernels string tree matching 
advances neural information processing systems 
cambridge ma mit press 
watkins 

dynamic alignment kernels 
smola sch olkopf bartlett schuurmans eds advances kernel methods 
cambridge ma mit press 
wolf shashua 

kernel principal angles classi cation machines applications image sequence interpretation 
ieee conf 
computer vision pattern recognition cvpr 
zhu williams rohwer 

gaussian regression optimal nite dimensional linear models technical report ncrg 
aston university neural computing research group 
