active sensor networks philip levis david gay david culler pal culler cs berkeley edu david gay intel com eecs department intel research berkeley university california berkeley avenue berkeley ca berkeley ca propose application specific virtual machines asvms reprogram deployed wireless sensor networks 
asvms provide way user define applicationspecific boundary virtual code vm engine 
allows programs concise tens hundreds bytes making program installation fast inexpensive 
additionally concise programs interpret instructions imposing little interpretation overhead 
evaluate asvms current proposals network programming runtimes show asvms energy efficient 
evaluate asvms hand built tinyos applications show interpretation imposes significant execution overhead low duty cycles realistic applications actual cost effectively 

wireless sensor networks limited resources tight energy budgets 
constraints network processing prerequisite scalable long lived applications 
sensor networks embedded uncontrolled environments user know exactly sensor data look able reprogram sensor network nodes deployment 
proposals domain specific languages area open investigation possible programming models writing programs 
queries example declare nodes aggregate data flows root collection tree 
wide range programming abstractions led similarly wide range supporting runtimes ranging network query processors native thread libraries node script interpreters 
vertically integrated solution making mutually incompatible 
additionally implementation assumptions simplifications lead unnecessary inefficiencies 
propose new programming approach network processing propose architecture implementing programming model underlying runtime 
extend prior mat virtual machine tiny bytecode interpreter ing simple vm architecture building application specific virtual machines asvms 
experiences showed mat harsh limitations complex instruction set precluded supporting higher level programming 
carefully relaxing restrictions allowing user customize instruction set execution triggering events asvms support dynamically reprogramming wide range application domains 
introducing lightweight scripting network easy process data close source 
processing improve network lifetime reducing network traffic improve scalability performing local operations locally 
similar approaches appeared domains 
active disks proposed pushing computation close storage way deal bandwidth limitations active networks argued introducing network processing internet aid deployment new network protocols active services suggested processing ip points 
nomenclature name process introducing dynamic computation sensor network active sensor networking 
prior efforts active networking similarity differing goals constraints internet sensor networks lead different solutions 
defer detailed comparison section 
pushing boundary higher level operations allows application level programs achieve high code density reduces ram requirements interpretation overhead propagation cost 
higher boundary sacrifice flexibility extreme case asvm single bytecode run program answer question boundary lie question answer depends application domain asvms provide flexibility application developer pick right level abstraction particulars deployment 
generally dense bytecodes sacrifice flexibility asvms customized domain interest 
regionsvm section asvm designed vehicle tracking extensions regions operations typical vehicle tracking programs order bytes long th size originally proposed regions implementation 
second asvm built queryvm supports sql interface sensor network lower energy usage tinydb system allows adding new aggregation functions dynamically 
contributions 
shows way introduce flexible boundary dynamic static sensor network code enabling active sensor networking lower cost prior approaches simultaneously gaining improvements safety expressiveness 
second presents solutions technical challenges faced approach include extensible type support concurrency control code propagation 
believe results suggest general methodology designing implementing runtimes network processing 
section describe background information relevant including mote network resource constraints operating system structure version mat 
observations derive ways mat insufficient establishing requirements network processing runtimes effective 
section asvms outlining structure decomposition 
section evaluate asvms series microbenchmarks compare asvm regions original implementations 
survey related section discuss implications results section conclude section 
background asvms run tinyos operating system programming model affects structure implementation 
general operating model tinyos networks low duty cycle network energy constraints lead limited node resources underutilization resources 
mat prior monolithic vm developed particular application domain 
observations derive set technical challenges runtime system support active sensor networking 
tinyos nesc tinyos popular sensor network operating system designed mote platforms 
nesc language implement tinyos applications provides basic abstractions component programming low overhead event driven concurrency 
components units program composition 
component set interfaces uses set interfaces provides 
programmer builds application connecting interface users providers 
interface parameterized 
component parameter ized interface copies interface distinguished parameter value essentially array interface 
parameterized interfaces support runtime dispatch set components 
example asvm scheduler uses parameterized interface issue instructions instruction instance interface scheduler dispatches opcode value 
tinyos event driven concurrency model allow blocking operations 
calls long lasting operations sending packet typically split phase call operation returns immediately called component signals event caller completion 
nesc programming binds callbacks statically compile time nesc interfaces function pointers passed run time 
mote networks motes need able operate unattended months years robustness energy efficiency dominant system requirements 
hardware resources limited minimize energy consumption 
current tinyos motes mhz microcontroller kb data ram kb program flash memory radio application level data transmission rates kb energy limitations force long term deployments operate low utilization 
mote limited resources application domains resources barely 
example great duck island deployment motes deep sleep minutes sensors second transmitted single data packet readings 
warm second cpu essentially idle 
motes awake time awake cpu cycles network bandwidth 
mote usually little awake activity nodes receive messages forward routing children link estimation updates neighbors 
mat designed implemented version mat tinyos pre nesc 
time dominant hardware platform rene mica just emerging kb ram kb program memory kbps software controlled radio 
mat predefined set events executes response 
ram constraints limited code particular event handler bytes long single packet 
order support network protocol implementations tiny amount space vm complex instruction set open assembly programming problematic compilation target 
requirements mat hard virtual native boundary prevents able support range programming models 
particular fails meet requirements flexibility mat vm concise programs designed single application domain 
provide support network processing runtime flexible customized wide range application domains 
supporting range application domains requires forms customization execution primitives vm instruction set set events executes response 
example data collection networks need execute response request forward packet collection tree suppression aggregation vehicle tracking network needs execute response receiving local broadcast neighbor 
concurrency introducing lightweight threading model top event driven tinyos mat provides greatly simplified programming interface enabling fine grained parallelism 
limited resources constrained application domain allowed mat address corresponding synchronization atomicity issues having single shared variable 
restriction suitable vms 
forcing explicit synchronization primitives programs increases length places onus correctness programmer may expert concurrency 
runtime manage concurrency automatically running handlers race free deadlock free allowing safe parallelism 
propagation mat handlers explicitly forward code forw instructions 
handler fit single packet instructions just simple broadcast 
hand explicit code forwarding allows user programs control propagation introducing additional flexibility requires program include propagation algorithms hard tune easy write incorrectly 
mat propagation data showed naive propagation policy easily saturate network rendering unresponsive wasting energy 
programming models fit programs single packet runtime needs able handle larger data images bytes provide efficient rapid propagation service 
prior trickle algorithm deals part propagation requirement proposing control algorithm quickly efficiently detect code updates needed 
propagation results assumed code fit single packet just broadcasts updates times 
leaves need asvm architecture 
protocol send code updates larger programs solution problem section 
provide useful systems support wide range programming models runtime meet requirements imposing large energy burden 
flexibility requires way build customized vms vm generator vm designed application domain 
section describes application specific virtual machine asvm architecture designed take step 

design shows asvm functional decomposition 
asvms major abstractions handlers operations capsules 
handlers code routines run response system events operations units execution functionality capsules units code propagation 
asvms threaded execution model stack architecture 
components asvm separated classes template asvm includes extensions application specific components define particular asvm 
template includes scheduler concurrency manager capsule store 
scheduler executes runnable threads fifo round robin fashion 
concurrency manager controls threads runnable ensuring race free deadlock free handler execution 
capsule store manages code storage loading propagating code capsules notifying asvm new code arrives 
building asvm involves connecting handlers operations template 
handler specific system event receiving packet 
event occurs handler triggers thread run code 
generally mapping handlers threads architecture require case 
concurrency manager uses interface bytecode instr parameter necessary primitives embedded operands operand instr opcode 
context executing thread 
command result execute uint instr context command uint nesc bytecode interface operations provide 
conservative flow insensitive context insensitive program analysis provide guarantees 
set operations asvm supports defines instruction set 
just mat instructions encapsulate split phase tinyos abstractions provide blocking interface suspending executing thread split phase call completes 
operations defined bytecode nesc interface shown commands execute 
thread issues instructions lets scheduler correctly control program counter 
currently asvms support languages tinyscript motlle sections 
kinds operations primitives language specific functions language independent 
distinction primitives functions important part providing flexibility 
asvm supports particular language including primitives compiles user tailors asvm particular application domain including appropriate functions handlers 
functions asvm correspondingly language asvms need minimal common data model 
additionally functions communication able support language specific data types knowing 
issues discussed section 
contrast primitives assume presence data types embedded operands 
example conditional jumps pushing constant operand stack primitives sending packet function 
rest section presents asvm data model core components template scheduler concurrency manager capsule store 
concludes example building asvm region programming 
data model asvm stack architecture 
thread operand stack passing data operations 
template provide program data storage operand stack facilities language specific correspondingly defined primi operation width name operand bits description rand rand random bit number pushc pushc push constant stack jumps jumps conditional jump table example operations rand function pushc jumps primitives 
tives 
architecture defines minimal set standard simple operand types bit values integers sensor readings defining useful language independent functions 
useful communication functions need elaborate types 
example function sends local broadcast packet needs able send data structures calling language provides 
function takes single parameter item broadcast program pushes operand stack invoking 
support kinds functions languages provide serialization support data types 
allows bcast implementation pop operand stack send serialized representation underlying command 
asvm receives packet converts serialized network representation back vm representation 
scheduler execution core asvm simple fifo thread scheduler 
scheduler maintains run queue interleaves execution fine granularity operations 
scheduler executes thread fetching bytecode capsule store dispatching corresponding operation component nesc parameterized interface 
parameter bit unsigned integer asvm support distinct operations top level dispatch 
scheduler issues instructions nesc interfaces selection implementation completely independent template top level instruction decode overhead constant 
primitives embedded operands cause take additional opcode values 
example primitive pushes bit constant operand stack bits embedded operand uses opcode slots 
primitives jump instructions need embedded operands longer bits 
primitives byte wide 
asvm generates instruction set know bits embedded operand operation 
similarly assembler transforms compiled assembly programs asvm specific opcodes know wide instructions 
operations follow naming convention width name operand width operand numbers name string 
width denotes bytes wide operation corresponds command bytecode interface operand bits embedded operand operation 
operation width field defaults operand field defaults zero 
table shows example operations 
language independence function primitive distinction determines operations called indirectly 
languages class functions function pointers program able invoke dynamically just statically instruction 
support functionality scheduler maintains function identifier function mapping 
allows functions invoked identifiers stored variables 
example motlle language calls function pushes function id operand stack issues instruction creates call stack frame invokes function level indirection 
concurrency manager parallelism handlers run response system events scheduler allows multiple handler threads run concurrently 
languages shared variables easily lead race conditions hard diagnose detect embedded devices 
common solution provide race free execution explicit synchronization written programmer 
explicit synchronization operations increase program size complexity costs energy ram increases chances month deployment scientist discovers collected data invalid trusted 
common case asvms need parallelism network traffic due limited ram available queuing 
handler blocking message send prevent handling message receptions presence shared wireless channel reason delay 
concurrency manager asvm template supports race free execution implicit synchronization handler operations 
operation component register concurrency manager compile time nesc wiring note accesses shared resource 
asvm installs new capsule concurrency manager runs conservative context insensitive flow insensitive analysis determine shared resources handler accesses 
registration concurrency manager entirely optional 
language prefers explicit synchronization operations declare shared resources concurrency manager limit parallelism 
hear newer version status fragment packet request receive complete capsule request timeout maintain respond hear older version status current asvm capsule propagation state machine 
handler event occurs handler implementation submits run request concurrency manager 
concurrency manager allows handler run exclusively access shared resources needs 
concurrency manager enforces phase locking starts executing handler thread hold resources may need release executes 
handler completes executes halt operation thread releases held resources 
releases execution explicit operations program 
thread accesses resource hold incorrectly released vm triggers error 
phase locking precludes deadlocks handlers run race free deadlock free 
new code arrives handler may variables inconsistent state 
waiting handler complete installing new capsule feasible update may example fix infinite loop bug 
new code arrives concurrency manager reboots asvm resetting variables 
implicit assumption synchronization model handlers short running routines hold resources long 
sensor network nodes typically low utilization generally case 
handler uses infinite loop call example block handlers indefinitely 
programming models languages prefer approach explicit synchronization described 
capsule store propagation field experience current sensor networks shown requiring physical contact cause node failures network programming critical 
asvms provide reliable code propagation 
mentioned earlier section mat explicit code forwarding mechanism problematic 
demonstrated trickle cost propagation low compared accompanying control traffic selective dissemination enables energy gains 
asvm template capsule store follows policy propagating new code node 
selective propagation asvms policy selective execution code nodes execute 
trickle suppression algorithm detecting nodes need code updates 
algorithm dynamically scales suppression intervals rapidly detect inconsistencies sends packets network consistent 
trickle define code propagates protocol greatly depends size data item 
deluge example transfers entire tinyos binaries uses cluster formation algorithm quickly propagate large amounts data 
mat virtual machine single packet programs propagation just simple local broadcast 
asvm programs extremes 
order packets long deluge heavy weight protocol simple broadcasts sufficient 
propagate code asvm capsule store maintains network independent instances trickle algorithm version packets contain bit version numbers installed capsules capsule status packets describe fragments mote needs essentially capsule fragments pieces capsule 
asvm states maintain exchanging version packets request sending capsule status packets respond sending fragments 
nodes start maintain state 
shows state transition diagram 
transitions prefer requesting responding node defer forwarding capsules thinks completely date 
type packet version capsule status capsule fragment separate network trickle 
example capsule fragment transmission suppress fragment transmissions version packets 
allows meta data data exchanges occur concurrently 
fragments means code propagates slow controlled fashion quickly possible 
significantly disrupt existing traffic prevents network overload 
show section asvm programs small propagate rapidly large multi hop networks 
building asvm building asvm scripting environment requires specifying things language functions handlers 
shows description file regionsvm asvm supports programming regions 
evaluate regionsvm versus native regions implementation section 
final handler line specifies asvm executes response event asvm boots reboots 
asvms include multiple handlers usually leads multiple threads regionsvm regions program vm name dir apps regionsvm language name tinyscript function name send function name mag function name cast function name id function name sleep function name function name function name function name function name function name function name handler name boot minimal description file regionsvm 
contains scripts asvm 
ming model single execution context includes runs vm reboots 
file generates tinyos source code implementing asvm java classes assembler uses map assembly asvm opcodes 
active sensor networking order support network processing asvms capable operating top range single hop multi hop protocols 
currently asvm libraries support concrete networking abstractions functions handlers single hop broadcasts toone routing aggregated collection routing regions 
experiences writing library asvm components protocols nesc statements including additional ones stable implementations emerge simple painless 

evaluation evaluate asvms efficiently satisfy requirements section concurrency propagation flexibility 
evaluate requirements examples microbenchmarks evaluate application level efficiency comparison alternative approaches 
microbenchmarks cycle counts mica node mhz bit microcontroller mega members mica family similar mcu faster clock rate mhz 
words bits memory access takes cycles bit architecture moving word pointer memory registers takes clock cycles 
concurrency measured overhead asvm concurrency control cycle counter mica mote 
table summarizes results 
values averaged operation cycles time lock unlock run analysis table synchronization overhead 
lock unlock acquiring releasing shared resource 
run moving thread run queue obtaining resources 
analysis full handler analysis 
mean std 
dev 
worst mote network vector packets sent status packets sent fragment packets sent total packets sent table propagation data 
mote motes experiments 
network times experiments time mote reprogram 
packets sent mote basis 
samples 
measurements asvm shared resources byte handler 
locking unlocking resources take order microseconds full program analysis shared resource usage takes millisecond approximately energy cost transmitting bits 
operations enable concurrency manager provide race free deadlock free handler parallelism low cost 
implicit concurrency management asvm prevent race condition bugs keeping programs short simple 
propagation evaluate code propagation deployed asvm mote testbed soda hall uc berkeley campus 
network topology approximately hops hops average node distance 
standard asvm propagation parameters 
injected byte fragment handler single node wired link 
repeated experiment times resetting nodes status version packets range second minutes redundancy constant 
fragments trickle suppression operate fixed window size second repeating twice redundancy constant 
request timeout seconds 
native regionsvm code flash kb kb data ram transmitted program kb table space utilization native regionsvm regions implementations bytes 
buffer packet packet packet light send packet tinyscript light pushc send asvm bytecodes tinyscript function invocation simple sense send loop 
operand stack passes parameters functions 
example scripting environment mapped variable packet buffer 
compiled program bytes long 
test restore trickle timers stable values 
table summarizes results 
average network reprogrammed seconds worst case seconds 
achieve rate node average transmitted packets total transmissions node network 
worst case node transmitted packets 
checking traces mote reprogram particular experiment suffered bad connectivity 
transmitted eleven version vectors nineteen status packets repeatedly telling nodes needed new code receiving 
parameters node stable network sends packets hour 
evaluate effect asvm code conciseness propagation efficiency compare cost native implementation proposed regions versus cost system regionsvm 
regions proposal users write short nesc programs single synchronous fiber compile tinyos binary 
reprogramming network involves propagating binary network 
regions compiles native tinyos code safety issues having protection boundary 
regions designed run simulator tinyos 
assumptions protocols available bandwidth prevent running motes precluded measuring energy costs empirically 
modifying configuration constants implementations share able compile measure ram utilization code size 
table shows results 
fiber stack accounts bytes native runtime ram overhead 
asvm doubles size tinyos image time cost wide range regions programs 
reprogramming native implementation requires sending total nineteen kilobytes reprogramming regionsvm implementation requires sending byte asvm handler size binary 
additionally handlers run sandboxed virtual environment benefit safety guarantees 
user decides particular networking abstractions asvm provides quite right new installed binary reprogramming 
deluge standard tinyos system disseminating binary images network 
reported experimental results network similar propagation experiments state disseminating kb takes transmissions disseminating kb native implementation take approximately transmissions 
contrast data table regionsvm program takes fewer transmissions cost providing safety 
tradeoff programs interpreted bytecodes native code imposing cpu energy overhead 
evaluate cost sections microbenchmarks application level comparison tinydb 
flexibility languages asvms currently support languages tinyscript motlle queries 
discuss section presenting queryvm 
tinyscript bare bones language provides minimalist data abstractions control structures 
basic imperative language dynamic typing simple data buffer type 
tinyscript dynamic allocation simplifying concurrency resource analysis 
resources accessed handler union resources accessed operations 
tinyscript mapping handlers capsules 
contains sample tinyscript code corresponding assembly compiles 
motlle mote language little extensions dynamically typed scheme inspired language syntax 
shows example heavily commented code 
main practical difference tinyscript richer data model motlle supports vectors lists strings class functions 
allows significantly complicated algorithms expressed asvm price accurate data analysis longer feasible mote 
preserve safety motlle serializes thread execution reporting concurrency manager handlers access shared resource 
motlle code transmitted single capsule contains handlers epoch set update update tree define timer handler timer handler result type send sends message tree encode encodes message epoch advances epoch snooped value may override send encode vector epoch id parent temp intercept snoop run node forwards overhears message 
intercept modify message aggregation 
fast forward epoch re snoop handler heard snoop msg intercept handler heard intercept msg heard msg decode bytes msg integer 
vector decode msg vector snoop epoch advances epoch needed snoop epoch simple data collection query motlle return node id parent routing tree temperature 
support incremental changes running programs 
flexibility applications built sample asvms regionsvm queryvm 
regionsvm designed vehicle tracking presents regions programming abstraction mpi reductions shared tuple spaces 
users write programs tinyscript regionsvm includes asvm functions basic regions library obtained regions source code authors 
shows regions pseudocode proposed welsh actual tinyscript code functionally identical invokes library functions 
nesc components regions library asvm functions approximately lines nesc code 
queryvm designed periodic data collection aggregated collection routing abstraction mentioned section 
queryvm provides programming interface similar tinydb presenting sensor network streaming database 
main extension sql sample period query repeated 
supports simple data collection aggregate queries select avg temperature interval measure average temperature network 
allow network processing reduce amount traffic sent aggregating nodes route data 
implementation compiles motlle code handlers aggregation collection tree library provides 
nice property ad location get location get nearest neighbors region nearest region create true reading get sensor reading store local data shared variables region reading key reading region reg key reading location region reg key reading location reading threshold id node max value max id region reduce op reading key am leader node 
max id id sum region reduce op sum reading key sum region reduce op sum reg key sum region reduce op sum reg key centroid sum sum centroid sum sum send basestation centroid sleep periodic delay regions pseudocode 
create nearest neighbor region reading int mag 
store local data shared variables reading reading reading reading threshold 
id node max value max id 
am leader node max id id sum sum sum buffer sum sum buffer sum sum send buffer sleep periodic delay tinyscript code regions pseudocode corresponding tinyscript 
pseudocode programming sensor networks regions tinyscript program right compiles bytes binary code 
dition queryvm supports writing new attributes network aggregates motlle 
contrast tinydb limited set attributes aggregates compiled binary 
efficiency microbenchmarks evaluation asvm efficiency series microbenchmarks scheduler 
compare asvms mat hand tuned monolithic implementation 
methodology mat measured bytecode interpretation overhead asvm imposes writing tight loop counting times ran seconds mica mote 
loop accessed shared variable involved lock checks concurrency manager 
asvm issue just instructions second mhz mica roughly cycles instruction 
asvm decomposition imposes overhead similar loop mat exchange handler instruction set flexibility race free deadlock free parallelism 
optimized interpreter cpu efficiency 
fact high level operations dominate program execution combined fact cpus sensor networks generally idle overhead acceptable decreasing course desirable 
example function regionsvm sends just pack operation script iteration sort time table execution time scripts milliseconds 
version sort operation version operation script version sorted script code 
ets asvm scripting overhead approximately cpu cycles energy overhead 
cost cycles bytecode means implementing complex mathematical codes asvm inefficient application domain needs significant processing include appropriate operations 
obtain insight tradeoff including functions writing operations script code wrote scripts 
script loop fills array sensor readings 
second script fills array sensor readings sorts array operation insertion sort 
third script insertion sorts array tinyscript operation 
measure execution time script placed iteration loop sent uart packet script start 
table shows results 
sorting array script code takes times long sorting operation dominates script execution time 
interpretation inefficient pushing common initialise operator fn bits vector bits update operator result get fn val update return average bits attr exponentially decaying average operator motlle 
name simple select id parent temp interval conditional select id humidity parent interval select avg temp interval table queries evaluate data collection implementations 
tinydb directly support time aggregates tinydb omit aggregate 
sive operations native code functions minimizes amount interpretation 
section shows flexible boundary combined low duty cycle common sensor networks leads interpretation overhead negligible component energy consumption wide range applications 
efficiency application queryvm motlle asvm designed support execution data collection queries 
compiler generates motlle code queries shown table generated code responsible timing data collection message layout process aggregate data hop routing tree 
code essentially generated simple query 
users write new attributes operators snippets motlle code 
instance shows lines motlle code add exponentially decaying average operator example table uses 
tree topology queryvm tinydb experiments 
square node tree root 
size bytes energy mw yield query tinydb vm tinydb vm tinydb vm simple conditional table query size power consumption yield tinydb queryvm 
yield percentage expected results received 
query results notion time epoch 
epoch numbers logical time scheme included query results help support aggregation 
queryvm includes functions handlers support multi hop communication epoch handling aggregation 
queryvm programs tree collection layer tinydb uses 
queryvm includes epoch handling primitives avoid replicating epoch handling logic program see usage 
temporal spatial nodes averaging logic readily expressed motlle including common aggregates queryvm reduces program size increases execution efficiency 
evaluate queryvm efficiency comparing power draw tinydb system queries shown table 
reflect power draw real deployment enabled low power listening implementations 
low data rate networks periodic data collection low power listening greatly improve network lifetime 
level utilization packet length important determinant energy consumption matched size routing control packets queryvm tinydb 
tinydb query result packets approximately bytes longer queryvm mica motes means tinydb spend extra packet received packet sent 
ran queries network mica motes spread ceiling office building 
motes mts weather board crossbow technologies 
environmental changes dynamically alter adhoc routing trees choosing link link changing forwarding pattern greatly affecting energy consumption 
sorts changes experimental repeatability fair comparisons unfeasible 
static stable tree experiments provide basis comparison implementations 
obtained tree running routing algorithm hours extracting parent sets explicitly setting node parents topology shown 
experiments run adaptive trees consistent results 
measured power consumption mote single child physically close root multihop network 
power reflects mote overhears lot traffic sends relatively messages power draw mw simple conditional tinydb synch stagger queryvm power consumption tinydb queryvm nesc implementations 
synch nesc implementation nodes start time 
stagger nodes start times staggered 
common case 
queries node sends data packet seconds routing protocol sends route update packet epochs seconds 
measured average power draw instrumented node intervals seconds sampling hz instantaneous samples 
table presents results experiments 
sample queries queryvm consumes energy tinydb 
believe improvement fundamental approaches 
differences yield mean measured mote overhearing different numbers messages increases queryvm power draw 
conversely having larger packets increases tinydb power draw packet cost estimate cost mw depending query measured mote hears distant motes 
factors shown experiments native tinyos implementation queries 
ran native implementations scenarios 
scenario booted nodes time operation closely synchronized 
second staggered node boots second sampling interval 
shows power draw scenarios alongside tinydb queryvm 
synchronized case yields native implementations varied staggered case yields 
results show details timing transmissions major effects yield power consumption 
separate networking effects basic system performance section repeats experiments node network 
efficiency interpretation node experiments measured mote executes query sends results second mote passive base station 
measured node forward packets contend transmitters power draw mw simple conditional tinydb nesc queryvm average power draw measurements node network 
conditional query monitored node parent sends packets 
error bars standard deviation samples 
energy consumption cost query execution reporting 
extra cost sending tinydb larger result packets negligible mw extra average power draw 
ran experiments longer full network ones intervals length seconds minutes measured intervals hours 
results show queryvm energy performance improvement tinydb 
asvm reusable software components common template hand coded vertically integrated system queryvm imposes energy burden deployment 
practice power draw real network dominated networking costs queryvm mw advantage give longer lifetime power draws 
determine queryvm power goes compared hand coded tinyos programs 
program process query just listened messages handled system timers 
allows distinguish cost executing query underlying cost system 
nesc implementations queries 
allow distinguish cost executing query overhead asvm runtime imposes 
basic system cost mw 
shows comparison queryvm hand coded nesc implementation query 
queries cost mw cost asvm negligible 
negligible cost surprising instance conditional query queryvm executes instructions sample period consume approximately ms cpu time 
mica node cpu power draw mw due external oscillator platforms draw mw corresponds average power cost 
node network cost snooping node results increase power draw 
queryvm sends viral code maintenance messages min steady state corresponding average power draw 
results table power consumption mw pair aa batteries mah approximately thirds usable mote days 
lowering sample rate seconds reasonably high rate optimizations believe lifetimes months readily achievable 
additionally energy cost asvm interpretation negligible portion system energy budget 
suggests asvm active sensor networking realistic option long term low duty cycle data collection deployments 

related mat virtual machine forms basis asvm architecture 
asvms address mat main limitations flexibility concurrency propagation 
proposal programming nodes interpreter proposes tcl scripts 
devices designed ipaqs megabytes ram verbose program representation node tcl interpreter acceptable overheads mote 
sos sensor network operating system supports dynamic native code updates loadable module system 
allows small incremental binary updates requires levels function call indirection 
sos sits extremes tinyos asvms propagation cost tinyos greater asvms execution overhead greater tinyos asvms 
native code achieve middle ground sos provide safety guarantees asvm 
sos approach suggests ways asvms dynamically install new functions 
impala middleware system sos allows users dynamically install native code modules 
sos allows modules call kernel invoke impala limits modules kernel interfaces 
asvms interfaces event driven bear degree similarity mat 
asvms impala provide general mechanisms change triggering events designed particular application domain zebranet 
customizable extensible abstraction boundaries asvms provide long history operating systems research 
systems scheduler activations show allowing applications cooperate runtime rich boundaries greatly improve application performance 
operating systems exokernel spin take aggressive approach allowing users write interface improve performance increased control 
sen sor networks performance general goal bandwidth operations second rarely primary metric low duty cycles resources plentiful 
robustness energy efficiency important metrics 
ants plan smart packets example systems bring active networking internet 
networks dynamically programmable system different goals research foci 
ants focuses deploying protocols network planet explores dealing security issues language design smart packets proposes active networking management tool 
ants uses java planet smart packets custom languages plan respectively 
internet communication resource model design decisions systems jvm unsurprisingly suited mote networks 
distinguishing characteristic sensor networks lack strong boundaries communication sensing computation 
internet data generation province points sensor networks node router data source 
initial mote deployment experiences demonstrated need simple network programming models higher level abstraction node tinyos code 
led variety proposals including tinydb sql queries diffusion aggregation regions mpi reductions market 
define programming model asvms provide way implement build runtime underlying whichever model user needs 

discussion section showed asvm effective way efficiently provide high level programming abstraction users 
means way 
obvious approaches standard virtual machine java sending lightweight native programs 
language java may suitable way program sensor network believe efficient implementation require simplifying removing features reflection 
java card taken approach essentially designing asvm smart cards supports limited subset java different program file formats 
java card supports single application domain provide guidance asvm support java language 
native code possible solution bytecode programs native code series library calls 
sensor mote cpus usually idle benefit native code provides efficient cpu utilization minimal programming layer sql queries data parallel operators scripts expressivity simplicity transmission layer application specific vm bytecodes efficiency safety execution layer nesc binary code changed rarely optimizations resource management hardware layered decomposition situ reprogramming 
user wants write complex mathematical codes 
asvm model codes written nesc exposed scripts functions 
additionally native code poses complexities difficulties greatly outweigh minimal benefit including safety conciseness platform dependence 
sos operating system suggests ways asvms support dynamic addition new functions 
asvms share high level goal active networking dynamic control network processing 
sort processing proposed systems ants planet different see sensor nets 
routing nodes active internet process data edge systems predominantly responsible generating data 
correspondingly active networking focused protocol deployment 
contrast motes simultaneously play role router data generator 
providing service edge applications active sensor nodes application 
section showed asvm queryvm simultaneously support sql queries motlle programs compiling shared instruction set 
addition energy efficient similar tinydb system queryvm flexible 
similarly regionsvm benefits code size concurrency safety native regions implementation 
believe advantages direct result asvms decompose programming distinct layers shown 
highest layer code user writes tinyscript sql 
middle layer active networks representation program takes propagates asvm bytecodes 
final layer representation program takes executes mote asvm 
tinydb combines top layers programs binary encodings sql query 
forces mote parse interpret query determine actions take different events coming system 
trades flexibility execution efficiency propagation efficiency 
separating programming layer transmission layer queryvm leads greater program flexibility efficient execution 
regions combines bottom layers programs tinyos images 
tinyos concurrency model virtual limits native regions implementation single thread 
additionally programs lines long compiling bytes regionsvm compiling tinyos image programs tens kilobytes long trading propagation efficiency safety execution efficiency 
separating transmission layer execution layer regionsvm allows high level abstractions minimize execution overhead provides safety 

constrained application domains sensor networks mean programs represented short high level scripts 
scripts control protocols abstractions domain requires motes generate data network processing perform 
vision papers existing proposals sensor network programming indicate approach exception systems rule 
pushing processing close data sources possible transforms sensor network active sensor network 
sensor networks specialized exact form active sensor networking takes open question question single answer 
propose particular active networking system useful circumstances proposed application specific virtual machines easily sensor network active described architecture building 
sample vms different applications programming models show architecture flexible efficient 
efficiency stems flexibility virtual native boundary allows programs concise 
conciseness reduces interpretation overhead cost installing new programs 
programming motes hard common claim sensor network community just programming wrong interface 
supported part defense department advanced research projects agency national science foundation nsf iis california micro program intel 
research infrastructure provided national science foundation eia 

acharya saltz 
active disks programming model algorithms evaluation 
asplos viii proceedings eighth international conference architectural support programming languages operating systems pages 
acm press 
amir mccanne katz 
active service framework application real time multimedia transcoding 
sigcomm proceedings acm sigcomm conference applications technologies architectures protocols computer communication pages 
acm press 
anderson bershad lazowska levy 
scheduler activations effective kernel support user level management parallelism 
acm transactions computer systems february 
bershad savage pardyak sirer becker fiuczynski chambers eggers 
extensibility safety performance spin operating system 
proceedings th acm symposium operating systems principles sosp 

han srivastava 
design implementation framework efficient programmable sensor networks 
proceedings international conference mobile systems applications services mobisys 
gay levis von behren welsh brewer culler 
nesc language holistic approach networked embedded systems 
sigplan conference programming language design implementation pldi june 
girod ramanathan elson estrin osterweil 
system simulation emulation deployment heterogeneous sensor networks 
sensys proceedings nd international conference embedded networked sensor systems pages 
acm press 

han kumar shea kohler srivastava 
dynamic operating system sensor nodes 
mobisys proceedings rd international conference mobile systems applications services 
hicks moore gunter nettles 
plan packet language active networks 
proceedings international conference functional programming icfp 
hicks moore alexander gunter nettles 
planet active internetwork 
proceedings ieee infocom 
hui culler 
dynamic behavior data dissemination protocol network programming scale 
proceedings second international conferences embedded network sensor systems sensys 
intanagonwiwat govindan estrin 
directed diffusion scalable robust communication paradigm sensor networks 
proceedings international conference mobile computing networking aug 
juang oki wang martonosi rubenstein 
energy efficient computing wildlife tracking design tradeoffs early experiences zebranet 
proceedings acm conference architectural support programming languages operating systems asplos oct 
kaashoek engler ganger brice hunt mazi res grimm jannotti mackenzie 
application performance flexibility exokernel systems 
proceedings th acm symposium operating systems principles sosp october 
levis culler 
mat tiny virtual machine sensor networks 
proceedings acm conference architectural support programming languages operating systems asplos oct 
levis lee welsh culler 
simulating large wireless sensor networks tinyos motes 
proceedings acm conference embedded networked sensor systems sensys 
levis patel culler shenker 
trickle self regulating algorithm code maintenance propagation wireless sensor networks 
usenix acm symposium network systems design implementation nsdi 
liu martonosi 
impala middleware system managing autonomic parallel sensor systems 
ppopp proceedings ninth acm sigplan symposium principles practice parallel programming pages 
acm press 
madden franklin hellerstein hong 
tinydb query processing system sensor networks 
transactions database systems tods 
madden franklin hellerstein hong 
tag tiny aggregation service ad hoc sensor networks 
proceedings acm symposium operating system design implementation osdi dec 
kang parkes welsh 
virtual markets program global behavior sensor networks 
proceedings th acm sigops european workshop leuven belgium 
polastre hill culler 
versatile low power media access wireless sensor networks 
proceedings second acm conferences embedded networked sensor systems sensys 
mer frank marr becker 
generic role assignment wireless sensor networks 
proceedings th acm sigops european workshop leuven belgium 
schwartz jackson strayer zhou rockwell partridge 
smart packets applying active networks network management 
acm computer systems 
sharp woo sastry karlof sastry culler 
design implementation sensor network system vehicle tracking autonomous interception 
proceedings second european workshop wireless sensor networks 
szewczyk polastre mainwaring culler 
analysis large scale habitat monitoring application 
proceedings second acm conference embedded networked sensor systems sensys 
tennenhouse wetherall 
active network architecture 
computer communication review 
welsh 
programming sensor networks regions 
usenix acm symposium network systems design implementation nsdi 
woo tong culler 
taming underlying challenges reliable multihop routing sensor networks 
proceedings international conference embedded networked sensor systems pages 
acm press 
