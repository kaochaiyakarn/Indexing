training conditional random fields gradient tree boosting thomas dietterich tgd cs orst edu adam engr orst edu cs orst edu school electrical engineering computer science oregon state university corvallis usa conditional random fields crfs lafferty mccallum pereira provide flexible powerful model learning assign labels elements sequences applications part speech tagging speech mapping protein dna sequence analysis information extraction web pages 
existing learning algorithms slow particularly problems large numbers potential input features 
describes new method training crfs applying friedman gradient tree boosting method 
tree boosting crf potential functions represented weighted sums regression trees 
regression trees learned stage wise optimizations similar adaboost objective maximizing conditional likelihood crf model 
growing regression trees interactions features introduced needed parameter space potentially immense search algorithm explicitly consider large space 
result gradient tree boosting scales linearly order markov model order feature interactions exponentially previous algorithms iterative scaling gradient descent 

applications machine learning involve assigning labels sequences objects 
example part speech tagging task assign part speech noun verb word appearing proceedings st international conference machine learning banff canada 
copyright authors 
sentence ratnaparkhi 
protein secondary structure prediction task assign secondary structure class amino acid residue protein sequence qian sejnowski 
call class problems sequential supervised learning ssl formalized follows set training examples form xi yi xi xi xi ti sequence ti feature vectors yi yi yi ti corresponding sequence class labels yi 
find classifier new sequence feature vectors predicts corresponding sequence class labels accurately 
famous ssl problem nettalk task pronouncing english words assigning phoneme stress letter word sejnowski rosenberg 
early attempts apply machine learning ssl problems sliding windows 
predict label yt sliding window method uses features drawn window sequence 
example element window wt features xt xt xt xt xt 
sliding windows convert ssl problem standard supervised learning problem ordinary machine learning algorithm applied 
ssl problems correlations successive class labels yt 
example part speech tagging adjectives tend followed nouns 
protein sequences alpha beta structures involve multiple adjacent residues 
correlations exploited increase classification accuracy 
new learning methods developed goal capturing correlations 
see dietterich review 
interesting new methods conditional random field crf proposed lafferty 

crf probabilistic model conditional probability input sequence produce output label sequence 
crf form markov random field geman exp yt yt yt yt yt yt functions capture respectively degree yt compatible degree yt compatible transition yt potential functions arbitrary realvalued functions 
exponential function ensures positive normalizing constant exp ensures sums 
representation completely general subject assumption besag hammersley clifford 
normally assumed potential functions depend adopt assumption 
apply crf ssl problem choose representation functions 
lafferty studied functions weighted combinations binary features yt aga yt yt yt yt yt trainable weights features ga fb boolean functions 
example part speech tagging yt xt word bank yt class noun 
sliding window methods natural define features depend sliding window wt ofx values 
linear parameterization seen extension logistic regression sequential case 
parameterization chosen crf trained maximize log likelihood training data possibly regularization penalty prevent overfitting 

denote tunable parameters model 
objective function maximize log yi xi log xi exp yi xi yi yi xi yi xi yi yi xi log xi aga yi xi yi yi xi log xi lafferty introduced iterative scaling algorithm maximizing reported exceedingly slow 
groups implemented gradient ascent methods naive implementations slow various parameters interact increasing parameter may require compensating changes 
mccallum mallet system mccallum employs bfgs algorithm approximate secondorder method deals parameter interactions 
drawback linear parameterization assumes feature independent contribution potential functions 
course possible define features capture combinations basic features leads combinatorial explosion number features dimensionality optimization problem 
example protein secondary structure prediction qian sejnowski residue sliding window gave best results neural network methods 
basic fb features defined window 
consider fourth order conjunctions features obtain features 
obviously infeasible 
mccallum mallet system starts single constant feature introduces new feature conjunctions conjunctions basic features features model 
candidate conjunctions evaluated incremental impact objective function 
demonstrates significant improvements speed classification accuracy compared crf includes basic features 
introduce different approach training potential functions gradient tree boosting algorithm 
approach potential functions represented sums regression trees grown stage wise manner adaboost freund schapire 
regression tree viewed defining new feature combinations corresponding path tree root leaf 
resulting potential functions form linear combination features features quite complex 
advantage gradient boosting approach algorithm fast straightforward implement 
addition may tendency avoid overfitting ensemble effect combining multiple regression trees 

gradient tree boosting suppose wish solve standard supervised learning problem training examples form xi yi yi 
wish fit model form exp gradient tree boosting idea functional gradient ascent 
ordinary gradient ascent parameterize way example linear function aga 

represent tunable parameters function 
gradient ascent fitted parameter vector iteration initial parameter vector series gradient ascent steps computed step direction gradient log likelihood function log yi xi parameter controls step size 
functional gradient ascent general approach 
assuming linear parameterization just assumes represented weighted sum functions computed functional gradient log ex functional gradient indicates function change order increase true log likelihood possible points 
unfortunately know joint distribution evaluate expectation ex set training examples sampled joint distribution compute value functional gradient training data points yi xi log yi xi 
point wise functional gradients define set functional gradient training examples xi yi yi xi train function hm approximates yi xi 
specifically fit regression tree hm minimize hm yi xi yi xi take step direction fitted function hm 
fitted function hm exactly desired point general direction assuming training examples 
ascent direction hm approximate true functional gradient ascent 
key thing note approach replaces difficult problem maximizing log likelihood data simpler problem minimizing squared error set training examples 
friedman suggests growing hm version cart algorithm breiman stopping regression tree reaches preset number leaves overfitting controlled tuning internal cross validation 

gradient tree boosting ssl principle straightforward apply functional gradient ascent ssl 
need represent train yt yt yt sums regression trees 
historical reasons took slightly different approach 
yt yt yt yt yt function computes desirability label yt values label yt input features functions label crf form exp yt yt 
compute functional gradient log respect yt yt 
table 
derivation functional gradient log wd wd yt yt wt log log yd yd wd yd yd wd yd yd wd exp wd yd yd exp wd yd yd yd yd simplify computation replace wt window sequence centered xt 
assume loss generality window unique occurrence wt sequence proposition functional gradient log respect wd log wd yd yd yd yd wd yd yd transition observed position position sequence yd yd wd predicted probability transition current potential functions 
demonstrate proposition introduce forward backward algorithm computing 
assume yt takes value 
define forward recursion exp exp wt 
define backward recursion exp wt variables iterate possible class labels 
normalizer computed position 
unroll recursion step write exp wt table shows derivation functional gradient 
line exactly yt yt wt terms match wd wd unique 
term derivative represent indicator function yd yd 
line expand position forward backward algorithm 
wd unique product give non zero derivative gives line 
right hand expression precisely joint probability yd yd wd occurs match contributes separately functional gradient 
functional gradient satisfying interpretation error probability scale 
transition observed training example predicted probability order maximize likelihood 
transition observed predicted probability 
functional gradient ascent simply involves fitting regression trees residuals 
table shows pseudo code tree boosting algorithm 
potential function class initialized zero 
iterations boosting executed 
iteration class set functional gradient training examples generated 
example consists window wt xi input sequence possible class label time target value 
regression tree hav table 
gradient tree boosting ssl treeboost data data xi yi class initialize class data hm hm return treeboost data example execute forward backward algorithm xi yi get ti yi yi xi exp wt xi xi yi yi yi yi xi insert wt xi return ing leaves fit training examples produce function hm 
function added previous potential function produce function 
words setting step size 
experimented performing line search point optimize expensive 
rely self correcting property tree boosting correct overshoot iteration 
way improve algorithm initialize potential functions intelligently 
pseudolikelihood yt isp yt yt yt 
probability correct label position correct labels yt yt 
pseudo likelihood computed performing forwardbackward iterations yt yt yt exp yt yt wt yt yt wt exp yt wt yt wt pseudo likelihood assumes correct labels known yt yt works eventual error rate small 
significantly sped training trials 
known consistent estimator likelihood besag 
perform iterations gradient tree boosting pseudo likelihood compute boosting examples 
switch full functional gradient 
sets generated examples large 
example classes training sequences length number training examples class 
regression tree algorithms fast consider training examples 
friedman suggests tricks speeding computation sampling influence trimming 
sampling random sample training data training 
influence trimming data points values close zero ignored 
apply techniques experiments 

making predictions crf model trained possible ways define classifier making predictions 
predict entire sequence highest probability argmax 
sense applications part ofspeech tagging goal coherent sequential prediction 
computed viterbi algorithm rabiner advantage need compute normalizer 
second way predictions individually predict yt ht argmax yt concatenate individual predictions obtain 
sense applications goal maximize number individual yt correctly predicted resulting predicted sequence incoherent 
example predicted sequence parts speech grammatically legal maximize number individual words correctly classified 
yt executing forward backward algorithm yt yt yt 
experimental studies implemented gradient tree boosting crfs compared mccallum mallet system benchmark data sets 
call algorithm 
viterbi predictions fb forward backward predictions 
mallet implements mccallum feature induction algorithm 
mallet predictions viterbi algorithm denote mallet 
data sets tested algorithms data sets protein secondary structure prediction usenet faqs ai general ai neural aix 
protein secondary structure benchmark published qian sejnowski 
protein consists sequence amino acid residues 
residue represented single feature possible values corresponding standard amino acids 
classes alpha helix beta sheet coil 
training set sequences test set sequences 
faq data sets consists frequently asked questions files usenet newsgroup mccallum 
faqs newsgroup divided separate files ai general files ai neural files aix files 
line faq labeled part header question answer part tail 
xt consists line faq file corresponding yt header question answer tail 
measure accuracy number individual lines correctly classified 
mccallum provided definitions features 
slight correction features results directly comparable 
newsgroup performance measured leave cross validation crf trained files tested remaining file 
repeated file results averaged 
mallet parameters set user 
algorithms user set window size order markov model number iterations train 
additional parameter depth limit regression trees 
mallet parameters regularization penalty squared weights called variance number iterations feature inductions kept con percent residues correct qian sejnowski mallet iterations 
protein secondary structure prediction stant number features add feature induction kept constant true label probability threshold kept constant training proportions kept constant number iterations train 
variance kept mallet parameters fixed values recommended andrew mccallum personal communication 
set remaining parameters manually tried handful settings chose setting gave best test set cross validation performance 
ideally set internal cross validation 
perform careful search parameter settings believe parameters highly tuned 

results shows results protein task 
cases qian sejnowski order crf employed 
input features consisted residue sliding window 
fb attains peak performance correct iterations 
best method neural network sliding window qian sejnowski attains 
mallet reaches iterations 
mcnemar test comparing peak performance fb mallet shows difference statistically significant 
worrying aspect mallet performance curve exhibits high degree fluctuation 
presumably due effect introducing new features 
find optimal stopping point avoiding overfitting 
peak performance achieved iteration 
second highest performance table 
training iteration run time seconds percent lines correct window size cpu seconds mallet 
faq ai general 
percentage lines correct function cpu time 
realistic estimate achievable performance cross validation determine stopping point 
difficult compare cpu time methods written mallet written java 
despite differences running times programs quite similar 
time required reach peak performance time required mallet reach peak performance residue window feasible run problem 
table compares cpu time iteration smaller window sizes 
see faster small window sizes slows exponentially window size grows 
plots percentage lines correctly classified algorithms ai general faq 
see mallet performance fluctuates wildly 
mcnemar test performance final iterations methods concludes better 
plots results ai neural faq 
time despite fluctuations mallet converges better classifier mcnemar test 
percent lines correct cpu seconds mallet 
faq ai neural 
percentage lines correct function cpu time percent lines correct mallet cpu seconds 
faq aix 
percentage lines correct function cpu time plots results aix faq 
difficult see graph mallet converges slightly better classifier 
note data set required twice time reach peak performance 

introduced novel method training conditional random fields gradient tree boosting 
evaluate dimensions 
ease implementation simpler implement mallet 
ease tuning introduces tunable parameter maximum number leaves permitted regression tree 
mallet parameters consider 
mallet performance fluctuates wildly improves smoothly 
scaling large numbers features tree boosting scales better original crf method 
appears match mallet gives dramatic speedups potential features 
run time experiments required run time factor mallet 
reasonable 
accuracy experiments accurate data sets accurate data sets 
scaling large numbers classes experiments shown attempted apply nettalk text speech problem classes 
infeasible cost performing forward backward algorithm required mallet compute gradients scales wheret length sequences order markov model 
nettalk previous research suggested 
means forwardbackward computation training sequence requires operations means slow 
important challenge ssl research develop methods handle large numbers classes 
gradient tree boosting may provide advantage methods standard parametric gradient ascent ability handle missing values inputs 
methods handling missing values growing regression trees including surrogate split method cart breiman instance weighting method quinlan 
evaluate methods training evaluating crfs 
authors gratefully acknowledge support nsf iis iis 
besag 

spatial interaction statistical analysis lattice systems 
journal royal statistical society 
besag 

efficiency pseudolikelihood estimation simple gaussian fields 
biometrika 
breiman friedman olshen stone 

classification regression trees 
wadsworth international group 
dietterich 

machine learning sequential data review 
structural syntactic statistical pattern recognition pp 

new york springer verlag 
freund schapire 

experiments new boosting algorithm 
icml pp 

morgan kaufmann 
friedman 

greedy function approximation gradient boosting machine 
annals statistics 
geman 

random fields inverse problems imaging 
ancona geman ikeda eds cole de xviii lecture notes mathematics 
berlin springer verlag 
hammersley clifford 

markov fields finite graphs lattices technical report 
unpublished 
lafferty mccallum pereira 

conditional random fields probabilistic models segmenting labeling sequence data 
icml pp 

san francisco ca morgan kaufmann 
mccallum 

efficiently inducing features conditional random fields 
uai pp 

san francisco ca morgan kaufmann 
mccallum freitag pereira 

maximum entropy markov models information extraction segmentation 
icml pp 

morgan kaufmann san francisco ca 
qian sejnowski 

predicting secondary structure globular proteins neural network models 
mol 
bio 
quinlan 

programs empirical learning 
san francisco ca morgan kaufmann 
rabiner 

tutorial hidden markov models selected applications speech recognition 
proc 
ieee 
ratnaparkhi 

maximum entropy model part speech tagging 
proceedings conference empirical methods natural language processing pp 

somerset nj acl 
sejnowski rosenberg 

parallel networks learn english text 
complex systems 
