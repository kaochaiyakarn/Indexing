high performance java middleware real application fabrice huet vrije universiteit amsterdam netherlands cs vu nl denis inria cnrs sophia antipolis france denis inria fr henri bal vrije universiteit amsterdam netherlands bal cs vu nl previous experiments high performance java initially disappointing 
years optimization investigates current suitability object oriented middleware high performance grid programming 
middleware offering high level abstractions proactive replaced standard java rmi layer optimized ibis rmi interface 
ibis grid programming environment featuring efficient communications 
electromagnetic application object oriented time domain finite volume solver maxwell equations conducted benchmarks single clusters including comparisons application fortran mpi 
grid experiments conducted simultaneously different clusters 
reports extremely promising results 
instance speed machines vs fortran speedup machines grid 
falls trend building high performance middleware high levels abstractions especially context objects java language 
performance issues related implementation rmi reported solutions proposed 
pioneered fully integrated middleware 
important improvement achieved 
solution proposed goes step 
grid programming environment featuring optimized communication layer ibis 
portable java native protocols proposed ibis advantage portable layer ibis portability layer ipl 
second element experiment high level environment grid proactive featuring object oriented group communications interactive xml deployments 
middleware considered application level tools hide low level details user 
services provided globus grid middlewares 
environment possible develop real demanding application object oriented solver maxwell equations 
computational program relies time permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage copies bear notice full citation page 
copy republish post servers redistribute lists requires prior specific permission fee 
copyright ieee deployment rmi serialization communication tcp udp mpi panda gm grid monitoring application security groups mobility components proactive runtime gmi satin ibis portability layer ipl topology discovery resource management nws gram native code pure java code information service gis 
global architecture proactive ibis proactive domain finite volume method 
reports experiment single cluster comparing performance speed java version equivalent program fortran mpi 
multi clusters benchmarks executing das grid 
past mpi bindings java proposed alternative solution benefit object paradigm high performance achievements 
clear approach experimented mpi binding just replacing rmi layer optimized consider fully portable solution 
previous focused ease deployment usability 
authors uses modified tuple space offer approach various advantages class downloading platform independence java 
tuple space paradigm offers imperative control 
section presents middleware architecture proactive functionality ibis optimizations integration 
section provides quick overview application focusing properties needed understand parallel performances 
section details experiments 
analyzing application memory usage compare execution proactive rmi proactive ibis fortran mpi single cluster 
grid experiments multi clusters reported 
building high level high performance middleware writing new middleware offer high level abstraction providing high performance decided stack existing respective records fields expertise 
main question result shown meet expectations 
remaining refer rmi sun jdk implementation rmi ibis ibis rmi implementation 
ibis proactive library application security groups mobility components proactive runtime rmi jini xml comm layer 
application running top proactive proactive java middleware parallel distributed concurrent programming features high level services weak migration group communication security deployment components 
current implementation different communication layers rmi jini environment discovery xml protocol 
base model distributed concurrent application built proactive composed number entities called active objects 
active object thread control decides order serve incoming method calls automatically stored queue pending requests 
method calls sent active objects asynchronous transparent objects synchronization handled mechanism known wait necessity 
active object running jvm belong node provides abstraction physical location 
time jvm hosts nodes 
group communications group communication mechanism achieves asynchronous remote method invocation group remote objects automatic gathering replies 
java class initiate group communications standard public methods class classical dot notation typed group communications 
furthermore groups automatically constructed handle result collective operations providing elegant effective way program gather operations 
standard java class example typical group creation group type created member created specified nodes specify parameters passed constructor objects object params 
ag newgroup params node node elements dynamically included typed group class equals extends class specified group creation 
note allow handle polymorphic groups 
example object class extending included group type java typing methods defined class invoked group 
method invocation group syntax similar standard method invocation ag foo group communication call asynchronously propagated members group parallel multithreading 
proactive basic model method call group non blocking provides transparent object collect results 
method call group yields method call group members 
important specificity group mechanism result typed group communication group 
result group transparently built invocation time elementary reply 
dynamically updated incoming results gathering results 
wait necessity mechanism valid groups replies caller blocks soon reply arrives result group method call result executed 
instance vg ag bar method call group result 
vg typed group vg collective operation subject wait necessity new method call automatically triggered soon reply call ag bar comes back group vg dynamically formed 
instruction vg completes called members 
deployment descriptors deployment descriptors provide mean source code application software hardware configuration 
provides integrated mechanism specify external processes launched way 
goal able deploy application having change source code necessary information stored xml descriptor file 
application deployment descriptors access api enabling queries runtime environment informations number nodes available 
deployment file parts 
declare nodes name source code application 
second part mapping describes virtual nodes mapped virtual machines 
infrastructure part describe virtual machines created 
mapping vm vm infrastructure vm local virtual machine vm ssh host local virtual machine 
simple deployment descriptor example deployment file 
sake clarity pseudocode syntax readable xml 
indicated italic symbolic names file 
names structure descriptor arbitrary value 
bold actual classes provided proactive 
application file able symbolic name source code access resources 
virtual node mapped virtual machines vm vm specified infrastructure part 
creation virtual machines follows 
created locally 
second trigger ssh connection host perform creation new local virtual machine 
part possible specify various environment variables classpath creation virtual machine 
ibis library ibis java grid programming environment allows efficient communication standards techniques optimized solutions special cases 
networks protocol provided jvm tcp ip udp rely low level protocols high speed interconnect gm available 
core library ibis portability layer ensures interfacing high level programming models low level communication protocols services 
possible write applications source compatible rmi implementation models group method invocation gmi divide conquer parallel programming satin 
noted previous order increase performance rmi possible different parts rmi protocol serialization 
current sun implementation rmi resends type information remote call ibis caches information connection basis reducing amount data sent network 
optimizations performed order reduce cost object serialization 
standard java serialization uses reflection convert objects byte sent wire 
possible avoid overhead generating serialization code class serialized moving cost runtime compile time 
generation performed bytecode rewriter require source code application 
second rebuilding object graph stream create objects calling user defined constructors avoid side effects 
standard java task performed private native call turns expensive standard new 
ibis associates serializable class generator class role create objects invoking specially generated constructor greatly reducing object creation overhead 
focus java version ibis order retain portability 
integration key decisions design proactive independent communication layer 
issues need tackled 
remote objects dependent communication layer 
application point view lower extent middleware point view communicating ibis rmi change source code 
second creation remote handled way fully transparent application middleware 
communication related parts remote isolated proxy pattern shielding parts communication code exception handling left 
creation handled factory pattern 
obtaining remote requires requesting factory implementation loaded requested communication protocol right 
enables delay runtime choice communication layer 
consequence switching rmi ibis communications require source code modification simply setting java property 
remote sun rmi soap ibis remote object sun rmi concrete factory factory xml concrete factory 
remote concrete factories computational application ibis concrete factory order demonstrate performance middleware conducted extensive experiments parallel object oriented numerical simulation tool applications 
written top proactive benefit high level features deployment group communication 
section review architecture application basics principle 
carefully study distribution drawing basic properties useful understanding parallel behavior 
overview numerically solves maxwell equations modeling time domain electromagnetic wave propagation phenomena 
relies finite volume approximation method designed deal unstructured tetrahedral discretization computation domain see details 
standard test case exact solution maxwell equations exists allowing precise validation regards numerical kernels parallelization aspects consists simulation propagation eigenmode cubic metallic cavity 
test case underlying tetrahedral mesh automatically built pre processing phase typical run 
mesh simply obtained defining cartesian grid discretization cube dividing element grid tetrahedra 
ongoing developments aim handling general irregular tetrahedral meshes complex geometries 
sequential version application cube divided tetrahedra local calculation electromagnetic fields performed 
precisely balance flux evaluated combination elementary fluxes computed facets tetrahedron 
step local values calculated tetrahedron passed neighbors new local calculation starts 
general algorithm displays phases running application initialization calculation 
complexity calculation changed modifying number tetrahedra cube done mesh size 
definition mesh size mesh size calculation triple fixing number points axis building tetrahedral mesh 
consequence higher mesh size higher number tetrahedra memory computation intensive application 
distributed version cube divided subdomains placed different machines 
definition domain subdomain border face domain volume calculation sequential application 
subdomain defines part distributed applica calculation flux balance magnetic field update electric field tmax initialization calculation discrete electromagnetic energy tmax solution saving calculation flux balance electric field update magnetic field 
simpli ed view algorithm application tion elements located address space 
faces tetrahedra located boundary subdomain called border faces 
inside subdomain calculation behaves domain tetrahedra located boundary subdomain communicate results ones located different address space border faces remote calls 
decided experiments cube opposed complex structure easily divided identical volumes avoiding creation bottleneck particular subdomains 
gives control execution experiments 
division defined follow definition division calculation division subdomains done specifying triplet indicates number subdomains axis cube 
triplets code automatically builds cartesian grid decomposition cube subdomain subcube interface neighboring subdomains composed triangular faces 
resulting decomposition non overlapping vertices triangular faces located interface duplicated definition neighboring subdomains 
architecture application written completely java top proactive third party native libraries perform calculation 
run standard java platform proactive available 
subdomains active objects communication done group communication mechanisms 
communication subdomains consists linked list elements border face contain array double 
special object called collector ensures initial synchronization subdomains perform monitoring steering application needed 
application works steps initialize calculation starts 
initialization part consists deployment remote jvms nodes cluster 
jvms started subdomain created 
creation subdomain linked neighbors group reports back collector starts computation 
communication interface 
possible division cube subdomains optimal division calculation noted previously network communications take place edge subdomains assuming network bottleneck need ensure frontiers minimal need find optimal division mesh size number nodes 
definition communication interface call communication interface side subdomain communication subdomain takes place 
set half border faces subdomains 
take half border faces border face located subdomain linked symmetric subdomain creating single interface 
illustrated division generate interfaces 
proposition cube edge size division number communication interfaces subdomains area interfaces abc bc proof considering cube edge size divisions interfaces axis axis axis 
total number abc bc ac ab 
evaluate area interfaces consider axis independently 
divisions axis generate total area applying reasoning axis concludes proof 
area interfaces abstraction calculation calculate number tetrahedron communicating interface mesh uniform value axis 
proposition border faces interface considering cube mesh division total number border faces tetrahedra located interface proof previous proposition able devise area interface remains compute number tetrahedra unit area 
considering area size calculating number border faces boils computing number triangles constructed dividing squares size 
replacing result area equation concludes proof 
example consider divisions 
create subdomains require border faces opposed second 
proposition optimal division optimal division calculation minimize number border faces 
order evaluate effect distribution calculation need able link sequential parallel executions 
word sequential problem parallel version subdomains equal size sequential 
relation sequential distributed problem size considering sequential experiment mesh size ms ms ms domain equivalent subdomain distributed application mesh size division ms ms ms proof proceed previous demonstrations considering axis 
ms points build tetrahedra 
consider encapsulating domain identically sized subdomains recall definition points located interfaces common subdomains need accounted 
clear domain ms points axis 
induction show equivalent domain ms axis 
applying reasoning remaining axis concludes proof 
experiments section study time taken application perform loops calculation see different mesh size number nodes 
compare results obtained rmi ibis 
fortran version algorithm 
experimental test bed conducted experiments distributed asci supercomputer das 
nodes composed dual pentium iii cpu running ghz gb memory running redhat linux linked fast ethernet 
remote sites das connected connections latency varies ms ms 
sun jdk experiments 
ibm jdk faster garbage collector application inducing pauses time seconds 
decided experiments get smoother deterministic executions 
high memory usage possible run single node high mesh values 
order able calculate speed needed extrapolate value done detailed description architecture www cs vu nl das 
running node relation holds calculation length constant number operations conducted list tetrahedrons linear complexity 
knowing value ratio number tetrahedrons mesh size calculate calculation length mesh size 
duration obtain close real experiment node sufficient amount memory 
memory usage studying impact communications application study memory usage know relying virtual memory decreases performance 
instrumented source code report memory key points execution java commands runtime runtime 
wanted measure total amount memory mesh size overhead distribution 
memory mb mesh mesh nodes mesh mesh nodes execution time 
memory node results depicted clearly shows phases application 
short period memory greatly increases initialization performing computation memory oscillates average value 
comparing sequential distributed versions see little memory overhead added distribution 
main difference experiments longer influence remote communications garbage collector stronger impact especially seconds execution 
ibis rmi shown led increase memory usage consider significant 
proactive ibis vs proactive rmi conducted experiments part das cluster order check importance communication layers homogeneous environment 
experiments run version application top proactive rmi ibis 
time number nodes proactive ibis proactive rmi proactive ibis amount exchanged data mb 
proactive rmi vs proactive ibis number nodes proactive rmi proactive ibis proactive rmi results depicted 
left hand part shows execution time function number nodes computation various mesh values 
indicated interpolated time node dashed line 
time node depicted 
right hand side shows total amount data sent network function number nodes 
increasing number nodes increase execution time amount data exchanged 
optimal division calculation described proposition mean increasing number nodes matching effect number border faces 
nodes optimal division generates border faces 
considering nodes optimal division border faces 
case increasing number nodes involved computation double amount data sent network clearly visible 
see important difference execution time ibis rmi 
programs executes faster 
amount data sent network lower ibis explain difference execution believe result achieved faster serialization remains important bottleneck rmi 
indicated obtained speedup application interpolated value node available 
ibis rmi increases dramatically speedup nodes 
comparison fortran mpi version original algorithm exploited fortran mpi version called em 
wanted perform comparison order provide insight relative performance versions 
aim compare java fortran implementations algorithm different architectures see distribution calculation efficient java fortran mpi 
access compiled version em able instrument perform fine measurements memory usage 
important difference design speedup proactive ibis proactive rmi proactive ibis proactive rmi proactive ibis proactive rmi ideal speedup number nodes 
speed proactive rmi proactive ibis single das cluster em fixed size data structures fine tuned available memory avoid swapping 
result source code modification needed reach arbitrary high mesh values 
able run das cluster libraries issues 
experiments run cluster dual pentium iii running mhz mb memory linked mb switched network located sophia antipolis france 
starting time memory java fortran mesh java fortran java fortran time memory mb mb mb mb table 
java vs fortran node point comparison execution time memory usage sequential versions algorithm 
couldn modify fortran version memory applications measured top linux results shown table 
ratio execution time java fortran versions believe account differences existing versions previously published benchmarks reported ratio applications 
memory usage ratio acceptable probably improved best knowledge goal design 
performed simple optimizations java code creating dynamic data structures final size having grow needed avoiding creation temporary variables structures 
considering distributed version results seen java version surprise slower fortran 
ratio execution time java fortran reached rmi time achieved better performance ibis lowering close sequential 
speedup achieved case lower fortran increase mesh size ibis scales better increasing speedup nodes compared rmi fortran 
time number nodes fortran proactive ibis proactive rmi speedup number nodes fortran proactive ibis proactive rmi ideal speedup 
execution time speedup java fortran versions sun implementation rmi limiting factor performance distributed applications 
different communication layer possible increase performance achieve speedups close obtained fortran mpi 
multi cluster experiments stating grid system coordinates resources centralized control standard protocols interfaces achieve non trivial quality service experimented das grid 
full advantage able perform multicluster experiments requesting nodes parts 
example distribution experiment subdomains distributed clusters fs fs fs fs shown 
subdomain neighbors exchanges locally calculated values 
report collector sake clarity indicate communications 
current version take advantage network architecture happen repartition calculation sub optimal having multiple communications slow links 
find conclusive proof experiments situation occurred 
application demonstrated speedup distributed multiple clusters 
order application run perform steps install needed classes file system ii write descriptor file describes architecture experiments 
descriptor files basically states run cluster local fs fs fs fs fs 
deployment process start fs 
pbs command issued book nodes 
request nodes clusters ssh connection established pbs command issued 
nodes granted scheduler jvms created specified descriptor file proactive runtime allow application deploy 
necessary install application classes downloaded dynamic class loading expense slower startup middleware ones needed 
time fs das nl fs das cs vu nl fs das nl fs das tudelft nl subdomain cluster collector intensive communications infrequent communication represented 
distribution calculation das grid subdomains number nodes speedup ideal speedup 
benchmarks available nodes das grid number nodes operations usually handled scripts various programs requires human intervention 
deployment mechanism done automatically inside application side effect provides useful feature 
application check proactive api number nodes allocated adapt execution available resources 
example requests nodes clusters get allocation issues reduce size calculation postpone resources available making self adaptive environment 
results experiments shown 
running single cluster maximum efficiency reached nodes mesh multiple clusters nodes considering control distribution elements calculation 
demonstrated take advantage complementary middlewares offer platform high level abstractions high performance communications remaining java 
result efficient highly flexible programming environment suited cluster grid programming 
extensive experiments das cluster shown real applications current implementation sun rmi greatly limits speed scalability simple data structures sent network linked lists 
previously demonstrated micro benchmarks believe time exhibited full scale application 
comparing proactive fortran mpi versions shown scalability java achieving nodes speedup fortran proactive ibis proactive rmi 
deployment mechanism accessible application xml api allows construction complex configurations spawning multiple clusters 
configuration dynamic adapting deployment actual number nodes obtained 
possible run electromagnetic application nodes obtaining speedup 
advocated java middleware benefit run portability believe native code limited carefully chosen parts middleware low level communication drivers bring important benefits limited drawbacks 
plan address ibis multiple protocols 
acknowledgments jacobs vrije universiteit netherlands help 
grateful stephane said el inria sophia antipolis france earlier comments 
partly funded inria 

efficient flexible typed group communications java 
joint acm java grande conference pages seattle 
acm press 
el gama 
parallel object oriented application 
proceedings th international parallel distributed processing symposium ipdps santa fe new mexico april 
baker carpenter ko li 
mpijava java interface mpi 
java high performance network computing europar 
huet re 
interactive descriptor deployment object oriented grid applications 
th ieee international symposium high performance distributed computing hpdc 
bull smith freeman 
benchmarking java fortran scientific applications 
java grande pages 

method object oriented concurrent programming 
communications acm september 
foster kesselman 
globus metacomputing infrastructure toolkit 
proceedings workshop environments tools parallel scientific computing siam lyon france august 
ian foster 
grid 
point checklist 
july 
gamma helm johnson vlissides 
design patterns elements reusable object oriented software 
addison wesley 
gray sunderam 
mpi java mpi contrasts comparisons lowlevel communication performance 
proceedings supercomputing portland oregon 
jason maassen henri bal 
gmi flexible efficient group method invocation parallel programming 
lcr sixth workshop languages compilers run time systems scalable computer 
philippsen 
efficient rmi java 
concurrent experience 
philippsen zenger 
transparent remote objects java 
concurrency practice experience november 

finite volume scheme threedimensional maxwell equations unstructured meshes 
siam journal numerical analysis 
de rde 
lightweight java framework scientific computing computational grids 
proceedings acm symposium applied computing melbourne florida 
van maassen jacobs bal 
ibis flexible efficient java grid programming environment 
concurrency computation practice experience appear 
rob van jason maassen henri bal 
satin simple efficient java grid programming 
accepted publication journal parallel distributed computing practices 

