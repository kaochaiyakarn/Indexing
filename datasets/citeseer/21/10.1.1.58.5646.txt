learning little comparison classifiers little training george forman ira cohen hewlett packard research laboratories page mill rd palo alto ca hpl hp com 
real world machine learning tasks faced problem small training sets 
additionally class distribution training set match target distribution 
compare performance learning models substantial benchmark binary text classification tasks having small training sets 
vary training size class distribution examine learning surface opposed traditional learning curve 
models tested include various feature selection methods coupled learning algorithms support vector machines svm logistic regression naive bayes multinomial naive bayes 
different models excel different regions learning surface leading meta knowledge apply different situations 
helps guide researcher practitioner facing choices model feature selection methods example information retrieval settings 
motivation scope goal advance state meta knowledge selecting learning models apply situations 
consider motivations 
information retrieval suppose building advanced search interface 
user page search results trains classifier fly provide ranking remaining results user positive negative indication result shown far 
learning model implement provide greatest precision conditions little training data markedly skewed class distribution 

semi supervised learning learning small training sets natural try leverage unlabeled examples 
common phase algorithms train initial classifier little data available apply select additional predicted positive examples predicted negative examples unlabeled data augment training set learning final classifier 
poor choice initial learning model augmented examples pollute training set 
learning model appropriate initial classifier 
table 
summary test conditions vary 
positives training set feature selection metrics negatives training set ig information gain fx features selected bns bi normal separation performance metrics learning algorithms tp true positives top nb naive bayes tn true negatives bottom multi multinomial naive bayes measure precision recall precision recall log logistic regression harmonic avg 
precision recall svm support vector machine 
real world training sets real world projects training set built scratch time 
period training examples especially long classes 
ideally able train effective classifiers point 
methods effective little training 

meta knowledge testing learning models new classification task hand agnostic inefficient route building high quality classifiers 
research literature continue strive give guidance practitioner models appropriate situations 
furthermore common situation shortage training data cross validation model selection inappropriate lead fitting 
may follow priori guidance studies demonstrating learning models superior large benchmarks 
order provide guidance compare performance learning models induction algorithms feature selection variants benchmark hundreds binary text classification tasks drawn various benchmark databases reuters trec ohsumed 
suit real world situations encountered industrial practice focus tasks small training sets small proportion positives test distribution 
note situations esp information retrieval fault detection ratio positives negatives provided training set match target distribution 
explore learning curve matching distributions explore entire learning surface varying number positives negatives training set independently positives negatives 
contrasts machine learning research tests conditions stratified crossvalidation random test train splits preserving distribution 
learning models evaluate cross product popular learning algorithms support vector machines logistic regression naive bayes multinomial naive bayes highly successful feature selection metrics information gain bi normal separation settings number top ranked features select varying 
examine results perspectives precision top ranked items precision negative class bottom ranked items measure appropriate different situations 
perspective determine models consistently perform varying amounts training data 
example multinomial naive bayes coupled feature selection bi normal separation closely competitive svms precision performing significantly better scarcity positive training examples 
rest organized follows 
remainder section puts study context related 
section details experiment protocol 
section gives highlights results discussion 
section concludes implications directions 
related numerous controlled benchmark studies choice feature selection learning algorithms :10.1.1.11.9519
study results bear different algorithms call different feature selection methods different circumstances 
study examines results maximizing measure maximizing precision top bottom ranked items metrics information retrieval recommenders 
great deal research fold fold cross validation repeatedly trains benchmark dataset 
contrast focuses learning small training sets phase training sets go built 
related shows naive bayes surpasses logistic regression region uci data sets 
extend results text domain learning models 
vary number positives negatives training set independent variables examine learning surface performance model 
akin studied effect varying training distribution size equivalent parameterization decision tree model benchmark uci data sets text domain require feature selection 
measured performance accuracy area roc curve 
measure performance middle measure extreme ends roc curve precision top bottom scoring items metrics better focused practical application information retrieval routing semi supervised learning 
example examining search engine results cares precision top displayed items roc ranking items database 
results studies useful guide research developing methods learning greatly unbalanced class distributions great deal 
common methods involve sampling minority class sampling majority class manipulating class distribution training set maximize performance 
study elucidates effect learning surface learning models 
table 
description benchmark datasets 
dataset cases features classes dataset cases features classes whizbang cora trec fbis ohsumed oh trec la ohsumed oh trec la ohsumed oh trec tr ohsumed oh trec tr ohsumed trec tr reuters re trec tr reuters re trec tr webace wap trec tr trec tr experiment protocol describe study conducted space allows certain parameter choices 
table shows parameter settings varied defines abbreviations 
learning algorithms evaluate learning algorithms listed table implementation default parameters weka machine learning library version 
naive bayes simple generative model features assumed independent class variable 
despite unrealistic independence assumptions shown successful variety applications settings 
multinomial variation naive bayes excel text classification 
regularized logistic regression commonly successful discriminative classifier class posteriori probability estimated training data logistic function 
weka implementation multinomial logistic regression model ridge estimator believed suitable small training sets 
support vector machine risk minimization principles proven equipped classifying high dimensional data text 
linear kernel varied complexity constant results weka default value discussion results varied discussion section 
weka implementation returns effectively boolean output class problems modify code slightly return indication euclidean distance separating hyperplane 
essential get reasonable tp tn performance 
feature selection feature selection important estimated component learning model accounts large variations performance 
study chose feature selection metrics information gain ig bi normal separation bns 
chose comparative study dozen features ranking metrics indicating table 
experiment procedure 
multi class dataset files classes positives negatives random split seeds randomly select positives negatives set leaving remaining cases testing set 
select training set positives negatives 
task established 
model parameters follow 
feature selection metric ig bns rank features feature selection metric applied training set 
fx select top fx features 
algorithm svm log nb multi train training set positives negatives 
score items testing set 
performance measure tp measure tn measure performance 
top performers svm classifiers learned features selected ig tended better precision bns proved better recall improving measure substantially 
expected ig superior goal precision top 
benchmark data sets prepared benchmark datasets available stem originally benchmarks reuters trec ohsumed 
comprise text datasets case assigned single class 
generate binary classification tasks identifying class positive vs classes 
binary tasks median percentage positives 
binary features representing stemmed word appears document 
data described briefly table 
details see appendix 
experiment procedure experiment procedure table pseudo code 
execution consumed years computation time run hundreds cpus hp utility data center 
class data sets randomly split times yielding binary tasks condition second loop ensures positives available training plus test set likewise negatives training testing 
importantly feature selection depends training set leak information test set 
order items random selecting cases amounts random selection means performance vs 
represents test set train table 
performance metrics different uses 
metric setting measure information filtering routing tp information retrieval tp tn semi supervised learning ing set additional positive added random may validly interpreted steps learning curve 
performance metrics analyze results independently metrics 
tp metric number true positives test cases predicted strongly classifier positive 

tn metric number true negatives test cases predicted negative 
rarity positives benchmark tasks scores upper common tn nearly tn 

measure harmonic average precision recall positive class 
superior grading classifiers accuracy error rate class distribution skewed 
different performance metrics appropriate different circumstances 
refer table 
recommendation systems information retrieval settings results displayed users incrementally relevant metric tp appropriate 
represents precision page results displayed 
information filtering document routing cares precision recall individual hard classification decisions taken classifier 
measure metric choice considering 
semi supervised learning settings additional positive negative cases sought unlabeled data expand training set appropriate metric consider tp tn 
maximum precision called situation heuristically extended training set polluted noise labels 
experiment results examining example set results average tp performance benchmark tasks training set positives negatives 
see fig 
vary number features selected logarithmic axis 
observations 
models rise naive bayes ig features tied multinomial naive bayes bns features 
second note opposite feature selection metric bayes models hurts performance substantially wrong number features 
illustrates choices feature selection induction algorithms interdependent best studied 
third svm known tp true positives top multi ig nb ig multi bns fx number features selected nb bns nb ig multi bns multi ig nb svm bns log bns log ig svm bns svm ig fig 

average tp performance learning model positives negatives varying number features selected fx 
performing text classification consistently inferior situation positives greatly represented training set shall see 
condensing fx dimension results single value high dimensionality results condense fx dimension presenting best performance obtained choice fx 
maximum chosen test results represents upper bound achieved method attempts select optimal value fx training set 
condensing fx dimension allows expose differences performance depending learning algorithm feature selection metric 
furthermore practitioner typically easy vary fx parameter harder change implemented algorithm feature selection metric 
visualization simplification vary number positives negatives training set derive learning surface combinations algorithm feature selection metric 
illustrating perspective fig showing learning surfaces just learning models multinomial naive bayes svm logistic regression bns feature selection 
performance measure average number true positives identified top tp 
visualization see log bns dominated entire region remaining models consistent regions performs best 
particular svm model substantially performs naive bayes positives 
competitive positives negatives 
tp tp svm bns neg multi bns log bns pos multi ig log nb bns nb ig multi bns multi ig tp svm bns fat line multi bns log log bns log ig svm bns svm ig fig 

tp performance 
learning surfaces models svm bns multi bns log bns vary number positives negatives training set 
topo map best models surfaces models viewed 
show identical tp performance 
cross section negatives varying positives 
cross section negatives varying positives 
perspective visualization difficult display surfaces learning models 
resolve plot surfaces view plot directly yielding topo map visualization shown fig 
reveals best performing model region 
visualization indicates absolute performance axis geographical topo map 
topo map shows model performed best region clear beat competing models 
show axis cross sections map near left right edges 
figures fix number negatives comparing performance models vary number positives recall design pos neg pos neg fig 

line indicates ratio positives negatives occur training sets obtained random sampling test population benchmark classification problems left topics realworld reuters benchmark right 
demonstrates practical emphasis topo map regions negatives greatly outnumber positives 
test set benchmark task fixed vary may view learning curves add random positive training examples 
tp results initial impetus study determine learning models yield best precision top little training data 
find answer varies vary number positives negatives consistent regions certain models excel 
yields meta knowledge apply different classifiers 
see fig 
observe bns generally stronger feature selection metric roughly number positives exceeds number negatives training set axis multi ig dominates 
recall test distributions typically small percentage positives common real world tasks 
random sample fall region near axis having positives multi bns dominates 
fig amplifies point showing training set class distributions obtained sample randomly test distribution benchmark problems left reuters benchmark right 
returning fig observe horizontal near axis little performance improvement increasing number negatives region best action take rapidly improve performance provide positive training examples random sampling 
show best tp performance providing training set representation positives say 
nb bns dominates see colors region multi bns closely competitive 
generally cross section views figs allow see competitive remaining learning models 
fig see positives multi bns svm bns nb bns competitive region positives multi bns stands substantially 
tn nb svm ig log bns log ig fig 

tn performance 
topo map best models tn 
cross section negatives varying positives 
legend fig tn results determined learning model yields true negatives bottom ranked list test items 
model dominant nb ig consistent performer shown fig especially negatives 
cross section shown fig see svm ig substantially outperform models 
performance high precision fact surprising considering negatives positives test sets 
unfortunately performance improvement attained positives may speculate 
measure results compare learning models measure 
shows topo map best performing models various regions 
observing greatest performance achieved svm bns appropriate oversampling positives 
random sampling test distribution labels negative put region poor measure performance near axis nb ig dominates 
see cross section fig fixed varying number negatives nb ig dominates models wide margin performance plateaus models experience declining performance increasing negatives 
likewise fig fixed performance models declines increase number positives 
fig fixed see substantial gains svm feature selection metric obtain positive training examples 
positives obtained random sampling svm greatly inferior nb ig multi bns 
observing fig best approach maximizing measure times building training set incrementally scratch svm bns ig keep class distribution training set roughly positives non random sampling method 
measure svm peak nb bns nb ig multi bns multi ig multi measure nb ig multi ig multi bns measure svm bns fat line nb ig log bns log ig svm bns svm ig fig 

measure performance 
topo map best models measure 
positives varying negatives 
cross section negatives varying positives 
cross section negatives varying positives 
discussion generalizing notion learning curve learning surface proves useful tool gaining insightful meta knowledge regions classifier performance 
saw particular classifiers excel different regions may constructive advice practitioners know region operating 
learning surface results highlight performance greatly improved non random sampling somewhat favors minority class tasks skewed class distributions balancing unfavorable 
practical real world situations may substantially reduce training costs obtain satisfactory performance 
ubiquitous research practices random sampling crossvalidation researchers routinely training set matches distribution test set 
mask full potential classifiers study 
furthermore hides view research opportunity develop classifiers sensitive training distribution 
useful practically industrial classification problems class distribution training set varied unknown advance match testing target distributions may vary time 
naive bayes models explicit parameter reflecting class distribution set distribution training set 
models frequently said sensitive training distribution 
empirical evidence tp tn measure shows naive bayes models relatively insensitive shift training distribution consistent theoretical results elkan surpass svm shortage positives negatives 
results showed svm excels tp measure training class distribution ideal svm proves highly sensitive training distribution 
surprising svm popularly believed resilient variations class distribution discriminative nature density 
raises question varying svm parameter try reduce sensitivity training class distribution 
study replicated entire experiment protocol values ranging 
fmeasure values substantially hurt svm performance regions fig near axes svm surpassed models 
large region svm dominates values increased performance cost making learning surface sensitive training distribution 
measure increased performance declines drastically number positives reduced 
additional results similar tp 
refer topo map fig 
value svm surpass performance multi bns region near axis 
upper right performance increased fortunate choices exceeds performance nb bns slightly 
boost comes cost poor performance lower region near axis 
values svm competitive multi ig top left performance worse decrease number positives 
varying lead fundamentally different regions performance 
address issue making choice classifier insensitive operating region 
furthermore regions performance improved remains seen optimal value determined automatically cross validation 
small training sets cross validation may lead fitting training set practical improvement 
summary compared performance different classifiers settings encountered real situations small training sets especially scarce positive examples different test train class distributions skewed distributions positive negative examples 
visualizing performance different classifiers learning surfaces provides meta information models consistent performers conditions 
results showed feature selection decoupled model selection task different combinations best different regions learning surface 
potentially includes expanding parameters classifiers datasets studied validating meta knowledge successfully transfers text non text classification tasks 
acknowledgments wish hp utility data center ample computing cycles weka project open source machine learning software anonymous reviewers helped improve 

liu dai li lee yu building text classifiers positive unlabeled examples 
intl 
conf 
data mining 

forman extensive empirical study feature selection metrics text classification 
journal machine learning research 
yang liu re examination text categorization methods 
acm sigir conf 
research development information retrieval 

ng jordan discriminative vs generative classifiers comparison logistic regression naive bayes 
neural information processing systems natural synthetic 

weiss provost learning training data costly effect class distribution tree induction 
journal artificial intelligence research 
japkowicz holte ling matwin eds aaai workshop learning imbalanced datasets tr ws aaai press 
witten frank data mining practical machine learning tools java implementations 
morgan kaufmann san francisco 
duda hart pattern classification scene analysis 
john wiley sons 
domingos pazzani independence conditions optimality simple bayesian classifier 
proc 
th international conference machine learning 

mccallum nigam comparison event models naive bayes text classification 
aaai workshop learning text categorization 

le van ridge estimators logistic regression 
applied statistics 
joachims text categorization support vector machines learning relevant features 
european conf 
machine learning 

han karypis centroid document classification analysis experimental results 
conference principles data mining knowledge discovery 

lewis yang rose li rcv new benchmark collection text categorization research 
journal machine learning research 
elkan foundations cost sensitive learning 
international joint conference artificial intelligence 

