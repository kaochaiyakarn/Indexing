operator scheduling data stream systems brian babcock babu datar rajeev motwani thomas stanford university babcock datar motwani cs stanford edu applications involving continuous data streams data arrival bursty data rate fluctuates time 
systems seek give rapid real time query responses environment prepared deal gracefully bursts data arrival compromising system performance 
discuss strategy processing bursty streams adaptive load aware scheduling query operators minimize resource consumption times peak load 
show choice operator scheduling strategy significant impact run time system memory usage output latency 
aim design scheduling strategy minimizes maximum run time system memory maintaining output latency prespecified bounds 
chain scheduling operator scheduling strategy data stream systems near optimal minimizing run time memory usage collection single stream queries involving selections projections joins stored relations 
chain scheduling performs queries sliding window joins multiple streams multiple queries types 
bursts input streams buildup unprocessed tuples chain scheduling may lead high output latency 
study online problem minimizing maximum run time memory subject constraint maximum latency 
preliminary observations negative results heuristics problem 
thorough experimental evaluation provided demonstrate potential benefits chain scheduling different variants compare competing scheduling strategies validate analytical 
growing number information processing applications data takes form continuous data streams traditional stored databases 
applications share distinguishing characteristics limit applicability standard relational database technology volume data extremely high basis data decisions arrived acted close real time 
combination factors traditional solutions effectively data loaded static databases offline querying impractical applications 
motivating applications include networking traffic engineering network monitoring intrusion detection telecommunications fraud detection data mining financial services arbitrage financial monitoring commerce clickstream analysis personalization sensor networks 
applications spawned considerable growing body research data stream processing bbd ranging algorithms data streams full fledged data stream systems aurora ccc gigascope jcss hancock cfp niagara nia stream str pmc tapestry telegraph tribeca sul :10.1.1.68.4467
part research data stream systems hitherto focused devising novel system architectures defining query languages designing space efficient algorithms 
important components supported part rambus stanford graduate fellowship nsf iis 
supported nsf iis iis 
supported part scholarship nsf iis 
supported part nsf iis foundation research microsoft veritas 
supported nsf eia nsf itr award number 
systems research received attention date run time resource allocation optimization 
focus aspect run time resource allocation operator scheduling 
features unique data stream systems opposed traditional relational dbmss run time resource allocation problem different arguably critical data stream systems typically characterized presence multiple long running continuous queries 
second data streams quite irregular rate arrival exhibiting considerable burstiness variation data arrival rates time 
phenomenon extensively studied networking context fp 
data network traffic widely considered self similar exhibit long range dependence 
similar findings reported web page access patterns mail messages kle :10.1.1.13.8182
consequently conditions data stream queries executed frequently quite different conditions query plans generated 
adaptivity critical data stream system compared traditional dbms 
various approaches adaptive query processing possible data may exhibit different types variability 
example system modify structure query plans dynamically reallocate memory query operators response changing conditions suggested take holistic approach adaptivity away fixed query plans altogether eddies architecture ah mshr 
approaches focus primarily adapting changing characteristics data changing selectivities focus adaptivity changing arrival characteristics data similar aft uf ufa 
mentioned earlier data streams exhibit considerable burstiness arrival rate variation 
crucial stream system adapt gracefully variations data arrival making sure run critical resources main memory bursts ensuring output latency bounded 
focus design techniques adaptivity 
processing high volume bursty data streams natural way cope temporary bursts unusually high rates data arrival buffer backlog unprocessed tuples periods light load 
important stream system minimize memory required backlog buffering 
total memory usage exceed available physical memory periods heavy load causing system page disk causes sudden decrease system throughput jcss :10.1.1.68.4467
show section operator scheduling strategy data stream system significant impact total amount memory required backlog buffering 
question address efficiently schedule execution query operators keep total memory required backlog buffering minimum assuming query plans operator memory allocation fixed 
initially focus entirely memory requirements ignoring metrics 
important stream system ensure reasonable output latency formalize latency requirement constraint output latency exceed certain threshold study constrained version memory minimization problem second part 
road map rest organized follows 
section briefly describing model processing stream queries illustrating example run time memory requirement greatly affected operator scheduling strategy motivating need intelligent scheduling strategy 
section formalize problem operator scheduling consider 
analytical results run time memory requirements proposed scheduling strategies provided section 
particular chain scheduling near optimal scheduling strategy case queries involve single stream possibly joining stored relations foreign key joins 
joins stored relations foreign key joins weaker guarantee 
section introduce various scheduling strategies compare qualitatively 
section extend operator scheduling strategies queries involving windowed joins multiple streams 
section study constrained version operator scheduling problem takes output latency consideration presenting preliminary observations negative results heuristics 
section describe experimental comparison performance different scheduling strategies 
describe related section conclude section 
stream query processing model assumptions data stream systems characterized presence multiple continuous queries ccc cf mshr 
query execution conceptualized data flow diagram ccc directed acyclic graph dag nodes edges nodes pipelined operators aka boxes process tuples edges aka arrows represent composition operators 
edge node node indicates output node input node edge represents input queue buffers output operator input operator input data streams represented leaf nodes input edges query outputs represented root nodes output edges 
single data stream input multiple queries assume multiple copies stream created system fed queries separately 
consequently assume streams participate query incoming tuple input single query 
making multiple copies system chooses share input buffer multiple queries optimal strategy may differ plan consider case 
concentrate case fixed query plan change time 
mentioned earlier assume operators execute streaming pipelined manner 
operators select project naturally fit category 
join operator manner symmetric hash join implementation wa :10.1.1.56.701
operators select project need maintain run time state symmetric hash join operator needs maintain state proportional size input seen far unbounded streams 
applications streams relevant notion join sliding window join ccc cf mshr tuple stream joined arrival bounded window tuples stream vice versa 
consequently state maintained sliding window join operator bounded 
example sliding window join tuple sliding window join window stream specified fixed number tuples 
clearly run time state stored tuple sliding window join operator fixed size 
summary operators consider act filters operate tuple produce tuples selectivity operator 
selectivity select project operators may greater join 
rest reader keep mind refer selectivity operator referring notion viewing operator filter average produces tuples processing tuple 
assume run time state stored operator fixed size variable portion memory requirement derived sizes input queues operators 
memory input queues obtained common system memory pool 
terms model goal operator scheduling minimize total memory requirement queues system subject user specified latency constraint 
important strategy handling rapid bursty stream arrival load shedding input tuples dropped quality service criteria bring system load manageable levels system overloaded bdm 
load shedding may necessary input rate consistently exceeds maximum processing rate system input queues build eventually run memory 
load shedding focus 
techniques load shedding proposed bdm orthogonal operator scheduling techniques discuss conjunction techniques 
focus issue average arrival rate computational limits may bursts high load leading high memory usage buffer backlog unprocessed tuples 
want schedule operators efficiently order keep peak memory usage minimum 
assume average arrival rate computational limits eventually possible clear backlog unprocessed tuples bursts high arrival rate 
tuple enters system pass unique path operators referred operator path 
recall share tuples query plans 
arrival rate tuples uniform done 
simple scheduling strategy minimum memory requirement tuple enters system schedule operators operator path 
operator path join produces multiple tuples schedule resulting tuple turn remaining portion operator path 
henceforth strategy referred fifo strategy 
note mentioned earlier uniformity arrival seldom case need sophisticated scheduling strategies guaranteeing queue sizes exceed memory threshold 
example illustrates scheduling strategy fare better fifo difference exceeding memory threshold 
example consider simple operator path consists operators followed 
assume takes unit time process tuple produces tuples selectivity 
assume takes unit time operate tuples alternatively time units operate tuple produces tuples outputs tuple system selectivity 
takes units time tuple pass operator path 
assume time average arrival rate tuples tuple units time 
assumption guarantees unbounded build tuples time 
arrival tuples bursty 
consider arrival pattern tuple arrives time instant tuples arrive time 
consider scheduling strategies fifo scheduling tuples processed order arrive 
tuple passed operators consecutive units time time tuple processed 
greedy scheduling time instant tuple buffered operated time unit tuples buffered tuples processed time unit 
table shows total size input queues strategies time greedy scheduling fifo scheduling time input queue sizes strategies decline reach time 
observe greedy scheduling smaller maximum memory requirement fifo scheduling 
fact memory threshold set fifo scheduling infeasible greedy scheduling 
example illustrates need intelligent scheduling strategy order execute queries limited amount memory 
aim design scheduling strategies form core resource manager data stream system 
desirable properties scheduling strategy strategy provable guarantees performance terms metrics resource utilization response times latency 
executed time steps strategy efficient execute 
strategy sensitive inaccuracies estimates parameters queue sizes operator selectivities operator execution times 
tuple refer individual tuple fixed unit memory page contains multiple tuples assume selectivity assumptions hold average 
details provided section 
tuple size lower envelope time progress chart operator scheduling memory requirements mentioned earlier query execution captured data flow diagram tuple passes unique operator path 
queries represented rooted trees 
operator filter operates tuple produces tuples operator selectivity 
obviously selectivity assumption hold granularity single tuple merely convenient abstraction capture average behavior operator 
example assume select operator selectivity select tuples tuples processes 
henceforth tuple thought individual tuple viewed convenient abstraction memory unit page contains multiple tuples 
adequately large memory units assume operator selectivity operates inputs require unit memory output require units memory 
fifo scheduling strategy example maintains entire backlog unprocessed tuples operator path 
sizes intermediate input queues small case fifo albeit cost large queues operator path 
fact greedy strategy performed better fifo example precisely chose maintain backlog input queue operator second 
operator low selectivity beneficial buffer fractional tuples size intermediate queue buffer single tuple size operator path 
suggests important consider different sizes tuple progresses operator path 
capture notion progress chart illustrated 
horizontal axis progress chart represents time vertical axis represents tuple size 
operator points 
tm sm 
tm positive integers represent operator path consisting operators ith operator takes ti ti units time process tuple size si produces tuple size si 
selectivity operator si si 
assume operators preempted middle execution resumed time unit represents smallest duration operator continuously run preemption 
adjacent operator points connected solid line called progress line representing progress tuple operator path 
imagine tuples moving progress line 
tuple enters system size 
processed ith query operator tuple received ti total processor time size reduced si 
point say tuple progress ti 
way interpret progress chart ith horizontal segment represents execution ith operator vertical segment represents drop tuple size due operator selectivity 
operator path operator selectivity sm 
eject tuples produces system longer need buffered 
tuple size lower envelope operator time progress chart operator selectivity operator selectivities equal progress chart nonincreasing shown 
query plans include join operator selectivity greater progress chart looks shown 
assume selectivities tuple processing times known operator 
construct progress chart explained 
selectivities processing times learned query execution gathering statistics period time 
expect values change time strategy ah divide time fixed windows collect statistics independently window statistics ith window compute progress chart st window :10.1.1.34.8546
consider operator path operators represented operator points 
tm sm 
point progress line ti ti derivative point respect jth operator point tj sj sj tj derivative undefined derivative negative slope line connecting point operator point right 
steepest derivative point ti ti denoted operator point maximum achieved defined steepest descent operator point tb sb min consider subsequence operator points start point 
reach point xk tm sm 
connect sequence points 
xk straight line segments obtain lower envelope progress chart 
figures lower envelope represented dashed line 
observe lower envelope convex 
simple observation regarding lower envelope 
proposition 
tk sk denote sequence points lower envelope progress chart 
magnitude slopes segments joining ti si ti si nonincreasing proof proof contradiction 
suppose exists index magnitude slope segment joining ti si ti si strictly greater segment joining ti si ti si 
slope segment joining ti si ti si strictly greater segment joining ti si ti si 
case definition ti si point ti si belong lower envelope 
ready operator scheduling strategies 
section focus scheduling operators minimize total memory requirement queues system 
consider output latency metric section 
break ties multiple points smallest index 
scheduling strategies run time memory minimization defining framework specifying operator scheduling strategy 
ideally view scheduling strategy invoked unit time smallest duration operators run preemption 
invocation strategy select operator nonempty input queues schedule time unit 
reality need invoke strategy time unit 
turns cases required periodically certain events occur operator currently scheduled run finishes processing tuples input queue new block tuples arrives input stream 
scheduling strategies considered section choose operator schedule statically assigned priorities scheduling operator execution change operator priorities 
scheduling strategy causes little overhead priorities need recomputed operators scheduled stopped 
model describe strategies assign priorities different operators queries provide guarantees possible 
see momentarily priority scheduling strategy assigns operator completely determined progress chart operator belongs 
need ensure estimates selectivities tuple processing times progress charts computed outdated 
periodically recompute progress charts statistics gathered window time query execution 
task recomputing progress charts statistics straightforward incurs little overhead 
queries consider categorized types 

single stream queries queries typically involve selections projections single stream may involve joins static stored relations possibly grouping aggregation 
fairly common class queries data stream applications 
single stream queries discussed section 
multi stream queries distinguishing feature class queries involve join streams 
queries typically correlate data streams query joins network packet streams routers find packets passed routers 
indicated earlier assume joins streams tuple sliding window joins 
multi stream queries discussed section 
single stream queries operator scheduling algorithm call chain scheduling 
name chain scheduling comes fact algorithm groups query operators operator chains corresponding segments lower envelope query progress chart 
operator path progress chart denote lower envelope simulation defined progress chart progress line consists lower envelope consider tuple progress line segment li joining ti si ti si say tuple lies li ti ti 
recall section denotes progress operator path 
say li ti middle li ti ti 
consider data stream system distinct operator paths represented progress charts 
pn lower envelope simulations 

chain scheduling strategy henceforth simply chain brevity system proceeds follow chain time instant consider tuples currently system 
schedule single time unit tuple lies segment steepest slope lower envelope simulation 
multiple tuples select tuple earliest arrival time 
way describe strategy may appear tuple decisions level tuple 
case chain statically assigns priorities operators tuples slope lower envelope segment operator belongs 
time instant operators tuples input queues highest priority chosen executed time unit 
special structure lower envelope show chain optimal strategy collection progress charts define strategy full knowledge progress charts tuple arrivals 
clearly strategy knowledge notion provides useful benchmark compare performance valid scheduling strategy 
compare chain strategies 
theorem denote chain scheduling strategy denote scheduling strategy 
consider identical systems processing identical sets tuples identical arrival times operator paths having progress charts moment time system require memory system implying chain strategy optimal collection lower envelopes 
proof tuple arrival times system differences memory requirements time due number tuples system able process time 
denote number tuples consumed processing total reduction size tuples strategies time instant wish show 
dl denote distinct slopes segments lower envelopes arranged decreasing order 
slope segment fraction tuple consumed reduction size tuple tuple moves unit time segment 
instant respectively ta denote number time units moved segments slope di strategy respectively strategy 
strategy prefers move segment steepest slope proposition slopes nonincreasing lower envelope follows ta number tuples consumed strategy tc di number consumed ta di 
dl follows 
consider performance chain general progress charts observation proposition segments particular slope chain guarantees tuple lies middle segments 
remaining tuples respective segments 
consequently strategy maintains arrival order tuples 
see proposition true recall tuples lying steepest descent segment chain prefers keep moving tuple earliest timestamp keep moving tuple cleared segment moved segment 
chain implemented general progress chart tuples move lower envelopes reality tuples move actual progress chart show matter memory requirements chain memory requirements lower envelope simulation consider segment joining ti si ti si lower envelopes denote maximum segment difference tuple size coordinates vertical axis lower envelope value time coordinate horizontal axis 
lemma denote number tuples consumed chain strategy moving lower envelopes 
ac denote number tuples consumed chain tuples move actual progress charts 
time instant ac sum taken segments corresponding lower envelopes 
proof consider tuple making moves time axis functions corresponding actual progress chart lower envelope 
size tuple segment 
size differs tuple middle segment 
segment maximum difference equal definition proposition guarantees chain strategy tuple middle segment 
putting obtain ac simple observation progress charts follows fact lower envelope lies beneath actual progress chart 
proposition progress tuple progress chart lower envelope simulation measure memory requirement actual progress chart underestimate memory requirement 
follows proposition memory requirement strategy progress chart greater memory requirement chain lower envelope simulation proved earlier theorem chain strategy optimal collection lower envelopes 
combine preceding observations prove statement near optimality chain general progress chart performance chain strategy lower envelope lower bound optimum memory usage strategies obtain chain strategy applied actual progress chart optimal additive factor important case operator selectivities corresponds queries non foreign key joins stored relations give tight bound obtain result theorem selectivities operators total number queries ac proof selectivities operators progress chart nonincreasing step function shown 
segment joining ti si ti si case si si 
consequently sum segments belong lower envelope query equals 
result sum segments lower envelopes bounded number queries combining lemma implies ac selectivities chain differs optimal unit memory operator path 
emphasize guarantee stronger merely saying maximum time memory usage maximum memory usage optimal strategy 
fact guaranteed chain strategy unit memory query compared strategy unfair knowledge tuple arrivals instants time just compare maximum memory usage 
fairly strong worst case bound performance chain 
quite surprising knowledge arrivals chain able maintain near optimality time instants 
knowledge tuple arrival patterns algorithm efficiently determine optimal schedule assuming np demonstrated theorem theorem offline problem scheduling operators minimize required memory np complete 
proof memory requirements schedule easily verified polynomial time problem finding schedule minimizes memory required class np 
prove problem np complete reduction knapsack problem 
knapsack problem inputs knapsack capacity set items 
xn associated sizes 
sn benefits 
bn 
objective find subset items maximum total benefit xi bi subject constraint total size xi si exceed capacity construct progress charts operators 
ith progress chart consists operator takes time si selectivity bi followed operator takes time si selectivity zero 
consider sequence tuple arrivals time time tuple arrives progress charts 
tuples arrive time time tuples arrive time easy see maximum memory usage occurs time denotes amount memory freed due processing time units amount memory time equal processing operators single progress chart completion requires time units operator progress chart processed completion time units 
completing operator ith process chart frees bi units memory requires si time units processing memory freed partially processing operator 
convert solution scheduling problem solution knapsack problem add ith knapsack item scheduling solution chooses schedule ith progress chart si time units time units 
benefit gained knapsack solution memory freed scheduling solution time units optimal schedule yields optimal knapsack solution 
analysis chain tight exist cases suffer worse optimal strategy additive factor comparison operator scheduling strategies proceeding case queries joining multiple streams natural scheduling strategies compare chain scheduling 
round robin standard round robin strategy cycles list active operators schedules operator ready execute 
scheduled operator runs fixed time quantum expires input queue operator empty 
contrast chain priority scheduling strategies round robin desirable property avoiding starvation operator tuples input queue goes unscheduled unbounded amount time 
chain especially bursts ready execute operators low priority chains may wait scheduled 
simplicity starvation avoidance round robin come cost lack adaptivity bursts 

fifo fifo strategy example processes input tuples order arrival tuple processed completion tuple considered 
general fifo strategy minimize response time tuples query result 
round robin fifo ignores selectivity processing time operators shows adaptivity bursts 

greedy greedy strategy operator treated separately opposed considering chains operators static priority selectivity tuple processing time operator 
ratio captures fraction tuples eliminated operator unit time 
problem strategy take account position operator vis vis operators operator path 
instance suppose fast highly selective operator follows selective operators 
operator get high priority ones preceding 
result time instants ready scheduled input queues empty 
demonstrates need prioritize earlier operators inductive manner notion captured lower envelope chain 
conclude subsection discussion points pushing selections query optimizers try order operators selective operators precede selective making query plans selective operators early 
precisely type query plan chain performs best compared strategies fifo round robin 
fifo strategy exploit low selectivity operators query plan accumulate large backlog unprocessed tuples operator path bursty periods illustrated example 
round robin scheduling strategy similar problem treats ready operators equally 
interestingly greedy mirror chain operators plan decreasing order priority operator form chain 
non commutativity operators result query plans favor chain greedy 
example tuple sliding window joins ones consider commute operators including selections 
pushing selection tuple sliding window join change result join filtering tuples reach join slowing rate tuples expire sliding window 
starvation response times mentioned earlier chain strategy may suffer starvation poor response times especially bursts 
address problem section 
scheduling overhead clearly scheduling overhead negligible simple strategies round robin fifo 
chain scheduling decisions need operator finishes processing tuples input queue new block tuples arrives input stream 
case scheduling decision involves picking operator highest priority chain contains ready operator 
underlying progress charts chain priorities need recomputed operator selectivities tuple processing times change operator chains priority order fixed 
recomputing progress charts statistics chains priorities progress charts takes little time 
chain incurs negligible overhead greedy behaves similarly 
context switching overhead context switching overhead incurred scheduling strategy depends underlying query execution architecture 
assume operators scheduler run single thread 
get operator process input queues scheduler calls specific procedure defined operator 
query execution model similar framework implemented data stream projects 
context switching operator equivalent making new procedure call low cost modern processor architectures 
context switching costs significant different operators part separate threads aurora ccc 
context switching costs significant expect costs hamper effectiveness chain 
compared scheduling policies round robin fifo chain tends minimize number context switches 
chain force context switch operator finishes processing tuples input queue new block tuples arrives input stream unblocks higher priority operator currently scheduled 
throughput techniques perform amount computation considering scheduling costs context switching costs negligible 
expect throughput time provided main memory threshold exceeded 
bursts fifo momentarily produce result tuples compared strategies 
experimental results section validate intuitive statements analytical results section 
multi stream queries extend chain scheduling queries multiple streams contain sliding window join streams 
presentation particular section tuple refers single stream tuple opposed larger unit memory page case previous sections 
recall assume tuple sliding window joins 
assume tuple globally unique timestamp streams tuples single stream arrive increasing order timestamp 
instance stream system timestamping tuples arrive system 
procedural description result tuple sliding window join follows join streams tuple arrives stream joined latest ws tuples ws size sliding window stream 
symmetric processing occurs tuples arriving stream tuples timestamps joined timestamp result tuple max 
synchronization inherent semantics restricts freedom operator scheduling 
guarantee correctness ensure join output produced sorted timestamp order need synchronize inputs join processing strict timestamp order input streams similar merge sort timestamp attribute 
words joining streams tuple timestamp processed join operator processed tuples timestamp result sliding window join operator block input queues empty tuples available input queue 
extending chain scheduling joins order extend chain scheduling need extend progress chart model multi stream queries 
query multiple streams rooted tree input streams leaves tree 
break tree parallel operator paths input stream connect individual leaf nodes representing input streams root tree 
operator paths obtained share common segments 
operator path individually broken chains scheduling purposes 
example consider query sliding window join streams followed selection condition output join 
additionally project operator stream joins decomposition tree corresponding query gives operator paths 
segment shared operator paths 
note join operator part operator paths part operator chains paths broken chains scheduling 
discussed earlier section join operator processes tuples strict timestamp order input queues irrespective chain part gets scheduled 
furthermore sliding window join operator block input queues empty 
tuple processing times selectivities operators join defined straightforward manner similar case single stream queries 
specify quantities join operator streams 
sliding window join abstracted model similar described 
average number tuples stream unit time timestamp tuples average tuple selectivity convenient abstraction capture average behavior sliding window join operators 
consider sliding window join operator streams processes tuples timestamps belonging interval size run join operator process average input tuples produce result tuples average selectivity semi join stream sliding window average number tuples sliding window tuple joins 
defined analogously 
system time taken run tr ts tx average time taken process tuple stream ts includes time taken compare head tuples queues time probe sliding window produce result tuples time update sliding window tr ts input streams selectivity sliding window join tuple processing time wall clock time tr sts easy inductively derive stream result intermediate operator query plan 
having specified values windowed join operator streams build progress chart operator path described section 
basic chain strategy remains unchanged 
difference earlier operator blocked scheduled input queue empty 
case join left input queue tuples right input queue empty 
case chain corresponding left queue want schedule join operator operator blocked input right input queue 
chain considered scheduling join operator unblocked 
scheduling strategy executed operator finishes processing tuples input queues new block tuples arrives input stream highest priority ready chain scheduled 
single stream case analytical results adaptation chain multiple stream case 
experimental results suggest chain performs extremely single stream multiple stream queries compared scheduling strategies considered 
incorporating latency constraints development chain algorithm focused solely minimizing maximum run time memory usage ignoring important aspect output latency 
bursts input streams chain suffers tuple starvation chain prefers operate new tuples lie steeper slope segments lower envelope neglecting older tuples system lie segments slopes incurring high latency old tuples 
minimizing run time memory usage important stream applications require responses stream inputs specified time intervals 
chain described far scenarios turn task modifying chain handle latency issue 
approach design scheduling strategy minimizes peak memory usage subject constraint output latency exceed user specified threshold 
show section simple extensions chain provide efficient solutions problem 
discussion assume exists feasible scheduling strategy adheres latency constraint 
imposes certain restrictions input pattern high volume bursts 
restrictions satisfied impossible meet latency constraint tuples 
cases relax latency constraint drop input tuples perform load shedding bdm 
load shedding orthogonal issue discussed 
denote maximum allowable latency specified system administrator part user query 
parameter imposes latency constraint input tuple processing time arrives time output denote total processing time single block tuples coordinate operator point progress chart latency bound expressed multiple processing time typical values thousands value order seconds value milliseconds 
known exists schedule process tuples deadlines earliest deadline edf scheduling process tuples deadlines :10.1.1.27.5797
edf online strategy need know input sequence priori 
single progress chart single query edf degenerates fifo scheduling strategy 
note edf tuple scheduling strategy scheduling decisions run time state tuples includes knowledge individual deadlines progress far 
tuple strategies higher overhead scheduling opposed operator scheduling strategies chain static operator priority 
scheduling strategies consider tuple despite potentially higher implementation overheads associated 
techniques improving efficiency certain tuple strategies covered section 
preliminary observations followed negative results show difficulty achieving near optimal memory usage presence latency constraints 
ease exposition restrict scenario query plan consists single operator path progress chart negative results clearly extend case multiple 
mentioned earlier exists feasible scheduling strategy input adhere certain restrictions 
particular easy see number tuples input consecutive period pt time units exceed 
time tuples system input ft time units 
putting observations point time tuples completely processed 
definition efficiency measures additive amount algorithm diverges optimal 
definition efficient algorithm online algorithm said efficient operator scheduling problem requires units memory memory requirement best offline algorithm 
theorem shows online scheduling algorithm better efficient 
words efficient online algorithm exists value sublinear progress charts operator selectivities non foreign key joins lower bound asymptotically tight memory requirement feasible algorithm bounded maximum number unprocessed tuples 
theorem exist progress charts arrival patterns online algorithm efficient positive number 
proof ease exposition proof times normalized dividing total tuple processing time consider progress chart lower envelope small positive numbers 
consider arrival pattern tuples inserted system starting time tuples inserted system uniform time intervals width 
note tuples inserted time th tuple inserted time ft slack value 
tuple complete execution time units arrival th tuple complete execution time st tuple completely processed time 
tuples take time units process leaves slack time units go processing tuples specified earlier 
note exists algorithm maximum memory requirement input specified far 
time tf adversary begins injecting second batch tuples starting uniform intervals 
number tuples inserted second batch decided adversary runtime tuples tuples batch 
note slack ft allows combined processing ft time units tuples second batch tuples need processed completion 
order reduce memory usage helpful process newly arriving tuples possible time units 
tuples going injected second batch afford process units time 
available slack ft time units tuples arrive second batch preferable process tuple second batch units reduction memory usage large possible necessary process st tuple completion order meet deadline 
know online algorithm juncture optimal choice tuples second batch process time units process units 
consider intermediate point tuples second batch arrived system 
choices scheduler taken tuple process tuple units memory requirement brought nearly zero process tuple units memory requirement reduced spend time units processing tuples arrived batch 
loss generality assume online algorithm process time tuples arrived time units steepest segment progress chart 
choice achieves lesser reduction memory compared choice advantage choice slack consumed greater number tuples may arrive processed time units 
note time units processing tuple important spending time segment progress chart decreases memory usage faster second segment 
point tuples second batch arrived assume online algorithm processed tuples time tuples time amount memory consumed far online algorithm second batch tuples 
slack remaining kt ft consider scenarios 
scenario tuples enter system batch intervals 
online algorithm slack remaining able consume memory 
total memory consumed online algorithm second batch tuples 
optimal offline algorithm scenario ft slack processing tuples batch time arrives consuming total memory second batch 
scenario online algorithm fails efficient 

scenario tuples enter system batch tuples 
optimal offline algorithm case processed tuples second batch time units arrived achieving memory reduction opposed online algorithm reduced memory usage 
scenario online algorithm fails efficient 
online algorithm select minimize loss adversarial input pattern tuples 
loss maximum scenario selected adversary max 
online algorithm minimize choosing 
get online algorithm efficient 
improve constant argument complicated argument asymptotically change result 
result holds general progress charts 
specific types charts example progress charts segments possible get near optimal online algorithm illustrated subsection 
note lower bound theorem progress chart segments 
algorithm chain flush chain flush simple modification chain dealing latency constraints 
chain flush algorithm proceeds exactly chain deadline constraints tight forcing change strategy 
suppose unprocessed partially processed tuples system 
dn times remaining deadlines tuples tn amounts processing time required tuple 
note tuples ordered arrival order oldest unfinished tuple 
tj di tj ith tuple verge missing deadline 
point processing arriving tuples performed ith tuple earlier tuples completely processed 
point chain flush recursively applied processing restricted tuples 
tuples completely processed 
point chain flush resumes processing remaining unprocessed tuples including new tuples may arrived interim 
theorem progress chart consisting operators selectivity having lower envelope composed segments chain flush efficient 
proof total processing time spent tuples moment divided time spent chain flush steeper segment progress chart time spent second segment 
slopes segments total memory consumed 
suppose algorithm divides time differently spending segment second segment 
chain flush greedily prefers execute operators segment possible scheduling second segment minimum number times necessary meet latency constraints show judging lower envelope progress chart memory consumed alternate algorithm memory consumed chain flush 
memory requirement instant difference total memory tuples entering system memory consumed point 
memory consumption measured progress lower envelope chain flush maximum memory consumption instant minimum memory requirement 
memory consumption indicated lower envelope approximates actual progress chart actual memory usage chain flush may somewhat greater 
argument proof theorem applied demonstrate total amount additional memory exceed 
implementing chain flush efficiently basic chain algorithm chain flush tuple involves considerable scheduling overhead 
tuple queued system need keep track total processing time remaining tuples arrived queued system 
overhead direct implementation chain flush unsuitable data stream systems continuous queries run concurrently 
course overhead reduced tracking remaining processing time blocks tuples opposed individual tuples potential cost missing deadlines 
fact remains scheduling overhead chain flush described section proportional total amount data queued system 
fortunately stream applications aware sqr soft latency constraints missing deadlines small margins acceptable tradeoff applications improve throughput resource utilization 
section describe efficient approximate implementation chain flush leverages property 
discussion assume deadlines specified level query maximum latency tolerated output tuples query denoted latency threshold query 
purposes discussion assumptions 
timestamp input stream tuple wall clock time tuple arrived system 

consider plans consisting selection join operators 
timestamp tuple passed selection operator remains unchanged 
timestamp joined tuple produced join operator higher timestamps joining tuples 

latency tuple output system difference timestamp tuple wall clock time tuple output 
maintain data structure denoted queue system store tuple form th dq pq th timestamp head tuple queue dq latency threshold query corresponding query plan containing pq sum average tuple processing times operators starting operator reads output operator plan containing consider head tuple denote current time 
definition assuming head tuple produces output tuple eventually pq earliest time produce output tuple 
furthermore deadline output tuple th dq 
maintained incrementally nondecreasing order th dq pq operators execute 
recall section chain needs scheduling decision operator finishes processing tuples input queue new block tuples arrives input stream 
chain flush needs scheduling decisions frequently avoid operator input tuples share processor causing tuples parts query plan delayed indefinitely 
ensure scheduling decisions timely manner increasing scheduling overhead substantially operator scheduled maximum number tuples process returns control back scheduler 
scheduling step scheduler checks entry th dq pq satisfies pq th dq current time 
schedules operator chain scheduled point operator highest priority 
scheduler switches flush mode processing process head tuple completion 
effectively creates virtual segment consisting operators starting reader operator output operator plan containing operators segment scheduled succession head tuple tuples intermediate queues operators segment processed completion 
implementation chain flush provide hard guarantees meeting latency constraints 
experiments section show practice implementation able keep output latency extremely close specified latency threshold 
furthermore scheduling overhead chain flush implementation comparable chain significantly lower direct implementation chain flush section 
chain flush implementation invokes scheduler chain extra scheduling step simply involves accessing entry comparing current time 
algorithm mixed mixed simple modification chain deal latencies caused multiple segments low slope progress chart 
mixed clips segments lower envelope slope threshold fifo applied segments 
words mixed modifies lower envelope combine segments slope threshold form unified segment 
example consider simple progress chart lower envelope points 
second segment steeper slope third mixed combines form single segment modified lower envelope 
mixed applies chain modified lower envelope 
compare fifo chain mixed respect maximum memory requirement latency output produced 
note latency tuple difference time completely processed available processing 
average latency latency averaged output tuples 
consider arrival patterns 

tuples arrive th tuple arriving time arrival pattern chain algorithm selects arriving tuple processing immediately 
metrics chain fifo mixed max memory avg latency max latency 
consider arrival pattern tuples arrive th tuple arriving time metrics chain fifo mixed max memory avg latency max latency pattern shows tuples arriving rates just processing rates cause large latencies chain 
second pattern shows fifo non adaptive large memory requirements 
algorithm mixed lower latency chain 
experiments section describe results various experiments conducted compare performance various operator scheduling policies described 
brief description simulation framework 
section compare performance scheduling policies terms total memory requirement queues system section compare performance terms output latency 
implementation fifo processes block input stream tuples completion processing block tuples strict arrival order 
round robin cycles list operators ready operator scheduled time quantum 
size time quantum affect performance round robin change nature results 
notion progress chart captures average behavior query execution terms sizes memory units progress operator paths 
experiments describe designed choosing particular progress chart real synthetic data sets adjusting selection conditions join predicates closely reflect progress chart 
course actual query execution short term deviations average behavior captured progress chart 
experiments follow query execution report memory usage various times 
experiments described static estimates operator selectivities processing times derived preliminary pass data build progress charts 
performed experiments allowed adaptivity dividing time windows statistics gathered ith time window build progress charts st time window 
results similar experiments report 
briefly describe data sets various experiments 
synthetic data set networking community performed considerable research model bursty traffic closely approximate distributions prevalent real data sets 
willinger paxson 
model generate synthetic bursty traffic flows poisson process mean inter arrival time equal time unit send packets continuously duration chosen heavy tailed distribution 
pareto distribution packet durations probability mass function experiments 
arrival times generated attribute values generated uniformly numeric domain allows choose predicates desired selectivities 
total queue size kbytes fifo greedy round robin chain time milliseconds queue size vs time single stream operators real data set 
real data set internet traffic archive ita source real world stream data sets 
traces named dec pkt contains hour worth wide area traffic digital equipment rest world 
trace real world data set experiments 
attributes ip addresses packet sizes selection join predicates 
exact predicates chosen give desired selectivities experiment 
memory requirement queues experimental evaluation total memory requirement queues system chain fifo greedy round robin policies 
section presents experimental results single stream queries section considers queries having joins stored relations section considers sliding window join queries section considers multiple queries 
chain flush mixed evaluated section considered section 
ignore cost context switching experiments section 
discussed section chain fewer context switches compared fifo greedy round robin 
included context switching costs relative performance chain better shown 
single stream queries joins experimental results compare performance different scheduling strategies single stream queries joins 
consider simple query operators 
progress chart terms coordinates ti si operator points times microseconds 
terms terminology section tuple progress chart contains individual tuples size bytes 
query consists fast highly selective operator followed slow operator consumes fewer tuples unit time 
similar example 
figures show variation total queue size time real synthetic data set respectively 
observe chain greedy identical performance simple query plan 
explained fact operator forms chain expected behave identically 
round robin performs chain data sets 
experiments see number operators increase performance round robin degrades 
fifo performs badly respect chain utilizes fast highly selective operator burst 
second query consider operators selectivities 
progress chart total queue size kbytes fifo greedy round robin chain time milliseconds queue size vs time single stream operators synthetic data set total queue size kbytes greedy round robin chain time milliseconds queue size vs time single stream operators real data set terms coordinates ti si operator points 
third operator fast highly selective hidden second operator lower tuple consumption unit time 
typical scenario greedy expected perform badly compared chain 
observe figures show variation total queue size time query real synthetic data sets respectively 
legibility shown performance fifo round robin figures respectively perform nearly bad greedy case 
greedy schedule selective second operator bursts fast selective third operator remains utilized explaining greedy bad performance 
uses lower envelope determine priorities chain schedules selective fast third operator hidden selective operator 
notice previous case round robin badly compared chain 
total queue size kbytes fifo greedy chain time milliseconds queue size vs time single stream operators synthetic data set total queue size kbytes time milliseconds greedy fifo chain queue size vs time single stream synthetic data queries having joins stored relations recall section join relation result operator selectivity strictly greater 
real data set worked include stored relations report experimental results synthetic data 
progress chart terms coordinates ti si operator points second operator join stored relation 
shows performance different scheduling strategies bursty synthetic data 
legibility shown performance round robin performs badly greedy 
connected points corresponding fifo line segments 
fifo round robin take operator selectivities account performance remains similar observed previous experiments section 
low priority join operator bursts greedy utilize fast selective operator follows join 
hand operators comprise single segment chain fast selective total queue size kbytes greedy fifo round robin chain time milliseconds queue size vs time sliding window join selections real data set total queue size kbytes greedy fifo round robin chain time milliseconds queue size vs time sliding window join selections synthetic data set third operator bursts leading substantial benefits greedy 
queries sliding window joins streams study performance different strategies query streams joined sliding window join 
sliding window join average selectivity 
output windowed join passes selection conditions 
furthermore joining stream passes selection condition 
selection conditions selective selection join selective 
performance graphs query real synthetic data sets shown figures respectively 
observe greedy fifo perform worse chain 
round robin compares chain real data set really badly synthetic data set 
experiment described section greedy badly sliding window join preceding highly selective operator 
low priority total queue size kbytes time milliseconds greedy fifo round robin chain queue size vs time plan multiple queries real data set total queue size kbytes time milliseconds greedy fifo round robin chain queue size vs time plan multiple queries synthetic data set join discourages greedy scheduling greedy utilizes highly selective operator 
fifo performs badly reasons mentioned earlier presence selective operator relatively high tuple processing time 
multiple queries compared performance different strategies collection queries sliding window join query similar experiment section single stream queries selectivities similar section 
performance graphs query workload real synthetic data sets shown figures respectively 
improved legibility connected points corresponding chain fifo line segments 
graphs show maximum memory requirement chain lower schedulers 
reason impressive performance chain increase complexity size underlying latency milliseconds chain round robin greedy fifo time milliseconds output latency vs time single stream operators real data set problem 
chain able pick particular chain operators effective reducing memory usage schedule repeatedly burst input arrival 
hand larger number operators multi query experiment compared earlier single query experiments round robin ends executing best operator frequently lesser number operators 
words number operators increases fraction time round robin schedules selective operator decreases 
holds fifo 
greedy performs badly reasons mentioned earlier queries consist highly selective operators hidden selective ones chain recognizes greedy fails recognize 
experiment suggests increase number queries benefits chain pronounced 
results figures obtained going single query collection queries 
real system queries benefits greater 
output latency experiments previous section show chain keeps total queue memory requirement lower fifo greedy round robin 
evaluate performance scheduling policies terms latency output produced 
specifically experiments answer questions 
chain compare policies terms output latency 

chain flush mixed compare chain policies terms memory requirement terms output latency 
figures show output latency different scheduling policies 
figures real data set progress chart operator query described section total queue size time shown 
figures real data set set progress charts queries described section total queue size time shown 
reduce clutter plot average latency consecutive blocks output tuples graphs 
clear figures chain introduces considerable delays producing tuples periods long bursts 
clarity plots greedy round robin shown 
performance similar observed 
interesting observation intervals time chain outputs tuples latency lower produced fifo 
reason behavior follows presence multiple queries fifo restricted processing blocks latency milliseconds chain chain flush latency threshold ms chain flush latency threshold ms fifo time milliseconds output latency vs time single stream operators real data set latency milliseconds time milliseconds chain fifo output latency vs time plan multiple queries real data set tuples global order arrival input streams 
chain similar restriction schedule query plans produce output tuples faster temporarily 
clear peak latency chain higher fifo 
show output latency produced chain flush values latency threshold milliseconds milliseconds 
notice cases chain flush imitates chain closely long chain output latency remains threshold chain flush prevents output latency going threshold 
similar observation 
figures show tradeoff peak queue memory requirement axis latency threshold axis chain flush algorithm 
uses real data set progress chart operator query section total queue size time shown 
seen peak queue memory requirement query happens burst milliseconds 
uses real data set set progress charts latency milliseconds chain chain flush latency threshold ms chain flush latency threshold ms time milliseconds output latency vs time plan multiple queries real data set peak queue size kbytes chain flush greedy fifo round robin chain latency threshold milliseconds peak queue size vs latency threshold single stream operators real data set queries section total queue size time shown 
seen peak queue memory requirement experiment happens burst milliseconds 
latency threshold high chain flush needs invoke flush step behaves identically chain 
latency threshold reduced chain flush starts deviate chain consequently higher peak queue memory requirement 
latency threshold set minimum value sustained workloads peak output latency fifo milliseconds see milliseconds see chain flush queue memory requirement fifo 
lower values latency threshold memory requirement chain flush increases fifo efficiency reasons implementation chain flush mirror fifo flush mode see section 
peak queue size kbytes chain flush round robin fifo greedy chain latency threshold milliseconds peak queue size vs latency threshold plan multiple queries real data set total queue size kbytes fifo mixed slope threshold mixed slope threshold mixed slope threshold chain time milliseconds queue size vs time single stream operators synthetic data set final set experiments shown figures compare mixed algorithm chain fifo algorithms terms run time queue memory requirement terms output latency 
experiments real data set single stream query operators progress chart terms coordinates ti si operator points 
operators progress chart decreasing order selectivity unit time chain puts operator separate segment 
figures show run time queue memory requirement output latency respectively chain fifo invocations mixed mixed mixed mixed slope threshold set respectively 
mixed ends combining third fourth operators single segment runs chain resulting segments 
similarly mixed combines second third fourth operators single segment mixed combines operators single segment 
figures show slope threshold mixed increased behavior deviates latency milliseconds fifo mixed slope threshold mixed slope threshold mixed slope threshold chain time milliseconds output latency vs time single stream operators real data set chain fifo notice performance policies burst milliseconds figures 
related extended version appeared proceedings sigmod 
basic chain algorithm theoretical experimental analysis reported sigmod 
np completeness result showing intractability problem minimizing memory section theoretical results experiments handling latency constraints sections respectively time 
considerable research activity pertaining stream systems data stream algorithms 
overview provided survey ozsu go 
closely related suite research adaptive query processing 
see ieee data engineering bulletin special issue adaptive query processing ll 
papers telegraph project ah cf mshr rdh pertain novel architectures strategies adaptive query processing streams 
research focuses primarily adapting changing data characteristics focus adapting changing arrival characteristics data particular bursty nature data documented networking community fp 
earlier adaptive query processing includes query scrambling urhan ufa adaptive query execution system tukwila iff data integration mid query re optimization techniques developed kabra dewitt kd 
closely related dynamic query operator scheduling aft aimed improving response times face unpredictable bursty data arrival rates xjoin operator urhan franklin uf optimized reduce initial intermittent delay dynamic pipeline scheduling improving interactive performance online queries uf 
cases focus exclusively improving response times considering run time memory minimization 
various operator scheduling strategies suggested stream systems ranging simple ones round robin scheduling complex ones aim leveraging intra nonlinearities processing ccc ccr 
best knowledge address problem scheduling aim minimizing memory usage latency constraints 
assume average input stream arrival rate eventually possible clear unprocessed tuples bursts high arrival rate 
bdm propose algorithms scenario stream system drop input tuples bring system load manageable levels 
algorithms largely orthogonal algorithms propose conjunction algorithms 
algorithms disk efficiently stream systems proposed mt 
algorithms orthogonal algorithms 
related set papers ccc cf mshr sliding window joins stream systems 
rate optimization framework naughton vn considers problem static query optimization modified aim maximizing throughput queries stream systems address run time scheduling 
problem allocating main memory concurrent operators traditional dbms order speed query execution considered dz nd 
techniques extend directly data stream systems 
open problems studied problem operator scheduling data stream systems goal minimizing memory requirements buffering tuples 
proposed chain scheduling strategy proved optimality case single stream queries selections projections foreign key joins static stored relations 
furthermore showed chain scheduling performs types queries including queries sliding window joins 
demonstrated chain result high output latency scenarios proposed adaptations chain algorithm chain flush mixed designed simultaneously achieve low memory usage low latency 
interesting open problems include designing scheduling strategies adaptive query plans structure query plan allowed change time considering sharing computation memory query plans 
aft franklin tomasic 
dynamic query operator scheduling wide area remote access 
journal distributed parallel databases july 
ah avnur hellerstein :10.1.1.34.8546
eddies continuously adaptive query processing 
proc 
acm sigmod intl 
conf 
management data pages may 
bbd babcock babu datar motwani widom 
models issues data stream systems 
proc 
acm symp 
principles database systems june 
babcock babu datar motwani 
chain operator scheduling memory minimization data stream systems 
proc 
acm sigmod intl 
conf 
management data june 
bdm brian babcock datar rajeev motwani 
load shedding aggregation queries data streams 
proc 
intl 
conf 
data engineering february 
appear 
valduriez 
memory adaptive scheduling large query execution 
proc 
acm cikm intl 
conf 
information knowledge management pages november 
ccc carney cetintemel cherniack convey lee stonebraker tatbul zdonik 
monitoring streams new class data management applications 
proc 
th intl 
conf 
large data bases august 
ccr carney cetintemel zdonik cherniack stonebraker 
operator scheduling data stream manager 
proc 
intl 
conf 
large data bases september 
cf chandrasekaran franklin 
streaming queries streaming data 
proc 
th intl 
conf 
large data bases august 
cfp cortes fisher pregibon rogers smith 
hancock language extracting signatures data streams 
proc 
acm sigkdd intl 
conf 
knowledge discovery data mining pages august 
das gehrke 
approximate join processing data streams 
proc 
acm sigmod intl 
conf 
management data june 
dz zait 
sql memory management oracle 
proc 
intl 
conf 
large data bases august 
fp floyd paxson 
wide area traffic failure poisson modeling 
ieee acm transactions networking june 
go ozsu 
issues data stream management 
sigmod record june 
hellerstein franklin chandrasekaran deshpande kris hildrum sam madden raman shah 
adaptive query processing technology evolution 
ieee data engineering bulletin june 
iff ives florescu friedman levy weld 
adaptive query execution system data integration 
proc 
acm sigmod intl 
conf 
management data pages june 
ita internet traffic archive 
www acm org sigcomm ita 
jcss johnson cranor shkapenyuk :10.1.1.68.4467
gigascope stream database network applications 
proc 
acm sigmod intl 
conf 
management data june 
kd kabra dewitt 
efficient mid query re optimization sub optimal query execution plans 
proc 
acm sigmod intl 
conf 
management data pages june 
kao garcia molina 
overview real time database systems 
son editor advances real time systems pages 
prentice hall 
kle kleinberg :10.1.1.13.8182
bursty hierarchical structure streams 
proc 
acm sigkdd intl 
conf 
knowledge discovery data mining august 
kang naughton 
evaluating window joins unbounded streams 
proc 
intl 
conf 
data engineering march 
david karger cliff stein joel wein 
scheduling algorithms 
atallah editor handbook algorithms theory computation 
crc press 
ll lomet levy 
special issue adaptive query processing 
ieee data engineering bulletin june 
leland taqqu willinger wilson 
self similar nature ethernet traffic 
ieee acm transactions networking february 
mshr madden shah hellerstein raman 
continuously adaptive continuous queries streams 
proc 
acm sigmod intl 
conf 
management data june 
mt motwani thomas 
caching queues memory buffers 
proc 
annual acm siam symp 
discrete algorithms january 
motwani widom arasu babcock babu datar manku olston rosenstein varma 
query processing approximation resource management data stream management system 
proc 
biennial conf 
innovative data systems research cidr january 
nd nag dewitt 
memory allocation strategies complex decision support queries 
proc 
acm cikm intl 
conf 
information knowledge management pages november 
nia niagara project 
www cs wisc edu niagara 
pmc parker muntz chau 
stream query processing system 
proc 
intl 
conf 
data engineering pages february 
parker simon valduriez 
svp model capturing sets lists streams parallelism 
proc 
intl 
conf 
large data bases pages august 
rdh raman deshpande hellerstein 
state modules adaptive query processing 
proc 
intl 
conf 
data engineering march 
shah madden franklin hellerstein 
java support data intensive systems experiences building telegraph dataflow system 
sigmod record december 
sqr sqr stream query repository 
www db stanford edu stream sqr 
str stanford stream data management stream project 
www db stanford edu stream 
sul sullivan 
tribeca stream database manager network traffic analysis 
proc 
intl 
conf 
large data bases page september 
tatbul cetintemel zdonik cherniack stonebraker 
load shedding data stream manager 
proc 
intl 
conf 
large data bases pages september 
terry goldberg nichols oki 
continuous queries append databases 
proc 
acm sigmod intl 
conf 
management data pages june 
uf urhan franklin 
xjoin reactively scheduled pipelined join operator 
ieee data engineering bulletin june 
uf urhan franklin 
dynamic pipeline scheduling improving interactive performance online queries 
proc 
intl 
conf 
large data bases september 
ufa urhan franklin 
cost query scrambling initial delays 
proc 
acm sigmod intl 
conf 
management data pages june 
vn naughton 
rate query optimization streaming information sources 
proc 
acm sigmod intl 
conf 
management data june 
wa wilschut apers :10.1.1.56.701
dataflow query execution parallel main memory environment 
proc 
intl 
conf 
parallel distributed information systems pages december 
willinger paxson riedi taqqu 
long range dependence data network traffic 
long range dependence theory applications oppenheim taqqu eds birkhauser 
willinger taqqu erramilli 
bibliographical guide self similar traffic performance modeling modern high speed networks 
kelly zachary editors stochastic networks theory applications pages 
oxford university press 

