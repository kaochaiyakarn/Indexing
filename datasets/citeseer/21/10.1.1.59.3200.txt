wire open source web information retrieval environment carlos castillo dept technology universitat pompeu fabra center web research universidad de chile carlos castillo upf edu describe wire web information retrieval environment project focus details crawler component 
wire crawler scalable highly configurable high performance open source web crawler study characteristics large web collections 

center web research www cwr cl developing software suite research web information retrieval called wire web information retrieval environment 
aim study problems web search creating efficient search engine 
search engines play key role web searching currently generates traffic web sites 
furthermore users arriving web site time clicked link search engine results 
wire software suite generated sub projects including modules depicted 
far developed efficient general purpose web crawler format storing web collection tool extracting statistics collection generating reports search engine page rank non uniform normalization 
objective design crawler collection order millions tens millions documents 
bigger web sites smaller complete web worked national domains country code top level domains cl gr 
main characteristics wire crawler scalability designed large volumes documents tested documents 
current implementation require ricardo baeza yates professor universitat pompeu fabra center web research universidad de chile ricardo baeza upf edu 
possible sub projects wire highlighting completed parts 
scale billions documents process data structures disk main memory 
highly configurable parameters crawling indexing configured including scheduling policies 
high performance entirely written high performance 
downloader modules wire crawler executed machines 
open source programs code freely available gpl license 
presents implementation web crawler detail 
source code technical documentation including user manual available www cwr cl projects wire 
rest organized follows section details main programs crawler section statistics obtained 
section presents 
programs wire environment currently approximately lines code 
code depends library domain name queries library xml parsing configuration file 
section main programs run cycles crawler execution manager harvester gatherer shown 

high level architecture wire crawler 
manager long term scheduling manager program generates list urls downloaded cycle pages default 
procedure generating list maximizing profit downloading page 

operation manager program 
current value page depends estimation intrinsic quality estimation probability changed crawled 
process selecting pages crawled includes filtering pages downloaded estimating quality web pages estimating freshness web pages calculating profit downloading page 
balances process downloading new pages updating ones 
example behavior manager depicted 
select pages cycle give higher profit 
harvester short term scheduling harvester program receives list urls attempts download web 
policy chosen open simultaneous connection website wait configurable amount seconds accesses default 
larger websites certain quantity pages default waiting time reduced default seconds 
shown harvester creates queue web site opens connection active web site sites 
web sites idle transfered pages sites exhausted pages batch 
implemented priority queue web sites inserted time stamp visit 

operation harvester program 
implementation linux threads blocking thread 
worked able go threads pcs processors ghz gb ram 
entire thread system designed threads time higher degrees parallelization 
current implementation uses single thread non blocking array sockets 
poll system call check activity sockets 
harder implement multi threaded version practical terms involves programming context switches explicitly performance better allowing download web sites time lightweight process 
gatherer parsing pages gatherer program receives raw web pages downloaded harvester parses 
current implementation html plain text pages accepted harvester 
parsing html pages done parser 
events oriented parser sax xml build structured representation documents just generates function calls certain conditions met 
substantial amount pages formed tags balanced parser tolerant malformed markup 
contents web pages stored variable sized records indexed document id inserts deletions handled free space list fit allocation 
data structure implements duplicate detection new document stored hash function contents calculated 
document hash function length contents documents compared 
equal document id original document returned new document marked duplicate 

storing contents document 
process storing document contents document id depicted 
storing document crawler check document duplicate search place free space list write document disk 
module requires support create large files large collections disk storage grows gb offset provided variable type long 
linux lfs standard provides offsets type long long disk operations 
usage continuous large files millions pages small files save lot disk seeks noted patterson 
url resolver program receives list urls gatherer adds collection criteria configuration file 
criteria includes patterns accepting rejecting transforming urls 
patterns accepting urls include domain name file name patterns 
domain name patterns suffixes cl uchile cl file name patterns file extensions 
patterns rejecting urls include substrings appear parameters known web applications login logout register lead urls relevant search engine 
avoid duplicates session ids patterns transforming urls remove known session id variables urls 

checking url host name searched hash table web site names 
resulting site id concatenated path filename obtain doc id 
structure holds urls highly optimized common operations crawling process name web site obtain site id web site local link obtain document id full url obtain site id document id process converting full url shown 
process optimized exploit locality web links links page point pages located web site 
implementation uses hash tables converting web site names site ids second converting site id path name doc id obtaining statistics run crawler large collection user specify site suffix es crawled kr upf edu provide starting list seed urls 
crawling limits provided including maximum number pages site default maximum exploration depth default levels dynamic pages static pages 
configurable parameters including amount time crawler waits accesses web site fine tuned distinguishing large small sites number simultaneous downloads timeout downloading pages 
standard pc ghz intel processor gb ram standard ide disks usually download parse pages day 
wire stores metadata possible web pages web sites crawl includes tools extracting data obtaining statistics 
analysis includes running link analysis algorithms pagerank hits aggregating information documents sites generating histograms property stored system 
includes module detecting language document dictionary stopwords languages included wire 
process generating reports includes analysis data extraction generation gnuplot scripts plotting compilation automated reports atex 
generated reports include distribution language histograms degree link scores page depth response codes age including site average minimum maximum summations link scores site histogram pages site bytes site analysis components web structure distribution links multimedia files links domains outside delimited working set crawler 
far wire study large web collections including national domains brazil chile greece south korea 
currently developing module supporting multiple text encodings including unicode 
downloading thousands pages bunch web sites relatively easy building web crawler deal millions pages misconfigured web servers bad html coding requires solving lot technical problems 
source code documentation wire including step step instructions running web crawl analysing results available athttp www cwr cl projects wire doc 
search engine referrals nearly double worldwide 
com html id 
xml parser toolkit gnome 
www org 
baeza yates castillo 
de la web 
technical report center web research university chile 
baeza yates davis 
web page ranking link attributes 
alternate track papers posters th international conference world wide web pages new york ny usa 
acm press 
baeza yates 
characteristics korean web 
technical report korea chile cooperation center 
broder kumar maghoul raghavan rajagopalan stata tomkins wiener 
graph structure web experiments models 
proceedings ninth conference world wide web pages amsterdam netherlands may 
acm press 
castillo 
effective web crawling 
phd thesis university chile 
castillo baeza yates 
new crawling model 
poster proceedings eleventh conference world wide web honolulu hawaii usa 
castillo 
charting greek web 
proceedings conference american society information science technology providence rhode island usa november 
american society information science technology 
jackson 

www org uk ian 
jaeger 
large file support linux 
www de aj linux lfs html 
kleinberg 
authoritative sources hyperlinked environment 
journal acm 

simple api xml sax 
sax sourceforge net 
pereira ziviani castillo baeza yates 
un novo da web 
proceedings brazil 
nielsen 
statistics traffic referred search engines navigation directories 
www com html 
page brin motwani winograd 
page rank citation ranking bringing order web 
technical report stanford digital library technologies project 
patterson 
writing search engine hard 
acm queue april 
