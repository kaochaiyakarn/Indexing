independent components natural scenes edge filters 
anthony bell terrence sejnowski howard hughes medical institute computational neurobiology laboratory salk institute pines road la jolla california field suggested neurons line edge selectivities primary visual cortex cats monkeys form sparse distributed natural scenes barlow reasoned responses emerge unsupervised learning algorithm attempts find factorial code independent visual features 
show new unsupervised learning algorithm information maximisation non linear infomax network bell sejnowski applied ensemble natural scenes produces sets visual filters localised oriented 
filters gabor resemble produced sparseness maximisation network olshausen field 
addition outputs filters independent possible infomax network able perform independent components analysis ica 
compare resulting ica filters associated basis functions decorrelating filters produced principal components analysis pca zero phase whitening filters zca 
ica filters sparsely distributed outputs natural scenes 
resemble receptive fields simple cells visual cortex suggests neurons form natural informationtheoretic ordinate system natural images 
please send comments tony salk edu 
submitted vision research 

classic experiments hubel wiesel neurons visual cortex decades feature detection vision marr hildreth left open question succinctly phrased barlow edge detectors 
coding principles predict formation localised oriented receptive fields 
barlow answer edges suspicious coincidences image 
mathematical framework analysing coincidences information theory cover thomas 
barlow led propose visual cortical feature detectors result redundancy reduction process barlow atick activation feature detector supposed statistically independent possible 
factorial code potentially involves dependencies orders studies second order statistics required decorrelating outputs set feature detectors 
variety hebbian feature learning algorithms decorrelation proposed linsker miller oja sanger atick redlich particular external constraints solutions decorrelation problem non unique see section 
popular decorrelating solution principal components analysis pca principal components natural scenes amount global spatial frequency analysis hancock 
second order statistics suffice predict formation localised edge detectors 
additional constraints required 
field argued importance sparse minimum entropy coding barlow feature detector activated rarely possible 
led algorithms intrator projection pursuit huber flavour successful olshausen field demonstration self organisation local oriented receptive fields sparseness criterion 
results similar olshausen field direct information theoretic criterion maximises joint entropy non linearly transformed output feature vector 
previously demonstrated ability non linear information maximisation process bell sejnowski find statistically independent components solve problem separating mixed audio sources jutten herault 
dependent components analysis ica problem comon equivalent barlow redundancy reduction problem barlow reasoning correct expect ica solution yield localised edge detectors 
primary result 
secondary result outputs resulting filters sparsely distributed decorrelating filters supporting arguments field helping explain results olshausen network information theoretic point view 
come back issues sparseness noise higher order statistics discussion section 
describe concretely problem 
earlier account application techniques natural sounds appears bell sejnowski 
blind separation natural images 
starting point olshausen field depicted 
perceptual system exposed series small image patches drawn larger images 
imagine image patch represented vector formed linear combination basis functions 
basis functions form columns fixed matrix weighting linear combination varies image vector component vector associated basis function represents underlying cause image 
linear image synthesis model goal perceptual system simplified framework linearly transform images matrix filters resulting vector wx recovers underlying causes possibly different order rescaled 
representing arbitrary permutation matrix zero single row column arbitrary scaling matrix non zero entries diagonal system converged pss scaling permuting causes arbitrary factors consider causes defined ps identity matrix 
basis functions columns filters recover causes rows simple relation gamma remains defining algorithm learn decide constitutes cause 
number proposals discussed section sections concentrate algorithms producing causes decorrelated attempting produce causes statistically independent 
decorrelation independence 
matrix decorrelating matrix covariance matrix output vector satisfies diagonal matrix general decorrelate 
example case eq gamma clearly leaves freedom choice special solutions eq 

orthogonal global solution ww 
principal components analysis pca orthogonal solution eq 
principal components come eigenvectors covariance matrix columns matrix satisfying ede gamma diagonal matrix eigenvalues 
substituting eq eq solving gives pca solution gamma solution unusual filters rows orthogonal ww gamma scaling matrix 
filters special properties pca filters define orthogonal directions vector space image 
pca basis functions columns rows gammat see fig just scaled versions pca filters rows 
property true ww gamma means gammat dw 
image statistics stationary field pca filters global fourier filters ordered amplitude spectrum image 
example pca filters shown fig 

symmetrical local solution ww 
force symmetrical solution wz eq wz gamma decorrelating filters pca basis functions filters coming wz different orthogonal 
call solution zca filters produces zero phase symmetrical 
zca ways polar opposite pca 
produces local centre surround type whitening filters ordered phase spectrum image 
filter pixel image preserving spatial arrangement image flattening frequency amplitude spectrum 
wz related transforms described goodall atick redlich 
example zca filters basis functions shown fig 

independent semi local solution 
way constrain solution attempt produce outputs just decorrelated statistically independent stronger requirement independent components analysis ica jutten herault comon 
independent probability distribution fu follows fu equivalently zero mutual information number approaches ica relations describe notably cardoso laheld karhunen amari cichocki pham 
refer reader papers bell sejnowski background ica 
show section ica natural images produces decorrelating filters sensitive phase locality frequency information just transforms involving oriented gabor functions daugman wavelets semi local depicted fig path local zca global pca solutions space decorrelating solutions 
example ica filters shown fig corresponding basis functions shown fig 
ica algorithm 
important recognise differences finding ica solution decorrelation methods 
may ica solution ica algorithm may find solution exists approximations involved 
senses ica different pca zca calculated analytically example second order statistics covariance matrix gaussian case 
approach developed bell sejnowski maximise stochastic gradient ascent joint entropy linear transform squashed sigmoidal function non linear function scaling shifting cumulative density functions underlying independent components shown nadal parga non linear infomax procedure minimises mutual information exactly required ica 
cases pick non linearity detailed knowledge probability density functions underlying independent components 
resulting mismatch gradient nonlinearity underlying may cause infomax solution deviate ica solution 
cases super gaussian meaning longer tailed gaussian having kurtosis greater repeatedly observed logistic tanh nonlinearities maximisation leads ica solutions exist experiments speech signal separation bell 
infomax algorithm described ica see proceedings ieee april special issue wavelets 
previous conference bell sejnowski published proof result ought referenced equivalent proof nadal parga 
algorithm fuller understanding needs developed exactly conditions may fail converge ica solution 
basic infomax algorithm changes weights entropy gradient 
defining transformed output variables learning rule deltaw ln jj denotes expected value un jj absolute value determinant jacobian matrix det ij stochastic gradient ascent remove expected value operator eq evaluate gradient give bell sejnowski deltaw gamma yx yn elements depend nonlinearity follows ln amari cichocki yang proposed modification rule utilises natural gradient absolute gradient 
natural gradient exists objective functions functions matrices case relative gradient concept developed cardoso laheld 
amounts multiplying absolute gradient giving case altered version eq deltaw yu rule twin advantages eq avoiding matrix inverse converging orders magnitude quickly data 
speedup explained fact convergence longer dependent conditioning underlying basis function matrix eq 
equivariant property explained cardoso laheld 
writing eq terms individual weights deltaw ij ij kj weighted sum non local term rule seen result simple backwards pass weights linear output vector inputs weight knows influence input possible write rule recurrent terms 
known jutten herault network may feedback matrix giving network gamma vu 
solving gives gamma showing just coordinate transform eq 
learning rule coordinate transform rule calculated follows 
relationship gamma may write gamma gamma differentiating quotient rule matrices gives deltav delta gamma gamma deltaw gamma inserting eq rearranging gives learning rule feedback weight matrix deltav yu terms individual feedback weight ij rule deltav ij ffi ij ij ik ffi ij 
feedback rule nonlocal time involving backwards pass recurrent weights quantities calculated non linear output vector recurrent ica system developed recovering sources linearly convolved temporal filters torkkola 
non locality algorithm interesting come consider biological significance learned filters section 
methods 
took natural scenes involving trees leafs converted greyscale byte values 
training set fxg generated samples images 
training set sphered subtracting mean multiplying times local symmetrical zero phase whitening filter eq fxg wz fxg gamma hxi removes second order statistics data covariance matrix equal 
appropriately scaled starting point training infomax eq raw data logistic function gammau gamma produces vector approximately satisfies 
way ensure subsequent transformation wx learnt approximate orthonormal matrix rotation scaling roughly satisfying relation karhunen 
moves solution decorrelating manifold zca ica see fig 
matrix initialised identity matrix trained logistic function version eq eq evaluates gamma training conducted follows sweeps data performed order data vectors permuted avoid cyclical behaviour learning 
sweep weights updated presentations order matlab code efficient 
learning rate proportionality constant eq set follows sweeps sweeps 
process took hours running matlab sparc machine reasonable result filters achieved minutes 
verify result affected starting condition training repeated randomly initialised weight matrices data 
results qualitatively similar convergence slower 
full ica transform raw image calculated product sphering zca matrix learnt matrix images gif files available web directory ftp ftp cnl salk edu pub tony basis function matrix calculated gamma pca matrix calculated eq 
original data transformed decorrelating transforms kurtosis filters calculated formula gamma hu hu gamma hu ii gamma mean kurtosis filter type ica pca zca calculated averaging filters input data 
results 
filters basis functions resulting training natural scenes displayed fig fig 
fig displays example filters basis functions type 
pca filters fig spatially global ordered frequency 
zca filters basis functions spatially local ordered phase 
ica filters trained zca whitened images fig original images fig semi local filters specific orientation preference 
basis functions fig calculated fig ica filters local naturally spatial frequency characteristics original images 
basis functions calculated fig pca filters corresponding filters matrix orthogonal 
order show full variety ica filters fig shows lower resolution filters matrix reverse order vector lengths filters filters corresponding higher variance independent components appear top 
general result ica filters localised oriented 
basis functions displayed olshausen field cover broad range spatial frequencies 
appropriate comparison ica basis functions basis functions olshausen field 
ica basis functions fig oriented localised difficult observe multiscale properties 
ran ica algorithm olshausen images preprocessed whitening lowpass filter algorithm yielded basis functions localised multiscale gabor patches similar olshausen 
part difference results attributable different preprocessing techniques 
discussion comparison approaches deferred section 
fig shows result analysing distributions image histograms produced filter types 
ruderman field general form histograms double exponential sparse meaning long tail compared gaussian 
shows clearly fig log histograms seen roughly linear orders magnitude 
histogram ica filters departs linearity having longer tail zca pca histograms 
spreading tail signals greater sparseness outputs ica filters reflected kurtosis measure ica compared pca zca 
univariate statistics capture part story fig displayed contour plots average bivariate log histograms pairs filters ica zca pca respectively 
contrast joint probability distributions fig shows corresponding distribution outputs filters independent outer product marginal univariate distributions fig 
ica joint histogram captures diamond shape characteristic product sparse univariate distributions satisfying greater extent independence criterion 
summary simulations show filters ica algorithm eq logistic non linearity localised oriented produce outputs distributions high kurtosis 
significance results addressed 
discussion substantial literature exists self organisation visual receptive fields 
contributions emphasised roles decorrelation pca oja sanger miller hancock 
accompanied information theoretic arguments 
lines linsker proposed infomax principle underlies 
linsker approach atick redlich bialek van hateren uses second order covariance matrix approximation required information theoretic quantities generally assumes gaussian signal gaussian noise case second order information complete 
explicit noise model restriction second order statistics mark differences approaches approach infomax 
noise 
assumption noise model generally thought necessary ingredient 
case decorrelating filters local zca type see section noise model required atick redlich avoid centre surround receptive fields peaks single pixel wide fig see atick redlich 
case pca style global filters noise automatically associated filters high spatial frequency selectivity eigenvectors small eigenvalues 
cases questionable assumptions noise useful 
case pca priori reason associate signal low spatial frequency noise high spatial frequency 
contrary sharp edges presumably high interest contain high frequency components 
case local zca type filters form spatial integration assumed necessary average photon shot noise 
know photoreceptors brains associated operate single photon detection regime 
shot noise cases considered neural systems noisy ignored systems appear operate limit spatial acuity allowed lattices receptors 
general information theoretic framework distinguish signal noise priori question concept noise models 
course signals lesser greater relevance organism signature spatial temporal structure distinguishes important 
signal noise subjective concepts prior expectations organism neural subsystem 
case simple linear mappings considering internal state filters store prior expectations consider noiseless infomax appropriate framework making level predictions information theoretic reasoning 
second higher order statistics 
second difference earlier infomax models restriction secondorder statistics questioned field olshausen field 
coincided general rise awareness simple hebbian style algorithms special constraints unable produce local oriented receptive fields area visual cortex produce solutions pca zca type depending constraint put decorrelating filter matrix technical reason failure second order statistics correspond amplitude spectrum signal fourier transform autocorrelation function image power spectrum square amplitude spectrum 
remaining information higher order statistics corresponds phase spectrum 
phase spectrum consider informative part signal remove phase information image looks noise remove amplitude information example zero phase whitening zca transform image recognisable 
edges consider features images suspicious coincidences phase spectrum fourier analysis edge consists sine waves different frequencies aligned phase edge occurred 
noise feel general information theoretic approach required 
time mean approach account statistics orders 
approach sensitive phase spectra images characteristic local structure 
borne results report demonstrate emergence local oriented receptive fields second order statistics fail predict 
sparseness 
approaches arisen deal unsatisfactory results simple hebbian anti hebbian schemes 
field emphasised barlow arguments goal image transformation convert higher order redundancy 
formal terms output filters may write joint entropy sum individual entropies minus mutual information gamma meant higher order redundancy term 
creation minimum entropy codes shifting redundancy term terms 
assuming term constant minimisation creates minimum entropy marginal distributions 
low entropy example means distribution sparse quality identified field fourth moment distribution kurtosis 
sparse distributions long tails positive kurtosis 
referred super gaussian 
field arguments led olshausen field motivated approach attempt learn receptive fields maximising sparseness 
terms fig attempted find receptive fields identified basis functions columns matrix underlying causes sparsely distributed possible 
sparseness constraint imposed non linear function pushes activity components zero 
search minimum entropy sparse codes guarantee attainment factorial code infomax net increase redundancy distributions maintaining full basis set general remove mutual information elements similarity results produced olshausen network may explained fact produce sparsest possible distributions different means 
emphasising sparseness directly information theoretic criterion olshausen field force causes low mutual information decorrelated 
basis function matrices singular non invertible making difficult say filters correspond basis functions 
flaw 
presently reason decorrelation full rank filter matrix absolutely necessary properties neural coding system 
interesting point shown fig fig sought property sparseness emergent property direct information theoretic approach 
exploration kurtosis seeking network performed fyfe baddeley slightly negative 
study baddeley argued kurtosis maximisation partly grounds produce filters pixels wide 
extent results fig filters achieving highest kurtosis fig seen dominated thin edge detectors 
result debatable see section 
projection pursuit approaches 
sparseness captured kurtosis projection index mentioned projection pursuit methods huber look multivariate data directions interesting distributions 
intrator pioneered application projection pursuit reasoning feature extraction problems 
index emphasizing multimodal projections connected bcm bienenstock cooper munro learning rule 
law cooper bcm rule self organise oriented somewhat localised receptive fields ensemble natural images 
bcm rule non linear hebbian anti hebbian mechanism 
nonlinearity undoubtedly contributes higher order statistical information clear olshausen network nonlinearity contributes solution 
principle predictability minimisation brought bear problem schmidhuber 
approach attempts ensure independence output moving receptive field away predictable nonlinear lateral network outputs 
prager formalised inhibitory feedback network learns non orthogonal oriented receptive fields 
biological significance 
simplest properties classical simple cell receptive fields hubel wiesel local oriented properties filters fig failing emerge external constraints previous self organising network models linsker miller atick redlich 
transformation retina analog signals spike coding pyramidal cells clearly complex matrix working 
evidence feedforward origin oriented properties simple cells cat ferster 
zca filters approximate static response properties ganglion cells retina relay cells lateral geniculate nucleus approximation inputs reaching cortex 
accept primitive model transformation objections arise 
object representation learned algorithm filters fig predominantly high spatial frequency octave spread seen cortex hubel wiesel 
reason high spatial frequency filters smaller required tile pixel array filter 
active control eye movements topographic nature spatial maps means visual cortex samples images different way random spatially unordered sampling pixel patches 
changing model realistic respects produce different results 
judge algorithm biologically implausible 
learning rule eq non local 
non locality severe original algorithm involved matrix inverse 
feedforward eq feedback eq versions involves backpropagation information output layer 
try imagine mechanism capable performing back propagation 
difficult identify parameters static matrix true biophysical parameters prefer imagine potentially real biophysical self organisational processes see example bell occur local spatial media feedforward feedback information tightly functionally coupled microscopic dynamic analogue eq may operate 
final note notable learning rule deviation simple hebbian anti hebbian correlational way thinking unsupervised learning 
correlational component eq nonlinearly transformed output term weighted feedback linear outputs 
experimental search biophysical learning mechanisms focus simple correlational hebbian rules 

analysis problem learning single layer linear filters ensemble natural images 
localised edge detectors produced result information theoretic learning rule phase sensitivity result sensitivity rule higher order statistics 
edges level invariance images detectable linear filters 
levels invariance shifting rotating scaling lighting clearly exist natural objects natural settings 
levels may extractable similar information theoretic techniques multi layer non linear networks necessary 
done greatly increase computational empirical predictive power unsupervised learning techniques 
emerged extremely useful discussions bruno olshausen david field 
grateful paul viola barak pearlmutter helpful discussions 
supported howard hughes medical institute 
amari cichocki yang 
new learning algorithm blind signal separation advances neural information processing systems mit press 
atick redlich 
theory early visual processing neural computation atick 
information theory provide ecological theory sensory processing 
network atick redlich 
convergent algorithm sensory receptive field development neural computation baddeley 
searching filters interesting output distributions uninteresting direction explore network appear barlow 
unsupervised learning neural computation barlow 
edge detectors 
optical society america technical digest 
barlow 
computational goal neocortex 
koch 
ed 
large scale neuronal theories brain 
cambridge mass mit press 
bell 
self organisation real neurons anti hebb channel space moody eds advances neural information processing systems morgan kaufmann bell sejnowski 
information maximization approach blind separation blind deconvolution neural computation bell sejnowski 
fast blind separation information theory proc 
intern 
symp 
nonlinear theory applications las vegas dec bell sejnowski 
learning higher order structure natural sound network computation neural systems bialek ruderman zee 
optimal sampling natural images design principle visual system 
advances neural information processing systems touretzky ed morgan kaufmann bienenstock cooper munro 
theory development neuron selectivity orientation specificity binocular interaction visual cortex neurosci 
burr morrone 
feature detection biological artificial vision systems 
ed 
vision coding efficiency camb 
univ press cardoso 
laheld 
equivariant adaptive source separation ieee trans 
signal proc appear cichocki unbehauen 
robust learning algorithm blind separation signals electronics letters comon 
independent component analysis new concept 
signal processing cover thomas 
elements information theory john wiley 
daugman 
uncertainty relation resolution space spatial frequency orientation optimized dimensional visual cortical filters opt 
soc 
am ferster chung wheat 
orientation selectivity thalamic input simple cells cat visual cortex nature 
field 
relations statistics natural images response properties cortical cells opt 
soc 
am 
field 
goal sensory coding 
neural computation 
forming sparse representations local anti hebbian learning biol 
cybern fyfe baddeley 
finding compact sparse distributed representations visual images network goodall 
performance stochastic net nature hancock baddeley smith 
principal components natural images network 
prager 
development low entropy coding recurrent network network appear haykin 
ed 

blind deconvolution prentice hall new jersey 
hubel wiesel 
receptive fields functional architecture monkey striate cortex physiol hubel wiesel 
uniformity monkey striate cortex parallel relationship field size scatter magnification factor 
comp 


huber 
projection pursuit annals statistics intrator 
feature extraction unsupervised neural network neural computation jutten herault 
blind separation sources part adaptive algorithm neuromimetic architecture signal processing karhunen wang 
neural estimation basis vectors independent component analysis proc 
icann paris karhunen oja wang 
class neural networks independent component analysis submitted ieee trans 
neural networks 

simple coding procedure enhances neuron information capacity law cooper 
formation receptive fields realistic visual environments cooper munro bcm theory proc 
natl 
acad 
sci usa linsker 
local synaptic learning rules suffice maximise mutual information linear network neural computation linsker 
self organization perceptual network 
computer marr hildreth 
theory edge detection 
proc 
soc 
lond 
ser 


miller 
correlation models neural development neuroscience connectionist theory gluck rumelhart eds lawrence erlbaum hillsdale nj nadal 
parga 
non linear neurons low noise limit factorial code maximises information transfer 
network 
oja 
neural networks principal components linear neural networks neural networks 
olshausen field 
natural image statistics efficient coding network computation neural systems 
pham jutten 
separation mixture independent sources maximum likelihood approach proc 
eusipco ruderman 
statistics natural images network computation neural systems sanger 
optimal unsupervised learning single layer network neural networks schmidhuber 
semi linear predictability minimization produces known feature detectors neural computation appear 
formation organisation receptive fields input environment composed natural scenes ph thesis dept physics brown university 
torkkola 
blind separation convolved sources information maximisation proc 
ieee workshop neural networks signal processing japan sept van hateren 
theory maximising sensory information biol 
cybern filters basis functions causes image patch image ensemble blind linear image synthesis model olshausen field 
patch image viewed linear combination underlying basis functions matrix associated element underlying vector causes causes viewed statistically independent image sources 
causes recovered vector matrix filters loosely receptive fields attempt invert unknown mixing unknown basis functions constituting image formation 
space pca zca ica decorrelating local frequency global frequency global space local space semi local schematic depiction weight space 
subspace matrices represented loop property decorrelating input vectors manifold special linear transformations distinguished pca global space local frequency zca local space global frequency ica privileged decorrelating matrix exists higher second order moments 
ica filters localised single pixel level zca filters see fig pca zca ica selected decorrelating filters basis functions extracted natural scene data 
type decorrelating filter yielded filters display subset 
column contains filters basis functions particular type rows number relating row filter basis function matrix displayed 
pca st th th principal components calculated eq showing increasing spatial frequency 
need show basis functions filters separately pca thing 
zca wz entries column show pixel wide centre surround filter preserving phase spectrum 
identical shifted 
lower entries show basis functions columns inverse wz matrix 
weights learnt ica network trained wz whitened data showing descending order dc filter localised oriented filters localised checkerboard filters 
corresponding ica filters calculated looking whitened versions filters 
corresponding basis functions columns gamma patterns optimally stimulate corresponding ica filters stimulating ica filter matrix filters obtained training zca whitened natural images 
filter row matrix ordered left right top bottom reverse order length filter vectors 
rough characterisation order appearance filters consist dc filter top left oriented filters diagonal vertical horizontal localised checkerboard patterns 
diagonal filters longer vertical horizontal due bias induced having square circular receptive fields 
ica kurt zca kurt pca kurt filter output log probability ln log distributions univariate statistics outputs ica zca pca filters averaged filters type 
approximately double exponential distributions ica distribution slightly longer tail showing sparser 
ica zca pca contour plots log distributions pairwise statistics outputs ica zca pca filters 
left column joint log distributions averaged pairs output filters type images 
right column product marginal univariate distributions 
ica solution best satisfies independence criterion joint distribution form product marginal distributions 
