dynamic metadata management scale file systems sage weil sage cs ucsc edu pollack cs ucsc edu scale distributed file systems decouple read write metadata operations behavior metadata server cluster critical system performance scalability 
dynamic subtree partitioning adaptive metadata management system designed efficiently manage hierarchical metadata workloads evolve time 
examine relative merits approach context traditional workload partitioning strategies demonstrate performance scalability adaptability advantages simulation environment 
compelling architecture large distributed storage systems involves decoupling metadata transactions file read write operations 
system client consult metadata server mds cluster responsible maintaining file system namespace receive permission open file information specifying location contents 
subsequent reading writing takes place independent mds cluster communicating directly object storage devices intelligently manage disk storage enforce security policies 
size metadata relatively small compared size system metadata operations may file system operations making performance mds cluster critical importance 
furthermore capacity osd cluster easily scale increasing number relatively independently operating devices metadata exhibit higher degree interdependence making design scalable system challenging 
metadata server cluster system efficiently maintain file system directory permission semantics variety workloads including scientific computing applications general purpose computing 
workloads file systems may involve files ieee scott brandt scott cs ucsc edu university california santa cruz ethan miller elm cs ucsc edu ranging bytes multiple terabytes directories containing millions files thousands clients accessing disparate identical files 
furthermore character workload may change time mds cluster able continually adapt current demands dynamically repartitioning workload maintain high system performance long term scalability 
describe dynamic metadata management system efficiently distribute responsibility metadata extremely large file systems cluster servers 
utilize dynamic subtree partitioning strategy continually adapt metadata distribution current demands facilitate traffic control simultaneously preserving exploiting locality 
evaluate competing metadata management strategies simulation basis system performance adaptation file systems workloads evolve time ultimately ability scale efficiently manage metadata large storage systems 
results demonstrate effectiveness dynamic subtree partitioning static partitioning hash approaches 
background examine distributed metadata management context scale byte storage system designed university california santa cruz handle general purpose scientific computing workloads exporting posix compliant interface 
architecture consist tens metadata servers thousands object storage devices potentially hundreds thousands clients 
applications system include scientific computing environments internet archive large data centers storage demands may typical distributed file systems years time 
system architecture primary design features object storage systems separation metadata data management 
decoupling metadata operations depend metadata server cluster metadata access metadata storage clients object storage data access storage system architecture 
complicated interdependent set semantics file data trivially parallelizable bottleneck typical conventional file servers avoided 
intelligent object storage devices simplify file system design handling block level allocation internally presenting simple object interface 
clients communicate separate metadata server cluster responsible managing file system namespace directory hierarchy file directory permissions mapping files objects 
clients read write data contacting directly allowing efficient data transfers large numbers clients 
data distribution file data striped replicated large number objects large number maximize throughput data safety 
data storage traditional file systems typically seek group related files data distributed deterministic pseudo random algorithm guarantees probabilistically balanced distribution data system 
critical feature file data distribution algorithm sequence object identifiers osd devices recalculated client interaction mds cluster single small input value algorithm inode number 
system value augmented replication group identifier facilitate reliable replication recovery 
details distribution reliability mechanism described implication file object mapping defined file fixed size bytes 
simplifies metadata management storage avoiding cumbersome block object lists typical file systems 
metadata partitioning metadata workload effectively partitioned cluster metadata servers average case behavior properly utilizes available resources system efficiently cope extreme workloads thousands clients opening file writing directory 
partition cache overlap servers minimized max mds cluster ability mask requests underlying metadata storage subsystem 
system augmented failover mechanism failed node workload redistributed servers assumed standby 
significantly size storage system grows mds cluster able expand encompass additional servers minimal effort required redistribute workload 
storage ultimately metadata stored sort permanent disk storage 
metadata file system may contain files consume terabyte disk space 
large reside completely collective ram metadata server cluster 
ideally mds memory caches satisfy reads periodically need go disk retrieve requested information updates saved stable store disk 
generally speaking shared metadata store consisting necessary appropriate partitioning strategies including offers fundamental advantages directly attached storage easing mds failover utilizing generic hardware 
workload file read write operations involve primarily client metadata cluster need concern relatively restricted set operations 
metadata transactions fall categories operations close applied metadata records files directories inodes operations manipulate directory entries defining file system namespace hierarchy 
typical sequences operations tend represent majority file system metadata workload open followed close readdir followed stats 
addition efficient operation general case system additionally handle extreme usage patterns common scientific computing applications common flash crowd behavior general purpose workloads including thousands clients opening file creating files directory 
mds able keep metadata ram cluster design scale collective cache mask sufficient read operations reduce demands response times reasonable level 
related draws number areas including distributed metadata management architectures file system simulation file system workload characterization 
metadata distribution emergence large scale storage architectures separate metadata management file read write operations metadata management interesting research problem 
designing metadata server cluster partitioning metadata servers critical importance maintaining efficient mds operation desirable load distribution cluster 
current approaches traditional modern distributed file systems serve basis discussion performance scalability issues involved 
subtree partitioning networked file systems traditionally partitioned workload storage statically assigning portions directory hierarchy different file servers approach taken systems nfs afs coda sprite countless 
static subtree partitioning typically requires system administrator decide file system distributed manually assign subtrees hierarchy individual file servers 
static distribution simplifies clients task identifying servers responsible metadata servers able process requests communicating aware nodes subtrees hierarchy treated independent structures 
strategy allow storage system scale breadth depth 
statically partitioned cluster accommodate file system expansion growth data remains evenly partitioned available servers new data written exclusively new servers allocated new portions hierarchy 
file systems typically expand regular fashion requiring manual redistribution hierarchy accommodate new data increased client demand existing data 
furthermore client workload evenly distributed file data static partitioning vulnerable imbalance individual servers overloaded hot spots popularity certain parts hierarchy 
hashing provide load balancing metadata servers systems distribute files servers hash unique file identifier inode number path name 
vesta intermezzo rama lustre hash file pathname unique identifier determine location metadata data 
long mapping defined simple strategy number advantages 
clients locate contact responsible mds directly average workloads behaved hash functions requests evenly distributed cluster 
hot spots activity hierarchical directory structure heavy create activity single directory correlate individual metadata servers metadata location relation directory hierarchy 
hot spots consisting individual files overwhelm single responsible mds 
directing requests random servers allow cluster distribute sudden load caching metadata expense costly average case 
google file system attempts address problem read replication metadata shadow metadata servers alleviate hot spots formed clients trying read file expense vastly simplified metadata semantics 
hashed metadata distribution process expanding mds cluster accommodate growth difficult size desired output range suddenly changes 
significantly distributing metadata hashing eliminates hierarchical locality locality benefits typical local file systems 
systems distribute metadata hash directory portion path allow directory contents grouped mds nodes disk 
approach facilitates prefetching methods exploiting locality metadata workload 
satisfy posix directory access semantics mds cluster traverse prefix ancestor directories containing requested piece metadata ensure directory permissions allow current user access metadata data question 
files directories located mds scattered directory hierarchy hashed metadata distribution results high overhead traversal metadata scattered multiple servers cache prefixes replicated locally 
prefix caches nodes exhibit high degree overlap parent directories inodes replicated mds serving children consuming memory resources caching data 
lazy hybrid lazy hybrid lh metadata management seeks capitalize benefits hashed distribution avoiding problems associated path traversal merging net effect permission check file metadata record 
hashing approaches lh uses hash file full path name distribute metadata 
alleviate potentially high cost traversing paths scattered cluster lh uses dual entry access control list stores effective access information entire path traversal metadata file 
information usually represented compactly large general purpose file systems 
lh need traverse path access controls need updated ancestor directory access permissions changed affecting effective permissions files nested beneath 
similarly renaming moving directory affects path name hash output metadata location files nested beneath requiring metadata migrated 
previous trace analysis shown changes happen infrequently affect small numbers files occur 
possible perform update time avoid sudden burst network activity metadata servers having mds maintain log updates fully propagated lazily update nested items requested 
lh avoids path traversal cases provided certain metadata operations sufficiently infrequent workload 
analysis shown update cost amortized network trip affected file long updates eventually applied quickly created changes directories containing lots items trigger potentially millions updates single update lh delivers net savings scalability 
file hashing approaches avoids overloading single mds presence directory hot spots scattering directories 
doing locality benefits lost system remains vulnerable individually popular files 
importantly low update overhead essential lh performance predicated low prevalence specific metadata operations may hold workloads 
dynamic metadata management metadata cluster architecture utilizing dynamic subtree partitioning strategy distributing metadata cluster metadata servers 
subtree partitioning natural approach partitioning hierarchy provides number advantages hashed distributions including greater mds independence greater locality workload 
difficult create maintain partition dynamic partitioning strategy employed adapt changing file system workload 
dynamic partitioning number advantages techniques terms metadata storage traffic control flexible resource utilization policies 
hierarchical partition central dynamic subtree partitioning approach treatment file system hierarchy 
file system partitioned delegating authority subtrees hierarchy different metadata servers 
delegations may nested usr may assigned mds instance usr local reassigned 
absence explicit subtree assignment entire directory tree nested beneath point assumed reside server 
implicit structure process hierarchy traversal order nested inodes located opened subsequent descent file hierarchy 
path traversal necessary verify user access permissions nested items required posix semantics 
process may costly locating file deep directory hierarchy locality typical scientific general purpose computing workloads allows costs amortized subsequent accesses directories :10.1.1.10.5857
importantly lh permission management hierarchically defined structure allows system move change effective permissions arbitrarily sized subtrees directory tree modifying subtree root directory fixed cost 
likewise individual subtrees hierarchy fully independent siblings semantics dependent prefix ancestor directories leading root file system 
allow client requests path traversal required properly respond processed efficiently mds caches prefix inodes items cache point cached subset hierarchy remains tree structure 
leaf items may expired cache directories may removed items contained expired 
allows permission verification known items proceed additional costs hierarchical consistency preserved 
authority collaborative caching metadata updates serialized point metadata cluster atomicity consistency maintained 
significant body research investigated distributed locking consistency strategies potentially unreliable environments experience shown simplest solutions perform best provided scale appropriately 
maximize average case performance metadata item defined authority mds hierarchical partition responsible serializing updates committing changes stable storage managing cache consistency coherence record replicated nodes mds cluster 
mds node receives request portion hierarchy responsible ordinarily forward request authority 
needs replicate metadata directory popular flagged replication mds needs prefix metadata traverse subtree hierarchy responsible request relevant inode authority 
item replicated mds cache authoritative mds responsible communicating updates maintain cache coherence 
similarly node discards inode authoritative cache notify authority free remove copy memory 
approach ensures mds cluster state file system remains consistent defined 
certain cases updates metadata distributed 
instance fields modification time file size monotonically increasing operations replicas serving concurrent writers periodically send value authority retains maximum value seen far initiates callback latest information client reads stat check file size 
approach taken gpfs file system facilitate shared parallel write access files typical scientific computing workloads 
requirements client consistency potentially demanding 
system serving hundreds thousands clients require significant amount memory maintain state necessary provide clients strong consistency guarantees callback cache coherence data block level sprite coda file systems 
conversely weak consistency provided fully stateless approaches nfs earlier cause number problems client applications 
believe relatively simple inexpensive metadata coherence strategies may prove sufficient appropriate approach dependent specific workload requirements outside scope 
load balancing order adapt file system evolution changing workload demands mds cluster adjust directory partition maintain optimal distribution workload 
dynamic distribution necessary size popularity portions hierarchy change time non uniform unpredictable fashion 
metadata partition modified time allowing mds nodes transfer authority subtrees directory hierarchy 
periodically mds nodes exchange heartbeat messages include description current load level 
point busy nodes identify portions hierarchy appropriately popular initiate double commit transaction transfer authority non busy nodes 
exchange ac tive state cached metadata transfered newly authoritative node maintain consistency avoid disk required newly authoritative node re read orders magnitude slower 
prototype busy node initially try entire trees delegated delegating subtrees workload nodes 
helps keep partition simple possible 
investigation distributed algorithms exchanging subtrees hierarchy warranted minimize complexity partition small overhead associated delegation authority cache containing directory prefix inodes subtrees 
dynamic subtree partition improvement statically hashed directory distribution individual directories essentially randomly delegated 
smallest unit authority delegation purely subtree partition directory 
single directory extraordinarily large busy may undesirable inefficient reside single mds 
address situation individual directory contents hashed cluster authority directory entry defined hash file name directory inode number 
delegation authority defined file name individual mds nodes act independently directory operations requires communication directories 
similar directory hashing approach utilized lustre propose decision hash directory dynamic directories grow popular may appropriate hash shrink popular consolidated single node efficient manipulation storage 
distribution workload cluster typically seeks balance load 
distribution dependent appropriate load metric number mds resources may limit mds performance including memory cpu network utilization 
distributing metadata mds throughput equalize relative performance mds nodes may maximize cluster efficiency different nodes may bound different resource constraints 
portions hierarchy cluster current working set may equally busy nodes managing widely sparsely utilized portions hierarchy may limited memory nodes serving busy hot spots may bound cpu network utilize small fraction available cache 
robust load balancing strategy seek equalize utilization resources cluster 
balanced distribution may ideal 
fair distribution workload consuming scarce resource memory may ensure mds caches operate equally inefficiently example 
experimentation indicates maximizing total cluster throughput necessarily achieved balanced workload distribution 
furthermore may appropriate assume systems metadata equally important 
hashed partitioning strategies probabilistically equalize resource utilization distributing metadata cluster reality mds performance favor items near root hierarchy items prefixes cached 
contrast dynamic distribution algorithm predicated hierarchical performance metric need vanilla balancing 
policies formulated prioritize active portions file system expense archival data instance adjusting subtree partition appropriately flexibility possible hashed distributions ignore file system structure 
traffic control effectively adapt changing workload mds cluster cope situations large number clients access file directory hierarchy time period time suddenly warning 
unfortunately extremely popular files directories sudden flash crowds common scientific computing workloads large numbers nodes may acting unison general purpose workloads large numbers users access similar files due external events 
tens thousands clients access single mds simultaneously node able handle request workload efficiently 
fundamental problem client knowledge metadata partition clients know access piece metadata time welldefined hashing strategy instance prevent simultaneously accessing item 
similarly clients ignorant metadata distribution requests directed randomly forwarded mds cluster pass sort proxy case requiring extra network hop requests 
ideally combination situations access unpopular items directed authoritative mds nodes access popular items directed nodes replicating popular metadata distribute traffic 
dynamic subtree partitioning strategy control client requests directed clients initial ignorance metadata distribution achieve traffic flow popular unpopular metadata 
mds nodes monitor popularity metadata simple access counter value decays time measure estimate extent item appears client caches precision isn necessary 
responses sent clients include current distribution information mds nodes client contact metadata requested prefix directories cached client 
unpopular items mds cluster tell clients direct requests authoritative node popular items client told item replicated nodes 
popularity metric approximates prevalence item client caches mds cluster effectively bound number nodes believing particular file subtree file hierarchy located place times avoiding potential flash crowds occur allowing requests unpopular data directed efficiently 
strategy works explored unexplored portions hierarchy 
client requests directed deepest known prefix potential flood requests initiate set mutually known popular directories extreme cases root directory known clients consequently highly replicated 
directory locality hierarchical partition metadata facilitates exploitation locality workload existing structure file system preserved 
exploit workload locality storing directly related information possible prefetching potentially related information inodes directory efficiently satisfy requests typical workloads 
traditional file systems store inodes large table typically split cylinder groups order keep close associated directory entries disk 
global inode table facilitates ability create multiple hard links different file names different directories inode distribution table disk serves minimize expensive disk seeks directory lookup invariably followed operation involving file inode 
general approach metadata storage persisted various forms decades despite fact overwhelming majority files linked single directory entry 
store inode metadata directly directory entries link embedded inodes ffs 
process performing directory lookup mds cluster simultaneously fetches embedded inodes subsequent transactions require additional disk seeks table lookups 
addition making single lookup operations faster entire contents directories prefetched cache multiple lookups directory common scientific general purpose computing workloads satisfied disk prefetched metadata inserted near tail cache lru list avoid displacing known useful information information potentially useful reducing window 
embedding inodes directories avoids difficult problem efficiently consistently managing distributed table sparsely populated potentially billions inodes 
lack globally indexed inode table certain implications notably alternative simpler mechanism allocated unique identifiers employed 
inodes longer globally inode number contexts inode numbers may revealed limited containing directory known accessible 
inodes exposed clients file opened read write instance mds take care remember inode stored hierarchy disk retain inodes deleted open 
similarly usual posix treatment multiple hard links file directory requires modification directory entry linking inode embedded different directory index locate 
problem fundamentally complicated fact inode location file system hierarchy change containing parent prefix directories moved renamed path name reliable identifier reasons symbolic links durable 
fortunately hard links rare operations avoid lookup inode table containing multiply linked inodes table cheaper maintain vastly reduced size 
slower access names locality benefits embedded inodes partially exploited locating inode directory entry providing mechanism locate 
accomplished global table mapping inode numbers parent directory inode numbers populating multiply linked inodes ancestor directories 
combined count nested items embedded inodes located recursively identifying containing directories 
table easily modified directories moved hierarchy counts facilitate inclusion directories inodes necessary 
similar structure ffs ffs counter subsequently include directories lookup table 
storage metadata transactions quickly written stable storage safety 
significant portion reads expected satisfied metadata memory caches primary demand raw write bandwidth 
utilize bounded log structure immediate storage updates metadata server 
entries fall log subsequent modifications written second permanent tier storage 
log size order amount memory mds arrangement convenient property log represents approximation node working set allowing memory cache quickly preloaded millions records startup failure 
nvram metadata servers mask latency writes log storage 
ideally long term data layout optimized reads expected access patterns allow related records fetched additional disk seeks 
file system strategy abandoned favor write approach authors simply writing metadata disk order modified preserves temporal locality similarly advantageous 
system clients expect temporal correlation access patterns insignificant 
store directory contents embedded inodes better match expected file system usage patterns 
directory sizes vary widely sized objects located collection provide appropriate shared storage medium 
directory contents entries inodes stored tree structure similar xfs allows incremental updates small numbers creates deletes minimal modifications disk structures rewriting changed tree nodes 
tree structure facilitates copy write techniques safe updates advanced file system features snapshots 
appear appropriate choice short term mds logs shared access facilitates takeover case node failure 
evaluation validate design decisions evaluate effectiveness specific design choices implemented dynamic metadata management system event driven simulation environment static subtree partitioning hashing files directories lazy hybrid metadata strategies serve points comparison 
simulations validate number design hypotheses 
show subtree partitioning strategies allow metadata servers operate higher efficiency avoiding duplication data nodes effectively utilizing available memory 
show utility exploiting directory locality improving mds performance reducing demands 
demonstrate ability dynamic partitioning strategy control flash crowds exploiting client ignorance 
illustrate distribution workload complex issue traditional load balancing homogeneous distributions ideal 
simulation simulation analysis provides practical mechanism evaluate file system design behavior accuracy results dependent detail file system code implemented 
focus simulation efforts mds behavior workload generation underlying disk storage behavior 
significant body research investigated accurate disk simulation storage system evaluation distributed metadata management systems evaluating exist underlying disk subsystem 
reason simplify storage simulation reflect average disk latencies transactional throughputs 
contrast metadata server prototype implements simulates features system design including metadata updates cache coherence mds cluster embedded inodes tiered storage mechanism dynamic subtree partitioning load balancing traffic control 
hashing static subtree servers implement subsets functionality accommodate different partitioning mechanisms 
metadata quite small feasible simulate full scale system consisting tens mds nodes millions files simulated server maintains state real system 
run simulations smaller file systems mds memory somewhat fewer clients appropriately throttled rates 
environment demonstrate efficiency performance scaling behavior range system variables including mds cluster size cache size file system size 
initial metadata partition dynamic static subtree partitioning created hashing directories near root hierarchy 
dynamic subtree partitioning system clearly suboptimal strategy small subtrees near root hierarchy scattered facilitates testing generating relatively distribution quickly minor performance penalty 
prototype real time workload distribution algorithm attempts balance load redistributing metadata single load metric weighted combination node throughput cache misses 
approach primitive experience shown poor choice maximizing total cluster throughput sufficient show promise dynamic partitioning strategy alternatives 
workload effectiveness file system simulations relies heavily type workload utilized 
general trace real system preferable generated workload accurately reflects realistic access patterns 
number file system traces analyzed literature publicly available correct metadata server simulation requires trace file system activity snapshot file system metadata 
chose simulate client workload prior research characterizing file system usage executed snapshots actual file systems readily available 
approach provides larger body available content feed simulations allowing easily scale simulated client workload 
metadata operations comprising generated client workload primarily study trace general purpose workload 
simulated clients submit different types metadata operations frequencies mimic observed general usage patterns 
client activity engineered favor operations localized areas file hierarchy resemble typical general purpose workloads 
simulations scientific computing workloads analysis file system traces scientific computing clusters lawrence livermore national labs 
analysis bursts activity nodes access file set files directory 
extreme locality exhibited clients accessing similar files presents difficult challenge metadata management general purpose workloads clients exhibit individual independent locality system facilitate highly concurrent access individual files ordinarily residing individual servers 
performance scalability initially evaluate relative performance scalability different metadata management strategies fixing mds memory scaling entire system file system size number mds servers client base 
shows performance degradation individual mds nodes different system sizes predominately static file system client workload 
dynamic static subtree partitioning show best performance difference static strategy employ load balancing adjust initial partition 
real workload environment static partition practical file systems workloads evolve time hierarchies typically easily partitioned workload large collection home directories 
apparent performance penalty load balancing interesting discussed detail section 
significantly perfor average mds throughput ops sec mds cluster size mds performance file system cluster size client base scaled 
mance file directory hashed distributions degrades quickly subtree partitions due inefficiencies analyzed section 
file hashing lazy hybrid distributions show significantly lower performance due inefficient metadata operations involve disk requests load individual inodes cache 
contrast subtree directory hashing partitioning strategies exploit presence locality workload embedding inodes storing entire directories disk allow efficient lookups prefetching 
benefits approach best seen contrasting performance directory file hashing strategies identical 
lazy hybrid performance interesting scales linearly due ability avoid performing path traversals evaluated workload 
ability predicated rarity modifications directory permissions hierarchy lazily eventually propagated potentially large quantities metadata 
prefix caching performance metadata partitioning strategies tightly linked metadata cache efficiency 
primary factors affecting cache utilization need cache prefix inodes ancestor directories purposes path traversal 
overhead associated caching prefix inodes hashed partitions particularly high directories scattered hierarchy prefix directory inodes locate replicated widely cluster 
shows percentage mds cache associated prefix inodes file system client base cluster size scale 
utilization static subtree partitioning represents baseline file system simulated related ratio directories files average branching factor average file depth 
dynamic subtree partition devotes slightly cache prefixes fraction cache consumed prefixes mds servers percentage cache devoted prefix inodes file system client base mds cluster size scales 
hashed distributions devote large portions caches prefix directories 
dynamic subtree partition slightly prefixes static partition due re delegation subtrees nested hierarchy 
cache hit rate cache size relative total metadata size cache hit rate function cache size fraction total file system size 
smaller caches inefficient cache utilization due replicated prefixes results lower hit rates 
anchor subtrees nested hierarchy re delegated mds nodes balance load 
consumption cache memory prefix inodes effect decreasing cache hit rate mds performance 
extent affects performance related average depth directories hierarchy obviously flat namespace easily distributed lazy hybrid tries artificially flatten namespace achieve effect 
prefix cache overhead greater smaller cache sizes memory scarce demand prefixes path traversal related distribution requests file system just factors proportional size cache 
shows cache performance varies cache size expressed fraction total size file system meta data 
note convergence hit rates cache size increases predicated degree locality workload random distribution requests result performance similar smaller cache sizes 
dynamic partitioning workload evolution static dynamic subtree partitioning strategies show similar performance cases total throughput static partition better 
difference mechanisms tested dynamic approach implements simple load balancing algorithm dynamically redistribute workload replicate popular items 
key points relative performance merit approaches 
interesting observation perfectly balanced distribution load may ideal depending benefit metric 
cluster performance measured simply cluster throughput perfect load balance counterproductive tends ensure nodes achieve equally mediocre performance 
contrast serving portions hierarchy disproportionately result extremely high cache hit rates certain nodes extremely fast response times clients nodes poor cache performance go disk requests 
effect highlights fact load balancing fairness best approach reflected irregular static partitioning performance due irregular partition 
dynamic partitioning strategy ultimately implement kind workload distribution policy faire hands approach resembling static partition implementing policies favoring important portions hierarchy 
fundamental problem static partition hierarchy file systems file system workloads static 
directory hierarchies grow non uniform ways time distribution client requests changes faster 
hashed approaches rely behaved hash functions maintain distribution subtree approaches intentionally coarse simplicity efficiency subsequently intelligently adjust workload demands 
shows relative performance dynamic static subtree partitioning approach changing workload 
short time half clients change local region activity create new files portions hierarchy served single mds 
static approach results heavy load mds saturation single busy mds nodes remain relatively idle dynamic strategy adapts re delegating subtrees busy node workload non busy nodes 
despite primitive load balancing algorithm employed average mds throughput significantly higher dynamic approach cluster adapt 
mds throughput ops sec time dynamic subtree static subtree range average throughput shown dynamic workload 
clients migrate create files new portions hierarchy static subtree distribution remains unbalanced dynamic partition re balances load achieves higher average performance migrating newly popular portions hierarchy non busy nodes 
client ignorance consequence dynamic lesser degree static subtree partitioning clients initially ignorant location metadata mds cluster 
clients discover new portions hierarchy requests may directed non authoritative mds servers incurring forwarding overhead 
similarly movement metadata mds nodes workload partition changes time result requests 
forwarding requests clusters interconnect network cheap effect observed client latency 
shows forwarded requests dynamic workload client activity shifts new portion file system 
static partitioning employed forwards reflect clients initial process discovering new portions file system dynamic approach additionally requires rediscover metadata migrated nodes due load balancing 
spike time represents shift workload clients move modify new portions file system higher level dynamic partitioning due load balancing algorithm 
traffic control key advantages dynamic partitioning strategy ability manage client ignorance prevent simultaneous access tens thousands users overwhelming individual metadata server 
shows number requests processed time individual nodes mds cluster clients simultaneously request file scenario typical scientific computing workloads 
requests requests sec requests sec portion requests forwarded time dynamic subtree static subtree forwarded requests static dynamic partitioning dynamic workload 
spike represents shift workload difference point highlights overhead due client ignorance metadata movement dynamic load balancing 
replies forwards time replies forwards traffic control top nodes forward requests authoritative mds slowly responds sequence 
traffic control bottom authoritative node quickly replicates popular item nodes respond requests 
directed randomly clients know mds node responsible file 
traffic control top mds nodes simply forward requests authoritative node quickly saturated slowly real situations inefficiently responds 
traffic control enabled bottom authority quickly recognizes file sudden popularity replicates metadata nodes 
response time flash crowd begins effectively distributed cluster dependent number factors including replication threshold rate client requests received forwarded mds nodes latency requests may required load requested metadata cache 
response time reduced non authoritative mds nodes recognized sudden flood requests preemptively cached metadata requested waiting told authoritative node noticed flood requests waiting metadata loaded disk 
metadata server cluster designed service scale distributed file system 
system utilizes dynamic subtree partitioning strategy distribute workload maximizing scalability 
utilize embedded inodes exploit locality scientific computing general purpose workloads simplify storage 
metadata stored tiered strategy initially writing updates log fast commits stable storage quick recovery cache warming committing directory contents object storage pool 
leverage dynamic metadata distribution collaborative caching framework avoid flash crowds preemptively replicating popular metadata distributing client requests file system hot spots 
simulations indicate uniform workload distributions inherent hashing approaches ideal static subtree partitions fail adapt file system workload evolution time 
show dynamic partitioning approach accommodate variety load distribution policies effectively accommodate heterogeneous system growth scaling terms total file system size client base 
ultimately performance dynamically partitioned system depend algorithms appropriately distribute workload load balancing policies 
simple load balancing algorithm utilized simulation prototype sufficient demonstrate adaptability scalability advantages dynamic approach relative static subtree hashed distributions metadata intelligent algorithms heuristics may necessary control incremental redistribution changing directory hierarchy workload real system 
furthermore fully validate simulation findings fully distributed working prototype necessary evaluate real world performance scale 
currently completing development metadata cluster server software object distributed file system test design large cluster environment 
thorough performance evaluation require testing wider range workloads 
actual workload traces matching file system metadata snapshots allow evaluate system behavior realistic workloads 
full scale eval uation realistic synthetic workloads need generated reflect expected workload characteristics scale 
callahan schwan 
intermezzo file system 
proceedings rd perl conference reilly open source convention monterey ca usa aug 

lustre storage architecture 
brandt xue miller long 
efficient metadata management large distributed file systems 
proceedings th ieee th nasa goddard conference mass storage systems technologies pages apr 

cabrera long 
swift distributed disk striping provide high data rates 
computing systems 
corbett feitelson 
vesta parallel file system 
acm transactions computer systems 
floyd ellis 
directory patterns hierarchical file systems 
ieee transactions knowledge data engineering 
ganger kaashoek 
embedded inodes explicit groupings exploiting disk bandwidth small files 
proceedings usenix annual technical conference pages 
usenix association jan 
ghemawat gobioff 
leung 
google file system 
proceedings th acm symposium operating systems principles sosp bolton landing ny oct 
acm 
gibson nagle amiri butler chang gobioff hardin riedel zelenka 
cost effective high bandwidth storage architecture 
proceedings th international conference architectural support programming languages operating systems asplos pages san jose ca oct 
hitz lau 
file system design nfs file server appliance 
proceedings winter usenix technical conference pages san francisco ca jan 
miller 
replication scalable hashing family algorithms scalable decentralized data distribution 
proceedings th international parallel distributed processing symposium ipdps santa fe nm apr 
ieee 
long brandt miller wang lin xue xin 
design implementation large scale objectbased storage system 
technical report ucsc crl university california santa cruz nov 
mckusick joy leffler fabry 
fast file system unix 
acm transactions computer systems aug 
miller katz 
rama easy high performance parallel file system 
parallel computing 
morris satyanarayanan conner howard rosenthal smith 
andrew distributed personal computing environment 
communications acm mar 
ousterhout douglis nelson welch 
sprite network operating system 
ieee computer feb 
pawlowski smith hitz 
nfs version design implementation 
proceedings summer usenix technical conference pages 
rodeh 
scalable distributed file system object disks 
proceedings th ieee th nasa goddard conference mass storage systems technologies pages apr 
roselli lorch anderson 
comparison file system workloads 
proceedings usenix annual technical conference pages june 
satyanarayanan kistler kumar okasaki siegel steere 
coda highly available file system distributed workstation environment 
ieee transactions computers 
schmuck 
gpfs shared disk file system large computing clusters 
proceedings conference file storage technologies fast pages 
usenix jan 
schwan 
lustre building file system node clusters 
proceedings linux symposium july 
sweeney hu anderson peck 
scalability xfs file system 
proceedings usenix annual technical conference pages jan 
thekkath wilkes lazowska 
techniques file system simulation 
software practice experience spe nov 
wang brandt miller long 
file system object storage devices 
proceedings st ieee th nasa goddard conference mass storage systems technologies college park md apr 
ieee 
wang xin hong brandt miller long 
file system workload analysis large scale scientific computing applications 
proceedings st ieee th nasa goddard conference mass storage systems technologies college park md apr 
wilkes 
pantheon storage system simulator 
technical report hpl ssp storage systems program computer systems laboratory hewlett packard laboratories palo alto ca may 
xin miller schwarz long brandt litwin 
reliability mechanisms large storage systems 
proceedings th ieee th nasa goddard conference mass storage systems technologies pages apr 
