journal artificial intelligence research submitted published xx hybrid bdi pomdp framework multiagent nair nair usc edu milind tambe tambe usc edu department computer science university southern california th place los angeles ca current large scale multiagent team implementations characterized intention bdi paradigm explicit representation team plans 
despite promise current bdi team approaches lack tools quantitative performance analysis uncertainty 
distributed partially observable markov decision problems pomdps suited analysis complexity finding optimal policies models highly intractable 
key contribution article hybrid bdi pomdp approach bdi team plans exploited improve pomdp tractability pomdp analysis improves bdi team plan performance 
concretely focus role allocation fundamental problem bdi teams agents allocate different roles team 
article provides key contributions 
describe role allocation technique takes account uncertainties domain prior multiagent role allocation failed address uncertainties 
introduce rmtdp role markov team decision problem new distributed pomdp model analysis role allocations 
technique gains tractability significantly rmtdp policy search particular bdi team plans provide incomplete rmtdp policies rmtdp policy search fills gaps incomplete policies searching best role allocation 
second key contribution novel decomposition technique improve rmtdp policy search efficiency 
limited searching role allocations combinatorially role allocations evaluating rmtdp identify best extremely difficult 
decomposition technique exploits structure bdi team plans significantly prune search space role allocations 
third key contribution significantly faster policy evaluation algorithm suited bdi pomdp hybrid approach 
experimental results domains mission rehearsal simulation robocuprescue disaster rescue simulation 

teamwork software agents robots people critical capability large number multiagent domains ranging mission rehearsal simulations robocup soccer disaster rescue personal assistant teams 
large number multiagent teams developed range domains pynadath tambe yen yin miller xu volz stone veloso jennings grosz kraus decker lesser tambe pynadath da silva demazeau 
existing practical approaches characterized situated general belief desire intention bdi approach paradigm designing multiagent systems increasingly popular due programming frameworks tambe decker lesser tidhar facilitate design large scale teams 
approach inspired explicitly implicitly bdi logics agents explicitly represent reason team goals plans wooldridge 
article focuses analysis bdi teams provide feedback aid human developers possibly agents participating team team performance complex dynamic domains ai access foundation morgan kaufmann publishers 
rights reserved 
improved 
particular focuses critical challenge role allocation building teams tidhar rao sonenberg grosz agents allocate various roles team :10.1.1.56.6956
instance mission rehearsal simulations tambe need select numbers types helicopter agents allocate different roles team 
similarly disaster rescue kitano noda matsubara takahashi shimada role allocation refers allocating fire engines ambulances fires greatly impact team performance 
domains performance team linked important metrics loss human life property critical analyze team performance suggest improvements 
bdi frameworks facilitate human design large scale teams key difficulty analyzing role allocation teams due uncertainty arises complex domains 
example actions may fail world state may partially observable agents owing physical properties environment imperfect sensing 
role allocation demands uncertainties taken account fact agent may fail execution may may replaced taken account determining role allocation 
current role allocation algorithms address uncertainty see section 
uncertainty requires quantitative comparison different role allocations 
tools quantitative evaluations bdi teams currently absent 
uncertainties may required experimentally recreate large number possible scenarios real domain simulations evaluate compare different role allocations 
fortunately emergence distributed partially observable markov decision problems pomdps provides models bernstein zilberstein immerman boutilier pynadath tambe xuan lesser zilberstein quantitative analysis agent teams uncertain domains 
distributed pomdps represent class formal models powerful express uncertainty dynamic domains arising result non determinism partial observability principle generate evaluate complete policies multiagent team 
shortcomings models prevents application analysis role allocation 
previous analysis focused communication pynadath tambe xuan role allocation coordination decisions 
second shown bernstein 
problem deriving optimal policy generally computationally intractable corresponding decision problem nexp complete 
applying optimal policies analysis highly intractable 
address difficulty derive rmtdp role multiagent team decision problem distributed pomdp framework quantitatively analyzing role allocations 
framework show general problem finding optimal role allocation policy computationally intractable corresponding decision problem nexp complete 
shows improving tractability analysis techniques role allocation critically important issue 
order quantitative analysis multiagent teams rmtdp tractable second contribution provides hybrid bdi pomdp approach combines native strengths bdi pomdp approaches ability bdi frameworks encode large scale team plans pomdp ability quantitatively evaluate plans 
hybrid approach key interactions improve tractability rmtdp optimality bdi agent teams 
interaction shown 
particular suppose wish analyze bdi agent team agent consisting bdi team plan domain independent interpreter helps coordinate plans acting domain 
shown model domain rmtdp rely bdi team plan interpreter providing incomplete policy rmtdp 
rmtdp model evaluates different completions incomplete policy provides optimally completed policy feedback bdi system 
rmtdp fills gaps incompletely specified bdi team plan optimally 
gaps rmtdp search policy space completed policy additions bdi team plan incomplete policy domain rmtdp model bdi team plan bdi interpreter integration bdi pomdp 
concentrate role allocations method applied key coordination decisions 
restricting optimization role allocation decisions fixing policy points able come restricted policy space 
effectively search restricted space order find optimal role allocation 
restricted policy search key positive interaction hybrid approach second interaction consists efficient policy representation converting bdi team plan interpreter corresponding policy see new algorithm policy evaluation 
general agent policy distributed pomdp indexed observation history bernstein pynadath tambe 
bdi system agent performs action selection set privately held beliefs obtained agent observations applying belief revision function 
order evaluate team performance sufficient rmtdp index agents policies belief state represented privately held beliefs observation histories 
shift representation results considerable savings amount time needed evaluate policy space required represent policy 
third key interaction hybrid approach exploits bdi team plan structure increasing efficiency rmtdp analysis 
rmtdp policy space restricted filling gaps incomplete policies policies may result large number possible role allocations 
enumerating evaluating possible policy domain difficult 
provide branch bound algorithm exploits task decomposition sub teams team significantly prune search space provide correctness proof worst case analysis algorithm 
order empirically validate approach applied rmtdp allocation bdi teams concrete domains mission rehearsal simulations tambe robocuprescue kitano 
significant speed gained interactions mentioned 
domains compared role allocations approach state theart techniques allocate roles uncertainty reasoning 
comparison shows importance reasoning uncertainty determining role allocation complex multiagent domains 
robocuprescue domain compared allocations allocations chosen humans actual robocuprescue simulation environment 
results showed role allocation technique article capable performing human expert levels robocuprescue domain 
article organized follows section presents background motivation 
section introduce rmtdp model key complexity results 
section explains bdi team plan evaluated rmtdp 
section describes analysis methodology finding optimal role allocation section presents empirical evaluation methodology 
section related section list 

background section describes domains consider article mission rehearsal domain tambe robocuprescue domain kitano 
domain requires allocate roles agents team 
team oriented programming top framework describing team plans described context domains 
focus top discussed section techniques applicable frameworks tasking teams stone veloso decker lesser 
domains domain consider mission rehearsal simulations tambe 
expository purposes intentionally simplified 
scenario follows helicopter team executing mission transporting valuable cargo point point enemy terrain see 
paths different lengths different risk due enemy fire 
scouting sub teams sent path larger size scouting sub team safer scouts clear path transports move safely path 
scouts may fail path may need replaced transport cost transporting cargo 
owing partial observability transports may receive observation scout failed route cleared 
wish transport amount cargo quickest possible manner mission deadline 
key role allocation decision fixed number helicopters allocated scouting transport roles 
allocating scouts means scouting task succeed fewer helicopters left transport cargo consequently reward 
allocating scouts result mission failing altogether 
allocating scouts routes scouts sent 
shortest route preferable risky 
sending scouts route decreases likelihood failure individual scout beneficial send different routes scouts risky short route safe longer route 
role allocations consider 
evaluating difficult role allocation look ahead consider implications uncertainty scout helicopters fail scouting may need replaced transport 
furthermore failure success scout may visible transport helicopters transport may replace scout transports may fly destination 
second example scenario see set robocuprescue disaster simulation environment kitano consists fire engines different fire stations stations station ambulances ambulance center 
fires top left bottom right corners map start need extinguished fire engines 
transports scout route route route mission rehearsal domain 
enemy gun fire extinguished ambulance agents need save surviving civilians 
number civilians location known ahead time total number civilians known 
time passes high likelihood health civilians deteriorate fires increase intensity 
agents need rescue civilians possible minimal damage buildings 
part goal scenario determine fire engines assign fire 
fire engines gathered information number civilians fire transmitted ambulances 
part goal allocate ambulances particular fire rescue civilians trapped 
ambulances rescue civilians fires fully extinguished 
partial observability agent view objects visual range uncertainty related fire intensity location civilians health add significantly difficulty 
team oriented programming aim team oriented programming top pynadath tambe tambe tidhar framework provide human developers automated symbolic planners useful abstraction tasking teams 
domains described section consists key aspects team team organization hierarchy consisting roles ii team reactive plan hierarchy iii assignment roles sub plans plan hierarchy 
developer need specify low level coordination details 
top interpreter underlying coordination infrastructure automatically enables agents decide communicate reallocate roles failure 
top abstraction enables humans rapidly provide team plans large scale teams unfortunately qualitative assessment team performance feasible 
key top weakness inability quantitatively evaluate optimize team performance allocating roles agents qualitative matching capabilities may feasible 
discussed hybrid bdi pomdp model addresses weakness providing techniques quantitative evaluation 
concrete example consider top mission rehearsal domain 
specify team organization hierarchy see 
task force highest level team organization consists roles scouting transport scouting sub team roles scouting sub sub teams 
specify hierarchy reactive team plans 
reactive team plans explicitly express joint activities relevant team consist pre conditions plan proposed ii termination conditions plan ended iii team level actions robocuprescue scenario denote fire locations denote fire stations respectively denotes ambulance center 
executed part plan example plan discussed shortly 
highest level plan execute mission sub plans doscouting path safe transports move transports path scouts reached destination get 
task force scouting team transport team transport team doscouting task force execute mission task force scouting team scouting team transport team top mission rehearsal domain organization hierarchy plan hierarchy 
shows coordination relationships relationship indicated solid arc relationship indicated dashed arc done need performed 
temporal dependence relationship sub plans implies sub teams assigned perform doscouting plan completed 
execute parallel 
assign roles plans shows assignment brackets adjacent plans 
instance task force team assigned jointly perform execute mission assigned 
team plan corresponding execute mission shown 
seen team plan consists context pre conditions post conditions body constraints 
context describes conditions fulfilled parent plan pre conditions particular conditions cause sub plan execution 
execute mission pre condition team mutually believes mb start location 
post conditions divided achieved unachievable irrelevant conditions sub plan terminated 
body consists sub plans exist team plan 
lastly constraints describe temporal constraints exist sub plans body 
description plans plan hierarchy appendix context pre conditions mb location start achieved mb achieved doscouting achieved time mb achieved helo alive helo location helo unachievable mb unachievable doscouting mb unachievable achieved helo alive helo location helo irrelevant body doscouting constraints doscouting doscouting example team plan 
mb refers mutual belief 
just htn dix avila nau zhang erol hendler nau plan hierarchy top gives decomposition task smaller tasks :10.1.1.145.3912
language tops richer language early htn planning erol contained just simple ordering constraints 
seen example plan hierarchy tops contain relationships 
addition just htn planning dix sub plans tops contain preconditions post conditions allowing conditional plan execution 
main differences tops htn planning tops contain organization hierarchy addition plan hierarchy ii top interpreter ensures team executes plans coherently 
seen tops analyzed 
mutual belief wooldridge shown mb team refers private belief held agent team believe fact true agents team believe true agent believes agent believes true 
infinite levels nesting difficult realize practice 
practical bdi implementations purposes article mutual belief approximated private belief held agent agents team believe true 
new observation agent belief update function mapping observations beliefs 
private beliefs agent expressiveness including conditional execution analysis focus fixed time horizon loops task description unrolled time horizon 
execution agent copy top 
agent maintains set private beliefs set propositions agent believes true see 
agent receives new beliefs observations including communication belief update function update set privately held beliefs 
instance seeing scout crashed transport may update privately held beliefs include belief doscouting 
practical bdi systems belief update computation low complexity constant linear time 
beliefs updated agent selects plan execute matching beliefs pre conditions plans 
basic execution cycle similar standard reactive planning systems prs georgeff lansky 
team plan execution observations form communications arise coordination actions executed top interpreter 
instance top interpreters exploited bdi theories teamwork levesque theory joint intentions levesque cohen nunes require agent comes privately believe fact terminates current team plan matches achievement conditions team plan communicates fact rest team 
performing coordination actions automatically top interpreter enables coherence initiation termination team plans top 
details examples tops seen pynadath tambe tambe tidhar concretely illustrate key challenges role allocation mentioned earlier 
human developer allocate available agents organization hierarchy find best role allocation 
combinatorially allocations choose grosz tambe 
instance starting just homogeneous helicopters results different ways deciding agents assign scouting transport sub team 
problem exacerbated fact best allocation varies significantly domain variations 
example shows different assignments agents team organization hierarchy analysis best setting failure observation probabilities details section 
example increasing probability failures routes resulted number transports best allocation changing see see additional scout added 
failures possible number transports increased see 
analysis takes step selecting best allocations 
shows top robocuprescue scenario 
seen plan hierarchy scenario consists pair plans done parallel decompose individual plans 
individual plans get fire engines ambulances move streets specific search algorithms individual plans relevant discussions article interested readers refer description robocuprescue team entered robocup competitions nair ito tambe marsella 
organizational hierarchy consists task force comprising engine sub teams fire task force scouting team transport team medium probability task force scouting team transport team low probability task force scouting team transport team zero probability best role allocations different probabilities scout failure 
ambulance team engine teams assigned fires ambulance team assigned civilians 
particular top assignment ambulances conditioned communication indicated 
described detail refers communication received fire engines describes number civilians fire 
problem engines assign engine team possible value ambulances assign ambulance team 
note engines differing capabilities owing differing distances fires ambulances identical capabilities 
task force task force top robocuprescue scenario organization hierarchy plan hierarchy 

role multiagent team decision problem multiagent team decision problem mtdp pynadath tambe inspired economic theory teams marschak radner ho yoshikawa 
order quantitative analysis key coordination decisions multiagent teams extend mtdp analysis coordination actions interest 
example com mtdp pynadath tambe extension mtdp analysis communication 
article illustrate general methodology analysis aspects coordination rmtdp model quantitative analysis role allocation reallocation concrete example 
contrast bdi systems introduced previous section rmtdp enables explicit quantitative optimization team performance 
note mtdp possible distributed pomdp models potentially serve basis bernstein xuan 
multiagent team decision problem team agents mtdp pynadath tambe defined tuple consists finite set states feature world state 
agent perform action set actions ai nai 
gives probability transitioning state state agents perform actions 
jointly 
agent receives observation function 

gives probability agents receive observations 
world state perform 
jointly 
agents receive single joint reward 
state joint action 

joint reward shared equally members private reward individual agents receive actions 
agents motivated behave team actions jointly yield maximum expected reward 
agent mtdp chooses actions local policy mapping observation history actions 
time agent perform action 

contrasts single agent pomdp index agent policy belief state probability distribution world state kaelbling littman cassandra shown sufficient statistic order compute optimal policy sondik :10.1.1.107.9127
unfortunately directly pomdp techniques kaelbling maintaining updating belief states kaelbling mtdp single agent pomdp mtdp agent observation depends actions unknown actions agents 
distributed pomdp models bernstein xuan mtdp local policies indexed observation histories 

refers joint policy team agents 
extension explicit coordination mtdp step methodology explicit separation actions coordination actions interest 
earlier introduced com mtdp model pynadath tambe coordination action fixed communication action got separated 
coordination actions separated domain level actions order investigate impact 
investigate role allocation actions allocating agents roles reallocate roles separated 
define rmtdp role multiagent team decision problem tuple rl new component rl 
particular rl 
rs set roles agents undertake 
instance role rj may assigned agent fulfill 
actions agent distinguishable types role actions contains role actions agent means agent takes role rj rl 
role execution actions rj rl contains execution actions agent set agent actions executing role rj rl addition define set states roles feature roles vector gives current role agent taken 
reason introducing new feature assist mapping bdi team plan rmtdp 
time agent performs new role action successfully value feature roles updated reflect change 
key model agent initial role action subsequent role reallocation 
modeling allocation reallocation important accurate analysis bdi teams 
note agent observe part feature pertaining current role may observe parts pertaining agents roles 
roles allows represent specialized behaviors associated role transport vs scout role 
filling particular role rj agent perform role execution actions may different role execution actions irl role rl 
feature roles filter actions role execution actions correspond agent current role permitted 
worst case filtering affect computational complexity see theorem practice significantly improve performance trying find optimal policy team number domain actions agent choose restricted role agent taken 
different roles produce varied effects world state modeled transition probabilities team reward 
policies ensure agents role capabilities benefit team 
just mtdp agent chooses action perform indexing local policy observation history 
epoch agents doing role actions doing role execution actions 
agent local policy divided local role role execution policies observation histories 

null 
null 

refers joint role policy team agents 
refers joint role execution policy 
article explicitly model communicative actions special action 
communication treated role execution action communication received agents treated observations 
complexity results rmtdp section qualitatively emphasized difficulty role allocation rmtdp helps understanding complexity precisely 
goal rmtdp come joint policies maximize total expected reward finite horizon note agents change roles local role policies 
agent role execution policy subsequent change contain actions pertaining new role 
theorem illustrates complexity finding optimal joint policies 
theorem decision problem determining exist policies rmtdp yield expected reward finite horizon nexp complete 
proof sketch proof follows reduction mtdp pynadath tambe rmtdp 
reduce mtdp rmtdp set rmtdp role actions null set rmtdp actions mtdp set actions reduce rmtdp mtdp generate new mtdp set actions equal 
finding required policy mtdp pynadath tambe theorem shows solving rmtdp optimal joint role role execution policies finite horizon highly intractable 
focus complexity just determining optimal role policy fixed role execution policy 
fixed role execution policy mean action selection agent predetermined role executing 
theorem decision problem determining exists role policy rmtdp yields expected reward fixed role execution policy finite horizon nexp complete 
proof sketch reduce mtdp rmtdp different role role execution action corresponding action mtdp 
rmtdp role action agent take role rj created action aj ai mtdp role rj contains 
explicit analysis communication please refer done pynadath tambe goldman 

single role execution action 
rmtdp construct transition function role action succeeds affected state feature roles 
role execution action transition probability mtdp action aj ai corresponding role action fixed role execution policy simply perform action corresponding successful role action decision problem rmtdp fixed role execution policy hard decision problem mtdp 
furthermore theorem conclude nexp completeness result suggests fixing role execution policy solving rmtdp optimal role policy intractable 
note theorem refers completely general globally optimal role policy number agents change roles point time 
result general globally optimal role policy doubly exponential complexity may left choice run brute force policy search enumerate role policies evaluate determine run time finding globally optimal policy 
number policies doubly exponential number observation histories number agents 
rmtdp enables quantitative evaluation team policies computing optimal policies intractable furthermore low level abstraction contrast top difficult human understand optimal policy 
contrast rmtdp top root hybrid model described section 

hybrid bdi pomdp approach having explained top rmtdp detailed view hybrid methodology quantitatively evaluate top 
provide detailed interpretation 
bdi team plans essentially top plans bdi interpreter top coordination layer 
shown rmtdp model constructed corresponding domain top interpreter converted corresponding incomplete rmtdp policy 
analyze top analysis techniques rely evaluating rmtdp policy rmtdp model domain 
hybrid approach combines strengths tops enabling humans specify tops coordinate large scale teams strengths rmtdp enabling quantitative evaluation different role allocations 
hand synergistic interaction enables improve performance top bdi teams 
hand identified specific ways tops easier build efficiently search rmtdp policies discussed section section 
particular ways 
tops exploited constructing rmtdp models domain section 
tops exploited incomplete policies restricting rmtdp policy search section 
top belief representation exploited enabling faster rmtdp policy evaluation section 
top organization hierarchy exploited hierarchically grouping rmtdp policies section 
top plan hierarchy exploited decomposing section 
top plan hierarchies exploited cutting observation belief histories section 
result efficient policy search completed rmtdp policy improves top performance 
exploit top framework frameworks tasking teams decker lesser stone veloso benefit similar synergistic interaction 
guidelines constructing rmtdp shown analysis approach uses input rmtdp model domain incomplete rmtdp policy 
fortunately top serve direct mapping rmtdp policy utilized constructing rmtdp model domain 
particular top determine domain features important model 
addition structure top exploited decomposing construction rmtdp 
elements rmtdp tuple rl defined procedure relies top underlying domain 
procedure automated key contribution recognizing exploitation top structures constructing rmtdp model 
order determine set states critical model variables tested pre conditions termination conditions context components sub plans top 
note state needs model features tested top top pre condition expresses complex test feature test modeled state gets defining incomplete policy input rmtdp 
define set roles rl leaf level roles organization hierarchy top 
furthermore specified section define state feature roles vector containing current role agent 
having defined rl roles define actions follows 
role rj rl define corresponding role action succeed fail depending agent performs action state action performed 
role execution actions agent role rj allowed role top 
defined rl top 
illustrate steps consider plans 
pre conditions leaf level plan see appendix instance tests start location helicopters start location termination conditions test scouts location locations helicopters modeled features set states rmtdp 
organization hierarchy define set roles rl role corresponding different kinds leaf level roles rl 
role role execution actions defined follows role action defined corresponding roles rl member scouting teams transport team 
domain specifies transport change scout role action fail agent current role agent scout 
role execution actions obtained top plans corresponding agent role 
mission rehearsal scenario agent fulfilling scout role members goes forward making current position safe reaches destination execution action consider move making safe 
agent transport role members transport team waits obtains observation signal scouting sub team reached role execution actions wait move forward 
define obtain set observations agent directly domain 
instance transport may observe status scout normal destroyed signal path safe 
determining functions requires combination human domain expertise empirical data domain behavior 
shown section approximate model transitional observational uncertainty sufficient deliver significant benefits 
defining reward transition function may require additional state variables modeled implicitly modeled top 
mission rehearsal domain time scouting transport mission completed determined amount reward 
time implicitly modeled top needed explicitly modeled rmtdp 
interested analyzing particular top respect uncertainty procedure constructing rmtdp model simplified exploiting hierarchical decomposition top order decompose construction rmtdp model 
high level components top represent plans executed different sub teams may loosely interact 
component sub team members may exhibit tight interaction focus loose coupling components results component feed components independently contribute team goal 
procedure constructing rmtdp exploits loose coupling components plan hierarchy order build rmtdp model represented combination smaller factors 
note decomposition infeasible approach applies benefits hierarchical decomposition unavailable 
classify sibling components parallel sequentially executed contains temporal constraint 
components executed parallel independent dependent 
independent components define components sub team executing component affect transitions observations reward obtained sub teams executing components 
procedure determining elements rmtdp tuple component sk ak pk ok rk identical procedure described earlier constructing rmtdp 
component smaller set relevant variables roles specifying elements corresponding rmtdp easier 
combine independent components obtain rmtdp corresponding higher level component 
higher level component child components independent set states sl fs fsl child true fsl sets features set states sl set states sk 
state sl sl said correspond state sk sk sl sk state sl value state sk features state sk 
transition function defined follows pl sl child true pk ak sk sl component corresponds states sk component ak joint action performed sub team assigned component corresponding joint action performed sub team assigned component observation function defined similarly ol sl child true ok sk ak 
reward function defined rl sl child true rk sk ak 
case sequentially executed components connected temporal constraint components loosely coupled states preceding component specify start states succeeding component 
component active time transition function defined follows pl sl pk ak sk component active child component sk represent states component corresponding states sl component ak joint action performed sub team assigned component corresponding joint action performed sub team corresponding component similarly define ol sl ok sk ak rl sl rk sk ak active child component 
consider example mission rehearsal domain components exhibit sequential dependence parallel independence 
concretely component doscouting executed followed parallel independent doscouting active active point execution 
transition observation reward functions parent execute mission corresponding functions doscouting combination corresponding functions 
top approach order determine construct factored rmtdp plan hierarchy 
shown algorithm replace particular sub plan constituent sub plans independent sequentially executed 
rmtdp defined particular sub plan 
process applied recursively starting root component plan hierarchy 
concrete example consider mission rehearsal simulation domain hierarchy illustrated 
temporal constraints doscouting doscouting exploited sequential decomposition parallel independent components 
replace doscouting 
apply process doscouting 
constituent components doscouting independent sequentially executed doscouting replaced constituent components 
rmtdp mission rehearsal domain comprised smaller doscouting 
algorithm build rmtdp top top sub plan subplan children subplan children subplan children returns sub plans subplan children null children loosely coupled independent rmtdp define rmtdp subplan automated return rmtdp child children factors child build rmtdp top child rmtdp factors return rmtdp top identify relevant variables building factored rmtdp utilizing structure top decompose construction procedure reduce load domain expert model construction 
furthermore shown section factored model greatly improves performance search best role allocation 
exploiting top beliefs evaluation rmtdp policies technique exploiting tops speeding evaluation rmtdp policies 
explain improvement describe original algorithm determining expected reward joint policy local policies agent indexed entire observation histories pynadath tambe nair pynadath yokoo tambe marsella 
obtain rmtdp policy top follows 
obtain action performed agent observation history action performed agent top set privately held beliefs corresponding observation history compute expected reward rmtdp policy projecting team execution possible branches different world states different observations 
time step compute expected value joint policy 
team starting state st set past observations 
follows 





expected reward joint policy null 
null start state 
time step computation performs summation possible world states agent observations time complexity 
computation repeated states observation histories length times 
time horizon complexity algorithm discussed section team oriented program agent action selection just currently held private beliefs note mutual beliefs modeled privately held beliefs agents footnote 
similar technique exploited mapping top rmtdp policy 
evaluation rmtdp policy corresponds top speeded agent local policy indexed private beliefs refer top congruent belief state agent rmtdp 
note belief state probability distribution world states single agent pomdp privately held beliefs bdi program agent time belief rmtdp policy evaluation leads speedup multiple observation histories map belief state speedup key illustration exploitation synergistic interactions top rmtdp 
instance belief representation techniques top reflected rmtdp resulting faster policy evaluation help optimize top performance 
detailed example belief state brief explanation belief rmtdp policies evaluated 
just evaluation observation histories compute expected reward belief policy projecting team execution possible branches different world states different observations 
time step compute expected value joint policy 
team starting state team belief state 




follows 

complexity computing function expression bf bf represents complexity belief update function 
time step computation value function done state possible reachable belief states 
represent maximum number possible belief states agent max point time number belief states agent complexity algorithm 
bf note algorithm exponent algorithm expression 
evaluation method give large time savings quantity 
ii belief update cost low 
practical bdi systems multiple observation histories map belief state usually 
furthermore belief update function mirrors practical bdi systems complexity low polynomial constant 
experimental results show significant speedups result switching top congruent belief states absolute worst case belief update function may simply append new observation history past observations top congruent beliefs equivalent keeping entire observation histories belief evaluation complexity observation history evaluation 
turn example belief policy evaluation mission rehearsal domain 
time step transport helicopters may receive observation scout failed observation function 
observation history representation policy transport agent maintain complete history observations receive time step 
example setting scout helicopters route route particular transport helicopter may different observation histories length 
time step transports may receive observation scout alive having failed 
time transport helicopter observation histories length sct alive sct alive sct sct sct alive sct sct sct sct alive sct action selection transport helicopters depends critical failure remaining scout crashed taken place change role 
failure critical determined passing observation belief update function 
exact order observations received precise times failure non failure observations received relevant determining critical failure taken place consequently transport change role scout 
observation histories map belief states 
example observation histories map belief doscouting critical failure taken place 
results significant speedups belief evaluation equation needs executed smaller number belief states linear domains opposed observation history evaluation equation executed exponential number observation histories 
actual speedup obtained mission rehearsal domain demonstrated empirically section 
optimizing role allocation section focused mapping domain interest rmtdp algorithms policy evaluation section focuses efficient techniques rmtdp policy search service improving bdi top team plans 
top essence provides incomplete fixed policy policy search optimizes decisions left open incomplete policy policy completed optimizes original top see 
enabling rmtdp focus search incomplete policies providing decompositions tops assist quickly searching policy space illustrated section 
focus particular problem role allocation grosz modi shen tambe yokoo tidhar wooldridge critical problem teams 
top provides incomplete policy keeping open role allocation decision agent rmtdp policy search provides optimal role action role allocation decision points 
contrast previous role allocation approaches approach determines best role allocation consideration uncertainty domain costs 
demonstrated solving role allocation problem methodology general apply coordination decisions 
hierarchical grouping rmtdp policies mentioned earlier address role allocation top provides policy complete role allocation decisions 
rmtdp policy search optimally fills role allocation decisions 
understand rmtdp policy search useful gain understanding role allocation search space 
note role allocation focuses deciding types agents allocate different roles organization hierarchy 
role allocation decision may time may time conditioned available observations 
shows partially expanded role allocation space defined top organization hierarchy helicopters 
node role allocation space completely specifies allocation agents roles corresponding level organization hierarchy ignore number right node 
instance root node role allocation space specifies helicopters assigned task force level organization hierarchy leftmost leaf node level specifies helicopter assigned zero zero helicopters transport team 
see leaf node role allocation space complete valid role allocation agents roles organization hierarchy 
order determine leaf node role allocation superior evaluate rmtdp constructing rmtdp policy 
particular example role allocation specified leaf node corresponds role actions agent execute time 
example case leftmost leaf time agent recall section homogeneous team specific agent matter member agents members transport team 
agent role policy include null agents include null joint 
case assume rest role policy roles reallocated scout fails obtained role reallocation algorithm bdi top interpreter steam algorithm tambe 
example role reallocation performed steam algorithm steam reallocation policy included incomplete policy rmtdp initially provided 
best role allocation computed keeping mind steam reallocation policy 
steam failure agent playing agent playing replace criticality criticality criticality critical agents observations critical failure taken place replacing agent decision replace computed expression included incomplete policy input rmtdp 
incomplete policy completed role allocation leaf node technique able construct policy rmtdp corresponds role allocation 
domains robocuprescue allocation decisions time 
domains possible role allocation conditioned observations communication obtained course execution 
instance shown robocuprescue scenario ambulances allocated sub team information location civilians conveyed fire engines 
allocation ambulances conditioned communication number civilians location 
shows partially expanded role allocation scaled rescue scenario civilians ambulances fire engines station station 
depicts fact ambulances fire engine station 
shown level allocation fire engines gives number engines partially expanded role allocation space mission rehearsal domain 
assigned station 
level leaf level different leaf nodes possible assignment ambulances depending value communication 
civilians exclude case civilians particular fire possible messages civilian fire civilians fire 
partially expanded role allocation space rescue domain fire engine station fire engine station ambulances civilians 
able exploit top organization hierarchy create hierarchical grouping rmtdp policies 
particular leaf node represents complete rmtdp policy role allocation specified leaf node parent node represents group policies 
evaluating policy specified leaf node equivalent evaluating specific role allocation uncertainties account 
brute force search role allocations evaluating order determine best role allocation 
number possible role allocations exponential leaf roles organization hierarchy 
prune search space 
pruning role allocation space prune space valid role allocations upper bounds parents leaves role allocation space admissible heuristics section 
leaf role allocation space represents completely specified policy upper bound maximum value policies parent node evaluated rmtdp 
obtain parent nodes shown brackets right parent node branchand bound style pruning see algorithm 
discuss algorithm note essence performs branch bound style pruning key novelty step discuss section 
branch bound algorithm works follows sort parent nodes estimates start evaluating children parent highest algorithm steps 
evaluate rmtdp child refers evaluation leaf level policy child rmtdp model 
evaluation leaf level policies step done methods described section 
case role allocation space start evaluating leaves parent node helicopter scouting team transport team 
value evaluating leaf node shown right leaf node 
obtained value best leaf node algorithm steps case compare parents role allocation space algorithm steps 
see result pruning parent nodes leftmost parent right parents avoid evaluation leaf level policies 
proceed evaluate leaf nodes parent scouting team transport team 
result pruning remaining unexpanded parent nodes return leaf highest value case node corresponding allocated transport team 
demonstrated level hierarchy methodology applying deeper hierarchies straightforward 
algorithm branch bound algorithm policy search 
parents list parent nodes compute maxexp parents algorithm sort parents decreasing order maxexp parent parents done parent false pruned parent false parent parents done parent false pruned parent false child parent child leaf level policy parent child null done parent true evaluate rmtdp child best child parent parents maxexp parent pruned parent true return best exploiting top calculate upper bounds parents discuss upper bounds parents called calculated parent 
parent defined strict upper bound maximum expected reward leaf nodes 
necessary upper bound pruning potentially useful role allocations 
order calculate parent evaluate leaf nodes rmtdp benefit subsequent pruning 
turn top plan hierarchy see break evaluation parent node components evaluated separately decomposing problem 
words approach exploits structure bdi program construct small scale decomposition techniques just assume decomposition ultimately rely domain experts identify interactions agents reward transition functions dean lin guestrin venkataraman koller 
parent role allocation space small scale evaluate values top component 
fortunately discussed section exploited small scale corresponding top components constructing larger scale 
put small scale evaluating policies component obtain upper bounds 
note just evaluation leaf level policies evaluation components parent node done observation histories see equation belief states see equation 
describe section observation history evaluation method computing values components parent summed obtain upper bound children values 
parent role allocation space represents group policies top components sub plans allow component wise evaluation group obtain upper bound expected reward policy group 
algorithm exploits smaller scale rmtdp components discussed section obtain upper bounds parents 
order evaluate parent node role allocation space identify start states component evaluate 
explain step parent node scouting team transport team see 
component preceding components start states corresponds start states policy top mapped 
components component linked sequential dependence start states states preceding component 
explained section significantly reduce list start states component evaluated 
similarly starting observation histories component observation histories completing preceding component observation history component 
bdi plans normally refer entire observation histories rely key beliefs typically referred preconditions component 
starting observation history shortened include relevant observations obtaining reduced list starting observation sequences 
divergence private observations problematic cause agents trigger different team plans 
indicated earlier section top interpreters guarantee coherence key aspects observation histories 
instance discussed earlier top interpreter ensures coherence key beliefs initiating terminating team plans top avoiding divergence observation histories 
order compute maximum value particular component evaluate possible leaf level policies component possible start states observation histories obtain maximum algorithm steps 
evaluation store states observation histories evaluation subsequent components 
shown algorithm maxexp method calculating upper bounds parents role allocation space 
parent search space maxexp parent component corresponding factors rmtdp section component preceding component obtain start states states states states discard features si obtain corresponding observation histories start obtain start states states observation histories start null leaf level policies parent max max si states evaluate rmt dpi si maxexp parent evaluation doscouting component parent node helicopters assigned scouting team transport team leaf level policies correspond possible ways helicopters assigned teams transport team helo helo transport team transport team role allocation tells agents role take step 
remainder role policy specified role replacement policy top infrastructure role execution policy specified doscouting component top 
obtain parent node role allocation space simply sum maximum values obtained component algorithm steps maximum values component see right component summed obtain 
seen third node left upper bound 
calculation parent nodes faster evaluating leaf nodes cases reasons 
firstly parent nodes evaluated component wise 
multiple leaf level policies component result state remove duplicates get start states component 
component contains state features relevant number duplicates greatly increased 
duplication evaluation effort avoided leaf nodes policy evaluated independently start finish 
instance doscouting component role allocation role allocation states common eliminating irrelevant features scout allocation scout allocation fail 
feature elimination algorithm steps state features retained component route number transports transports may replaced failed scouts shown 
second reason computation parents faster number starting observation sequences number observation histories preceding components 
observations observation histories component relevant succeeding components algorithm steps 
function reduces number starting observation histories observation histories preceding component 
refer methodology obtaining parent maxexp 
variation maximum expected reward failures obtained similar fashion assume probability agent failing 
able assumption evaluating parent node focus obtaining upper bounds parents obtaining exact value 
result branching evaluation component proceed quicker 
heuristic works evaluation policy failures occurring higher evaluation policy failures possible 
normally case domains 
evaluation heuristics role allocation space helicopters shown square brackets 
doscouting alloc alloc transports transports component wise decomposition parent exploiting top 
transports theorem shows maxexp method finding upper bounds finds upper bound yields admissible search heuristic branch bound search role allocation space 
theorem maxexp method yield upper bound 
proof see appendix theorem conclude branch bound policy search algorithm find best role allocation parents true upper bounds 
help theorem show worst case branch bound policy search complexity doing brute force search 
theorem worst case complexity evaluating single parent node maxexp evaluating leaf node constant factor 
proof sketch worst case complexity maxexp arises 
states component executing policy removing features irrelevant succeeding component similarly states component executing policy removing features irrelevant succeeding component null duplication states occur 

ohj observation histories component executing policy removing observations irrelevant succeeding component similarly ohj observation histories component executing policy removing observation histories irrelevant succeeding component ohj ohj null duplication observation histories occur 
note belief evaluation replace observation histories top congruent belief states see sect 
case computational advantage evaluating component separately 
equivalent evaluating child node parent 
worst case maxexp computation parent evaluating children constant factor 
addition worst case pruning result maxexp leaf node need evaluated 
equivalent evaluating leaf node twice 
worst case complexity doing branch bound search maxexp finding best role allocation evaluating leaf node 
refer brute force approach 
worst case complexity maxexp 
owing pruning savings decomposition computation significant savings average case 
section highlights savings mission rehearsal robocuprescue domains 

experimental results section presents sets results context domains introduced section viz 
mission rehearsal robocuprescue kitano 
investigated empirically speedups result top congruent belief states belief evaluation observation evaluation algorithm section brute force search 
focus determining best assignment agents roles assume fixed top top infrastructure 
second conducted experiments investigate benefits considering uncertainty determining role allocations 
compared allocations rmtdp role allocation algorithm allocations consider kind uncertainty ii allocations consider observational uncertainty consider action uncertainty 
third conducted experiments domains determine sensitivity results changes model 
fourth compare performance allocations rmtdp role allocation algorithm allocations human subjects complex domains robocuprescue simulations 
results mission rehearsal domain mission rehearsal domain top discussed section 
seen organization hierarchy requires determining number agents allocated scouting sub teams remaining allocated transport sub team 
different numbers initial helicopters attempted varying 
details rmtdp constructed domain appendix probability failure scout time step routes respectively 
probability transport observing alive scout routes respectively 
false positives possible transport observe scout alive failed 
probability transport observing scout failure routes respectively 
false positives possible transport observe failure taken place 
shows results comparing different methods searching role allocation space 
show methods 
method adds new speedup techniques previous 
obs brute force evaluation role allocation determine best 
agent maintains complete observation history evaluation algorithm equation 
agents rmtdp projected order reachable states order observation histories role allocation evaluated largest experiment category limited agents 

bel brute force evaluation role allocation 
difference method obs belief evaluation algorithm see equation 

maxexp branch bound search algorithm described section uses upper bounds evaluation parent nodes find best allocation 
evaluation parent leaf nodes uses belief evaluation 

modification branch bound heuristic mentioned section 
essence maxexp upper bounds computed making assumption agents fail 
heuristic correct domains total expected reward failures failures give significant speedups agent failures primary sources stochasticity 
method evaluation parent leaf nodes uses belief evaluation 
note upper bounds computed failure assumption changes assumed actual domains 
axis number nodes role allocation space evaluated includes leaf nodes parent nodes axis represents runtime seconds logarithmic scale 
figures vary number agents axis 
experimental results previous distributed pomdps restricted just agents exploiting hybrid models able vary number agents shown 
clearly seen pruning significant reductions obtained maxexp bel terms numbers nodes evaluated 
reduction grows quadratically fold agents 
obs identical bel terms number nodes evaluated methods leaf level policies evaluated method evaluation differs 
important note maxexp result number nodes evaluated domains necessarily true 
general evaluate nodes maxexp estimate high maxexp estimate 
upper bounds computed quicker 
shows bel method provides significant speedup obs actual run time 
instance fold speedup bel obs agent case obs executed day problem settings greater agents 
empirically demonstrates computational savings possible belief evaluation observation history evaluation see section 
reason belief evaluation maxexp approaches 
number nodes agents obtained experiments rest calculated formula 
represents number heterogeneous role types number homogeneous agents 

referred rising factorial 
remaining experiments 
maxexp heuristic results fold speedup bel agent case 
heuristic quick compute upper bounds far outperforms maxexp heuristic fold speedup maxexp agents 
speedups maxexp continually increase increasing number agents 
speedup method maxexp marked domain ignoring failures results branching 
number nodes number agents maxexp obs bel time secs log scale number agents maxexp bel obs performance role allocation space search mission rehearsal domain left number nodes evaluated right run time seconds log scale 
conducted experiments illustrating importance rmtdp reasoning action observation uncertainties role allocations 
compared allocations rmtdp role allocation algorithm allocations different methods see 
role allocation constraint optimization cop modi lesser allocation approach cop approach leaf level sub teams organization hierarchy treated variables number helicopters domain variable domain may helicopters 
reward allocating agents sub teams expressed terms constraints allocating helicopter scout route assigned reward corresponding route distance ignoring possibility failure ignoring transition probability 
allocating helicopters subteam obtained proportionally higher reward 
allocating helicopter transport role assigned large reward transporting cargo destination 
allocating helicopters subteam obtained proportionally higher reward 
allocating scout role assigned reward negative infinity exceeding total number agents assigned reward negative infinity 
rmtdp complete observability approach consider transition probability ignore partial observability achieved assuming complete observability rmtdp 
mtdp 
modi focused decentralized cop investigation emphasis resulting role allocation generated cop decentralization se 
complete observability equivalent markov decision problem mdp pynadath tambe actions joint actions 
refer allocation method mdp method 
shows comparison rmtdp allocation mdp allocation cop allocation increasing number helicopters axis 
compare expected number transports get destination axis metric comparison primary objective domain 
seen considering forms uncertainty rmtdp performs better just considering transition uncertainty mdp turn performs better considering uncertainty cop 
shows actual allocations methods helicopters helicopters 
case helicopters bars rmtdp mdp identical helicopters scouting route helicopters transport role 
cop allocation consists scout route transports 
allocation proves myopic results fewer transports getting destination safely 
case helicopters cop chooses just scout helicopter route shortest route 
mdp approach results scouts route longest route albeit safest 
rmtdp approach considers observational uncertainty chooses additional scout route order take care cases failures scouts go undetected transports 
number transports number agents rmtdp cop mdp number rmtdp cop mdp rmtdp cop mdp rt rt rt transports comparison performance different allocation methods allocations different allocation methods 
noted performance rmtdp allocation depend values elements rmtdp model 
experiment revealed getting values exactly correct necessary 
order test sensitivity performance allocations actual model values introduced error various parameters model see allocations incorrect model perform original model errors 
emulates situation model correctly represent domain 
shows expected number transports reach destination axis mission rehearsal scenario helicopters error axis introduced various parameters model 
instance percentage error failure rate route route failure rate erroneous failure rate actual failure rate difference number transports reached destination 
percentage error greater allocation conservative resulting fewer transports getting destination 
similarly percentage error allocation risky scouts assigned resulting failures 
general shows model insensitive errors model parameters mission rehearsal domain model parameters outside range non optimal allocations result 
comparing non optimal allocations cop find perform better cop range errors tested failure rate observability routes 
instance error failure rate route rmtdp managed transports safely reach destination cop managed get transports reach safely 
comparing non optimal allocations mdp find performed better mdp range error observability routes 
allocations incorrect model non optimal performed better cop mdp large ranges errors model 
shows getting model exactly correct necessary find allocations 
able obtain benefits rmtdp insisting accurate model 
results robocuprescue domain number transports percentage error route failure rate route failure rate route failure rate route observability model sensitivity mission rehearsal domain 
speedups robocuprescue domain set experiments highlight computational savings obtained robocuprescue domain 
scenario experiment consisted fires different locations city 
fires different initially unknown number civilians total number civilians distribution locations civilians chosen known ahead time 
experiment fix number civilians set distribution choose civilians locations uniform 
number fire engines set located different fire stations described section vary number ambulances located ambulance center 
reason chose change number ambulances small number fire engines unable extinguish fires changing problem completely 
goal determine fire engines allocate fire information civilians transmitted ambulances send fire location 
highlights savings terms number nodes evaluated actual runtime increase number agents 
show results bel maxexp 
obs run slowness 
heuristic identical maxexp agents fail scenario 
rmtdp case reachable states 
figures increase number ambulances axis 
show number nodes evaluated parent nodes leaf nodes logarithmic scale 
seen maxexp method results fold decrease number nodes evaluated compared bel ambulances decrease pronounced number ambulances increased 
shows time seconds logarithmic scale axis compares run times maxexp bel methods finding best role allocation 
bel method find best allocation day number ambulances increased 
ambulances fire engines maxexp resulted fold speedup bel 
number nodes log scale number ambulances maxexp run time secs log scale number ambulances maxexp performance role allocation space search robocuprescue left number nodes evaluated log scale right run time seconds log scale 
allocation robocuprescue set experiments shows practical utility role allocation analysis complex domains 
able show significant performance improvements actual robocuprescue domain role allocations generated analysis 
construct rmtdp rescue scenario described section guidance top underlying domain described section 
maxexp heuristic determine best role allocation 
compared rmtdp allocation allocations chosen human subjects 
goal comparing rmtdp allocations human subjects mainly show rmtdp capable performing near human expert levels domain 
addition order determine reasoning uncertainty impacts allocations compared rmtdp allocations allocations determined additional allocation methods 
number nodes evaluated bel computed number fire engines station respectively number ambulances number civilians 
node provides complete conditional role allocation assuming different numbers civilians fire station 

allocations robocuprescue agents entered robocuprescue competitions nair finished third place 
agents local reasoning decision making ignoring transitional observational uncertainty 

rmtdp complete observability discussed earlier complete observability rmtdp leads mdp refer method mdp method 
note comparisons performed robocuprescue simulator multiple runs deal stochasticity scenario described section 
fix number fire engines ambulances civilians 
experiment consider settings location civilians drawn uniform distribution cases civilians fire civilian fire civilians fire fire civilians fire fire remaining civilian fire civilians fire 
speedup results section obtained distribution 
skewed distribution cases civilians fire civilian fire remaining civilian fire civilians fire 
note consider case civilians located fire optimal ambulance allocation simply assign ambulances fire civilians located 
skewed distribution chosen highlight cases difficult humans reason allocation choose 
human subjects experiment researchers usc 
familiar robocuprescue 
time study setup time limit provide allocations 
subject told allocations going judged basis number civilian lives lost damage sustained due fire 
exactly criteria robocuprescue kitano 
compared rmtdp allocation human subjects robocuprescue simulator mdp 
compared performance allocations basis number civilians died average damage buildings lower values better criteria 
criteria main criteria robocuprescue kitano 
values shown obtained averaging simulator runs uniform distribution runs skewed distribution allocation 
average values plotted account stochasticity domain 
error bars provided show standard error allocation method 
seen rmtdp allocation better allocations terms lower number civilians dead human quite close 
example averaging runs rmtdp allocation resulted civilian deaths human allocation resulted civilian deaths 
terms average building damage allocations humans performing marginally better 
skewed distribution difference allocations perceptible see 
particular notice rmtdp 
mission rehearsal domain run actual mission rehearsal simulator simulator public domain longer accessible difference tested role allocations mission rehearsal robocuprescue domains 
allocation better humans terms number civilians dead 
human particularly badly bad allocation fire engines 
resulted damage buildings consequently number civilians dead 
comparing rmtdp mdp approach showed reasoning transitional uncertainty mdp better static reactive allocation method reasoning transitional observational uncertainty 
uniform distribution case rmtdp better mdp mdp method performing better 
skewed distribution case improvement allocations rmtdp greater 
averaging simulation runs rmtdp allocations resulted civilians deaths mdp resulted 
allocation method resulted fires allocated fire engines 
allocations determined mdp approach turned human 
tailed test performed order test statistical significance means allocations 
means number civilians dead rmtdp allocation human allocations statistically different confidence uniform skewed distributions 
difference fire damage statistically significant uniform case difference rmtdp allocation human fire damage statistically significant skewed case 
rmtdp human human human mdp civilians casualties building damage rmtdp human human human mdp civilians casualties building damage comparison performance robocuprescue left uniform right skewed 
considering just average performance different allocations highlight individual cases marked differences seen performance 
comparison particular settings allocation methods showed bigger difference rmtdp terms allocations 
standard error shown error bars allocation 
figures compare allocations uniform civilian distributions setting civilian fire civilians fire civilian setting civilians fire fire civilian setting respectively 
seen rmtdp allocation results fewer civilian casualties slightly damage buildings due fire difference fire damage statistically significant damage values close 
figures compare allocations skewed civilian distribution 
key difference arises human 
seen human results damage due fire 
human allocated fire engines buildings turn resulted building burnt completely 
consequently civilians located fire location ambulances 
see specific instances allocation done rmtdp allocation algorithm superior allocations human comes 
rmtdp rmtdp human human human human human human mdp mdp civilians casualties building damage civilians casualties building damage rmtdp rmtdp human human human human human human mdp mdp civilians casualties building damage civilians casualties building damage comparison performance robocuprescue particular settings top left uniform civilian setting top right uniform civilian setting bottom left skewed civilian setting bottom right skewed civilian setting 
table shows allocations fire agents assigned fire allocated fire rmtdp role allocation algorithm human subjects skewed civilian setting consider case shows difference 
particular table highlights differences various allocators skewed civilian setting helps account differences seen performance actual simulator 
seen main difference performance terms number civilians saved 
recall scenario civilians fire fire 
human subjects mdp chose send ambulance fire number ambulances allocated fire number ambulances allocated fire 
lone ambulance unable rescue civilian fire resulting humans mdp saving fewer civilians 
chose send ambulances fire greedy selection method proximity civilians resulting civilians fire dying terms fire engine allocation human sent fire engines fire civilians located number engines allocated fire number engines allocated fire 
unfortunately 
strategy ambulances going closest civilian worked fairly ambulances usually spread lone fire engine fire able extinguish fire causing fire spread parts city 
distribution rmtdp human human human mdp engines station skewed engines station engines station ambulances table allocations ambulances fire engines fire 
experiments show allocations rmtdp role allocation algorithm performs significantly better allocations chosen human subjects mdp cases significantly worse case 
particular distribution civilians uniform difficult humans come allocation difference human allocations rmtdp allocation significant 
conclude rmtdp allocation performs near human expertise 
experiment done robocuprescue simulator introduced error rmtdp model order determine sensitive model errors parameters model 
compares allocations ambulances fire engines civilians terms number civilian casualties axis error axis introduced probability fire spread probability civilian health deterioration 
seen increasing error probability fire spread higher results allocations save fewer civilians fire choose concentrate effort fires 
resulting allocation value terms number civilians casualties consider uncertainty 
reducing error probability fire impact allocations 
increasing error probability civilian health deterioration higher caused civilians sacrificed 
allocation value terms number civilians casualties 
decreasing error probability civilian health deterioration lower negative caused number ambulances allocated fire number civilians fire human 
civilian casualties percentage error fire rate civilian health model sensitivity robocuprescue scenario 

related related areas research wish highlight 
considerable amount done field multiagent teamwork section 
second related area research decision theoretic models particular distributed pomdps section 
third area related describe section hybrid systems markov decision process bdi approaches 
section related role allocation reallocation multiagent teams described 
bdi teamwork formal teamwork theories joint intentions cohen levesque sharedplans grosz kraus proposed tried capture essence multiagent teamwork logic beliefs desires intentions 
practical models teamwork collagen rich sidner grate jennings steam tambe built teamwork theories cohen levesque grosz kraus attempted capture aspects teamwork reusable domains 
addition complement practical teamwork models team oriented programming approach pynadath tambe tidhar introduced allow large number agents programmed teams 
approach expanded applied variety domains pynadath tambe yen da silva demazeau 
approaches building practical multiagent systems stone veloso decker lesser explicitly team oriented programming considered family 
research reported article complements research teamwork introducing hybrid bdi pomdp models exploit synergy bdi pomdp approaches 
particular top teamwork models traditionally addressed uncertainty cost 
hybrid model provides capability illustrated benefits reasoning detailed experiments 
article uses team oriented programming tambe da silva demazeau tidhar example bdi approach relevant similar techniques modeling tasking collectives agents decker lesser approach 
particular language provides abstraction tasking collaborative groups agents similar top gpgp infrastructure executing tasks analogous top interpreter infrastructure shown 
lesser explored distributed mdps analyses gpgp coordination xuan lesser exploited structures decomposition abstraction searching optimal policies distributed mdps suggested article 
article complements lesser illustrating significant avenue efficiency improvements analyses 
distributed pomdp models distributed pomdp models represent collection formal models expressive capture uncertainty domain costs rewards associated states actions 
group agents problem deriving separate policies maximize joint reward modeled distributed pomdp models 
particular dec pomdp decentralized pomdp bernstein mtdp multiagent team decision problem pynadath tambe generalizations pomdps case multiple distributed agents basing actions separate observations 
frameworks allow formulate constitutes optimal policy multiagent team principle derive policy 
exceptions effective algorithms deriving policies distributed pomdps developed 
significant progress achieved efficient single agent pomdp policy generation algorithms monahan cassandra littman zhang kaelbling 
research directly carried distributed case 
finding optimal policies distributed pomdps nexp complete bernstein 
contrast finding optimal policy single agent pomdp pspace complete papadimitriou tsitsiklis 
bernstein note bernstein suggests fundamental difference nature problems 
distributed problem treated separate pomdps individual policies generated individual agents possible cross agent interactions reward transition observation functions 
action agent may different rewards possible actions agents may take 
approaches solve distributed pomdps 
approach typically taken simplifying assumptions domain 
instance guestrin 
assumed agent completely observe world state 
addition assumed reward function transition function team expressed sum product reward transition functions agents team 
becker 
assume domain factored agent completely observable local state domain transition independent agent affect agent local state 
second approach taken simplify nature policies considered agents 
example chad 
restrict agent policies memoryless reactive policies simplifying problem solving multiple mdps 
peshkin 
take different approach gradient descent search find local optimum finite controllers bounded memory 
nair 
algorithm finding locally optimal policy space unrestricted finite horizon policies 
third approach taken hansen 
involves trying determine globally optimal solution making simplifying assumptions domain 
approach attempt prune space possible complete policies eliminating dominated policies 
brave frontal assault problem method expected face significant difficulties scaling due fundamental complexity obtaining globally optimal solution 
key difference research focused hybrid systems leverage advantages bdi team plans practical systems distributed pomdps quantitatively reason uncertainty cost 
particular tops specify large scale team plans complex domains finding best role allocation teams 
hybrid bdi pomdp approaches pomdp models context analysis single agent wooldridge parsons multiagent pynadath tambe xuan behavior 
compare various strategies intention reconsideration deciding deliberate intentions modeling bdi system pomdp 
key differences approach apply analysis single agent case consider issues exploiting bdi system structure improving pomdp efficiency 
xuan lesser pynadath tambe analyze multiagent communication 
xuan lesser dealt finding evaluating various communication policies pynadath tambe com mtdp model deal problem comparing various communication strategies empirically analytically 
approach general explain approach analyzing coordination actions including communication 
concretely demonstrate approach analysis role allocation 
additional key differences earlier pynadath tambe follows rmtdp illustrate techniques exploit team plan decomposition speeding policy search absent com mtdp ii introduce techniques belief evaluation absent previous 
combining rmtdp com mtdp interesting avenue research preliminary steps direction nair tambe marsella 
hybrid systems focused analysis scerri 
employ markov decision processes team oriented programs adjustable autonomy 
key difference mdps execute particular sub plan top plan hierarchy making improvements top 
dtgolog boutilier reiter soutchanski thrun provides order language limits mdp policy search logical constraints actions 
shares key idea synergistic interactions mdps golog differs focuses single agent mdps fully observable domains exploit plan structure improving mdp performance 
isaac nair tambe marsella system analyzing multiagent teams employs decision theoretic methods analyzing multiagent teams 
probabilistic finite automaton pfa represents probability distribution key patterns team behavior learned logs team behaviors 
key difference analysis performed having access actual team plans agents executing advice provided directly applied improving team need human developer change team behavior advice generated 
role allocation reallocation different approaches problem role allocation reallocation 
example tidhar 
tambe 
performed role allocation matching capabilities grosz proposed combinatorial auctions decide roles assigned 
modi 
showed role allocation modeled distributed constraint optimization problem applied problem tracking multiple moving targets distributed sensors 
shehory kraus suggested coalition formation algorithms deciding quickly agent took role 
wooldridge auctions decide task allocation 
important note competing techniques free problem model problem model transition probabilities 
approaches team reconfiguration methods due dunin keplicz self adapting organizations lesser dynamic re organizing groups barber martin 
scerri 
role re allocation algorithm allows autonomy role reallocation shift human supervisor agents 
key difference prior stochastic models evaluate allocations enables compute benefits role allocation account uncertainty costs reallocation failure 
example mission rehearsal domain uncertainties considered just scout allocated leading costly mission failure 
lookahead depending probability failure multiple scouts sent routes resulting fewer higher expected reward 

bdi approach agent teamwork provided successful applications tools techniques provide quantitative analyses team coordination team behaviors uncertainty lacking 
emerging field distributed pomdps provides decision theoretic method quantitatively obtaining optimal policy team agents faces serious intractability challenge 
article leverages benefits bdi pomdp approaches analyze improve key coordination decisions bdi team plans pomdp methods 
order demonstrate analysis methods concentrated role allocation fundamental aspect agent teamwork provided key contributions 
introduced rmtdp distributed pomdp framework analysis role allocation 
second article rmtdp methodology optimizing key coordination decisions bdi team plan domain 
concretely article described methodology finding best role allocation fixed team plan 
combinatorially role allocations introduced methods exploit task decompositions sub teams significantly prune search space role allocations 
third hybrid bdi pomdp approach uncovered synergistic interactions bdi team plans distributed pomdps 
tops useful constructing rmtdp model domain identifying features need modeled decomposing model construction structure top 
rmtdp model evaluate top 

tops restricted policy search providing incomplete policies limited number open decisions 

bdi approach helped coming novel efficient belief representation policies suited hybrid bdi pomdp approach corresponding algorithm evaluating policies 
resulted faster evaluation compact policy representation 

structure top exploited decompose problem evaluating policies resulting significant pruning search optimal role allocations 
constructed domains robocuprescue mission rehearsal simulation determined best role allocation domains 
furthermore illustrated significant speedups rmtdp policy search due techniques introduced article 
detailed experiments revealed advantages approach state art role allocation approaches failed reason uncertainty 
key agenda continue scale larger scale agent teams 
scale require efficiency improvements 
propose continue exploit interaction bdi pomdp approaches achieving scale 
instance disaster rescue distributed sensor nets large area monitoring applications benefit scale 
acknowledgments research supported nsf 
jim blythe anthony cassandra jung sven koenig michael littman marsella david pynadath paul scerri discussions related article 
reviewers article comments helped significantly improving article 
appendix top details section describe top helicopter scenario 
details subplan shown context pre conditions mb location start achieved mb achieved doscouting achieved time mb achieved helo alive helo location helo unachievable mb unachievable doscouting mb unachievable achieved helo alive helo location helo irrelevant body doscouting constraints doscouting doscouting doscouting context pre conditions achieved unachievable irrelevant body constraints context doscouting pre conditions achieved unachievable mb helo alive helo irrelevant body op context doscouting achieved unachievable irrelevant mb helo alive helo body constraints context pre conditions achieved mb helo location helo unachievable time mb helo alive helo irrelevant body location start route location move forward context pre conditions achieved mb helo location helo unachievable time mb helo alive helo irrelevant body location start route location move forward context pre conditions achieved mb helo location helo unachievable time mb helo alive helo irrelevant body location start route location move forward context pre conditions achieved mb location unachievable time mb helo alive helo irrelevant body location start mb achieved route elseif mb achieved route elseif mb achieved route route null location move forward context pre conditions achieved mb location unachievable time mb helo alive helo location helo irrelevant body location move forward predicate achieved true achieved conditions true 
similarly predicates unachievable irrelevant true unachievable conditions irrelevant conditions true respectively 
predicate location team true members team 
shows coordination relationships relationship indicated solid arc relationship indicated dotted arc coordination relationships indicate irrelevance conditions enforced top infrastructure 
relationship team sub plans means team sub plans fail parent team plan fail 
parent team plan achieved child sub plans achieved 
doscouting done achieved mb achieved achieved unachievable mb unachievable unachievable relationship means subplans fail parent fail success subplans means parent plan succeeded 
need performed achieved mb achieved achieved achieved unachievable mb unachievable unachievable unachievable relationship affects irrelevance conditions subplans joins 
parent unachievable subplans executing irrelevant 
irrelevant mb unachievable similarly irrelevant mb unachievable assign roles plans shows assignment brackets adjacent plans 
instance task force team assigned jointly perform execute mission 
appendix rmtdp details section details rmtdp constructed top 
get features state attributes tested preconditions achieved unachievable irrelevant conditions body team plans individual agent plans 
relevant state variables location helicopter role helicopter route helicopter status helicopter alive time 
team helicopters state tuple time role 
loc 
route 
status 

consider actions primitive actions agent perform individual plans 
top infrastructure enforces mutual belief communication actions 
analyzing cost focus research consider communication implicit model effect communication directly observation function 
consider kinds actions role role execution actions 
assume initial allocation specify roles agents 
specifies agent scout transport scout scout team assigned 
scout transport change team initial allocation transport change role role actions role role execution actions agent obtain transition function help human expert simulations simulator available 
domain helicopters crash shot start location 
probability scouts get shot depends route probability crash route probability crash route probability crash route scouts spot 
assume probability transport shot location location 
probability multiple crashes obtained multiplying probabilities individual crashes 
action effect null loci dead 
cases location agent gets incremented 
assume role actions succeed role performing agent transport assigned route 
transport start observe status agents probability depending positions 
helicopter particular route observe helicopters route completely observe helicopters routes 
observation function gives probability group agents receive particular joint observation 
domain assume observations agent independent observations agents current state previous joint action 
probability joint observation computed multiplying probabilities individual agent observations 
probability transport start observing status alive scout route 
probability transport start observing alive scout don false negatives 
similarly scout route crashes probability visible transport start probability transport doesn see failure 
similarly probabilities observing alive scout route route respectively probabilities observing crash route route respectively 
reward function obtained help human expert helps assign value various states cost performing various actions 
analysis assume actions cost 
consider negative reward cost replacement action negative reward failure helicopter rf reward scout reaching reward transport reaching 
rf 
rl roles individual agents take top organization hierarchy 
rl transport 
appendix theorems theorem maxexp method yield upper bound 
proof sketch policy leaf level policy highest expected reward particular parent node restricted policy space 
max children reward function specified separately component separate expected reward rewards constituent components starting states starting observation histories components 
team plan divided components components parallel independent sequentially executed 
vj expected value obtained component greater highest value obtained policy 
max states vj max children max states vj max children vj barber martin 

dynamic reorganization decision making groups 
proceedings fifth international conference autonomous agents agents pp 

becker zilberstein lesser goldman 

transition independent decentralized markov decision processes 
proceedings second international joint conference autonomous agents multi agent systems aamas pp 

bernstein zilberstein immerman 

complexity decentralized control mdps 
proceedings sixteenth conference uncertainty artificial intelligence uai pp 

boutilier 

planning learning coordination multiagent decision processes 
proceedings sixth conference theoretical aspects rationality knowledge tark pp 

boutilier reiter soutchanski thrun 

decision theoretic high level agent programming situation calculus 
proceedings seventeenth national conference artificial intelligence aaai pp 

cassandra littman zhang 

incremental pruning simple fast exact method partially observable markov decision processes 
proceedings thirteenth annual conference uncertainty artificial intelligence uai pp 

chad scherrer charpillet 

heuristic approach solving decentralized pomdp assessment pursuit problem 
proceedings acm symposium applied computing sac pp 

cohen levesque 

teamwork 
nous 
da silva demazeau 

vowels ordination model 
proceedings international joint conference autonomous agents multiagent systems aamas pp 

dean lin 

decomposition techniques planning stochastic domains 
proceedings fourteenth international joint conference artificial intelligence ijcai pp 

decker lesser 

quantitative modeling complex computational task environments 
proceedings eleventh national conference artificial intelligence aaai pp 

dix avila nau zhang 

impacting shop putting ai planner multi agent environment 
annals mathematics artificial intelligence 
dunin keplicz 

reconfiguration algorithm distributed problem solving 
engineering simulation 
erol hendler nau 

htn planning complexity expressivity 
proceedings twelfth national conference artificial intelligence aaai pp 

wooldridge 

adaptive task resource allocation multi agent systems 
proceedings fifth international conference autonomous agents agents pp 

georgeff lansky 

procedural knowledge 
proceedings ieee special issue knowledge representation 
goldman zilberstein 

optimizing information exchange cooperative multi agent systems 
proceedings second international joint conference autonomous agents multi agent systems aamas pp 

grosz kraus 

planning acting 
ai magazine 
grosz kraus 

collaborative plans complex group action 
artificial intelligence 
guestrin venkataraman koller 

context specific multiagent coordination planning factored mdps 
proceedings eighteenth national conference artificial intelligence aaai pp 

hansen bernstein zilberstein 

dynamic programming partially observable stochastic games 
proceedings nineteenth national conference artificial intelligence aaai pp 

ho 

team decision theory information structures 
proceedings ieee 
lesser 

self diagnosis adapt organizational structures 
proceedings fifth international conference autonomous agents agents pp 

grosz 

combinatorial auction collaborative planning 
proceedings fourth international conference multiagent systems icmas pp 

jennings 

controlling cooperative problem solving industrial multi agent systems joint intentions 
artificial intelligence 
kaelbling littman cassandra 

planning acting partially observable stochastic domains 
artificial intelligence 
kitano noda matsubara takahashi shimada 

robocup rescue search rescue large scale disasters domain multiagent research 
proceedings ieee conference systems men cybernetics smc pp 

levesque cohen nunes 

acting 
proceedings national conference artificial intelligence pp 

menlo park calif aaai press 
lesser 

solving distributed constraint optimization problems cooperative mediation 
proceedings third international joint conference agents multiagent systems aamas pp 

marschak radner 

economic theory teams 
foundation yale university press new haven ct modi shen tambe yokoo 

asynchronous complete method distributed constraint optimization 
proceedings second international joint conference agents multiagent systems aamas pp 

monahan 

survey partially observable markov decision processes theory models algorithms 
management science 
nair ito tambe marsella 

task allocation rescue simulation domain 
robocup robot soccer world cup vol 
lecture notes computer science pp 

springer verlag heidelberg germany 
nair pynadath yokoo tambe marsella 

taming decentralized pomdps efficient policy computation multiagent settings 
proceedings eighteenth international joint conference artificial intelligence ijcai pp 

nair tambe marsella 

team formation reformation multiagent domains robocuprescue 
kaminka lima 
eds proceedings robocup international symposium pp 

lecture notes computer science springer verlag 
nair tambe marsella 

automated assistants analyze team behavior 
journal autonomous agents multi agent systems 
papadimitriou tsitsiklis 

complexity markov decision processes 
mathematics operations research 
peshkin meuleau kim kaelbling 

learning cooperate policy search 
proceedings sixteenth conference uncertainty artificial intelligence uai pp 

pynadath tambe 

communicative multiagent team decision problem analyzing teamwork theories models 
journal artificial intelligence research 
pynadath tambe 

automated teamwork heterogeneous software agents humans 
journal autonomous agents multi agent systems 
rich sidner 

collagen agents collaborate people 
proceedings international conference autonomous agents agents pp 

scerri johnson pynadath rosenbloom si schurr tambe 

prototype infrastructure distributed robot agent person teams 
proceedings second international joint conference agents multiagent systems aamas pp 

scerri pynadath tambe 

adjustable autonomy real world 
journal artificial intelligence jair 
wooldridge parsons 

reasoning intentions uncertain domains 
proceedings sixth european conference symbolic quantitative approaches reasoning uncertainty ecsqaru pp 

shehory kraus 

methods task allocation agent coalition formation 
artificial intelligence 
sondik 

optimal control partially observable markov processes 
ph thesis stanford 
stone veloso 

task decomposition dynamic role assignment low bandwidth communication real time strategic teamwork 
artificial intelligence 
tambe 

flexible teamwork 
journal artificial intelligence research 
tambe pynadath 

building dynamic agent organizations cyberspace 
ieee internet computing 
tidhar 

team oriented programming preliminary report 
tech 
rep australian artificial intelligence institute 
tidhar 

team oriented programming social structures 
tech 
rep australian artificial intelligence institute 
tidhar rao sonenberg 

guided team selection 
proceedings second international conference multi agent systems icmas pp 

wooldridge 

multiagent systems 
john wiley sons 
xuan lesser 

multi agent policies centralized ones decentralized ones 
proceedings international joint conference agents multiagent systems aamas pp 

xuan lesser zilberstein 

communication decisions multiagent cooperation 
proceedings fifth international conference autonomous agents agents pp 

yen yin miller xu volz 

cast collaborative agents simulating teamwork 
proceedings seventeenth international joint conference artificial intelligence ijcai pp 

yoshikawa 

decomposition dynamic team decision problems 
ieee transactions automatic control ac 

