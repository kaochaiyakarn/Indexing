theoretical comparison gini index information gain criteria laura kilian university computer science departement pierre ch switzerland kilian ch phone fax knowledge discovery databases kdd active important research area promise high payoff business scientific applications 
main tasks kdd classification 
particular efficient method classification decision tree induction 
selection attribute node tree split data split criterion crucial order correctly classify objects 
different split criteria proposed literature information gain gini index 
obvious produce best decision tree data set 
large amount empirical tests conducted order answer question 
conclusive results 
introduce formal methodology allows compare multiple split criteria 
permits fundamental insights decision process 
furthermore able formal description select split criteria data set 
illustration apply methodology widely split criteria gini index information gain 
early field decision tree construction focused mainly definition realization classification systems 
systems described 
different measures impurity entropy goodness select split attribute order construct decision tree 
certain number algorithms defined lot research dedicated compare 
relatively hard task different systems evolved different backgrounds information theory discriminant analysis encoding techniques comparisons predominantly empirical 
briefly enumerate reported supported number swiss national science foundation 
experiments 
authors proposed measure distance bias evaluation metrics gave numerical approximations 
thorough understanding behavior split functions demands analytical direct comparison external measure 
contribution introduce formal methodology allows analytically compare multiple split criteria 
permits fundamental insights decision process 
furthermore able formal description select split criteria dataset 
illustration apply methodology widely split criteria gini index information gain 
notations realize theoretical analysis introducing notations definitions 
learning sample klk denote klk number objects measurement vector measurement space 
jg fc set classes 
prior probability object belongs class kc klk test possible outcomes denote set objects having outcome probability test outcome estimated kt klk kc denotes number objects lay class outcome test probability object lays outcome kc klk conditional probability jt object lays class condition test outcome estimated obviously jt ng jt jt ng kg 
gini index information gain criteria binary tree classifiers constructed repeatedly splitting subsets descendant subsets 
split smaller smaller subsets select splits way descendent subsets purer parents 
introduced goodness split criterion derived notion impurity function 
impurity function function defined set tuples numbers satisfying kg properties achieves maximum point achieves minimum points symmetric function 
impurity function impurity measure node defined jt jt jt 
split node divides examples subsets proportions decrease impurity defined 

goodness split defined 

test node test attribute having possible values expressions defined generalized follows jt jt jt 

breiman adopts gini diversity index form jt jt jt jt jt jt node impurity function gini index criterion assigns example class probability jt 
estimated probability item class jt 
estimated probability misclassification rule gini index jt jt jt function interpreted terms variance 
node assign examples belonging class value examples value 
sample variance values jt jt 
classes corresponding variances summed jt jt jt having test outcomes goodness split expressed gini index follows gini jt jt gini index criterion selects test maximizes function 
information gain function origin information theory 
notion entropy characterizes impurity arbitrary set examples 
randomly select example set announce belongs class probability message equal kc klk amount information conveys log 
expected information provided message respect class membership expressed info log quantity info measures average amount information needed identify class example quantity known entropy set relative wise classification 
logarithm base entropy measure expected encoding length measured bits 
consider similar measurement partitioned accordance outcomes test expected information requirement weighted sum subsets info info 
information gained partitioning accordance test measured quantity gain info info 
rewrite information gain gain log jt log jt information gain criterion selects test maximizes information gain function 
selected test criteria satisfy gini max possible test gini gain max possible test gain respectively 
gini gini possible test gain gain possible test 
order obtain characterization criteria compare restraint loss generality situation possible outcomes test possible classes 
gini jt jt gain log jt log jt simplicity denote jt jt 
jt jt 
notations simple calculations rewrite gini index information gain functions gini rp gain log log log log log log 
theoretical analysis gini index information gain criteria suppose tests different attributes split node 
analyze gini index criterion information gain criterion select test 
case know conditions criteria select differently 
write gini index information gain functions tests gini rp gini gain log log log log log log gain log log log log log log 
observe kc klk probability remains constant independently selected test 
number examples belonging class class respectively remains constant independently selected test relation holds relates respectively relates follows cases treated separately 
furthermore conditions satisfied difference gini index functions corresponding written gini gini rp rq rp rp rq 
simplify expression introduce difference gini index functions corresponding tests positive favorite test gini index criterion favorite test holds information gain functions 
difference corresponding information gain functions expressed follows gain gain log log log log log log log log 
simplify expression function substitute log log 

derivative negative interval positive interval second derivative positive 
function monotonically decreasing monotonically increasing strictly convex function 
difference information gain functions corresponding tests rewritten gain gain 
fr 
apply lagrange theorem function intervals 
function continuous derivative exists finite lagrange theorem theorem conditions satisfied similarly express information gain difference gain gain fr re 
establish sign difference conditions denote ratio proposition analysis establish order points proposition strictly convex function defined proof 
strictly convexity 



proposition results satisfied terms simultaneously positive simultaneously negative consequently terms negative positive term negative terms positive 
characterization gini index information gain functions done account possible cases 
results section intervals coincidence non coincidence choice split attribute gini index information gain criteria 
sign differences gini index functions corresponding tests information gain functions established possible situations 
details case illustration 
complete analysis 
sign difference gini index functions gini gini sign difference information gain functions gain gain split criteria select attribute split select different attributes split 
case 
case subdivided subcases case proof easy show 


assure satisfied 
satisfied 
verify satisfied necessary ratios positive smaller conclude case addition easily show knowing position relative establish sign difference gini gini 
gini gini gini gini 
evaluate difference gain gain proceed way 
conditions obtained remain valid 
find time position establish order points ordered considering possible permutations 
applying proposition points find 
strictly monotonically increasing derivative positive conclude analyze case cases contradict monotonicity easy show 
gain gain gain gain 
minff split selected 
minff different splits selected 
split selected 
case proof establish position conditions obtain 
gini gini 
gini gini 
gini gini 
evaluate difference gain gain proceed way 
conditions obtained remain valid 
points ordered previous case 
applying proposition points strictly monotonically increasing conclude possible cases case 
gain gain 
case gain gain 
proof true 
behavior split functions identical choosing split 
case proof gini gini 
gini gini 
gini gini 
applying proposition points fact strictly monotonically increasing conclude cases analyze gain gain 
proof true situation 
gain gain 
behavior split functions identical choosing split 
case proof analogously obtain gini gini 
gini gini 
applying proposition points strictly monotonically increasing conclude case analyze 
gain gain gain gain 
proof true strict convexity proof 
true strict convexity minff split minff different splits split 
case proof case dropped contradicts conditions 
possible cases listed treated manner 
remaining cases divided sub cases account position domains established sub case identical path case 
complete detailed analysis 
results obtained cases identified resumed way 
case obtained situations split criteria selecting different tests symmetry obtain case situations 
cases similar symmetry obtain situation selection test done differently criteria 
cases symmetric obtain situation different selection 
formal analysis able study behavior gini index information gain give exact mathematical description situations choosing test split 
allows constructing decision trees decide database gini index criterion information gain criterion select split attribute 
order compare split functions general way obtained results compute frequency agreement disagreement split functions 
sequence tests calculated considered sizes databases number cases disagreement 
higher 
explains empirical studies concluded significant difference criteria 
course exclude specific databases important difference 
formal comparison behavior popular split functions gini index function information gain function 
situations split functions agree disagree selected split mathematically characterized 
characterizations able analyze frequency agreement gini index function information gain function 
disagree explains previously published empirical results concluded possible decide tests prefer 
emphasize methodology introduced limited analyzed split criteria 
successfully formalize compare split criteria 
gained deeper insights split process currently working system select optimal criterion user defined optimality criterion 
preliminary results 

extraction diagnostic rules recursive partitioning systems comparison 
artificial intelligence medicine october 
breiman friedman olshen stone 
classification regression trees 
wadsworth international group 
lopez de mantaras 
distance attribute selection measure decision tree induction 
machine learning 
gama brazdil 
characterization classification algorithms 
pinto ferreira editors progress artificial intelligence th portuguese conference artificial intelligence pages 
springer verlag 
igor kononenko 
biases estimating multi valued attributes 
chris mellish editor ijcai proceedings fourteenth international joint conference artificial intelligence pages montreal canada august 
morgan kaufmann publishers san mateo ca 
lim yin loh yu shan shih 
comparison prediction accuracy complexity training time old new classification algorithms 
machine learning 
john mingers 
empirical comparison selection measures decision tree induction 
machine learning 

criteria selecting variable construction efficient decision trees 
ieee transactions computers january 
moret 
decision trees diagrams 
computing surveys 
venkata sreerama murthy 
growing better decision trees data 
phd thesis john hopkins university baltimore maryland 
pagallo 
adaptive decision tree algorithms learning examples 
phd thesis university california santa cruz 
quinlan 
programs machine learning 
morgan kaufmann publishers 
john ross quinlan 
simplifying decision trees 
international journal man machine studies 
laura 
theoretical comparison gini index information gain functions 
technical report de sciences universite de 

survey decision tree classifier methodology 
ieee transactions systems man cybernetics 
sahami 
learning non linearly separable boolean functions linear threshold unit trees madaline style networks 
aaai press editor proceedings eleventh national conference artificial pages 
kilian laura 
selecting optimal split functions large datasets 
research development intelligent systems xvii bcs conference series 
ricardo daniel 
quantification distance bias evaluation metrics classification 
proceedings th international conference machine learning 
stanford university 
allan white wei zhang liu 
bias il information measures decision tree induction 
machine learning june 

