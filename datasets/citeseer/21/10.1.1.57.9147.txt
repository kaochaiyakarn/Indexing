optimal kernel shapes local linear regression dirk ormoneit trevor hastie ormoneit stat stanford edu department computer science technische universitat munchen department statistics stanford university stanford ca 
local linear regression performs low dimensional forecasting problems 
high dimensional spaces performance typically decays due known curse dimensionality 
specifically volume weighting kernel contains fixed number samples increases exponentially number dimensions 
bias local linear estimate may unacceptable real world data sets 
possible way control bias varying shape weighting kernel 
suggest new data driven method estimating optimal kernel shape 
experiments artificially generated data sets data uc irvine repository show benefits kernel shaping 
keywords local linear regression smoothing lazy learning entropy bandwidth selection nearest neighbors projection pursuit regression sliced inverse regression dirk ormoneit trevor hastie 
local linear regression attracted considerable attention statistical machine learning literature flexible tool nonparametric regression analysis 
statistical smoothing approaches local modeling suffers called curse dimensionality known fact proportion training data lie fixed radius neighborhood point decreases zero exponential rate increasing dimension input space 
due problem bandwidth weighting kernel chosen big contain reasonable sample fraction 
result estimates produced typically highly biased 
possible way reduce bias local linear estimates vary shape weighting kernel 
suggest method estimating optimal kernel shape training data 
purpose parameterize kernel terms suitable shape matrix minimize mean squared forecasting error respect approach meaningful size weighting kernel constrained minimization avoid overfitting 
propose new entropy measure kernel size constraint 
analogy nearest neighbor approach bandwidth selection suggested measure adaptive regard local data density 
addition leads efficient gradient descent algorithm computation optimal kernel shape 
experiments artificially generated data sets data uc irvine repository show kernel shaping improve performance local linear estimates substantially 
remainder organized follows 
section briefly review local linear models introduce notation 
section formulate objective function kernel shaping section discuss entropic neighborhoods 
estimation procedure described section section summarizes experimental results 
section presents delineates efforts apply kernel shaping locally 
discuss relationship optimal kernel shapes local linear regression kernel shaping alternative methods projection pursuit regression sliced inverse regression section 
local linear models consider nonlinear regression problem continuous response ir predicted dimensional predictor ir tg denote set training data 
estimate conditional expectation yjx consider local linear expansion ff gamma fi neighborhood detail minimize weighted squares criterion ff fi gamma ff gamma gamma fi determine estimates parameters ff fi standard way define weighting kernel applying univariate non negative mother kernel oe distance measure jjx gamma jj omega gamma gamma oe jjx gamma jj omega oe jjx gamma jj omega intuitively assigns weight residuals neighborhood residuals distant omega positive definite theta matrix determining relative importance assigned different directions input space 
example oe standard normal density normalized multivariate gaussian mean covariance matrix omega gamma simplicity defined satisfy equation 
rescaling irrelevant regard minimization convenient discussion entropic neighborhoods section 
shorthand notation fl ff fi solution minimization problem may conveniently written fl omega gamma gamma theta design matrix rows gamma vector response values theta diagonal matrix entries 
dirk ormoneit trevor hastie resulting local linear fit inverse covariance matrix omega simply omega gamma ff obviously omega gamma depends omega definition weighting kernel 
discussion focus choices omega lead favorable estimates unknown function value 

kernel shaping local linear estimates resulting different choices omega vary considerably practice 
common strategy choose omega proportional inverse sample covariance matrix 
problem finding correct scaling factor equivalent problem bandwidth selection univariate smoothing 
example bandwidth frequently chosen function distance kth nearest neighbor practical applications 
take different viewpoint argue optimizing shape weighting kernel important optimizing bandwidth 
specifically fixed volume weighting kernel bias estimate reduced drastically shrinking kernel directions large nonlinear variation stretching directions small nonlinear variation 
statement formal noting asymptotic bias local linear estimate proportional tr omega gamma hessian 
idea kernel shaping illustrated example shown 
plotted function sigmoidal index vector constant directions orthogonal shaped weighting kernel shrunk direction stretched orthogonal direction minimizing exposure kernel nonlinear variation 
formally distinguish metric bandwidth weighting kernel rewrite omega follows omega delta ll corresponds inverse bandwidth may interpreted metric shape matrix 
suggest algorithm designed min optimal kernel shapes local linear regression 
left example single index model form tanh 
right contours function straight lines orthogonal bias respect kernel metric 
clearly approach meaningful need restrict volume weighting kernel bias estimate minimized trivially choosing zero bandwidth 
ways define kernel volume 
possibility consider determinant omega natural way formulate volume constraint choose equation satisfy omega constant serious disadvantage idea contrast nearest neighbor approach omega independent design 
appropriate alternative define terms measure number neighboring observations 
detail fix volume terms entropy weighting kernel 
choose satisfy resulting entropy constraint 
definition bandwidth determine metric minimizing mean squared prediction error gamma omega gamma respect way obtain approximation optimal kernel shape expectation differs bias variance term independent details entropic neighborhood criterion numerical minimization procedure described 
dirk ormoneit trevor hastie 
entropic neighborhoods mentioned shape matrix choose bandwidth parameter fulfill volume constraint weighting kernel 
purpose interpret kernel weights probabilities 
particular definition formulate local entropy omega gamma gamma log entropy probability distribution typically thought measure uncertainty 
context weighting kernel omega gamma may smooth measure size neighborhood averaging 
see note extreme case equal weights placed observations entropy maximized 
extreme single nearest neighbor assigned entire weight entropy attains minimum value zero 
fixing entropy constant value essentially equivalent fixing number nearest neighbor approach 
justifying correspondence derive intuitive parametrization entropy constraint 
detail specify terms hypothetical weighting kernel places equal weight nearest neighbors zero weight remaining observations 
note entropy hypothetical kernel log natural characterize size entropic neighborhood terms determine numerically solving nonlinear equation system omega gamma log precisely report number neighbors terms equivalent sample fraction ae intuition 
idea illustrated dimensional example 
equivalent sample fractions ae ae respectively 
note cases weighting kernel wider regions observations narrower regions observations 
optimal kernel shapes local linear regression consequence number observations contours equal weighting remains approximately constant predictor space 

left univariate weighting kernel delta evaluated sample data set observations indicated bars bottom 
samples generated mixture uniform distribution selected probability normal distribution 
chose equivalent neighborhood size ae 
right multivariate weighting kernel delta sample data set observations 
samples generated mixture uniform distribution selected probability normal distribution mean covariance matrix 
ellipsoids correspond contours weighting kernel evaluated summarize define value fixing equivalent sample fraction parameter ae subsequently minimize prediction error training set respect shape matrix minimization procedure variant gradient descent accounts entropy constraint 
details optimization procedure described 

estimation practical implementation kernel shaping need efficient procedure minimization respect shape matrix particular omega gamma depends complicated fashion closed form expression optimal kernel shape available general 
numerical methods account entropy constraint minimization prohibits standard 
specifically suggest variant dirk ormoneit trevor hastie gradient descent algorithm gradient evaluated projections 
approach performed experiments identifying optimal solution relatively iterations largely independent initialization values 
heart algorithm evaluation gradient prediction error gamma gamma omega omega notation omega emphasize exposition section omega depends location forecast value equation 
apply equation practice need evaluate derivative matrix omega evaluation carried steps accounting dependence local linear estimate omega omega subsequently adjusting 
regard step note omega depends omega changes ff component fl omega weighting matrix changes 
implicit differentiation equation gives omega omega gamma theta theta array defined appendix 
second step apply chain rule omega omega condition constant obtain omega omega omega gamma omega result formally derived appendix 
interestingly term parenthesis interpreted projection unconstrained gradient omega omega manifold characterized equation 
error gradient re carry straightforward gradient descent search optimal gamma optimal kernel shapes local linear regression ff step size parameter determined experimentally 
note section defined multiplier contingent shape matrix updated gradient descent step 
spent considerable effort development efficient numerical algorithm purpose described detail appendix 
test runs gradient descent algorithm converged steps corresponding minutes computation time heavily hp workstation data set observations 
particular sophisticated optimization routines robust conjugate gradient method combined backtracking line search led efficiency improvements experiments 
statistical perspective crucial issue regard estimation error number free parameters means controlling number allow possibility may reduced rank note result definition omega positive definite regardless value summary experimental results described 

experiments section compare kernel shaping standard local linear regression fixed spherical kernel examples 
evaluate performance dimensional toy problem allows estimate confidence intervals prediction accuracy monte carlo simulation 
second repeat experiment dimensional predictor space 
third investigate data set machine learning data base uc irvine 

mexican hat function example employ monte carlo simulation evaluate performance kernel shaping dimensional regression problem 
purpose sets data points generated independently model cos delta exp gamma dirk ormoneit trevor hastie predictor variables drawn randomly standard normal distribution 
note regression carried dimensional predictor space really function variables particular dimensions contribute information regard value kernel shaping effectively discard variables 
note noise example 

left true mexican hat function 
middle local linear estimate spherical kernel ae 
right local linear estimate kernel shaping ae 
estimates training set consisting data points 
shows plot true function spherical estimate estimate kernel shaping functions true function familiar mexican hat shape recovered estimates different degrees 
quantify influence kernel shaping predictive performance example evaluate local linear estimates values equivalent neighborhood fraction parameter ae range 
value ae models estimated artificially generated training sets subsequently performance evaluated training set test set theta grid points shown 
shape matrix maximal rank experiment 
results local linear regression fixed spherical kernel kernel shaping summarized table 
performance measured terms mean value models standard deviations reported parenthesis 
results table indicate optimal performance test set obtained parameter values ae kernel shaping optimal kernel shapes local linear regression table 
performances toy example 
results kernel shaping obtained gradient descent steps 
kernel training test spherical kernel ae spherical kernel ae spherical kernel ae spherical kernel ae spherical kernel ae kernel shaping ae ff kernel shaping ae ff kernel shaping ae ff kernel shaping ae ff spherical kernel 
large difference values conclude kernel shaping clearly outperforms spherical kernel data set 

eigenvectors estimate omega obtained training sets 
graphs ordered left right increasing eigenvalues decreasing extension kernel direction 
shows eigenvectors optimized omega training set 
eigenvectors ordered size corresponding eigenvalues 
note rightmost eigenvectors correspond directions minimum kernel extension span exactly space true function lives 
kernel stretched remaining directions effectively discarding nonlinear contributions dirk ormoneit trevor hastie 
mexican hat function ii repeat previous experiment higher dimensional environment predictor variables 
detail artificial data generated model augmented normally distributed nuisance variables case 
difficult prediction problem feasible double size training set 
simultaneously number training data sets monte carlo simulation reduced limit computational burden 
note maximum number free parameters shape matrix quadruples comparison previous section 
control estimation efficiency choose shape matrix rank experiment 
note fact really rank sufficient represent mexican hat function 
results experiments summarized table organized section 
note performance spherical estimate suffers drastically increase dimensionality optimal ae comparison section 
kernel shaping optimal performance decreases ae comparison 
kernel shaping clearly outperforms local linear estimation spherical kernel 
regard shape matrix omega gamma eigenvectors qualitatively resemble findings previous section 
weighting kernel correctly identifies important random variables bottom right discards nonlinear contributions estimates noisy due increased number free parameters 

abalone database task third example predict age abalone measurements 
specifically response variable obtained counting number rings shell time consuming procedure 
preferably optimal kernel shapes local linear regression table 
performances second mexican hat example 
results kernel shaping obtained gradient descent steps 
kernel training test spherical kernel ae spherical kernel ae spherical kernel ae spherical kernel ae spherical kernel ae spherical kernel ae spherical kernel ae kernel shaping ae ff kernel shaping ae ff kernel shaping ae ff kernel shaping ae ff kernel shaping ae ff kernel shaping ae ff kernel shaping ae ff kernel shaping ae ff kernel shaping ae ff kernel shaping ae ff 
eigenvectors estimate omega obtained training sets 
graphs ordered top left bottom right increasing eigenvalues decreasing extension kernel direction 
age abalone predicted alternative measurements may obtained easily 
data set candidate measurements including sex dirk ormoneit trevor hastie dimensions various weights reported number rings abalone 
predictors normalized zero mean unit variance prior estimation 
data set consists observations 
prevent possible artifacts resulting order data records randomly draw observations training set remaining observations test set 
results summarized table various settings rank equivalent fraction parameter ae gradient descent step size ff 
optimal choice ae kernel shaping spherical kernel 
note performance improvement due kernel shaping negligible experiment 
table 
results abalone database gradient descent steps 
kernel training test spherical kernel ae spherical kernel ae spherical kernel ae spherical kernel ae spherical kernel ae spherical kernel ae kernel shaping ae ff kernel shaping ae ff kernel shaping ae ff kernel shaping ae ff kernel shaping ae ff kernel shaping ae ff 
extensions introduced data driven method improve performance local linear estimates high dimensions optimizing shape weighting kernel 
experiments kernel shaping clearly outperformed local linear optimal kernel shapes local linear regression regression spherical kernel dimensional toy example led small performance improvement third real world example 
explain results third experiment note kernel shaping aims exploiting global structure data 
absence larger performance improvement may simply suggest globally optimal kernel shape example 
optimal kernel shape may exist locally point prediction optimal shapes vary sufficiently predictor space approximated particular global shape 

local kernel shaping promising idea circumvent difficulty apply kernel shaping locally augment error criterion secondary weighting kernel local gamma omega gamma intuitively constrains local domain training data estimate shape matrix minimizing different locations predictor space may lead different estimates assess approach applied mexican hat example section omitting nuisance variables simplicity 
shape estimates sample points shown 
note computed kernel shapes intuitively obvious correctly emphasize tangential direction rotation symmetric surface function locally constant 
complicated problems shape estimates extremely noisy secondary kernel effectively reduces number available training data 
additional disadvantage numerical minimization carried location forecast global kernel shaping 
due difficulties efforts lines lead favorable procedure 
dirk ormoneit trevor hastie 
locally estimated kernel shapes observations gamma gamma gamma gamma observations estimate kernel shapes equivalent sample fraction ae 
ellipsoids correspond contours local weighting kernels 

projection pursuit regression exists interesting connection kernel shaping alternative regression methods linear projections subspace original predictor space 
subspace typically defined terms set projection vectors fi fi example projection pursuit regression method friedman stuetzle concerned models form oe fi oe delta univariate kernel smoothers 
generally li considers model fi fi sliced inverse regression 
cases authors suggest numerical procedures identify projection vectors fi fi maximize fit training data 
problem closely related kernel shaping 
specifically optimal kernel shapes local linear regression may think multivariate kernel smoothers shape matrix form fi fi parametrization omega gamma omega delta ll effectively project fi fi evaluate weighting function projected space 
gives model form local averaging smoothing method 
local linear estimate effectively obtain varying coefficient model form fl coefficient vector fl depends weighting projected space 
correspondence kernel smoothers projection methods suggests combine methods practice 
particular kernel shaping compute optimal projection vectors 
purpose need modify computation error gradient re section account 
approach appears particularly appealing dimensionality projection space unknown kernel shaping correct value need specified advance 
demonstrated toy examples sections correct dimensionality orientation subspace inferred eigenvalues eigenvectors omega gamma alternatively may carry pilot estimation full rank determine subsequently estimate values projection vectors projection pursuit regression sliced inverse regression 
acknowledgments dirk ormoneit supported deutsche forschungsgemeinschaft dfg part post doctoral program 
trevor hastie partially supported dms national science foundation roi ca national institutes health 
carrie grimes pointed misleading formulations earlier drafts 
dirk ormoneit trevor hastie appendix 
derivative matrices elements tree way array equation gamma fl omega omega term corresponds elements matrix omega gamma gamma gamma regard transformed gradient application chain rule gives omega gamma omega omega vec omega omega gamma vec ll applying transformation entropy level omega gamma noting definition obtain gamma omega vec omega vec ll substituting back gives scalar defined vec omega omega gamma vec ll vec omega vec ll 
scaling algorithm noted section disadvantage entropic neighborhood method scaling factor computed numerically gradient descent step optimization specifically recall defined solution ll gamma log aet depth discussion numerical solution methods nonlinear equation systems see 
regard application turns optimal kernel shapes local linear regression solution time critical operation algorithm 
standard bisection search suggest newton method purpose gamma derivative gamma log gamma ll gamma important note convergence depends crucially absolute value 
particular non concave non convex small bandwidths take values close zero 
may fail converge situations 
standard way modify give globally convergent algorithm carrying backtracking line search 
automatically accepting new compare new old function value 
detail accepted jg jg rejected trial value 
backtracking step repeated sufficiently close accepted 
note backtracking restriction essentially prevents situations runs cycle 
experiments usually converged iterations backtracking steps 
dirk ormoneit trevor hastie 
atkeson moore schaal 
locally weighted learning 
artificial intelligence review 

blake merz 
uci repository machine learning databases 
technical report department information computer science university california 
www ics uci edu mlearn mlrepository html 

cleveland 
robust locally weighted regression smoothing scatterplots 
journal american statistical association 

dennis schnabel 
numerical methods unconstrained optimization nonlinear equations 
classics applied mathematics 
siam 

fan gijbels 
local polynomial modelling applications 
chapman hall 

friedman stuetzle 
projection pursuit regression 
journal american statistical 

ker chau li 
sliced inverse regression dimension reduction 
journal american statistical association 
discussion 

ruppert wand 
multivariate locally weighted squares regression 
annals statistics 
