polynomial value iteration algorithms deterministic mdps madani department computing science university alberta edmonton ab canada madani cs ualberta ca value iteration commonly empirically competitive method solving markov decision process problems 
known value iteration pseudopolynomial complexity general 
establish somewhat surprising polynomial bound value iteration deterministic markov decision problems 
show basic value iteration procedure converges highest average reward cycle problem iterations mn total time denotes number states number edges 
give extensions value iteration solve mn time 
explore analysis policy iteration algorithms report empirical study value iteration showing convergence faster random sparse graphs 
markov decision processes offer clean rich framework problems control decision making uncertainty bdh rn 
infinite horizon fully observable mdp problems mdps classic optimization problems framework 
mdp problems significant solutions problems repeatedly solving problem variants stochastic games partially observable mdps sha han 
preferred methods solving mdp problems dynamic programming strategies particular contain called value iteration policy iteration loop put lit han gkp 
methods converge optimal solutions quickly practice know little asymptotic complexity 
known algorithms value iteration better pseudo polynomial run time mdp prob algorithm pseudo polynomial run time complexity runs time polynomial unary representation lems tse lit 
analyze basic value iteration procedure deterministic mdp problem average reward criterion problem establish positive results 
problem known maximum minimum mean cycle problem directed weighted graph amo 
solving interested finding highest average weight cycle optimal cycle average weight cycle highest mean 
policy problem simply subgraph vertex state single edge action choice leading directed path optimal cycle 
just case general mdps direct indirect applications example solving network flow problems system performance analysis amo dg 
establish graphs vertices edges basic value iteration converges optimal cycle iterations irrespective initial assignment values 
somewhat unexpected considering value iteration generally pseudo polynomial 
show bound tight giving example value iteration takes iterations 
note optimal cycle polynomial time examples exist value iteration takes pseudo polynomial time converge optimal policy 
occurs may take iterations states reside optimal cycle choose optimal action section 
sense value iteration polynomial finding optimal policies 
finding optimal cycles main task expect value iteration converges faster practice 
experiments random graphs show value iteration converges exponentially faster worstcase bound suggest see section 
insight analyses allows show making small modifications value iteration optimal cycles policies iterations 
numbers input exponential binary representation 
iteration takes time gives algorithm ties algorithm run time mn kar amo 
algorithm described additional interesting property distributed vertex performs simple local computation need communicate immediate neighbors edges iterations vertices know highest mean 
iterations optimal cycle form 
algorithmic technique developed extends give polynomial algorithms general problem classes edges may parameters probability time cost addition reward mad 
give polynomial algorithm similar multi chain policy iteration algorithm put 
polynomial bound proof algorithm identical simple value iteration conjecture bound tight 
describe similarity policy iteration difficulty analyzing policy iteration indicate promising ways addressing open problems 
investigate convergence value iteration random sparse graphs 
experiments suggest value iteration converges optimal cycles expected log iterations exponentially faster worst case bound indicates 
experiments provide valuable insights algorithms excellent performance practice 
problem definitions notation section 
section describe analysis value iteration 
section presents modified algorithms mn time 
give results policy iteration section report experiments discuss previous empirical studies algorithms 
section concludes summary discussion open problems directions 
tried describe line argument proofs sketches important steps 
complete proofs explanations expanded empirical section appear mad 
preliminaries give graph theoretic definition problem save space 
directed graph vertices edges function edge set real numbers edge reward vertex single edge 
walk 
progression edges vertex start vertex walk may repeated edges 
call edge resp 
walk connecting vertex start vertex vertex edge resp 
walk 
walk jwj denote number edges total reward average reward mean 
cycle walk start vertex vertex vertex repeated 
max ranges cycles call maximum mean solving problem interested problem computing finding optimal cycle mean problems equivalent 
problem shown solvable mn time karp kar 
algorithms run times mn log algorithms believed faster unmodified karp algorithm practice karp algorithm takes mn irrespective underlying graph 
best knowledge algorithms known mn prior ho dg modifications karp algorithm 
basic value iteration process shown fig 


denote time points time immediately iteration algorithm 
denote value vertex time initial value 
value edge iteration vertex iteration performs computation obtain new value max edge call subgraph degree exactly vertex policy 
choice edge vertex iteration value iteration defines policy say value iteration visits say vertex changes edges simply switches iteration choice edge eq 
changes previous iteration 
assumption important correctness algorithms vertex chosen edge previous iteration ties best value edge chosen 
words value iteration lazy changing policy iteration 
ties iteration previous edge highest valued edge may broken arbitrarily 
note iteration value iteration takes time 
fig 
shows iterations value iteration vertices assigned zero values initially 
sequence values vertex hx hx 
interestingly related characterization maximum eigenvalue matrix edge weights max plus operations multiplication summation ax translate summation maximum respectively 
turn corresponds dynamic programming operation eq 

result shows value vector vector special performing value iteration iterations visits policy containing optimal cycle 

vertex begins value 
repeat 
vertex chooses highest valued edge obtains value pseudocode value iteration 
vertex graph shown edge rewards 
iteration value iterations vertices start zero initial values visited policies vertex values iteration shown 
corresponding mean zero parallel graph sect 
value iterations 
polynomial convergence value iteration show analyzing value iteration transformed graph refer mean zero graph suffices showing polynomial convergence analyze value iteration mean zero graphs 
consider value iteration proceeds graph vertices initialized arbitrary values 
value walk iteration jwj sum total reward value vertex jwj time points ago example fig 
value walk composed single edge iteration iteration value walk iteration 
lemma relates value walks value vertices 
shown induction time length walks 
lemma maximum value values walks length start vertex history walk vertex iteration informally walk formed sequence edge selections value iteration starting going back time 
precisely iteration vertex edge history walk time step consists edge history walk steps concatenated history walk length steps vertex iteration 
history walk necessarily maximum valued walk lemma 
fig 
history walk length vertex iteration vertex 
call graphs parallels identical vertex edge sets reward function graph constant offset reward function constant 
average reward cycle offset cycles keep relative merits corresponding graphs particular optimal cycles remain transformation edge rewards furthermore value iteration behaves identically parallel graphs meaning vertex selects edge problem iteration subject ties consequence lemma lemma consider value iteration started equal initial values pair parallel graphs reward functions respectively constant 
value vertex time graph value vertex time proof sketch values walk length particular history walk parallel graphs different lemma 
graph maximum mean parallel graph offset maximum mean fig 

consequence optimal cycles identical parallel graphs identical behavior value iteration parallel graphs properties structure history walks policies show hold graph section lemma hold general case 
fig gives high level picture identical behavior value iteration parallel graphs rate convergence optimal cycles policies 
value iteration mean zero graphs line argument section roughly follows 
show vertex sequences values maximum maximum reached iterations lemmas 
vertices optimal cycle obtain maximum values collection upper bound values defined keep values choosing edges optimal cycle show fact happens lemmas 
transformation kar 
iterations pseudo polynomial exponential worst case permanent policy initial optimal cycles policies visited optimal value iteration visits sequence policies starts identical initial values parallel graphs 
policies vertex values may repeat way optimal policies 
convergence take exponentially iterations iterations formation optimal cycles 
mean zero case cycle positive reward lemma follows 
lemma mean zero case vertex history walk length iteration cycle suboptimal negative mean 
consider sequence values vertex hx hx value iteration proceeds mean zero graph 
lemma central results bounds latest time increase maximum value occur sequence hx subsequences 
take lemma period interval look values sequence hx consider simplest case look values sequence 
lemma states vertex increase maximum value values seen far occur iterations 
words sequence values maximum occurs iterations 
similarly consider odd subsequences 
lemma states highest values subsequence hx odd subsequence hx appear iterations 
note highest values highest value subsequence appear iterations 
similar results hold higher periods lemma time dominance property holds mod pn proof 
sketch assume dominance property holds vertex time consider subsequence vertices formed examining history walk steps example vertex sequence history walk hv 
vertices hv 

jwj see repeat vertex due dominance property lemma 
follows jw pn 
lemma consequences 
integer vertex denote highest value vertex obtains iteration mod values defined bounded lemma 
call values highest values example mean zero parallel graph fig 
vertex obtains value sequence 
zero initial vertex value assignment highest values assume vertex mean zero optimal cycle mean zero follows lemma obtains highest value jcj highest values gets back jcj iterations 
note highest values may equal 
highest value obtains 
consequence lemma obtains value iterations highest values kn iterations 
show basically jcj highest values reach vertices mean zero cycle vertices cycle need deviate cycle choose edges cycle subsequent value iteration lemma 
lemmas help establish convergence 
observe vertex obtains highest value immediate neighbor obtain highest value iteration general vertex obtains highest value subsequent iteration 
lemma generalization property jcj highest values defined 
statement lemma consequence lemma 
second statement follows addition lemmas 
lemma assume vertex edge optimal cycle obtains jth highest value jcj obtains st highest value jcj iteration 
lemma history walk vertex optimal cycle obtains highest value includes optimal cycles 
iteration history walk vertex optimal cycle includes optimal cycles 
lemma vertices optimal cycles obtained highest values iterations optimal cycle appears subsequent visited policies 
proof 
vertices optimal cycle find highest values vertex chooses edge example graph takes iterations optimal cycle form time 
edges displayed reward zero reward 
numbered vertices top row form optimal cycle vertices zero mean 
remaining rows vertices 
vertex initialized zero initialized cycle switch change edge due assumption lazy policy change 
vertices optimal cycles find highest values vertex obtains history walk contain mean zero general optimal cycles 
consider cycle walk 
vertices highest values vertices switch 
multiple optimal cycles examples exist vertices may repeatedly change edges forever shown eventually vertices path optimal cycle visited policy 
consider vertex high reward edge cycle mean edge mean zero optimal cycle 
hard see eventually choose edge mean zero cycle take iterations 
example formalized show worst case number iterations optimal policies pseudo polynomial see example zp 
hard give example time highest values arrive vertices optimal cycle mean time optimal cycle fixed worst case 
fig 
shows time formation optimal cycle takes time worst case 
graph vertex initialized zero vertices may assigned value example explained expanded mad 
consequence lemmas example graph obtain theorem value iteration 
theorem value iteration converges optimal cycle problem iterations 
common variation value iteration referred gauss value iteration put necessarily converge optimal cycle 
variation vertices numbered vertex values updated order iteration new value vertex soon available 
note natural implementation value iteration sequential machine 
history walks case longer number iterations lemma particular breaks gauss value iteration gauss value iteration identical behavior parallel graphs 
shown converges cycle properties value iteration mean zero graphs hold gauss 
algorithms histories compute optimal cycle basic value iteration iterations convergence takes iterations 
lemma shows 
keep track edge chosen vertex iteration iterations value iterations progresses reconstruct cycles history walks cycle optimal lemma 
searching optimal cycle takes time algorithm takes mn mn time unfortunately requires space 
describe variation reduces space back linear 
algorithm desirable property just value iteration distributed nature vertex performs simple local computation vertices discover optimal mean 
algorithm works phases phase simple value iteration iterations 
second phase takes iterations vertex performs additional computation addition updating value edge choice 
iteration vertex keeps track current value chosen edge updates parameters characterizing super edge 
super edges summarize history walks may viewed packets sent edges 
super edge dropped updated passed iteration 
vertex discovers super edge sent computes average value cycle corresponding super edge parameters super edge updates current highest mean far 
highest mean second phase optimal super edge parameters vertex ends number edges super edge total reward 
iteration second phase super edge vertex may undefined 
vertex computes super edge iteration follows 
assume chooses edge iteration case super edge iteration undefined super edge defined super edge super edge vertex obtained cyclic super edge mean respectively rs case running estimate updated necessary vertex marks current super edge undefined 
example ver keeping track super edges starting iteration fig 
super edges vertex iterations respectively vertices began keeping track super edges starting iteration super edges vertex iterations respectively iteration vertex update highest mean far necessary case mark current super edge undefined 
algorithm takes iterations iteration takes constant time edge run time mn extra space 
correspondence super edges history walks lemma establish correctness 
subtlety multiple optimal cycles ties edge selections occur 
case assume vertex chooses edge vertex super edge lowest numbered vertex 
second iteration vertex super edge assume ties broken lower numbered vertex 
call rule lowest index rule 
lemmas establish properties super edges lead correctness algorithm lemma time point super edge length vertex corresponds history walk length vertex time 
highest mean averages cyclic super edges discovered second phase 
lemma shown noting walk start vertex possibly cycles inside mean greater mean greater lemma cyclic super edge average reward greater optimal mean value algorithm 
cyclic super edge computed second phase mean hard see optimal cycle unique highest value mean zero graph created traces optimal cycle iterations 
case multiple optimal cycles ties edges may broken arbitrarily examples show cyclic super edge corresponding optimal cycle created second iterations 
lowest index rule prevents 
expect easier rules example vertex breaks ties consistently locally give correct algorithms 
lemma assume lowest index rule breaking ties 
vertex obtains cyclic super edge corresponding optimal cycle second iterations 
correctness algorithm shall refer history walk algorithm follows 

arbitrary policy 
repeat new cycle discovered 
update edge rewards edge choices vertex values 
apply value iteration new cycle discovered iterations 
generic phased policy iteration 
theorem history walk algorithm takes mn time uses space finding optimal mean natural question vertices may keeping track super edges iteration answer negative worst case far algorithm just described cyclic super edges may form case mad 
policy iteration algorithms consider change value iteration call augmented value iteration iteration cycles visited policy identified highest cycle mean computed 
vertex gets self arc edge reward mean choose self arc subsequent iterations 
algorithm finds optimal mean iterations convergence arguments value iteration bound may tight 
hand algorithm similar called multi chain policy iteration algorithm average reward mdp problems put 
algorithms viewed working phases shown fig 
phase begins mean discovered cycle updates edge rewards vertex values appropriately begins series value iterations cycle 
algorithms differ update values edge choices edge rewards guarantee cycle discovered higher mean 
augmented value iteration algorithm vertex values changed new cycle mean added 
alternatively augmented value iteration may subtract edge rewards keep zero reward self arcs vertex change optimal cycle behavior value iteration lemma 
policy iteration addition subtracting edge rewards vertex redirects cycle mean algorithm finds policy ver path cycle vertices reassigned values follows arbitrary vertex cycle current loss generality may assume graph strongly connected 
algorithm performs component 
policy assigned get total re ward path reassignment values vertices policy iteration appears analysis difficult 
progress shown variants augmented value iteration vertices get reassigned zero values value vector remains constant phases value iteration begins phase terminate polynomial number phases run polynomial time 
just policy iteration algorithms vertex values increase value iteration phase cycles discovered improve phase 
basic behavior seen results vertices behave identically terms edges choose corresponding iterations phase 
exception progressively vertices zero increase value switching edge choices subsequent phase 
results fact vertices value vector example zero phase simplifies comparison phases aids analysis 
relaxing constraints improving bounds algorithms may provide fruitful insights path establishing efficiency policy iteration algorithms 
behavior random graphs algorithms developed require linear number iterations 
suspected random graphs relatively iterations value iteration suffice finding maximum mean 
explored questions random graphs vertex edges vertex edge chosen uniformly random remaining vertices reward edges chosen uniformly random 
tested sparse graphs number actions edges vertex usually small mdp problems linear function leads problems sparse graphs 
averages graphs size obtained samples size maximum fig 
suggests growth expected optimal cycle length number iterations optimal cycle formed policy increase polylogarithmic functions graph size apparently bounded log log respectively 
way compute optimal mean quickly test current visited policy periodically compute average reward cycle policy optimal 
efficient way test subtract candidate expect relatively high average length optimal cycles size graphs due small size graphs 
larger graph sizes limiting distribution random variable kick 
graph size average optimal cycle length time formation graph size karp find history find policy averages optimal cycle lengths iteration optimal cycle 
comparison run times 
mean edge rewards efficient implementation bellman ford shortest paths algorithm detect presence positive cycles amo clr 
positive cycles candidate mean maximum 
shortest path detection algorithm mn run time empirically efficient may linear expected time kb 
fig 
verifies expectation 
shows run times algorithms test periodically log initial value iterations 
tests performed pentium iii megabytes ram small load 
algorithm uses super edges find history simply checks cycles formed policies find policy 
run times algorithms close linear time expected 
plots show find policy version perform better probably due lower overhead find history wait number iterations optimal cycle formed super edge 
algorithms including policy iteration value iteration tested graph families dg conclude policy iteration fastest 
expect algorithms competitive empirically due low overhead 
particular augmented value iteration algorithm sec 
may lower overhead policy iteration need compute path values vertices phase simply continues current vertex values 
compare performance algorithms related problems 
discussion noted value iteration solve problem finding optimal policy polynomial time 
problem may considered borderline problem value iteration polynomial 
shortest path problems negative cycles cycles share single vertex value iteration finds optimal policy polynomial time 
higher problem hierarchy general stochastic mdp problems subclasses degree stochasticity limited takes exponential time converge optimal cycles policies 
closest problem discounted deterministic mdp problem 
problems discount approaches optimal cycle highest average reward cycle put small problem easy approximate 
approximation property may hold runs value iteration starting initial vector sufficient converge approximately optimal cycles discounted deterministic mdps 
motivated analysis policy iteration mdps line developing complete picture efficiency value policy iteration algorithms mdps 
studying simpler problem classes lead techniques algorithm design analysis applicable general problems giving better understanding new ideas needed techniques fail generalize 
hope gaps understanding algorithms efficient various problem classes mdp problems continue filled 
acknowledgments supported part nsf iis performed large part phd author university washington 
author indebted advisors richard anderson steve hanks guidance support research 
ali valuable discussions comments earlier version 
russ greiner anonymous referees suggestions improving presentation 
amo ahuja magnanti orlin 
network flows theory algorithms applications 
prentice hall englewood cliffs nj 
bdh boutilier dean hanks 
decision theoretic planning structural assumptions computational leverage 
jair pages 
clr cormen leiserson rivest 
algorithms 
mit press mcgrawhill book th edition 
cohen 

numerical computation spectral elements max plus algebra 
proc 
ifac conf 
systems structure control pages 
dg gupta 
faster maximum minimum mean cycle algorithms analysis 
ieee transactions computer aided design integrated circuits systems 
gkp guestrin koller parr 
max norm projections factored mdps 
aaai pages 
han hansen 
finite memory control partially observable systems 
phd thesis mass amherst 
ho hartmann orlin 
finding minimum cost time ratio cycles small integral transit times 
networks 
kar karp 
characterization minimum cycle mean digraph 
discrete mathematics 
kb karp orlin 
parametric shortest paths algorithm application cyclic staffing 
discrete applied mathematics 
lit littman 
algorithms sequential decision making 
phd thesis brown 
mad madani 
policy iteration newton method polynomial policy iteration algorithms 
proc 
th national conference artifical intelligence 
appear 
mad madani 
polynomial value iteration algorithms deterministic mdps 
technical report university alberta 
available www cs ualberta ca madani ps 
put puterman 
markov decision processes 
wiley inter science 
rn russell norvig 
artificial intelligence modern approach 
prentice hall 
sha shapley 
stochastic games 
proceedings national academy sciences usa 
tse tseng 
solving horizon stationary markov decision process time proportional log 
operations research letters 
young tarjan orlin 
faster parametric shortest paths minimum balance algorithms 
networks 
zp zwick paterson 
complexity mean payoff games graphs 
theoretical computer science may 
