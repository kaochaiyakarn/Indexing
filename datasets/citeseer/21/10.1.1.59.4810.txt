anisotropic local likelihood approximations theory algorithms applications vladimir katkovnik alessandro foi karen egiazarian jaakko astola institute signal processing tampere university technology fin tampere finland 
consider signal restoration observations corrupted random noise 
local maximum likelihood technique allows deal quite general statistical models signal dependent observations relaxes standard parametric modelling standard maximum likelihood results exible nonparametric regression estimation signal 
deal anisotropy signal multi window directional local polynomial approximation 
data driven sizes windows obtained intersection con dence interval ici algorithm allow form adaptive neighborhoods pointwise estimation 
developed approach quite general applicable multivariable data 
fast adaptive algorithm implementation proposed 
applied photon limited imaging poisson distribution data 
simulation experiments comparison best results eld demonstrate advanced performance developed algorithms 
keywords nonparametric regression local maximum likelihood adaptive window size adaptive scale imaging poisson regression 

continuous models discrete image intensity widely image processing corresponding estimates rough order useful applications sharpness details rst priority 
discrete images lacking global differentiability continuity reliable way obtain accurate restoration process locally 
nonparametric local regression originated mathematical statistics offers original approach signal processing problems 
basically results linear ltering linear lters designed moving window local approximations 
adaptive versions algorithms able produce efficient ltering varying window size scale bandwidth pointwise adaptive see 
pointwise adaptive scale selection idea known approach 
algorithm searches largest local vicinity point estimation estimate ts data 
estimates yh calculated set window sizes scales compared 
adaptive scale de ned largest windows grid estimate differ signi cantly estimators corresponding smaller window sizes 
intersection con dence intervals ici rule versions approach appeared quite efficient adaptive scale image restoration 
cited papers adaptive scale kernel estimation concern scalar scale parameter assume estimators ordered variances 
vector scale parameter kernels dimensional space special interest anisotropic functions highly varying properties different directions 
imaging typical examples problems 
direct generalization approach adaptive smoothing vector scale parameter faces principal obstacle variance estimates calculated different ordered semi ordered estimators similar variance 
main intention new approach anisotropic estimation introduced obtain data driven way largest local vicinity estimation point underlying model ts data 
assumed vicinity set approximated segmentation say sectors 
estimators equipped univariate scale parameters de ning size supports sector 
ici rule exploited times sector order nd optimal pointwise 
neighborhood estimation point best estimation set unit ball segmentation sectors adaptive lengths approximation adaptive scales sector estimates combined nal 
way reduce dimensional scale selection problem multiple univariate 
illustrates concept shows sequentially local best estimation neighborhood segmentation unit ball approximation adaptive scales de ning length corresponding sectors 
varying size sectors enable get approximation neighborhood estimation point provided body 
previously obtained techniques strategy concerned gaussian observation model linear estimators 
main contribution concerns problems 
generalize directional anisotropic image processing broad class non gaussian observation models nonlinear lters local maximum likelihood methods 
second local polynomial approximation technique ici rule modi ed nonlinear estimation problems 
example developments novel adaptive image restoration algorithm poissonian data 

local likelihood 
basic concept observations pairs zs xs regular irregular discrete dimensional grids random conditional probability density function pdf 
parameter pdf depending model random zs depends corresponds xs 
gaussian pdf expectation arrive standard additive observation model invariant standard deviation noise zs 
depending pdf parameter considered models may allow may allow additive noise representation 
representation valid noise general signal dependent 
examples sort imaging problems wish mention impulse noise observations radar remote sensing applications poisson random observations typical medical imaging problems 
independent observations zs xed xs conventional log likelihood de ned log zs xs 
various models methods function order obtain reasonable reconstruction function provided single observation xs 
standard likelihood approach starts global model yc estimation parameters maximum likelihood ml method argmax log zs xs 
estimated 
local maximum likelihood lml nonparametric counterpart widely parametric ml technique 
extends scope parametric ml wider class functions 
local likelihood model longer assume global parametric form yc ts model locally window 
local likelihood local log likelihood window function localization likelihood function neighborhood point interest replace global parametric model yc local 
linear models log likelihoods 
linear models known statistics generalized linear models 
standard linear model assumes quadratic loss function results estimates linear respect observations 
generalized means linear model exploited general type distributions necessarily quadratic loss functions 
generalized linear models result estimates nonlinear respect observations 
polynomial linear model local log likelihood introduced form lh log zs yh xs wh xs yh wh vector polynomials window function stays scalar scale window size parameter 
function yh form called local polynomial approximation lpa ofy point termed centre lpa polynomials centered window wh point 
scale window function rd scaling polynomials rd distinguish localized likelihood lh original non local likelihood uses global parametric model assumed valid local likelihood relaxes assumption assumes linear model valid locally neighborhood estimation point 
idea lpa function estimate yh point calculated follows yh yh yh argmax lh 
means parameter selected maximization log likelihood corresponding function estimate obtained local ml yh follows immediately yh 
known linear estimates degrade signi cantly random observations obey non gaussian distribution 
non gaussian noise linear lpa replaced relevant nonlinear operations 
local maximum likelihood approach introduced generalization linear lpa 
provides exible tools design nonlinear methods algorithms variety stochastic models different standard gaussian 
approach combines different ideas generalized linear model localization parametric tting 
generalized linear models parameters distributions random observations assumed varying linear regression varying parameters 
ml approach estimate parameters 
localization introduced order relax restrictions imposed parametric modeling standard ml 
follows restrict attention case directionality transparent way 
generalization higher number dimensions straightforward 

polynomials introduce polynomials form xk powers different 
polynomials linearly independent total number polynomials maximum power equal 
multi index elements showing powers corresponding variables 
lpa estimates de ned model vector composed polynomials multi index explicitely de nes lpa model 
windows nite support window key tool design directional estimate 
call window directional narrow oriented axis pointed directions axis 
sectors clear examples directional supports 
consider rotated support window angle main direction 
axis bound rotated sector 
axis points direction orthogonal 
linked rotation transform equation cos sin sin cos 
rotated window de ned 
scaled rotated window polynomials lpa powers respect variables 
directional estimates observation sector de ned directional local log likelihood lh log zs yh xs wh xs yh wh wh yh argmax lh 
window satisfy standard lpa assumptions 
particular origin corresponds maximum value directionality estimate guaranteed support shape window function 
requirement restricting window scale selection likelihood lh convex maximization lh unique solution 
window scale called condition holds 
zero order model nonempty window wh admissible 

particular distributions models observations considered allow cover lot practical scenarios noise variance signal dependent 
general terms models allow take consideration delicate case distribution signal dependent 
order simply notation assume drop index estimates distributions 
order obtain estimate direction rotation transform 

additive noise observation model changed global yc local zs yh xs yh xs xs random probability density deterministic part yh xs model local depending local model valid single xed remind lpa argument treated xed parameter model 
local observation model depends scale parameter probability density yc set observations local log likelihood forms lh ln es wh xs es zs yh xs 
denote ln 
ln es es local log likelihood estimates problem argmin rm es wh xs es zs yh xs 
loss function residuals 
way derive local nonparametric lpa robust estimates loss function de ned probability density link ml guarantees estimates properties ml provided loss function noise probability density agree 

gaussian noise signal dependent variance observation noise gaussian variance depending omitting invariant part ln ln ln ne loss function weighted mean squared residual criterion yh xs wh xs weighted lsm ml estimate form es zs yh xs 
argmin rm yh xs wh xs 
note yh xs estimate signal calculation variance 

gaussian noise constant variance constant gives standard linear lpa estimate argmin rm xs 
case analytical solution wh xs xs zs wh xs xs xs 
inserting obtain estimate kernel form yh gh xs zs gh wh 
unrestricted regular grid observation points xs matrix constant kernel gh shift invariant 
estimate calculated convolution yh gh zs 
zero order lpa yh xs gh wh wh xs 
estimate weighted sample mean known nadaraya watson nonparametric regression estimate 

laplacian noise observation noise additive model independent laplacian exp 
ln ln loss function de ned absolute value residual criterion wh xs es es zs yh xs 
way derive estimates loss function ml estimate 
absolute error loss function means estimate weighted median 

poisson observations imaging systems recorded observations physical meaning numbers detected photons 
photons counted different spatial locations way form image object 
poisson distribution conventional probabilistic model random number photons detected exposure time 
number photons relatively small level noise high problem referred photon limited signal processing xed observed random discrete poisson distribution var stays intensity mean value poisson distribution 
emphasized nonnegative mean value integer counts 
model argument essential var see variance observation signal dependent 
problem reconstruct function observations zs xs 
random variable nonnegative integer 
corresponding likelihood observations zs xs de ned se xs xs zs log likelihood ln xs zs ln xs ln zs 
local version log likelihood lh wh xs yh xs zs ln yh xs ln zs yh yh 

zero order model zero order model special interest local estimate yh xs constant lh zs ln wh xs arg max lh yh 
equation clh yh gh xs zs gh wh wh xs 
arrive nadaraya watson linear nonparametric estimate 

higher order model higher order model principal problems 
parameters maximizing log likelihood arg max lh yh gives estimate yh 
ways overcome problem 
statistics canonical variable recommended lpa exploited new variable estimate inversion log function yh exp local ml estimate 
estimate non negative yh 
way tradition engineering signal processing project estimate half line 

adaptive scale selection properties introduced nonlinear nonparametric regression estimates quite similar known linear counterparts 
go details general features similarity noted 
firstly estimated polynomials powers lpa ml estimates asymptotically unbiased 
asymptotic mainly assumes number observations scale parameter appropriate way details seen 
ensures convergence polynomial smoothness estimates 
secondly bias variance estimate asymptotically form cha dh respectively 
bias small small large large order bias ha depends smoothness estimated regression order lpa 
variance large small scales small large ones 
obtain mean squared error estimation expression ha dh mean squared error concave optimal scale equation hl 
optimal scale gives optimal variance bias trade typical nonparametric estimation 
optimal value called ideal calculation requires accurate knowledge derivatives 
information available practical scenario 
similarity sort accuracy results general ml estimates results known linear estimates allows conclude intersection con dence ici rule applicable ml estimates gives adaptive scale estimates close ideal ones 
remind ici rule technique 
nite set ordered scales hj corresponding varying scale kernel estimates decreasing standard deviations yh yh determine sequence con dence intervals dj yh yh threshold parameter 
ici rule stated follows consider con dence intervals ij tj di largest indexes ij non empty ij ij 
optimal scale de ned hj optimal scale kernel estimate 
procedure xed produced way obtain varying adaptive scale 
procedure requires know estimates different scales corresponding variances estimates 

anisotropic estimator directional optimizations basic strategy deal anisotropic follows 
calculate collection yh varying scale directional estimates yh gh order nd best scale direction point gives pointwise adaptive scale directional estimates yh 
sectors adaptive scale estimates adaptive estimates fused combined nal 
nal estimate convex combination adaptive directional estimates inverse variances weights xk xk variance adaptive estimate xs zs gh wh wh xs 
note weights data driven adaptive depend adaptive point wise 
recursive lpa ici adaptive variance av algorithm algorithm developed poisson distributed data assumes observation variance 
specifying dependence variance signal algorithm signal dependent variance problems 

lpa ici adaptive variance lter ici lter narrow conical supports estimation kernels 
linear lpa estimates zero order calculated scales directions yh gh 
formula yh xs xs 
xs estimate signal 
xs xs 
steps recursive lpa ici adaptive variance algorithm algorithm steps different input data methods calculation variances input data parameters algorithms 
step initialization input data observations variances observations ici fusing simplest initial guess observations variance 
experiments show choice works quite 
particular better constant variance assumption 
output data estimates superscript shows step number 
step input data original observations zs 
variances observations ici fusing 
fact ned variance observations 
steps correspond lpa ici algorithm iteration variance update 
outputs estimates 
ltered estimate longer poissonian variance equal pointwise mean value observation 
step input data estimates original data 
variances required ici calculated gh gh empirical formula try take consideration correlation estimates independent ltering previous step directional kernels origin pixel common point 
note variance observation calculated latest estimate signal 
outputs estimates 
step identical step input signal obtained previous steps 
comment particular form recursive ltering exploited third step onwards re ned poissonian version recursive lpa ici ltering gaussian distributed observations 

numerical experiments 
optimization algorithm done order optimize design parameters algorithm 
optimization algorithm parameter values multiple experiments part follows 
directional kernels gh de ned linear combination zero rst order kernels gh gh gh 
gh gh directional lpa kernels designed set window functions wh constant support 
scale parameters ne length support taken set 
ici rule applied selection length kernel gh 
signi cant improvement achieved rst steps described 
maximum number steps restricted 
parameter combined kernel gh taken different values different steps algorithm starting zero order consistent considerations section increasing importance rst order component algorithm progress 
performance bene ts observations single step smoothing provided subsequent iterations 
selected threshold parameter ici small 
thesame steps 

poisson observations simulations poissonian case order achieve different level randomness different snr noisy observations rst multiply true signal range scaling factor 
var andy std better snr corresponds larger 
modelling poisson data allows produce comparison similar simulation scenarios appeared number publications 
comparison wavelet methods developed poisson data demonstrating performance 
visualise data divide back factor expected range signal intensity 
illustrates effect scaling factor modelling poisson observations 
comparing images gure lower 
cross section note level random disturbance clearly signal dependent 
large value signal means larger level noise 

simulation results images shown figures show noisy original images estimates obtained rst initialization fourth iteration 
mse values demonstrate fast performance improvement successive iterations 
quality nal estimates quite visually numerically 
particular cameraman achieve db db db db 
numerical results comparison methods cameraman lena images table 
results table values mse mean value table includes extends results shown 
comparing mse values obtained successive steps note main improvement achieved rst steps 
starting second step recursive procedure lpa ici shows superior results essentially better methods 
true image noisy image noisy image cross section 
cameraman fragment true poisson noisy images 

filtering fragment cameraman image left right noisy data mse estimate rst iteration lpa ici av algorithm mse db estimate fourth iteration mse db original image 
table 
mse cameraman lena images different algorithms different levels noise noisy image tn method improved tn method lpa ici st step lpa ici nd step lpa ici rd step lpa ici th step cameraman 
acknowledgments lena rst author supported part visiting fellow nokia foundation 

fan gijbels local polynomial modelling application 
london chapman hall 

foi katkovnik egiazarian astola novel anisotropic local polynomial estimator directional multiscale optimizations proc 
th ima int 
conf 
math 
signal processing uk pp 


katkovnik new method varying adaptive bandwidth selection ieee trans 
signal proc vol 
pp 


filtering lena image top row left noisy data mse snr db top row right rst iteration lpa ici av algorithm mse db second row left fourth iteration mse db second row right original image 

katkovnik multiresolution kernel nonparametric regression new approach pointwise spatial adaptation discrete signal processing print 

katkovnik egiazarian astola adaptive window size image de noising intersection con dence intervals ici rule 
math 
imaging vision vol 
pp 


katkovnik egiazarian astola 
adaptive varying scale methods image processing 
tampere international center signal processing series tampere tty 

katkovnik foi egiazarian astola directional varying scale approximations anisotropic signal processing eusipco wien 

optimal spatial adaptation inhomogeneous smoothness approach kernel estimates variable bandwidth selection 
annals statistics vol 

loader local regression likelihood series statistics computing springer verlag new york 

nemirovski spatial adaptive estimation nonparametric regression math 
meth 
statistics vol 
pp 


huber robust statistics 

astola fundamentals nonlinear digital filtering newyork 

nowak multiscale modeling estimation poisson processes application photon limited imaging ieee trans 
information theory vol 
pp 


lu kim anderson improved poisson intensity estimation application poisson data ieee trans 
image processing vol 
pp 

hastie tibshirani 
generalized linear models 
london chapman hall 

tibshirani hastie local likelihood estimation american statistical association vol 
pp 


nowak baraniuk wavelet domain ltering photon imaging systems ieee trans 
image processing vol 
pp 

willett nowak platelets multiscale approach recovering edges surfaces photon limited medical ieee trans medical imaging vol 
pp 

