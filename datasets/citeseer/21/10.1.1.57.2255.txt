appears advances neural information processing systems vol 

mit press cambridge ma 
extracting tree structured representations trained networks mark craven jude shavlik computer sciences department university wisconsin madison west dayton st madison wi craven cs wisc edu shavlik cs wisc edu significant limitation neural networks representations learn usually incomprehensible humans 
novel algorithm trepan extracting comprehensible symbolic representations trained neural networks 
algorithm uses queries induce decision tree approximates concept represented network 
experiments demonstrate trepan able produce decision trees maintain high level fidelity respective networks comprehensible accurate 
previous area algorithm general applicability scales large networks problems high dimensional input spaces 
learning tasks important produce classifiers highly accurate easily understood humans 
neural networks limited respect usually difficult interpret training 
contrast neural networks solutions formed symbolic learning systems quinlan usually amenable human comprehension 
novel algorithm trepan extracting comprehensible symbolic representations trained neural networks 
trepan queries network induce decision tree describes concept represented network 
evaluate algorithm real world problem domains results demonstrate trepan able produce decision trees accurate comprehensible maintain high level fidelity networks extracted 
previous area algorithm general applicability scales large networks problems high dimensional input spaces 
task address defined follows trained network data trained produce concept description comprehensible classifies instances way network 
concept description produced algorithm decision tree generated popular decision tree induction algorithms breiman quinlan 
reasons comprehensibility induced concept descriptions important consideration 
designers users learning system confident performance system understand arrives decisions 
learning systems may play important role process scientific discovery 
system may discover salient features relationships input data importance previously recognized 
representations formed learner comprehensible discoveries accessible human review 
problems comprehensibility important neural networks provide better generalization common symbolic learning algorithms 
domains important able extract comprehensible concept descriptions trained networks 
extracting decision trees approach views task extracting comprehensible concept description trained network inductive learning problem 
learning task target concept function represented network concept description produced learning algorithm decision tree approximates network 
inductive learning problems available oracle able answer queries learning process 
target function simply concept represented network oracle uses network answer queries 
advantage learning queries opposed ordinary training examples garner information precisely needed learning process 
algorithm shown table similar conventional decision tree algorithms cart breiman quinlan learn directly training set 
trepan substantially different conventional algorithms number respects detail 
oracle 
role oracle determine class predicted network instance query 
queries oracle complete instances specify constraints values features take 
case oracle generates complete instance randomly selecting values feature ensuring constraints satisfied 
order generate random values trepan uses training data model feature marginal distribution 
trepan uses frequency counts model distributions discrete valued features kernel density estimation method silverman model continuous features 
shown table oracle different purposes determine class labels network training examples ii select splits tree internal nodes iii determine node covers instances class 
aspects algorithm discussed detail 
tree expansion 
decision tree algorithms grow trees depth manner trepan grows trees best expansion 
notion table trepan algorithm 
trepan training examples features queue sorted queue nodes expand example training examples net label examples class label oracle initialize root tree leaf node put ht training examples fg queue queue empty size tree size limit expand node remove node head queue example set stored constraint set stored features build set candidate splits calls oracle evaluate splits best binary split search best split seed internal node split outcome children nodes new child node fs sg calls oracle determine remain leaf members outcome split put hc queue return best node case greatest potential increase fidelity extracted tree network 
function evaluate node reach theta gamma fidelity reach estimated fraction instances reach passed tree fidelity estimated fidelity tree network instances 
split types 
role internal nodes decision tree partition input space order increase separation instances different classes 
splits single feature 
algorithm murphy pazzani id algorithm forms trees expressions splits 
expression boolean expression specified integer threshold set boolean conditions 
expression satisfied conditions satisfied 
example suppose boolean features expression fa cg logically equivalent 
split selection 
split selection involves deciding partition input space internal node tree 
limitation conventional tree induction algorithms amount training data select splits decreases depth tree 
splits near bottom tree poorly chosen decisions training examples 
contrast trepan oracle available able instances desired select split 
trepan chooses split considering smin instances smin parameter algorithm 
selecting split node oracle list previously selected splits lie path root tree node 
splits serve constraints feature values instance generated oracle take example satisfy constraints order reach node 
id algorithm trepan uses hill climbing search process construct splits 
search process begins selecting best binary split current node trepan uses gain ratio criterion quinlan evaluate candidate splits 
valued features binary split separates examples values feature 
discrete features values consider binary splits allowable value feature color red color blue 
continuous features consider binary splits thresholds manner 
selected binary split serves seed search process 
greedy search uses gain ratio measure heuristic evaluation function uses operators murphy pazzani ffl add new value set hold threshold constant 
example fa bg fa cg 
ffl add new value set increment threshold 
example fa cg fa dg 
id trepan constrains splits feature disjunctive splits lie path root leaf tree 
restriction oracle solve difficult satisfiability problems order create instances nodes path 
stopping criteria 
trepan uses separate criteria decide growing extracted decision tree 
node leaf tree high probability node covers instances single class 
decision trepan determines proportion examples fall common class node calculates confidence interval proportion hogg 
oracle queried additional examples prob gamma ffl ffi ffl ffi parameters algorithm 
trepan accepts parameter specifies limit number internal nodes extracted tree 
parameter control comprehensibility extracted trees domains may require large trees describe networks high level fidelity 
empirical evaluation experiments interested evaluating trees extracted algorithm criteria predictive accuracy ii comprehensibility fidelity networks extracted 
evaluate trepan real world domains congressional voting data set features examples cleveland heart disease data set features examples uc irvine database promoter data set features examples complex superset uc irvine data set task recognize protein coding regions dna features examples craven shavlik 
remove physician fee freeze feature voting data set problem difficult 
conduct experiments fold cross validation methodology domain 
certain domain specific characteristics data set fold cross validation experiments 
measure accuracy fidelity examples test sets 
accuracy defined percentage test set examples correctly classified fidelity defined percentage test set examples classification table test set accuracy fidelity 
domain accuracy fidelity networks id trepan trepan heart promoters protein coding voting tree agrees neural network counterpart 
comprehensibility decision tree problematic measure measure syntactic complexity trees take representative comprehensibility 
specifically measure complexity tree ways number internal non leaf nodes tree ii number symbols splits tree 
count ordinary single feature split symbol 
count split symbols split lists feature values 
neural networks experiments single layer hidden units 
number hidden units network chosen cross validation network training set validation set decide training networks 
trepan applied saved network 
parameters trepan set follows runs instances training examples plus queries considered selecting split set ffl ffi parameters stopping criterion procedure maximum tree size set internal nodes size complete binary tree depth 
baselines comparison run quinlan algorithm murphy pazzani id algorithm testbeds 
recall id similar learns trees splits 
pruning method algorithms cross validation select pruning levels training set 
cross validation runs evaluate unpruned trees trees pruned confidence levels ranging 
table shows test set accuracy results experiments 
seen data set neural networks generalize better decision trees learned id 
decision trees extracted networks trepan accurate id trees domains 
differences accuracy neural networks conventional decision tree algorithms id statistically significant domains level paired tailed test 
test significance accuracy differences trepan decision tree algorithms 
promoter domain differences statistically significant 
results table indicate range interesting tasks algorithm able extract decision trees accurate decision trees induced strictly training data 
table shows test set fidelity measurements trepan trees 
results indicate trees extracted trepan provide close approximations respective neural networks 
table shows tree complexity measurements id trepan 
data sets trees learned trepan fewer internal nodes trees produced id 
cases trees produced trepan id symbols splits table tree complexity 
domain internal nodes symbols id trepan id trepan heart promoters protein coding voting complex 
data sets trepan trees trees comparable terms symbol complexity 
data sets id trees complex trepan trees 
results argue trees extracted trepan comprehensible trees learned conventional decision tree algorithms 
discussion previous section evaluated algorithm dimensions fidelity syntactic complexity accuracy 
advantage approach generality 
numerous extraction methods hayashi mcmillan craven shavlik sethi tan ganascia alexander mozer setiono liu trepan algorithm place requirements architecture network training method 
trepan simply uses network black box answer queries extraction process 
fact trepan extract decision trees types opaque learning systems nearest neighbor classifiers 
existing algorithms require special network architectures training procedures saito nakano fu gallant 
algorithms assume hidden unit network accurately approximated threshold unit 
additionally algorithms extract rules extract conjunctive rules 
previous craven shavlik towell shavlik shown type algorithm produces rule sets typically far complex comprehensible 
thrun developed general method rule extraction described algorithm verify rule consistent network developed rule searching method able find concise rule sets 
strength algorithm contrast scalability 
demonstrated algorithm able produce succinct decision tree descriptions large networks domains large input spaces 
summary significant limitation neural networks concept representations usually amenable human understanding 
algorithm able produce comprehensible descriptions trained networks extracting decision trees accurately describe networks concept representations 
believe algorithm takes advantage fact trained network queried represents promising advance goal general methods understanding solutions encoded trained networks 
research partially supported onr 
alexander mozer 

template algorithms connectionist rule extraction 
tesauro touretzky leen editors advances neural information processing systems volume 
mit press 
breiman friedman olshen stone 

classification regression trees 
wadsworth brooks monterey ca 
craven shavlik 

learning symbolic rules artificial neural networks 
proc 
th international conference machine learning pp 
amherst ma 
morgan kaufmann 
craven shavlik 

learning predict reading frames coli dna sequences 
proc 
th hawaii international conference system sciences pp 
hi 
ieee press 
craven shavlik 

sampling queries extract rules trained neural networks 
proc 
th international conference machine learning pp 
new brunswick nj 
morgan kaufmann 
fu 

rule learning searching adapted nets 
proc 
th national conference artificial intelligence pp 
anaheim ca 
aaai mit press 
gallant 

neural network learning expert systems 
mit press 
hayashi 

neural expert system automated extraction fuzzy ifthen rules 
lippmann moody touretzky editors advances neural information processing systems volume 
morgan kaufmann san mateo ca 
hogg 

probability statistical inference 
macmillan 
mcmillan mozer smolensky 

rule induction integrated symbolic subsymbolic processing 
moody hanson lippmann editors advances neural information processing systems volume 
morgan kaufmann 
murphy pazzani 

id constructive induction concepts discriminators decision trees 
proc 
th international machine learning workshop pp 
evanston il 
morgan kaufmann 
quinlan 

programs machine learning 
morgan kaufmann 
saito nakano 

medical diagnostic expert system pdp model 
proc 
ieee international conference neural networks pp 
san diego ca 
ieee press 
sethi yoo 

extraction diagnostic rules neural networks 
proc 
th ieee symposium computer medical systems pp 
ann arbor mi 
ieee press 
setiono liu 

understanding neural networks rule extraction 
proc 
th international joint conference artificial intelligence pp 
montreal canada 
silverman 

density estimation statistics data analysis 
chapman hall 
tan 

rule learning extraction self organizing neural networks 
proc 
connectionist models summer school 
erlbaum 
ganascia 

bayesian framework integrate symbolic neural learning 
proc 
th international conference machine learning pp 
new brunswick nj 
morgan kaufmann 
thrun 

extracting rules artificial neural networks distributed representations 
tesauro touretzky leen editors advances neural information processing systems volume 
mit press 
towell shavlik 

extracting refined rules knowledge neural networks 
machine learning 
