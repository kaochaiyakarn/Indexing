error correcting output coding corrects bias variance eun bae kong department computer engineering national university south korea ac kr thomas dietterich department computer science oregon state university corvallis tgd cs orst edu previous research shown technique called error correcting output coding ecoc dramatically improve classification accuracy supervised learning algorithms learn classify data points ae classes 
presents investigation ecoc technique works particularly employed decision tree learning algorithms 
shows ecoc method form voting committee reduce variance learning algorithm 
furthermore methods simply combine multiple runs learning algorithm ecoc correct errors caused bias learning algorithm 
experiments show bias correction ability relies non local behavior 
error correcting output coding ecoc method applying binary class learning algorithms solve class supervised learning problems 
works converting class supervised learning problem large number class supervised learning problems 
learning algorithm handle class problems decision tree algorithm quinlan applied learn problems 
classify new test example learned decision trees evaluated 
ecoc method tells combine results evaluations predict class test example 
previous experimental research bakiri dietterich bakiri wettschereck dietterich dietterich bakiri shown error correcting output coding uniformly improves classification accuracy decision tree neural network classifiers compared standard approaches class learning problems :10.1.1.72.7289
goal explain ecoc method works 
shows ecoc strategy viewed compact form voting multiple hypotheses 
key success voting errors committed learned binary functions substantially uncorrelated 
explain ecoc strategy works explain learned binary functions uncorrelated errors 
understand causes uncorrelated errors appeal statistical notions bias variance 
regression squared error partitioned squared bias term variance term 
bias term describes component error results systematic errors learning algorithm 
variance term describes component error results random variation noise training sample random behavior learning algorithm 
develop analogous definitions bias variance classification problems show bias variance measured experimentally 
measuring bias variance identify causes uncorrelated errors 
breiman shown decision tree algorithms cart high variance hypotheses produced algorithms change substantially small changes training set 
second show source uncorrelated errors ecoc strategy bias errors problems substantially different 
bias errors different problems creates different geometrical arrangements decision boundaries leads different bias errors 
near independence errors various learning problems results combination variance varia tion bias due variation problems 
explosion papers showing classification performance learning algorithms significantly improved generating multiple hypotheses result different random seeds neural network learning different training sets decision tree learning voting hypotheses hansen salamon leblanc tibshirani perrone perrone cooper perrone meir breiman 
call homogeneous voting multiple runs algorithm learning problem combined voting 
argue definition bias homogeneous voting reduce variance bias 
papers reported improved performance non homogeneous voting voting multiple hypotheses constructed different learning algorithms applied problem bates granger winkler clemen schapire hampshire ii waibel zhang mesirov waltz cardie quinlan 
nonhomogeneous voting reduce bias variance bias errors various algorithms different 
error correcting output coding involves homogeneous non homogeneous voting 
homogeneous voting learning algorithm applied times 
applications algorithm solves different learning problem obtain bias reduction benefits non homogeneous voting 
remainder structured follows 
describe error correcting output coding method summarize previous results 
second define bias variance show measure 
third discuss conditions voting reduce error ecoc approach kind voting 
fourth show ecoc method reduces bias variance methods bootstrap aggregating breiman reduce variance 
report experiments show differences bias errors various problems depend non local behavior 
understanding reasons ecoc success predict succeed learning algorithms 
definitions previous goal supervised learning classification learn classification function takes description input object classifies classes fc gamma learn classification function learning algorithm analyzes set training examples xm xm training example pair consisting description object correct classification 
represented vector feature values describe properties refer feature values describing example 
view vector feature values point dimensional feature space 
learning algorithms quinlan cart breiman friedman olshen stone solve way classification problems directly 
learning algorithms designed solve binary class classification problems 
error correcting output coding ecoc technique studied techniques converting way classification problem set binary classification problems 
works follows 
gamma distinct binary strings length chosen hamming distance pair strings large possible 
hamming distance binary strings number bit positions strings differ 
call string codeword class table shows example set binary strings 
hamming distance pair strings bits 
define binary classification functions fl th bit 

correspond columns table 
learning functions learned re coding examples xm xm applying class learning algorithm learn result set hypotheses flg 
classify new example apply learned functions compute vector binary decisions fl determine codeword nearest vector hamming distance 
predicted value class corresponding nearest codeword example suppose predicted outputs 
table shows hamming distances string predictions rows table 
string class smallest hamming distance predict advantage scheme codewords fs constitute error correcting code 
table bit error correcting output code class problem code word class table hamming distances string predicted bits codewords table class hamming distance minimum hamming distance pair codewords gamma errors individual corrected nearest codeword correct codeword 
contrast standard approach converting way classification problem set binary classification problems define function class zero see nilsson 
call class opc method 
learning set hypotheses learned 
classify new example compute value predicted value class maximized 
approach works best learning algorithms produce probability activation output 
step mapping nearest codeword extended handle learning algorithms produce activations probabilities simple classifications 
place hamming distance sum absolute value difference component corresponding component codeword 
method experiments 
dietterich bakiri shown multiclass glass vowel pos soybean audiology isolet letter nettalk performance relative multiclass class ecoc performance class ecoc methods relative direct multiclass method 
asterisk indicates difference significant level better 
glass vowel soybean audiology standardized encoding isolet letter recognition nettalk data sets irvine repository murphy aha 
pos task predict part speech unknown words claire cardie personal communication context 
error correcting output coding technique works decision tree algorithm 
compares performance domains 
configurations compared multiclass single decision tree constructed way classification class decision trees constructed error correcting output coding decision trees constructed 
dark bars show performance percent correct class approach light bars show performance longest error correcting code tested 
performance plotted number percentage points algorithm differs multiclass approach 
asterisk indicates difference statistically significant level binomial test difference proportions 
see class method performs significantly worse multiclass method domains statistically indistinguishable remaining domains 
important observation error correcting output code approach significantly superior multiclass approach domains indistinguishable remaining 
results quite exciting explain ecoc method works 
goal 
decomposing error rate bias variance components consider regression algorithm applied set training examples produce hypothesis fs suppose draw sequence training sets size apply construct hypotheses fs expected error point defined average squared error average taken training sets limit large error lim gamma ideal voted hypothesis defined lim bias algorithm defined error ideal voted hypothesis bias gamma variance algorithm expected value squared difference ideal voted hypothesis individual hypothesis ar lim gamma known theorem regression states error bias ar words expected squared error algorithm test data point trained sample size equal sum squared error ideal voted hypothesis variance individual hypothesis respect ideal voted hypothesis 
extending notions bias variance classification problems straightforward alternative approaches possible efron 
approach previously seen idealized voting 
classification learning algorithm hypothesis produced trained training set test set example equal possible classes fc gamma define probability probability taken possible random training sets size average error rate algorithm written error gamma correct class average error rate test point just minus probability correctly classified 
ideal voted hypothesis chooses class highest probability class votes individual argmax define bias error ideal voted hypothesis bias ae words bias particular data point ideal voted hypothesis classifies correctly error 
define variance difference expected error rate ideal voted hypothesis error rate ar error gamma bias unintuitive property variance particular test set point negative 
occurs learning algorithm usually misclassifies point incorrectly occasionally gets right 
case average error reduced occasional lucky correct classifications 
analogous situation regression bias may high time time regression algorithm may correctly predict value 
regression uses squared error variance positive 
test set examples expected error rate error 
component due bias bias component due variance ar artificial simulated learning problems measure bias variance directly simulating definitions 
start defining target function input space probability distribution 
draw series training sets size sampling labeling example draw large test set possible consisting 
goal estimate test set 
training training sets computing test example proportion times classified classes 
compute expected error rate bias variance directly definitions 
followed slightly different procedure gave better results experimentally 
producing simple classification test example produce class probability vector gives probability example belongs classes 
sum class probability vectors component wise hypotheses constructed series training sets obtain voted class probability vector normalize probabilities sum 
ideal voted hypothesis classifies test example class voted probability maximum 
computations release pruning windowing simple class learning problem shown 
constructed training sets size sampling uniformly replacement region theta 
test set containing examples focused primarily decision boundaries constructed 
test set mean error rate replications 
bias component variance component 
shows bias errors 
breiman shown error rate algorithms significantly reduced voting multiple bootstrap runs 
measured bias variance breiman procedure voting bootstrap replicates multiclass training sets 
reduced mean error 
bootstrap voting increases bias component decreases variance component 
see bootstrap aggregation reduces variance bias 
sense bootstrap aggregation viewed way approximating ideal voting algorithm 
ideal voting training set drawn random entire input space bootstrap replicate reuse randomly selected subset training set extent bootstrap aggregation approximates ideal voting error rate bootstrap aggregation approach error rate ideal voting definition bias 
bootstrap aggregation reduce bias reduce variance 
true homogeneous voting scheme algorithm applied multiple times data set 
multiple votes produce better approximations ideal probabilities change true values 
homogeneous voting reduce variance bias 
ecoc voting apply error correcting output coding method problem constructed bit error correcting code exhaustive method described dietterich bakiri :10.1.1.72.7289
understand error correcting code graphically considering shows decision boundaries 
decision boundary segment unique label boundaries separating class class 
apply multiclass problem forced learn decision boundaries simultaneously 
contrast ecoc method important properties 
individual binary function learn decision boundaries set decision boundaries varies 
second boundary learned times 
third predicted binary string decoded error correcting code procedure mapping nearest codeword equivalent vote functions learned relevant boundaries 
expand observations 
see individual binary function learns decision boundaries consider function function labels examples classes 
learn boundaries labeled 
function hand labels examples classes 
learn boundaries 
note boundaries learned turns boundary learned exactly times bit code 
bit code codeword hamming distance away codeword 
means point cross learned boundaries move class functions change classifications 
explains decoding step form voting 
suppose learned misclassify test point learned correctly classified placed correct side decision boundaries correctly classified 
nearest codeword ham class class class class class class class class problem training examples 
class class class class class class class class bias errors problem estimated replications 
class class class class class class class decision boundaries learning problem 
boundary unique name 
superimposed decision boundaries learned functions example learning problem 
ming distance corresponds class territory reached crossing fewest number learned boundaries changing fewest number bits 
learned boundaries vote classify shows boundaries learned functions bit error correcting output code remaining functions omitted improve readability 
see boundaries learned approximately times various learned boundaries identical 
point misclassified learned boundaries classified correctly decoded 
decision boundary perspective errorcorrecting output coding explains class approach works poorly 
opc method boundary learned twice 
example boundary learned attempt discriminate class classes learned try discriminate class classes 
hypotheses boundary participating vote way recover errors 
established previous section voting improves performance errors various voters highly correlated 
observation holds ecoc 
errors various uncorrelated 
bad votes near decision boundary points misclassified 
discussion see effectiveness ecoc depends errors different bit positions code relatively uncorrelated 
bias errors bit ecoc configuration 
ecoc reduces variance bias measured bias variance ecoc approach problem 
ecoc mean error reduced 
virtually reduction obtained bootstrap aggregating bias increased variance decreased 
case improvement results reductions bias variance 
shows ecoc reduce bias variance 
variance reductions expected ecoc form voting 
reduction bias interesting 
bias errors ecoc plotted 
compared see bias reduced 
measured plotted bias errors individual 
shows bias errors see bias errors quite different decision boundaries 
corrected error correcting code 
variance reduction ecoc large comparison variance reduction obtained performing breiman replicate bootstrap 
explanation problem voting decision trees boundary may reduce variance voting trees 
suggests composite strategy construct performing fold bootstrap vote 
results separate elections combined decoding step ecoc classify test example 
measurements confirmed prediction mean error rate reduced bias variance 
ecoc strategy fruitfully combined breiman bootstrap voting achieve better improvements performance 
bias differences caused non local behavior fact observe uncorrelated errors leaves open question errors uncorrelated 
examination plots suggests different bias errors non local interactions training examples 
consider example bias errors bits region 
bias errors result decision approximate boundary horizontal line 
places line near chops pointed regions class meets class 
bias errors bits learn boundaries including 
chooses vertical splits bias shifts dramatically 
short places splits global heuristics presence different distant boundaries different leads different bias errors 
test hypothesis implemented version uses local information place splits 
specifically local works nearest neighbor algorithm test example nearest neighbors identified construct decision tree 
tree applied classify discarded 
local trees local information predict bias errors different correlated 
eliminate ability ecoc correct bias errors 
shows bias reduction obtained applying local algorithm multiclass ecoc configurations replicates measure bias mean error 
see fewer neighbors bias rate ecoc bias rate multiclass bias reduction error correction 
provide neighbors ecoc clearly correcting bias errors improving classification accuracy 
conclude lack correlation bias errors individual functions results geometry decision boundaries 
vary learning problem difficult general statement degree bias errors uncorrelated 
observed experimental superiority ecoc suggests suitable decision boundary geometry arises quite practice 
bias errors bit bias errors bit bias errors bit bias errors bit bias errors ecoc bit functions measured replications problem ecoc bias reduction number neighbors reduction bias produced bit ecoc compared multiclass local version constructs ad hoc tree classify test example nearest neighbors 
note log scale horizontal axis 
discussion shown error correcting output code approach multiclass problems kind voting 
homogeneous voting ecoc reduce bias errors variance errors bias errors individual voting functions somewhat uncorrelated 
experiments previous section showed bias errors somewhat uncorrelated simulated problem results nonlocal interactions decision boundaries 
experiments predict algorithms non local behavior feedforward sigmoidal networks perceptrons benefit error correcting output coding 
algorithms nearest neighbor radial basis functions highly local benefit error correction 
prediction borne experiments reported wettschereck dietterich ecoc gave small improvement generalized radial basis function algorithm 
error correcting output code approach imposes distributed output representation learning algorithm 
results study may help explain distributed representations 
important open problem research find ways converting ecoc hypotheses collections voted hypotheses representations smaller efficient evaluate 
applications cost storing evaluating large number decision trees neural networks infeasible apply ecoc approach 
obtain advantages voting having hold elections 
authors leo breiman rob tibshirani helpful pointers 
authors gratefully acknowledge support nsf iri cda 
bakiri 

converting english text speech machine learning approach 
tech 
rep oregon state university corvallis 
bates granger 

combination forecasts 
op 
res 

breiman friedman olshen stone 

classification regression trees 
wadsworth 
breiman 

bagging predictors 
tech 
rep dept statistics univ cal berkeley ca 
cardie 

decision trees improve case learning 
proc 
th intl 
conf 
mach 
learn pp 
san francisco ca 
morgan kaufmann 
clemen 

combining review annotated bibliography 
int 
forecasting 
dietterich bakiri 

errorcorrecting output codes general method improving multiclass inductive learning programs 
proc 
aaai pp 

aaai press mit press 
dietterich bakiri 

solving multiclass learning problems error correcting output codes 
art 
int 
res 
efron 

regression anova data measures residual variation 
am 
stat 

hampshire ii waibel 

novel objective function improved phoneme recognition time delay neural networks 
ieee trans 
neural networks 
hansen salamon 

neural network ensembles 
ieee pami 
leblanc tibshirani 

combining estimates regression classification 
tech 
rep dept statistics univ toronto 
winkler 

averages forecasts empirical results 
management sci 
meir 

bias variance combination estimators case linear squares 
tech 
rep technion israel 
murphy aha 

uci repository machine learning databases machine readable data repository 
univ cal irvine 
nilsson 

learning machines 
mcgrawhill new york 
perrone 

improving regression estimation averaging methods variance reduction extensions general convex measure optimization 
ph thesis brown univ perrone 

putting methods combining neural networks 
cowan tesauro alspector 
eds advances neural information processing systems 
morgan kaufmann san francisco ca 
perrone cooper 

networks disagree ensemble methods hybrid neural networks 

ed neural networks speech image processing 
chapman hall 
quinlan 

programs empirical learning 
morgan kaufmann san francisco ca 
quinlan 

combining instance model learning 
utgoff 
ed proc 
int 
conf 
mach 
learn 
san francisco ca 
morgan kaufmann 
schapire 

strength weak learnability 
mach 
learn 
wettschereck dietterich 

improving performance radial basis function networks learning center locations 
moody hanson lippmann 
eds advances neural information processing systems 
morgan kaufmann san francisco ca 
zhang mesirov waltz 

hybrid system protein secondary structure prediction 
mol 
biol 
