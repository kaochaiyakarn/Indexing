distance metric learning application clustering side information eric xing andrew ng michael jordan stuart russell university california berkeley berkeley ca ang jordan russell cs berkeley edu algorithms rely critically metric inputs 
instance data clustered plausible ways clustering algorithm means initially fails find meaningful user recourse may user manually tweak metric sufficiently clusters 
applications requiring metrics desirable provide systematic way users indicate consider similar instance may ask provide examples 
algorithm examples similar desired dissimilar pairs points learns distance metric respects relationships 
method posing metric learning convex optimization problem allows give efficient local optima free algorithms 
demonstrate empirically learned metrics significantly improve clustering performance 
performance learning datamining algorithms depend critically metric input space 
instance means nearest neighbors classifiers kernel algorithms svms need metrics reflect reasonably important relationships data 
problem particularly acute unsupervised settings clustering related perennial problem right answer clustering algorithms cluster set documents clusters authorship clusters topic third clusters writing style say right answer 
worse algorithm clustered topic wanted cluster writing style relatively systematic mechanisms convey clustering algorithm left tweaking distance metrics hand 
interested problem suppose user indicates certain points input space say considered similar automatically learn distance metric respects relationships assigns small distances similar pairs 
instance documents example hope giving pairs documents judged written similar styles learn recognize critical features determining style 
important family algorithms implicitly learn metrics unsupervised ones take input dataset find embedding space 
includes algorithms multidimensional scaling mds locally linear embedding lle 
feature distinguishing learn full metric input space focusing finding embed ding points training set 
learned metric generalizes easily previously unseen data 
importantly methods lle mds suffer right answer problem example mds finds embedding fails capture structure important user unclear systematic corrective actions available 
similar comments apply principal components analysis pca 
motivating clustering example methods propose pre processing step help unsupervised algorithms find better solutions 
supervised learning setting instance nearest neighbor classification numerous attempts define learn local global metrics classification 
problems clear cut supervised criterion classification error available optimized 
see different way supervising clustering 
literature wide survey relevant examples include gives overview 
methods learn metrics classification clear learn general metrics algorithms means particularly information available structured traditional homogeneous training sets expected 
context clustering promising approach proposed wagstaff clustering similarity information :10.1.1.20.7363
told certain pairs similar dissimilar search clustering puts similar pairs dissimilar pairs different clusters 
gives way similarity side information find clusters reflect user notion meaningful clusters 
similar mds lle instance level constraints generalize previously unseen data similarity dissimilarity training set known 
discuss detail examine effects methods propose conjunction methods 
learning distance metrics suppose set points pairs similar learn distance metric points specifically similar points close 
consider learning distance metric form information certain similar respects ensure metric satisfying non negativity triangle inequality require positive semi definite setting gives euclidean distance restrict diagonal corresponds learning metric different axes different weights generally parameterizes family mahalanobis distances learning distance metric equivalent finding rescaling data replaces point applying technically allows imply note putting original dataset non linear basis function considering non linear distance metrics learned 
standard euclidean metric rescaled data useful visualizing learned metrics 
simple way defining criterion desired metric demand pairs points say small squared distance trivially solved useful constraint add ensure collapse dataset single point 
set pairs points known dissimilar information explicitly available may take pairs gives optimization problem choice constant right hand side arbitrary important changing constant positive results replaced problem objective linear parameters constraints easily verified convex 
optimization problem convex enables derive efficient local minima free algorithms solve 
note consider various alternatives choice despite giving simple linear constraint 
result rank data projected line 
case diagonal case want learn diagonal efficient algorithm newton raphson method 
define derive straightforward show minimizing subject equivalent multiplication positive constant solving original problem 
newton raphson efficiently optimize 
case full case learning full matrix constraint slightly trickier enforce newton method requiring prohibitively expensive time invert hessian parameters 
gradient descent idea iterative projections derive different algorithm setting 
proof reminiscent derivation fisher linear discriminant 
briefly consider max decomposing sible gives pos quotient quantity solution say solving generalized eigenvector problem recognize rayleigh principal eigenvector setting replace newton step size parameter optimized line search give largest downhill step subject update ensure true iff diagonal elements non negative iterate iterate converges convergence gradient ascent iterative projection algorithm 
matrices pose equivalent problem 
frobenius norm 
gradient ascent step optimize followed method iterative projections ensure constraints hold 
specifically repeatedly take gradient step repeatedly project sets gives algorithm shown 
motivation specific choice problem formulation projecting done inexpensively 
specifically projection step involves minimizing quadratic objective subject single linear constraint solution easily solving time sparse system linear equations 
second projection step space positive semi definite matrices done finding diagonalization diagonal matrix eigenvalues columns contains corresponding eigenvectors 
see 
experiments examples giving examples distance metrics learned artificial data show methods improve clustering performance 
examples learned distance metrics consider data shown divided classes shown different symbols available colors 
suppose points class similar reflecting 
depending learn diagonal full obtain visualize fact discussed earlier learning equivalent hopefully moves similar pairs finding rescaling data algorithm shown includes small refinement gradient step taken orthogonal subspace minimally direction projection disrupt constraint 
empirically modification significantly speeds convergence 
experiments synthetic data randomly sampled pairs similar points 
class data original class data projection newton class data projection ip original data different classes indicated different symbols colors available 
rescaling data corresponding learned diagonal 
rescaling corresponding full class data original class data projection newton class data projection ip original data 
rescaling corresponding learned diagonal 
rescaling corresponding full 
shows result plotting see algorithm successfully brought similar points keeping dissimilar ones apart 
shows similar result case clusters centroids differ directions 
see learned diagonal metric correctly ignores direction 
interestingly case full algorithm finds surprising projection data line maintains separation clusters 
application clustering application methods clustering side information learn distance metric similarity information cluster data metric 
specifically suppose told pair means belong cluster 
consider algorithms clustering 
means default euclidean metric points cluster centroids define distortion ignoring 

constrained means means subject points assigned cluster 

means metric means distortion defined distance metric learned 
constrained means metric constrained means distance metric learned cluster implemented usual means step points assigned cluster centroids assign 
generally imagine drawing edge pair points points resulting connected component constrained lie cluster pick 
original class data class data 
means accuracy 
constrained means accuracy 
means metric accuracy 
constrained means metric accuracy original dataset data scaled learned metric 
result shown gave visually indistinguishable results 
cluster point assigned automatic clustering algorithm correct desired clustering data 
case cluster data measure match accuracy indicator function 
equivalent probability points drawn randomly dataset clustering agrees clustering true belong different clusters 
simple example consider shows clustering problem true clusters indicated different symbols colors plot distinguished coordinate data original space cluster better coordinate 
shown accuracy scores means constrained means failed find clusterings 
learning distance metric clustering metric easily find correct clustering separating true clusters 
gives example showing similar results 
applied methods datasets uc irvine repository 
true clustering data class labels 
ran experiment little side information side information 
results 
see problem learned diagonal full metric leads significantly improved performance naive means 
problems learned metric constrained means th bar diagonal th bar full outperforms constrained means th bar large case clusters evaluation metric tends give inflated scores clustering correctly predict pairs different clusters 
setting uniformly random cluster determined chance different clusters chance modified measure averaging drawn matches mis matches weight 
results reported means multiple restarts averages trials wine trials 
generated picking random subset pairs points sharing class 
case little side information size subset chosen resulting number resulting connected components see footnote roughly size original dataset 
case side information changed 
original data 
means accuracy 
constrained means accuracy 
means metric accuracy 
constrained means metric accuracy projected data original dataset data scaled learned metric 
result shown gave visually indistinguishable results 
boston housing kc kc wine kc kc bean kc kc ionosphere kc kc balance kc kc protein kc kc iris plants kc kc breast cancer kc kc diabetes kc kc clustering accuracy uci datasets 
panel bars left correspond experiment little side information right side information 
left right bars set respectively means means diagonal metric means full metric constrained means kmeans kmeans diagonal metric kmeans full metric 
shown size dataset number classes clusters dimensionality data mean number connected components see footnotes 
bars shown 
performance performance protein dataset kmeans kmeans kmeans metric diag kmeans metric diag kmeans metric full kmeans metric full ratio constraints performance performance wine dataset kmeans kmeans kmeans metric diag kmeans metric diag kmeans metric full kmeans metric full ratio constraints plots accuracy vs amount side information 
axis gives fraction pairs points class randomly sampled included margin 
surprisingly see having side information leads metrics giving better clusterings 
typically shows typical examples quality clusterings increases amount side information 
problems wine algorithm learns diagonal full metrics quickly small amount side information protein distance metric particularly full metric appears harder learn provides benefit constrained means 
algorithm examples similar pairs points learns distance metric respects relationships 
method posing metric learning convex optimization problem allowed derive efficient free algorithms 
showed examples diagonal full metrics learned simple artificial examples demonstrated artificial uci datasets methods improve clustering performance 
atkeson moore schaal 
locally weighted learning 
ai review 
cox cox 
multidimensional scaling 
chapman hall london 
gunopulos 
adaptive nearest neighbor classification support vector machines 
advances neural information processing systems 
mit press 
golub van loan 
matrix computations 
johns hopkins univ press 
hastie tibshirani 
discriminant adaptive nearest neighbor classification 
ieee transactions pattern analysis machine learning 
jaakkola haussler 
exploiting generative models classifier 
proc 
tenth conference advances neural information processing systems 
jolliffe 
principal component analysis 
springer verlag new york 
rockafellar 
convex analysis 
princeton univ press 
roweis saul 
nonlinear dimensionality reduction locally linear embedding 
science 
scholkopf smola 
learning kernels 
press 
tishby pereira bialek 
information bottleneck method 
proc 
th allerton conference communication control computing 
wagstaff cardie rogers 
constrained means clustering background knowledge 
proc 
th international conference machine learning 
