frontiers web data management junghoo john cho ucla computer science department los angeles ca cho cs ucla edu decade web primary source information people 
due ease ubiquity people look pages web need look certain information 
popularity web brought interesting challenges 
particular expanding size web increasingly difficult discover store organize retrieve information web help users identify looking 
briefly go new challenges context describe research efforts address 
decade witnessed tremendous proliferation information world wide web 
world wide web initially developed help scientists exchange new results findings quickly decade exponential growth millions world wide users daily basis 
success proliferation indicate plentiful information world wide web useful people 
users access web variety purposes casual browsing general topic focused research particular problem 
success web introduced new challenges need addressed information overload success web partly due ease publication 
user publish information web time 
ease led exponential growth web decade simultaneously brought problem information overload topics simply web pages potentially relevant users waste significant time efforts reading irrelevant pages 
example issued query database google major search engines returned web pages potentially relevant topic 
limited time user user read pages 
user identify page answer particular question user 
projected growth rate web problem certainly get worse time 
information transience information internet inherently ephemeral 
distributed millions servers administered multitude organizations 
new information constantly posted server existing information continuously updated deleted administrator regardless important information transience information causes frustration users information frequently access may suddenly disappear 
information disappears users waste significant time efforts looking similar information sources web may impossible access information 
guarantee particular information available access 
information bias web primary source information people starts introduce significant bias people perception 
types information presence web discovered looked people web introduce significant influence people perceive world 
example issued query jaguar google top result page new version mac operation system 
top results mac operating system jaguar car 
mean original meaning jaguar cat relevance current society 
similarly large part web currently indexed major search engines due technological limitations 
part web called hidden web users rely search engines access web pages pages returned search engines essentially hidden inaccessible users 
ho bias introduced hidden web 
way reduce bias 
research group conducting various research projects address challenges 
sections describe project hidden web project detail 
archiving web order alleviate information transience problem project tries build system store archive history evolution web tracking changes web storing multiple versions web documents concise way providing archived information users intuitive interface 
effective archive system significantly benefit multiple disciplines ways archive human knowledge web popular widespread increasing number people rely web primary source information 
significant amount information available web digital form 
information disappears web information may permanently lost 
archive constantly changing web long period time may lose information taken decades discover 
web research testbed constant changes web documents pose challenges web caches web crawlers network routers 
web cache web crawler know web page changes download cache page multiple times page changed 
large body research conducted address challenge innovative new algorithms protocols developed 
due lack web change data difficult validate algorithms practice 
central archive web history provide valuable web change data research testbed researchers develop validate new ideas 
study knowledge evolution new topics genres grow popularity suddenly attract interest large number people 
don understand exactly topics started suddenly popular 
example linux project popular decade 
popular 
community start 
people discover project 
answering questions easy may get better understanding analyzing history web documents 
instance wanted study evolution linux project go back web years ago study pages mentioning linux time pages linked sequence created 
analysis reveal community developed time 
order build effective web archive system trying address challenges arise distributed nature web independent change information sources internet updated autonomously constantly 
exist changed pages archive system download system guess pages updated intelligently decide updated pages download 
uses limited download storage resources efficiently may important changes pages 
scale data web archive system download store enormous amount data 
textual data web estimated terabytes constantly updated 
order handle sheer scale data archive system employ novel techniques store organize compress web history data 
intuitive access archive system intuitive users search browse analyze 
addition able handle diverse queries users may pose 
example user may simply want browse multiple versions particular web page user may want pose complex query topics popularity measured number pages mentioning topic increased rapidly months order handle queries system employ novel indexing query processing techniques 
project tries address major technical challenges building effective web archive system fully scalable easy system handle dynamic web 
attain goal currently investigating main research issues 

efficient change detection designing efficient change detection download algorithms identify change characteristics web pages 
new algorithms efficiently limited download resources minimizing loss information 

efficient storage developing effective ways store organize web history data efficiently compactly 
multiple versions web page tremendous potential efficient storage compression changes web page minor 

effective access developing appropriate index structures query processing techniques web history data users intuitively express queries system handle queries efficiently 
part project currently building web archive prototype store monthly change history web pages related computer science 
relatively small subset web prototype proof concept various techniques develop 
prototype provide valuable real history dataset researchers test verify ideas 
believe successful completion research provide invaluable real web history data technologies useful managing archiving evolving textual database 
surfacing hidden web increasing amount information web available search interfaces 
users type set keywords queries search interface order access web pages certain web sites sites provide static links pages 
web crawlers simply follow links web discover pages search engines download pages referred hidden web deep web 
shows example process typical user goes access pages hidden web 
site user interested types list keywords search interface example shown site returns list potentially relevant pages user clicks links retrieve actual pages majority web users rely traditional search engines discover access information web hidden web practically inaccessible users hidden 
pages returned major search engines users simply give ignore pages 
users aware certain part hidden web crawler program downloads web pages search engines search interface pubmed 
list matching pages query liver 
matching page liver 
pages pubmed web site 
query breast cancer metasearcher relevant documents database selection result fusion pubmed process hidden web databases internet web need go painful process issuing queries potentially relevant sites hidden web contents investigating results manually 
hand information hidden web estimated significantly larger higher quality surface web indexed search engines 
research group conducting research projects hidden web easy access average users hidden web metasearcher hidden web crawler projects 
subsections briefly describe projects 
hidden web metasearcher goal project build metasearcher mediator automatically selects relevant hidden web sites user query users simply come metasearcher get access relevant hidden web data knowing individual sites 
user query breast cancer shown metasearcher determines sites relevant directs user query sites collects search results back user 
scenario effective metasearcher needs accomplish challenging tasks formal model hidden web crawling problem 
user query metasearcher identify sites relevant direct query sites arrows labelled 
task referred database selection database discovery 
metasearcher directs query breast cancer sites pubmed 

metasearcher gathers query results selected sites selectively presents results multiple sources user arrows labelled 
task known result merging result fusion 
far research group mainly focused database selection problem plan investigate result fusion problem 
believe success research significantly simplify people information search hidden web internet search engines simplified search surface web 
hidden web crawler approach build hidden web crawler download pages hidden web sites automatically known indexing technologies identify set pages relevant user query 
pages hidden web may generated dynamically user issues query main challenge hidden web crawler discover pages hidden web sites 
exist static links hidden web pages crawler automatically generate issue queries order discover pages 
automatic query generation clearly hard crawler understand semantics query interface 
note hidden web crawler limited time network resources 
crawler carefully select issue keyword queries download maximum number pages minimum resources 
issues completely random queries return matching pages may waste resources simply issuing queries retrieving actual pages 
theoretically query selection problem formalized minimum cover problem graph 
assume crawler downloads pages web site set pages rectangle 
represent web page vertex graph dots 
represent www nlm nih gov potential query qi crawler issue hyperedge graph circles 
hyperedge qi connects vertices pages returned crawler issues qi site 
hyperedge associated weight represents cost issuing query 
formalization problem select set hyperedges queries cover maximum number vertices web pages minimum total weight cost 
main difficulties formalization 
hidden web crawler know web pages returned query hyperedges graph unknown 
knowing hyperedges crawler select 
second minimum cover problem known np hard known exists efficient algorithm solve problem optimally 
currently investigating various ideas address issues 
main idea predict pages returned query qi analyzing pages downloaded previous queries 
qi 
example keyword medicine appears keyword violin pages returned previous queries may expect site may return pages issue query medicine violin prediction may completely accurate believe approach provide clues crawler able select significantly better queries 
conducted preliminary experiment real web site web pages result promising 
relatively straightforward query selection algorithm able download site issuing fewer queries 
continue study experimentally theoretically 
briefly went exciting challenges web brought today 
described research projects try address challenges 
project developing essential technologies archive history web access important information disappears web 
technologies develop archive type textual databases pages archive provide valuable dataset researchers investigate 
hidden web project developing effective framework provide single access point hidden web information 
developing novel hidden web crawler download pages hidden web automatically user input 
proliferation information web clearly enabled average person access enormous amount information possible decade ago 
believe success research projects web closer ideal information source user access information time intuitive interface 
cost query consists factors cost issuing query site cost retrieving answer contains list matching pages cost downloading matching pages 

probabilistic solution selection fusion problem distributed information retrieval 
proc 
acm sigir conference 
bergman 
deep web surfacing hidden value 
berners lee 
weaving web original design ultimate destiny world wide web 
edition 
cybenko 
dynamic web 
proceedings international world wide web conference www may 
brin page 
anatomy large scale hypertextual web search engine 
proceedings international world wide web conference www april 
callan connell du 
automatic discovery language models text databases 
sigmod conference pages 

chang li zhang 
structured databases web observations implications 
technical report university illinois urbana champaign 
cho garcia molina 
evolution web implications incremental crawler 
proceedings sixth international conference large databases vldb september 
cormen leiserson rivest stein 
algorithms 
mcgraw hill second edition 
douglis feldmann krishnamurthy 
rate change metrics live study world wide web 
proceedings second usenix symposium internetworking technologies systems october 
garey johnson 
computers intractability guide theory np completeness 
freeman 
huberman adamic 
growth dynamics world wide web 
nature september 
ipeirotis gravano 
distributed search hidden web hierarchical database sampling selection 
proceedings eighth international conference large databases 
ipeirotis gravano sahami 
probe count classify categorizing hidden web databases 
sigmod conference 
lawrence giles 
searching world wide web 
science april 
lawrence giles 
accessibility information web 
nature july 
meng 
liu yu wang chang 
determining text databases search internet 
proc 
th int 
conf 
large data bases vldb pages 
xu callan 
effective retrieval distributed collections 
proceedings st international acm sigir conference research development information retrieval pages 
yu meng wu 
liu 
efficient effective metasearch text databases incorporating linkages documents 
sigmod conference 
lee 
server ranking distributed text retrieval systems internet 
database systems advanced applications pages 

