online kernel change detection algorithm fr ric manuel davy christian umr cnrs rue de la bp nantes cedex france umr cnrs bp cit scientifique cedex france number abrupt change detection methods proposed past efficient model techniques generalized likelihood ratio glr test 
consider case accurate tractable model model free approach called kernel change detection kcd 
kcd compares sets descriptors extracted online signal time instant immediate past set immediate set 
soft margin single class support vector machine svm build dissimilarity measure feature space sets estimating densities intermediary step 
dissimilarity measure shown asymptotically equivalent fisher ratio gaussian case 
implementation issues addressed particular dissimilarity measure computed online input space 
simulation results synthetic signals real music signals show efficiency kcd 
index terms abrupt change detection kernel method single class svm online music segmentation 
october draft detecting abrupt changes signals systems longstanding problem various approaches proposed number papers 
particular likelihood ratio approaches generalized likelihood ratio glr test marginal likelihood ratio test quite efficient accurate tractable signal model exists implemented :10.1.1.22.6332
online versions statistical filtering performance 
model approaches perform efficient offline bayesian segmentation 
aside model techniques number general ad hoc model free methods designed detect abrupt changes signals 
typical examples time frequency approaches wavelet approaches cepstral coefficients approaches 
general model free framework online abrupt change detection similar model free techniques detection abrupt changes descriptors extracted signal interest 
main subject feature extraction abrupt change detection algorithm uses descriptors 
algorithm quite general sense applies dimensional signals music signals speech signals vibration signals large dimensional signals video monitoring multiple sensors 
precisely principle technique explained follows 
assume descriptors xt 
space extracted online possibly large dimensional signal function 
time indexes necessarily coincide may extract descriptors sample time 
problem consists detecting abrupt changes time series xt 
note typical descriptor extraction techniques dimension xt may large see example :10.1.1.117.6291
general framework non parametric online abrupt change detection time series descriptors may ways design abrupt change detector 
techniques compute distance measure successive descriptors order build stationarity index see 
techniques implement glr test gaussian mixture modeling descriptors distribution see 
technique hardly deal large dimensional inputs number model parameters estimated increases quickly dimension problem know curse dimensionality 
techniques special instances generic framework consider time descriptors subsets immediate past subset xt xi immediate subset xt xi depicted fig 

state online abrupt change detection problem follows 
time instant assume samples xt resp 
xt sampled probability density function pdf resp 

hypotheses holds abrupt change occurs abrupt change occurs word online meaning indicate address sequential framework 
real time applications implementation directly addressed 
october draft xt current time xt time fig 

general abrupt change detection framework time series descriptors xt represented circles 
xt test applied practice pdfs known 
standard practical approach uses dissimilarity measure estimated sole knowledge sets xt xt 
xt xt dissimilarity measure previous problem written follows xt xt abrupt change occurs xt xt abrupt change occurs threshold tunes sensibility robustness tradeoff detection framework 
detection performance closely related dissimilarity measure selected pdf estimation technique implemented 
possible choices methods inferred descriptors distribution pdf descriptors xt denoted supposed shape gaussian unknown parameter set denoted 
parameter estimates denoted computed sets resulting pdfs compared likelihood ratio densities dissimilarity measure kullback leibler kl divergence :10.1.1.22.6332
approach includes algorithms statistics empirical means covariances estimated descriptors compared dissimilarity measure 
approach adapted data large dimension due curse dimensionality 
methods descriptors distribution jointly prior distributions implementing bayes decision theory bayesian approach 
methods empirical descriptors density estimation density estimation algorithm implemented typical example parzen window estimator estimated densities compared dissimilarity measure 
examples context independent component analysis ica 
approach adapted large dimensional data due difficulty estimate accurately densities cases 
methods aimed comparing sets xt xt intermediate density estimation step 
family methods main subject 
machine learning approach framework described shows abrupt change detection seen machine learning problem statistical behavior sets xt xt learned compared 
propose new approach october draft inspired machine learning theory referred kernel change detection kcd abrupt change occurs location samples xt xt approximately 
hand abrupt change occurs happens samples xt located different parts space samples xt show chosen descriptor extraction technique yield situation 
vapnik principle argue situations convenient derive dissimilarity measure compares estimated density supports estimated pdfs support density roughly defined part space large 
order implement idea necessary derive robust accurate density support estimator dissimilarity measure regions comparison 
note dissimilarity measure needs valid regions complex winding possibly non connected shape large dimension 
organization section ii describe density support estimation technique support vector sv approach single class problems 
particular recall technique adapted large dimensional data robust outliers 
section iii abrupt change detection algorithm built single class support vector machines svms 
algorithm discussed section iv 
comparisons techniques 
section devoted simulations synthetic real data 
research directions proposed section vi 
ii 
sv approach single class classification problems section briefly recall elements sv approach single class classification problems relevant kcd 
descriptors referred vectors descriptors relevant 

xm set called training vectors practically euclidean space isomorphic finite stronger assumption set needed 
set called training set input space 
assumption 
training vector xi distributed unknown pdf independently xj 

aim single class classification referred novelty detection estimation region sole knowledge training set vectors drawn fall vectors distributed 
adopt equivalent representation real valued decision function fx fx fx vapnik principle possess restricted amount information solving problem try solve problem directly solve general problem intermediate step 
possible available information sufficient direct solution insufficient solving general intermediate problem 
october draft estimation fx realized risk minimization 
define errors vectors distributed loss function defines cost assign errors 
standard choices loss function hinge loss leads sv algorithms explained :10.1.1.11.2062
empirical risk remp fx defined remp fx xi fx xi 
classically estimating fx minimizing remp fx leads overtraining avoided minimizing regularized risk fx remp fx fx fx measure complexity fx tunes amount regularization 
sv approach achieved restricting fx elements class simple functions minimal complexity 
developed subsection dedicated sv single class classification 
sv single class classification order sv approach introduce called feature space mapping defined input space values feature space dot product defined define kernel xi xj xi xj xi xj loss generality assume normalized 
notation norm induced dot product 
words mapped input space subset hypersphere radius centered origin denoted provided positive subset positive orthant hypersphere 
training vectors mapped lie depicted fig 

sv approach single class classification consists separating training vectors non margin sv radius non svs margin sv fig 

feature space mapped training vectors xi xi 
located hypersphere optimization problem eq 
yields define separating hyperplane feature space density support estimate segment limited distance hyperplane called margin equals center hypersphere hyperplane see fig 

hyperplane written set fact kernel satisfying eq 
normalized xi xj resulting functional kernel see 
true long intersection hyperplane positive orthant empty 
xi xi xj non zero october draft parameters verifies mapped training vectors eq 
required training points verify realistic situations training set may contain outliers vectors descriptors representative signal system considered :10.1.1.2.6040:10.1.1.11.2062
choosing eq 
equivalent choosing decision function fx fx recall 
similar input space settings decision function fx defines region segment hypersphere fx positive 
remainder region resp 
referred density support estimate unknown pdf resp 

selection optimal hyperplane possible hyperplanes sv approach selects maximum margin see fig 
results max subject xi :10.1.1.2.6040:10.1.1.11.2062
positive parameter tunes amount possible outliers :10.1.1.2.6040:10.1.1.11.2062
optimization problem admits interpretation margin maximized constraint training vectors xi 
located half space bounded contain center see fig 

called slack variables 
implement hinge loss linearly penalizing outliers training vectors located wrong side margin maximization core principle sv algorithms shown maximum margin hyperplanes minimum regularized risk fx 
note complexity fx penalized eq 
convex quadratic optimization problem solved introducing lagrange multipliers 
yielding dual optimization problem see min jk xi xj subject :10.1.1.11.2062
solved numerical procedure loqo algorithm 
ixi implies fx ik xi maximum margin hyperplanes ensure minimum vapnik chervonenkis upper bound true risk 
october draft solution sparse lagrange multipliers referred weights zero 
corresponding training vectors called non support vectors located inside equivalently inside see fig 

vectors margin support vectors located boundary 
non margin support vectors outliers verify parameter plays important role upper bound fraction lower bound fraction svs 
mild conditions asymptotically equals fraction svs probability 
mercer kernels results exposed depend selection mapping defines dot product practice observe appears dot products need consider kernel eq 

reverse point view possible specify kernel mapping space verify eq 
:10.1.1.11.2062
kernels need verify necessary sufficient condition mercer 
explicitly stated consider euclidean space kernel gaussian kernel noted spread parameter note elements remain true mercer kernel norm exp iii 
kernel change detection algorithm explained section general framework abrupt change detection requires dissimilarity measure aimed comparing sets descriptors xt xt 
precisely relevant output low values xt xt occupy region space large values xt xt occupy distinct regions 
density support estimation technique described section ii build dissimilarity measure regions comparison shown 
dissimilarity measure feature space consider analysis time assume single class classifiers trained independently sets xt xt yielding regions xt rx xt equivalently feature space hyperplanes wt wt parameterized wt wt 
vectors wt wt define dimensional plane denoted pt intersects hypersphere circle center radius depicted fig 

probable case wt wt collinear highly infinity planes pt select 
feature space plane wt resp 
wt bounds segment mapped points xt resp 
xt lie 
indication dissimilarity xt xt arc distance october draft wt ct wt radius pt pt fig 

sv single class classifiers yield regions xt rh xt pt ct wt wt density support estimates feature space 
circle represented corresponds intersection plane pt uniquely defined wt wt intersection vector wt resp 
wt yields ct resp 
ct intersection hyperplane wt resp 
wt plane pt yields points denoted pt resp 
pt 
situation plotted corresponds abrupt change regions strongly overlap 
segments centers ct ct denoted ct ct see fig 

dissimilarity measure useful abrupt change detection scaled spread xt xt 
propose dissimilarity measure defined feature space see fig 
definition pt pt dh xt xt ct ct ct pt ct pt clearly dh intra regions inter regions ratio inspired fisher ratio see subsection iv 
considering xt see arc distance feature space 
samples spread larger distance ct pt measure spread samples xt ct pt smaller margin wt 
dissimilarity measure dh expected behavior feature space large separated sets small strongly overlapping sets 
section iv deeper study behavior dh 

probable cases 
probable case wt wt collinear ct ct impossible detect differences spread sets xt xt 
problem tackled adding small numerator eq 

similarly probable case xt xt zero spread overcome introducing denominator eq 

assume probable situations occur correspond realistic situations 
october draft computation input space dissimilarity measure dh derived completely defined feature space 
important question remains dh computed directly input space explicitly computing 
computation dh input space possible express function kernel applied vectors input space feature space arc distance vectors norm verifies angle putting arc distance ab cos ab ab cos arccos case arccos function properly vectors consider located orthant words 
dot product eq 
evaluated terms kernel linear span vectors :10.1.1.11.2062
case arc distance ct ct expressed terms firstly ct wt wt ct wt wt ct ct arccos wt wt wt wt secondly wt wt linear combinations weighted kernels shown eq 
wt wt wt wt kt kt kt resp 
column vector entries parameters wt resp 
wt computed training see eq 

kernel matrix kt uv entries row column training vector set xt similar calculations applied ct pt ct pt completes derivation 

equivalent definition dh 
ct ipt arccos kt dh equivalently defined replacing pt eq 
margin sv denoted msv pt 
arc distance ct pt equals ct msv pt msv located intersection wt arc distance ct 
reasoning holds pt msv october draft general framework kcd algorithm elements derived section dissimilarity measure introduced enable presentation full abrupt change detection algorithm 
algorithm summarizes procedure 
intermediate step assume descriptors extracted input time series consider series descriptors xt 
step initialization algorithm kernel change detection kcd algorithm select training sets sizes algorithm parameter threshold 
set gaussian kernel parameter 
set 
step online change detection train sv single class classifier immediate past training set xt xt 
xt train inde sv single class classifier immediate training set xt xt 
xt 
optimization process yields wt wt equivalently parameters 
compute decision index dh xt xt defined eq 
eq 
decide change detected time instant change detected time instant set go step 
algorithm abrupt change detected time index threshold 
approach classical tunes false positive false negative ratio :10.1.1.22.6332
testing time instant requires knowledge descriptors time series time instant 
sv single class classifier trained twice iteration novelty detection performed immediate past immediate sets time instant non computationally efficient procedure consist re computing scratch parameters solving optimization problem eq 
iteration 
noticed xt obtained xt incrementing xt decrementing xt efficient solutions implementing procedure include procedures incremental decremental technique derived stochastic gradient descent 
iv 
discussion section iii described dissimilarity measure dh arc distance feature space 
simple case pdfs gaussian radial fisher ratio standard measure dissimilarity defined df xt xt october draft empirical estimates mean covariance matrix pi set xt 
key question dh behave fisher ratio simple case 
negative answer question dh inappropriate 
subsection iv show behaves fisher ratio simple cases 
addition show dh deal complicated situations fisher ratio properly defined 
subsection iv devoted discussion comparison dissimilarity measures built parzen density estimates divergences 
subsection iv investigates sv approaches abrupt change detection 
kernel selection tuning algorithm dealt subsection iv 
connection fisher ratio input space consider xt reasoning holds xt assume pdf pt gaussian mean covariance matrix identity matrix 
consider training set xt randomly generated pt 
special case fisher ratio df xt xt theorem holds proof 
theorem 
set vectors sampled pdf mean function distance dx normalized kernel exp dx 
probability center yielded class svm converges 
note theorem general gaussian case considered 
theorem probability limit center ct 
words center ct segment sphere infinitely close image feature space 
write dh xt xt replacing arc distances arccos dot products feature space kernels arccos dh xt xt gaussian kernel dh xt xt arccos msv arccos msv msv msv arccos exp shown msv variance pi dh xt xt asymptotically proportional october draft psfrag replacements note increasing function constants 
result quite important shows sets radial gaussian distributions dh xt xt behaves standard fisher ratio df xt xt input space 
assessed simulations fig 

psfrag replacements psfrag replacements evolution training sets xt xt input space psfrag replacements psfrag replacements df xt xt dh xt xt dm xt xt psfrag replacements psfrag replacements fig 

comparison dissimilarity measure dh fisher ratio input space gaussian training sets xt xt mean 
top training sets input space 
left right distance increases sets better better separated 
solid lines represent regions estimated single class svm 
bottom fisher ratio input space dh plotted distance seen dissimilarity measures increase training sets better separated 
dissimilarity measure dm introduced subsection iv shows unsatisfactory fluctuations 
defined feature space points ct pt defined dh adapted situations vectors located regions complicated shapes 
particular support density say non connected dh defined 
fig 
simulations toy example show dh expected behavior small similar training sets xt xt large training sets different shapes mean remains close mean 
illustrates interest dh general situations 

kernel fisher discriminant 
dh built principle rayleigh coefficient fisher analysis 
completely different kernel fisher discriminant analysis kfd see case direction vectors wt computed independent single class sv machines 
kfd optimized obtain projection directions maximize variance projected vectors 
asymptotic result consistency check dissimilarity measure dh behavior known optimal measure gaussian assumption fisher ratio 
terms computational complexity fisher ratio generally cheaper 
noticed dh addresses general classes problems including limited amount training samples large dimensional descriptors october draft psfrag replacements psfrag replacements psfrag replacements evolution training sets xt xt input space psfrag replacements psfrag replacements df xt xt dh xt xt dm xt xt psfrag replacements psfrag replacements fig 

comparison dh input space fisher ratio training sets 
training set xt sampled gaussian pdf mean kept steady xt mixture gaussian pdfs means 
top training sets input space 
left right distance increases sets better better separated keep means 
bottom fisher ratio input space dh plotted distance seen dh increases training sets better separated fisher ratio decreases 
dissimilarity measure dm shows unsatisfactory fluctuations 
unknown pdfs possibly complicate shapes non connected supports situations fisher ratio inadequate 
little sense compute fisher ratio size training set small compared dimension input space 
comparison parzen density estimation techniques kernel dx parzen windows density estimation technique provides estimate pm pdf pm xi abrupt change detection achieved computing pm pm see subsection respectively xt xt comparing estimated densities dissimilarity measure kl divergence 
numerical computation difficult involves integral possibly large dimension 
pm dies strong rate monotonic tails monte carlo approximation kl divergence sampling done pm numerically unstable 
approach tackle problem ica literature see 
note sv class decision function build parzen window estimate :10.1.1.11.2062
possible solution consists computing fisher ratio directly feature space 
approach deal outliers 
empirical estimate covariance matrix poor computed october draft mj training vectors dimension mj 
possible sv approaches abrupt change detection subsection investigate possible sv approaches abrupt change detection 
alternative sv approach considers composite class training set time written xt xt xt applies mere class svm see detailed presentation method :10.1.1.11.2062
sets xt xt attributed different labels instance xt xt define classes vectors 
class svm yields margin wt denoted dm xt xt indicates distance feature space separating hyperplane closest mapped training vectors called margin svs 
course larger margin separated sets xt xt 
approach satisfactory context abrupt change detection major reasons 
firstly abrupt change occurs vectors xt xt located region defining margin meaningless 
secondly outliers defined different ways kcd approach class svm class case vectors considered outliers close hyperplane best separates xt xt 
words outliers set xt defined set xt conversely underlying process generates time series descriptors see fig 
thirdly computational load heavier class svm approach training performed vectors vectors training classical svm scales roughly optimized solvers complexity reaches see fig 
change occurs training class svm takes long time compared approach course occurs large values kernel spread parameter see fig 
greater greater runtime class change detection method 
fixed number training vectors kcd algorithm runtime roughly constant 
tuning algorithm kernel selection sake clarity kcd algorithm decision layer previously extracted descriptors 
interpretation procedure property property mercer kernel valued function mercer kernel 
words preprocessing function maps descriptors xt gaussian kernel applied xt interpreted direct application general mercer kernel includes preprocessing function original time series parameters kcd algorithm parameters gaussian kernel case parameters detection threshold parameter 
simulations conducted spider toolbox default optimizer switch optimization routines change observed behavior 
october draft psfrag replacements runtime gaussian kernel spread parameter fig 

runtime class classification approach computation dh circles kcd approach computation dm crosses functions gaussian kernel spread parameter toy example fig 

parameters algorithms 
influence clearly depends descriptors extraction technique selected 
kcd efficiency clearly related ability characterize abrupt changes shifts descriptors space typical descriptor extractors audio time frequency representations discussion time frequency preprocessing parameters tuning 
generally kcd algorithm parameters tuned follows applications 
dealing complicated applications speech music processing industrial applications underlying physical model available high level descriptors proves particularly efficient 
include time frequency representations mel cepstral coefficients wavelet coefficients redundant transforms known properties reflecting transient behavior expected provide descriptors adapted change detection framework address 
descriptors yield large dimensional problem due nice properties sv novelty detection see section ii :10.1.1.11.2062
techniques adapted signal considered 
case time frequency representations adaptivity includes choice time frequency kernel windows 
course change expected happen known domain projection relevant subspace considered 
kernel defines map gaussian kernel case influences location vectors xi hypersphere smaller distances feature space angle close 
words occupies large portion positive orthant hypersphere contrary mapped training vectors close feature space angle vectors close 
situations satisfactory choosing order magnitude smaller average distance sensible easy implement see fig 

tuning generally imposed dynamics signal system small information time frequency representations october draft psfrag replacements psfrag replacements psfrag replacements psfrag replacements psfrag replacements psfrag replacements psfrag replacements psfrag replacements fig 

comparison dissimilarity measure dh different values kernel width 
training sets gaussian sets fig 

seen kcd index sensible behavior large range values 
kcd algorithm detect frequent small changes 
contrary large enable detection long term changes neglect small changes 
external constraints may considered small detection lag required kept small 
note machine learning approach accuracy training phase computation wt wt increases 
rate outliers tuned detection requirements values influence outliers limited reduces rate false alarms 
significant difference observed detection performance values range 
contrary called hard margin case yields false alarms 
specific case examined subsection iv 
tuning threshold automatically open research kcd algorithm change detection techniques 
audio framework supervised tuning short signal sample considered effectively method employed music segmentation simulations reported section 
simulation results section dedicated simulation situations 
compare kcd algorithm glr algorithm case synthetic noisy sum sines signals 
kcd applied music signal segmentation organ defining convenient model suited glr algorithm intractable 
noisy sum sines signals subsection consider noisy sum sines signals yt sin 
sin fnt 
fi frequencies ai amplitudes 
gaussian white noise variance implement modified online version original glr approach october draft apply kcd algorithm time frequency descriptors simulations propose intend compare behavior kcd glr realistic situation :10.1.1.22.6332
describe modified online glr simulation 
glr applies directly signal requires choice model parameters threshold 

estimated signal yi 
producing prediction errors 
corresponds hypothesis change occurs interval hypothesis 
model parameters estimated yi resp 
yi prediction errors resp 

change detected likelihood ratio immediate past set size resp 
immediate set size may differ size resp 
exceeds threshold 
kcd corresponds set descriptors 
tuning kcd glr parameters mi chosen approaches set signal samples 
time instant 
framework modified glr fig 

current time yt fig 

framework modified online glr subsection approach applies directly signal represented circles 

classical glr approach 
classical batch approach glr applies procedure sets yt yt defined yt yi yt yi prediction errors computed signal 
likelihood ratio computed time instant 
information contained signal test time instant possible change time 
kcd adapted context defining accordingly immediate past time instant xt xi immediate xt xi size xt size xt 
change times estimated classical kcd dh xt xt peaks 
synthetic signals points long sines constant amplitudes frequencies may jump abruptly time 
different snrs db signal realizations generated half abrupt change time half 
spectrograms realizations plotted fig 

engineering problems music speech processing perfectly suitable relevant parametric model rarely obtained choose model best fits data time takes account applying gaussian kernel time frequency descriptors results fact composite gaussian time frequency kernel october draft time psfrag replacements frequency signal noise ratio db :10.1.1.117.6291
signal noise ratio db 
time psfrag replacements fig 

spectrograms realizations sum sines signals simulation left snr db right snr db 
spectrogram settings described subsection 
realizations abrupt change occurs time 
constraints complexity computational load standard approach music speech signal segmentation consists implementing glr test jointly autoregressive ar dynamic model see 
sensible compare kcd state art glr ar model order 
fair comparison methods provided amount data testing possible change time instant 
noted modeling error implementing order ar model mild assumes presence frequencies considered signal correctly deal additive noise 
illustrates realistic situations modeling errors avoided 
modeling errors increase snr decreases 
kcd issue met directly approach model free signal dependence issues embedded choice descriptors frequency sizes sets yt yt glr 
kcd algorithm parameterized follows 
descriptors time frequency sub images extracted smoothed pseudo wigner ville time frequency representation tfr see examples time frequency descriptors 
tfr smoothing windows gaussian length points time smoothing window points frequency smoothing window 
extracted descriptor sub image extracted tfr consecutive tfr columns overlap training sets sizes 
preprocessing uses points signal define immediate past sets time instant methods set samples signal test time instant sv parameters cent outliers allowed training set gaussian kernel 
kcd glr algorithms yield stationarity index peaks supposed indicate abrupt changes 
performance assessed roc curves false alarm rate true alarm rate defined follows true alarm decided true change detected estimated time instant points aside true change time instant corresponds length 
false alarm decided abrupt change detected matches point point 
roc curves obtained different exemple full parametric model music may 
october draft time psfrag replacements values snr plotted fig 

snr db methods yield excellent results 
true alarm rate false alarm rate roc curves snr db psfrag replacements false alarm rate glr kcd fig 

comparison performance glr kcd algorithms noisy sum sines signals 
roc curves display true alarm rate function false alarm rate glr left kcd right 
curves plotted different values snr bottom top snr db 
snr db methods excellent performance smaller snrs kcd outperforms glr approach modeling errors glr increase 
level noise increases kcd exhibits superior performance snr kcd roc curve glr roc curve see fig 

behavior due modeling errors glr model order autoregressive model signal sum sines noise 
sense kcd subject modeling errors simulation emphasizes approach proves robust noise example 
applications music segmentation true alarm rate subsection apply kcd algorithm abrupt change detection music perform music segmentation 
signals processed recorded church pipe organ 
third signal short extract 
segmenting organ church signals considered specially difficult task sound reflects church walls considerably original sound reverberation dies away quite slowly 
difficulties arise score notes alternate quickly 
music signal consider includes examples issues 
purpose detect accurately abrupt frequency changes music signal changes notes signal 
choice known piece music easier validate heuristically change detected kcd simply listening piece music reading corresponding score 
music signal extract minor bach 
kcd parameters tuned seconds extract sampling frequency fs khz 
corresponding score fig 

second extract validation see fig 

october draft psfrag replacements score rest kcd changes fig 

top score training organ extract considered subsection 
music signal seconds long half measure minor bach played pipe organ church segmentation tough task 
bottom kcd results notes represented upper row theoretical changes noted lower row corresponds numbered change detected kcd 
changes due segmentation 
music signal spectrogram modified spectrogram psfrag replacements time frequency hz psfrag replacements time time fig 

music signal mainly composed notes alternate quickly impossible distinguish time domain left 
spectrogram considered signal computed middle 
spectrogram scaled power right descriptors extracted modified tfr 
organ extract extract difficult segmentation task notes alternate quickly final played vibrato oscillation peaks may detected abrupt changes segmentation algorithm 
notes nd rest th notes played quickly difficulty raised frequency gap sharp sharp half step 
musician plays melodic line hands distance octave 
kcd algorithm tuned obtain accurate segmentation results extract 
music signal sampled factor yielding frequency bandwidth hz cutting partials frequency bandwidth smaller domain contains sufficient information segmentation 
instantaneous energy sampled signal set 
descriptors extracted spectrogram preprocessed signal represented fig 

frequency smoothing window gaussian length points ms 
extracted descriptor sub image tfr consecutive tfr columns 
october draft frequency hz psfrag replacements kcd index fig 

extract kcd index peaks certain threshold horizontal solid line changes detected music signal 
changes represented dotted vertical line 
changes correctly detected limited segmentation observed false alarms 
listening piece music confirms correctness segmentation 
tfr computed frequency bins 
vector training set scaled power optimization problem eq 
large values preferred require small weight rescaling training vectors avoids effect 
training sets sizes training sets durations ms training vectors durations ms sv kernel parameter gaussian kernel 
expect tfr perturbed echoes harmonics algorithm robust specifying cent training set really significant wishes estimate xt rx xt fig 
displays kcd index threshold chosen heuristically 
changes correctly identified see fig 

provided segmenting remains limited bothering issue consider segmentation preprocessing step coming analysis speech recognition harmonic modeling limited segmentation leads small computational overload segmentation far unde effects 
change detects fact echo second changes changes expected score fig 

correctness detection confirmed listening music signal 
fig 

top score second extract considered subsection 
music signal seconds long part measure fugue minor bach 
bottom kcd results notes represented upper row theoretical changes noted lower row corresponds numbered change detected kcd 
october draft psfrag replacements time fig 

kcd index test music sample 
changes correctly detected 
second organ extract apply kcd second extract parameters tuned extract check generalization ability algorithm 
second extract comes recording seconds extract measure fugue minor 
corresponding score fig 
kcd segmentation results 
kcd index plotted fig 

changes correctly detected 
change detected lag 
changes detected kcd algorithm correspond change similarly changes player ends note starting playing 
may notice situation hard conclude listening extract due fast alternation notes trailing 
change detected lag changes detected properly 
implemented glr approach ar model order 
spite attempts tune glr parameters obtain convincing results mainly ar model order need high start obtaining results requires long windows larger duration shortest notes enable accurate ar parameters estimation computations long 
subsection applied kcd problem music segmentation 
dealing difficult music signal recorded environment approach yields results better results obtained easier signals classical piano solo played reed section jazz 
extract apply kcd extract plotted fig 
previously assess quality standard model approaches cited 
segmentation results frequency domain time domain approaches including glr provided 
apply kcd signal time frequency descriptors wavelet descriptors 
known represent transients 
time frequency descriptors 
descriptors extracted spectrogram exactly parameter tuning organ signal sections illustrates generalization ability kcd 
parameters sv single class classifier kept unchanged 
change detection index october draft psfrag replacements psfrag replacements time fig 

sampled sax signal time domain length signal plotted fig 

true changes correctly detected 
frequency hz time psfrag replacements time fig 

spectrogram sax signal computed scaled power left 
descriptors extracted modified tfr 
kcd index right peaks correspond change sax signal 
results obtained parameters organ signal section 
wavelet descriptors 
descriptors extracted computed points ms morlet wavelet voices sampled signal represented fig 

scaled power 
training vector consecutive columns correspond ms training set sizes training set lengths ms sv kernel parameter gaussian kernel 
kcd index plotted fig 

true changes correctly detected 
vi 
proposed novel machine learning approach abrupt change detection problem detection achieved means new dissimilarity measure defined feature space computed input october draft psfrag replacements voices time psfrag replacements time fig 

sax signal computed ans scaled power left 
descriptors extracted modified time scale representation 
kcd index right peaks correspond change sax signal 
peaks time frequency descriptors 
space kernel trick 
approach robust outliers need assume statistical modeling underlying distributions 
simulations showed kcd algorithm standard generalized likelihood ratio glr approach 
application kcd music segmentation showed results 
perspectives include research application kcd segmentation music speech signals particular focusing design descriptors specifically dedicated tasks improve results obtained 
basseville detection abrupt changes theory application :10.1.1.22.6332
prentice hall april 
gustafsson marginalized likelihood ratio test detecting abrupt changes ieee trans 
automatic control vol 
pp 

li fabri particle filtering fault detection non linear stochastic systems international journal systems science vol 
pp 
mar 
andrieu doucet fitzgerald bayesian curve fitting applications signal segmentation ieee trans 
signal processing vol 
pp 

bayesian retrospective detection multiple changepoints corrupted multiplicative noise 
application sar image edge detection signal processing vol 
pp 
sept 
laurent stationarity index abrupt changes detection time frequency plane ieee signal processing letters vol 
pp 
february 
crouse nowak baraniuk wavelet statistical signal processing hidden markov models ieee trans 
signal processing vol 
pp 
apr 

lucas wavelet packet basis selection abrupt changes detection multicomponent signals eusipco rhode island greece sept 
bimbot experiments speech tracking audio documents gaussian mixture modeling ieee icassp salt lake city 
gustafsson adaptive filtering change detection 
john wiley sons 
davy godsill detection abrupt spectral changes support vector machines 
application audio signal segmentation ieee icassp orlando usa may 
october draft duda hart pattern classification scene analysis 
new york john wiley sons 
davy classification chirp signals hierarchical bayesian learning mcmc methods ieee trans 
signal processing vol 
pp 
feb 

pham fast algorithms mutual information independent analysis ieee trans 
signal processing submitted 
vapnik statistical learning theory 
new york wiley 
smola sch lkopf learning kernels :10.1.1.11.2062
mit press 
cristianini shawe taylor support vector machines 
cambridge university press 
sch lkopf platt shawe taylor smola williamson estimating support high dimensional distribution neural computation vol 
pp 

theory reproducing kernels transactions american mathematical society vol 
pp 

sch lkopf smola williamson bartlett new support vector algorithms neural computation vol 
pp 

vanderbei loqo interior point code quadratic programming department civil engineering operations research princeton university tech 
rep tr sor 
weston elisseeff version spider matlab september 
online 
available www tuebingen mpg de bs people spider index html mercer functions positive negative type connection theory integral equations philos 
trans 
roy 
soc 
lon don vol 
pp 

line class support vector machines 
application signal segmentation ieee icassp hong kong china april 
davy online support vector machine abnormal events detection signal processing submitted 
kivinen smola williamson online learning kernels forthcoming 
davy online kernel change detection algorithm rapport interne 
sch lkopf mika burges 
ller tsch smola input space vs feature space kernel methods ieee trans 
neural networks vol 
pp 
september 
davy doucet rayner optimised support vector machines nonstationary signal classification ieee signal processing letters vol 
pp 
dec 
flandrin time frequency time scale analysis 
academic press san diego 
davy bartels improved optimization time frequency signal classifiers ieee signal processing letters vol 
pp 
february 
davy doucet rayner optimised support vector machines nonstationary signal classification ieee signal processing letters vol 
pp 
december 
davy godsill bayesian harmonic models musical signal analysis seventh valencia international meeting bayesian statistics spain june 
musical signal parameter estimation ms thesis electrical engineering computer sciences university rennes france 
center new music audio technologies berkeley 
andre new statistical approach automatic segmentation continuous speech signals ieee trans 
assp vol 
january 
bach minor helmut organ st netherlands deutsche 

online 
available www berkeley edu thesis html basseville 
online 
available www irisa fr sigma michele segmentation html october draft 
