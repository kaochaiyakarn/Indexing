maximum margin matrix factorization nathan dept computer science university toronto toronto canada cs toronto edu jason rennie tommi jaakkola computer science artificial intelligence lab massachusetts institute technology cambridge ma usa tommi csail mit edu novel approach collaborative prediction low norm low rank factorizations 
approach inspired strong connections large margin linear discrimination 
show learn low norm factorizations solving semi definite program discuss generalization error bounds 
fitting target matrix low rank matrix minimizing sum squared error common approach modeling tabulated data done explicitly terms singular value decomposition desirable minimize different loss function loss corresponding specific probabilistic model mean parameters plsa natural parameters loss functions hinge loss appropriate binary discrete ordinal data 
loss functions squared error yield non convex optimization problems multiple local minima 
loss entries observed case collaborative filtering local minima arise svd techniques longer applicable 
low rank approximations constrain dimensionality factorization uv constraints sparsity non negativity suggested better capturing structure lead non convex optimization problems 
suggest regularizing factorization constraining norm constraints arise naturally matrix factorizations viewed feature learning large margin linear prediction section 
low rank factorizations constraints lead convex optimization problems formulated semi definite programs section 
focus low norm factorizations collaborative prediction predicting unobserved entries target matrix subset observed entries ys 
section generalization error bounds collaborative prediction low norm factorizations 
matrix factorization feature learning low rank model collaborative prediction straightforward matrix sought minimizes loss versus observed entries ys 
unobserved entries predicted matrices rank factored uv seeking low rank matrix equivalent seeking low dimensional factorization 
matrices say fixed matrix needs learned fitting column target matrix separate linear prediction problem 
row functions feature vector column linear predictor predicting entries corresponding column features collaborative prediction unknown need estimated 
thought learning feature vectors rows rows enabling linear prediction prediction problems columns concurrently different linear predictor columns 
features learned external information constraints impossible single prediction task labels features 
underlying assumption enables collaborative filtering situation prediction tasks columns related features possibly different ways 
low rank collaborative prediction corresponds regularizing limiting dimensionality feature space column linear prediction problem low dimensional space 
suggest allowing unbounded dimensionality feature space regularizing requiring low norm factorization predicting large margin 
consider adding loss penalty term sum squares entries fro fro fro denotes frobenius norm 
conditional problem fitting vice versa decomposes collection standard time regularized linear prediction problems 
appropriate loss function constraints observed entries correspond large margin linear discrimination problems 
example learn binary observation matrix minimizing hinge loss plus regularization term conditional problem decomposes collection svms 
maximum margin matrix factorizations matrices factorization uv low frobenius norm recall dimensionality longer bounded characterized equivalent ways known low trace norm matrices definition 
trace norm sum singular values lemma 
minx uv fro fro minx uv fro fro characterization terms singular value decomposition allows characterize low trace norm matrices convex hull bounded norm rank matrices lemma 
conv uv rn rm particular trace norm convex function set bounded trace norm matrices convex set 
convex loss functions seeking bounded trace norm matrix minimizing loss versus target matrix convex optimization problem 
contrasts sharply minimizing loss low rank matrices non convex problem 
sum squared error versus fully observed target matrix minimized efficiently svd despite optimization problem non convex minimizing loss functions minimizing squared loss versus partially observed matrix difficult optimization problem multiple local minima 
known nuclear norm ky fan norm 
fact trace norm suggested convex surrogate rank various rank minimization problems 
justify trace norm directly natural extension large margin methods providing generalization error bounds 
simplify presentation focus binary labels consider matrix factorization seek minimum trace norm matrix matches observed labels margin ia consider soft margin learning minimize trade trace norm hinge loss relative ys minimize max 
ia maximum margin linear discrimination inverse dependence norm margin 
fixing margin minimizing trace norm equivalent fixing trace norm maximizing margin 
large margin discrimination certain infinite dimensional radial kernels data separable sufficiently high trace norm trace norm sufficient attain margin 
max norm variant constraining norms rows average constrain rows small norm replacing trace norm max minx uv maxi ui maxa va ui va rows norm discrimination clean geometric interpretation 
note predicting target matrix signs rank matrix corresponds mapping items columns points users rows homogeneous hyperplanes user hyperplane separates positive items negative items 
hard margin low max norm prediction corresponds mapping users items points hyperplanes high dimensional unit sphere user hyperplane separates positive negative items large margin margin inverse 
learning maximum margin matrix factorizations section investigate optimization problem learning low norm factorization uv binary target matrix 
bounding trace norm uv fro fro characterize trace norm terms trace positive semi definite matrix lemma lemma 
rn iff exists tr tr 
proof 
note matrix fro tr write product 
uv fro fro tr tr establishing conversely write uv tr uu tr consider matrix uu lemma order formulate minimizing trace norm semi definite optimization problem sdp 
soft margin matrix factorization written min tr tr ia ia ia ia ia denotes positive semi definite associating dual variable qia constraint xia dual section max qia 
qia ia denotes sparse matrix ia ia zeros 
problem strictly feasible duality gap 
constraint dual equivalent bounding spectral norm dual written optimization problem subject bound spectral norm bound singular values max qia ia qia ia typical collaborative prediction problems observe small fraction entries large target matrix 
situation translates sparse dual semi definite program number variables equal number observed entries 
large scale sdp solvers take advantage sparsity 
prediction matrix minimizing part primal optimal solution extracted directly 
interesting study optimal prediction matrix directly recovered dual optimal solution 
unnecessary relying interior point methods sdp solvers return primal dual optimal pair enable specialized optimization methods advantage simple structure dual 
recovering linear programming recovering primal optimal solution directly dual optimal solution possible sdps 
hard margin problem slack possible describe optimal prediction matrix recovered dual optimal solution calculating singular value decomposition solving linear equations 
dual optimal consider singular value decomposition recall singular values bounded consider columns singular value 
possible show section complimentary slackness matrix rr optimal solution maximum margin matrix factorization problem 
furthermore bounded number non zero ia ia assuming hard margin constraints box constraints dual complimentary slackness dictates ia irr providing linear equation entries symmetric rr hard margin matrix factorization recover entries rr solving system linear equations number variables bounded number observed entries 
recovering specific entries approach described requires solving large system linear equations variables observations 
furthermore especially observations sparse small fraction entries target matrix observed dual solution compact prediction matrix dual involves single number observed entry 
desirable avoid storing prediction matrix explicitly calculate desired entry sign directly dual optimal solution consider adding constraint xi primal sdp 
exists optimal solution original sdp optimal solution modified sdp objective value 
optimal solution modified sdp optimal original sdp optimal value modified sdp higher worse optimal value original sdp 
introducing constraint xi primal sdp corresponds introducing new variable qi dual sdp appearing yi objective 
modified dual optimal solution original dual feasible 
primal optimal solutions modified primal sdp higher value dual longer optimal new dual 
checking optimality modified dual attempting re optimize recover sign repeat test yi yi corresponding xi 
yi optimal solutions dual solution improved introducing qi sign yi 
predictions new users far assumed learning done known entries rows 
commonly desirable predict entries new partially observed row new user included original training set 
essentially requires solving conditional problem known new row learned predictor new user new partially observed row matrix factorization standard svm problem 
max norm sdp max norm variant written sdp primal dual forms min ia ia ia max qia 
aii baa ia ia ia diagonal tr tr qia ia generalization error bounds low norm matrix factorizations similarly standard feature prediction approaches collaborative prediction methods analyzed terms generalization ability confidently predict entries error observed entries ys 
generalization error bounds holds target matrix random subset observations bound average error entries terms observed margin error central assumption paralleling source assumption standard feature prediction observed subset picked uniformly random 
theorem 
target matrices sample sizes log uniformly selected sample entries probability bounds special cases bounds general loss functions prove section 
prove bounds bound rademacher complexity bounded trace norm bounded max norm matrices balls norms 
unit trace norm ball convex hull outer products unit norm vectors 
bound rademacher complexity outer products boils analyzing spectral norm random matrices 
consequence inequality unit max norm ball factor convex hull outer products sign vectors 
rademacher complexity outer products bounded considering cardinality 
sample selection holds matrices nm ia ln nm nm ia max ia ln ia ln log ln log ln ln universal constant depend quantity 
understand scaling bounds consider matrices uv norms rows bounded matrices max trace norm matrices bounded nm bounds agree log factors cost allowing norm low average uniformly 
recall conditional problem fixed learned collection low norm large margin linear prediction problems 
norms rows bounded similar generalization error bound conditional problem include term matching bounds theorem log factors learning introduce significantly error learning just 
interest comparison bounds low rank matrices rank fro particular rank entries bounded second term right hand side ln ln best log factors expected scale sensitive bounds combinatorial approach dependence magnitude entries margin avoided 
implementation experiments ratings collaborative prediction tasks labels binary discrete ratings ordered levels star stars 
separating levels thresholds generalizing hard margin constraints binary labels require xia 
soft margin version constraints slack variables constraints observed rating corresponds generalization hinge loss convex bound zero level agreement error zoe 
obtain loss convex bound mean absolute error mae difference levels predicted level true level introduce slack variables observed rating general loss functions bounds theorem depend lipschitz constant loss best log factors achieved explicitly bounding magnitude loss function 
constraints xia xia 
soft margin problems immediate threshold threshold formulated sdps similar 
furthermore straightforward learn thresholds appear variables primal correspond constraints dual single set thresholds entire matrix separate threshold vector row matrix user 
doing allows users ratings differently alleviates need normalize data 
experiments conducted preliminary experiments subset movielens dataset consisting users movies ratings 
csdp solve resulting sdps ratings discrete scale experimented generalizations hinge loss allowing user thresholds 
compared medians described baseline learners 
randomly split data sets 
possible test sets remaining sets calculate fold cross validation cv error method medians trace norm max norm immediate threshold hinge loss range parameters rank number centers medians slack cost 
splits selected learners lowest cv zoe mae baseline learners lowest cv zoe mae measured error held test data 
table lists cv test errors average test error test sets 
average test sets achieves lower mae baseline learners test sets achieves lower zoe baseline learners 
test zoe mae set method cv test method cv test rank medians rank medians rank medians rank medians avg 
max norm max norm trace norm max norm max norm max norm max norm max norm avg 
table baseline top bottom methods parameters achieved lowest cross validation error training data train test split error predictor test data 
listed learners threshold objective 
discussion learning maximum margin matrix factorizations requires solving sparse semi definite program 
experimented generic sdp solvers able learn tens thousands labels 
propose just generic qp solvers perform svm problems special purpose techniques advantage simple structure dual necessary order solve large scale problems 
sdps suggested related different problem learning features www cs umn edu research grouplens solving immediate threshold loss took minutes ghz intel xeon 
solving threshold loss took hours 
matlab code available www ai mit edu equivalently kernel best single prediction task 
task hopeless features completely unconstrained formulation 
lanckriet suggest constraining allowed features linear combination base feature spaces base kernels represent external information necessary solve single prediction problem 
possible combine approaches seeking constrained features multiple related prediction problems way combining external information details users items collaborative information 
alternate method introducing external information formulation adding additional fixed non learned columns representing external features 
method degenerates standard svm learning vector matrix 
important limitation approach described observed entries assumed uniformly sampled 
explicit generalization error bounds 
assumption typically unrealistic users tend rate items 
extreme desirable predictions positive samples 
situations possible learn low norm factorization appropriate loss functions derived probabilistic models incorporating observation process 
obtaining generalization error bounds case harder 
simply allowing arbitrary sampling distribution calculating expected loss distribution possible trace norm possible max norm satisfying guarantee low error items user want anyway items predict 
acknowledgments sam roweis pointing 
hofmann 
unsupervised learning probabilistic latent semantic analysis 
machine learning journal 
collins dasgupta schapire 
generalization principal component analysis exponential family 
advances neural information processing systems 
nathan tommi jaakkola 
weighted low rank approximation 
th international conference machine learning 
lee seung 
learning parts objects non negative matrix factorization 
nature 
hofmann 
latent semantic models collaborative filtering 
acm trans 
inf 
syst 
benjamin marlin 
modeling user rating profiles collaborative filtering 
advances neural information processing systems volume 
hindi stephen boyd 
rank minimization heuristic application minimum order system approximation 
proceedings american control conference volume 
nathan 
learning matrix factorization 
phd thesis massachusetts institute technology 
alon jaakkola 
generalization error bounds collaborative prediction low rank matrices 
advances neural information processing systems 
amnon shashua levin 
ranking large margin principle approaches 
advances neural information proceedings systems volume 
borchers 
csdp library semidefinite programming 
optimization methods software 
marlin 
collaborative filtering machine learning perspective 
master thesis university toronto 
lanckriet cristianini bartlett el ghaoui jordan 
learning kernel matrix semidefinite programming 
journal machine learning research 
