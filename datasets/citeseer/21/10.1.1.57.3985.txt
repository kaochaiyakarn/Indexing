scalable parallel computing grand unified theory practical development mccoll programming research group oxford university computing laboratory wolfson building parks road oxford ox qd uk mccoll comlab ox ac uk bulk synchronous parallel bsp model provides unified framework design programming general purpose parallel computing systems 
describe bsp model discuss developments architecture algorithms programming languages currently pursued part new unified approach scalable parallel computing 
keyword codes keywords multiprocessors concurrent programming models computation 
von neumann produced proposal general purpose stored program sequential computer captured fundamental principles turing practical design 
design come known von neumann computer served basic model sequential computers produced late time 
sequential computation stability von neumann model permitted development decades variety high level languages compilers 
turn encouraged development large diverse software industry producing portable applications software wide range von neumann machines available personal computers large mainframes 
stability underlying model allowed development robust complexity theory sequential computation set algorithm design software development techniques wide applicability 
single model parallel computation come dominate developments parallel computing way von neumann model dominated sequential computing 
variety models vlsi systems systolic arrays distributed memory multicomputers careful exploitation network locality crucial algorithmic efficiency 
generic term special purpose refer type parallel computing 
describe number aspects done years design special purpose parallel computing systems 
major challenge computing science computing industry determine extent general purpose parallel computing achieved 
goal deliver scalable parallel performance architecture independent parallel software 
special purpose parallel computing architectures declarative languages having shown achieved 
success front enable parallel computing expand scientific engineering applications normal method computing example new commercial applications emerging companies reengineer business processes 

bsp model parallel random access machine pram consists collection processors compute synchronously parallel communicate common global random access memory 
major issue theoretical computer science late determine extent idealised pram model efficiently implemented physically realistic distributed memory architectures 
number new routing memory management techniques developed show efficient implementation possible cases 
efficient implementation single address space distributed memory architecture requires efficient method distributed routing read write requests replies read requests network processors 
consider problem packet routing processor network 
relation denote routing problem processor packets send various processors network processor due receive packets processors 
packet word information real number integer 
twophase randomised routing example show log relation realised processor hypercube log steps 
deal problem hot spots points links network overloaded large number processors simultaneously try access memory module uniformly distribute memory hashing address space 
bsp model describe generalisation pram model permits frequency barrier synchronisation demands routing network controlled 
bulk synchronous parallel bsp computer consists set processor memory pairs communications network delivers messages point point manner mechanism efficient barrier synchronisation subset processors 
specialised broadcasting combining facilities 
define time step time required single local operation basic operation addition multiplication locally held data values performance bsp computer characterised parameters number processors processor speed number time steps second synchronisation periodicity minimal number time steps successive synchronisation operations total number local operations performed processors second total number words delivered communications network second 
parameter related network latency time required non local memory access situation continuous message traffic 
parameter corresponds frequency non local memory accesses machine higher value non local memory accesses frequently 
formally related time required realise relations situation continuous message traffic value relation performed gh time steps 
bsp computer operates way 
computation consists sequence parallel supersteps superstep sequence steps followed barrier synchronisation point non local memory accesses take effect 
superstep processor carry set programs threads perform number computation steps set threads values held locally start superstep ii send receive number messages corresponding non local read write requests 

architectures parameters characterise communications performance bsp computer contrasts sharply way communications performance described distributed memory architectures market today 
major feature bsp model lifts considerations network performance local level global level 
longer particularly interested network array butterfly hypercube implemented vlsi optical technology 
interest global parameters network describe ability support non local memory accesses uniformly efficient manner 
design implementation bsp computer values achieved depend capabilities available technology ii amount money willing spend communications network 
computational performance machines performance captured continues grow find keep low necessary continually increase investment communications hardware percentage total cost machine 
central thesis bsp pram approaches scalable parallel computing costs paid parallel machines new level efficiency flexibility architecture independent programmability obtained 
asymptotic terms values expect various processor networks ring array butterfly log log hypercube log 
asymptotic estimates entirely degree diameter properties corresponding graph 
practical setting channel capacities routing methods vlsi implementation significant impact actual values achieved machine 
new optical technologies may offer prospect reductions values achieved providing efficient means non local communication possible vlsi 
simple practical randomised method routing relations optical communications system described 
optical system physically realistic method requires log loglog steps 
parallel computing system regarded bsp computer benchmarked accordingly determine bsp parameters bsp model prescriptive terms physical architectures applies 
parallel computer bsp computer kind 
unifying property bsp model crucial importance provides evolutionary path today machines typically high values kinds scalable parallel computing systems lower values time broad classes parallel machine commercially available distributed memory architectures shared memory multiprocessors networks clusters workstations 
years expect see classes alike reasons 
manufacturers distributed memory architectures increasingly aim abstraction single address space programmer high performance global communications 
applications possible automatic memory management techniques systems provide form virtual shared memory 
ii manufacturers shared memory multiprocessors aim increase degree scalable parallelism systems replacing bus architectures interconnection networks offering high performance global communications 
iii clusters workstations normally connected powerful switching networks basic technologies currently token rings 
enable huge latencies experienced today systems dramatically reduced 
turn allow fine grain parallelism exploited efficiently systems 
combination technological commercial software reasons expect see steady convergence years standard architectural model scalable parallel computing 
consist collection workstation processor memory pairs connected pointto point communications network efficiently support global address space 
message passing shared memory programming styles efficiently supported architectures 
successful models plenty scope different designs technologies realise systems different forms depending cost performance requirements 
example systems optical technologies achieve efficient global communications achieved vlsi systems 

algorithms complexity superstep bsp algorithm determined follows 
maximum number local computation steps executed processor maximum number messages sent processor maximum number messages received processor original bsp model cost gh gh time steps 
alternative charge gh gh time steps superstep difference costs general significant 
small bsp computer corresponds closely pram determining degree parallel slackness required achieve optimal efficiency 
bsp computer kind low value hashing achieve efficient memory management 
case corresponds idealised pram parallel slackness required 
processor algorithm implemented processor machine said parallel slackness factor machine 
designing algorithms bsp computer high value need achieve measure communication slackness exploiting thread locality level memory ensure non local memory access request able perform approximately operations local data 
achieve architecture independence bsp model appropriate design parallel algorithms programs parameterised size problem number processors resulting algorithms efficiently implemented range bsp architectures widely differing values 
new framework design analysis bsp computations emerged done couple years 
denote upper bound sequential complexity problem 
aim bsp algorithm design produce parallel algorithm achieves upper bound widest possible range values simple example illustrates approach 
problem multiplication theta matrices processors 
standard sequential algorithm adapted run processors follows 
processor computes theta submatrix require elements number processor computation requirement operations inner product requires operations communications requirement number non local reads assume distributed uniformly processors processor receiving elements matrix processors simply replicate send appropriate elements processors requiring 
communications requirement approximately messages sent 
total parallel time complexity provided 
alternative algorithm requires fewer messages altogether implemented give optimal runtime large 
briefly describe bsp complexity standard problems 
point fft evaluated processor bsp computer log log supersteps processor computes point fft 
cost superstep log gn pg 
point fft computation optimal runtime log achieved provided log log 
solution theta triangular linear system ax computed parallel form back substitution supersteps costing gn 
total time gn lp shows optimal runtime achieved provided 
standard sequential method lu decomposition theta matrix implemented bsp algorithm consisting supersteps costing gn 
total time gn lp shows optimal runtime achieved provided 
fundamental problem scientific computing multiplication large sparse matrix dense vector 
give detailed theoretical experimental analysis efficiency problem solved bsp architectures 
describe new efficient methods domain partitioning bsp architectures 
noted situations processors bsp architecture increase runtime 
example consider bsp architecture ring 
runtime bsp algorithm solution theta triangular linear system np 
runtime minimised increasing number processors value increase runtime 

programming described bsp computer architectural model view bulk synchrony programming model kind programming methodology 
essence bsp approach notion superstep idea input output associated superstep reading writing depending views performed global operation involving set individual sends receives 
viewed way bsp program simply proceeds phases necessary global communications place phases 
bsp approach regarded programming methodology applicable kinds parallel architecture 
oxford parallel parallel computing centre oxford university 
major part oxford parallel concerned development scalable portable parallel software wide variety practical applications science engineering commerce 
bsp unifying program design methodology development fully portable single source parallel software run optimally distributed memory architectures shared memory multiprocessors networks workstations 
number bsp software tools developed support 
oxford bsp library 
library embodies static spmd single program multiple data programming model parallel process executes program text 
processes proceed sequence supersteps superstep different processes may take different execution paths processes reach superstep proceed 
process private data space globally shared data process explicitly access variables process 
remote data access asynchronous request fetch store remote data item guaranteed satisfied current superstep processes synchronised 
core library consists just routines bsp start max procs num procs pid bsp finish bsp sstep bsp sstep bsp fetch pid data data nbytes bsp store pid data data nbytes process management synchronisation communication 
routines callable standard sequential languages fortran library proved extremely useful number industrial commercial projects writing new parallel applications adapting existing code parallel environment 
initial version library built top pvm 
highly efficient scalable native implementations library currently developed oxford parallel various distributed memory architectures shared memory multiprocessors networks workstations 
design object oriented bsp libraries kind area current interest 
high performance fortran may program bsp architectures ideal task 
see discussion hpf produce scalable parallel programs 
described new architecture independent programming model scalable parallel computing 
model designed permit efficient programming bsp pram data parallel computations 
allows programmer control scheduling synchronisation degree virtual concurrency locality variables properties program may crucial achieving efficient scalable parallel performance 
model supports static dynamic bsp computations 
language implementation model currently developed part esprit project foundations general purpose parallel computing 

years see rapid convergence place field parallel computer systems 
various classes parallel computer currently exist distributed memory architectures shared memory multiprocessors networks workstations alike 
architectural convergence enable parallel computing normal method computing encourage growth large diverse parallel software industry similar currently exists sequential computing 
important establish solid foundation guide process convergence enable architecture independent parallel software developed emerging range scalable parallel systems 
bsp approach provides just foundation 
offers prospect achieving scalable parallel performance architecture independent parallel software provides framework permits performance parallel distributed systems analysed predicted precise way 
exciting new research problems emerging area try understand issues involved design implementation architecture independent programs scalable parallel computing systems 
hope expect research problems currently pursued enable bsp approach developed point provide unifying framework design programming kinds parallel distributed systems analogous von neumann model provided sequential systems years 

burks von neumann 
preliminary discussion logical design electronic computing instrument 
part volume 
institute advanced study princeton 
report army department 
edition june 
second edition september 
appears papers john von neumann computing computer theory burks editors 
volume charles institute reprint series history computing mit press 

turing 
computable numbers application entscheidungsproblem 
proceedings london mathematical society 
series 
corrections ibid 

mccoll 
special purpose parallel computing 
gibbons spirakis editors lectures parallel computation 
proc 
alcom spring school parallel computation volume cambridge international series parallel computation pages 
cambridge university press cambridge uk 

aj 
parallel algorithms 
addison wesley 

valiant 
general purpose parallel architectures 
van leeuwen editor handbook theoretical computer science volume algorithms complexity pages 
north holland 

valiant 
bridging model parallel computation 
communications acm 

mccoll 
general purpose parallel computing 
gibbons spirakis editors lectures parallel computation 
proc 
alcom spring school parallel computation volume cambridge international series parallel computation pages 
cambridge university press cambridge uk 

valiant 
combining mechanism parallel computers 
meyer auf der heide monien rosenberg editors parallel architectures efficient 
proceedings heinz symposium paderborn november 
lncs vol 
pages 
springer verlag 


efficient optical communication parallel computers 
proc 
th annual acm symposium parallel algorithms architectures pages 

valiant 
direct bulk synchronous parallel algorithms 
technical report tr extended version aiken computation laboratory harvard university 
shorter version appears proc 
rd scandinavian workshop algorithm theory july 
lncs vol 
pp springer verlag 

aggarwal chandra snir 
communication complexity prams 
theoretical computer science 

mccoll 
scientific computing bulk synchronous parallel architectures 
technical report department mathematics university utrecht december 

miller reed 
oxford bsp library users guide 
version 
oxford parallel technical report oxford university computing laboratory 

geist sunderam 
network concurrent computing pvm system 
concurrency practice experience june 

snir 
scalable parallel computers scalable parallel codes theory practice 
meyer auf der heide monien rosenberg editors parallel architectures efficient 
proceedings heinz symposium paderborn november 
lncs vol 
pages 
springer verlag 

mccoll 
architecture independent programming model scalable parallel computing 
ferrante hey editors portability performance parallel processors 
john wiley sons 
appear 
