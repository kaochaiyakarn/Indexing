sparse cooperative learning kok science uva nl nikos vlassis vlassis science uva nl informatics institute faculty science university amsterdam netherlands learning multiagent systems suffers fact state action space scale exponentially number agents 
interested learning learn coordinated actions group cooperative agents sparse representation joint stateaction space agents 
examine compact representation agents need explicitly coordinate actions predefined set states 
coordination graph approach represent values value rules specify coordination dependencies agents particular states 
show qlearning efficiently applied learn coordinated policy agents framework 
demonstrate proposed method predator prey domain compare related multiagent qlearning methods 

multiagent system mas consists group agents potentially interact weiss vlassis 
interested fully cooperative multiagent systems agents learn optimize global performance measure 
key problems systems problem coordination ensure individual decisions agents result jointly optimal decisions group 
reinforcement learning rl techniques sutton barto applied successfully single agent systems learning policy agent appearing proceedings st international conference machine learning banff canada :10.1.1.32.7692
copyright authors 
uncertain environments 
principle possible treat multiagent system big single agent learn optimal joint policy standard single agent reinforcement learning techniques 
state action space scale exponentially number agents rendering approach infeasible problems 
alternatively agent learn policy independently agents transition model depends policy learning agents may result oscillatory behavior 
hand problems agents need coordinate actions states cleaning robots want clean room rest states agents act independently 
coordinated states known advance priori clear agents learn act cooperatively states 
describe multiagent learning technique called sparse cooperative learning allows group agents learn jointly solve task global coordination requirements system particular action choices agents known 
examine compact representation agents learn take joint actions predefined set states 
uncoordinated states agents learn independently 
generalize approach context specific coordination graph guestrin specify coordination dependencies subsets agents current context dynamically 
proposed framework allows sparse representation joint state action space agents resulting large computational savings 
demonstrate proposed technique predator prey domain popular multiagent problem number predator agents try capture poor prey 
method achieves trade speed solution quality 

mdps learning section review markov decision process mdp framework 
observable mdp tuple finite set world states set actions markovian transition function describes probability state performing action state ir reward function returns reward obtained action state agent policy defined mapping objective find optimal policy maximizes expected discounted reward max st state expectation operator averages reward stochastic transitions discount factor 
represent values store expected discounted reward state possible action max 
optimal policy state action arg maxa maximizes expected discounted reward 
reinforcement learning rl sutton barto applied estimate :10.1.1.32.7692
learning widely learning method transition reward model unavailable 
method starts initial estimate state action pair 
exploration action taken state reward received state observed corresponding value updated max appropriate learning rate 
conditions learning known converge optimal watkins dayan 

multiagent learning framework discussed previous section involves single agents 
interested systems multiple agents set actions collaboratively solve task 
collaborative multiagent mdp guestrin extends single agent mdp framework include multiple agents joint action impacts state transition received reward 
transition model represents probability system move state performing joint action ai ri ir reward function returns reward ri agent joint action taken state global reward function ri take sum individual rewards received agents 
framework differs stochastic game shapley agent wants maximize social welfare sum payoffs payoff 
framework different choices affect problem description possible solution concepts agents allowed communicate observe selected joint action perceive individual rewards agents case assume agents allowed communicate able share individual actions rewards 
discuss approach describe learning methods environments multiple agents 

mdp learners principle collaborative multiagent mdp regarded large single agent joint action represented single action 
optimal values joint actions learned standard single agent learning 
order apply mdp learners approach central controller models complete mdp communicates agent individual action agents model complete mdp separately select individual action corresponds identity 
case communication needed agents observe joint action individual rewards 
problem exploration solved random number generator seed agents vlassis 
approach leads optimal solution infeasible problems agents joint action space exponential number agents intractable 

independent learners extreme independent learners il approach claus boutilier agents ignore actions rewards agents system learn strategies independently 
standard convergence proof qlearning hold case transition model depends unknown policy learning agents 
despite lack guaranteed convergence method applied successfully multiple cases tan sen 

context specific learning problems agents coordinate actions specific context guestrin 
example cleaning robots take care obstruct cleaning room 
different rooms independently 
section describe reinforcement learning method explicitly models types coordination requirements 
main idea learn joint action values states agents need coordinate actions 
create sparse representation joint stateaction space specifying states agents coordinate actions 
learning agents apply il method uncoordinated states mdp learners approach coordinated states 
practical problems agents typically need coordinate actions states framework allows sparse representation complete action space resulting large computational savings 
distinction action types different states distinguish different representations values 
agent maintains single action value table qi ai uncoordinated states joint action value table coordinated states 
coordinated states global value directly relates shared joint table 
uncoordinated states assume global value sum individual values qi ai 
agents observe state transition values different tables combined order update values 
different situations taken account 
moving coordinated uncoordinated states respectively apply mdp learners il approach 
case agents move coordinated state uncoordinated state back individual joint value ri max qi 
conversely moving uncoordinated state 
graphical representation tables case agents 
state coordinated states state uncoordinated state 
coordinated state back joint qvalue different individual values qi ai qi ai ri ai max 
case agent rewarded fraction expected discounted reward resulting coordinated state 
essentially implies agent contributes equally coordination 
fig 
shows graphical representation transition states problem involving agents 
state agents coordinate actions shared table determine joint action 
joint action observing transition uncoordinated state joint action value updated eq 

similarly agent chooses action independently moving state updates individual value qi eq 

terms implementation shared table stored centrally agents access shared resource updated identically individual agents 
note case agents rely strong common knowledge assumptions observed actions rewards agents 
furthermore agents coordinate actions coordinated state 
remainder discuss approach generalization described algorithm section 
framework coordination requirements specified subsets agents global value distributed different agents 
discuss generalized approach review notion context specific coordination graph 

context specific coordination graphs context specific coordination graph cg represents dynamic context dependent set coordination requirements multiagent system guestrin 

group agents node cg context represents agent ai edge defines dependency agents 
interconnected agents coordinate actions time step 
example left graph fig 
shows cg agent problem agent coordinate coordinate coordinate 
global payoff function decomposed sum local payoff functions cg replaces global coordination problem number local coordination problems involve fewer agents solved distributed manner message passing scheme 
guestrin global payoff function distributed agents set value rules 
propositional rules form context element set possible combinations state action variables ir payoff added global payoff holds 
definition agents neighbors cg value rule contains actions agents clearly set value rules form sparse representation global payoff function state action combinations defined 
fig 
shows example context specific cg simplicity actions state variables assumed binary left graph show initial cg corresponding set value rules 
note agents involved rules neighbors graph 
center show value rules cg updated agents condition current context state true 
information state rule irrelevant removed 
consequence optimal joint action independent edge deleted graph shown center fig 

order compute optimal joint action maximum total payoff cg variable elimination algorithm briefly illustrate example fig 

agents conditioned action corresponds true action false 

initial cg left conditioning context true center elimination right 
context center agents eliminated graph 
assume eliminate 
agent collects rules involved possible actions agent determines conditional strategy case equal eliminated graph 
algorithm continues agent computes conditional strategy eliminated 
agent left fixes action 
second pass reverse order performed agent distributes strategy neighbors determine final strategy 
results optimal joint action global payoff 

sparse cooperative learning method discussed section defined state coordinated state agents coordinate actions uncoordinated state agents act independently 
situations agents coordinate actions 
section describe sparse cooperative learning allows group agents learn coordinate predefined coordination structure differ states 
guestrin distributing global value different agents 
agent associated local value function qi depends subset possible state action variables 
global value equals sum local values agents qi 
suppose exploration joint action taken state agent receives reward ri state observed 
decomposition global learning update rule reads qi qi ri max qi 
variable elimination algorithm discussed section agents compute optimal joint action arg maxa state compute contribution qi total payoff show 
allows update decomposed locally agent qi qi ri qi qi 
discuss local functions represented 
notation value rule representation section specify coordination requirements agents specific state 
richer representation il mdp variants allows represent possible dependencies agents context specific manner 
qi depends value rules consistent state action pair agent involved qi nj number agents including agent involved rule representation qi regarded linear expansion set basis functions peaked specific state action context may potentially involve agents 
weights basis functions rules values updated follows nj nj ri psfrag replacements qi qi add contribution agent involved rule 
example assume set 
example representation components agents transition state state value rules furthermore assume performed joint action state optimal joint action variable elimination algorithm state conditioning context rules apply state rules apply state graphically depicted fig 

eq 
update value rules state follows 
note order update discounted values 
furthermore component state coordinated action agent agent rule state agent coordinate agent rule 

experiments section apply method problem goal predators capture prey fast possible discrete world kok vlassis 
concentrate 
possible capture position predators 
coordination problem predators toroidal grid capture single prey 
agent move adjacent cells remain current position 
prey captured predators located adjacent cell prey agents moves location prey 
possible capture situation depicted fig 

predators move cell predator moves prey position nearby predator penalized placed random positions field 
policy prey fixed stays current position probability cases moves free adjacent cells uniform probability 
complete state action space problem consists combinations predator positions relative prey joint action predators states 
states predators coordinate actions 
initialize predator set individual value rules include state action predator 
example rule defined prey move payoff value rules initialized value corresponds maximal reward episode 
ensures predators explore possible action combinations sufficiently 
specific coordination requirements predators added 
predators coordinate actions close add extra value rules depending joint action situations manhattan distance predator smaller equal cells predators distance cells prey value rule prey captured situation fig 
looks prey pred move move west results generation value rules predator coordinated states uncoordinated states 
second predator holds set rules uncoordinated states action rules predator coordinated states 
learning eq 
update payoffs rules 
predator receives reward ri helps capture prey negative reward collides predator 
agent moves prey support reward 
cases reward 
greedy exploration step learning rate discount factor 
compare method learning methods mentioned section 
case independent learners value derived state consists position prey predator possible actions 
corresponds different state action pairs agent 
mdp learners model system complete mdp joint action represented single action 
case number state action pairs equals 
fig 
shows capture times learned policy episodes different methods 
results generated running current learned policy interval episodes times fixed set starting configurations 
test episodes exploration actions performed 
repeated different runs 
starting configurations selected randomly runs 
independent learners proposed method learn quickly respect mdp learners learning fewer state action pairs 
independent learners converge single policy keep oscillating 
caused fact take action agent account 
predators located prey predator moves prey position predator able distinguish situation note creating value rules full state information decomposing action space result uncoordinated states capture time mdp learners independent learners manual policy sparse cooperative learning episode 
capture times learned policy different methods episodes 
results averaged runs 
predator remains current position performs actions exploration action 
case positive reward returned second case large negative reward received 
situations value updated 
coordination dependencies explicitly taken account approaches 
mdp learners modeled state results slowly decreasing learning curve takes longer state action pairs explored 
context specific approach quicker decreasing learning curve joint actions considered coordinated states 
see fig 
methods result identical policy 
table shows average capture times different approaches test runs fig 
manual implementation predators minimize distance prey wait till predators located prey 
predators located prey social conventions relative positioning decide predators moves prey position 
context specific learning approach converges slightly higher capture time mdp learners 
explanation small difference fact necessary coordination requirements added value rules 
construction value rules assume agents coordinate located far away method avg 
time values independent learners manual policy sparse cooperative mdp learners table 
average capture time learning averaged episodes number state action pairs different methods 
coordinating states positive influence final result 
constraints added extra value rules learning time increase increased state action space 
clearly trade exists expressiveness model learning time 

discussion discussed learning approach cooperative multiagent systems context specific coordination graphs value rules specify coordination requirements system specific context 
rules regarded sparse representation complete state action space defined subset state action variables 
value rule contributes additively global value updated learning rule adds contribution involved agents rule 
effectively agent learns coordinate neighbors dynamically changing coordination graph 
results predator prey domain show method improves learning time multiagent learning methods performs comparable optimal policy 
approach closely related coordinated reinforcement learning approach guestrin 
approach global value represented sum local functions local function assumes parametric function representation 
main difference update weights local value agent difference global agents current discounted state plus immediate rewards 
approach update function agent rewards values neighboring agents graph 
advantageous subgroups agents need separately coordinate actions 
perspective local learning updates closer spirit local sarsa updates russell 
related approach schneider agent updates local qvalue value neighboring nodes 
weight function determines qvalue agent contributes update value agent just approach function defines graph structure agent dependencies 
dependencies fixed learning process mention possibility dynamically changing 
approach learning involves back propagating averages individual values case qlearning involves back propagating individual components joint values 
applied distributed value function approach predator prey problem weighting function averaged value evenly agents 
policy converge average capture time cycles agents affect uncoordinated states 
instance agent low valued state exploratory action influences individual action taken agent negatively 
directions 
current implementation assumed agents contribute equally rules involved see eq 

investigate consequence choice 
furthermore apply approach continuous domains agent dependencies investigate methods learn coordination requirements automatically 
acknowledgments reviewers detailed constructive comments 
research supported progress embedded systems research program dutch organization scientific research nwo dutch ministry economic affairs technology foundation stw project aes 
claus boutilier 

dynamics reinforcement learning cooperative multiagent systems 
proc 
th nation 
conf 
artificial intelligence 
madison wi 
guestrin 

planning uncertainty complex structured environments 
doctoral dissertation computer science department stanford university 
guestrin lagoudakis parr 

ordinated reinforcement learning 
proc 
th int 
conf 
machine learning 
sydney australia 
guestrin venkataraman koller 

context specific multiagent coordination planning factored mdps 
proc 
th nation 
conf 
artificial intelligence 
edmonton canada 
kok vlassis 

pursuit domain package technical report ias uva 
informatics institute university amsterdam netherlands 
russell 

decomposition reinforcement learning agents 
proceedings th international conference machine learning 
washington dc 
schneider wong moore riedmiller 

distributed value functions 
proc 
int 
conf 
machine learning 
bled slovenia 
sen hale 

learning coordinate sharing information 
proc 
th nation 
conf 
artificial intelligence 
seattle wa 
shapley 

stochastic games 
proceedings national academy sciences 
sutton barto 

reinforcement learning 
cambridge ma mit press 
tan 

multi agent reinforcement learning independent vs cooperative agents 
proc 
th int 
conf 
machine learning 
amherst ma 
vlassis 

concise multiagent systems distributed ai 
informatics institute university amsterdam 
www science uva nl vlassis 
watkins dayan 

technical note qlearning 
machine learning 
weiss 
ed 

multiagent systems modern approach distributed artificial intelligence 
mit press 
