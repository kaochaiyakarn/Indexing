generalizing swendsen wang sampling arbitrary posterior probabilities adrian song chun zhu vision tasks formulated graph partition problems minimize energy functions 
problems gibbs sampler provides general solution slow methods ncut graph cuts computationally effective specific energy forms generally applicable 
new infer ence algorithm generalizes swendsen wang method arbitrary probabilities defined graph partitions 
computing graph edge weights local image features 
algorithm iterates steps 
graph clustering forms connected components cutting edges probabilistically weights 
ii graph relabeling selects connected compo nent flips probabilistically coloring vertices component simultaneously 
realizes split merge re grouping chunk graph contrast gibbs sampler flips single vertex 
prove algorithm simulates ergodic reversible markov chain jumps space graph partitions applicable arbitrary posterior probabilities energy functions defined graphs 
demonstrate algorithm typical problems computer vision image segmentation stereo vision 
experimentally show times faster cpu time classical gibbs sampler times faster ddm cmc segmentation algorithm 
stereo compare performance graph cuts belief propagation 
show algorithm automatically infer generative models obtain satisfactory results better graphic cuts belief propagation amount time 
published proc 
th int conf 
computer vision nice france 
version extended accepted ieee trans 
pami 
departments computer science statistics university california los angeles los angeles ca 
mail ucla edu stat ucla edu computer vision tasks goes component formulated graph partition coloring problem 
example segmentation grouping perceptual organization correspondence stereo motion 
common objective tasks partition various image elements vertices adjacency graph number coherent visual structures bayesian posterior probability energy function optimized 
formulation graph partition increasing number algorithms com puter science modern statistical physics brought computer vision come influential 
prominent method graph spectral analysis normalized cuts variants segmentation grouping mini discriminative energy functions 
second popular method minimum cut graph cut map energy minimization problems maximum flow problems solve low order polynomial time 
third method generalized belief propagation graphs shown minimize approximate energy functions 
methods computationally efficient limited specific forms en ergy functions generally applicable visual inference 
shall address limitations comparison method section 
graph partition problems classic markov chain monte carlo methods gibbs sampler heat bath physics provide general solutions experience slow convergence especially adjacent vertices graph strongly coupled coloring vertices interlocked locally 
illustrates example gibbs sampler flips color single vertex step wait exponentially changing color set coupled vertices 
speed problem gibbs sampler addressed celebrated swendsen wang sw method 
step sw algorithm clusters coupled vertices connected components having color flips color connected component jointly 
classic ising potts models new bounding chain technique developed diagnose convergence sw invariant probability exact sampling furthermore convergence speed markov chain mixing time polynomial graph size sw method valid ising potts models cancelation required deriving sw method observed general probabilities energies 
worse sw slows presence external field data likelihood 
specifically integrates potts model prior probability likelihood bayesian inference slow graph clustering step data 
shall discuss sw method properties details section iii 
generalize sw arbitrary posterior probabilities energy functions derive generic solution graph partition 
basic ideas summarized 
initialization 
adjacency graph compute local discriminative probabilities edge external field prior 
computer vision local image features statistical tests obtain edge weights 
algorithm iterates steps 
ii graph clustering 
current partition coloring removes edges vertices different colors 
remaining edges connects adjacent vertices color turned weight 
discriminative probabilities informative edges object boundaries high chance turned 
obtains number connected components subgraphs having color usually connected components correspond strongly coupled vertices stand parts objects image see 
define swendsen wang cut connected component set edges connect component neighboring vertices color 
word edges swendsen wang cut turned probabilistically 
connected components regarded samples approximation posterior potts model accepted posterior probability step 
iii graph flipping 
selects multiple connected component flips probability driven posterior coloring vertices selected component simultaneously 
realizes split merge re grouping chunk graph contrast gibbs sampler flips single vertex 
flipping procedure automatically change number colors general original sw method works fixed number colors potts model 
shall show new algorithm simulates ergodic reversible markov chain jumps finite space possible graph partitions 
algorithm valid sampling arbitrary posterior probability energy functions 
new algorithm mainly contributions 
firstly generalize sw method perspective metropolis hastings method derive simple ana formula acceptance probability reversible metropolis hastings step 
formula see theorem expressed terms product discriminative proba bilities edges small number swendsen wang cuts 
secondly compute discriminative probabilities edges input image external field physics term 
observe empirically discriminative probabilities con nected components effective comparison uniform probability original sw method 
similar spirit data driven markov chain monte carlo 
thirdly various versions algorithm 
variants direct generalization gibbs sampler 
flips coloring connected component conditional probability rectifying factor flip accepted probability 
demonstrate algorithm typical problems computer vision image seg mentation stereo vision 
image segmentation choose generative image represen tation classes image models 
works times faster cpu time classic gibbs sampler obtains results seconds pc 
stereo matching problem adopt energy function graph cut benchmark comparison 
obtains results better belief propagation minutes image slower graph cuts 
computing speed certainly depends discriminative probabilities problem domain 
optimization prob lems method uses simulated annealing quicker schedule gibbs sampler sweeps opposed sweeps start high initial temperature 
algorithm start initial solutions speed convergence 
compare method graph partition algorithms computer vision 
distinct graph spectral analysis normalized cuts 
argue discriminative energies discriminative grouping clustering algorithms difficulties expressing global visual patterns shading effects perspective projection effects contour closure furthermore natural images contain diverse visual structures coherent different ways single discriminative criterion generally applicable correctly partition visual structures images 
example criterion prefers compact regions break elongated curve patterns 
need generative bayesian formulation incorporating number diverse competing image models 
family models explains pattern generated stands coherence criterion 
example families models texture color shading clutter regions image segmentation 
algorithm uses ncut type discriminative probabilities edges making proposals accepted rejected bayesian posterior probability incorporates families image models global prior knowledge 
second graph cut minimum cut algorithms effective minimizing energy functions shown limited classes energy functions mapped maximum flow problems 
example far methods applicable generative models multiple classes image models 
third method addition data driven markov chain monte carlo ddmcmc algorithm segmentation parsing solves bayesian inference mixing number reversible jumps 
jumps divided types 
type solves sub tasks model selection switching fitting 
ddmcmc algorithm computes discriminative models color texture clustering expresses form non parametric probabilities drive jumps 
type ii solves goes sub tasks grouping segmentation corre 
swendsen wang cut algorithm improves type ii jumps theoretical formulation computational speed 
speeds ddmcmc algorithm times segmentation 
shall focus type ii reversible jumps graph partition space 
omit discussion model spaces type jumps referred 
organized 
bayesian formulation graph partition section ii 
discuss difficulties sampling graph partitions introduce original sw algorithm section iii 
section iv presents new swendsen wang cut algorithm variants 
show groups experiments section image segmentation stereo matching 
section vi concludes discussions advanced topics extending analyzing swendsen wang cuts 
bayesian formulation ii 
bayesian formulation graph partition consider adjacency graph go eo vn set nodes need partitioned atoms pixels edge elements image primi tives atomic regions nearly constant intensities eo set edges connecting neighboring nodes 
partition graph denoted vn vi vi vj 
visual structures coherent different ways subset vi assigned color ci represents model usually consisting type constant spline parameters 
objective compute world representation input cn optimization problem maximizing bayesian posterior probability minimizing energy solution space arg max arg min 
choose typical vision problems examples 
denote iv image attributes vertex iv attributes set example image segmentation shown 
vertex atomic region nearly constant intensity iv intensity 
partitioned subset vi corresponds coherent region ri model ci type image model model parameters 
adopt types simple image models prior probability section 
usually models color texture input image atomic regions go segmentation fig 

image segmentation graph partition 
input image 
atomic regions canny edge detection followed edge tracing contour closing vertex graph go 
segmentation result 
shading implemented ddmcmc 
likelihood ivi ivi may different dimensions different types models 
second example stereo matching 
graph go pixel lattice iv left right image intensity ci disparity vi discretized epipolar line ci dmax 
energy function formulated section 
applied sw cuts algorithm motion ci ui vi motion velocity ci vector includes motion image segmentation 
algorithm curve grouping 
solution space markov chain jumps subsection consider structures solution space necessary markov chain design optimization space 
place graph partition optimization 
eqn 
denote space possible partitions set types image models model parameter space family type solution space 
factorization space corresponds types moves necessary exploring entire space 

type moves selecting model fitting model parameters vi model fitting omitted stereo matching experiment 
usually quantize model spaces finite 

type ii goes moves grouping segmentation correspondence partition space finite space 
types moves tightly coupled implement number reversible jumps simulate markov chain searches space 
markov chain starts initial solution wo designed unique invariant stationary probability 
suppose denote state probability markov chain time pt wo 
classic measure convergence total variation pt wo tv pt wo 
measure speed algorithm mixing rate minimum time markov chain come close stationary probability wo max min pt wo tv 
wo usually go function graph size go number vertices edges 
algorithm said rapid mixing polynomial logarithmic 
shall study type ii moves omit type moves discussed ddmcmc algorithm 
interesting note human brain mapping study shows recognition task type handled dorsal stream spatial vision type ii processed ventral stream 
iii 
gibbs sampler swendsen wang method limitations section discuss gibbs sampler original swendsen wang algorithm graph partition set background 
difficulty graph partition gibbs sampler fig 

difficulty sampling ising potts models 
difficulty sampling partition space reflected simple ising potts model vision prior model 
shows string spins label color 
potts model may colors 
ising potts model exp cs ct 
eo cs ct cs ct adjacent vertices zero 
obviously highest probability achieved vertices label 
best visiting scheme suppose single site update algorithm gibbs sampler flips spins cracks 
probability flipping spin po 
flip string spins successfully expected number steps po exponential waiting typical general graph partition 
intuitively desirable flip big set vertices color step 
course need ensure moves keep stationary probability 
swendsen wang method 
swendsen wang potts models theoretical results ways interpret sw algorithm including random cluster model auxiliary variables slice sampling decoupling 
interpret sw method metropolis hastings step interpretation leads generalizing arbitrary probabilities section iv 
fig 

sw algorithm flips color set vertices vo step ising potts models 
set edges marked crosses called swendsen wang cut 
consider potts model eqn 
lattice 
shows partition states vo vo differ labels vertices vo inside center window 
sw algorithm realizes reversible move single step 
state sw algorithm proceeds way 
edge eo removed cs ct cs ct turned probability qo turned removed 
yields number connected components subset vertices color 

randomly selects connected component vo resulting graph see left 
dark edges remain edges turned 

chooses label vo uniform probability 
example vo change color black white obtain partition state right 
reversely state chance select vo flip black color return swendsen wang cuts sets edges connecting vo respectively marked crosses 
ca vo vo cb vo vo 
state combinatorial number ways connected component cases edges ca cut probabilistically 
similarly state edges cb turned order vo connected component 
look moves states perspective metropolis hastings method 
computationally difficult compute proposal proba bilities compute ratio easily cancelation 
qo ca qo cb qo ca cb 
ca cardinality set ca 
remarkably probability ratio potts model decided swendsen wang cuts cb ca cb ca acceptance probability move min qo ca cb take qo proposal accepted 
vo selected new color picked random having go metropolis hastings step due cancelation 
inverse temperature potts models lower temperature qo sw flips larger patch time 
potts models eqn 
huber developed new bounding chain technique diagnose convergence sw exact sampling perfect sampling 
number steps reaching exact sampling order log eo temperature far far critical temperature 
path coupling technique cooper frieze showed mixing time see eqn 
polynomial vertex graph go connected number neighbors connectivity vertex grow size usually observed vision problems lattice 
mixing time exponential worst case go fully connected 
case may occur vision problems 
excitement sw algorithm limited reasons 

restricted ising potts models posterior probabilities vision tasks complex forms 

slow potts models presence external fields data 
qo constant utilize input data clustering connected components 

assumes number labels fixed 
markov chain create new labels cases unknown vision usually number models unknown 
ties 
section overcome limitations extend sw arbitrary iv 
graph partition swendsen wang cuts discriminative probabilities edges running reversible jumps augment adjacency graph go eo discriminative probabilities initial stage 
partition samples obtained probabilities subsection proposals full posterior probability 
vertex extract number features fa edge eo assign binary random variable 
indicates edge turned 
compute discriminative probability qe local features 
take adjacency graph example 
atomic region vertex go compute bin intensity histogram normalized 
edge vi vj define qe hi hj kl hi hj kl hj hi kl kullback leibler divergence histograms temperature factor 
usually qe close zero object boundary 
suppose turn edges independently qe eo obtain sparse graph probability qe eo qe 
fig 

random clustering adjacency graph independent discriminative models edges 
uniform region connected component 
consists number connected components 
shows examples go 
region uniform grey level connected component consists number atomic regions 
show random partitions sampled temperatures 
reasonable temperature various parts cheetah obtained legs body tail connected components proposed candidates partition reversible jumps 
example shows discriminative models heuristics partition 
partitions limited local features 
complex posterior probabilities global generative models needed accept proposals done 
swendsen wang cuts variants go cp fig 

stages graphs algorithm 
adjacency graph go graph current partition coloring connected components cp turning edges swendsen wang cut algorithm engages types graphs shown 
starts adjacency graph go eo fig 
time step partition vn assigns color vertex cv obtain graph fig cs ct 
edge turned probability qe independently obtain sparse graph cp number connected components 
version swendsen wang cut algorithm 
swendsen wang cut swc input go eo qe eo posterior 
output samples 

initialize partition random clustering see fig 

repeat current state vn 
turn probability qe 

divided connected components 
collect connected components set cp 

select connected component vo cp prob 
vo cp say vo 
usually vo cp cp uniform fig example vo partition 

propose assign vo new label probability vo obtain fig merged existing color fig vo assigned new color 

accept proposal probability defined theorem 
proposal probability vo uniform better dependent similarity vo vl 
step model switching fitting type jumps performed deterministically sampled proposal probabilities see section 
cp state cp state cp state fig 

reversible move partition states differ color 
vertices connected thick edges form connected component 
thin lines marked crosses edges sw cuts 
algorithm vo vo move realize types moves depending choice new color vo 
number color decided automatically 

re grouping vo split merged existing color 
number colors unchanged 
fig 

adjacent fact discrete version boundary evolution region competition 

splitting vo split new color 
fig 

merging vo merged existing color 
fig 

second version algorithm differs way selects set vo 
sampling edges current partition starts single vertex seed grows connected component vo 
swendsen wang cuts swc 
repeat current state vn 
select seed vertex say 
set vo 
repeat vo vo vo vo 
vo vo vo vo 

turn probability qe 
set vo vo 

propose assign vo new label prob 
vo 

accept move probability defined theorem 
compute acceptance probability swc swc 
start computing probability ratio selecting vo 
theorem pair reversible partition states differ coloring vo vo vo vo vo vo vo qe vo 
vo vo qe 
vo vo qe proof see appendix important step obtaining acceptance probability 
states fact combinatorial number ways selecting vo probability ratio simple due cancellations 
theorem notation acceptance probability move vo vo qe min vo vo qe vo vo 
proof metropolis hastings method acceptance probability min re grouping case see fig 
path moving states selecting flipping vo 
vo vo vo 
vo follows straight theorem 
splitting merging case see fig 
paths 
put proof append clarity 
partition space finite markov chain swc ergodic observation non zero probability node chosen vo assigned new color 
markov chain move partition partition non zero probability steps 
include type moves model selecting fitting augment move partitions states state set vo image model set image model 
state vo split merged 
set vo new model set vo model obtained sampling proposals iv vo iv vo respectively 
acceptance probability min iv iv iv vo iv vo dimensions model parameters matched ratio 
proposal probabilities iv set product discriminative probabilities vertices 
computed bottom step data clustering see 
swc generalized gibbs sampler design probability vo achieve acceptance probability 
third version algorithm named swc generalized gibbs sampler 
vn current partition vo connected component color choices 
vo merged sets sl sn vn sn 
assigning possible partitions vl vo denote respectively 
vo merged si 
partitions may colors 
clarity notation 
denote swendsen wang cuts vo sj ci vo si vo ci vo vo 
number edges sw cuts fixed regardless number colors denote weight partitions qe 
cj theorem partition vo posterior probability partition choose new color vo vp ip mj jp proposed move accepted probability 
proof partitions acceptance probability theorem min 
denominator eq 

intuitively pick vo merge vo si posterior probability modified sw cut factor ci qe ensure markov chain follows posterior 
vo single site ci reduces gibbs sampler 
get third version swc algorithm swendsen wang cuts swc 
repeat current partition vn 

select candidate set vo swc swc 
draw random sample probability vo 
merge si comparison swc computationally costly evaluate posteriors step 
limit number color sets adjacent vo 
swc smaller computational cost swc tests small number edges graph clustering step 
swc choose initial seed vertex goodness fit avoid picking large components time 
experiments segmentation stereo section apply sw cut algorithms classical vision problems image segmentation stereo matching 
optimizing posterior probability needs simulated annealing procedure raises posterior probability certain power called temperature 
temperature slowly decreased cooling schedule 
initial temperature tmax big order avoid stuck local minima reduced tmin number sweeps sweep steps 
initial temperature tmax depends efficiency algorithm 
shows empirically gibbs sampler needs high initial temperature decrease slowly sweeps order reach solutions 
initial solution wo destroyed randomized high temperature 
comparison swendsen wang cuts walk fast low temperature start tmax small usually tmax decrease fast sweeps utilize initial solutions 
temperature tmin range experiments 
experiment image segmentation reduce size adjacency graph canny edge detector edge tracing divide image atomic regions constant intensities 
depending image size texture atomic regions vertex go 
atomic regions helps reduce computational time aware risk able break wrong especially case leakage occurs atomic region big 
overcome problem hierarchic sw cut method works multiple levels adjacency graphs vertices various granularities 
adopt simple image models sophisticated models easily added 
coordinates pixel 
model assumes constant intensity additive noise modeled non parametric histogram 
second model assumes linear function additive noise linear model ax 
third model assumes quadratic function additive noise ax cx dxy ey 
fig 

image segmentation input image atomic regions segmentation result 
selection model studied previous ddmcmc 
models useful fitting smoothness regions global shading effects 
texture modeled non parametric histogram practice represented vector bins hb normalized sum 
region model 
likelihood ir iv nj exp entropy 
nj number pixels fall jth bin histogram 
prior encourage large connected regions 
number regions region may consist sub regions 
denote connected components rm prior fix experiments 
area ri 
model parameters regions computed deterministically step best square fit 
replaced separate steps model fitting switching purpose experiments 
segmentation results obtained swc shown figures 
computational speed comparison compare speed algorithm gibbs sampler figures 
run swc algorithm times cheetah image types initializations 
random initialization assigns random color atomic region independently colors total 
uniform initialization sets atomic regions color 
happens uniform initialization lower energy log random initializations 
achieve low energy level gibbs sampler upper curves fig 
start high temperature logarithmic annealing schedule sweeps remains stuck higher energy level 
contrast swc starts temperature decreases sweeps 
plots energy run function cpu time seconds 
fig 

convergence comparison swc gibbs sampler upper curves cpu time seconds 
left seconds 
right zoomed view seconds 
swc runs times random uniform initializations 
upper curves gibbs sampler random uniform initialization respectively 
swc converges faster plot zoom view seconds 
show swc runs random uniform initializations 
uniform initialization lower energy start swc algorithm converges faster seconds 
contrast gibbs sampler utilize initialization raise temperature high 
study effects discriminative probabilities qe convergence speed compare performance algorithm discriminative probabilities fig 
run swc algorithm times edges having constant probability qe respectively note gibbs sampler equivalent swc qe 
annealing schedules runs slower starting higher temperature obtain final energy 
algorithm reach low energy discriminative models 
fig displays energy vs cpu time seconds runs swc airplane image shown fig 
energies swc runs constant edge probability qe shown dotted lines runs start uniform initialization 
significantly slower swc 
worth mentioning swc runs discriminative probabilities equivalent original sw algorithm general energy function original sw applied 
fig 

left convergence comparison swc solid swc constant edge probabilities qe dotted 
right comparison swc swc second image 
plots cpu time 
swc overhead step slower example 
fig 
compares swc swc 
swc effective swc computational overhead swc move data driven information swc swc existent design vo 
compared ddmcmc algorithm algorithm speed times cpu time 
model fitting switching steps quite simple observed full featured model fitting switching steps take time split merge steps focus algorithm 
incorporating full featured model fitting switching steps algorithm remain times faster ddmcmc 
experiment ii comparison graph cuts belief propagation stereo section compare performance sw cuts graph cuts loopy belief propagation stereo matching benchmark 
pair stereo images il ir assign integer disparity value color cv dv pixel left image 
adjacency graph go simply lattice nearest neighbor connections 
energy benchmark potts model external field dv ds dt external field data term measures goodness intensity match left right images disparity dv dv min min dv dv il ir left image swc result graph cuts result manual truth fig 

stereo matching tsukuba sequence row sawtooth sequence second row 
coefficient prior term dependent inhomogeneous potts model il il 
energy shortcomings 

low level markov random field generative model fitting 
example slanted planes fig 
second row broken pieces 
ii treat half occluded pixels explicitly ground truth higher energy algorithms output see fig 
forced energy order compare graph cut bp type energy minimize 
compare swc graph cuts implementations provided scharstein package extension belief propagation available online 
stereo problem define discriminative probabilities vertices edges get better empirical results 
vertex pixel compute vertex probability dv dv normalized dv dmax 
measures pixel disparity dv local information 
compute marginal probability disparity level edge define edge probability dmax ds dt 
dmax probabilities edge disparity level 
swc step choose disparity level probability edge probability clustering connected component vo 
energy costs contributed boundary pixels due lack half occlusion treatment 
swc seed vertex chosen equal probability boundary pixels sampling goodness fit probability dv dv dv current assigned disparity wish choose pixels assigned disparity level dv lower probability 
grow component vo swc seed propose flip label 
new disparity level color vo chosen probability vo vo vo dt 
fig 
compares energy curves cpu time seconds swc runs different annealing schedules graph cuts belief propagation versions initialized system swc version working atomic regions decreased energy seconds 
swc version working pixel lattice provided final result 
final energy obtained swc final energy graph cuts algorithm tsukuba sequence sequences 
parameters kept experiments 
fig 

performance comparison swc graph cuts belief propagation tsukuba sequence 
curves plot energy cpu time seconds 
energy level indicator quality results ground truth results higher energy algorithms 
experiments show swc reaches lower energy belief propagation slower graph cuts 
release simple energy model eqn adopt generative models piecewise planar surfaces obtain bayesian posterior probability similar left image swc result graph cuts manual truth fig 

bayesian formulation generative models fitting piecewise planar surfaces algorithm obtains better results set stereo images 
running time reduced minutes pc 
segmentation problem experiment algorithm runs minutes obtains better results shown closer ground truth 
run swc algorithm atomic regions run boundary diffusion steps smooth object boundary 
vi 
discussion generic inference algorithm sampling arbitrary probabilities energy functions general graphs extending sw method physics gibbs sampler swc 
method extends sw method metropolis hastings perspective different interpretations literature 
fact early attempts applying sw image analysis partial decoupling concept 
speed sw cut method depends discriminative probabilities edges vertices 
probabilities theoretical analysis convergence difficult 
ongoing projects studying ways bounding sw cut convergence external field data diagnosing exact sampling advanced techniques 
incorporating sw cuts ddmcmc framework image parsing 
acknowledgment supported nsf iis onr 
authors tu wu discussions dr rangarajan anonymous reviewers helpful comments 
zhu graph partition swendsen wang cuts proc 
int conf 
computer vision nice france 
zhu multigrid multi level swendsen wang cuts hierarchic graph partition proc 
ieee conf 
computer vision pattern recognition washington dc 
barker rayner 
unsupervised segmentation images spie conf 
bayesian inference inverse problems pp july 
boykov veksler zabih 
fast approximate energy minimization graph cuts 
ieee trans 
pami vol 
pp cooper frieze 
mixing properties swendsen wang process classes graphs random structures algorithms vol 
pp 

edwards sokal generalization swendsen wang representation monte carlo algorithm phys 
rev lett pp fox exact map states expectations perfect sampling proc 
th int workshop bayesian inference maximum entropy methods france 
gdalyahu weinshall werman self organization vision stochastic clustering image segmentation perceptual grouping image database ieee 
trans 
pami vol 
pp 
geman geman stochastic relaxation gibbs distributions bayesian restoration images ieee trans 
pami vol 
pp 

gore sinclair swendsen wang process mix rapidly stat 
phys 
hastings monte carlo sampling methods markov chains applications biometrika vol 
pp 
higdon auxiliary variable methods markov chain monte carlo simulations preprint inst 
stat 
decision science hofmann buhmann pairwise data clustering deterministic annealing ieee trans 
pami vol 
pp 

huber 
bounding chain swendsen wang random structures algorithms vol pp jain dubes algorithms clustering data prentice hall englewood cliffs nj 
kirkpatrick gelatt vecchi optimization simulated annealing science pp 
kolmogorov zabih energy functions minimized graph cuts proc 
eccv pp 

vol 
copenhagen denmark 
metropolis rosenbluth rosenbluth teller teller equations state calculations fast computing machines chem 
physics pp potts generalized order disorder transformations proceedings cambridge soci pp 
propp wilson 
exact sampling coupled markov chains applications statistical random structures algorithms vol 
pp 

puzicha hofmann buhmann theory proximity clustering structure detection optimization pattern recognition pp 
roy cox maximum flow formulation camera stereo correspondence problem proc 
iccv bombay india 
scharstein szeliski 
taxonomy evaluation dense frame stereo correspondence algorithms ijcv shi malik normalized cuts image segmentation ieee transactions pami pp 

swendsen wang critical dynamics monte carlo simulations physical review letters pp 
freeman comparison graph cuts belief propagation stereo identical mrf parameters iccv tu zhu image segmentation data driven markov chain monte carlo ieee trans 
pami 
tu zhu parsing images region curves curve processes submitting ijcv short version appeared proc 
eccv 
wang relationship ventral stream object recognition dorsal stream spatial vision fmri erp study human brain mapping pp 
winkler image analysis random fields dynamic monte carlo methods page springer verlag 
wolff collective monte carlo updating spin systems physical review letters vol 
pp 

wu leahy optimal graph theoretic approach data clustering theory application image segmentation ieee trans 
pami pp 
yedidia freeman weiss generalized belief propagation merl report tr 
zhu yuille region competition unifying snake balloon region growing bayes mdl energy multi band image segmentation ieee trans 
pami pp 
proof theorem combinatorial number ways selecting vo partitions proposal probabilities ratio vo vo simple due cancellation 
follows compute ratio swc 
ratio derived swc swc steps 
firstly calculate probability vo selecting vo partition vn 
loss generality assume vo 
edges different colors removed set remaining edges denoted eon eo jc vi vj 
edge eon turned probability qe independently denote edge variables eon eon 
denote sets edges turned respectively eon eon eon 
probability event simply eon qe qe 
clustering step color vi broken number ni connected components 
cp set connected components state 
ways arrive cp depending edge probability 
event denote set connected components cp 
set connected components obtained combinatorial number edge probabilities probability cp cp cp cp 
interested set cp include vo connected components probability choosing vo vo vo cp vo cp 
cp vo vo cp arbitrary say vo cp vo cp 
cp summarize cp vo observe common property edges sw cut vo vo turned vo connected vertices violate premise vo connected component 
vo vo cp vo 
vo vo cp vo 
take common factor summation vo vo vo qe cp vo qe cp eon qe secondly calculate probability vo selecting vo partition loss generality assume vo 
steps vo vo vo qe cp vo cp eon qe qe partitions consecutive swc steps differ coloring vo observations 
cp vo corresponding cp vo cp cp 
furthermore eon eon 
differs sw cuts 
correspondence vo vo obtain ratio canceling common probability eqns 
vo vo vo vo qe 
vo vo qe special case qe qo eo obtain proposal ratio equation original sw method 
proof theorem splitting merging cases re grouping case vo vo way moving select vo 
merging splitting cases paths illustrated fig 

loss generality write vn vn 
paths moving respectively 
fig 

state subgraphs merged state paths choose choose 
path 
choose vo 
state choose merge vo reversely state choose split vo 
path 
choose vo 
state choose merge vo reversely state choose split vo 
proposal probability ratio vo vo vo vo vo vo vo vo 
state sw cut vo vo paths state cut paths 
theorem probability ratios choosing vo vo equal vo vo vo vo 
vo selected vo vo remaining partition denoted vo vo 
proposing new label vo easily observe vo vo vo 
vo acceptance rate theorem follows equations 
