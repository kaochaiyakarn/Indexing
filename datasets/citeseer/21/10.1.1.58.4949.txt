choosing learning algorithms calibrated tests remco bouckaert xm nz remco cs waikato ac nz 
mountain information technology auckland 
computer science department university waikato hamilton new zealand designing hypothesis test determine best machine learning algorithms small data set available simple task 
popular tests suffer low power cv high type error weka cross validation :10.1.1.37.3325
furthermore tests show low level replicability tests performed different scientists pair algorithms data sets hypothesis test may different results 
show cv resampling fold cv suffer low replicability 
main complication due need data multiple times 
consequence independence assumptions hypothesis tests violated 
pose case reuse data causes effective degrees freedom lower theoretically expected 
show calibrate effective degrees freedom empirically various tests 
tests indicating flaw design 
ones show similar behavior 
type error tests mark wide range circumstances show power replicability considerably higher currently popular hypothesis tests 

choosing learning algorithms single dataset trivial task 
straightforward approach hypothesis test kind decide reject null hypothesis algorithms perform 
just limited amount data available reuse data get number samples provides indication difference accuracy algorithms 
values calculate sort statistic statistic hypothesis test normally assume samples independent 
know completely independent partly data 
practice result performing test type error differs considerably desired significance level observed previous :10.1.1.37.3325
effects having dependence samples estimated variance lower actual variance way overcome defect compensate variance estimate 
article look effect degrees freedom lower theoretically expected number 
compensate calibrating degrees freedom 
section various known hypothesis tests variations introduced detail followed tests repeated cross validation section 
section sets procedure calibrating hypothesis tests actual degrees freedom tests 
section study empirically behavior various tests varying parameters environment tests applied 
finish concluding remarks recommendations pointers research 

hypothesis tests describe hypothesis tests fold cross validation resampling corrected resampling 
cross validation test considered undesirable low replicability see table making hard justify extra involved setting experiments 
degrees freedom explicit formulas usually indicate df proceedings twentieth international conference machine learning icml washington dc 
clear changes df fixed value differs default 

fold cross validation fold cross validation data split approximately equal parts algorithms trained data fold fold 
accuracy estimated data fold left test set 
gives accuracy estimates algorithms denoted pa pb fold left 
xi difference xi pa pb mean xi normally distributed algorithms folds sufficiently large containing cases 
mean simply estimated xi un biased estimate variance xi statistic approximating distribution df degrees freedom df dietterich demonstrated empirically fold cross validation test slightly elevated type error default degrees freedom :10.1.1.37.3325
section calibrate test range values df selecting gives desired type error 

resampling corrected resampling resampling repeatedly splitting training data randomly training fraction testing remaining section applying paired test 
popular way determine algorithm performance demonstrated unacceptable high type error 
pa pb accuracy algorithm algorithm measured run xj difference xj pa pb mean xj estimated xj variance estimated xj get unbiased estimate multiplied nadeau bengio observe high type due underestimation variance samples independent 
propose correction estimate variance multiply fraction data training fraction testing 
altogether gives statistic approximating distribution df degrees freedom resampling corrected resampling 
xj df xj df 
multiple run fold cross validation experiments demonstrate tests previous section suffer low replicability 
means result researcher may differ doing experiment data hypothesis test different random splits data 
higher replicability expected hypothesis tests utilize multiple runs fold cross validation 
various ways get data describe help 
illustrates results run fold cross validation experiment inside box left half practice run fold cross validation appropriate 
cell contains difference accuracy algorithms trained data folds measured data single fold left test data 
data approach naive way data 
considers outcomes independent samples 
mean folds test averages cells single fold cross validation experiment considers averages samples 
test numbers right column 
averages known better estimates accuracy 
mean runs test averages cells fold number numbers row top matrix 
better way sort folds averaging 
sorting numbers folds gives matrix bottom 
mean sorted runs test uses averages runs sorted folds 
mean folds average var test uses alternative way estimate variance mean folds test 
estimating variance directly numbers accurate estimate variance may formal description elaborated motivation tests 

example illustrating data averaging folds runs sorted runs 
table 
overview hypothesis tests multiple run fold cv 
test mean variance df data xij xij folds folds averaged var runs xi runs averaged var sorted runs ij sorted runs averaged var 
folds averaged df runs averaged sorted runs averaged obtained averaging variances obtained data fold cv experiments 
idea extended mean run mean sorted run test giving rise mean run averaged var mean sorted run averaged var tests 
mean folds average test uses similar idea mean folds averaged var test averaging just variances averages test statistic obtained individual fold experiment 
idea extended mean run mean sorted run tests giving mean run averaged mean sorted run averaged tests 
formally runs folds learning schemes accuracy aij bij fold run 
difference accuracies xij aij bij 
short notation xij xi 
xij 
short notation xij xij xi 
test sorting ordering ij 
df xi 
df xi 
df df df df 
ij xi 
table gives overview various tests way calculated 
tests mean variance estimated test statistic calculated 
evident table averaged tests slightly different approach 

calibrating tests tests calibrated type error generating random data sets independent binary variables 
class probability set value typically generates highest type error 
training sets learning algorithms naive bayes implemented weka 
nature data sets ensures outperformed 
test indicates difference algorithms contributes type error 
test run degrees freedom ranging 
degrees freedom type error measured closest equal significance level chosen calibrated value 

degrees freedom left type error percentage right coincide significance level folds various numbers runs 

legend 
note experiments stratification different runs 
stratification ensures class values evenly distributed various folds tends result accurate accuracy estimates 
left half shows degrees freedom obtained runs 
significance level folds chosen calibrate values fairly literature 
right half shows corresponding type error shows legend 
observed calibrated degrees freedom resampling refer technical report full numeric details results significance level 
corrected resampling consistently increases increasing number runs tests doing 
df resampling lower expected df corrected resampling circa lower expected 
indicates variance correction applied corrected resampling right calibration helps bit 
df fold cross validation constant runs vary different runs 
calibrated df consistently lower expected various significant levels type error close mark 
comparing results significance levels df tests constant different significance level difference attributed sample variations 
df tests varies runs constant higher numbers runs difference 
explained having look type error lower numbers runs little choice values calibrated df 
due variation data df may available type error close value may available 
df mean folds low far expected 
indicates degrees freedom main issue test variance underestimation probably mean runs low glitch runs 
dfs zeros indicating values df type error exceed expected value 
indicates value df issue test 
df sorted runs averaged var average low 
df data folds folds meant runs runs meant sorted runs constantly circa 
tests type error mark indicating calibration df may fixed invalid independence assumptions 
number tests considered low erratic values calibrated df indicate df main issue tests examining results experiments confirm clarity presentation omitted 
tests mean folds mean runs sorted runs averaged var sorted runs averaged 
empirical performance section study behavior various tests empirically measuring type error varying class probability number runs significance level 
type ii error replicability measured learning algorithms considered 

vary class probabilities table shows type error measured calibrated tests independent binary variables class probability range 
total data sets instances 
naive bayes trained data sets 
number table indicates number data sets test indicates performance algorithms differs 
run tenfold cross validation data significance level 
averaged tests tests type error exceeding expected level 
fact type error tends considerably lower lower class probabilities 

vary number runs sections randomly generated bayesian networks generate table 
type error range class probabilities percentages test resampling cor resampling fold cv data folds averaged var folds averaged runs averaged var runs averaged sorted runs table 
average accuracies test data training data sets percentages 
algorithm set set set set naive bayes difference data sets instances see details resulting different performances naive bayes 
table shows properties data sets learned data sets measured single instances test set 
set set calibrating tests 
shows type error various numbers runs folds cross validation significance level 
tests calibrated data set comes surprise type error close targeted 
power tests measured data sets outperforms naive bayes increasing margin see table 
shows power tests data sets 
observations note fold cv test vary increasing numbers runs conceptually uses run ignores 
power resampling corrected resampling tests increases increasing number runs 
increased runs amount datapoints increases results better power 
interestingly calibrated tests data power close tests 
indication calibration job compensating dependency items test 

result multiple runs percentages see legend 
type error set power set power set power set tests show little sensitivity number runs power decreases slightly increasing number runs 
may due small variability calibrated df 

vary significance level datasets measure sensitivity tests significance level 
tests run folds 
table shows type error significance levels 
surprisingly type error close expected levels tests calibrated set 
observations increasing significance level increases power comes course cost increased type error 
large gap resampled corrected resampled fold cv test hand tests hand 
tests data folds considerably better power runs averaged test best top tests listed 
slightly elevated type error significance level 
data test top tests listed 
tests power mutually close indication calibration compensates appropriately dependence samples 

measuring replicability section shows results measuring replicability 
test run times data set dif refer measurements power set 
table 
performance various significance levels percentages 
type error set test resampling cor resampling fold cv data folds averaged var folds averaged runs averaged var runs averaged sorted runs ferent random splits 
table shows replicability defined number times tests reject reject null hypothesis 
minimum data sets listed meaningful location replicability lowest typically area decisions need completely obvious 
table shows minimum columns calculated table significance levels 
observations big difference tests rest 
tests data folds show better replicability 
judging column table data highest replicability 
little variance replicability tests data folds 
table 
replicability defined fraction tests having outcome times different partitionings percentages 
tests calibrated cv 
set set set set min 
resampling cor resampling fold cv cv data folds averaged var folds averaged runs averaged var runs averaged sorted runs table 
replicability defined fraction tests percentages having outcome times different partitionings different significance levels 
test resampling cor resampling fold cv data folds averaged var folds averaged runs averaged var runs averaged sorted runs indicates calibrating compensates appropriately dependence samples 
table shows significance level major impact replicability tests data folds tests considerably affected 
cv test particular low replicability appears clearly set 
conservative nature test set set outcome clear powerful tests set role 
reason low replicability outcome test dominated accuracy estimate outcome single run cv experiment vary greatly table shows 
calibration repair problem 

learning algorithms naive bayes chosen calibration algorithms completely different principles dependence learning algorithm influences test minimally 
algorithms sufficiently fast perform large number experiments 
see tests calibrated algorithm perform algorithms compared nearest neighbor tree augmented naive bayes voted perceptron default settings imple table 
type error percentage set various algorithms cv data test 
binary nb nn tan naive bayes nb nearest neighbor nn tree 
nb tan voted perceptron mented weka 
table shows type error significance level run fold data test 
table fairly representative fold cv tests tests type error differs absolute value table 
type errors acceptable suggesting calibrated test wider range algorithms 

studied calibrating degrees freedom hypothesis tests way compensate difference desired type error true type error 
empirical results show tests 
ones show surprisingly similar behavior varying parameters number runs significance level indication incorrect numer degrees freedom cause difference observed theoretical type error 
furthermore calibrated tests show high replicability particular compared cross validation corrected resampling fold cross validation 
choosing algorithms recommend time repeated fold cross validation test individual accuracies estimate mean variance degrees freedom binary data 
conceptually simplest test properties calibrated repeated fold cross validation tests 
furthermore empirically outperforms cross validation corrected resampling fold cross validation power replicability 
research required confirm test performs non binary data 
ways techniques article generalized 
example sufficient data data set justify normality assumption test sign test may applicable 
benefit sign tests assumptions need distribution variable sampled difference accuracies case 
calibrated tests purpose 
main limitation pairwise approach algorithms 
practice multiple algorithms available choose 
gives rise called multiple comparison problems choosing learning algorithms 
suppose algorithms perform domain probability algorithms outperform significantly increases just flipping coin multiple times increases change throwing head times row 
similar issues arise comparing algorithms multiple data sets multiple algorithms multiple data sets 
want eibe frank stimulating discussions topic members machine learning group university waikato 
bouckaert 
choosing learning algorithms calibrated tests 
working computer science department university waikato 
dietterich 
approximate statistical tests comparing supervised classification learning algorithms 
neural computation 
jensen cohen 
multiple comparisons induction algorithms 
machine learning 
john langley 
estimating continuous distributions bayesian classifiers 
proceedings eleventh conference uncertainty artificial intelligence 
pp 
morgan kaufmann san mateo 
kohavi 
study cross validation bootstrap accuracy estimation model selection 
proceedings fourteenth international joint conference artificial intelligence pages 
san mateo ca morgan kaufmann 
mitchell 
machine learning 
mcgraw hil 
nadeau bengio 
inference generalization error 
advances neural information processing systems mit press 
quinlan 
programs machine learning morgan kaufmann publishers san mateo ca 
wild weber 
probability statistics 
department statistics university auckland new zealand 
salzberg 
comparing classifiers pitfalls avoid recommended approach 
data mining knowledge discovery 
witten frank 
data mining practical machine learning tools techniques java implementations 
morgan kaufmann san francisco 
