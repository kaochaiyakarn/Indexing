statistical model general contextual object recognition peter de freitas barnard dept computer science university british columbia vancouver canada cs ubc ca dept computer science university arizona tucson arizona cs arizona edu 
consider object recognition process attaching meaningful labels specific regions image propose model learns spatial relationships objects 
set images associated text keywords captions descriptions objective segment image crude sophisticated fashion find proper associations words regions 
previous models limited scope representation 
particular fail exploit spatial context images words 
develop expressive model takes account 
formulate spatially consistent probabilistic mapping continuous image feature vectors supplied word tokens 
learning word region associations object relations proposed model augments scene segmentations due smoothing implicit spatial consistency 
context introduces cycles undirected graph rely straightforward implementation em algorithm estimating model parameters densities unknown alignment variables 
develop approximate em algorithm uses loopy belief propagation inference step iterative scaling pseudo likelihood approximation parameter update step 
experiments indicate approximate inference learning algorithm converges local solutions 
experiments diverse array images show spatial context considerably improves accuracy object recognition 
significantly spatial context combined nonlinear discrete object representation allows models cope segmented scenes 
peter computer vision community invested great deal effort problem recognising objects especially years 
attention paid formulating understanding general object recognition properly isolating identifying classes objects ceiling polar bear agent environment 
say object recognised labeled concept appropriate consistent fashion 
allows propose practical answer question object object semantic concept case noun image 
pursuing general object recognition may appear premature unconstrained object representations remain elusive 
maintain principled exploration simple learned representations offer insight direction 
approach permits examination relations high level computer vision language understanding 
ideally train system images objects properly segmented accurately labeled 
collection supervised data manually labeling semantically contiguous regions images time consuming problematic 
require captions image level image region level result large quantities data disposal thousands corel images keywords museum images meta data news photos captions internet photo stock agencies 
previous shows reasonable loosely labeled data problems vision image retrieval 
stress annotations solely testing training data includes text associated entire images 
cost longer exact associations objects semantic concepts 
order learn model annotates labels classifies objects scene training implicates finding associations alignments correspondence objects concepts data 
result learning problem unsupervised semi supervised 
adapt unsupervised problem learning lexicon aligned bitext statistical machine translation general object recognition proposed 
data consists images paired associated text 
image consists set blobs identify objects scene 
blob set features describes object 
note imply scene necessarily segmented easily implement scale invariant descriptors represent object classes 
abstractly consists bag semantic concepts describes objects contained image scene 
time restrict set concepts english nouns bear floor 
see fig 
examples images paired captions 
major contributions 
contribution address limitation existing approaches translating image regions words assumption blobs statistically independent usually simplify computation 
model relaxes assumption allows interactions blobs markov random statistical model contextual object recognition building grass sky crab rock boat water sky house trees snow fig 

examples images paired captions 
crucial point model learn word belongs part image 
field mrf 
probability image blob aligned particular word depends word assignments neighbouring blobs 
due markov assumption retain structure 
introduce relations different scales hierarchical representation 
dependence neighbouring objects introduces spatial context classification 
spatial context increases expressiveness words may indistinguishable low level features colour sky water neighbouring objects may help resolve classification airplane 
context alleviates problems caused poor blob clustering 
example birds tend segmented parts inevitably get placed separate bins due different colours 
contextual model learn occurrence blobs increase probability classifying bird appear scene 
experiments sect 
confirm intuition spatial context combined basic nonlinear decision boundary produces relatively accurate object annotations 
second propose approximate algorithm estimating parameters model completely observed partition function intractable 
previous detection man structures mrfs pseudo likelihood parameter estimation go consider unsupervised setting learn potentials labels 
algorithms loopy belief propagation algorithm theoretical guarantees convergence empirical trials show reasonably stable convergence local solutions 
third discuss contextual model offers purchase image segmentation problem 
segmentation algorithms commonly segment low level features insufficient forming accurate boundaries objects 
object recognition data semantic information form captions reasonable expect additional high level information improve segmentations 
barnard show translation models suggest appropriate blob merges word predictions 
instance high level groupings link black white halves penguin 
spatial consistency learned semantic information smooths labellings proposed contextual model learns cope segmented images 
fact model plausible strategy start image grid patches segmentations emerge part labeling process see fig 

peter specification contextual model introduce notation 
observed variables words wn 
blobs bn 
paired image document mn number blobs regions image ln size image 
blob image need align word attached 
unknown association represented variable anu anu blob corresponds word wni 
sets words blobs alignments documents denoted respectively 
wni represents separate concept object set 
total number word tokens 
results suggest representation mixture gaussians facilitates data association task improves object recognition performance 
retain blob discretisation proposed scales better large data sets find model computation easier manage 
means assign blob feature space rf clusters 
number features number blob tokens 
translation lexicon table entries denotes particular word token denotes particular blob token 
define table potentials describing relation blob annotations 
define spatial context symmetric 
set model parameters 
set cliques document denoted cn 
complete likelihood documents mn anu zn anu cn define translation spatial context clique potentials ln anu ln ln anu wni anu wni anu zn partition function disjoint graph document indicator function anu anu 
example representation single document shown fig 

model computation spatial context improves expressiveness comes elevated computational cost due cycles introduced undirected graph 
variation expectation maximisation em computing approximate maximum likelihood estimate 
step loopy belief propagation statistical model contextual object recognition fig 

sample markov random field blob sites 
omitted subscript 
potentials defined vertical lines horizontal lines 
corresponding pseudo likelihood approximation 
complete likelihood compute marginals anu anu 
partition function intractable potentials cliques complete parameter estimation step difficult 
iterative scaling works arbitrary exponential models saving grace convergence exponentially slow 
alternative maximum likelihood estimator pseudo likelihood maximises local neighbourhood conditional probabilities sites mrf independent sites 
conditionals neighbourhoods vertices allow partition function decouple render parameter estimation tractable 
pseudo likelihood neglects long range interactions empirical trials show reasonable consistent results 
essentially pseudo likelihood product undirected models undirected model single latent variable anu observed partner conditioned variables markov blanket 
see fig 
example 
pseudo likelihood approximation mn anu anu set blobs adjacent node partition function neighbourhood site document iterative scaling allows tractable update step bounding log pseudo likelihood 
take partial derivative tractable lower bound respect model parameters resulting equations mn ln wni anu mn ln wni anu peter mn mn ln ln ln wni anu ln wni anu take anu estimate alignment anu conditional empirical distribution current parameters 
find conditionals run universal propagation scaling ups pseudo likelihood site nu neighbours clamped current marginals 
ups exact undirected graph neighbourhood tree 
note requires estimation blob densities addition alignment marginals 
partial derivatives decouple expect feature counts number cliques site neighbourhood 
observing polynomial expressions term degree find new parameter estimates plugging solution update new cadez smyth prove gradient pseudo likelihood respect global parameter conditioned unique positive root 
large data sets updates slow 
optionally boost step additional iterative proportional fitting ipf step converges faster doesn bound gradient log likelihood 
permitted perform ipf update associated clique neighbourhood 
ipf update new mn ln wni anu mn ln wni anu 
stabilise parameter updates place weak priors respectively 
find near uninformative prior works caution prior selection mrfs notoriously difficult 
experiments experiments compare models 
dind discrete translation model proposed assumes object annotations independent 
contextual model developed 
evaluate object recognition results data sets composed variety images examine effect different segmentations performance models 
composed sets denoted data set training images test images words training set 
data set contains total annotated images experiment data available www cs ubc ca 
statistical model contextual object recognition divided training test sets numbering size 
training set total distinct concepts 
frequencies words labels manual annotations shown fig 

consider scenarios 
normalized cuts segment images 
second scenario take object recognition task aid sophisticated segmentation algorithm construct uniform grid patches image 
examples segmentations shown fig 

choice grid size important features scale invariant 
patches approximately th size image smaller patches introduce noise features larger patches contain objects 
scenarios denoted grid 
blobs described simple colour features 
vertical position simple useful feature discrete models means clustering tends poor 
number blob clusters significant factor small classification non separable large finding correct associations near impossible 
rule thumb 
relative importance objects scene task dependent 
ideally collecting user annotated images evaluation tag word weight specify prominence scene 
practice problematic different users focus attention different concepts mention fact burdensome task 
rewarding prediction accuracy blobs objects reasonable performance metric matches objective functions translation models 
compare models evaluation procedures proposed 
prediction error mn mn anu max nu max nu model alignment highest probability anu ground truth annotation 
caveat regarding evaluation procedure segmentation scenarios directly comparable manual annotation data slightly different grid 
testing purposes annotated image segments scenarios hand 
expect segmentation methods perfectly delineate concepts scene single region may contain subjects deemed correct 
normalized cuts segments frequently encompassed objects uniform grid segments virtue smaller regularly contained single object 
result evaluation measure report error scenarios actual fact uniform grid produces precise object recognition 
address role segmentation object recognition faithfully process building data sets ground truth annotations segmentations 
compares model annotations trials different 
model dind took order minutes converge local minimum log likelihood model generally took peter fig 

prediction error models grid segmentations data sets 
results displayed box whisker plot 
middle line box median 
central box represents values percentile upper lower statistical medians 
horizontal line extends minimum maximum value excluding outside far values displayed separate points 
dotted line top random upper bound 
contextual model introduced substantively reduces error dind grid segmentation case 
statistical model contextual object recognition precision data set grid label annotation dind pr 
pr 
word train test train test train test train test airplane astronaut atm bear bill bird building cheetah cloud coin coral crab dolphin earth fish flag flowers fox goat grass hand map mountain person rabbit road rock sand shuttle sky snow space tiger tracks train trees trunk water whale wolf zebra totals fig 

columns list probability finding particular word image manually annotated image region data grid segmentation 
final columns show precision models dind averaged trials 
precision defined probability model prediction correct particular word blob 
precision minus error equation total precision training test sets matches average performance shown fig 

variance precision individual words table 
note words appear training test sets 
underline fact agent access test image labels information annotation column 
peter fig 

algorithm learned contextual relations data grid segmentation matrix averaged learning trials 
darker squares indicate strong neighbour relationship concepts 
white indicates words observed 
example fish goes coral 
intriguing planes go buildings 
hours learn potentials 
striking observation contextual model shows consistently improved results dind 
additionally variance high despite increase number parameters lack convergence guarantees 
recognition grid segmentation tends better normalized cuts results keeping mind require grid annotations precise discussed 
suggests achieve comparable results expensive segmentation step 
cautious strong claims utility sophisticated low level segmentations object recognition uniform evaluation framework examined segmentation methods sufficient variety detail 
shows precision individual words grid experiment averaged trials 
shown figures noticed considerable variation individual trials words pre statistical model contextual object recognition high precision 
example model grid segmentation predicts word train average success precision individual trials ranges training set 
significantly spatial context model tends better words described simple colour features building bear airplane 
depicts potentials grid experiment 
note table symmetric 
diagonal dark words appear 
strong diagonal acts smoothing agent segmented scenes 
high affinities logical airplane cloud defy common sense trees trunk weak affinity due incorrect associations blobs words 
selected annotations predicted model training test sets displayed fig 

instances observers dind annotations appealing precision model higher 
part tends accurate background sky observers prefer getting principal subject airplane correct 
suggests explore alternative evaluation measures decision theory subjective prior knowledge 
discussion showed spatial context helps classify objects especially blob features ineffective 
poorly classified words may easier label paired easily separable concepts 
spatial context insecure predictions acts smoothing agent scene annotations 
pseudo likelihood approximation allows satisfactory results precisely gauge extent skews parameters suboptimal values 
intuition gives undue preference diagonal elements spatial context potentials 
normalized cuts widely considered produce segmentations scenes surprisingly experiments indicate crude segmentations equally better object recognition 
consideration results sensible 
attempting achieve optimal balance loss information compression data association mutual information blobs labels 
normalized cuts settings tend fuse blobs containing objects introduces noise classification data 
cope lower levels compression performs better smaller segments ignore object boundaries 
model fuses blobs high affinities claim small step model learns scene segmentations annotations concurrently 
couple considerable obstacles development model design efficient training algorithms creation evaluation schemes uniformly evaluate quality segmentations combined annotations 
peter fig 

selected model annotations training top test sets bottom 
important emphasize model annotations probabilistic clarity display classification highest probability 
predictions image information 
yee teh discussions parameter estimation graphical models kevin murphy advice belief propagation 
knowledge invaluable financial support iris nserc 
statistical model contextual object recognition 
barnard duygulu forsyth clustering art 
ieee conf 
comp 
vision pattern recognition 
barnard duygulu forsyth de freitas blei jordan matching words pictures 
machine learning res vol 

barnard duygulu guru forsyth effects segmentation feature choice translation model object recognition 
ieee conf 
comp 
vision pattern recognition 
barnard forsyth learning semantics words pictures 
intl 
conf 
comp 
vision 
berger improved iterative scaling algorithm gentle 
carnegie mellon university 
besag statistical analysis dirty pictures 
royal statistical society series vol 

blei jordan modeling annotated data 
acm sigir conf 
research development information retrieval 
sarkar framework performance characterization intermediate level grouping modules 
ieee trans 
pattern analysis machine intelligence vol 

brown della pietra della pietra mercer mathematics statistical machine translation parameter estimation 
computational linguistics vol 

cadez smyth parameter estimation inhomogeneous markov random fields pseudolikelihood 
university california irvine 
de freitas gustafson thompson bayesian feature weighting unsupervised learning application object recognition 
workshop artificial intelligence statistics 
schmid selection scale invariant neighborhoods object class recognition 
intl 
conf 
comp 
vision 
duygulu barnard de freitas forsyth object recognition machine translation learning lexicon fixed image vocabulary 
european conf 
comp 
vision 
fergus perona zisserman object class recognition unsupervised scale invariant learning 
ieee conf 
comp 
vision pattern recognition 
freeman pasztor learning low level vision 
intl 
comp 
vision vol 

kumar hebert discriminative random fields discriminative framework contextual interaction classification 
intl 
conf 
comp 
vision 
kumar hebert fields modeling spatial dependencies natural images 
adv 
neural information processing systems vol 

lowe object recognition local scale invariant features 
intl 
conf 
comp 
vision 
murphy weiss jordan loopy belief propagation approximate inference empirical study 
conf 
uncertainty artificial intelligence 
seymour parameter estimation model selection image analysis gibbs markov random fields 
phd thesis north carolina chapel hill 
schmid performance evaluation local descriptors 
ieee conf 
comp 
vision pattern recognition 
shi malik normalized cuts image segmentation 
ieee conf 
comp 
vision pattern recognition 
teh welling unified propagation scaling algorithm 
advances neural information processing systems vol 

