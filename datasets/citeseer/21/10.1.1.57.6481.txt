robust regression boosting median bal department computer science operations research university montreal cp succ 
centre ville montr eal canada iro umontreal ca 
boosting regression algorithms weighted average base regressors final regressor 
analyze choice weighted median 
propose general boosting algorithm approach 
prove boosting type convergence algorithm give clear conditions convergence robust training error 
algorithm recovers adaboost adaboost special cases 
boosting confidence rated predictions leads new approach outputs different decision interprets robustness different manner approach weighted average 
general non binary case suggest practical strategies analysis algorithm experiments 
boosting algorithms designed regression weighted average base regressors final regressor 
algorithms theoretical practical advantages general natural extensions adaboost sense recover adaboost special case 
main focus analysis medboost generalization adaboost uses weighted median final regressor 
average type boosting received attention regression domain idea weighted median final regressor new 
freund briefly mentions proves special case main theorem 
adaboost algorithm freund schapire returns weighted median response space restricted parameter updating steps complicated 
drucker uses weighted median base regressors final regressor parameter updates heuristic convergence method analyzed 
consider algorithm similar medboost response space prove convergence theorem special case weaker result factor 
intrator construct triplets weak learners show median regressors smaller error individual regressors 
idea weighted median appeared context bagging name 
research supported part natural sciences engineering research council nserc canada 
main result proof algorithmic convergence medboost 
theorem gives clear conditions convergence robust training error 
algorithm synthesizes versions boosting binary classification 
particular recovers adaboost marginal boosting algorithm adaboost warmuth special cases 
boosting confidence rated predictions leads strategy different schapire singer approach 
particular medboost outputs different decision interprets robustness different manner approach weighted average 
general non binary case suggest practical strategies analysis algorithm 
show algorithm provides clear criteria growing regression trees base regressors 
analysis algorithm suggests strategies controlling capacity base regressors final regressor 
propose approach window base regressors abstain 
learning curves obtained experiments model show medboost regression behaves similarly adaboost classification 
rest organized follows 
section describe algorithm state result algorithmic convergence 
section show relation medboost boosting algorithms special case binary classification 
section analyse medboost general non binary case 
experimental results section draw section 
medboost algorithm convergence result algorithm basically follows lines adaboost 
main difference returns weighted median base regressors weighted average consistent adaboost binary classification shown section 
differences come subtleties associated general case 
formal description training data 
data points set algorithm maintains weight distribution 
data points 
weights initialized uniformly line updated iteration line 
suppose base learner algorithm base iteration returns base regressor coming subset 
general base learner attempt minimize average weighted cost training data tube loss function satisfying indicator function argument true 
consider cost functions satisfy condition cost function base learner handle weighted data usual resample weight distribution 
medboost dn base dn 
base dn attempt minimize base awards arg min 
return med equivalent return med exp exp exp return med fig 

pseudocode medboost algorithm 
dn training data cost function base dn base regression algorithm attempts minimize weighted cost robustness parameter number iterations 
cost function 
emphasize relation binary classification simplify notation theorem define base awards training point 
base regressor 

note condition cost function base awards upper bounded 
computing base awards line algorithm sets weight base regressor value minimizes base awards larger algorithm returns actual regressor line 
intuitively means capacity set base regressors large 
equivalently algorithm returns weighted median base regressors iteration line 
intuitively means capacity set base regressors small find new base regressor decrease training error 
general easily line search convexity 
special cases computed analytically 
note practice base minimize weighted cost algorithm continue base regressor 
lines algorithm returns weighted median base regressors 
analysis algorithm formally define final regressor general manner 
weighted quantiles respectively base regressors 
respective weights 
formally min max 
weighted median defined med 
analysis robust error define robust prediction furthest quantiles real response formally 

notation prediction 
main result analyzes relative frequency training points robust prediction precise larger error 
formally robust training error defined 
convex equivalent 
equivalent minimizing consistent mason gradient descent approach function space 
sake simplicity notation suppress fact depends sequence base regressors weights final regressor gives relative frequency training points regressor larger error 
equality exactly average cost regressor training data 
small value indicates regressor predicts training points precision small value suggests prediction precise robust sense small perturbation base regressors weights increase 
theorem upper bounds robust training error regressor output medboost 
theorem 
defined suppose condition holds cost function 
define base awards weight training point tth iteration updated line weight base regressor computed line 

proof see appendix observation median base regressors goes real response training point base regressors far giving small base awards point 
proof follows proof theorem exponentially bounding step function 
note theorem implicitly appears cost function 
weaker result explicit proof case 
note convex positive means min condition line guarantees upper bound decreases step 
binary classification special case section show certain sense medboost natural extension original adaboost algorithm 
derive marginal boosting developed variant adaboost special case medboost 
show special case medboost provides approach boosting predictions different algorithm proposed schapire singer 
adaboost problem binary classification response variable comes set 
original adaboost algorithm assumed base learners replaced 
generate binary functions function set contains base decisions set adaboost returns weighted average base decision functions converted decision simple rule 
weighted median med returned medboost identical simple case 
training errors base decisions counted natural way cost function cost function base awards defined 
base decisions minimizing weighted error line optimal weights base decisions computed explicitly line log convenient property adaboost stopping condition line reduces satisfied closed multiplication 
settings training error theorem reduces theorem 
observation partly explains generalization ability adaboost tends minimize training error robust training error 
schapire define robust training error real valued function 
main tools analysis theorem shows 
lemma shows special case definitions robust training errors coincide special case theorem 
lemma 

sequence binary decisions 
sequence positive real numbers define 
defined respectively 
proof 
observe special case binary base decisions equivalent 
sided error weighted quantiles respectively defined 
loss generality suppose 
equivalent 
definition equivalent 
adaboost schapire analyzed robust training error general case idea modify adaboost right hand side explicitly minimized proposed warmuth 
algorithm analyzed authors 
case optimal weights base decisions computed explicitly line log stopping condition line principle true closed multiplication 
choice theorem identical lemma 
particular confidence rated adaboost general direction adaboost extended relax requirement base decisions binary 
schapire singer approach weak hypotheses range adaboost algorithm returns weighted sum base decisions converted decision 
upper bound training error proven general case base decisions weights lines minimizing bound 
upper bound formally identical right hand side setting base awards 
interestingly algorithm special case medboost differences give interesting insights 
technical difference choice satisfy range real line 
medboost theorem suggests set training error decision generated 
range base decisions restricted choice cost function generates base awards identical factor 
note general settings medboost allow cost functions optimization easier practice 
second fundamental difference approaches decision return way measure robustness decision 
case binary base decisions lemma case equal 
lemma shows robustness vary relatively large interval depending actual base predictors weights 
lemma 
margin possible construct set base predictors weights 

proof 
statement consider weight weight second statement weight weight lemma shows possible methods predict different labels point base predictors weights identical 
lemma robustness definitions suggest average type boosting gives high confidence set base decisions weighted average far highly dispersed mean medboost prefers sets base decisions weight side close decision threshold 
real restriction allowing sided cost functions looking lower quantiles upper quantiles truncating range base functions 
base decisions abstain schapire singer considers special case confidence rated boosting base decisions binary allowed abstain come subset set problem interesting complicated case optimal weights computed analytically 
similar model experiments section illustrate algorithm general case 
final regressor converted decision differently special case schapire singer approach special case medboost 
general asymmetry causes problems degenerate cases 
median base decisions returned non degenerate cases 
case assign label point unreasonable 
solve problem 
binary decision assigned output medboost 
modification cost function medboost identical algorithm 
base awards defined 
base decisions minimizing weighted error tth iteration weighted abstention rate weighted correct rate 
optimal weights base decisions line log 
stopping condition line reduces robust training error theorem gives note settings minimize upper bound 
minimize adaboost obtain regularized version 
minimization done analytically formulas quite complicated omit 
case medboost possibility final decision abstain convert binary decision 
case abstaining costly setting small non zero value 
option choose cost function 
case base awards penalized 
optimization complicated optimal parameters analytically 
general case theory general fashion algorithm theorem allows different cost functions sets base regressors base learning algorithms 
practice base regressors capacity controlled linear regressors regression stumps impractical 
see consider case cost function 
case theorem translated pac type statement weak strong learning weak regressor precise weighted points final regressor precise points 
extreme case find base regressor precise unweighted points algorithm terminates iteration line 
second extreme case points base regressor algorithm simply return line 
means capacity set base regressors carefully chosen weighted errors half avoid overfitting close zero 
regression trees possible practically feasible choice regression trees base regressors 
minimization weighted cost line provides clear criteria splitting pruning 
complexity easily controlled allowing limited number splits 
stopping criterion line growth tree continued positive equivalent cost function 
algorithm stops iterations tree returned base regressor judged complex 
complexity final regressor controlled strong trees base regressors nonzero 
regressors abstain general solution capacity control problem base regressors abstain 
formally define cost function base awards 
experiments section window base functions constant inside ball radius centered data points abstain outside ball 
minimization line straightforward finite number base functions different average costs 
line minimized analytically 
case handled care want minimize error rate abstain rate 
problem solved formally setting small non zero value 
validation able assess regressor validate parameters algorithm suppose observation response form pair random variables values define tube error function 
suppose mode function conditional distribution defined arg inf exists unique 
tube bayes error distribution 
tube cost function medboost empirical tube error interpret objective medboost minimizing empirical tube error 
gives clear criteria validation minimizing tube test error 
validating tougher problem 
note terminating condition line need base regressors empirical weighted tube error smaller overfitting unavoidable note situation happen special case binary classification 
practical behavior algorithm case returns overfitting regressor set base regressors large capacity stops iteration line 
cases problem small detected 
algorithm behaving 
similarly case binary classification expect algorithm works particularly see section 
point give general criteria choosing best design parameter parameter validate 
hand goal minimize certain cost function quadratic absolute error parameters validated test cost 
second set experiments section know know validate error practice unknown clearly unfeasible 
experiments experiments section designed illustrate method general case 
concentrated similarities differences classification regression method 
clearly experiments required evaluate algorithm practical viewpoint 
linear base regressors objective experiments show base regressors adequate terms data generating distribution medboost works 
data generating model uniform random noise generated different symmetric distributions interval 
linear base regressors minimize weighted quadratic error iteration 
generate base awards cost function set algorithm usually stopped reaching limit 
final regressor compared linear regressor minimized quadratic error 
comparison validation error regressor large 
table shows average errors standard deviations experiments points generated 
results show medboost beats individual linear regressor experiments margin grows variance noise distribution 
base learner minimizes quadratic error linear regressor say additional base regressors improve best linear regressor 
noise noise pdf noise best distance distribution std medboost linear uniform inverted triangle quadratic extreme prob 
table 
medboost versus linear regression 
window base regressors abstain experiments tested medboost window base regressors abstain section toy problem 
data model section time noise gaussian zero mean standard deviation 
experiment training test points 
tested combinations typical learning curves indicate algorithm behaves similarly binary classification case 
overfitting overtraining sense test error plus rate data points decreases monotonically 
second similarity test error decreases training error crosses may explained observation medboost minimizes robust training error 
interestingly actual test error goes test error plus rate data points approaches quite 
intuitively means algorithm gives hard points abstaining overfitting trying predict 
training error test error training abstain test abstain training error abstain test error abstain bound bayes error training error test error training abstain test abstain training error abstain test error abstain bound bayes error fig 

learning curves medboost window base regressors abstain 


compared validation strategies test error distance test error distance minimized radius expected distance tends small 
validating minimum distance quite stable relatively large range table 
best sense dist best test error test error test error table 
validation parameters medboost 
analyzed medboost boosting algorithm regression uses weighted median base regressors final regressor 
proved boosting type convergence algorithm gave clear conditions convergence robust training error 
showed adaboost recovered special case medboost 
boosting confidence rated predictions proposed new approach medboost 
general non binary case suggested practical strategies feasible choices set base regressors 
experiments models showed medboost regression behaves similarly adaboost classification 
appendix proof theorem observe definition robust prediction follows max 
cases 



definition implies 


follows similarly case definition fact case 


follows case follows case 
shown implies exp exp exp exp line exp normalizing factor line 
note proof identical proof theorem 

intrator 
boosting regression estimators 
neural computation 


boosting algorithm regression 
proceedings international conference artificial neural networks pages 

uhlmann 
bagging improving prediction algorithms 
editors advances trends nonparametric statistics appear 


drucker 
improving regressors boosting techniques 
proceedings th international conference machine learning pages 

duffy helmbold 
leveraging regression 
proceedings th conference computational learning theory pages 

freund 
boosting weak learning algorithm majority 
information computation 

freund schapire 
decision theoretic generalization line learning application boosting 
journal computer system sciences 

friedman 
greedy function approximation gradient boosting machine 
technical report dept statistics stanford university 

mason bartlett baxter frean 
boosting algorithms gradient descent 
advances neural information processing systems volume pages 
mit press 

warmuth 
marginal boosting 
proceedings th conference computational learning theory 

warmuth 
efficient margin maximizing boosting 
journal machine learning research submitted 

schapire freund bartlett lee 
boosting margin new explanation effectiveness voting methods 
annals statistics 

schapire singer 
improved boosting algorithms confidence rated predictions 
machine learning 

zemel 
gradient boosting algorithm regression problems 
advances neural information processing systems volume pages 

