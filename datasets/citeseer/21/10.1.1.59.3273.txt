discovery extremely low dimensional clusters semi supervised projected clustering kevin yip david cheung department computer science university hong kong cs hku hk michael ng department mathematics university hong kong mng maths hku hk studies suggest projected clusters extremely low dimensionality exist real datasets 
number projected clustering algorithms proposed past years identify clusters dimensionality lower total number dimensions commonly real datasets gene expression profiles 
propose new algorithm accurately identify projected clusters relevant dimensions total number dimensions 
robust objective function combines object clustering dimension selection single optimization problem 
algorithm utilize domain knowledge form labeled objects labeled dimensions improve clustering accuracy 
believe semi supervised projected clustering algorithm 
theoretical analysis experimental results show small amount input knowledge possibly covering portion underlying classes new algorithm improved accurately detect clusters dimensions relevant 
algorithm useful getting target set clusters multiple possible groupings objects 

studies suggested presence low dimensional clusters high dimensional real datasets 
example typical microarray gene expression dataset contains expression values thousands genes different samples common find tens genes expression patterns unique cluster samples 
genes called relevant genes opposed irrelevant genes help identifying cluster members 
due large number genes irrelevant cluster samples cluster low similarity measured similarity function considers expression values genes 
clusters undetectable traditional clustering algorithms 
projected clustering problem defined scenario 
projected cluster set member objects associated set relevant dimensions member objects similar subspace formed relevant dimensions dissimilar objects outside cluster 
measure object similarity euclidean distance dimension relevant cluster projections members dimension closer remote projections objects 
goal projected clustering algorithm identify clusters objects relevant dimensions certain objective function average cluster similarity relevant dimensions optimized 
clusters real datasets contain extremely low percentage relevant dimensions genes previous example reported current projected clustering algorithms unable identify clusters low dimensionality 
mainly due objective functions highly rely accuracy input parameters similarity calculations involve dimensions may reflect real similarity different objects 
addition unsupervised methods algorithms little domain knowledge clustering process despite fact small amount domain knowledge usually available applications 
example gene expression datasets functions small number genes usually known biologists 
order better utilize domain knowledge clustering process number semi supervised clustering algorithms proposed review section 
basic idea domain knowledge guide clustering process 
example semi supervised means algorithm domain knowledge relationships objects force assignment object pairs cluster different clusters :10.1.1.20.7363
reported studies clustering accuracy greatly improved inputting small amount domain knowledge :10.1.1.33.906:10.1.1.34.7653
noted domain knowledge semi supervised clustering different classification supervised learning knowledge semi supervised clustering may suitable sufficient classification 
instance knowledge needs form objects class labels required classification 
amount knowledge class small statistically insignificant build classifier captures general properties class input knowledge biased side class 
input knowledge semi supervised clustering needs cover classes possible produce corresponding clusters 
previous studies semi supervised clustering focused non projected clustering consider relevance dimensions 
semi supervised approach fact useful projected clustering 
instance small amount example objects cluster labeled objects tumor samples known certain type relevant dimensions cluster estimated dimensions objects significantly close 
similarly having dimension specified relevant cluster labeled dimension gene known relevant tumor type cluster members estimated regions unexpectedly high object densities 
semi supervised clustering important application handling datasets multiple possible groupings 
example cancer study patients grouped response certain treatment risk having cancer recurrence 
unsupervised methods produce single set clusters may correspond groupings 
semi supervised approach supplying different input knowledge single clustering algorithm guided produce kinds clusters different runs 
observations motivate current study major contributions proposing robust objective function projected clustering naturally involves dimension selection optimization process 
proposing domain knowledge labeled objects labeled dimensions improve accuracy projected clustering 
developing new algorithm theoretically empirically detect clusters extremely low dimensionality accuracy improved incorporating domain knowledge clustering process 
section review related projected clustering semi supervised clustering 
section formally define semi supervised projected clustering problem assumptions study 
section describe new algorithm 
experimental results section observations discussions 
section summarize study discuss extensions 

related 
projected clustering existing projected clustering algorithms classified categories partitional cluster time hierarchical 
partitional approach proclus earliest projected clustering algorithms 
traditional medoids approach goal minimizing average cluster dispersion 
differs traditional medoids distance different cluster members computed relevant subspace cluster 
relevant subspace cluster determined measuring average distance medoid set neighboring objects close dimensions considered 
dimensions smallest average distances medoid cluster selected relevant dimensions cluster form relevant subspace 
partitional algorithm orclus improves proclus selecting principal components clusters parallel original dimensions detected 
adds hierarchical part potentially reduce errors due inaccurate initial object assignments 
limitation partitional methods determination neighboring objects similarity calculations involve dimensions 
number relevant dimensions cluster small different members cluster may appear dissimilar dimensions considered 
methods neighboring objects medoid need come real cluster relevant dimensions suggested wrong 
approaches objective function tends give better scores fewer dimensions regarded relevant cluster require users supply average number relevant dimensions cluster usually unknown users 
incorrect parameter values clustering accuracy seriously affected 
monte carlo method doc variant identify projected clusters 
find cluster object randomly selected seed number objects randomly sampled determine relevant subspace cluster 
dimension regarded relevant cluster objects distance seed dimension 
cluster form hypercube width 
quality cluster evaluated objective function takes account number objects falling hypercube number relevant dimensions cluster 
objects relevant dimensions cluster formed random chance receives better score 
relative importance number objects relevant dimensions controlled user parameter 
algorithm repeatedly tries different seeds neighboring objects certain number iterations reached cluster highest score returned algorithm try find new cluster 
algorithms perform cluster form hypercube parameter values specified correctly cases requirements met clustering results quite unsatisfactory 
number seeds neighboring objects required try large causes algorithms run long time 
hierarchical algorithm harp dynamic threshold loosening mechanism avoid problems previous methods 
basic assumption objects belong cluster similar dimensions 
clusters allowed merge similar number dimensions minimum similarity minimum number similar dimensions controlled internal threshold variables 
asking users supply fixed values variables harp automatically adjusts values clustering process 
thresholds set harsh values merges group objects belonging real cluster allowed performed 
relevant dimensions cluster apparent threshold values allow cluster merges 
process repeats thresholds reach baseline values target number clusters reached 
method successfully avoids extensive distance calculations involve dimensions user parameters values hard determine 
due hierarchical nature algorithm intrinsically slow 
number relevant dimensions cluster extremely low dataset dimensionality accuracy harp may drop basic assumption valid due presence large amount noise values dataset 
summary existing projected clustering algorithms objective functions effectiveness rely greatly accuracy parameter values hard users determine 
involve similarity calculations consider dimensions quite misleading cluster dimensionality small 
explicit domain knowledge common forms labeled objects labeled dimensions 
thorough survey algorithms algorithms proposed related problems subspace clustering biclustering 

semi supervised clustering trend machine learning research combine techniques developed unsupervised learning supervised learning handle datasets partial external information 
foci semi supervised clustering actively uses available domain knowledge guiding clustering process 
methods categorized groupings knowledge input knowledge input knowledge affects clustering process 
simplest type input labeled objects :10.1.1.7.9416
cases users know exact class labels objects knowledge objects put cluster specified links links :10.1.1.5.877:10.1.1.138.1114:10.1.1.11.5360:10.1.1.20.7363:10.1.1.34.7653
studies propose input classification rules examples similar objects general comments cluster particular object put :10.1.1.33.906:10.1.1.58.3667
knowledge supplied different time 
supplied clustering guide clustering process clustering evaluate clusters guide round clustering :10.1.1.33.906:10.1.1.7.9416
algorithms actively request users supply specific information appropriate time :10.1.1.5.877:10.1.1.11.5360
various ways input knowledge guiding formation seed clusters forcing recommending objects put cluster different clusters modifying objective function similarity function distance matrix :10.1.1.33.906:10.1.1.138.1114:10.1.1.11.5360:10.1.1.20.7363:10.1.1.58.3667:10.1.1.34.7653

problem definition formally define semi supervised projected clustering problem 
start data model 
possibly dataset objects dimensions objects partitioned clusters ci empty set outliers 
assume cluster random sample corresponding hidden class associated set relevant dimensions form relevant subspace 
denote dj cij projections ci dimension vj respectively 
suppose vj relevant subset rj clusters cluster ci rj cij random sample local gaussian population small variance ij set projected values vj dj ci rj cij random sample global population variance larger local gaussians 
intuitively relevant subspace cluster cluster members average close remote objects cluster 
alternatively cluster unknown relevant subspace dimension relevant cluster projection cluster dimension smaller variance 
distinguish actual clusters due hidden classes clusters produced clustering algorithm remaining call real clusters simply clusters 
call dimensions determined clustering algorithm relevant cluster selected dimensions 
inputs semi supervised projected clustering algorithm dataset target number clusters possibly empty set labeled objects obj 
id class label pairs indicates object member class 
set may may cover classes 
possibly empty set labeled dimensions dim 
id class label pairs indicates dimension relevant class 
dimension specified relevant multiple classes 
set may may cover classes 
outputs algorithm clusters selected dimensions possibly empty set outliers 
goal optimize objective function value objective score reflects quality clusters 
non projected clustering algorithm means objective function defined total squared error 
shown partition objects minimize function corresponds maximum likelihood hypothesis model irrelevant dimensions 
objective function modified projected clustering relevant dimensions involved distance calculations part objective score cluster normalized number selected dimensions 
due normalization function tends gives better smaller scores clusters fewer selected dimensions forces algorithm request users supply average cluster dimensionality order select dimension cluster 
function summation variances different dimensions worse dimension larger variance constitutes objective score 
data model variance cluster smaller relevant dimension irrelevant dimension irrelevant dimensions accidentally selected objective score dominated constituents irrelevant dimensions remain virtually unchanged relevant dimensions 
designed new objective function goals facilitate selection dimensions particular data properties different clusters dimensions value constituted relevant dimensions relatively robust 
function denoted defined follows nd ij ij vj vi ni xj ij ij ci ni ij ij ij ij vi set selected dimensions cluster ci ni size number objects ci xj projection object dimension vj ij ij ij sample median sample mean sample vari selection threshold meaning explained ance projection ci vj respectively ij 
objective function composed score components cluster turn sum score components ij selected dimension 
ij basically negation sample variance ij adjusted range ni members ci projected value vj ij takes maximum value ni 
major differences objective function defined 
normal 
discussed ized number selected dimensions ci threshold ij section allows dimension selection mechanism data characteristics ci vj design goal 
avoids existence trivial best score cluster selects dimension 
second setting ij value larger sample variance ij selected dimension ij positive better dimension dimension smaller ij larger constituent design goal 
clustering reflected large value goal maximize objective score 
third cluster dispersion measured distance cluster median centroid function affected outliers 
discuss robustness 
definition objective function leads lemma lemma set clusters ci objective function maximized dimensions ij ij ij smaller ij selected dimensions selected 
proof maximize dimension vj selected ij positive selected ij negative correspond cases ij ij ij smaller larger ij respectively 
dimension selection procedure follows directly lemma ij called selection threshold values critical accuracy clustering algorithm 
section show simple scheme determine values ij advanced probabilistic scheme certain conditions satisfied 
cases user parameter required value critical clustering accuracy 
objective function defined generic way different ij cluster dimension create tremendous amount parameters algorithm 
procedure ci target cluster foreach dimension vj select vj ci ij ij ij ij listing dimension selection procedure 
study confine scope number assumptions implicitly explicitly previous studies projected clustering semi supervised clustering 
possibility relaxing discussed section 
determination relevant dimensions mainly help identify object clusters opposed biclustering rows columns dataset treated equally 

clusters disjoint opposed subspace clustering axis parallel opposed orclus 

object similarity difference projected values opposed pattern clustering 

objects class close certain subspace class corresponds real cluster opposed decision trees objects class form multiple clusters 

input knowledge correct biased 
instance object input member cluster really belong cluster may projections relevant dimensions highly deviated cluster mean 

new algorithm outline new algorithm sspc semi supervised projected clustering shown listing 
partitional method similar medoids algorithms 
determines seeds potential medoids cluster draws medoid 
object dataset assigned cluster gives greatest improvement objective score value ij equation temporarily substituted projection medoid vj 
object improve score cluster put outlier list 
assigning objects selected dimensions cluster objective score recomputed actual medians 
new score best encountered far clusters recorded 
best clusters restored 
bad cluster identified current best set clusters new medoid selected attempt improve objective score iteration 
medoid cluster replaced cluster median virtual object projected value dimension equal median cluster members medoid projected values deviated cluster center relevant dimensions data model section 
iteration values ij substituted medians 
term cluster representative call medoid median represents cluster 
replacing old cluster representatives members cluster removed new iteration object assignment score comparison cluster representatives replacement carried 
process repeats best objective score remains unchanged certain number iterations 
algorithm sspc initialization determine seeds relevant dimensions cluster cluster draw medoid seeds assign object dataset cluster outlier list gives greatest improvement objective score call ci cluster ci calculate objective score record clusters give best objective score far restore best clusters replace cluster representative cluster remove members repeat score improvements observed certain number iterations listing outline sspc algorithm 
main differences sspc previous partitional approaches projected clustering proclus orclus 
seeds determined domain knowledge labeled objects labeled dimensions supplied potentially accurate 
second proclus orclus selected dimensions determined distance calculations involve dimensions seeds cluster associated estimated set relevant dimensions determined initialization 
seed picked medoid cluster initially replace old cluster representative associated dimensions selected dimensions cluster 
seen process rely distance calculations involve dimensions user parameters critical clustering accuracy values hard determine 
allows sspc identify low dimensional clusters accurately 
third iteration replacing bad cluster representative new medoid cluster representatives replaced cluster medians avoid problems due potential biased projected values medoids discussed 
coming subsections core mechanisms sspc discussed detail followed analysis algorithm 

determining ij order procedure calculate objective score need determine values selected dimensions 
smaller ij mentioned section ij greater sample variance ij expected variance random sample population variance population ij variance global population members ci similar vj group random objects 
viewed baseline maximum value ij value estimated sample variance dj denoted propose schemes set actual value ij scheme set ms user parameter 
smaller selection criterion 
scheme simple resulting clustering accuracy stable wide range values shown experimental results 
second scheme probabilistic reasoning 
users asked specify value bounds maximum probability dimension irrelevant cluster selected chance 
suppose ci cluster dimension vj irrelevant 
write ij ij 
sampling distribution ij known computed accordingly 
probability density function pdf value ij example suppose global populations gaussian 
random variable ni ij distribution ni degrees freedom 
specified approximated value ij chi square computed inverse cumulative chi square distribution 
scheme generic need assume properties global populations 
known pdf second scheme recommended parameter case sampling distribution ij stronger intuitive meaning notice ij take different values different ci parameter vj cases objective function procedure involves user parameter value critical clustering accuracy sspc quite robust 
experimental results section range values give clustering results usually wider range values average cluster dimensionality proclus 
general reasonable values user ideas proper value 
value tuned clusters appear selected dimensions tuned objects discarded outliers 

initialization initialization step sspc needs determine seeds 
medoids methods clarans cluster draws medoid single pool seeds sspc puts seeds different seed groups 
seed group contains set seeds expected come single real cluster 
seeds set relevant dimensions cluster estimated 
time seed seed group picked medoid cluster set dimensions seed group selected dimensions 
sspc creates kinds seed groups private public 
cluster input knowledge easier form accurate seed group terms seeds estimated relevant dimensions 
private seed group created solely cluster 
clusters share public seed groups number groups larger number clusters medoids drawn different seed group combinations 
cluster needs draw new medoid randomly drawn private seed group public seed groups currently clusters 
order seed group creation important 
having created seed groups clusters accurately easier accurately create remaining seed groups 
objects close seeds previous seed groups corresponding subspaces members clusters need considered determining seeds new seed groups 
show section easier create seed groups accurately clusters input knowledge suggests clusters input knowledge seed groups created earlier 
ease creating accurate seed groups sspc creates seed groups order clusters inputs clusters inputs clusters inputs clusters inputs 
category clusters larger amount inputs initialized earlier 
discuss details seed group creation process cases separately 
cases private seed group created ci denote target cluster gi denote resulting seed group iv sets labeled objects labeled dimensions ci respectively 
gi contains set objects seeds set estimated relevant dimensions treat cluster sake discussion 

clusters kinds inputs case pieces important information 
center ci located near median io objects io biased side ci relatively close center ci compared objects 
second set objects io viewed temporary cluster ci dimensions larger values relevant dimensions ci 
third assume input knowledge correct dimensions iv included set relevant dimensions seed group 
leads step process seed group creation 
identify seeds gi objects dataset close median io dimensions large values 
set relevant dimensions seed group selected gi plus ones iv dimensions gi ci number seeds controlled larger number objects io probably give accurate estimate relevant dimensions 
developed mechanism step simple idea 
form grid multi dimensional histogram dataset fixed number dimensions dimensions relevant ci cell contain large number objects correspond center ci subspace 
dimensions irrelevant ci peak density highest number objects cells lower 
multiple grids built different sets dimensions objects peak cell highest density chosen members gi 
issues consider 
number dimensions build grid large number cells increases exponentially number building dimensions increases cell objects creates heavy computational overhead 
normally dimensional grid serves purpose quite 
second issue dimensions build grids 
basic principle allow dimensions greater chance relevant ci higher probabilities involved grid building 
define candidate set includes dimensions selected ci iv dimension vj set probability proportional value selected building dimension grid 
third issue set grid building dimensions simultaneously relevant multiple clusters case grid contain multiple peaks 
cluster center expected close median io looking absolute peak grid perform localized hill climbing search starting search solves problem median may biased cell contains median io side ci 
involves computation sample variance io contain objects 

clusters labeled objects seed group creation process previous case dimensions ci involved grid building gi set relevant dimensions seed group 

clusters labeled dimensions case temporary cluster ci formed 
dimensions iv involved grid building probability involved 
ci starting point localized hill climbing search seeds chosen objects absolute peak grid 

clusters inputs case build grids directly due lack input knowledge 
alternative mechanism information seed groups developed 
similar max min mechanism proclus identifies object minimum distance seeds picked seed groups maximum 
distance calculations performed subspace defined relevant dimensions seed groups normalized number dimensions 
identified object remote picked seeds chance belongs clusters seed group created 
replace median previous cases starting point hill climbing search 
dimensional histogram constructed dimension dataset measure object density identified object dimension 
dimension high object density identified object relevant cluster centers object dimension 
dimension high probability involved grid building 
probabilities determined seed group creation process case labeled objects performed 
extreme case clusters receive input knowledge object seed group randomly drawn dataset max min mechanism 

cluster representatives replacement improve objective score sspc needs identify bad cluster replace cluster representative appropriately 
common situation bad cluster exists medoids drawn clusters belong real cluster 
result clusters quite similar compete score real cluster 
hand real clusters represented cluster members put outlier list 
ways detect occurrence situation 
check exists cluster low score absolutely compared maximum possible score relatively compared score clusters 
cluster loser competing clusters 
way check exists clusters similar calculating change score clusters merged 
clusters medoids belonging real cluster resulting score approach sum original scores 
resulting score lower dimensions selected members different real clusters mixed 
case bad cluster cluster representative replaced new medoid randomly drawn associated seed groups currently clusters 
cluster medoid may really member cluster may close cluster center relevant dimensions 
result real members located side cluster may attracted cluster 
cluster improved replacing medoid median cluster probably closer center real cluster 

complexity divide complexity analysis sspc parts seed groups creation actual clustering 
substantial amount time seed groups creation spent building grids involve projecting objects cells 
number building dimensions number bins dimension fixed constant time building grid searching cell peak density 
number grids build cluster fixed constant total time spent grid building searching kn 
estimate relevant dimensions ij scores computed take nid cluster ci selection thresholds precomputed parameter 
total time complexity seed groups creation knd 
size grid fixed constants space complexity depends size dataset nd 
actual clustering assigning object involves computing resulting scores cluster take total time kd local means variances computed incremental fashion cluster features cf 
iteration takes knd time object assignment objective score calculations 
cluster refinements require computation medians done time brute force way projected values 
total time iteration knd 
time complexity algorithm depends number iterations unknown value vary different runs 
partitional clustering algorithms observe experimental results grow linearly suggest time complexity sspc algorithm knd 
space complexity actual clustering nd space complexity algorithm 
linear time space complexities practical cluster large datasets compared projected clustering algorithms doc harp orclus 

input needed 
real situations amount available domain knowledge limited costly obtain knowledge 
important predict relationship amount input knowledge resulting clustering accuracy minimize amount input knowledge getting satisfactory accuracy 
case labeled objects available 
suppose certain cluster ci receives labeled objects 
objects form temporary cluster determine grid building dimensions 
want estimate probability grid built dimensions really relevant ci crucial accuracy seed group clustering accuracy turn 
assume global distributions gaussian parameter computing values ij definition probability dimension irrelevant ci selected temporary cluster dis cussed section value ij computed inverse cumulative chi square distribu tion 
suppose dimension vj relevant ci 
probability vj selected temporary cluster ij io ij ij io ij variance local population lying class ci vj 
estimated labeled objects histogram vj estimated accordingly cumulative chi square distribution 
di number relevant dimensions ci number true positives selected dimensions relevant ci binomial random variable di trials probability similarly number false positives selected dimensions irrelevant ci binomial random variable di trials probability denote number true positives false positives respectively 
suppose grid built dimensions dimension selected temporary cluster chance involved building grid probability grid built dimensions relevant ci di di di qt di di di notation represents number ways picking unordered outcomes possibilities 
grids built cluster probability involves relevant dimensions visualize change value different input sizes consider real values experiments section 
suppose variance ratio local population corresponding global population 
vary io di ratio 
shows estimated probabilities grid formed relevant dimensions 
shows fixed di ratio having input objects increases probability forming grid relevant dimensions 
addition curve observed sharp increase followed flattened region 
means users estimate smallest amount input lead near maximal accuracy 
exciting result see di inputs guarantee probability number input objects probability grid formed relevant dimensions labeled objects available 
probability number input dimensions probability grid building dimensions relevant ci formed labeled dimensions available 
grid formed relevant dimensions 
shows fixed amount input probability increases di increases suggests input objects better clusters relevant dimensions 
consider case labeled dimensions available 
suppose dimension vj specified relevant cluster ci 
dimensional histogram built vj expect find peak center ci 
cell highest object density close center ci probably vj relevant cluster 
suppose clusters average cluster di relevant dimensions 
probability vj relevant clusters di denote probability probability forming dimensional grid dimensions relevant ci want estimate probability forming grid grids built indicator chance forming accurate seed group 
suppose iv input dimensions grids allowed exactly set building required probability iv dimensions 
iv parameter values setting estimated probabilities various io values shown 
di general labeled dimensions supplied higher chance forming grid building dimensions relevant ci 
reveals interesting phenomenon labeled objects better di large labeled dimensions better small chance single dimension relevant multiple clusters small 
suggests trying identify clusters extremely low dimensionality main focus study effective labeled dimensions input knowledge 
inferences show small amount input knowledge enhance accuracy lot 
kinds input complement synergy supplied time provided amount input objects small causes large amount irrelevant dimensions di di di di di di di di di di building grids 
empirical results section 

experiments section various experimental results sspc comparing algorithms 
describe algorithms involved general setting performance metric 
details experiments subsequent subsections 
compare accuracy projected clustering algorithms harp proclus sspc non projected medoids clustering algorithm clarans 
include orclus consider axis parallel clusters long execution time large datasets 
tried include experiments great difficulty setting parameter values 
preliminary experiments rapidly moved putting objects single cluster discarding lot outliers objects included clusters slight change value parameter 
focused algorithms listed left thorough comparisons different projected clustering algorithms study 
harp produces clusters clustering dataset multiple times set parameter values 
true algorithms random numbers procedures 
repeated experiment times report result gives best algorithm specific objective score 
primary performance metric evaluating quality clustering result adjusted rand index ari validates produced clusters known real clusters :10.1.1.28.8161
rand index expected index value taken account :10.1.1.20.7363
measures similar partition objects real clusters partition clustering result 
denote number object pairs cluster cluster cluster different clusters respectively ari defined follows ari ad bc similar partitions larger smaller larger ari value 
identical index value 
random partition index value zero 
input knowledge involved run sspc labeled objects removed resulting clusters computing ari values order eliminate direct performance gain due input objects 

raw accuracy set experiments compared raw accuracy algorithms accuracy input knowledge 
series synthetic datasets generated 
actual average dimensionality clusters varies accounting dataset dimensionality 
datasets generated data model described section global distributions uniform local distributions having variances ranging value range global distributions 
set algorithms default parameter values harp clarans tried different values critical parameters proclus sspc 
proclus tried different values dataset 
sspc different values dataset 
results different figures 
best results results highest ari values trying different parameter values shown 
best average results proclus sspc shown comparison relative robustness 
shows projected clustering algorithms performed cluster dimensionality high 
comparison clarans non projected algorithm lower accuracy 
dataset dimensionality low best performance proclus begins drop 
performance ari clarans harp proclus best sspc best sspc best best raw accuracies algorithms datasets various average cluster dimensionality 
ari proclus avg proclus best sspc avg sspc best sspc avg sspc best comparison best average raw accuracies proclus sspc datasets various average cluster dimensionality 
projected clustering algorithms went sspc performance drop matter parameter parameter 
somewhat unexpected raw performance sspc parameter close performance parameter global distributions non gaussian 
may due fact dimension selection mechanism sspc assumptions global distribution 
performance parts algorithm may compensate invalid assumptions parameter 
shows sspc robust proclus average accuracies different parameter values closer best accuracies 
individual clustering results shown 
proclus performed value supplied correctly performance went input moved away true value 
contrast sspc performed various parameter values tried robust 
ari parameter value proclus sspc sspc raw accuracy proclus sspc dataset various parameter values 
ari actual amount outliers accuracy sspc datasets various amount outliers 
amount objects outlier list actual amount outliers amount objects put outlier list sspc clustering datasets various amount outliers 

outlier immunity set experiments studied sspc affected outliers 
series synthetic datasets generated 
amount outliers varies 
set arbitrary value 
clustering accuracies amount objects outlier lists shown respectively 
results show sspc high noise immunity moderate accuracy decrease amount outliers increases 
amount objects detected outliers highly resembles actual amount outliers datasets 

performance input knowledge sspc encouraging raw accuracy performance drops small 
set experiments lower average cluster dimensionality see accuracy sspc improved input knowledge 
generated dataset configuration highly resembles real gene expression dataset goal cluster samples number relevant genes sample class low set tried different coverage ratios fraction clusters receiving inputs input categories inputs different input sizes 
example coverage kinds inputs supplied input size clusters receive input knowledge labeled objects labeled dimensions 
inputs supplied clusters 
inputs drawn randomly real cluster members relevant dimensions 
point coming figures median repeated runs independent sets inputs 
shows accuracy sspc coverage 
ari values harp proclus correct value supplied respectively lower raw accuracy sspc input size 
general sspc larger accuracy improvement inputs supplied 
ari input size io iv accuracy sspc various amount input knowledge coverage 
ari coverage io iv accuracy sspc various coverage input knowledge input size 
accuracy stable objects dimensions equal default value number building dimensions grid 
observations consistent earlier analysis section 
accuracy sspc appears stable labeled dimensions inputs 
particular accuracy lower raw accuracy observed labeled objects supplied cluster 
due large probability objects close irrelevant dimensions seriously dimension selection mechanism see 
hand probability pair dimensions relevant multiple clusters lower due low average cluster dimensionality results observable accuracy improvement labeled dimensions supplied 
figures show accuracy sspc changing coverage input size respectively 
general trend increasing accuracy coverage increases increasing trend stable larger input size 
interesting observation peak performance reached coverage suggests necessary input domain knowledge cluster 
max min mechanism section clusters input knowledge locate cluster centers provided seed groups clusters created accurately 
object assignment members clusters input knowledge assigned correctly chance remaining clusters identify members relevant dimensions correctly increased 

datasets multiple possible groupings discussed section important application semi supervised clustering produce different desired clusters different input knowledge 
set experiments verify capability sspc achieving 
generated datasets 
members relevant dimensions clusters datasets independent 
combined datasets produce dataset dimensions come original dataset come second 
average cluster dimensionality remains tested accuracy harp proclus sspc dataset correct value supplied proclus 
sspc tested accuracy different scenarios inputs raw accuracy input knowledge ari coverage io iv accuracy sspc various coverage input knowledge input size 
table accuracy algorithms dataset possible groupings 
sets ari values computed actual clusters original datasets respectively 
algorithm ari set ari set harp proclus sspc inputs sspc size sspc size sspc size sspc size sspc size sspc size original dataset input knowledge second original dataset 
ari values algorithms computed actual clusters original datasets shown table 
performance harp seriously affected simultaneous existence possible groupings 
objects cluster close dimensions due fact belong cluster grouping ruin threshold loosening mechanism harp 
performance proclus better encouraging 
raw accuracy sspc better harp proclus evaluated set clusters worse evaluated second set 
shows external inputs sspc tends form clusters similar set 
external inputs supplied accuracy sspc significantly improved cases 
results confirm importance external inputs guiding formation desired clusters multiple possible groupings 

scalability set experiments tested scalability sspc respect increasing dataset size dimensionality 
generated series datasets varies 
second varies 
measured execution time repeated runs experiment execution time proclus 
prefer report time repeated runs order reduce random variation execution time run 
real situations repeated runs usually required obtain satisfactory results 
execution time algorithms shown figures 
figures confirm linear time complexity sspc respect speed comparable proclus implementations 
input knowledge execution time sspc reduced fewer histograms built histogram searches localized fewer iterations required locally optimal objective score reached 
execution time runs dataset size proclus sspc execution time proclus sspc increasing dataset size 
execution time runs dataset dimensionality proclus sspc execution time proclus sspc increasing dataset dimensionality 

summary extensions discussed potential limitations existing projected clustering algorithms including inability detect clusters low dimensionality user parameters proper values hard determine potential accuracy drop improper parameter values supplied 
proposed new projected clustering algorithm robust able detect clusters extremely low dimensionality uses robust objective function avoids distance calculations involve dimensions 
addition proposed ways utilize available domain knowledge form labeled objects labeled dimensions 
experimental results show clear accuracy improvement input knowledge incorporated clustering process 
peak performance readily reached small amount knowledge supplied knowledge covers classes 
obvious directions study 
important test new algorithm real datasets expected contain projected clusters gene expression profiles 
applying complex noisy real data data model objective function may revised observed data properties 
direction allow incorrect inputs 
inputs incorrect validated guide clustering process example comparing assumed data model observed data values 
possible study fuzzy inputs contains confidence level indicates chance belonging cluster quality level specifies chance object certain distance cluster center 
current study focused distance clustering similarity different objects measured distance function 
applications similarity measured means rise fall pattern values 
semi supervised clustering algorithms distance methods means medoids adaptations may required trying incorporate domain knowledge pattern clustering 
interesting study case class corresponds multiple clusters 
occur example samples type tumor belong different subtypes 
samples best modeled multiple clusters different properties common clusters 
simplest way handle datasets assign different sub class label cluster treat cluster originating different class 
corresponding domain knowledge available may resort forming single cluster class common properties 
interesting algorithm clustering follows direction described modifies distance matrix artificially move objects class :10.1.1.11.5360
general approach allow formation multiple clusters class currently fully studied 
aggarwal procopiuc wolf yu park 
fast algorithms projected clustering 
acm sigmod international conference management data 
aggarwal yu 
finding generalized projected clusters high dimensional spaces 
acm sigmod international conference management data 
agrawal gehrke gunopulos raghavan 
automatic subspace clustering high dimensional data data mining applications 
acm sigmod international conference management data 
basu banerjee mooney 
semi supervised clustering seeding 
proceedings nineteenth international conference machine learning 
basu banerjee mooney 
active semi supervision pairwise constrained clustering 
proceedings siam international conference data mining 
basu bilenko mooney 
comparing unifying search similarity approaches semisupervised clustering 
icml workshop continuum labeled unlabeled data machine learning data mining 
cheng church 
biclustering expression data 
proceedings th international conference intelligent systems molecular biology 
cohn caruana mccallum :10.1.1.33.906
semi supervised clustering user feedback 
unpublished 
demiriz bennett embrechts 
semi supervised clustering genetic algorithms 
artificial neural networks engineering 
hartigan wong 
means clustering algorithm 
applied statistics 
klein kamvar manning :10.1.1.11.5360
instance level constraints space level constraints making prior knowledge data clustering 
proceedings nineteenth international conference machine learning 
mitchell 
machine learning 
mcgraw hill 
ng han 
efficient effective clustering methods spatial data mining 
th international conference large data bases september santiago chile proceedings 
pei zhang cho wang yu 
maple fast algorithm maximal pattern clustering 
ieee international conference data mining 
tamayo angelo mclaughlin kim black lau allen olson curran poggio mukherjee rifkin califano louis mesirov lander golub 
prediction central nervous system tumour outcome gene expression 
nature 
procopiuc jones agarwal murali 
monte carlo algorithm fast projective clustering 
acm sigmod international conference management data 
quinlan 
programs machine learning 
morgan kaufmann 
rand :10.1.1.20.7363
objective criteria evaluation clustering methods 
journal american statistical association 

integrating declarative knowledge hierarchical clustering tasks 
international symposium intelligent data analysis 
wagstaff cardie :10.1.1.58.3667
clustering instance level constraints 
proceedings seventeenth international conference machine learning 
wagstaff cardie rogers :10.1.1.20.7363
constrained means clustering background knowledge 
proceedings eighteenth international conference machine learning 
wang wang yang yu 
clustering pattern similarity large data sets 
acm sigmod international conference management data 
xing ng jordan russell :10.1.1.58.3667
distance metric learning application clustering 
advances neural information processing systems 
yeung ruzzo :10.1.1.28.8161
empirical study principal component analysis clustering gene expression data 
bioinformatics 
yip cheung ng 
harp practical projected clustering algorithm 
ieee transactions knowledge data engineering 
appear 
yip 
harp practical projected clustering algorithm mining gene expression data 
master thesis university hong kong december 
www cs hku hk papers thesis pdf 
zhang ramakrishnan livny 
birch efficient data clustering method large databases 
acm sigmod international conference management data 
