bayesian learning richard dearden department computer science university british columbia vancouver bc canada dearden cs ubc ca nir friedman computer science division soda hall university california berkeley ca nir cs berkeley edu stuart russell computer science division soda hall university california berkeley ca russell cs berkeley edu central problem learning complex environments balancing exploration untested actions exploitation actions known 
benefit exploration estimated classical notion value information expected improvement decision quality arise information acquired exploration 
estimating quantity requires assessment agent uncertainty current value estimates states 
adopt bayesian approach maintaining uncertain information 
extend watkins learning maintaining propagating probability distributions values 
distributions compute myopic approximation value information action select action best balances exploration exploitation 
establish convergence properties algorithm show experimentally exhibit substantial improvements known model free exploration strategies 
reinforcement learning rapidly growing area interest ai control theory 
principle reinforcement learning techniques allow agent competent simply exploring environment observing resulting percepts rewards gradually converging estimates value actions states allow behave optimally 
particularly control problems reinforcement learning may significant advantages supervised learning requirement skilled human provide training examples second exploration process allows agent competent areas state space seldom visited human experts training examples may available 
addition ensuring robust behavior state space exploration crucial allowing agent discover reward structure environment determine optimal policy 
sufficient incentive explore agent may quickly settle policy low utility simply looks better current address institute computer science hebrew university ram jerusalem israel nir cs huji ac il 
copyright american association artificial intelligence www aaai org 
rights reserved 
unknown 
hand agent keep exploring options reason believe suboptimal 
exploration method balance expected gains exploration cost trying possibly suboptimal actions better ones available exploited 
optimal solution exploration exploitation tradeoff requires markov decision problem information states set possible probability distributions environment models arrived executing possible action sequences receiving possible percept sequence reward sequence 
aim find policy agent maximizes expected reward 
problem defined prior distribution possible environments easy solve exactly 
solutions known restricted cases called bandit problems environment single state actions unknown rewards 
section discusses existing approaches exploration model free learning algorithm underlying learning method 
presents new approaches exploration value sampling wyatt proposed value sampling method solving bandit problems 
idea represent explicitly agent knowledge available rewards probability distributions action selected stochastically current probability optimal 
probability depends monotonically current expected reward exploitation current level uncertainty actual reward exploration extend approach multi state reinforcement learning problems 
primary contribution bayesian method representing updating propagating probability distributions rewards 
myopic vpi myopic value perfect information provides approximation utility action terms expected improvement decision quality resulting new information 
provides direct way evaluating exploration exploitation tradeoff 
value sampling myopic vpi uses current probability distributions rewards control exploratory behavior 
section describes algorithms detail bayesian approach computing reward distributions 
section prove convergence results algorithms section describe results 
current state 
select action perform 

reward received performing resulting state 
update reflect observation follows gamma ff ff fl max ff current learning rate 

go step 
learning algorithm 
number experiments comparing exploration strategies 
experiments myopic vpi uniformly best approach 
learning assume reader familiar basic concepts mdps see kaelbling 
notation mdp tuple set states set actions transition model captures probability reaching state execute action state reward model captures probability getting reward executing action state focus infinite horizon mdps discount factor fl 
agent aim maximize expected discounted total reward fl denotes reward received step letting denote optimal expected discounted reward achievable state denote value executing standard bellman equations max delta fl reinforcement learning procedures attempt maximize agent expected reward agent know focus learning simple elegant model free method learns values learning model section discuss results carry model learning procedures 
learning agent works estimating values experiences 
select actions values 
algorithm shown 
action performed state infinitely ff decayed appropriately eventually converge 
strategy select action perform step crucial performance algorithm 
reinforcement learning algorithm balance exploration exploitation 
commonly methods semi uniform random exploration boltzmann exploration 
semi uniform random exploration best action selected probability probability gamma action chosen random 
cases initially set quite low encourage exploration slowly increased 
boltzmann exploration sophisticated approach probability executing action state pr temperature parameter decreased slowly time decrease exploration 
approach probability action selected increases current estimate value 
means sub optimal actions tend selected clearly poor actions 
exploration methods undirected meaning exploration specific knowledge 
number directed methods proposed best known interval estimation 
directed techniques thought selecting action perform expected value action plus exploration bonus 
case interval estimation assume normal observed values action state select action maximizing upper bound gamma ff confidence interval confidence coefficient ff distribution 
exploration bonus interval estimation half width confidence interval 
exploration proposed frequency recency action performed difference predicted observed values 
exploration specific information interval estimation algorithm strictly local nature 
exploration bonus calculated values observed current state 
exploration done globally selecting actions believe lead explored parts state space 
backing exploration specific information values 
meuleau bourgine propose closely related interval estimation backs values uses compute local exploration bonus 
interval estimation backs exploration bonus combines compute new exploration value action 
survey directed undirected exploration techniques see 
bayesian learning consider bayesian approach learning probability distributions represent uncertainty agent estimate value state 
case undirected exploration techniques select actions perform solely basis local value information 
keeping propagating distributions values point estimates informed decisions 
shall see results global exploration explicit exploration bonus 
value distributions bayesian framework need consider prior distributions values update priors agent experiences 
formally random variable denotes total discounted reward received action executed state optimal policy followed 
initially uncertain distributed particular want learn value 
start making simplifying assumption assumption normal distribution 
claim assumption fairly reasonable 
accumulated reward discounted sum immediate rewards random event 
appealing central limit theorem fl close underlying mdp ergodic optimal policy applied approximately normally distributed 
assumption implies model uncertainty distribution suffices model distribution mean precision 
precision normal variable inverse variance oe turns simpler represent uncertainty precision variance 
course mean corresponds value 
assumption prior beliefs independent assumption prior distribution independent prior distribution assumption fairly innocuous restricts form prior knowledge system 
note assumption imply posterior distribution satisfy independencies 
return issue 
assume prior distributions parameters particular family assumption prior normal gamma distribution 
define motivate choice distribution 
see details 
normal gamma distribution mean precision unknown normally distributed variable determined tuple hyperparameters ae ff fii 
say ng ff fi gamma gamma ff gamma fi standard results show update prior distribution receive independent samples values theorem ng ff fi prior distribution unknown parameters normally distributed variable independent samples ng ff fi nm ff ff fi fi gamma gamma single normal gamma prior posterior sequence independent observations distribution 
assumption implies represent agent prior distribution need maintain tuple hyperparameters ae ff fi assumptions represent prior collection hyperparameters state action theorem implies independent samples compact representation joint posterior 
assume posterior form assumption stage agent posterior independent posterior mdp setting assumption violated agent observations reward go different states actions strongly correlated fact related bellman equations 
shall assume represent posterior observations independent collection hyperparameters ae normal gamma posterior mean precision parameters exploit compact representation bayesian learning algorithm similar standard qlearning algorithm storing value store hyperparameters ae sections address remaining issues select action current belief state mdp update beliefs transition 
action selection iteration learning algorithm need select action execute 
assuming probability distribution states actions select action perform current state 
consider different approaches call greedy value sampling myopic vpi 
greedy selection possible approach greedy approach 
approach select action maximizes expected value 
unfortunately easy show simply estimate mean greedy approach select action greatest mean attempt perform exploration 
particular take account uncertainty value 
value sampling described wyatt exploration multi armed bandit problems 
idea select actions stochastically current subjective belief optimal 
action performed probability pr arg max pr gamma pr pr dq step derivation justified assumption states posterior distribution values separate actions independent 
evaluate expression marginal density normal gamma distribution 
action action action action examples value distributions actions value sampling exploration policy payoff exploration higher 
lemma ng ff fi gamma delta fi ff ff ff fi gamma gamma ff pr gamma ff fi ff cumulative degrees freedom 
var fi ff gamma practice avoid computation 
sample value execute action highest sampled value 
straightforward show procedure selects probability 
course sampling form nontrivial requires evaluation cumulative distribution 
fortunately evaluated efficiently standard statistical packages 
experiments library routines brown 
value sampling resembles extent boltzmann exploration 
stochastic exploration policy probability performing action related distribution associated values 
drawback value sampling considers probability best action consider amount choosing improve current policy 
show examples cases value sampling generate exploration policy 
cases pr 
case exploration useful case potential larger rewards higher second action case 
myopic vpi selection method considers quantitatively question policy improvement exploration 
value information 
application context reminiscent tree search seen form exploration 
idea balance expected gains exploration form improved policies expected cost doing potentially suboptimal action 
start considering gained learning true value knowledge change agent rewards 
clearly knowledge change agent policy rewards change 
interesting scenarios new knowledge change agent policy 
happen cases new knowledge shows action previously considered sub optimal revealed best choice agent beliefs actions new knowledge indicates action previously considered best inferior actions 
derive value new information cases 
case suppose best action actions suppose new knowledge indicates better action 
expect agent gain gamma virtue performing case suppose action highest expected value second best action 
new knowledge indicates agent perform expect gain gamma summarize discussion define gain learning value gain gamma gamma actions best second best expected values respectively 
agent know advance value revealed need compute expected gain prior beliefs 
expected value perfect information vpi gamma gain pr dx simple manipulations reduce vpi closed form equation involving cumulative distribution computed efficiently 
proposition vpi equal gamma pr equal gamma pr ff ff fi ff gamma ff ff ff gammaff value perfect information gives upper bound myopic value information exploring action expected cost incurred exploration difference value value current best action max gamma 
suggests choose action maximizes vpi gamma max gamma clearly strategy equivalent choosing action maximizes vpi see value exploration estimate way boosting desirability different actions 
agent confident estimated values vpi action close agent choose action highest expected value 
updating values turn question update estimate distribution values executing transition 
analysis updating step complicated fact distribution values distribution expected total rewards available observations instances actual local rewards 
bayesian updating results theorem directly 
suppose agent state executes action receives reward lands state know complete sequence rewards received onwards available 
random variable denoting discounted sum rewards assume agent follow apparently optimal policy distributed action highest expected value hope substitute way unknown experiences 
discuss ways going 
moment updating idea moment updating randomly sample values distribution update sample flr flr take sample weight theorem implies need moments sample update distribution 
assuming tends infinity moments flr fle flr fl fl estimate distribution distribution mean variance standard properties normal gamma distributions compute moments lemma normally distributed variable unknown mean unknown precision ng ff fi 
delta fi ff gamma update hyperparameters ae seen collection examples total weight mean second moment approach results simple closed form equation updating hyperparameters unfortunately quickly confident value mean see note roughly interpret parameter confidence estimate unknown mean 
clear value perfect information optimistic assessment value performing performing get perfect information training instance 
consider weighting vpi estimate constant 
leave 
method just described updates mean unknown reward just fle confident true sample 
uncertainty value represented second moment mainly affects estimate variance uncertainty directly translated uncertainty mean leads higher estimate variance upshot precision mean increases fast leading low exploration values premature convergence sub optimal strategies 
ad hoc way dealing problem exponential forgetting 
method reduces impact previously seen examples priors constant usually close update 
due space considerations review details forgetting operation 
mixture updating problem described preceding section avoided distribution slightly different way 
posterior distribution observing discounted reward observed value updated distribution flx 
capture uncertainty value weighting distribution probability results mixture posterior mix gamma flx dx unfortunately posterior mix simple representation updating posterior lead complex 
avoid complexity mix distribution update 
compute best normal gamma approximation minimizing kl divergence true distribution 
theorem density measure ffl 
constrain ff greater ffl ng ff fi minimizes divergence kl defined equations gamma gamma delta gamma ff max ffl log gamma log fi ff inverse log gamma digamma function 
requirement ff ffl ensure ff normal gamma defined 
theorem give closed form solution ff find numerical solution easily monotonically decreasing function 
complication approach requires compute log respect mix 
expectations closed form solutions approximated numerical integration formulas derived fairly straightforwardly theorem 
summarize section discussed possible ways updating estimate values 
moment update leads easy closed form update overly confident 
second mixture update cautious requires numerical integration 
convergence interested knowing algorithms converge optimal policies limit 
suffices show means converge true values variance means converges 
case value sampling myopic vpi strategies eventually execute optimal policy 
going details standard convergence proof learning requires action tried infinitely state infinite run ff ff ff learning rate 
conditions met theorem shows approximate values converge real values 
theorem show moment updating algorithm converges correct mean 
theorem action tried infinitely state algorithm uses moment updating mean converges true value state action moment updating prove variance eventually vanish theorem action tried infinitely state algorithm uses moment method update posterior estimates variance var converges state action combining results see moment updating procedure converge optimal policy actions tried eventually 
case select actions value sampling 
select actions myopic vpi longer guarantee action tried infinitely 
precisely myopic vpi starve certain actions apply results 
course define noisy version action selection strategy boltzmann adjusted expected values guarantee convergence 
stage counterparts theorems mixture updating 
conjecture estimated mean converge true mean similar theorems holds 
experimental results examined performance approach different domains compared number different exploration techniques 
parameters algorithm tuned possible domain 
algorithms follows semi uniform learning semi uniform random exploration 
task 
task 
task 
navigation problem 
start state 
agent receives reward reaching number flags collected 
domains experiments 
boltzmann learning boltzmann exploration 
interval learning kaelbling interval estimation algorithm 
meuleau algorithm 
bayes bayesian learning value sampling myopic vpi select actions moment updating mixture updating value updates 
variants denoted qs vpi mom mix respectively 
possible variants bayesian learning algorithm denoted example vpi mix 
tested learning algorithms domains chain domain consists chain states shown 
consists states actions probability agent slips performs opposite action 
optimal policy domain assuming discount factor action 
learning algorithms get trapped initial state preferring follow loop obtain series smaller rewards 
loop domain consists loops shown 
actions deterministic 
problem learning algorithm may converged action state larger reward available state backed 
optimal policy action 
st phase nd phase domain method avg 
dev 
avg 
dev 
chain uniform boltzmann interval bayes qs mom bayes qs mix bayes vpi mom bayes vpi mix loop uniform boltzmann interval bayes qs mom bayes qs mix bayes vpi mom bayes vpi mix maze uniform boltzmann interval bayes qs mom bayes qs mix bayes vpi mom bayes vpi mix table average standard deviation accumulated rewards runs 
phase consists steps chain loop steps maze 
maze maze domain agent attempts collect flags get goal 
experiments maze shown 
marks start state marks goal state marks locations flags collected 
reward received reaching number flags collected 
agent reaches goal problem reset 
total states mdp 
agent actions left right 
small probability agent slip perform action goes perpendicular direction 
agent attempts move wall position change 
challenge sufficient exploration collect flags reaching goal 
domains designed suboptimal strategies exploited 
learning algorithm converges fast discover higher scoring alternatives 
third domain larger tricky admits inferior policies 
evaluate various exploration scale 
ways measuring performance learning algorithms 
example want measure quality policy recommend number steps 
unfortunately misleading algorithm recommend continue explore receive smaller rewards 
measured performance learning algorithms total reward collected fixed number time steps table 
additionally measured discounted total reward go point run 
precisely suppose agent receives rewards run length define reward go time fl gammat course estimate reliable points far run 
plot average reward go function averaging values runs different random interval bayes voi mom bayes voi mix bayes qs mom results chain domain 
interval bayes voi mom bayes voi mix bayes qs mom results loop domain 
boltzmann interval bayes voi mom bayes voi mix results maze domain 
plots actual discounted reward axis function number steps axis methods domains 
curves runs method 
curves chain maze smoothed 
seeds 
results show smallest domains methods competitive superior state art exploration techniques 
analysis suggests due methods effective small numbers data points 
results maze domain particular show vpi methods directing search promising states making significantly fewer observations interval estimation 
mixture updating combined vpi action selection gives best performance expect valuable techniques expand model learning 
weakness algorithms significantly parameters interval estimation 
full version analyze dependence results various parameters 
main parameters effect performance method variance initial prior ratio fi ff gamma priors larger variances usually lead better performance 
described bayesian approach learning exploration exploitation directly combined representing values probability distributions distributions select actions 
proposed methods action selection value sampling 
experimental evidence shown fairly simple problems approaches explore state space effectively conventional model free learning algorithms performance advantage appears increase problems larger 
due action selection mechanism takes advantage information previous approaches 
major issue computational requirements greater conventional learning action selection updating values 
note applications reinforcement learning performing actions expensive tha computation time 
currently investigating ways bayesian approach model reinforcement algorithms 
case explicitly represent uncertainty dynamics system estimate usefulness exploration 
investigating alternative action selection schemes approximations reduce computational requirements algorithm 
possible function approximators extend problems large continuous state spaces 
understood theory bayesian neural network learning ch 
allows posterior means variances computed point input space fed directly algorithm 
performed parameter adjustment find best performing parameters method 
results reported algorithm probably somewhat optimistic 
full version intend show sensitivity method changes parameters 
acknowledgments grateful useful comments david andre craig boutilier daphne koller ron parr 
nicolas meuleau help implementing algorithm 
nir friedman stuart russell supported part aro muri program integrated approach intelligent systems number daah onr number 
abramowitz stegun editors 
handbook mathematical functions 
dover 
bellman 
dynamic programming 
princeton univ press 
berry 
bandit problems sequential allocation experiments 
chapman hall 
bishop 
neural networks pattern recognition 
oxford univ press 
brown russell 
library routines cumulative distribution functions inverses parameters 
ftp 
tmc edu pub source 
tar gz 
cover thomas 
elements information theory 
wiley 
degroot 
statistics 
addisonwesley 
howard 
information value theory 
ieee trans 
systems science cybernetics ssc 
kaelbling littman moore 
reinforcement learning survey 
artificial intelligence research 
kaelbling 
learning embedded systems 
mit press 
bourgine 
exploration multi states environments local measures back propogation uncertainty 
machine learning 
appear 
russell wefald 
right thing studies limited rationality 
mit press 
thrun 
role exploration learning control 
white eds handbook intelligent control neural fuzzy adaptive approaches 
van nostrand reinhold 
watkins 
models delayed reinforcement learning 
phd thesis psychology department cambridge university 
watkins dayan 
learning 
machine learning 
whitehead ballard 
learning act trial error 
machine learning 
wyatt 
exploration inference learning reinforcement 
phd thesis department artificial intelligence university edinburgh 
