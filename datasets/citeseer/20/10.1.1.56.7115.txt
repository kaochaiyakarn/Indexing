reinforcement learning perceptual aliasing perceptual distinctions approach chrisman school computer science carnegie mellon university pittsburgh pa chrisman cs cmu edu known perceptual aliasing may significantly diminish effectiveness reinforcement learning algorithms whitehead ballard perceptual aliasing occurs multiple situations indistinguishable immediate perceptual input require different responses system 
example robot see forward presence battery charger determines backup immediate perception insufficient determining appropriate action 
problematic reinforcement algorithms typically learn control policy immediate perceptual input optimal choice action 
introduces predictive distinctions approach compensate perceptual aliasing caused incomplete perception world 
additional component predictive model utilized track aspects world may visible times 
addition control policy model learned allow stochastic actions noisy perception probabilistic model learned experience 
process system discover important distinctions world 
experimental results simple simulated domain additional issues discussed 
reinforcement learning techniques received lot interest due potential application problem learning situated behaviors robotic tasks sutton lin mahadevan connell mill chapman kaelbling 
objective reinforcement learning agent acquire policy choosing actions maximize performance 
action environment provides feedback form scalar reinforcement value discounted cumulative reinforcement customarily assess performance 
predictive model reinforcement learner act reward act percepts internal state value data flow system components 
effectiveness reinforcement learning techniques may significantly diminish exist pertinent aspects world state directly observable 
difficulty arises whitehead ballard termed perceptual aliasing perceptually identical states require different responses 
agent learns behavior function immediate percepts choice action susceptible perceptual aliasing effects 
common factors presence physical obstructions limited sensing resources restricted field view resolution actual sensors incomplete observability ubiquitous facet robotic systems 
lion algorithm whitehead ballard cs ql algorithm tan invoke algorithm wixson previously introduced cope perceptual aliasing 
algorithms compensates aliasing effects accessing additional immediate sensory input 
introduces new approach overcomes limitations previous techniques important ways 
assumptions deterministic actions noiseless sensing dropped 
second new technique applies tasks requiring memory result incomplete perception chrisman example warehouse robot permanently closed sealed box box contents determines action necessary remember box contents 
incomplete perception sort overcome obtaining additional immediate perceptual input 
current predictive distinction approach introduces additional predictive model system predictive models reinforcement learning systems various purposes experience shown 
predictive model tracks world state various features visible times 
learning transfer function percepts evaluation action reinforcement learning learns transfer function internal state predictive model action evaluation 
deterministic actions noiseless sensing assumed predictive model probabilistic sufficient predictive model usually supplied system priori model acquired improved part learning process 
learning model involves estimating transition observation probabilities discovering states world drescher 
perceptual discriminations longer assumed correspond directly world states 
noisy sensor may possible observe different percepts state perceptual incompleteness may cause identical percepts register distinct world states 
experiments agent begins initially small randomly generated predictive model 
agent proceeds execute actions world performing variation learning watkins action selection internal state predictive model perceptual input 
experience gathered experience improve current predictive model 
maximum likelihood estimation probabilities updated 
program attempts detect distinctions world missing current model 
experience gives statistically significant evidence support missing discrimination new distinction introduced recursively partitioning internal state space model probabilities 
system cycles new improved model support learning 
section introduces general form predictive model bayesian estimation procedure uses update belief state world 
process reinforcement learning model provided discussed followed model learning algorithm 
empirical results reviewed important issues research topics listed 
predictive model predictive model theory predictions effects actions agent expects perceive 
general may necessary maintain internal state order track replay lin dyna sutton may possible apply recurrent neural networks learn predictive model similar fashion jordan rumelhart lin personal communication 
aspects world may occasionally unobservable agent 
example predict seen turning predictive model remember contents location time visible 
interestingly ability predict characteristic predictive models useful overcoming perceptual aliasing 
internal state formed utilized predictions valuable reinforcement learner 
central idea current approach information needed maximize usually information missing perceptually aliased inputs 
predictions need deterministic fact context deterministic models inappropriate 
models stochastic 
assumed single instant world state exactly finite number classes class ng class identity sufficient stochastically determine perceptual response action effects markov assumption 
single class model may correspond possible world states 
agent available finite set actions action pair classes specifies probability executing action class move world class class world state directly observable probabilistic clues identity available form percepts 
perceptual model specifies probability observing world state class form complete predictive model 
assumed finite nominal unordered variable 
predictive models form commonly referred partially observable markov decision processes lovejoy monahan 
actual class world instant directly observable result general possible determine current class absolute certainty 
belief maintained form probability vector believed probability class current time action executed new observation obtained bayesian conditioning update belief vector follows delta action executed sensed percept normalizing constant chosen components sum 
reinforcement learning predictive model available system task reinforcement learner learn value action possible belief state 
specifically system learn function theta returns estimated cumulative discounted rewards referred value action vector probabilities specifying belief state world 
learn function variation learning algorithm watkins barto 
learning algorithm modified case domain discrete finite unmodified algorithm requires 
learn function simple approximation 
class predictive model action value learned 
approximated approximation accurate model highly predictive class distinctions impact optimal control probability mass usually distributed classes agree optimal action 
approximation works applications requiring memory address perceptual aliasing inappropriate choosing information gathering actions active perception 
learning involves adjusting values done learning rule class treated fractionally occupied 
entry adjusted fraction update rule gamma fi fi flu max action taken reward received fl discount factor fi learning rate 
updated action 
model identifies single class non zero probability mass update rule reduces conventional learning 
predictive model hand updating rule place scenario part system reinforcement learning systems 
cycle agent obtains current state estimate case predictive model executes action having largest 
action completes bayesian conditioning uses new perceptual input obtain 
updating rule applied cycle repeats 
model learning experience obtained agent executing actions resulting perceptual input improve current predictive model 
task model learning algorithm obtain best predictive model experience 
involves aspects 
set classes action transition probabilities observation probabilities adjusted order maximize 
second algorithm detect incorporate additional distinctions existing world currently accounted model 
incorporating new distinction involves enlarging number classes recognized model 
initial predictive model supplied system system begins state model randomized probabilities uses algorithm improve enlarge experience 
probabilistic model learning involves statistical assessment making necessary collect body experience running model learning algorithm 
agent executes prespecified number cycles recording action observation pair continuously performing modified learning 
agent invokes model learning algorithm recorded experience input produce improved predictive model 
entire process repeats 
policy model learning interleaved model learning sensitive current control policy jordan rumelhart 
control policy tends optimality recorded experience primarily composed states path goal achievement leading model learning algorithm learn situations impact goal achievement 
probability adjustment task improving model adjusting probabilities maximize model 
simultaneously learning action perception models presents difficulties true class world directly revealed system 
true class world known instant problem trivial transition observation frequencies simply counted 
possible probability distribution representing belief class world 
example time drop action executed assess expected actual transition 
count number times drop action results transition class class incremented theta 
expected counts tallied counts divided usual obtain expected frequencies resulting model probabilities 
frequency counting baum forward backward procedure rabiner obtain improved estimate 
baum forward backward procedure efficient dynamic programming algorithm run time complexity delta jj length experience trace jj size model number probabilities model 
model denoted adjusted procedure process repeated quiescence reached 
current system quiescence detected parameter model changes 
denote action taken time baum proved iteration algorithm guaranteed improve predictive power model measured pr ja quiescence reached 
algorithm portion model learning variant welsh algorithm maximum likelihood estimation adapted learn partially observable markov decision processes 
discovering new distinctions generally known classes suffice obtaining necessary level predictability classes 
second portion algorithm responsible detecting current model missing important distinctions incorporating model 
primary challenge discovering important distinctions detecting difference random chance underlying hidden influences missing current model 
done system performing experiments slightly different conditions comparing experiences 
behavior system differs statistically significant amount experiments determined underlying influence missing model unknown influence partially determined experimental conditions new distinction introduced 
current case turns equivalent detecting markov property model hold 
model learning algorithm invoked sequential list action observation pairs recorded algorithm input 
experience groups earliest half forming group latest half forming second group 
reinforcement learning actively changing agent policy experience gathered experimental conditions determined policy slightly different groups 
forms basis detecting missing distinction 
class model expected frequencies group tallied 
action class yields estimates number expected transitions action groups respectively 
sampled distributions compared statistically significant difference chi squared test 
similar test performed observation counts expected number times observed class test shows statistically significant difference distribution class split causing total number classes increment 
somewhat reminiscent algorithm chapman kaelbling class split complete model learning algorithm recursively re invoked 
experimental results section reports results applying complete system simple simulated docking application incomplete perception non deterministic actions noisy sensors 
scenario consists space stations separated small amount free space loading located station 
task transport supplies 
time agent successfully attaches visited station receives reward 
order dock agent position front station back dock backup 
agent collides station forward dock receives penalty 
times receives zero reinforcement 
actions available agent backup turnaround 
agent facing exactly stations turnaround causes face 
depending current state action loading dock launches free space approaches station free space collides penalty space station directly ahead 
backup inverse extremely unreliable 
backup launches station probability 
space approaches station reverse probability 
docking position fails dock time 
actions fail agent remains position accidentally gets randomly turned 
agent perception limited 
station sees station empty space depending way facing 
free space perception noisy probability agent sees forward station sees empty space 
stations appear identical agent visited station displays accepting deliveries sign visible agent exactly station visible 
interior dock visible 
reward observable time unit receipt 
system began randomly generated state predictive model zero initialized values delta delta 
model learning invoked actions improve model 
complete cycle repeated times 
discounting rate fl learning rate fi reinforcement learning 
detect missing distinctions chi squared test significance level ff 
run agent executed random action probability executed action largest rest time 
dot plotted corresponding time steps auto estimated performance estimated performance training 
agent best estimate utility current choice action decision cycle density dots lines appear eventually stand 
correspond states world agent spends time optimal policy learned 
loading dock agent launches free space approaches station turns backs attach dock 
egocentric viewpoint agent appears started 
run recognizes situations value estimate fairly stable forming discernible lines graph 
frequently visited states system learns predictive model total classes 
final learned model compared simulator reality 
case learned model distinguishes different regions free space station visible sets transition probabilities launching respective region free space 
model exactly equivalent simulator different ontology 
optimal path model detailed accurate extra classes necessary 
optimal path existing spurious distinctions identified transition probabilities error undoubtedly due lack experience situations 
agent estimate performance misleading indication actual performance 
comparing graph measured cumulative discounted rewards verified give valid indication actual performance convergence rate appear somewhat worse provides far information actual performance internal workings system 
additional issues system run additional simple simulated applications experience number issues identified 
applications system performed poorly leading investigation underlying reasons identification issues 
additional concerns listed 
dealing limitations constitutes area research 
exacerbated exploration problem exploration exploitation tradeoff known difficulty reinforcement learning general kaelbling thrun problem amplified tremendously perceptual incompleteness 
additional stems fact state space structure provided system discovered 
result agent tell difference unexplored portions world heavily explored portions world discovered difference areas look 
inherent problem accompanying incomplete perception unique current approach 
increasing frequency choosing random actions overcame problem reason believe efficiently overcoming problem general may require external teacher whitehead lin 
problem extended concealment crucial features domains characteristic hidden feature influence crucial performance feature rarely allows influence perceived 
single significant limitation predictive distinction approach 
problem high quality prediction possible crucial feature ignored 
words internal state useful making predictions may cases include internal state necessary selecting actions 
problem arises space station docking domain accepting deliveries sign leaving agent difficult task discovering crucial concept visited 
common current system learn classes necessary 
nearly identical states may desirable mahadevan connell 
utile distinction conjecture possible introduce class distinctions impact utility color block perceivable irrelevant agent task possible agent avoid introducing color distinction model time learning distinctions utile 
conjecture possible necessary recognize distinction gather experience distinction identified order obtain information regarding utility distinction 
refutation conjecture extremely provide ideal solution input generalization problem chapman kaelbling 
perceptual aliasing presents serious troubles standard reinforcement learning algorithms 
standard algorithms may unstable result perceptually identical states require different responses whitehead ballard 
predictive distinction approach uses predictive model track portions world totally observable 
assumes adequate model supplied system model learned 
model fully probabilistic learning involves learning transition perception probabilities discovering important underlying class distinctions exist world 
bayesian updating conditioning track world state variation learning watkins learns mapping internal state model utility possible action 
approach central idea internal state useful prediction may capture important information choosing actions missing perceptually aliased inputs 
am grateful tom mitchell help guidance development tom mitchell reid simmons sebastian thrun reading making helpful comments previous drafts 
research sponsored nasa contract number 
views contained author interpreted representing official policies implied nasa government 
barto andy bradtke steven singh satinder 
real time learning control asynchronous dynamic programming 
technical report coins department computer science university massachusetts 
baum leonard petrie ted soules george weiss norman 
maximization technique occurring statistical analysis probabilistic functions markov chains 
annals statistics 
chapman david kaelbling leslie pack 
input generalization delayed reinforcement learning algorithm performance comparisons 
ijcai 
chrisman caruana rich wayne 
intelligent agent design issues internal agent state incomplete perception 
proc 
aaai fall symposium sensory aspects robotic intelligence 
drescher gary 
minds constructivist approach artificial intelligence 
mit press 
jordan michael rumelhart david 
forward models supervised learning distal teacher 
cognitive science 
press 
see mit center cognitive science occasional 
kaelbling leslie pack 
learning embedded systems 
ph dissertation stanford university 
tr 
lin long ji 
programming robots reinforcement learning teaching 
proc 
ninth national conference artificial intelligence 
lin long ji 
personal communication 
lovejoy william 
survey algorithmic methods partially observable markov decision processes 
annals operations research 
mahadevan sridhar connell jonathan 
automatic programming behavior robots reinforcement learning 
proc 
ninth national conference artificial intelligence 
mill jos del 
learning avoid obstacles reinforcement 
proc 
eighth international machine learning workshop 
monahan george 
survey partially observable markov decision processes theory models algorithms 
management science 
rabiner lawrence 
tutorial hidden markov models selected applications speech recognition 
proceedings ieee 
sutton richard 
integrated architecture learning planning reaction approximating dynamic programming 
proc 
seventh international conference machine learning 
tan ming 
cost sensitive reinforcement learning adaptive classification control 
proc 
ninth national conference artificial intelligence 
thrun sebastian 
efficient exploration reinforcement learning 
technical report cs cmu school computer science carnegie mellon university 
watkins chris 
learning delayed rewards 
ph dissertation cambridge university 
whitehead steven ballard dana 
learning perceive act trial error 
machine learning 
whitehead steven 
complexity analysis cooperative mechanisms reinforcement learning 
proc 
ninth national conference artificial intelligence 
wixson lambert 
scaling reinforcement learning techniques modularity 
proc 
eighth international machine learning workshop 
