relative performance various mapping algorithms independent sizable variances run time predictions robert armstrong debra hensgen taylor kidd computer science department naval postgraduate school monterey ca study performance mapping algorithms 
algorithms include naive ones opportunistic load balancing olb limited best assignment lba intelligent greedy algorithms nm greedy algorithm greedy algorithm 
algorithms olb expected run times assign jobs machines 
expected run times rarely deterministic modern networked server systems experimentation determine plausible run time distributions 
distributions execute simulations determine mapping algorithms perform 
performance comparisons show greedy algorithms produce schedules executed perform better naive algorithms exact run times available schedulers 
conclude intelligent mapping algorithms beneficial expected time completion job deterministic 
describes experiments simulations executed determine relative performance certain mapping algorithms different heterogeneous environments 
assume jobs independent 
communicate synchronize 
type architecture common today lan distributed server environment 
goal determine intelligent mapping algorithms beneficial research supported darpa contract number 
additional support provided naval postgraduate school institute joint warfare analysis 
jobs run exactly amount time expected 
intelligent mapping algorithms utilize expected run times job different machine attempt minimize scalar performance metric 
experiments metric time job completes 
particular concerned beneficial intelligent mapping jobs run substantially different amount time expected accurately characterized statistically 
determining perfect mapping np complete problem examined performance different polynomial heuristics 
algorithms chose listed 
ffl naive algorithm known opportunistic load balancing olb 
algorithm simply places job order arrival available machine 
ffl simple nm algorithm known limited best assignment lba 
algorithm uses expected run time job machine 
assigns job machine expected run time ignoring loads machines including produced jobs assigned 
algorithm easily implementable scheduling framework automatically assigns jobs machines similar algorithm users remotely start jobs hand supercomputer centers examining queue lengths 
ffl greedy algorithms order nm order 
algorithms expected run time job machine expected loads machine 
algorithms fully described section 
primary reasons study jobs rarely execute exactly expected run time expected run times exactly known 
system job exclusive machine differences actual predicted run times occur compute characteristics known enumerated designer program time access memory disk stochastic deterministic 
course environments additional non determinism due jobs running machine simultaneously shared network shared file server 
focuses cases jobs scheduled run times differ substantially expected run time 
cases seek determine advantage algorithm expected run times computationally simpler algorithm require estimating run times opportunistic load balancing olb yield equivalently performance 
section describe greedy algorithms experiments simulations 
describe experiments concerning non determinism expected run times examine derived distributions simulations performance intelligent algorithms 
collect run times various jobs various machines analyze distributions extrapolate distributions simulations 
conclude short summary comparison related 
greedy algorithms addition simple olb lba algorithms described previous section experiments greedy algorithms 
describe algorithms detail 
algorithm nm algorithm number jobs number machines second algorithm order 
algorithm estimates expected run time job machine assuming job execute particular machine estimation set large number 
describe algorithms consider expected run times elements dimensional matrix called expected run time job machine nm algorithm smartnet documentation call fast greedy considers jobs order requested determines value mg 
assigns job machine adds ng 
remaining job mg determines value mg 
assigns job machine adds fp ng 
step assigning job best machine previous assignments 
note jobs assigned order requested 
algorithm borrowing smartnet nomenclature call simply greedy computes mappings different sub algorithms chooses mapping gives smallest sum predicted run times minimized machines 
sub algorithms similar greedy algorithm differing order assign jobs machines 
enumerate steps sub algorithm 

initialize set contain jobs 

find 
call ai min 
determine ap minp ai min 

remove scheduling job machine minp 
add ap minp ai minp 

empty return step 
idea sub algorithm step attempt minimize time job far scheduled finishes 
second sub algorithm differs third step finds minp min 
algorithm tries minimize worst case time step 
effect non determinism algorithm performance examine effect non determinism performance greedy lba algorithms described 
reason studying describing algorithms term order requested mean order job requests placed prior invocation algorithm 
investigated performance algorithms jobs sorted algorithms invoked 
lba greedy algorithms expected run time produce mappings 
major motivations determine intelligent algorithms useful actual run time non deterministic essentially sampled distribution expected run time 
order determine distributions sample run times simulation conducted experiments actual programs try determine types distributions characterize run times 
job run time distributions explained job machine runtimes typically constant vary distribution 
test performance algorithms essential draw samples run times jobs particular distribution need determine realistic distributions simulations 
repeatedly executed parallel sequential programs gathered run time statistics analyzed 
performed experiments nas benchmarks 
benchmarks determine types run time distributions may typical jobs machines 
needed determine sample parameters run time distributions reproduced simulator 
performing tests controlled environmental characteristics server location network server load number processors amount memory processor speed 
table summarizes configurations machines caesar elvis ran experiments 
caesar elvis type sgi challenge onyx proc speed mhz proc type mips processors memory mbytes secondary unified cache mb mb table configuration sgi machines caesar elvis running irix 
jobs experiments sources nasa implementation nas benchmarks implementations nas benchmarks met required criteria 
experiments version nas integer sort benchmark implemented parallel processors single processor mode 
experiments nas embarrassingly parallel ep benchmark run single processor 
explain experiments results 
integer sort executed processors experiment examined run time distribution version nas integer sort benchmark executed processors 
implemented integer sort counting sort pages algorithm 
silicon graphic light weight process thread support functions including implement version benchmark 
ran sort heavily loaded network obtaining executable data file server heavily loaded 
run caesar run time distribution executions appears gaussian 
shows histogram distribution 
run elvis run time distribution executions appears exponential shown 
note origin exponential distribution shown translated approximately 
means sort run seconds stopping 
distribution see closely matches exponential distribution mean translated seconds right 
expect jobs distribution similar jobs run amount time experiments see memory size need swap local disk definite effect run time distribution job 
integer sort elvis completes average sooner job caesar 
note case amount memory influence form distributions determined carefully selecting bin size curve fitting 
authors visual analytical tests normality analytical tests strong visual similarity frequency plots normal curve 
fact sample point frequencies lie selected normal distribution due number samples finite 
phenomena appeared data points sampled known normal run time distribution 
exponential distribution defined start 
applied translation case mean strong possibility near zero run times 
run time seconds parallel counting sort caesar samples loaded network mean sigma dat forked counting sort caesar 
run time seconds parallel counting sort elvis samples loaded network mean sigma dat forked counting sort elvis 
run time job speed processor 
primary importance observation indicating job running different machines different mean runtimes distribution run times different yielding gaussian distribution machine exponential distribution 
integer sort single processor experiment discussed section exception run single processor distributed processors 
slightly different implementation program counting sort 
integer sort run caesar elvis run time distribution easily characterized appears related gaussian distribution 
histograms distributions similar shown possibly multimodal indicates multiple distributions may 
experiment provide definitive results point fact run time distributions quite complex 
suspect conditions related changes network server loads 
run time seconds counting sort caesar single processor samples loaded network mean sigma dat counting sort caesar single processor 
set experiments showed additional memory greatly enhance run time performance 
tests elvis ran times faster run caesar faster processors 
tests show run time distributions complex may difficult reproduce simulation 
simulations complex distributions modeled 
run time seconds counting sort elvis single processor samples loaded network mean sigma dat counting sort elvis single processor 
embarrassingly parallel nas benchmark set experiments describe compared run time distributions compute intensive jobs run local disk run network file server 
tests describe section executed caesar elvis sufficiently large local disk available 
implementation nasa nas embarrassingly parallel ep benchmark 
implementation uses portable message passing interface mpi parallelize code 
tests ran compiled executed single processor ep benchmark run times test 
see figures 
simulation experiments describe simulation experiments aimed examining mapping algorithms performed jobs scheduled execute exactly mean run time 
matrices refer description rows indexed job columns indexed machine 
ffl matrix format 
different matrices containing jobs machines varying characteristics 
matrix contained mean run times different jobs different machines 
average means corresponding columns rows mpi mechanism utilized ep benchmark compiled single processor 
run time seconds epa nas benchmark caesar samples code machine network involved mean sigma epa caesar dat epa nas benchmark executable residing local disk 
run time seconds epa nas benchmark caesar samples run network mean sigma epa dat epa nas benchmark files obtained lightly loaded network 
matrices jobs quite heterogeneous 
ffl job request sets 
order obtain different results matrix generated random sequences job requests call individual request chosen uniform random distribution different jobs 
generated ran dom sets time job requests calling 
look performance variations job request orderings examine performance differences occur fewer jobs requested 
ffl job request format 
generated jobs request random 
experiments jobs requested random order 
done order job request affects schedule 
fast greedy algorithm maps schedules jobs machines order submitted 
greedy algorithm uses order break ties 
chose execute randomly ordered requests closely mimic real environment different jobs submitted different users wished examine algorithms performed better worse unsorted opposed sorted requests submitted 
ffl run time generation simulations 
executed simulation times 
run different value seed random number generator generate simulated actual runtime duration 
total time required execute schedule summed average computed 
multiple seeds ensure results skewed ffl baseline calculations 
addition simulations generated simulated run times particular distributions performed baseline calculations 
baseline calculations provided results effect equivalent running simulation run time job machine exactly expected run time 
ffl actual run time distributions 
generated run times different mean predicted run times ran experiments gaussian exponential distributions 
experiments nas ep benchmarks chose implement translated exponential distribution 
earlier experiments described section chose truncated gaussian distribution simulation experiments mimic gamma distribution best fit data 
chose truncate left mean gamma oe 
results simulation experiments jobs ran times different predicted run times set experiments examined performance intelligent mapping algorithms job run times common method reduce influence single random number generation sequence may biased 
differed expected run times develop mappings 
distributions identified previous experiments instantiated specific parameters order simulate typical jobs 
simulated jobs exponential truncated gaussian run time distributions 
summarize results individual results additional individual experiments consistent armstrong thesis 
graphs section compare final completion times jobs various mappings 
label baseline mean value represented completion time jobs ran exactly predicted mean run times 
order emphasize differences values plot graph include olb run times 
olb run times exponential gaussian distribution simulations discuss averaged seconds cases shown requests 
exponential distribution experiments results experiments compare performance various mapping algorithms jobs exponential run time distribution 
recall sample run times experiments closely fit shifted exponential distribution mean 
lba greedy fast greedy thousands run time seconds baseline exponential submission sequence exponential run time distribution results 
compare time job finishes executed mappings assuming job started machine job completes 
figures section show expected completion time assuming deterministic run times assumption run times exponentially distributed shifted right mean matches expected runtime 
shows comparisons matrices simulations 
shows schedules built intelligent mapping algorithms effective actual run time job machine differ greatly expected run time 
truncated gaussian experiments performed additional simulations examine performance intelligent mapping algorithms jobs approximately gamma run time distributions 
determined experiments approximate distribution truncating gaussian distribution left mean roughly gamma oe 
experiment mean expected run time individual job machine pair oe set 
experiments useful determining variance large jobs greedy algorithms performed better lba olb algorithms 
negative run times generated experiments truncation value positive 
results show schedules finishing previous experiments 
unexpected truncation shift mean resulting distribution right 
section provide theoretical discussion expect times 
results show greedy algorithms perform better olb lba algorithms job run time distributions truncated gaussian large variances 
experiments theoretical explanation imply may worthwhile update mapping jobs executed minimize effect large job variances 
lba greedy fast greedy run time seconds baseline gaussian submission sequence truncated gaussian run time distribution results 
theoretical explanation longer run times shown gaussian experiments theoretically predict new mean truncated distribution described section simple gaussian statistics 
loss generality explanation uses standard gaussian distribution mean standard deviation 
area distribution mean easily shown new mean new truncated distribution new gamma gamma see new mean new oe 
unfortunately truncation gaussian distribution accounts increase mean 
explanation leaves unaccounted 
remaining due factors 
traced fact truncated gaussian gamma distribution 
second fact expected value maximum gaussian distributions maximum expected values 
application known probability result quality service metrics documented 
comparison greedy algorithms note results armstrong thesis greedy fast greedy algorithms appeared perform similarly 
experiments saw greedy algorithm performing better fast greedy algorithm 
suggested improvement higher 
knowledge presenting sorted requests mapping algorithms 
theoretical explanation results scope discussed 
related knowledge studied performance intelligent heterogeneous mapping algorithms run times jobs nondeterministic distributions run times actual programs determined different resource loadings 
ibarra kim study performance algorithms concentrated 
analytical study centered determining worst case performance algorithms 
weissman simulation study interference policies policies take account fact increase load shared resource rate execution jobs decreases 
policies simulations assumed jobs executed come served basis 
study performance genetic algorithms proposed way schedule tasks heterogeneous resources particularly communication synchronization needed tasks 
systems followed lead smartnet implementing intelligent schedulers describe resource management systems 
summary experimented applications resources differing loads fitted run times distributions 
distributions determine simulation run times non deterministic beneficial intelligent algorithms expected run times compute mapping 
continues beneficial expected run time distributions large variances 
distributions simulations derived execution actual programs distributions realistic 
additional distributions realistic examined 
intend pursue 
probability statistics third ed 
freeman london england 
armstrong investigation effect different run time distributions smartnet performance 
master thesis naval postgraduate school september 
bailey nas parallel benchmarks 
tech 
rep nas nasa ames research center december 
beguelin user guide 
oak ridge national laboratory university tennessee december 
document available web cs utk edu 
cormen leiserson rivest algorithms 
mit press cambridge massachusetts 
freund kidd hensgen moore smartnet scheduling framework heterogeneous computing 
proceedings international symposium parallel architectures algorithms networks 
hensgen kidd armstrong comparison greedy algorithms scheduling jobs heterogeneous environments 
progress 
ibarra kim 
heuristic algorithms scheduling independent tasks processors 
journal acm 
kidd hensgen mean inadequate accurate scheduling decisions 
progress 
kidd hensgen freund campbell compute characteristics useful characterization job runtimes 
preparation submission 
neuman rao prospero resource manager scalable framework processor allocation distributed systems 
concurrency practice experience 
user guide mpi 
tech 
rep department mathematics university san francisco march 
singh youssef mapping scheduling heterogeneous task graphs genetic algorithms 
proceedings heterogeneous computing workshop 
wang siegel roychowdhury genetic algorithm approach task matching scheduling heterogeneous computing environments 
proceedings heterogeneous computing workshop 
weissman interference paradigm network job scheduling 
proceedings heterogeneous computing workshop 
zhou zheng wang delisle 
utopia load sharing facility heterogeneous distributed computer systems 
software practice experience 
biographies major robert armstrong currently charge modeling simulation laboratory marine corps air ground combat center california 
received bs engineering united states naval academy graduate warfare school virginia earned ms computer science naval postgraduate school monterey california 
major armstrong served capacity artillery officer st marine division korea kuwait 
interests include computer architecture distributed systems modeling simulation training 
debra hensgen received ph computer science area distributed operating systems university kentucky 
currently associate professor computer science naval postgraduate school monterey california 
moved monterey university cincinnati years ago appointed assistant professor associate professor electrical computer engineering 
research interests include resource management allocation systems tools concurrent programming 
authored numerous papers areas 
currently subject area editor journal parallel distributed computing chief architect principal investigator darpa funded mshn project part darpa larger quorum program 
taylor kidd associate professor computer science naval postgraduate school nps monterey california 
received ph electrical computer engineering university california san diego ucsd 
received ms bs electrical computer engineering ucsd respectively 
prior accepting position nps researcher navy laboratory san diego california 
current interests include distributed computing application stochastic filtering estimation theory distributed systems 
principal investigator debra hensgen darpa funded mshn project part darpa larger quorum program 
