learning mixtures bayesian networks bo thiesson christopher meek david maxwell chickering david heckerman microsoft research redmond wa meek dmax microsoft com february technical report msr tr microsoft research advanced technology division microsoft microsoft way redmond wa describe heuristic method learning mixtures bayesian networks possibly incomplete data 
considered class models mixtures mixture component bayesian network encoding conditional gaussian distribution fixed set variables 
variables may hidden missing observations 
key idea approach treat expected data real data 
allows interleave structure parameter search take advantage closed form approximations marginal likelihood 
addition treating expected data real data search criterion factors variable making search processes efficient 
evaluate approach synthetic real world data sets 
keywords mixture models bayesian networks structure learning parameter learning hidden variables em algorithm 
growing interest class models density estimation known bayesian networks 
years researchers including cooper herskovits spiegelhalter dawid lauritzen cowell buntine heckerman geiger chickering developed methods learning structure parameters bayesian networks data 
consider mixtures bayesian networks methods learning class model possibly incomplete data 
generalize bayesian networks important classes models including mixtures multivariate gaussian distributions 
take decidedly bayesian perspective problem learning structure parameters 
concentrate model selection opposed model averaging computational reasons 
key step bayesian approach learning graphical models computation marginal likelihood data set model 
quantity ordinary likelihood function data model parameters averaged parameters respect prior distribution 
data set complete sample contains observations variable model marginal likelihood calculated efficiently criterion model selection bayesian networks 
particular complete data assumptions discussed marginal likelihood bayesian network closed form factors separate marginal likelihoods family family node parents bayesian network cooper herskovits heckerman geiger 
property search network structure particularly efficient 
contrast observations missing including situations variables hidden observed exact determination marginal likelihood typically intractable cooper herskovits 
consequently approximate techniques computing marginal likelihood 
monte carlo large sample approximations available 
concentrate large sample approximations due efficiency 
efficiency refer computational efficiency opposed statistical efficiency 
data incomplete simple approach bayesian model selection search structures perform greedy search replace exact computation marginal likelihood large sample approximation 
computation approximation involves search parameter values models evaluated 
simple approach alternate parameter structure search 
unfortunately simple approach inefficient reasons 
search parameters slow 
approximations factor scores families 
visit additional model structures recompute score structure anew 
describe heuristic approach addresses problems 
heuristic approach alternate structure parameter search interleave parameter structure search 
particular interleave em steps search parameters changes model structure 
key idea allows treat expected data real data 
treatment produces search criterion closed form factorable 
demonstrate effectiveness methods real synthetic data sets 
organized follows 
section describe 
shall see mbn viewed special case bayesian multinet introduce general model 
bayesian multinet consists distinguished discrete variable set bayesian networks encodes distribution jc different possible states mbn bayesian multinet distinguished variable hidden 
section describe bayesian methods learning bayesian multinets concentrating case data complete 
section consider bayesian methods learning mbn models necessarily consider situation data complete 
section consider simple computationally expensive algorithm replaces exact computation marginal likelihood large sample approximation 
sections describe heuristic approach learning efficient simple approach 
sections describe experimental results related respectively 
bayesian multinets mixtures bayesian networks introduce notation 
denote variable upper case letter theta state value corresponding variable letter lower case 
denote set variables bold face capitalized letter letters pa jxj denote cardinality set corresponding bold face lower case letter letters pa denote assignment state value variable set 
say variable set configuration xjy xjy shorthand denote probability probability density xjy denote probability distribution mass functions density functions xjy refers probability probability density probability distribution clear context 
suppose problem domain consists variables 
bayesian network graphical factorization joint probability distribution repre sentation consists components structure set local probability distributions 
structure directed acyclic graph represents conditional independence assertions factorization joint distribution jpa pa configuration parents structure consistent local distributions associated bayesian network precisely equation 
discussion assume local distributions depend finite set parameters theta rewrite equation xj jpa refer bayesian network structure parameters 
denote assertion hypothesis precisely independence assertions implied hold true joint distribution find useful include structure hypothesis explicitly factorization joint distribution compare model structures 
particular write xj jpa notation unnecessary argument term pa simpler expression possible 
structure bayesian network encodes limited form conditional independence call context non specific conditional independence 
particular structure implies sets variables independent configuration variables independent configuration general form conditional independence sets variables may independent configuration dependent configuration bayesian multinet geiger heckerman generalization representation mbn models encode context specific conditional independence 
particular bayesian multinet distinguished variable set component bayesian networks encodes joint distribution state probability distribution bayesian multinet encodes joint probability distribution encode context specific conditional independence variables structure component bayesian network may different 
example bayesian multinet jcj 
denote structure parameters bayesian multinet addition denote structure parameters cth bayesian network component multinet 
denote hypothesis precisely independence assertions implied hold true joint distribution joint distribution encoded bayesian multinet xj cj xjc xj bayesian multinet bayesian network components 
jcj jcj parameters bayesian multinet cj shorthand conjunction refer bayesian multinet 
limit discussion situations component bayesian networks defined conditional gaussian distributions lauritzen 
particular variables may include finite discrete continuous variables discrete variable discrete parents 
local distributions variable network follows 
discrete variable local distributions consist set multinomial distributions configuration parents 
continuous variable local distributions set linear regressions continuous parents gaussian error distribution configuration discrete parents 
addition assume distinguished variable multinomial distribution 
discussion model learning consider possibility variables hidden 
delta gamma denote discrete continuous hidden variables respectively 
similarly delta gamma denote discrete continuous non hidden variables respectively 
consider important special case distinguished variable hidden 
circumstances joint distribution xj jcj xj refer models mixtures bayesian networks 
experimental evaluation learning algorithms pay special attention important special sub class mbn models 
known bayesian network variables continuous local distributions consisting linear regressions uniquely determines multivariate gaussian distribution shachter 
model structure bayesian network part determines shape multivariate gaussian distribution 
consequently mbn model hidden variables mixture components observed variables continuous corresponds mixture multivariate gaussian distributions component may different shape 
learning bayesian multinets consider bayesian approach learning bayesian multinets 
assume true joint distribution encoded bayesian multinet uncertain structure parameters 
bayesian approach encode uncertainty probability 
particular define discrete variable states correspond possible true models encode uncertainty structure probability distribution 
addition model structure define continuous vector valued variable theta configurations correspond possible true parameters 
encode uncertainty theta probability density function js 
random sample xn true distribution compute posterior distributions bayes rule jd djs djs jd js dj djs djs dj js marginal likelihood model structure hypothesis interest hyp determine probability hyp true data averaging possible models parameters rules probability jd jd example hyp may event observation xn situation obtain jd jd jd likelihood model 
approach referred bayesian model averaging 
note single model structure learned 
possible models weighted posterior probability 
model averaging appropriate analysis 
example models may desired domain understanding fast prediction 
situations select model structures possible models exhaustive 
procedure known model selection model chosen selective model averaging model chosen 
course model selection selective model averaging useful impractical average possible model structures 
model depend particular application 
example model understanding causal relationships domain necessarily model classification regression task 
model decision making quality depend alternatives available preferences decision maker 
issues discussed detail spiegelhalter 
heckerman 
log relative posterior probability model structure log jd log log log djs selective model averaging model selection 
remainder concentrate model selection log posterior model structure probability 
simplify discussion assume possible model structures equally priori case model selection log marginal likelihood 
marginal likelihood criterion consider bayesian network encodes conditional gaussian distribution theta denote parameter set associated local distribution variable buntine heckerman geiger shown parameters theta theta mutually independent parameter priors theta jb conjugate data complete missing observations marginal likelihood djb closed form computed efficiently 
observation extends bayesian multinets 
theta ic denote set parameter variables associated local distribution variable component pi denote set parameter variables pi pi jcj corresponding mixture weights 
parameters pi theta theta theta jcj theta mutually independent parameter priors theta ic js conjugate data complete marginal likelihood djs closed form 
particular log djs log jcj log jb data restricted variable data restricted variables cases term marginal likelihood trivial bayesian network having single node terms sum marginal log likelihoods component bayesian networks bayesian multinet 
djs closed form 
structure search important issue regarding model selection search model structures high scores 
consider problem finding bayesian network structure highest marginal likelihood set networks node parents 
chickering shown problem np hard 
follows immediately problem finding bayesian multinet structure highest marginal likelihood set bayesian multinets node component parents np hard 
consequently heuristic search algorithms including greedy search greedy search restarts best search monte carlo methods 
various model selection criteria including log marginal likelihood assumptions described factorable 
say criterion crit bayesian multinet structure factorable written follows crit jcj pa data restricted set pa parents component pa data restricted variables pa cases functions 
criterion factorable search efficient reasons 
component bayesian networks non interacting may search bayesian network structure component separately 
search structure component need reevaluate criterion structure 
example greedy search structure iteratively transform graph choosing transformation improves score transformation possible 
typical transformations include removal reversal addition arc constrained resulting graph acyclic 
factorable criterion need reevaluate variable number incoming arcs changed 
large sample approximations marginal likelihood learning bayesian multinets complete data seen marginal likelihood closed form 
contrast learning assumption data complete hold distinguished variable hidden 
data incomplete tractable closed form marginal likelihood available cooper herskovits 
approximate marginal likelihood monte carlo large sample methods 
straightforward approach model selection search structures perform greedy search replace exact computation marginal likelihood approximate 
section examine approximations 
problems wish consider monte carlo methods relatively inefficient 
consequently examine large sample approximations 
basic idea large sample approximations sample size increases jd dj delta js approximated multivariate gaussian distribution 
particular log dj delta js define configuration maximizes 
configuration maximizes jd consequently known maximum posteriori map configuration second degree taylor polynomial approximate obtain gamma gamma gamma gamma transpose row vector gamma negative hessian evaluated raising power equation obtain dj js dj js expf gamma gamma gamma approximation jd dj js gaussian 
integrating sides equation logarithm obtain approximation log djs log dj log js log gamma log ja dimension number non redundant parameters bayesian networks satisfying assumptions described section gamma 
approximation technique integration known laplace method refer equation laplace approximation 
kass 
shown certain conditions relative error approximation djs laplace gamma djs correct djs correct number cases laplace approximation extremely accurate 
compute laplace approximation determine hessian gammag evaluated class models local distribution exponential family map configuration em algorithm dempster laird rubin hessian standard techniques bernardo smith page 
obtain efficient accurate approximation retaining terms equation increase log dj increases linearly log ja increases log large approximated configuration maximizes dj known maximum likelihood ml configuration obtain log djs log dj gamma log approximation called bayesian information criterion bic 
schwarz shown relative error approximation limited class models 
extended result curved exponential models 
kass wasserman raftery shown particular priors bic relative error gamma 
approximation marginal likelihood fact djs computed efficiently complete data 
consider equality djs js js js completion data set complete data set compute js 
suppose apply laplace approximations numerator denominator second term 
roughly speaking resulting approximation best quantities js js regarded functions similar shape errors laplace approximations tend cancel 
functions similar absolute sense contains information js peaked js 
functions similar completing peak configuration want map configuration equal way obtain equality complete sufficient statistics match expected sufficient statistics choice desirable computational standpoint em algorithm find expected sufficient statistics computed expectation step 
applying laplace approximation equation numerator denominator equation fact obtain log djs log js gamma log log ja log dj gamma log ja negative hessian log js evaluated derive approximation laplace approximations equation relative error worse 
careful derivation may show accurate 
efficient approximation obtained applying bic mdl approximation numerator denominator equation 
case log djs log js gamma log log log dj gamma log map ml configuration allowed possibility dimension complete data may greater dimension actual data 
equation correction dimension introduced cheeseman stutz model selection criterion autoclass 
refer equation cheeseman stutz cs approximation 
basic idea cs approximation treat expected data complete data 
idea underlies maximization step em algorithm 
shall see section idea applied structure search 
learning practical algorithm simple approach selecting described previous section inefficient reasons 
em algorithm iterative scoring single model slow 
laplace bic cheeseman stutz scores factorable 
consequently time transformation applied structure search entire structure may need 
consider heuristic approach addresses problems 
approach completing parameter search structural change considered interleave partial parameter searches structure search 
schematic algorithm pick initial structure run em parameter search compute expected sufficient statistics complete model structure search pretending expected sufficient statistics real sufficient statistics complete data schematic algorithm mbn model selection 
shown 
choose initial model structure parameter values 
perform iterations standard em algorithm find fairly values parameters structure 
parameter values compute expected sufficient statistics complete mbn encodes conditional independence 
call statistics structure parameters data expected complete model sufficient statistics 
detailed discussion computation section 
treat expected sufficient statistics sufficient statistics complete data set perform structure search 
pretend data set complete scores closed form factorable making structure search efficient 
structure search reestimate parameters new structure find map parameters expected sufficient statistics 
em computation structure search parameter reestimation steps iterated convergence criterion satisfied 
remainder section describe important points algorithm 
discuss detail score evaluating mbn structure initialization structure parameters approach variations extensions approach 
search phase marginal likelihood expected complete model sufficient statistics js scoring mbn structure score relates cheeseman stutz approximation rewrite djs js dj differences basic part cs score marginal likelihood expected data term correction term 
correction term numerator term factor 
second cs score demands evaluation sufficient statistics basis map parameters dc da gd gb initial structure component bayesian network 
structure consideration 
avoid multiple map estimations structure search approximating sufficient statistics expected complete model sufficient statistics evaluated parameterization obtained preceding em parameter search 
consequently score structure search factorable computed efficiently 
score heuristic approximation true marginal likelihood experiments describe section suggest score guides structure search models 
approach requires initial structure initial parameterization chosen 
aspects initialization important 
initialize component structure shown 
structure arc hidden variable observed variable exception continuous variables point discrete variables 
initialize component bayesian network structure empty containing arcs conjecture algorithm discover connections hidden observed variables 
mixture components contain hidden continuous variables initialize parameters component bayesian network structure follows 
remove hidden nodes adjacent arcs creating model 
determine map configuration data data complete respect compute map closed form 
create conjugate distribution configuration maximum value agrees map configuration just computed equivalent sample size specified user 
non hidden node configuration hidden discrete parents initialize parameters local distribution drawing conjugate distribution just described 
hidden discrete node configuration possible parents initialize multinomial parameters associated local distribution fixed distribution uniform 
mixture components contain hidden continuous variables simpler approach initializing parameters random drawing distribution 
possible schedules alternating parameter structure search 
respect parameter search run em convergence step fixed number steps number steps depending times performed search phase 
respect structure search perform network transformations fixed number steps number steps depends times performed search phase find local maximum 
iterate steps consisting em computation structure search mbn structure change consecutive search phases approximate marginal likelihood resulting mbn structure increase 
second schedule algorithm guaranteed terminate marginal likelihood increase indefinitely 
schedule know proof algorithm terminate 
experiments greedy structure search schedule halts 
evaluate various combinations schedules section 
find convenient describe schedules regular grammar denote step step computation structure search respectively 
example em denote case outer iteration run em convergence compute expected complete model sufficient statistics run structure search convergence perform step 
schedule examine em schedule run em steps computing expected complete model sufficient statistics 
algorithm described compare models contain different variables models variable different number states 
perform additional search number states discrete hidden variable applying algorithm initial models different numbers states hidden variables 
discard discrete hidden variable model setting number states 
best mbn initialization identified select best structure criterion 
typically computationally expensive approximation marginal likelihood cheeseman stutz method relatively small number alternatives considered 
expected complete model sufficient statistics section examine complete model sufficient statistics closely 
particular describe statistics computation way simple example 
consider mixture bayesian networks components corresponding 
suppose observed variables hidden variable addition suppose bayesian network structure second components shown respectively 
described gamma gamma hidden observed continuous variables respectively delta delta hidden discrete variables respectively 
addition gamma denote set continuous variables fl denote configuration gamma denote number continuous variables 
delta denote set discrete variables including distinguished variable ffi denote configuration delta 
configuration observed variables configuration denote complete model sufficient statistics complete case 
bayesian multinet vector hhn rm sm ii triples number possible discrete configurations discrete variables delta 
suppose structure search leaves model structure unchanged force iteration outer loop run em convergence steps 
model structure changes forced iteration continue iterate em steps 
mixture bayesian network example 
components corresponding shown respectively 
table data set complete cases 
variables continuous remaining variables binary 
discrete variables take th configuration 
entries vectors length square matrices size theta entries fl fl fl fl transpose fl 
example data set table indexing discrete variable configurations shown table complete model sufficient statistics cases case hh ii second case case hh ii ffi table index discrete configuration 


table data set incomplete cases 
variables continuous remaining variables binary 
complete data set compute expected complete model sufficient statistics case jy case expectation taken respect joint distribution variables current case 
expectation performed inference see lauritzen 
sum simply scalar vector matrix addition appropriate triple coordinates vector 
example data set table expected complete model sufficient statistics case case hhp delta jy delta jy delta jy hp delta jy delta jy delta jy second case case hhp delta jy delta jy delta jy hp delta jy delta jy delta jy expected sufficient statistics correspond model includes class variable computation described require slot possible observation 
practice sparse representation slots complete observations consistent data 
experimental results section experimental evaluation learning algorithm synthetic real data sets 
variables domains consider continuous learning algorithm identify mixtures multivariate gaussian distributions 
provide evaluation mbn model consider types mixture models mixtures multivariate gaussian distributions covariance matrices diagonal mixtures multivariate gaussian distributions covariance matrices full 
types models correspond mbn models fixed empty structures fixed complete structures respectively components 
learning unrestricted apply versions search schedule described section em em em structure search schedules consists greedy edge modification algorithm component mixture 
particular edge additions edge deletions edge reversals greedily applied component algorithm reaches local maximum 
learning models models run em algorithm convergence identify parameters models 
deem em converged experiments models relative change log likelihood model falls gamma experiments perform outer search identify number components mixture model described section 
particular start considering mixture model containing single component run appropriate search algorithm described 
score resulting model exact formula marginal likelihood 
increase number components mixture model repeat approximate marginal likelihood cheeseman stutz score increase 
outer search returns mixture model received best cheeseman stutz score 
data set experiments generate splits data training sets test sets denoted respectively 
split algorithms identify best number components mbn model structure component map parameter configuration 
model evaluate predictive log score test set pred log results synthetic data section describe experimental results synthetic data set 
constructed data set follows 
parameterized bayesian networks 
networks stochastically sample cases 
concatenate cases form single synthetic data set consisting cases 
define gold standard mbn mbn contains components identical sampled bayesian networks mixture weights equal 
evaluate search algorithms perform comparing learned model structure structure gold standard mbn 
components gold standard mbn structure shown 
refer components comp comp respectively 
parameterizations chosen networks represent gaussians different shapes overlap 
particular comp comp uncon graphical structure third component gold standard mbn 
graphical structure second component 
number components cs score em diag em esm em esm em full cheeseman stutz score training data function number mixture components 
ditional means zero linear coefficients conditional variances 
see heckerman geiger explanation gaussian parameterization 
third bayesian network refer comp represents gaussian shape comp location shifted setting unconditional means 
cheeseman stutz score function number components shown random train test splits curves similar splits 
experiments agree correct number components mixture mixing coefficients deviating 
large number components indicates fine grained coverage non spherical gaussian multiple spherical gaussians 
table shows minimum number edge manipulations additions deletions reversals needed transform learned model model equivalent mbn 
learned close gold standard especially learned data data data search schedule comp comp comp comp comp comp comp comp comp em em eme table number edge differences learned gold standard 
learning algorithm train test split data data data table shows difference component mixture comp comp comp 
number marked situation edge deletions needed order reconstruct structure model equivalent particular component gold standard mbn 
em em interesting observation vast majority structural differences due extra edges learned model 
general comp complex comp shows minor differences comp learned perfectly 
investigate potential causes observed structural differences tried experiment 
cases generated gold standard components run ordinary greedy search starting empty model singleton bayesian networks see easily recover structure component 
comp comp learned perfectly additional edges appeared tried learn comp 
suggests structural differences gold standard mbn result forward greedy search algorithm alternation structure parameter search 
investigate claim repeated mbn learning experiments exhaustive structural search greedy search 
experiments recovered structure gold standard mbn 
exhaustive search computationally feasible situations 
advocate exploring heuristic search strategies greedy restarts best search annealing 
results suggest choice search schedule affect model quality 
experiments em em yield significantly better models em suggests em parameter search run iterations gain substantial computational efficiency running em convergence iteration 
predictive log scores learned mixture models shown table 
table shows learned schedules equally superior 
apparently structurally complex models gold standard amount data allows parameter estimates approximately capture conditional independences reflected learned structure 
reasons may preferred 
little data parameter estimations complex mbn models accurate models 
second learned model applications may prefer simple mbn model computational efficiency due fewer parameters ability exploit identified independence constraints 
relationships variables may easily understood verified 
discussed score guide structure search heuristic ap search schedule score mean sigmas 
em sigma em sigma em sigma em full sigma em diagonal sigma table log predictive score mean sigma standard deviation train test splits best model experimental condition 
structural search step cs score cheeseman stutz score structural change search em schedule 
proximation true marginal likelihood 
investigate quality approximation evaluate cheeseman stutz score approximation marginal likelihood intermediate models visited structure search 
heuristic score cheeseman stutz scores consistently increase structure search progresses 
shows cheeseman stutz score structural change search em schedule train test splits data 
splits curves appear monotonically increasing single small dip training sets 
careful inspection traces train test splits shows cheeseman stutz score decreases transitions total transitions respectively suggesting heuristic score 
results real data experimental evaluation real data chose vehicle data set machine learning repository 
data consists continuous dimensional features extracted silhouettes types vehicles double decker bus bus opel saab diagonal full em esm em esm van cheeseman stutz score training data function number mixture components 
cases saab cases opel manta cases van cases 
original data set classification 
data evaluate density estimators 
cheeseman stutz score function number components shown train test splits curves similar splits 
predictive log scores test sets shown table 
cheeseman stutz score significantly better density estimators 
holdout score suggests trend results significant 
related mixtures bayesian networks generalize known statistical models 
mixture component mbn model bayesian network may contain hidden variables 
researchers considered models discrete variables see 
models hidden variable called latent factor models 
linear factor analysis special case variables continuous related linear regressions hidden variables marginally independent observed variables conditionally independent hidden variables 
interesting class component models probabilistic principle component analysis tipping bishop 
models factor analytic models noise terms observed variables equal 
search schedule score mean sigmas bus opel saab em sigma sigma sigma em sigma sigma sigma em sigma sigma sigma em full sigma sigma sigma em diagonal sigma sigma sigma table log predictive score mean sigma standard deviation train test splits best model determined cheeseman stutz score experimental condition 
van experiments included table due fact experiments learn model test cases log density precision computer making comparisons non informative 
generalized include equality constraints 
see thiesson examples adding constraints bayesian networks 
variety mixture models special cases mixtures bayesian networks 
simple example naive bayes model clustering cheeseman stutz 
model viewed bayesian network edges connect single hidden variable observed variables mixture bayesian networks component contains arcs 
cheeseman stutz consider naive bayes model combination discrete continuous observed variables 
mixture models focussed case observed variables continuous 
classes considered previously mixtures factor analytic models hinton dayan revow mixtures probabilistic principle component analytic models tipping bishop 
interesting class mixture models considered obtained enforcing equality constraints mixture components 
banfield raftery developed class models extended celeux covariance matrix component reparameterized volume shape orientation 
terms mixtures bayesian networks authors consider equality constraints volume shape orientation size magnitude mixture weight various combinations mixtures complete models empty models empty models equal variances 
possible extend class models developed banfield raftery 
particular size volume multivariate gaussian distribution may varied independently network structure 
contrast structure bayesian network constrains shape orientation multivariate gaussian distributions 
considering component network structures equal equality constraints various combinations size volume structure shape orientation components capture entire banfield raftery hierarchy 
component network structures need restricted complete empty models mbn hierarchy extends original hierarchy 
consider related learning methods 
meila jordan consider problem learning mixtures bayesian networks component spanning tree 
extends chow liu greedy method finding optimal spanning tree iterative method finding mixtures spanning trees 
meila jordan interleave search parameters model structure 
similar approach treat expected data real data determine best spanning tree mixture component 
rely likelihood penalize complexity model 
consequently limited class mixtures models dimension 
friedman focuses case learning single bayesian network discrete variables incomplete data possibly including hidden variables consider mixtures bayesian networks 
similar approach friedman interleaves search parameters model structure treats expected data real 
approach calculates expected sufficient statistics new model current model 
model considered method requires runs inference case missing values compute required expected sufficient statistics 
contrast approach need perform inference case missing values compute expected complete model sufficient statistics 
statistics computed method efficiently compute score arbitrary structures 
unobserved discrete variables large percentage cases large number missing discrete variables computation expected complete model sufficient statistics intractable 
method described uses expected complete model sufficient statistics evaluate method intractable cases 
method described friedman cases tractably compute score models variables missing values densely connected 
approach making calculation expected complete model sufficient statistics tractable approximate statistics 
method doing calculate probabilities top map configurations missing variables case treat configurations configurations consistent case 
method renormalize probabilities configurations fill complete model sufficient statistics case 
approximate data set obtained summing results cases 
alternatively include configurations joint probability lies threshold 
methods adapted scoring method friedman tractable 
banfield raftery banfield raftery 

model gaussian non gaussian clustering 
biometrics 
bernardo smith bernardo smith 

bayesian theory 
john wiley sons new york 
celeux celeux 

gaussian clustering models 
pattern recognition 
cheeseman stutz cheeseman stutz 

bayesian classification autoclass theory results 
fayyad shapiro smyth uthurusamy editors advances knowledge discovery data mining pages 
aaai press menlo park ca 
chickering chickering 

learning bayesian networks np complete 
fisher lenz editors learning data pages 
springer verlag 
chow liu chow liu 

approximating discrete probability distributions dependence trees 
ieee transactions information theory 


latent class models 
handbook statistical modeling social behavioral sciences pages 
plenum press new york 
cooper herskovits cooper herskovits 

bayesian method induction probabilistic networks data 
machine learning 
dempster dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society 
geiger heckerman geiger heckerman 

bayesian networks similarity networks bayesian multinets 
artificial intelligence 


choice model fit data exponential family 
annals statistics 
heckerman heckerman 

tutorial learning bayesian networks 
technical report msr tr microsoft research redmond wa 
revised november 
heckerman geiger heckerman geiger 

likelihoods priors bayesian networks 
technical report msr tr microsoft research redmond wa 
heckerman heckerman geiger chickering 

learning bayesian networks combination knowledge statistical data 
machine learning 
hinton hinton dayan revow 

modeling manifolds images handwritten digits 
ieee transactions neural networks 
kass wasserman kass wasserman 

bayesian test nested hypotheses relationship schwarz criterion 
journal american statistical association 
lauritzen lauritzen 

propagation probabilities means variances mixed graphical association models 
journal american statistical association 
raftery raftery 

bayesian model selection social research 
marsden editor sociological methodology 
cambridge ma 
schwarz schwarz 

estimating dimension model 
annals statistics 
shachter shachter 

gaussian influence diagrams 
management science 


vehicle recognition rule methods 
technical report turing institute 
spiegelhalter spiegelhalter dawid lauritzen cowell 

bayesian analysis expert systems 
statistical science 
thiesson thiesson 

score information recursive exponential models incomplete data 
proceedings thirteenth conference uncertainty artificial intelligence providence ri 
morgan kaufmann 
tipping bishop tipping bishop 

mixtures probabilistic principle component analysers 
technical report ncrg neural computing research group 
