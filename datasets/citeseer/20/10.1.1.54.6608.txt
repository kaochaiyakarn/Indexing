neural network approach topic spotting erik wiener jan pedersen andreas weigend xerox palo alto research center dept computer science cb university colorado boulder boulder wiener cs colorado edu xerox palo alto research center coyote hill rd palo alto ca pedersen parc xerox com institute cognitive science dept computer science campus box university colorado boulder boulder andreas cs colorado edu www cs colorado edu andreas home html presents application nonlinear neural networks topic spotting 
neural networks allow model higherorder interaction document terms simultaneously predict multiple topics shared hidden features 
context model compare approaches dimensionality reduction representation term selection latent semantic indexing lsi 
different methods proposed improving lsi representations topic spotting task 
find term selection modified lsi representations lead similar topic spotting performance performance equal better published results corpus 
topic spotting problem identifying set predefined topics natural language document 
formally set topics document task output topic probability topic 
probabilities subsequently higher level analysis decision making simplest case thresholding yield binary presence decisions 
topic spotting may applied automatically assign subject codes newswire stories filter electronic mail line news documents information retrieval data extraction systems 
successful text categorization systems take expert systems approach manually constructing system inference rules top large body linguistic domain knowledge data driven approaches induce set rules corpus labeled training documents 
practical standpoint datadriven systems put place relatively quickly weeks months able adapt changes data environment hand crafted systems extremely accurate intended task may take person years develop ultimately brittle changes data environment 
data driven approach topic spotting applies nonlinear neural networks estimate topic probabilities 
inductive categorization approaches concerned constructing human readable rules render operation system intuitively understandable heuristically adjustable 
contrast neural network framework employ essentially nonlinear regression model fitting interactions feature space binary topic assignments 
models necessarily interpretable hopefully high performance 
examine degree high order interactions important models 
neural networks statistical models operate tens thousands input variables corresponding individual terms dimensionality reduction prime importance 
compare approaches term selection picks subset original terms features latent semantic indexing lsi constructs new features combinations large number original terms 
organized follows 
section describes related section describes corpus section discusses representational approach section discusses neural network models section presents experimental results section concluding discussion 
related classification methods text categorization bayesian belief networks decision trees nearest neighbor algorithms bayesian classifiers boolean decision rules :10.1.1.39.6139
systems words word couples features lewis tzeras hartmann example incorporated phrasal structure document representations 
describe detail text categorization studies reuters corpus subject experiments 
lewis ringuette compared categorization techniques bayesian classifier fairly simple conditional probability model decision trees 
cases separate classifier constructed topic 
represented documents binary word vectors important drastically reduce number words representation successful classification 
information gain measure pick words high individual predictive power different set words selected predicting topic 
results showed decision tree method give slightly higher performance bayesian classifier authors noted decision tree method resulted classifier rules easier interpret 
apte damerau weiss rule induction technique called swap produce disjunctive binary decision rules relating presence certain combinations terms presence topics :10.1.1.39.6139
previous approach separate classifiers trained topic local feature set 
authors classification performance improved word frequency values binary presence absence features performance improved selecting local representation frequent words topic informative words 
information gain measure system mutual information presence topic presence word documents corpus 
neural network approach topic spotting corpus experiments reuters corpus reuters newswire stories standard testbed text categorization research 
stories full collection stories topic assigned left training testing 
better full collection including stories deemed human predefined topics believe partitioning supported meaningful experiments group :10.1.1.39.6139
stories mean length words standard deviation 
topics appear training set cover areas commodities interest rates foreign exchange 
documents fourteen topics assigned mean topics document 
frequency occurrence varies greatly topic topic earnings example appears roughly documents platinum topic training documents 
unique terms collection performing inflectional stemming word removal conversion lower case elimination words appeared fewer documents 
representation starting point representations term document matrix containing word frequency information 
entries document vector called document reuters collection available anonymous ftp ftp cs umass edu pub doc reuters reuters carnegie group david lewis 
profile computed follows dk dk vector di word frequency 
square root effect high counts normalization removes effect variations document length 
term selection lsi algorithms applied document profile vectors 
term selection term selection aims find subset original terms useful classification task 
difficult impossible select set terms adequately discriminate classes documents time small serve feature set neural network 
divide problem independent classification tasks search set terms topic best discriminate documents topic 
select set terms topic score terms serve individual predictors topic pick top scoring terms 
score call relevancy score relation salton buckley call relevancy weight measures unbalanced term documents topic log tk tk tk number documents topic contain term total number documents topic highly positive highly negative considered measures information gain chi squared test produced similar term rankings relevancy score major difference set selected features 
erik wiener jan pedersen andreas weigend number terms networks selected term representation performance peaks terms 
performance averaged networks predicting topics varying frequency 
scores indicate useful terms discrimination 
terms yielded average best classification performance measured independent test set see 
number falls range feature set sizes little smaller number :10.1.1.39.6139
note decrease error training set including terms classification performance sample data quickly falls terms due overfitting 
overfitting occurs network starts memorize training patterns starts fitting peculiarities training data decreasing performance sample data 
overfitting typically problem free parameters weights model compared training samples 
see example 
sanity check computed topics predicted single key term 
top scoring term topic topic sole predictor topics predicted average precision 
quite surprising demonstrates topics reuters corpus simply hard predict 
results represent trivial baseline experiments corpus 
term selection advantage relatively little computation required resulting features direct interpretability drawbacks worthwhile exploring techniques 
example best individual predictors contain redundant information 
term may appear poor predictor may turn great discriminative power combination terms 
possible find set terms predicting individual topics term selection scales models predict large number topics single representation 
address problems applied latent semantic indexing reduce dimensionality feature set 
latent semantic indexing lsi originally developed address problems created synonymy polysemy standard vector space model information retrieval seeks transform original document vectors meaningful lower dimensional space analyzing correlational structure terms document collection :10.1.1.108.8490
transformation computed applying singular value decomposition svd original term document matrix 
dimensions new space direct interpretability rotations original space 
orthogonal 
terms related lsi extent travel corpus 
neural network approach topic spotting uments different terminology talk concept positioned near new space 
see details lsi technique :10.1.1.108.8490
property lsi vectors higher dimensions capture increasingly variance original data dropped minimal loss 
original experiments lsi ir tasks retaining dimensions gave optimal performance results suggest dimensions better tasks :10.1.1.108.8490
topic spotting experiments performance continues improve dimensions improvement rapidly slows dimensions 
process constructing lsi representations term document matrix straightforward 
svd computed training set new document vectors test set transformed simply projecting left singular matrix produced original decomposition 
cost computing svd incurred corpus 
note drawback lsi nonlinear classifier hope nonlinear manifold original data preserved linear subspace lsi representation 
improving lsi topic spotting svd optimal data reduction algorithm criterion squared error reconstructed vectors originals 
says little utility classification tasks 
find models basic global lsi representation perform increasingly worse topic frequency decreases 
due fact infrequent topics usually indicated infrequent terms infrequent terms may projected lsi representations utilize fraction original number dimensions 
particular low frequency term may important discriminating classes documents may appear svd mere noise compared rest data 
propose task directed methods addressing problem prior knowledge classification task improve representations 
local lsi method biases lsi representation particular topic set topics modeling local portion corpus related topics 
local portion includes documents close topics terminology related topics don necessarily topics assigned 
performing svd local set documents expect representation sensitive small localized effects correlated infrequent terms global svd 
consequently expect representation effective classification topics related local structure 
method entails global lsi representation replaced specialized lsi representations prediction particular topics benefit having single global representation sacrificed 
experiment approaches constructing local lsi representations embodying methods defining local structure 
approach define broad meta topics agriculture method similar spirit method proposed hull improving classification performance routing problem 
erik wiener jan pedersen andreas weigend energy foreign exchange government metals break corpus clusters containing documents particular meta topic 
clusters form local structures lsi representations call cluster directed representations 
predict topics particular meta topic cluster directed representation meta topic 
fine grained approach local lsi separate representation constructed classifying topic 
topic directed approach local region corpus relevant particular topic predictive terms topic query picking similar documents 
pick times number documents containing topic 
add randomly selected documents local set computing svd 
note trade defining regions local structure 
narrower regions flexible representations modeling classification multiple topics 
expect topic directed lsi approach practice high computational overhead svd topic dimensional representations document experiment understand performance local lsi representations limit fine grained structure 
relevancy weighting method creating task directed lsi representations uses term weights emphasize importance particular terms applying svd 
dumais showed inverse document frequency idf weighting term document matrix applying svd led average improvement set standard ir test sets 
idf weighting lead large improvements topic spotting performance experiments reported 
idf weighting inflates importance low frequency terms diminishes importance high frequency terms svd forced distribute resources evenly terms 
assumption general low frequency terms better discriminators high frequency terms 
topic spotting tune general assumption 
relevancy weighting emphasize terms proportion estimated topic discrimination power suggestion 
global relevancy weight term computed summing absolute value relevancy score topic cutting total value certain threshold 
admittedly crude way ensuring term predictor topics globally weighted times heavily term predicts single topic 
relevancy weight multiplied idf weight term get final weighting 
effect low frequency terms pulled idf estimated poor predictors pushed leaving relevant low frequency terms high weights 
svd concentrate modeling terms important classification task 
note task directed approach retains single global lsi representation prediction topics 
square idf weight multiplying relevancy weight 
regular idf high frequency terms predictors received unnecessary boost low frequency terms quite able compensate 
neural network approach topic spotting modeling section describe approach modeling topic assignment representations discussed 
basic framework regression model relating input variables features output variables binary topic assignments fit training data 
linear analysis logistic regression appropriate modeling binary output variables 
case functional form gammaj fi linear combination input features 
logistic function guarantees 
logistic regression converted binary classification method thresholding output probabilities 
technique quite successfully variety classification problems interested comparing nonlinear regression method models interactions features 
nonlinear neural network classifiers extend logistic regression modeling higher order term interaction non linear decision boundaries input space 
logistic regression baseline network performance 
briefly describe nature neural network classifiers discuss apply topic spotting task 
neural network classifiers essentially major components neural network model architecture cost function search algorithm 
architecture defines functional form relating inputs outputs terms network topology unit connectivity activation functions 
search weight space set weights minimizes cost function training process 
case binary targets cross entropy theoretically appropriate cost function presumes binomial error model outputs 
standard backpropagation method gradient descent search technique 
simplest linear classifier network called logistic network output unit logistic activation hidden layer resulting functional form equivalent logistic regression model 
useful separate model expressed network search algorithm fit model training data 
example learn weights logistic network gradient descent weight space iterated reweighted squares algorithm traditionally logistic regression 
applying logistic regression experiments typically logistic network gives greater control fitting process allowing add priors cost function early stopping training minimize overfitting 
hidden layers nonlinear activation functions allows network model nonlinear relationships input output variables 
hidden layer learns re represent input data discovering higher level features formed combinations features previous level 
logistic networks nonlinear networks logistic activations outputs estimate bayesian posteriori probability output input features assuming certain conditions met training 

neural networks topic spotting context topic spotting nonlinear neural networks extract hidden features nonlinear combinations terms 
erik wiener jan pedersen andreas weigend epoch training error validation error learning curve lsi network predicting topic gnp gross national product shows typical shape validation error reaches minimum begins increase long network converges 
network outputs estimates probability topic presence feature vector document 
interesting advantage nonlinear neural networks techniques multiple topics predicted simultaneously single model 
ths case hidden units form shared representation predicting topics 
believe potential benefits explore multiple simultaneous prediction 
experiment different network architectures flat modular 
flat architecture case entire training set train separate network topic 
global lsi representations network uses representation selected word representations local lsi different representations network 
simple networks single hidden layer logistic sigmoid units 
overfitting training data major problem topics worse low frequency topics 
training set learned extremely low error error separate validation set typically reaches minimum begins rising epochs see 
help alleviate problem employ simple regularization scheme weight elimination add term penalizing network complexity cross entropy cost function 
technique early stopping cross validation 
modular architecture second approach modular architecture decompose learning problem set smaller problems 
architecture shown 
component network trained full training set estimate probability meta topics agriculture energy foreign exchange government metals document 
meta topic said subtopics 
second component set network groups corresponding meta topics 
group consists separate network topic group trained region corpus corresponding meta topic group 
example wheat network trained documents contain topics agriculture meta topic 
focus finer distinctions example wheat grain wasting efforts easier distinctions wheat gold 
meta topic network uses fifteen hidden units penalty term encourages elimination small magnitude weights ij ij neural network approach topic spotting agriculture metal barley wool aluminum zinc meta topic network agriculture group metal group modular architecture consists meta topic network groups topic networks 
outputs meta topic network multiplied outputs individual topic networks get final topic predictions 
local topic networks hidden units 
compute topic predictions document modular network document meta topic network topic networks different representation network 
outputs network multiplied estimates topic networks produce final topic estimates 
example output network agriculture group multiplied agriculture output meta topic network network 
rationale architecture wish set networks experts local regions corpus need documents know weight opinion networks 
example get document cotton prices give weight estimates networks metal group haven trained give judgements agricultural topics 
meta topic network performs job determining high level incoming documents local networks deployed appropriately 
experimental results section describe evaluation measures results combinations representational modeling techniques discussed 
evaluating performance typical evaluation measure neural network literature mean squared error actual predicted values tell little network performed task separating positive negative examples 
relatively high error example arise examples perfectly separated probability estimates mid range near 
force binary decisions thresholding probabilities compute precision recall figures contingency tables constructed range decision thresholds 
recall percentage documents topic predicted having erik wiener jan pedersen andreas weigend topic precision percentage documents predicted having topic topic 
pick decision thresholds ways 
proportional assignment pick different set thresholds topic varying integer parameter set values level setting decision threshold just probability value kp th highest ranked document fraction positive examples topic training set 
fixed recall level approach set recall levels want compute precision analyze ranked documents test set determine decision thresholds topic lead desired set recall levels 
satisfying method picking operation threshold suffice characterizing effectiveness classifier full range potential thresholds 
set contingency tables computed range decision thresholds topic summarize performance microaveraging means adding contingency tables topics certain threshold computing precision recall meaning compute precision recall individually topic take average topics 
microaveraging proportional assignment picking decision thresholds macroaveraging fixed set recall levels 
microaveraging weight topics evenly prefer comparisons previously reported results case breakeven point summary value 
cases summary values obtained particular topics averaging precision evenly spaced recall levels 
results experiments reported features lsi representations generic task directed features selected term representations 
shows microaveraged precision recall curves networks generic lsi representation topic directed lsi representation td lsi relevancy weighted lsi representation rel lsi selected term representation 
breakeven points respectively 
corpus roughly compare results best algorithm didn give special weight words headlines stories reported breakeven point :10.1.1.39.6139
results quite microaveraged performance somewhat misleading task frequent topics weighted heavier average 
classifiers generally perform best high frequency topics case optimistic measure 
shows macroaveraged performance classifiers 
selected term network appears closer performance 
interesting note relative effectiveness representations low recall levels reversed high recall levels 
detailed plot showing average precision techniques frequent topics skipping topics occurred infrequently test set give reliable results 
topics listed order frequency precision averaged recall levels 
detailed view give feel kind variation hidden averages 
note considerable variation performance topics relative ups downs mira neural network approach topic spotting recall microaveraged 
td lsi lsi rel lsi terms recall precision macroaveraged 
td lsi lsi rel lsi terms plots show microaveraged macroaveraged curves networks topic directed lsi representation relevancy weighted lsi representation generic lsi representation selected term representation 
large degree plots 
clear lsi performance degrades quite markedly compared selected term performance topic frequency decreases task directed representations suffer effect 
note topics regular lsi performs better relevancy weighted lsi cases topics regular lsi better term selection 
suggests relevancy weighting hinders lsi representation cases large number terms important classification 
see nonlinear term network offers sight improvement linear model 
tables concise summary performance combinations techniques relative improvement relevant baseline techniques 
set frequent topics split thirds highest frequency topics group performance summarized group 
table displays macroaveraged precision networks term selection lsi variations 
experimented modular architecture representations cluster directed lsi cd lsi representation selected term representation hybrid representation document represented lsi terms plus selected terms 
meta topic network component modular architecture trained cases generic lsi representation 
agriculture energy foreign exchange metal clusters experiments frequent topics clusters ignored 
order able compare performance modular networks flat models recompute average precision flat networks topics predicted modular network 
table displays modular network performance flat networks comparison 
table displays models relative improvement relevant baseline 
erik wiener jan pedersen andreas weigend earn acq money fx grain loan crude trade interest wheat ship corn money dlr sugar coffee gnp gold oil soybean nat gas bop cpi cocoa reserves carcass copper jobs yen ipi iron steel cotton rubber gas barley rice palm oil meal feed zinc silver pet chem wpi tin strategic metal orange terms nonlin lin best term avg precision lsi reg rel topic avg precision shows average precision frequent topics selected term models nonlinear linear linear single best term lsi models regular lsi nonlinear topic directed lsi linear relevancy weighted lsi linear 
topics frequencies training set listed names 
neural network approach topic spotting model high medium low terms nonlinear lsi nonlinear rel lsi linear td lsi linear table table displays precision flat networks macroaveraged topic frequency ranges topics topics high frequency topics medium frequency topics low frequency 
model high medium low modular terms nonlinear modular lsi nonlinear modular hybrid nonlinear terms nonlinear lsi nonlinear rel lsi linear td lsi linear cd lsi linear table table displays average precision modular networks flat networks comparison 
precision macroaveraged frequency ranges table 
erik wiener jan pedersen andreas weigend model vs baseline high medium low lsi nonlinear vs linear cd lsi nonlinear vs linear modular cd lsi nonlinear vs linear terms nonlinear vs linear hybrid nonlinear vs linear rel lsi linear vs lsi linear td lsi linear vs lsi linear cd lsi nonlinear vs lsi nonlinear modular cd lsi nonlin vs flat cd lsi nonlin modular cd lsi nonlin vs flat lsi nonlin lsi nonlin vs terms nonlin td lsi linear vs terms linear modular terms nonlin vs flat terms nonlin table table displays relative improvement networks relevant baseline models 
improvement average precision reported tables frequency ranges 
statistically significant differences paired test level shown bold face 
neural network approach topic spotting discussion nonlinear networks perform consistently better linear models difference slight 
possibility positive examples support nonlinear fits generalize 
nonlinear networks apparently unable extract features generalize sample data training halted early stopping 
cases performance slightly improved hidden units improvement linear models substantial 
experiments show lsi representation able equal exceed performance selected term representations high frequency topics performs relatively poorly low frequency topics 
task directed lsi representations improve performance low frequency domain case relevancy weighting case local lsi 
modular approach conjunction local lsi improves performance low frequency topics 
reason huge gain cluster directed modular network individual networks trained domain local lsi performed 
achieving improved performance low frequency topics task directed representations trade 
case topic directed representations need construct separate representation topic costly 
case relevancy weighting trade slightly lower performance medium high frequency topics 
representation combined modular network appears strike balance handful separate representations required performance topics relatively high 
minimize cost worth exploring variations particular relevancy weighting approach combinations relevancy weighted local lsi 
interesting note selected term representations prove quite competitive sophisticated lsi technique 
indicate reuters corpus topics predictable small set terms single term 
encouraging simple representation sheer performance measure representation utility 
circumstances benefits having single global representation may outweigh costs slightly decreased performance certain tasks 
prime example circumstance front modular network draw large variety information order determine high level class document 
major challenge finding ways global representations lsi biased particular tasks losing flexibility global representations 
believe task directed approaches represent meaningful step direction 
acknowledgment wray buntine david hull tom landauer eric saund hinrich schutze john tukey helpful discussions 
apte damerau weiss :10.1.1.39.6139
language independent automated learning text categorization models 
proceedings th annual acm sigir conference 
erik wiener jan pedersen andreas weigend deerwester dumais furnas landauer harshman :10.1.1.108.8490
indexing latent semantic analysis 
journal american society information science 
dumais 
improving retrieval information external sources 
behavior research methods instruments computers 
hayes weinstein 
construe tis system contentbased indexing database news stories 
second annual conference innovative applications artificial intelligence 
hull 
improving text retrieval routing problem latent semantic indexing 
proceedings th annual acm sigir conference pages 
jacobs rau 
scisor extracting information line news 
communications acm 
landauer 
personal communication 

lewis 
evaluating text categorization 
proceedings speech natural language workshop pages 
lewis 
representation learning information retrieval 
phd thesis computer science dept univ massachussetts amherst february 
technical report 
lewis ringuette 
comparison learning algorithms text categorization 
symposium document analysis information retrieval 
masand waltz 
classifying news stories memory reasoning 
proceedings th annual acm sigir conference pages 
mccullagh nelder 
generalized linear models 
chapman hall london nd edition 
richard lippmann 
neural network classifiers estimate bayesian posteriori probabilities 
neural computation 
rumelhart durbin golden chauvin 
backpropagation basic theory 
smolensky mozer rumelhart editors mathematical perspectives neural networks pages 
erlbaum associates hillsdale nj 
salton buckley 
term weighting approaches automatic text retrieval 
information processing management 
tzeras hartmann 
automatic indexing bayesian inference networks 
proceedings th annual acm sigir conference pages 
weigend huberman rumelhart 
predicting connectionist approach 
international journal neural systems 
neural network approach topic spotting 
