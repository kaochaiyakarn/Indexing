advances hierarchical mixture experts signal classification viswanath joydeep ghosh department electrical computer engineering university texas austin austin texas hierarchical mixture experts hme architecture powerful tree structured architecture supervised learning 
efficient pass algorithm solve step em iterations training hme network perform classification tasks described 
substantially reduces training time compared irls method solve step 
pre processing stage proposed consisting radial basis function kernels aimed reducing tree height hme network 
alternatively employment localized form gating network suggested reduce tree height 
shorter hme trees fewer network parameters significantly faster train 
simulation results real life data set 

hierarchical mixture experts hme jordan jacobs tree structured architecture works principle divide conquer 
model employs probabilistic methods divide input space overlapping regions experts act 
expert networks occupy leaves tree set gating networks occupy non terminals tree 
gating network node tree weights outputs subtrees coming form combined output 
final network output obtained root tree 
level hme shown 
original mixture experts framework parameters network determined gradient descent naturally slow 
expectation maximization em algorithm obtain network parameters resulting substantially reduced training times 
em formulation step solved employing iteratively re weighted squares irls technique 
irls multi pass algorithm 
efficient pass algorithm solve step em algorithm regression problems 
waterhouse robinson extended hme model solve classification problems 
employ irls method solve step 
efficient pass algorithm described solve step classification problems 
gating network mixture experts model single layer feedforward network softmax output activation function 
softmax function ensures gating network outputs non negative sum 
gating network form input space divided different overlapping regions soft hyperplanes region assigned expert 
expert networks single layer networks 
limited structure hme network finds difficult perform job classification tree structure fairly deep 
unfortunately tree depth increases computational cost increases gating expert networks need trained 
pre processing stage consisting radial basis kernels proposed map inputs higher dimensional space making hme network perform classification tasks significantly reduced tree height 
xu jordan hinton come alternative form gating network 
new gating network divides input space soft hyper ellipsoids opposed soft hyperplane divisions created original gating network 
localized form gating network able come flexible input partitions performs classification tasks reduced hme tree height 
outline hme network trained perform classification efficient pass solution step 
followed description pre processing stage rbf kernels reduce tree height network 
brief description hme localized form gating network 

classification hierarchical mixture experts level hme network shown 
description level hme model 
straight forward extend model deeper trees 
expert network produces output ij corresponding input vector ij theta ij theta ij weight matrix fixed continuous non linearity 
component vectors theta ij matrix denoted 
classification problems assume softmax activation function exp exp appear proc 
icassp may atlanta ga fl ieee gating networks softmax output activation function 
top level gating network outputs denoted lower level gating network outputs denoted corresponding parameter vectors ij respectively 
output hme weighted sum expert outputs ij target output assumed generated probability density mean ij giving rise probability model ij ij superscript refers true value parameters 
denoting ij ij posterior probabilities defined ij ij ij ij joint posterior probability ij product em algorithm derived obtain network parameters 
step algorithm forward pass network obtain posterior probabilities step em algorithm reduces separate maximization problems 
ijm arg max ijm ij ln ij arg max ln ij arg max ij ln superscript running variable corresponding individual input patterns 
algorithm iteratively re weighted squares method irls described obtain parameters equations 
unfortunately irls iterative nature computationally inefficient 
jordan jacobs specific case regression problems efficient pass algorithm solve step assuming gaussian probability model ij term equation assumes form gaussian density function 
assumption irls method determine expert network parameters reduces directly standard pass weighted squares problem 
obtain approximate weighted squares formulation determine gating network parameters 
multinomial logit probability model employed classification problems 
irls technique solve step 
pass algorithm solve step classification problems 
logit probability model assumed classification problems 
multinomial probability density delta delta delta yn delta delta delta yn multinomial probabilities associated different classes 
density equivalently written form delta delta delta yn exp ln expert networks classification hme softmax activation function outputs expert networks try model probabilities associated different classes 
outputs expert network labeled delta delta delta classification problem substituting probability model step equation expert network ijm arg max ijm ij ym ln ijm differentiate equation directly respect ij set equal zero 
mean ijm ij ln ijm ij ijm ln ijm ij ijm ijm ijm shown ijm ijm ijm ffi gamma ijm mean ij ijm ijm ijm ij ffi gamma ijm ij gamma ijm gamma ijm ij gamma gamma ijm gamma ijm ij gamma ijm equation think ym targets ijm softmax function targets inverted approximate weighted squares problem expert networks 
inverting softmax function targets yields virtual targets gamma respectively 
gamma valid target approximated mean true target gamma approximation works classification task 
efficient pass algorithm described obtain gating network parameters 
entire step solved pass classification task 
method tested glass data set proben benchmark suite 
glass data set chosen known difficult classification task 
glass data set consists inputs outputs 
inputs results chemical analysis glass yields percentage content different elements glass refractive index glass 
outputs correspond different types glass 
glass data set consists training patterns test patterns 
table shows average performance standard deviation glass data set runs 
seen performance layer binary tree hme close best performing mlp set 
noted test set small single mis classification leads reduction classification rate 
seen standard deviation high hme network due limited number samples training test data sets 
table shows average number epochs needed reach various test set classification rates training networks 
seen terms number epochs needed train networks hme order magnitude faster 
mentioned training time epoch hme network 

radial basis kernels pre processors hme network section mentioned gating network produces soft hyper plane partitions input space 
hme tree short possible provides limited number ways input space partitioned get solution 
em algorithm hill climbing procedure small chance algorithm converge solution starts initial set random weights 
tree height increases increase number degrees freedom network performs better job classification 
seen previous section layer hme network solve glass data set classification problem 
layer binary tree consists expert networks gating networks 
total number parameters network works 
layer hme waterhouse robinson solve spiral problem consisted parameters 
training large number parameters leads increased computational cost epoch 
known radial basis function network literature classification problem cast lower dimensional space higher dimensional greater chance classes linearly separable 
lead try classification problems sufficient rbf kernels pre processors outputs fed hme network reduced tree height 
glass data set inputs classes rbf centers selected training samples corresponding different classes centers corresponding samples classes respectively 
centers selected performing means clustering training samples belonging different classes 
outputs rbf kernels fed hme network 
table 
shows results rbf pre processed data layer hme 
seen table single layer hme able perform classification task 
table 
shows average number epochs needed reach different classification rates 
compared table seen convergence faster 
tree height smaller training time epoch shorter 

localized gating network localized form gating network proposed 
gating network case divides input space soft hyper ellipsoids 
gating network output corresponding expert ff ff gamman sigma gamma expf gamma gamma sigma gamma gamma fff sigma constitute gating network parameters expert jth expert influence localized region gating network flexible network able solve classification problems single layer hme architecture 
em algorithm single layer mixture experts network naturally extended hierarchy estimating parameters localized gating network step expf gamma gamma gamma expf gamma gamma gamma step gating network ff jx sigma jx gammam gammam jx step em algorithm exactly solved pass 
expert network approximate algorithm derived section ii 
model tested glass data set 
hme network experts performs classifica tion task shown table 
gating network initialized means algorithm 
diagonal elements fixed lower thresholds considered sigma avoid singular matrices 
table 
gives average number epochs needed reach different classification rates 
number network parameters case significantly lesser compared deeper hme tree training time epoch shorter 

supported part aro contracts daah 
viswanath recipient motorola corporate software center university texas austin research 
authors richard palmer initial version hme code 
jordan jacobs 
hierarchical mixture experts em algorithm 
neural computation 
jordan jacobs 
hierarchies adaptive experts 
moody hanson lippmann editors advances neural information processing systems 
morgan kaufmann 
waterhouse robinson 
classification hierarchical mixture experts 
neural networks signal processing iv proceedings ieee workshop pages 
ieee press new york 
xu jordan hinton 
alternative model mixture experts 
gerald tesauro david touretzky todd leen editors advances neural information processing systems 
mit press 
lutz prechelt 
proben set benchmarks benchmarking rules neural network training algorithms 
technical report fakult ur informatik universit karlsruhe karlsruhe germany september 
anonymous ftp pub papers techreports ps ftp ira uka de 
gn gn 
en en en en en en gn 

level hme network table 
average test set classification rate performance architecture ave classification rate std 
dev hme layer binary architecture best mlp hidden nodes hidden layer table 
average number epochs required reach certain test set classification rate architecture classification rate test set hme layer binary architecture mlp single hidden layer hidden nodes table 
average test set classification rate performance architecture ave classification rate std 
dev hme rbf pre processing single layer experts hme localized gating single layer experts table 
average number epochs required reach certain test set classification rate architecture classification rate test set hme rbf pre processing single layer experts hme localized gating single layer experts 
