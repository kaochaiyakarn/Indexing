gaussian prior smoothing maximum entropy models stanley chen ronald rosenfeld february cmu cs school computer science carnegie mellon university pittsburgh pa certain contexts maximum entropy modeling viewed maximum likelihood training exponential models maximum likelihood methods prone overfitting training data 
smoothing methods maximum entropy models proposed address problem previous results clear smoothing methods compare smoothing methods types related models 
survey previous maximum entropy smoothing compare performance algorithms conventional techniques smoothing gram language models 
mature body research gram model smoothing close connection maximum entropy conventional gram models domain suited gauge performance maximum entropy smoothing methods 
large number data sets find smoothing method proposed lafferty performs better algorithms consideration 
general efficient method involves gaussian prior parameters model selecting maximum posteriori maximum likelihood parameter values 
contrast method previous gram smoothing methods explain superior performance 
keywords language models maximum entropy smoothing maximum entropy modeling successfully applied wide range domains including language modeling natural language tasks :10.1.1.103.7637:10.1.1.40.180:10.1.1.43.7345
problems type modeling viewed maximum likelihood ml training exponential models maximum likelihood methods prone overfitting training data 
smoothing methods maximum entropy models proposed address problem previous results clear smoothing methods compare smoothing methods types related models 
great deal research smoothing gram language models shown maximum entropy gram models closely related conventional gram models 
consequently domain suited gauging performance maximum entropy smoothing methods relative smoothing techniques 
survey previous maximum entropy smoothing compare performance algorithms conventional techniques smoothing gram language models 
evaluating perplexity method large number data sets find smoothing method proposed lafferty performs better algorithms consideration 
method gaussian prior model parameters applied maximum posteriori maximum likelihood parameter values selected 
simple efficient method exhibits behaviors observed chen goodman beneficial gram smoothing :10.1.1.131.5458
remainder section maximum entropy modeling discuss smoothing models necessary 
section introduce gram language models summarize previous smoothing models 
list desirable properties smoothing algorithms observed chen goodman 
section introduce maximum entropy gram models discuss relationship conventional gram models 
section survey previous smoothing maximum entropy models section lafferty gaussian prior method 
contrast method smoothing algorithms conventional gram models show satisfies criteria chen goodman 
section results experiments comparing number maximum entropy conventional smoothing techniques gram language modeling 
section discuss 
maximum entropy modeling consider task estimating probability distribution finite set omega training data set fx xn intuitively task find distribution similar empirical distribution training data denotes number times occurs size extreme case take identical typically lead overfitting training data 
better require match properties deem significant reliably estimated training data 
example consider english words training data list consecutive word pairs bigrams occur large corpus english text 
task estimating frequency english bigrams 
consider bigram occur training data say pig dog 
pig dog intuitively want pig dog bigram chance occurring 
example property deem significant want match exactly 
assume observe word occurs frequency training data abundance word presumably accurate estimate frequency reasonable require selected distribution satisfies analogous constraint generally select number nonnegative random variables features ff require expected value feature model equal empirical distribution constraint expressed equation expressed features 
constraints equation generally specify unique model set models maximum entropy principle states select model largest entropy gamma log 
intuitively models high entropy uniform correspond assuming world 
maximum entropy model interpreted model assumes knowledge represented features derived training data 
maximum entropy paradigm elegant properties :10.1.1.103.7637:10.1.1.43.7345
maximum entropy model unique shown exponential model form exp exp normalization factor parameters model 
furthermore maximum entropy model maximum likelihood model class exponential models equation 
log likelihood training properties hold constraining feature expectations equal training set 
constraining expectations alternate values maximum entropy model maximum likelihood model model exist constraints inconsistent 
data concave model parameters relatively easy find unique maximum entropy maximum likelihood model algorithms generalized iterative scaling improved iterative scaling 
models high entropy tend uniform smooth may constrain properties consider significant maximum entropy model overfit training data small numbers constraints 
example consider constraints frequency word mateo bigram san mateo assume word mateo occurs word san training data 
mateo mateo san mateo san mateo san mateo implies mateo san 
intuitively want omega bigrams chance occurring 
zero probabilities lead infinite loss log loss objective functions lead poor performance applications represents language model speech recognition 
desirable smooth maximum entropy models adjust parameter values away maximum likelihood estimates 
smoothing gram language models relatively little smoothing maximum entropy models great deal smoothing gram language models 
language model probability distribution word sequences models sequence occurs sentence 
language models applications including speech recognition machine translation spelling correction 
word sequence delta delta delta express probability pr pr pr theta pr jw theta delta delta delta theta pr jw delta delta delta gamma theta pr delta delta delta pr jw delta delta delta gamma token signals sentence 
widely language models far gram language models 
gram model approximation identity word depends past words identity gamma words giving pr pr jw delta delta delta gamma pr jw gamma gamma gamma notation denotes sequence delta delta delta gamman taken distinguished sentence token 
maximum likelihood estimate ml jw gamma gamma gamma probabilities pr jw gamma gamma gamma training data calculated simply counting token follows history context gamma gamma gamma dividing total number times history occurs ml jw gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma maximum likelihood estimates probabilities typically lead overfitting desirable smoothed estimates values 
example simple smoothing technique linearly interpolate maximum likelihood estimate gram probability ml jw gamma gamma gamma estimate gamma gram probability pr jw gamma gamma gamma int jw gamma gamma gamma ml jw gamma gamma gamma gamma int jw gamma gamma gamma lower order estimate defined analogously recursion unigram uniform distribution 
lower order distributions sparsely estimated training data interpolation generally reduces overfitting 
large number smoothing methods gram models proposed :10.1.1.131.5458
brief overview past gram model smoothing 
basic observation maximum likelihood estimate probability gram occur training data zero low consequently ml probabilities grams nonzero counts generally high 
dichotomy motivates framework expressing smoothing methods express existing smoothing techniques sm jw gamma gamma gamma ff jw gamma gamma gamma gamma gamma fl gamma gamma gamma sm jw gamma gamma gamma gamma gamma gram gamma gamma occurs training data estimate ff jw gamma gamma gamma estimate generally discounted version maximum likelihood estimate 
back scaled version gamma gram distribution sm jw gamma gamma gamma distribution typically defined analogously higher order distribution 
scaling factor fl gamma gamma gamma chosen assure conditional distribution sums 
algorithm described equation placed framework relations ff jw gamma gamma gamma int jw gamma gamma gamma fl gamma gamma gamma gamma sm jw gamma gamma gamma int jw gamma gamma gamma primary distinctions smoothing algorithms algorithm interpolated backed type discounting applied ml estimate calculate ff jw gamma gamma gamma lower order distributions computed 
interpolated models probability estimate ff jw gamma gamma gamma gram gamma gamma nonzero count depends probability assigned corresponding gamma gram gamma gamma equation 
backed models probability estimate gram nonzero count determined ignoring information lower order distributions 
interpolated models include jelinek mercer smoothing witten bell smoothing backed models include katz smoothing absolute discounting kneser ney smoothing 
describe different types discounting write ff jw gamma gamma gamma ff jw gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma fi gamma gamma gamma gamma viewed discount count space ml estimate fi gamma gamma contribution lower order distributions 
value fi gamma gamma zero backed models typically fl gamma gamma gamma sm jw gamma gamma gamma interpolated models 
linear discounting discount gamma gamma taken proportional original count gamma gamma equation discount gamma delta gamma gamma 
absolute discounting gamma gamma taken constant 
turing discounting discount calculated turing estimate theoretically motivated discount shown accurate non sparse data situations 
brief description turing estimate section 
jelinek mercer smoothing witten bell smoothing linear discounting smoothing uses absolute discounting katz smoothing church gale smoothing turing discounting 
final major distinction smoothing algorithms lower order probability estimates calculated 
smoothing methods define lower order model sm jw gamma gamma gamma analogously higher order model sm jw gamma gamma gamma kneser ney smoothing different approach taken 
gamma gram model chosen satisfy certain constraints derived training data gamma gamma gamma gamma gamma sm jw gamma gamma gamma gamma gamma gamma grams gamma gamma constraint rephrased expected number times gamma gamma occurs training data model sm jw gamma gamma gamma history frequencies gamma gamma gamma equal actual number times occurs 
kneser ney smoothing applied recursively lower order distributions case constraints satisfied exactly 
right hand side constraints discounted absolute discounting 
chen goodman provide extensive comparison widely smoothing techniques :10.1.1.131.5458
evaluate algorithm wide range training sets perplexity test data 
perplexity pp model test set reciprocal geometric average probability model assigns word test set 
derivative measure cross entropy log pp interpreted average number bits needed code word test set model chen conducted experiments investigating cross entropy language model related performance speech recognition system 
strong linear correlation metrics comparing models differ smoothing 
terms perplexity chen goodman kneser ney smoothing variations consistently outperform algorithms 
specifically main ffl factor affects performance modified lower order distributions kneser ney smoothing 
primary reason excellent performance algorithm 
original count trigram mw training bigram mw training trigram mw training bigram mw training ideal average discount grams count training data word training set word training set bigram trigram models ffl absolute discounting superior linear discounting 
grams count training data calculate average discount count space gamma gamma ml estimate cause expected number grams test set equal actual number assuming fi gamma gamma 
ideal average discount displayed counts training sets bigram trigram models 
graph see fixed discount works 
turing discounting better absolute discounting predicting average discount way predict correct discounts individual distributions 
ffl interpolated models outperform backed models considering performance just grams low counts training data 
lower order models provide valuable information estimating probabilities grams low counts 
ffl adding free parameters algorithm optimizing parameters held data improve performance algorithm 
observations chen goodman propose algorithm named modified smoothing outperform methods considered 
interpolated variation kneser ney smoothing augmented version absolute discounting 
single discount grams separate discounts grams count counts counts respectively 
motivated observation ideal discount counts counts significantly smaller ideal discount larger counts shown 
maximum entropy gram models construct language models similar conventional gram models maximum entropy framework 
maximum entropy models described section joint models create conditional distributions conventional gram models framework introduced brown 
estimating joint distribution samples estimate conditional distribution yjx samples 
constraints equation constraints form yjx interpreted replacing joint formulation yjx 
assume history frequencies taken training data estimate conditional probabilities 
conditional models share properties joint models including maximum likelihood models computational performance advantages joint models language modeling :10.1.1.40.180
conditional maximum entropy model form yjx exp construct maximum entropy gram model take gamma gamma gamma history word 
gram gamma gamma occurs training data include constraint forces conditional expectation frequency training data 
corresponding features gamma gamma ends substituting features equation simplifying arrive constraints form gamma gamma suffix gamma gamma gamma gamma gamma jw gamma gamma gamma fact solution constraints yjx ml yjx 
consequently maximum entropy model identical maximum likelihood gram model beneficial smooth estimates model parameters remarkably set models equation gram features identical set models described equation express existing smoothing algorithms conventional gram models 
see define set gram models jw gamma gamma gamma equation gram model contains features corresponding word sequences length models share parameter set describe maximum entropy model form terms equation take ff jw gamma gamma gamma jw gamma gamma gamma exp gamma gamma theta gamma gamma gamma gamma gamma gamma theta jw gamma gamma gamma fl jw gamma gamma gamma gamma gamma gamma gamma gamma gamma sm jw gamma gamma gamma jw gamma gamma gamma model form choose equations starting models higher order construct equivalent exponential model 
smoothing estimates maximum entropy gram models smoothing conventional gram models consider class models tasks closely related 
smoothing maximum entropy models section section survey previous maximum entropy model smoothing 
discuss turing discounting fuzzy maximum entropy fat constraints section gaussian prior method proposed lafferty 
describing methods joint maximum entropy formulation simplicity 
techniques apply equally conditional models analogous conditional equations derived replacing yjx 
turing discounting turing discounting proposed lau rosenfeld viewed maximum entropy analog katz smoothing conventional gram models :10.1.1.40.180
observe marginals model yjx constrained exactly empirical distribution yjx target values discounted conventional gram smoothing 
constraints equation propose constraints gamma gamma suffix gamma gamma gamma gamma gamma jw gamma gamma gamma gt gt turing estimate frequency 
turing estimate theoretically motivated method estimating average discount event count training data 
event occurs times samples contrast maximum likelihood estimate turing estimate event true frequency number members population exactly counts 
katz suggests applying estimate joint gram distribution separately 
furthermore low zero large katz proposes method grams large counts discounted discounts low counts adjusted compensate 
lau rosenfeld katz variation turing discounting 
constraining marginals model turing discounted marginals training data constraints may longer consistent maximum entropy model may exist 
example trigram model consider features constrain frequencies grams tic tac toe tac toe assume word tac follows word tic training data 
constraints tic tac tac gt tic tac toe equivalence exact exponential models express probabilities equal zero 
addition equivalence hold unigram model equation assign probability words occurring training data 
generally case existing smoothing algorithms 
gamma gamma tac gamma tac tic tac tac gt tac toe general gt tic tac toe gt tac toe discounts grams different length calculated independently consequently constraints inconsistent 
practice dire consequences having inconsistent constraints 
training algorithms iterative scaling may converge reasonable procedure training performance held set stops improving 
inconsistency symptomatic constraints lead poor parameter estimates 
lau compares performance turing discounting smoothing gram models deleted interpolation variation jelinek mercer smoothing conventional gram models 
word training set wall street journal text deleted interpolation yielded perplexity word test set 
maximum entropy model yielded slightly superior perplexity grams occurred training data excluded model 
results chen goodman strongly indicate smoothing methods conventional gram models modified kneser ney smoothing outperform deleted interpolation larger margin :10.1.1.131.5458
fuzzy maximum entropy fuzzy maximum entropy framework developed della pietra della pietra requiring constraints satisfied exactly penalty associated inexact constraint satisfaction 
finding maximum entropy model equivalent finding model satisfying constraints minimizes kullback leibler distance unif uniform model unif 
fuzzy maximum entropy objective function taken unif delta penalty function minimized constraints satisfied exactly weighting parameter 
della pietra della pietra suggest penalty function form oe gamma penalty function interpreted logarithm gaussian distribution diagonal covariance centered target constraint values 
variance oe associated feature estimated empirical distribution training data 
della pietra della pietra describe variant generalized iterative scaling find optimal model objective function 
interpret algorithm viewpoint maximum posteriori map estimation 
map estimation attempt find model highest posterior probability training data arg max pr qjx arg max pr pr jq arg max log pr log pr jq map objective function terms prior term log pr likelihood term log pr jq 
fuzzy objective function equation analogous map objective function 
term equation encourages model high entropy thought role prior distribution favoring uniform models 
second term encourages satisfy constraints fit training data 
term plays similar role likelihood term map estimation 
terms fuzzy objective function different map counterparts 
lafferty gaussian prior method discussed section traditional map objective function contrast approaches section 
lau constructed fuzzy maximum entropy gram model excluding grams count data sets described section yielding perplexity 
slightly worse perplexities achieved deleted interpolation turing discounted models 
fat constraints methods relaxing constraints include newman khudanpur 
algorithms selecting maximum entropy model models satisfy set constraints exactly require marginals fall range target values 
newman suggests constraint form gamma oe feature weights task estimating power spectra 
khudanpur suggests constraints form ff fi approaches viewed instances fuzzy maximum entropy framework 
smooth function penalty taken zero satisfies relaxed constraints infinite 
types methods applied language modeling 
gaussian prior lafferty proposes applying gaussian prior parameters smooth maximum entropy models 
technique applied previously 
recall maximum entropy model maximum likelihood model set models equation 
performing maximum posteriori maximum likelihood estimation apply gaussian prior centered smooth ml model uniform model hopefully overfitting 
precisely equate finding maximum entropy model finding parameters maximize log likelihood lx training data lx log gaussian prior take diagonal covariance objective function takes form equation lx log oe exp gamma oe lx gamma oe const oe variances gaussian 
contrast objective function objective function equation fuzzy maximum entropy 
functions terms prefers models uniform prefers models similar training data 
lafferty framework term prefers models similar training data log likelihood value analogous term fuzzy sum squared constraint errors 
suitability function ultimately depends application 
language modeling likelihood derivative measure perplexity useful performance metric 
gaussian prior method lafferty framework analogous term fuzzy favors uniform models unif 
function penalizes models large values function penalizes models far uniform 
correct function depend application argue function generally appealing 
example model single nonzero parameter intuitively receive small penalty regardless far uniform model hypothesize gaussian prior method preferable fuzzy maximum entropy 
gaussian prior method adds little computation existing maximum entropy training algorithms 
logarithm gaussian prior concave objective function concave reasonably easy find optimal model 
simple modification improved iterative scaling find map model 
original update algorithm take ffi ffi satisfies equation exp ffi 
gaussian prior equation replaced exp ffi ffi oe right hand side equation strictly monotonic ffi relatively easy find solution search algorithm 
derive modified update rule appendix 
gaussian prior method conventional gram smoothing section listed factors chen goodman significantly affect gram smoothing performance 
informative assess gaussian prior method criteria 
point modified lower order distributions kneser ney smoothing primary reason superiority conventional gram smoothing algorithms 
recall distributions chosen satisfy marginal constraints equation 
set constraints identical corresponding maximum entropy constraints gamma grams equation 
maximum entropy gram models similar modified lower order distributions kneser ney smoothing 
map models gaussian prior longer satisfy constraints exactly 
constraints satisfied form gamma oe empirical expectations discounted amount oe 
gram models positive 
qualitatively desirable meeting targets exactly empirical frequencies tend higher true frequencies events nonzero counts 
analogous behavior produced kneser ney smoothing applied recursively lower order distributions 
case target counts discounted absolute discounting 
derivation equation appendix 
second chen goodman point absolute discounting superior types discounting considered different discount counts counts flat discount modified kneser ney smoothing performs better 
gaussian prior discount gram linear seen equation 
probability assigned grows exponentially grows logarithmically function target probability count 
words roughly speaking gaussian prior method translates logarithmic discounting 
qualitatively appealing model ideal average discount displayed elegant multiple flat discounts 
third chen goodman report interpolated models outperform backed models grams low counts lower order models provide valuable information estimating probabilities 
happily gaussian prior behaves interpolated model gram probability estimates depend lower order information 
follows trivially observation probability assigns gram depends parameter values grams suffixes 
gaussian prior method uses information lower order models meaningful way 
gram gaussian prior method tends adjust zero zero corresponding feature effect model lower order gram probability estimate 
words prior adjusts gram probabilities lower order probability estimate desirable 
chen goodman note additional tunable parameters improve current smoothing methods 
gaussian prior natural free parameters variances oe contrast gaussian prior parameters previous gram smoothing priors applied directly probability space 
mackay peto dirichlet prior adas uses beta prior resulting linear discounting shown perform :10.1.1.50.1464
basic version gaussian prior method implemented free parameters oe grams length constrained variance oe gaussian prior method satisfies desiderata listed chen goodman may perform competitively gram smoothing 
experiments section show case 
experiments compare performance maximum entropy conventional gram smoothing techniques ran experiments training set sizes different text corpora bigram trigram models 
methodology conventional gram smoothing techniques implemented katz smoothing popular algorithm practice modified kneser ney smoothing shown outperform widely techniques :10.1.1.131.5458
addition implemented variation jelinek mercer smoothing equation single parameter different level gram model 
method perform particularly baseline algorithm expository purposes 
refer implementations mnemonics katz kneser ney mod baseline respectively 
implemented maximum entropy smoothing techniques 
technique parameters initialized zero improved iterative scaling applied train model 
iterative scaling terminated perplexity held set longer decreases significantly 
cluster expansion employed reduce computation 
implementation smooth smoothing performed 
training terminated performance held set longer improves probabilities converge zero case training continued convergence 
algorithm disc katz implementation turing discounting described section 
algorithm gauss implementation lafferty gaussian prior method described section 
mentioned earlier method free parameters oe level gram model 
data sources brown corpus contains text number miscellaneous sources wall street journal wsj newspaper text broadcast news bn corpus contains transcriptions television radio news shows switchboard swb corpus contains transcriptions telephone conversations 
experiment selected training set length source held sets source 
held set optimize parameters smoothing algorithm oe parameters gauss discounts modified kneser ney smoothing 
parameters selected minimize perplexity held set powell search algorithm perform search 
held set decide terminate iterative scaling models 
second held set evaluate final perplexity smoothing algorithm 
data source ran experiments training sets sentences words sentences words 
held sets sentences 
training set size sentences brown gram swb gram swb gram wsj gram wsj gram cross entropy baseline smoothing algorithm test set multiple training set sizes brown switchboard wall street journal corpora training sets language models may reach hundreds millions words practice unable consider larger training sets due computational limitations 
training maximum entropy gram models requires great deal computation training conventional gram models 
addition considering multiple parameter settings powell search oe parameters gauss iterative scaling algorithm applied separately parameter setting 
train single model method gauss word training set required hours computation mhz pentium ii computer 
substantially larger training sets feasible parameter optimization 
data sets identical chen goodman consequently results directly comparable analogous results chen goodman :10.1.1.131.5458
details methodology 
results display cross entropy baseline jelinek mercer smoothing algorithm range training set sizes corpora 
graphs follow display performance algorithm difference cross entropy test set cross entropy baseline method training set facilitate visualization 
point graphs represents single experiment analysis standard error observations refer chen goodman :10.1.1.131.5458
give rough idea statistical error involved figures difference kneser ney mod gauss may significant difference algorithms certainly data point 
compare performance various maximum entropy smoothing algorithms multiple training set sizes wall street journal data 
left graph bigram models right graph trigram models 
see smooth outperformed algorithms large margin demonstrating necessity smoothing maximum entropy models 
remaining algorithms gaussian prior method significantly outperformed discounting 
shown see similar behavior experiments training set size sentences relative performance algorithms wsj corpus gram baseline gauss disc katz smooth training set size sentences relative performance algorithms wsj corpus gram baseline gauss disc katz smooth performance relative baseline various maximum entropy smoothing algorithms multiple training set sizes wall street journal corpus bigram trigram models corpora 
figures compare performance maximum entropy smoothing algorithms conventional gram smoothing algorithms corpora 
conventional smoothing methods see katz smoothing generally outperforms baseline modified smoothing significantly better 
maximum entropy methods see disc katz performs comparably katz smoothing bigram models somewhat worse trigram models better larger data sets 
method gauss performs modified kneser ney smoothing slightly better data sets 
gaussian prior method performs better widely algorithms smoothing gram models 
investigate logarithmic discounting gaussian prior compares multiple absolute discounts modified kneser ney smoothing computed closely expected number certain grams test set model matched actual number grams test set 
particular grams occurring times word training set computed ratio expected number times grams occurred word test set actual number times occurred gamma gamma gamma gamma gamma gamma gamma jw gamma gamma gamma gamma gamma gamma gamma gamma gamma ratios displayed bigram trigram models 
gaussian prior achieves ratios closer ideal value modified kneser ney smoothing evidence gaussian prior method superior multiple flat discounts predicting correct average discounts 
investigated number independent variance parameters gaussian prior affects performance 
original implementation gauss different oe ran experiments complemented grams gram feature nonzero longer gram feature nonzero :10.1.1.40.180
resulted significantly inferior performance 
large spikes switchboard graphs discussed chen goodman :10.1.1.131.5458
caused duplicated segment text corresponding training set 
training set size sentences relative performance algorithms broadcast news corpus gram baseline katz kneser ney mod gauss disc katz training set size sentences relative performance algorithms broadcast news corpus gram baseline katz kneser ney mod gauss disc katz training set size sentences relative performance algorithms brown corpus gram baseline katz kneser ney mod gauss disc katz training set size sentences relative performance algorithms brown corpus gram baseline katz kneser ney mod gauss disc katz performance relative baseline various smoothing algorithms multiple training set sizes broadcast news brown corpora bigram trigram models training set size sentences relative performance algorithms switchboard corpus gram baseline katz kneser ney mod gauss disc katz training set size sentences relative performance algorithms switchboard corpus gram baseline katz kneser ney mod gauss disc katz training set size sentences relative performance algorithms wsj corpus gram baseline katz kneser ney mod gauss disc katz training set size sentences relative performance algorithms wsj corpus gram baseline katz kneser ney mod gauss disc katz performance relative baseline various smoothing algorithms multiple training set sizes switchboard wall street journal corpora bigram trigram models count ratio expected actual counts gram words training kneser ney mod gauss count ratio expected actual counts gram words training kneser ney mod gauss ratio expected number actual number test set grams count training data word wall street journal training set bigram trigram models training set size sentences relative performance algorithms wsj corpus gram baseline gauss gauss gauss training set size sentences relative performance algorithms wsj corpus gram baseline gauss gauss gauss performance relative baseline different parameterizations gaussian prior multiple training set sizes wall street journal corpus bigram trigram models level gram model 
considered single oe model gauss parameters oe oe oe level gram model applied grams counts training data respectively 
parameterization gauss analogous parameterization modified smoothing 
performance variations wall street journal corpus displayed 
variations gauss gauss yield identical performance variation gauss performs slightly worse 
having separate variances gram level leads improved performance useful distinction 
investigated parameter tying schemes significantly outperformed simple technique 
discussion argued maximum entropy models require smoothing uniform smooth possible constraints 
maximum entropy models viewed maximum likelihood exponential models similar properties maximum likelihood methods 
example seen data plentiful smoothing smaller effect data sparse smoothing essential 
tasks including language modeling superior performance achieved constructing large models parameters sparsely estimated smoothing 
maximum entropy models competitive techniques domains need effective maximum entropy smoothing algorithms 
showed gaussian prior smooth maximum entropy gram models achieve performance equal superior techniques optimal variances oe gaussian prior powell search range oe size training set 
multiplying converts variances probability space count space discounts relatively constant count space different training set sizes 
discounts tended grow data set size shrink reason variation parameters may outperform variation fewer parameters due search errors parameter optimization 
smoothing gram models field extensive body associated research 
clear demonstration maximum entropy smoothing method effective smoothing techniques types models possible construct maximum entropy models sparse data situations loss performance 
furthermore adds virtually computational cost maximum entropy training procedure 
large underlying computational cost maximum entropy algorithms building maximum entropy models large data sets challenging problem 
smoothing method expressed simply show possesses desirable qualities gram smoothing noted chen goodman empirical analysis 
addition achieves excellent performance fewer parameters comparably performing modified kneser ney smoothing 
gaussian prior kneser ney methods consistently outperform smoothing techniques 
distinction algorithms modified lower order distributions described sections 
distributions chosen satisfy certain marginal constraints derived training data 
marginal constraints may powerful technique designing novel smoothing algorithms language modeling domains 
enforcing marginal constraints mark significant departure traditional techniques smoothing 
addition gaussian prior qualitatively different prior previously gram smoothing 
touched section linear discounting motivated dirichlet beta prior probabilities shown perform poorly 
absolute discounting yields better performance unclear elegantly express technique prior distribution 
contrast gaussian prior applied parameters linear log probability leads logarithmic discounting 
simple prior yields discounting qualitatively quantitatively similar empirical ideal 
gaussian prior applied maximum entropy modeling applied general minimum divergence paradigm 
maximizing entropy equivalent finding model smallest kullback leibler divergence uniform distribution 
minimum divergence modeling selects model satisfying constraints closest default distribution 
model express prior knowledge domain 
minimum divergence models form md exp analysis section applies models modification 
maximum entropy modeling advantages competing approaches terms elegance generality performance gaussian prior powerful tool smoothing general models 
gaussian prior proves superior algorithms domains gram modeling open empirical question 
gram models features partially overlap case general 
addition parameters tied domains explored 
results analysis justify choice gaussian prior gram modeling strongly suggest situations 
gram models single variance oe model worked quite separate oe level gram model worked slightly better 
partitioning applicable general 
derivation modified constraints modified iterative scaling gaussian prior section derive modified constraints equation modified update improved iterative scaling equation gaussian prior method suggested lafferty 
conditional formulation 
details improved iterative scaling proof convergence refer 
derive modified constraints take partial derivatives objective function equation respect parameters set zero 
log yjx gamma oe const gamma log exp gamma oe const gamma exp gamma oe gamma jx gamma oe gamma jx yjx gamma oe gamma yjx gamma oe equation follows simply line 
derivation modified improved iterative scaling update identical original derivation presence extra terms prior 
iteration try find delta fffi maximizes increase objective function delta gamma ffi gamma log yjx exp ffi gamma oe ffi ffi clear maximize function directly find auxiliary function delta maximize bounds function 
maximum delta larger zero optimal satisfy constraints equation 
inequality log gamma get delta gamma ffi gamma yjx exp ffi gamma oe ffi ffi delta substituting applying jensen inequality arrive delta ffi gamma yjx exp ffi gamma oe ffi ffi delta partial derivative delta respect ffi get delta ffi gamma yjx exp ffi gamma ffi oe equation follows setting derivatives zero 
notice rb satisfies constraints 
follows maximum delta larger zero optimal desired 
lafferty personal communication 
berger della pietra della pietra maximum entropy approach natural language processing computational linguistics vol 
pp 

della pietra della pietra lafferty inducing features random fields ieee trans 
pattern analysis machine intelligence vol 

ratnaparkhi maximum entropy models natural language ambiguity resolution ph thesis university pennsylvania philadelphia pa 
rosenfeld maximum entropy approach adaptive statistical language modeling computer speech language vol :10.1.1.40.180
pp 
longer version published adaptive statistical language modeling maximum entropy approach ph thesis computer science department carnegie mellon university tr cmu cs apr 
berger miller just time language modeling icassp seattle wa 
lau adaptive statistical language modelling thesis department electrical engineering computer science massachusetts institute technology cambridge ma 
chen goodman empirical study smoothing techniques language modeling tech :10.1.1.131.5458
rep tr harvard university 
jaynes information theory statistical mechanics physics reviews vol 
pp 

darroch ratcliff generalized iterative scaling log linear models annals mathematical statistics vol 
pp 

bahl jelinek mercer maximum likelihood approach continuous speech recognition ieee trans 
pattern analysis machine intelligence vol 
pami pp 
mar 
brown cocke della pietra della pietra jelinek lafferty mercer statistical approach machine translation computational linguistics vol 
pp 
jun 
kernighan church gale spelling correction program noisy channel model proc 
thirteenth international conf 
computational linguistics pp 

jelinek mercer interpolated estimation markov source parameters sparse data proc 
workshop pattern recognition practice amsterdam netherlands north holland may 
brown della pietra della pietra lai mercer estimate upper bound entropy english computational linguistics vol 
pp 
mar 
bell cleary witten text compression prentice hall englewood cliffs 
katz estimation probabilities sparse data language model component speech recognizer ieee trans 
acoustics speech signal processing vol 
assp pp 
mar 
kneser ney improved backing gram language modeling proc 
ieee international conf 
acoustics speech signal processing vol 
pp 

ney essen kneser structuring probabilistic dependences stochastic language modeling computer speech language vol 
pp 

population frequencies species estimation population parameters biometrika vol 
pp 

church gale comparison enhanced turing deleted estimation methods estimating probabilities english bigrams computer speech language vol 
pp 

chen beeferman rosenfeld evaluation metrics language models darpa broadcast news transcription understanding workshop 
brown della pietra della pietra mercer nadas roukos maximum penalized entropy construction conditional log linear language translation models learned features generalized csiszar algorithm internal ibm report 
lau rosenfeld roukos adaptive language modeling maximum entropy principle proc 
arpa workshop human language technology pp 

della pietra della pietra statistical modeling maximum entropy unpublished report 
lau rosenfeld roukos trigger language models maximum entropy approach proc 
icassp apr pp 
ii ii 
newman extension maximum entropy method ieee trans 
information theory vol 
pp 
jan 
khudanpur method maximum entropy estimation relaxed constraints johns hopkins university language modeling workshop proc center language speech processing johns hopkins university baltimore 
mackay peto hierarchical dirichlet language model natural language engineering vol :10.1.1.50.1464
pp 

nadas estimation probabilities language model ibm speech recognition system ieee trans 
acoustics speech signal processing vol 
assp pp 
aug 
lafferty suhm cluster expansions iterative scaling maximum entropy language models maximum entropy bayesian methods hanson silver eds 
kluwer academic publishers 
kucera francis computational analysis day american english brown university press providence 
stern specification arpa hub evaluation unlimited vocabulary nab news baseline proc 
darpa speech recognition workshop pp 

rudnicky hub business broadcast news proc 
darpa speech recognition workshop pp 

godfrey mcdaniel switchboard telephone speech corpus research development proc 
icassp vol 
pp 

press flannery teukolsky vetterling numerical recipes cambridge university press cambridge 
kullback information theory statistics wiley new york 
beeferman berger lafferty model lexical attraction repulsion proc 
acl madrid spain 
