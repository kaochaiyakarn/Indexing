bayesian inference mixtures experts hierarchical mixtures experts models application speech recognition peng department mathematics statistics university nebraska lincoln robert jacobs department brain cognitive sciences university rochester martin tanner department statistics northwestern university september accepted publication journal american statistical association wish neal anonymous referees associate editor insightful comments de sa providing speech dataset 
tanner supported nih research ro ca 
jacobs supported nih research mh 
peng supported nih research ca 
machine classification acoustic waveforms speech events difficult due context dependencies 
vowel recognition task multiple speakers studied class modular hierarchical systems referred mixtures experts hierarchical mixtures experts models 
statistical model underlying systems mixture model mixture coefficients mixture components generalized linear models 
full bayesian approach basis inference prediction 
computations performed markov chain monte carlo methods 
key benefit approach ability obtain sample posterior distribution functional parameters model 
way information obtained provided point estimate 
avoided need rely normal approximation posterior basis inference 
particularly important cases posterior skewed multimodal 
comparisons hierarchical mixtures experts model pattern classification systems vowel recognition task reported 
results indicate model showed classification performance gave additional benefit providing opportunity assess degree certainty model classification predictions 
psychological studies human speech perception documented people extraordinary abilities categorize acoustic waveforms speech events 
abilities impressive consider people accurately perceive speech listening unfamiliar voices noisy surroundings widely varying rates speech production 
program computers topics research 
advances understanding speech perception achieved variety complementary research programs 
perceptual cognitive psychologists study perceptual cognitive linguistic mechanisms allow people process speech efficient seemingly effortless manner 
related research pursued statisticians computer scientists attempt build machines recognize speech wide range environments rabiner juang 
research paths converged pinpointing factors speech perception challenging task 
factors stem fact speech production highly context dependent 
example difficult partition speech waveform acoustic segments correspond distinct phonetic segments 
primarily due coarticulation vocal articulatory movements successive sounds overlap time motions produce particular phonetic segment dependent preceding succeeding phonetic segments 
second example identical phonemes spoken differently different speakers speaker different points time classified equivalent events 
differences speakers speech correlated speakers gender age dialect 
common approaches overcoming problems associated context dependency rabiner juang 
approach detect invariant features 
example features acoustic waveform corresponding phonetic segment may vary speaker speech characteristics may acoustic features common speakers 
related approach normalize acoustic waveform context 
may exist example transformation removes features arise due speaker characteristics 
third approach knowledge context aid classification speech events 
example different classifiers may depending gender speaker 
addition classifying speech events approach classify context 
considers application class statistical models speech recognition task 
models classify speech events detecting features invariant speakers speech characteristics 
data represent instances vowels spoken large number speakers 
instances taken utterances words began contained vowel middle ended 
data speakers uttered word twice words different random orders presentation 
speakers male adults female adults fifteen children 
spectral analysis performed utterance portion spectrogram corresponding vowel hand segmented formants extracted middle portion segmented region 
formants vocal tract resonant frequencies 
different placements speech articulators corresponding different vowels alter vocal tract shape frequency response 
horizontal vertical axes give second formant values properly normalized vowel instance 
classes vowels represented individual data points correspond instances labeled digits zero indicate vowel class belong 
vowel class table gives word vowel segmented digit label class 
data originally collected peterson barney benchmark database speech recognition literature 
consider task classifying vowel instances basis values instance second formants 
address problem class modular hierarchical systems known mixtures experts hierarchical mixtures experts hme models 
models attempt solve problems divide conquer formant horizontal vertical axes give values second formants respectively 
data point labeled digit indicates vowel class data point belongs 
vowel class word label heard heed hid head ae hud hod hood table vowel class table gives word vowel segmented digit label class 
strategy complex problems decomposed set simpler sub problems 
assume data adequately summarized collection functions defined local region domain 
approach adopted attempts allocate different modules summarize data located different regions 
hme models characterized fitting piecewise function data 
data assumed form countable set paired variables vector explanatory variables referred covariates vector responses 
hme models divide covariate space meaning space possible values explanatory variables regions fit simple surfaces data fall region 
piecewise approximators models regions disjoint 
regions soft boundaries meaning data points may lie simultaneously multiple regions 
addition boundaries regions simple parameterized surfaces parameter values estimated data 
hme models combine properties generalized linear models mixture models 
generalized linear models model relationship set covariate response variables 
typical applications include regression binary multiway classification 
standard generalized linear models assume conditional distribution responses covariates finite mixture distribution 
hme models assume finite mixture distribution provide motivated alternative non parametric models provide richer class distributions standard generalized linear models 
proposes markov chain monte carlo methodology inference context hme models 
sections mixtures experts hierarchical mixtures experts models respectively 
section presents bayesian approach training hme model multiway classification problem 
section reports application methodology speech recognition task 
mixtures experts model motivate mixtures experts model jacobs jordan nowlan hinton assume process generating data decomposable set sub processes defined possibly overlapping regions covariate space 
data item subprocess selected covariate selected sub process maps response precisely data generated follows 
covariate ffl label chosen multinomial distribution probability ijx matrix parameters underlying multinomial distribution ffl response generated probability jx phi parameter matrix phi represents possibly nuisance parameters explicitly relate approach generalized linear models framework assume conditional probability distribution jx phi member exponential family distributions 
expected conditional value response denoted defined generalized linear function parameter matrix quantities gamma phi respectively natural parameter dispersion parameter response conditional distribution jx phi 
total probability generating mixture density jx theta ijx jx phi theta phi phi matrix parameters 
assuming independently distributed data total probability dataset product densities likelihood ijx jx phi model viewed mixture model mixing weights mixed distributions dependent covariate presents graphical representation model 
model consists modules referred expert networks 
networks approximate data region covariate space expert network maps input covariate vector output vector assumed different expert networks appropriate different regions covariate space 
consequently model requires module referred gating network identifies covariate expert blend experts output approximate corresponding response vector gating network outputs set scalar coefficients weight contributions various experts 
covariate coefficients constrained nonnegative sum 
total output model convex combination expert outputs perspective statistical mixture modeling identify gating network selection particular sub process 
gating outputs interpreted gating network expert network expert network mixtures experts model 
covariate dependent multinomial probabilities selecting sub process different expert networks identified different sub processes expert models distributions associated corresponding sub process 
expert networks map inputs outputs stage process 
stage expert multiplies covariate vector matrix parameters 
vector assumed include fixed component allow intercept term 
expert matrix denoted resulting vector denoted second stage mapped expert output monotonic continuous nonlinear function selection nonlinear function nature problem 
regression problems may taken identity function experts linear 
probabilistic component model may gaussian 
case likelihood mixture gaussians sigma gamma gamma gamma sigma gamma gamma expert output expert dispersion parameter sigma interpreted mean covariance matrix expert gaussian distribution response covariate output entire model interpreted expected value binary classification problems may logistic function case may interpreted log odds success bernoulli probability model 
probabilistic component model assumed bernoulli distribution 
likelihood mixture bernoulli densities gamma gammay quantity expert conditional probability classifying covariate success expected success problems multiway classification counting rate estimation survival estimation may suggest choices cases take inverse canonical link function appropriate probability model mccullagh nelder 
gating network forms outputs stages 
linear stage computes intermediate variables inner product covariate vector vector parameters mapped gating outputs nonlinear stage 
mapping performed generalization logistic function note inverse function canonical link function multinomial response model mccullagh nelder 
bayesian inference regarding gating expert networks parameters performed markov chain monte carlo methods 
particular fact problem greatly simplified augmenting observed data unobserved indicator variables indicating expert model 
idea data augmentation tanner wong mixture problems mixing parameters unknown scalars discussed detail diebolt robert 
jordan jacobs showed parameters mixtures experts hierarchical experts model estimated augmentation idea context em algorithm 
jordan xu analysis convergence properties em algorithm applied hme models 
discussion relationship models neural networks peng jacobs tanner presents model regression illustrates bayesian learning nonlinear regression context 
hierarchical mixtures experts model philosophy underlying model solve complex modeling problems dividing covariate space restricted regions different generalized linear models fit data region 
divide conquer approach useful desirable pursue strategy logical extreme 
particular sensible divide covariate space restricted regions recursively divide region sub regions 
hierarchical extension mixtures experts model referred hme model tree structured model implements strategy jordan jacobs 
contrast single level model hme model summarize data multiple scales resolution due nested covariate regions 
jordan jacobs empirically models nested structure outperform single level models equivalent number free parameters 
hme model model contains multiple gating networks 
gating networks located non terminals tree implement recursive splitting covariate space 
expert networks terminals tree 
defined relatively small regions covariate space experts fit simple functions data generalized linear models 
illustrates hme model 
explanatory reasons limit presentation level tree extension trees arbitrary depth width straightforward 
bottom level tree contains number clusters model tree contains clusters enclosed box 
top level tree contains additional gating network combines outputs cluster 
matter notation letter index branches top level letter index branches bottom level 
expert networks map covariate output vectors ij output th cluster jji ij scalar jji output gating network th cluster corresponding th expert 
output model scalar output top level gating network corresponding th cluster 
probabilistic interpretation hme model simple extension model 
short assume process generates data involves nested sequence decisions covariate outcome sequence selection sub process maps response level tree example assume label chosen multinomial distribution probability ijx matrix parameters underlying distribution 
label chosen multinomial distribution probability jjx parameters underlying distribution dependent value label sub process gating network expert network expert network expert network expert network gating network gating network hierarchical mixtures experts model 
generates response sampling distribution probability ij phi ij ij gamma ij natural parameter distribution phi ij dispersion parameter 
probability may written jx ij phi ij 
total probability generating hierarchical mixture density jx theta ijx jjx jx ij phi ij theta matrix parameters 
model process hme model identify top level gating network selection label bottom level gating networks selection label different expert networks identified different sub processes 
give just example resulting likelihood function binary classification problem hierarchical mixture bernoulli densities jji ij gamma ij gammay quantity ij expert conditional probability classifying covariate success expected success hme model similarities classification regression tree models previously proposed statistics literature 
cart breiman friedman olshen stone mars friedman hme model tree structured approach piecewise function approximation 
major difference hme model cart mars way covariate space segmented regions 
cart mars hard boundaries regions data item lies exactly region 
hme model contrast allows regions overlap data items may reside multiple regions 
boundaries regions said soft 
mars restricted forming region boundaries perpendicular covariate space axes 
mars coordinate dependent sensitive particular choice covariate variables encode data 
hme model coordinate dependent boundaries regions formed hyperplanes arbitrary orientations covariate space 
addition cart mars nonparametric techniques hme model parametric model 
jordan jacobs compared performance hme model cart mars robot dynamics task hme model em algorithm estimate parameters yielded modest improvement 
hme model multiway classification basic simplification obtained augmenting observed data unobserved indicator variables indicating expert model 
define probability ijx jjx jx ij phi ij ijx jjx jx ij phi ij jji probability jji jjx jx ij phi ij jjx jx ij phi ij ij theta jji take value probability ij ijx jjx jx ij phi ij ijx jjx jx ij phi ij ij matrix parameters associated th expert network matrix parameters associated gating network top level model matrix parameters associated gating network th cluster bottom level 
vector indicator variables 
augmented likelihood hme model ijx jjx jx ij phi ij ij jji jx ij phi ij ij defined jji ij ik restricting attention multiway classification problems suppose delta delta delta outcome classes values ij ij ijn theta matrix parameters associated th expert network phi ij case conditional probability model written yjx ij phi ij ijk express probability model density form exponential family natural parameters defined ijk log ijk ijn ijk ijk ijk ijk noted inverse canonical link function multinomial response model mccullagh nelder 
follows augmented likelihood re written jji ijk ij ij ik ijk ijk ij equation independent normal priors mean variance oe theta yield standard densities full conditionals apply approach muller draw posterior sample top level gating network parameter matrix bottom level gating network parameter matrices bottom level expert network parameter matrices ij particular draw deviate full conditional metropolis algorithm 
example consider top level gating network parameters 
candidate value point metropolis chain drawn multivariate normal distribution current sample values mean diagonal variance covariance matrix allow variation current sample values gamma fl 
candidate value accepted rejected standard metropolis scheme tanner 
metropolis algorithm iterated final value chain treated deviate full conditional distribution 
speech recognition problem hme model applied speech recognition task described 
dataset consists data items speakers theta vowel classes theta presentations vowel note speakers data vowel missing dataset 
collection items randomly selected assigned training set remaining assigned prediction set 
hme model consisted gating networks expert networks arranged level tree structure 
bottom level clusters gating network expert networks 
outputs clusters combined gating network located top level 
model trained gibbs sampler algorithm described em algorithm 
gibbs sampler chains length created 
known posterior distributions mixture models highly multimodal 
approaches context gibbs sampler address aspect problem 
approach allowed mode jumping chain moved parameter space hopefully avoiding oversampling neighborhoods isolated minor modes 
facilitate iterations gibbs sampler parameter vector theta selected random modes defined independent randomly initialized runs em algorithm 
candidate value compared current point gibbs sampler chain metropolis test observed posterior 
data table figures final iterations chain 
metropolis algorithm sample full conditional distributions run iterations variance normal distribution equal 
variance normal priors components expert gating networks parameters equal 
noted mode jumping approach introduces possible approximation due fact transition kernel metropolis test discrete posterior distribution continuous 
expectation algorithm quickly converge approximation 
check adopted second approach gelman rubin 
particular formed mixture normals approximate observed posterior 
drew sample size approximation 
importance sampling obtain starting points gibbs sampler chains run mode jumping 
second approach confirmed results approach approaches yielded qualitatively similar outcomes 
alternative technique sampling multimodal posteriors neal 
convergence chains evaluated technique gelman rubin 
intuitively gelman rubin technique assessing convergence compares chain variation chain variation algorithm said converge chain chain variations comparable size 
assess convergence gibbs sampler algorithm input gelman rubin technique conditional predictive probability data item belongs particular vowel class vowel class jjx theta 
conditional predictive probability assessing convergence neal 
covariate values examined gelman rubin value exceed meaning suggested evidence lack convergence dataset model 
table shows classification performances systems data prediction set 
systems hme model trained gibbs sampler algorithm em algorithm respectively 
number correct classifications average chains system instances second system instances differ due random initial settings system parameter values 
cases vowel class corresponding response variable largest expected value model classification 
third fourth systems versions cart breiman friedman olshen stone 
version splits covariate space restricted perpendicular covariate axes table classification performances systems data prediction set 
systems hme model trained gibbs sampler algorithm em algorithm respectively systems versions cart 
total gibbs em cart cart ii category category correct correct correct correct second version splits allowed linear combinations covariate variables 
hme model trained gibbs sampler algorithm showed best performance 
similar performance levels achieved hme model trained em algorithm second version cart 
version cart showed worst performance 
major benefit sampling approach inference hme models ability assess degree certainty model classification 
illustrated figures 
graphs figures show estimated posterior distributions probability data item belongs particular vowel class values covariates 
horizontal axis graph gives probability vertical axis gives class probability density estimated posterior distribution probability data item belongs correct vowel class 
note mass concentrated unity indicating model confident prediction 
graph gives result chain gibbs sampler chains produced nearly identical results 
gelman rubin value equal 
density 
mean distributions figures predictive probability corresponding class jjx jjx theta 
data item consider member vowel class 
graph shows estimated posterior distribution probability item belongs correct vowel class covariate values 
graph gives result chain gibbs sampler chains produced nearly identical results 
distribution highly skewed majority mass concentrated probability unity 
model consistently correctly confident data item belongs vowel class 
shown distributions corresponding vowel classes mass concentrated zero 
second data item consider member vowel class 
shows estimated posterior distributions probability item belongs correct vowel class shows distributions incorrect vowel class class 
graphs figures correspond chains gibbs sampler 
horizontal axis graph figures indicating expected value posterior distribution 
expected values generally larger incorrect vowel class correct vowel class meaning model incorrectly classified data item 
note distributions vowel classes mass concentrated middle probability range suggesting model confident prediction vowel classes 
distribution remaining classes shown mass concentrated near probability zero 
distributions vowel classes indicate model confident data item belongs vowel class uncertain classes item belongs 
approach full bayesian analysis hme models 
basic gibbs sampler equations applied methodology vowel recognition task 
comparisons classification performance hme model trained gibbs sampler algorithm hme model trained em algorithm versions cart conducted 
results indicate hme model trained gibbs sampler yielded performance gave additional benefit providing opportunity assess degree certainty model classification predictions 
research extended diebolt robert case mixing coefficients mixture components generalized linear models 
feel idea soft boundaries regions covariate space shows promise study idea statistical community warranted 
models domains generalized linear models typically applied see having broad implications 
example may apply class models problems censored regression data poisson regression image analysis dna structure prediction 
interest development class probability density xx class probability class probability density class probability class probability density class probability class probability density class probability class probability density class probability estimated posterior distributions probability data item belongs correct vowel class 
horizontal axis graph indicates expected value distribution 
note mass concentrated middle probability range indicating model confident prediction 
gelman rubin value equal 
class probability density class probability class probability density class probability class probability density class probability class probability density class probability class probability density class probability estimated posterior distributions probability data item belongs incorrect vowel class 
horizontal axis graph indicates expected value distribution 
note mass concentrated middle probability range indicating model confident prediction 
gelman rubin value equal 
algorithms automatically specify number levels branching factors level model 
breiman friedman olshen stone 
classification regression trees 
belmont ca wadsworth international group 
diebolt robert 
estimation finite mixture distributions bayesian sampling 
journal royal statistical society 
friedman 
multivariate adaptive regression splines 
annals statistics 
gelman rubin 
inference iterative simulation multiple sequences 
statistical science 
jacobs jordan nowlan hinton 
adaptive mixtures local experts 
neural computation 
jordan jacobs 
hierarchical mixtures experts em algorithm 
neural computation 
jordan xu 
convergence results em approach mixtures experts architectures 
technical report department brain cognitive sciences massachusetts institute technology 
mccullagh nelder 
generalized linear models 
london chapman hall 
muller 
metropolis posterior integration schemes 
journal american statistical association press 
neal 
bayesian mixture modeling monte carlo simulation 
technical report crg tr department computer science university toronto 
neal 
sampling multimodal distributions tempered transitions 
technical report department statistics university toronto 

speech communication human machine 
reading ma addison wesley publishing 
peng jacobs tanner 
bayesian inference mixtures experts hierarchical mixtures experts architectures 
technical report department biostatistics university rochester medical center 
peterson barney 
control methods study vowels 
journal acoustical society america 
rabiner juang 
fundamentals speech recognition 
englewood cliffs nj prentice hall 
tanner 
tools statistical inference methods exploration posterior distributions likelihood functions 
new york springer verlag 
tanner wong 
calculation posterior distributions data augmentation 
journal american statistical association 
