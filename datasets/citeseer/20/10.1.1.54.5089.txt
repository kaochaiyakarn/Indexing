improved algorithm incremental induction decision trees paul utgoff technical report february updated april department computer science university massachusetts amherst ma utgoff cs umass edu appear proceedings eleventh international conference machine learning 
presents algorithm incremental induction decision trees able handle numeric symbolic variables 
order handle numeric variables new tree revision operator called introduced 
non incremental method finding decision tree direct metric candidate tree 
contents design goals improved algorithm incorporating training instance ensuring best test decision node information kept decision node tree transposition cutpoint ensure best test incremental training cost error correction mode inconsistent training instances direct metrics attribute selection lazy restructuring research agenda summary improved algorithm incremental induction incremental induction desirable number reasons 
importantly revision existing knowledge presumably underlies human learning processes assimilation generalization 
secondly knowledge revision typically expensive knowledge creation 
example receiving new training instance expensive revise decision tree build new tree scratch augmented set accumulated training instances utgoff 
ability revise knowledge efficient manner opens new possibilities algorithms remain prohibitively expensive 
example non incremental algorithm searches space decision trees deliberately typical greedy approach 
moving space trees highly impractical ability revise existing tree inexpensively 
considerable attention devoted incremental induction including samuel checkers player winston algorithm mitchell candidate elimination algorithm fisher cobweb laird soar explosion reinforcement learning sutton temporal difference learning 
non incremental induction received attention including michalski aq quinlan clark niblett cn rendell lfc 
non incremental algorithms possess desirable properties efficiency high classification accuracy intelligible classifier making highly useful tools data analysis 
need inductive algorithm embed agent knowledge maintainer non incremental algorithm impractical afford run repeatedly 
wishes desirable properties non incremental algorithms low incremental cost incremental algorithms 
design goals algorithm described section motivated variety design goals discussed greater detail section 
incremental cost updating tree lower cost building new decision tree scratch 
necessary sum incremental costs care cost brought date 

update cost independent nearly number training instances tree 

tree produced incremental algorithm depend set instances incorporated tree regard sequence instances 

algorithm accept instances described mix symbolic numeric variables attributes 

algorithm handle multiple classes just 
improved algorithm incremental induction 
algorithm inconsistent training instances 

algorithm favor variable larger value set 

algorithm execute efficiently time space 

algorithm handle instances missing values 

algorithm avoid overfitting noise instances 

algorithm ability find compound tests similar pagallo fringe 

algorithm capable grouping values variable 
earlier id algorithm utgoff meets goals 
regarding fourth goal id accepts symbolic variables 
id meet remaining goals 
improved algorithm section presents algorithm iti aims meet design goals described 
meets plans addressing described section 
basic algorithm follows id adds ability handle numeric variables instances missing values inconsistent training instances 
furthermore order partition conservatively possible decision node test decision node binary 
multi valued symbolic variable dynamically encoded set boolean variables 
example symbolic variable color value set red blue green automatically encoded boolean variables color red color blue color green 
numeric variable dynamically encoded threshold test manner similar quinlan 
example value automatically encoded boolean variable 
basic task iti accept new training instance update tree response 
update process revises tree extent necessary tree corresponds set training instances seen far 
updating tree involves steps 
described fully incorporate instance tree passing proper branches reaches proper leaf 
second step described traverse tree root leaves restructuring necessary decision node employs best available test partition training instances 
incorporating training instance initial tree empty tree nil 
instance incorporated tree tree nil tree replaced leaf node indicates class leaf instance attached saved leaf node 
instance incorporated branches tree followed values instance leaf reached 
instance class leaf instance improved algorithm incremental induction simply added set instances saved node 
instance different class label leaf algorithm attempts turn leaf decision node picking best attribute quinlan gain ratio metric implemented exactly described 
metric de distance metric 
instances saved leaf incorporated sending proper branch new test 
instance missing value needed test decision node instance simply saved decision node passing branch 
ensuring best test decision node immediately instance incorporated tree tree traversed root recursively ensuring best test possible node tested node 
usual test considered best favorable value attribute selection metric 
order training instances remain immaterial tie best test broken deterministically order 
new training instance various frequency counts node traversed instance change 
attribute selection metric function particular probabilities frequency counts attribute selection metric value test change may turn change assessment test considered best node 
efficiency implementation mark node stale test information changes occurs instance passed decision node incorporation subtree node revised explained 
traversal root ensure best test node subtree node marked stale need checked revised skipped entirely 
information kept decision node decision node iti algorithm maintains list possible tests test decision node 
binary test symbolic variable table frequency counts class value combination maintained 
numeric variable sorted list values seen tagged class maintained variable best cutpoint variable 
best cutpoint calculated considering midpoints adjacent pair values possible cutpoint 
fayyad values adjacent values tagged differing classes need considered 
storing best cutpoint numeric variable effectively encoding just best binary test numeric variable 
current implementation cost insertion deletion numeric value linear number items 
version cost reduced log number items avl tree maintain sorted list values 
balanced binary tree representation maintaining value list symbolic variable 
improved algorithm incremental induction delta delta delta delta delta delta delta delta 
tree transposition operator tree transposition tree revision operators employed iti transpose tree 
different test replace current decision node non leaf subtree recursively revised new test occurs root subtree 
tree transposed decision node illustrated 
id forces expansion immediate subtree leaf iti 
force decision node existence just sake complying preconditions tree transposition operator 
tree transposition operator enhanced handle cases subtree leaf 
important observation subtrees children node need examined revised tree transposition 
subtree corresponds set instances set changed transposition subtree need changed way 
similarly set instances corresponding root tree transposed change test information maintained decision node change 
reasons information kept nodes children needs revised 
done inexpensively re examining training instances simply combining information kept child children nodes 
symbolic variables adds frequency counts numeric variables merges sorted value lists 
instances may stored child node simply removed parent node find way proper node typically leaf 
cutpoint remaining tree revision operator employed iti cutpoint 
numeric variable may variable node test altered basis boolean test node comparison different cutpoint 
numeric variable different cutpoint required new test change cutpoint different value 
necessary restructure tree transposition simply change cutpoint 
training instances previously passed left right branch tree improved algorithm incremental induction variable value instance old cutpoint 
simply changing cutpoint havoc instances passed branch old cutpoint passed branch new cutpoint 
cutpoint instances current node need checked possibly moved subtree 
done traversing subtrees new cutpoint hand 
instance wrong subtree backed current node removing information tests way 
cutpoint changed instances backed correct subtree passing proper branch reaching proper node 
ensure best test seen bring desired boolean test root tree transposition 
general procedure bringing desired test symbolic numeric root subtree stated 
desired test current numeric variable wrong cutpoint cutpoint 
desired test wrong variable recursively bring desired test root immediate nonleaf subtree transpose tree 
recursively ensure best test subtrees 
discussed need recursive call subtree marked stale 
incremental training cost important goal incremental method incremental cost cost starting scratch 
incremental cost cost incorporating new instance revising tree extent necessary 
general incremental cost proportional number nodes tree 
size tree generally grows approximate final size early training rest training serving improve selection test node 
concern incremental training cost continues grow size tree stabilized context discussion 
case symbolic variables involved show analytically incremental cost tree revision entirely independent number training instances seen utgoff case numeric variables 
numeric variable node sorted list values observed instances maintained 
training instances means greater cost maintain list 
uses avl tree cost insertions deletions small dependent log number training instances 
question drastically change practice greatest potential lies requiring large number insertions deletions 
numerous uns variety problems suggests typically change issue studied carefully 
practical purposes variety problems tried date incremental training cost appears effectively independent training effort 
illustrate incremental training cost nearly independent training effort improved algorithm incremental induction instances training sequence seen far cpu seconds expected tests 
liver disorders numeric variables results running iti problem numeric variables shown 
graph shows noticeable evidence growth incremental cost tree size stabilized 
behavior typical runs numerical data date 
measure expected number tests shown graph corresponds closely tree size 
expected number tests sum number tests evaluated classifying training instances divided number training instances 
measure computed inexpensively single traversal tree 
graph indicates low incremental cost step 
worst incremental cost apparently lower cost rebuilding tree scratch point 
behavior discernible growth incremental cost observed runs date unusual cause quickly remedied 
audiology data uc irvine repository contains variable unique identifier instance 
training proceeded value set identifier variable growing instance large number values progressively slowing algorithm 
removal unusual variable eliminated growth shown 
somewhat disturbing see size tree measured expected number tests classify instance varies training 
indicates considerable sensitivity attribute selection metric underlying probabilities 
improved algorithm incremental induction instances training sequence seen far cpu seconds expected tests 
audiology symbolic variables error correction mode iti algorithm saves instances tree instances typically re examined reconstruct counting information 
instances retained leaf converted decision node counting information initialized immediately loss information 
instance counted inspected leaf just attached extra expense counting node converted decision node 
strictly case see discussion cause instance backed subtree added 
known alternative incorporating training instance tree incorporate instance existing tree misclassify 
mode training akin error correction procedures statistical pattern recognition suggested context decision tree induction schlimmer fisher 
stream training instances effectively discards instances tree currently correct 
fixed pool instances iti cycle pool repeatedly removing incorrectly classified instance pool incorporating decision tree tree misclassify instance remaining pool 
instances examined time training regimen departs somewhat notion stream training instances 
training error correction mode pool regimen results tree fewer instances built lower cost 
tree smaller improved algorithm incremental induction accurate tree instances utgoff reason phenomenon unknown 
example compare results wisconsin breast cancer task fold cross validation split training test data 
normal mode iti incorporated average instances building tree average accuracy test data 
run error correction mode splits iti incorporated average instances average accuracy 
kind behavior typical tasks tried iti 
instances noise free training error correction mode leads saving set training instances sufficient cause consistent tree 
instances discarded receipt 
stream instances contain noise error correction mode save instance incorrect time instance received 
lead unbounded saving instances tree institute scheme discarding older instances 
pool instances pool regimen error correction mode works properly sense leads iti build formed tree iti halts 
iti handles inconsistent instances continues pool instance remaining pool classified correctly 
instance pool misclassified removed pool added tree 
tree may perfect classifier instances incorporated continue train misclassified instances remaining pool 
inconsistent training instances instances inconsistent described variable values different class labels 
inconsistent instances occur directed leaf 
split impure leaf cause infinite recursion 
converting leaf decision node provide information easily detected gain ratio metric iti keeps node leaf simply adds instance set instances retained leaf making impure leaf 
direct metrics attribute selection ability restructure tree inexpensively opens possibility searching deliberately test decision node 
afford change test node see positive effect tree 
experiment prohibitively expensive rebuild subtrees time scratch 
idea trying different tests node measuring quality resulting tree led non incremental approach tree induction searches space trees deliberately typical greedy approaches 
move state state tree revision evaluate tree direct metric tree traditional indirect metric probabilities estimated training distribution node 
tree built new instances expected anytime soon tries possible test root 
try test means test node restructure subtrees indirect attribute selection metric 
improved algorithm incremental induction test tried evaluate tree direct metric 
mode iti currently measures expected number tests tree direct metric choice applied 
tests tried root select test gave best tree find subtrees recursively 
boolean test symbolic variable tried boolean test best cutpoint numeric variable considered 
algorithm greedy attribute selection metric direct function tree set probabilities 
approach expensive traditional indirect approach involves tree restructuring determine value 
direct metric mode iti produces dramatic improvement tree indirect attribute selection metric 
example classic bit multiplexor iti finds optimal tree fewest expected number tests fewest nodes cpu seconds dec 
hepatitis problem fold cross validation described iti tree direct metric average seconds 
tree indirect metric average expected tests nodes average accuracy remaining data tree direct metric data average expected tests nodes average accuracy 
illustration single problem stands reason method enumerates evaluates trees directly find tree higher quality method evaluates tests builds just tree 
monks problem finding tree direct metric required seconds 
tree built indirect metric training data expected tests nodes accuracy test data tree built direct metric expected tests nodes accuracy 
results indicate plenty room improvement attribute selection techniques 
case iti ability revise tree inexpensively possible consider metric function entire tree just probablistic information tallied node 
unworkable build trees fly nonincremental method 
lazy restructuring worth pointing effort iti goes ensuring training instance tree best test decision node 
batch instances received time wasteful restructure tree 
add instance tree revising tree simple operations occur incorporating instance 
instances batch incorporated single call procedure ensuring best test node brings tree proper form 
iti run purely incrementally purely 
generally know tree needed strategy improving efficiency revise tree needed purpose classification root node marked stale 
improved algorithm incremental induction research agenda reviewing design goals section iti meets providing large improvement id meets 
consider id obsolete 
goals discussed worth adding iti implemented procedures easy larger systems 
iti meet goals pursued actively discussed 
iti handle noisy instances sense prune subtrees overfit data 
retain characteristic tree set instances independent order want discard 
wants mark node virtually pruned retaining ability unmark time loss information 
subtrees marked existence expense destroying reconstructing 
effect classification instances tree construction se 
arrives virtually pruned decision node treats leaf returning corresponding class label 
method deciding subtree virtually pruned apply minimum description length principle 
lend incremental approach 
pagallo showed find candidate compound tests examining fringe existing decision tree 
considering compound test building new tree better fit data 
unfortunately fringe algorithm calls building new tree scratch iteration 
iti able accomplish result revising tree 
identifying compound test add traversal tree needed add compound variable decision node 
recursion inspect instances leaves order tally joint probability information compound variable 
return tree add variable decision node combine counting information value lists subtrees manner described tree transposition 
call routine ensure best test node bring needed restructuring inexpensively 
improve fit tree training data group values variable similar distributions instances 
avoids excessive partitioning generally reducing size improving accuracy quinlan 
fayyad devised greedy algorithm grouping values symbolic variable 
neil berkman currently investigating approaches grouping values iti 
summary algorithm iti incremental induction decision trees 
algorithm meets design goals set plans achieving remaining goals place 
iti algorithm borrows mechanism tree transposition id adds new capabilities notably ability handle numeric variables incremental induction 
principal advantage incremental learning expensive serial learning repeated running nonincremental algorithm 
addition running error correction mode boosts time improved algorithm incremental induction space efficiency classification accuracy 
notion able revise tree inexpensively searching space decision trees tractable step tree tree cheaply generate tree scratch 
non incremental method uses inexpensive revision capability select test node directly measuring quality resulting tree computing heuristic function probabilities tallied test 
approach infeasible ability revise trees inexpensively 
material supported national science foundation 
iri equipment ross quinlan australian research council 
am indebted ross quinlan excellent suggestions stay university sydney 
mike cameron jones carla brodley jeff clouse neil berkman jamie callan stephen soderland connell helpful comments 
clark niblett 

cn induction algorithm 
machine learning 
de 

distance attribute selection measure decision tree induction 
machine learning 
fayyad 

induction decision trees multiple concept learning 
doctoral dissertation computer science engineering university michigan 
fayyad irani 

handling continuous valued attributes decision tree generation 
machine learning 
fisher 

knowledge acquisition incremental conceptual clustering 
machine learning 
laird rosenbloom newell 

chunking soar anatomy general learning mechanism 
machine learning 
michalski 

learning told learning examples experimental comparison methods knowledge acquisition context developing expert system soybean disease diagnosis 
policy analysis information systems 
mitchell 

version spaces approach concept learning 
doctoral dissertation department electrical engineering stanford university palo alto ca 
pagallo 

adaptive decision tree algorithms learning examples 
doctoral dissertation university california santa cruz 
quinlan 

induction decision trees 
machine learning 
quinlan 

programs machine learning 
morgan kaufmann 
improved algorithm incremental induction rendell 

lookahead feature construction learning hard concepts 
machine learning proceedings tenth international conference pp 
amherst ma morgan kaufmann 
samuel 

studies machine learning game checkers 
ibm journal research development 
schlimmer fisher 

case study incremental concept induction 
proceedings fifth national conference artificial intelligence pp 

pa morgan kaufmann 
sutton 

learning predict method temporal differences 
machine learning 
utgoff 

improved training incremental learning 
proceedings sixth international workshop machine learning 
ithaca ny morgan kaufmann 
utgoff 

incremental induction decision trees 
machine learning 
winston 

learning structural descriptions examples 
winston ed psychology computer vision 
new york mcgraw hill 
