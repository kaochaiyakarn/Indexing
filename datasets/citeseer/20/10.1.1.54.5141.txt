latent semantic indexing lsi trec susan dumais bellcore south st morristown nj std bellcore com 
overview latent semantic indexing latent semantic indexing lsi extension vector retrieval method salton mcgill dependencies terms explicitly taken account representation exploited retrieval 
done simultaneously modeling interrelationships terms documents 
assume underlying latent structure pattern word usage documents statistical techniques estimate latent structure 
description terms documents user queries underlying latent semantic structure surface level word choice representing retrieving information 
advantage lsi representation query similar document share words 
latent semantic indexing lsi uses singular value decomposition svd technique closely related eigenvector decomposition factor analysis cullum model associative relationships 
large term document matrix decomposed set typically orthogonal factors original matrix approximated linear combination 
representing documents queries directly sets independent words lsi represents continuous values orthogonal indexing dimensions 
number factors dimensions smaller number unique terms words independent 
example terms similar contexts documents similar vectors lsi representation 
svd technique capture structure better simple document document correlations clusters 
lsi partially overcomes deficiencies assuming independence words provides way dealing synonymy automatically need manually constructed thesaurus 
lsi completely automatic method 
appendix provides brief overview mathematics underlying lsi svd method 
deerwester furnas additional mathematical details examples 
interpret analysis performed svd geometrically 
result svd vector representing location term document dimensional lsi representation 
location term vectors reflects correlations usage documents 
space cosine dot product vectors corresponds estimated similarity 
retrieval typically proceeds terms query identify point space documents ranked similarity query 
term document vectors represented space similarities combination terms documents easily obtained 
lsi method applied standard ir collections favorable results 
tokenization term weightings lsi method equaled outperformed standard vector methods variants case better cases deerwester 
standard vector method differential term weighting relevance feedback improve lsi performance substantially dumais 
lsi applied experiments relevance feedback dumais schmitt filtering applications foltz dumais 
system described gallant 
related lsi 
systems model relationships terms looking similarity contexts words exploit associations improve retrieval 
systems reduced dimension vector representation differ term document query vectors formed 

lsi trec trec conference opportunity scale tools explore lsi dimension reduction ideas rich corpus word usage 
pleased able existing lsi svd tools large collection 
see dumais details 
able compute svds docs words matrices numerical convergence problems standard dec sparc workstation 
limits size matrices handle divided trec documents separate subcollections ap doe fr wsj ziff ap fr wsj ziff worked 
theoretical reasons working subcollections sense 
topically coherent subcollections get better discrimination documents 
ziff subcollection analyzed separately example dimensions devoted differences computer related topics 
documents part large corpus fewer indexing dimensions devoted discriminating 
terms accuracy lsi performance trec average 
obvious problems initial pre processing documents omitted unanticipated problems combining subcollections 
top performing automatic systems smart preprocessing chose trec 
addition smart software purpose allows compare lsi comparable vector method examine contribution lsi se 
planned comparing lsi analysis subcollections trec lsi analysis entire collection trec 
high hopes able build trec trec 
practice changes pre processing algorithm decision single combined lsi analysis resulted starting scratch respects 
completed experiments trec get far liked especially adhoc topics 

lsi trec pre processing smart system pre processing documents queries 
markups delimiters removed hand indexed entries removed wsj ziff collections 
upper case characters translated lower case punctuation removed white spaces delimit terms 
smart list words smart stemmer modified lovins algorithm modification strip words endings 
phrases proper noun identification word sense disambiguation thesaurus syntactic semantic parsing spelling checking correction complex controlled vocabulary manual indexing 
result pre processing thought term document matrix cell entry indicates frequency term appears document 
entries term document matrix transformed ltc weighting 
ltc weighting takes log individual cell entries multiplies entry term row idf weight term normalizes document col length 
began processing documents cd cd 
minimal pre processing described unique tokens unique stems non zero entries term document matrix 
documents contained term 
decrease matrix size thought handle removed tokens occurring fewer documents 
resulted unique tokens unique stemmed words non zero entries 
resulting document term matrix starting point results reported 
ltc weights computed matrix 
svd analysis ltc matrix described input svd algorithm 
svd program takes ltc 
smart system version available smart group cornell university 
chris buckley especially generous consultations get software somewhat non standard things 
transformed term document matrix input calculates best reduced dimension approximation matrix 
result svd analysis reduced dimension vector term document vector singular values 
number dimensions experiments 
representation retrieval 
cosine term term document document term document vectors measure similarity 
able compute svd analysis full matrix described time include results 
runs submitted sample documents matrix 
appropriate documents sampled folded resulting reduced dimension lsi space 
cases weights matrix recompute samples 
routing experiments subset documents relevance judgements 
unique documents relevance judgements 
svd analysis computed relevant document term subset matrix containing non zero cells 
reduced dimensional approximation took hrs cpu time compute sparc workstation 
dimensional representation matching retrieval 
adhoc experiments took random sample documents 
reduced dimensional svd approximation computed document term matrix non zeros 
resulting reduced dimensional representation retrieval 
documents included sample folded dimension lsi space adhoc queries compared documents 
folded documents located weighted vector sum constituent terms 
vector new document computed term vectors terms document 
documents matrix derived vector corresponds exactly document vector svd 
new terms added analogous fashion 
vector new terms computed document vectors documents term appears 
adding documents terms manner assume derived semantic space fixed new items fit 
general space obtain new svd calculated original new documents 
previous experiments sampling scaling documents folding remaining documents resulted performance indistinguishable observed documents scaled 
scaling total corpus 
trec lsi analyses subcollections cd cd 
queries retrieval queries automatically processed way documents 
queries derived topic statement began full text topic topic fields stripped sgml field identifiers 
feedback queries full text relevant documents 
query vector new document case routing indicating frequency term appears query automatically generated topic 
query transformed smart ltc weighting 
note boolean connectors proximity operators query formulation 
implicit connectives ordinary vector methods fall ors ands additional kind fuzziness introduced dimension reduced association matrix representation terms documents 
terms query identify vector lsi space recall term vector representation space 
query simply located weighted vector sum constituent term vectors 
cosine query vector document vector computed documents ranked decreasing order similarity query 
fewer dimensions standard vector retrieval entries non zero inverted indices useful 
means query compared document 
dimensional vectors cosines computed minute sparc 
time includes comparison time ranking time assumes document vectors pre loaded memory 
adhoc queries time compare query documents minutes comparisons sequential 
straightforward split matching machines parallel hardware documents independent 
preliminary experiments pe maspar showed cosines computed sorted second 
important note step lsi analysis completely automatic involved human intervention 
documents automatically processed derive term document matrix 
matrix decomposed svd software resulting reduced dimension representation retrieval 
svd analysis somewhat costly terms time large collections need computed create reduced dimension database 
svd takes minutes sparc matrix time increases hours matrix 
trec routing experiments routing queries created filter profile training topics 
submitted results sets routing queries 
case filter just topic statements treated routing queries adhoc queries 
filter located vector sum terms topic 
call routing topic lsir results 
method training data representing topic adhoc query 
case information relevant documents training set 
filter case derived vector sum centroid relevant documents 
call routing lsir results 
average relevant documents topic range 
somewhat unusual variant relevance feedback replace combine original topic relevant documents terms appear nonrelevant documents 
extremes provide baselines compare methods combining information original query feedback relevant documents 
cases filter single vector 
new documents matched filter vector ranked decreasing order similarity 
new documents documents cd automatically processed described section 
important note terms cd cd training collection indexing documents 
new document located weighted vector sum constituent term vectors dimension lsi space just way queries handled 
new documents compared routing filter vectors cosine similarity measure dimensions 
best matching documents filter submitted nist evaluation 
results main results lsir lsir runs shown table 
runs differ profile vectors created weighted average words topic statement lsir routing topic weighted average relevant documents training collection cd cd lsir routing 
surprisingly lsir profile vectors take advantage known relevant documents better lsir profile vectors simply topic statement measures performance 
improvement average precision vs 
users get average additional relevant document top returned lsir method filtering 
table lsir lsir topic rel docs sum rel ret avg prec pr pr prec median table lsi routing results 
comparison topic words vs relevant documents routing filters 
compared trec systems lsi reasonably especially routing lsir run run discussed 
case lsir lsi median performance topics best score topics 
lsi performs average routing topic lsir run information training set forming routing vectors case course global term weights 
performed similar comparisons query vectors representing words queries centroid relevant documents standard ir test collections med cisi cranfield cacm time 
cases average improvement query replaced centroid relevant documents 
improvement top relevant documents just relevant document 
smaller advantages observed trec partially due statistical artifacts partially trec topics richer need statements usual ir queries 
examined topic profiles trec 
somewhat surprisingly query just topic terms accurate query relevant documents training 
attributable small number inaccuracy relevance judgements initial training set trec 
substantial impact performance topics queries relevant articles ignored original topic description 
lsir lsir runs provide baselines various combinations query information relevant document information measured 
tried simple combination lsir lsir profile vectors components equal weight 
took sum lsir lsir profile vectors topics profile vector 
results analysis shown third column table labeled 
combination somewhat better centroid relevant documents total number relevant documents returned average precision 
returned fewer documents topics documents returned method judged relevance suspect performance improved bit 
topics better maximum methods 
worse best method 
appears combination takes advantage best methods 
method combines query vector vector representing centroid relevant documents kind relevance feedback 
unusual variant relevance feedback words relevant documents words nonrelevant documents weighted query terms re weighted 
interestingly method appears produce improvements comparable obtained buckley allan salton traditional relevance feedback methods 
average precision method better lsir topic words vs quite similar improvement reported buckley allan salton richest routing query expansion method 
lsir method generally better lsir method substantial variability topics 
topics largest differences generally cosine lsir lsir topic vectors smallest 
cosines corresponding topic vectors range 
lsir method substantially better topics foreign military groups movement people country criminal actions officers failed financial institution crime aid computer production fiber optics equipment 
topics lsir substantially better lsir machine translation system information retrieval system actions corrupt public officials computer application crime solving 
entirely clear distinguishes topics especially topics example 
time look detail failures lsi system 
examine misses false alarms detail 
preliminary examination topics suggests lack specificity main reason false alarms highly ranked irrelevant documents 
surprising lsi designed method added tools easy 
examine query splitting ideas 
previously conducted experiments suggest performance improved filter represented separate vectors 
method trec results submitted 
see foltz dumais discussion multi point interest profiles lsi 
trec adhoc experiments submitted sets adhoc queries 
intended compare new smart pre processing single lsi space old trec pre processing separate subcollection spaces 
unfortunately serious errors translation internal document numbers labels documents lsi scaling mislabeled run results incomplete misleading 
corrected translation problem correct results labeled 
results summarized table 
completed comparison separate subspaces trec 
table error correct rel ret avg prec pr pr prec median table lsi adhoc results 
comparison standard vector method lsi corrected version missing relevance judgements 
terms absolute levels performance average 
smart results somewhat worse trec smart results reported buckley fuhr voorhees slightly different pre processing options include phrases 
generally difficult compare systems smart lsi runs meaningfully compared pre processing 
starting term document matrix cases 
disappointment reduced dimension lsi performance appears somewhat worse comparable smart vector method 
important realize documents returned judged relevance submitted official run 
table shows number documents judgements 
consider results just top documents query documents judged nist assessors 
documents judged official run relevant 
documents judged relevant 
documents relevant lsi performance comparable smart performance relevant lsi performance somewhat better 
similarly top documents documents relevance judgements 
table top top top top relevant relevant judged table summary missing relevance judgements standard vector method lsi 
missing relevance judgements direct comparisons smart lsi difficult decided look performance just documents relevance judgements 
looked performance considering just unique documents adhoc relevance judgements 
results shown table 
table rel ret avg prec pr pr prec table lsi adhoc results 
comparison standard vector method lsi documents relevance judgements available 
striking aspect results higher levels performance 
expected considering documents relevance judgements fewer documents official results 
considering subset documents small advantage lsi compared smart vector method 
taken results just top documents results suggest lsi outperform straightforward vector method 
somewhat disappointed relatively small difference lsi comparable vector method trec environment consistently observed larger advantages previously 
reason trec topics richer detailed descriptions searchers needs typical ir requests 
average trec query unique words specific names 
lsi primarily recall enhancing method little effect rich queries 
groups tried various kinds query expansion recall enhancing method reached see voorhees 
tried analysis new summary field topic 
field shorter topic statement quite similar description field covers relevance assessments 
results summarized table 
expected performance lower complete topics 
interestingly difference lsi standard vector method larger average precision 
somewhat smaller advantage seen previous experiments smaller test collections summary queries average unique words 
table rel ret avg prec pr pr prec table lsi adhoc results summary topics 
comparison standard vector method lsi documents relevance judgements available summary field 
reduced dimension lsi vector retrieval method offer performance advantages compared standard vector method large trec collection 
advantages larger shorter queries expected 
exact nature advantage documents retrieved lsi standard vector method needs examined detail 

improving performance improving performance speed lsi svd system built research prototype investigate different information retrieval interface issues 
retrieval efficiency central concern wanted assess method worked worrying efficiency initial applications lsi involved smaller databases documents 
effort went re designing tools efficiently large trec databases 
svd svd algorithms get faster time 
sparse iterative algorithm times faster method initially berry 
usual speed memory tradeoffs svd algorithms time probably decreased different algorithm memory 
parallel algorithms help little probably factor 
calculations done double precision time memory decreased single precision computations 
preliminary experiments smaller ir test collections suggest decrease precision lead numerical problems svd algorithm 
important note preprocessing svd analyses time costs relatively stable domains 
retrieval query vectors compared document 
process linear number documents database quite slow large databases 
practical efficient algorithms finding nearest neighbors dimensional spaces methods speed retrieval 
document clustering reduce number comparisons accuracy probably suffer 
explored heuristics clustering particularly effective high levels accuracy maintained 
hnc system gallant uses approach reduce number alternative documents matched 
initial keyword match eliminate documents calculate reduced dimension vector scores subset documents meeting initial keyword restriction 
may reasonable alternative long queries trec believe recall reduced short queries 
addition data structures need maintained 
query matching improved tremendously simply machine parallel hardware 
pe maspar attempt optimize data storage sorting decreased time required match dimensional query vector document vectors sort factor 
improving performance accuracy begun look large number parametric variations improve lsi performance 
important variable lsi retrieval number dimensions reduced dimension space 
previous experiments performance improves number dimensions increased dimensions decreases slowly level observed standard vector method dumais 
examined trec performance fewer dimensions reported routing queries adhoc queries consistently worse performance 
looks improve performance simply increasing number dimensions 
unfortunately requires rerunning svd 
noticed adhoc queries contained 
lsi boolean logic represents query vector sum constituent terms thought removing information help 
modified topic statements hand remove negated phrases 
performance improved 
need experiment different term weighting methods 
routing adhoc experiments smart ltc weighting corpus documents queries 
buckley salton trec suggests alternative weightings may effective large trec document collection 
reweighting query vectors easy 
reweighting document collection difficult changes term document matrix new svd required 
routing queries try alternative methods combining information original query relevant documents take better advantage training data available 
expect term re weighting negative information weighting terms non relevant documents improve performance 
order better understand retrieval performance begun examine kinds retrieval failures false alarms misses 
false alarms documents lsi ranks highly judged irrelevant 
misses relevant documents top returned lsi 
false alarms 
common reason false alarms lack specificity 
highly ranked irrelevant articles generally topic interest meet restrictions described topic statement 
topics required kind detailed processing fact finding lsi system designed address 
precision lsi matching increased standard techniques proper noun identification syntactic phrases pass approach involving standard initial global matching followed detailed analysis top documents 
buckley salton smart global local matching evans 
clarit evoke discriminate strategy nelson conquest global match followed locality information krupka rau ge pre filter followed variety stringent tests pass approaches advantage trec trec 
try methods focus general purpose completely automatic methods modified new domain query restriction 
possible reason false alarms appears result inappropriate query pre processing 
negation best example problem 
adhoc queries contain negation topic statement 
preliminary experiments described briefly small improvement performance negated information manually removed topics 
example inappropriate query processing involved logical connectives 
lsi handle boolean combinations words returned articles covering subset anded topics 
aspect query appears dominate typically described terms high weights 
limiting contribution term similarity score help problem 
clear false alarms returned lsi 
lsi uses statistically derived semantic space surface level word overlap matching queries documents difficult understand particular document returned 
advantage lsi method documents match queries words common produce spurious hits 
reason false alarms inappropriate word sense disambiguation 
lsi queries located weighted vector sum words words disambiguated extent query words 
similarly initial svd analysis context words articles determine location word lsi space 
word location appears middle 
related possibility concerns long articles 
lengthy articles talk distinct subtopics averaged single document vector produce spurious matches 
breaking larger documents smaller subsections matching help 
misses 
analysis examine random subset relevant articles top returned lsi 
relevant articles fairly highly ranked lsi notable failures seen persistent readers 
far systematically distinguished misses list 
misses examined represent articles primarily different topic query contained small section relevant query 
documents located average terms lsi space generally near dominant theme desirable feature lsi representation 
kind local matching help identifying central themes documents 
misses attributable poor text query pre processing tokenization 
open issues basis preliminary failure analyses exploring precision enhancing methods 
explore additional areas 
separate vs combined scaling separate trec experiments 
trec single scaling small sample 
finished complete scaling compare subcollection scalings sampled full scaling 
centroid query vs separate points interest single vector represent query 
cases vector average terms topic statement cases vector average previously identified relevant documents 
single query vector inappropriate interests facets near lsi space 
developed techniques allow match controllable compromise averaged separate vectors kane 
case routing queries example match new documents previously identified relevant documents separately average 
interactive interfaces lsi evaluations conducted noninteractive system essentially batch mode 
known underlying retrieval matching engine achieve different retrieval success different interfaces 
examine performance real users interactive interfaces 
number interface features help users faster accurate relevance judgements help explicitly reformulate queries 
see dumais schmitt preliminary results query reformulation relevance feedback 
interesting possibility involves returning richer rank ordered list documents users 
example clustering graphical display top documents quite useful 
done preliminary experiments clustered return sets extend trec collections 
general idea provide people useful interactive tools knowledge skills attempting build database representation matching components system 

onward trec quite pleased able existing lsi svd tools trec trec collections 
important finding regard large sparse svd problems computed numerical convergence problems 
modified preprocessing substantially trec basic tools place able conduct experiments comparing various indexing query matching ideas underlying lsi engine 
bigger svds faster query matching improving precision interactive interface issues major areas targeted improvement 

berry large scale singular value computations 
international journal supercomputer applications 
buckley allan salton automatic routing ad hoc retrieval smart trec 
appear proceedings trec 
buckley salton automatic retrieval locality information smart 
harman ed 
text retrieval conference trec nist special publication 
cullum lanczos algorithms large symmetric eigenvalue computations vol theory chapter real rectangular matrices 
boston 
deerwester dumais landauer furnas harshman indexing latent semantic analysis 
journal society information science 
dumais improving retrieval information external sources 
behavior research methods instruments computers 
dumais lsi meets trec report 
harman ed 
text retrieval conference trec 
nist special publication 
dumais schmitt iterative searching online database 
proceedings human factors society th annual meeting 
evans grefenstette hersh clarit trec design experiments results 
harman ed 
text retrieval conference trec nist special publication 
foltz dumais personalized information delivery analysis information filtering methods 
communications acm dec 
furnas deerwester dumais landauer harshman streeter lochbaum information retrieval singular value decomposition model latent semantic structure 
proceedings sigir 
gallant hecht nielson caid qing carleton tipster panel hnc system 
harman ed 
text retrieval conference trec nist special publication 
jacobs krupka rau boolean approximation method query construction topic assignment trec 
harman ed 
text retrieval conference trec nist special publication 
kane streeter dumais casella relevance density method multi topic queries information retrieval 
proceedings rd symposium interface ed 
nelson site report text retrieval conference 
harman ed 
text retrieval conference trec nist special publication 
salton mcgill modern information retrieval 
mcgraw hill 
voorhees expanding query vectors lexically related words 
appear proceedings trec 

appendix latent semantic indexing lsi uses singular value decomposition svd technique closely related eigenvector decomposition factor analysis cullum 
take large term document matrix decompose set typically orthogonal factors original matrix approximated linear combination 
formally rectangular matrix example matrix terms documents decomposed product matrices orthonormal columns diagonal rank called singular value decomposition largest singular values kept corresponding columns matrices rest deleted yielding matrices resulting matrix unique matrix rank closest squares sense idea matrix containing independent linear components captures major associational structure matrix throws noise 
reduced model usually approximate term document association data number dimensions reduced model smaller number unique terms minor differences terminology ignored 
reduced model closeness documents determined pattern term usage documents near regardless precise words describe description depends kind consensus term meanings dampening effects polysemy 
particular means documents share words user query may near consistent major patterns word usage 
term semantic indexing describe method reduced svd representation captures major associative relationships terms documents 
interpret analysis performed svd geometrically 
result svd dimensional vector representing location term document dimensional representation 
location term vectors reflects correlations usage documents 
space cosine dot product vectors corresponds estimated similarity 
term document vectors represented space similarities combination terms documents easily obtained 
retrieval proceeds terms query identify point space documents ranked similarity query 
attempt interpret underlying dimensions factors rotate intuitively meaningful orientation 
analysis require able describe factors verbally merely able represent terms documents queries way escapes unreliability ambiguity redundancy individual terms descriptors 
choosing appropriate number dimensions lsi representation open research question 
ideally want value large fit real structure data small fit sampling error unimportant details 
dimensions method begins approximate standard vector methods loses power represent similarity words 
dimensions discrimination similar words documents 
find performance improves increases decreases dumais 
lsi typically works relatively small compared number unique terms number dimensions shows dimensions fact capturing major portion meaningful structure 
