prefetching markov predictors doug joseph ibm watson research lab box ibm 
watson research yorktown heights ny watson ibm com dirk grunwald department computer science university colorado boulder colorado grunwald cs colorado edu prefetching approach reducing latency memory operations modern computer systems 
describe markov prefetcher 
prefetcher acts interface chip chip cache added existing computer designs 
markov prefetcher distinguished prefetching multiple predictions memory subsystem prioritizing delivery processor 
design results prefetching system provides coverage accurate produces timely results effectively processor 
cycle level simulations markov prefetcher reduces execution stalls due memory operations average various commercial benchmarks thirds memory demand fetch cache organization 
processors normally fetch memory demand fetch model processor issues load instruction specified datum fetched memory 
datum cache request external memory system fetch datum 
comparison memory prefetching mechanism attempts provide data processor requests data 
assume data placed prefetch buffer accessed processor uses mechanism perform non binding prefetch avoids disturbing current cache contents 
important metrics compare memory prefetchers coverage accuracy timeliness 
coverage indicates fraction memory requests supplied prefetcher demand fetched 
accuracy indicates fraction prefetched cache lines offered processor 
prefetched memory provided processor needed processor may stall execution 
timeliness indicates data offered prefetcher arrives needed early data discarded 
ideal prefetcher large coverage large accuracy produces timely data prefetcher offers processor data needs data needs processor needs data 
believe prefetching mechanisms designed step process architect envisions model describing way programs behave accessing memory attempts construct physical realization model provides suitable prefetch coverage 
describe hardware prefetch mechanism offers better performance prefetch mechanisms 
memory level simulations find prefetching mechanism reduce memory overhead cycles instruction greatly reducing memory stalls encountered model processor 
devote mbyte memory prefetcher data structures prefetcher reduces memory stalls reducing total amount memory devoted combination prefetcher second level cache mbytes memory prefetcher outperforms demand fetch memory system uses mbytes memory 
describe memory access model assumed prefetcher prefetcher physically implemented 
briefly survey alternative prefetcher designs 
follow description experimental design analysis estimate performance prefetcher 
prefetcher design implementation hardware prefetching prediction process current prediction state prefetcher guesses memory may requests location memory subsystem 
number possible sources prediction information 
source address stream sequence addresses referenced processor 
prediction source requires prefetching hardware chip processor 
prefetcher need efficient may need analyze cycle 
prefetchers examine address stream prediction source 
external memory subsystem due level caches occur frequently memory 
shows schematic design prefetcher built external part memory subsystem 
ll see important prefetcher may need considerable state information effective 
shown assume processor chip prefetch buffers examined concurrently level caches 
prefetched data items displace data resident cache 
prefetched data contends normal demand fetched memory processor bandwidth 
prefetch mechanisms usually assume programs access memory particular pattern access model 
example stream buffers assume memory accessed linear stream possibly non unit stride 
access model determined architects design hardware mechanism capture approximate stream 
section describe access model assume hardware implementation captures access model 
cache cache processor main memory cache cache cache prefetcher cache prefetcher cache pf buffer cache pf buffer addr control addr control bytes system design sample address string 
letter indicates cache different memory location 
markov model representing previous string transition probabilities 
modeling memory markov processes assume stream approximated observed markov model 
assume stream shown 
example different memory locations identified different letters 
sequence indicates missing memory location followed 
string build markov model shown approximates string transition frequency 
transition node node diagram assigned weight representing fraction followed example node example sequence 
see pattern time pattern time pattern time 
example uses previous predict 
intuitively program execute issue memory markov model predict pred st pred nd pred rd pred th 
pred st pred nd pred rd pred th 
pred st pred nd pred rd pred th addr pred st pred nd pred rd pred th current address address prefetch request queue prefetch request 
prefetch request cache cpu mux cpu address request hardware approximate markov prediction prefetcher missing 
example re execution appearance may lead hardware predict missing issue prefetch requests address 
general history markov model history information example training sequence prefetcher predict sequence seen 
examined performance general history models little added benefit additional information focus history models 
realizing markov prefetcher hardware problems encountered assuming pure markov model memory 
practice programs don repeat exactly patterns execution transition probabilities learned execution may benefit 
furthermore difficult efficiently represent pure markov model hardware node may arbitrary degree transition probabilities represented real values 
lastly programs millions addresses may possible record single table 
despite drawbacks data show markov model memory effective prefetch mechanism 
need set design choices address problem representing markov transition diagram hardware 
describe design markov predictor justify decisions simulation studies 
decision continuously rebuild markov model program executing 
approximate markov model captures past activity programs system uses information predict 
limit number possible nodes transition diagram limit maximal degree 
concretely represent markov transition diagram table shown 
sample configuration state markov model occupies single line prediction table transitions states 
total size table determined memory available prefetcher 
current address matches index address prefetch table address prediction registers eligible issue prefetch subject mechanisms described intended improve prefetch accuracy 
possible prefetches result transfer cache 
prefetch request associated priority 
prefetch addresses stored prefetch request queue higher priority requests lower priority requests 
prefetch request queue contends processor cache demand fetches processor higher priority 
series prefetches prefetch request queue may full lower priority requests discarded 
fetch request satisfied cache placed chip prefetch buffers 
demand fetch requests directly stored cache 
model chip prefetch buffers entry fully associative fifo buffer 
processor queries entries associatively searched cycle 
match relocated head fifo entries head slot shifted 
fifo searched updated avoid duplicate entries 
duplicates adding entry empty slot filled empty slots slot entry replaced 
design similar stream buffer design farkas 
primary difference entire buffer shifted discarding entries matching 
parameters affect performance hardware configuration 
trace driven simulation memory level performance model determine importance parameters compare performance markov prefetcher previously suggested prefetchers 
describe prior prefetchers describe experimental design compare performance markov prefetchers previous designs 
show effect various parameters markov prefetcher implementation 
prior hardware software prefetching schemes devised effective structured workloads 
research prefetching unstructured workloads nearly common results area begun appear 
section correlation prefetching especially relevant markov prefetching evolution correlation prefetching 
static predictors static predictors rely compiler determine possible cache misses embed information code form prefetch instructions 
mowry show structured scientific codes amenable approach 
show techniques failed improve performance pointer intensive applications study 
terms hardware resources compiler schemes inexpensive implement 
prediction information embedded program compile time compiler schemes lack flexibility account dynamic behavior workload 
compiler techniques proposed insert prefetch instructions sites pointer dereferences anticipated 
lipasti developed heuristics consider pointers passed arguments procedure calls insert prefetches call sites data referenced pointers 
classify loads data address comes previous load list accesses perform code motions separate instructions data fetched list accesses 
stride prefetchers chen baer investigate mechanism prefetching data characterized regular strides 
scheme prediction table rpt look ahead program counter lpc 
rpt cache tagged instruction address load instructions 
entries rpt hold previous address referenced corresponding load instruction offset address previous data address referenced instruction flags 
load instruction executed matches entry rpt offset data address load previous data address stored rpt calculated 
matches offset stored table prefetch launched data address offset ahead current data address 
address stream index prediction table 
practice little performance difference addresses address stream 
simulations stride prefetchers address stream 
stream buffers jouppi introduced stream buffers significant methods improved direct mapped cache performance 
contrast stride prefetchers stream buffers designed prefetch sequential streams cache lines independent program context 
design jouppi unable detect streams containing non unit strides 
kessler palacharla extended stream buffer mechanism detect non unit strides having direct access program context 
introduced noise rejection scheme improving accuracy stream buffers 
farkas enhanced stream buffers providing associative lookup capability mechanism detecting eliminating allocation stream buffers duplicate streams 
compare performance design farkas markov prefetchers 
stream buffers prefetch fill buffer servicing cache misses 
prefetches placed stream buffer separate prefetch buffer 
stream buffers allocated cache misses 
stream buffer contains entry matches current address taken stream buffer entries removed shifted head buffer prefetches launched sequentially consecutive cache line addresses fill open bottom part stream buffer 
match stream buffer new stream buffer allocated new stream 
model employed empty buffer selected replacement 
noise rejection scheme introduced kessler employed allocation stream buffers research 
simple filtering mechanism waits consecutive misses sequential cache line addresses allocating stream buffer stream 
stride prefetchers stream buffers complement various ways 
stream buffers generally exhibit greater prefetch coverage stride prefetchers inaccurate allocation filters 
detecting non unit strides natural stride prefetchers providing non unit stride detection stream buffers difficult 
stream buffers tend efficient resources stride prefetchers 
example program fragment stride prefetcher consume resources stride detection table stream buffer allocated 
studies stride prefetcher series stream buffers works 
allow accurate stride prefetcher issue prefetch able allocate stream buffer 
combination provides better coverage mechanism generally accurate stream buffers accurate stride prefetcher 
indirect stream detectors describes hardware data prefetching scheme recursion occurs linked list traversals 
simulated design performance uniformly worse correlation prefetching 
correlation prefetching markov prefetching continuing evolution called correlation prefetching 
basic concept correlation prefetching introduced baer context paged virtual memory systems 
baer associated single prefetch address memory address referenced developed algorithms updating prefetch address observed patterns 
occurs associated prefetch address checked residence physical memory 
prefetch page resident paged 
pairing temporally related related events current address prefetch address essence correlation prefetching 
address pair referred parent key select child prefetch address 
instance correlation prefetching applied data prefetching patent application 
hardware cache hold parent child information 
innovation introduce incorporate information parent key 
suggest bits instruction causing bits data address referenced 
introduce confirmation mechanism activates new pairs data prefetched 
mechanism allocation filters introduced kessler improve accuracy stream buffers serves similar purpose 
reeves extend mechanism apply stream directly load store stream 
publish results scheme improved mechanism significant ways 
introduce greater lead time prefetching fifo history buffer 
entering parent child pairs pair cache ancestors older parent paired child entered pair cache 
results reported impact cpu stalls demonstrated prefetch lead time significantly improved expense lower prefetch accuracy 
contribution study various alternate structures parent key 
study focused primarily different combinations bits instruction data addresses 
general marginal improvement prefetch accuracy coverage attempts 
important contribution reeves show stride prefetching combined correlation prefetching provide significant improvements prefetch coverage approach certain benchmarks 
scheme stride prefetcher placed front correlation prefetcher 
stride prefetcher prediction associated stride prediction filtered stream correlation prefetcher 
coverage improved reasons 
correlation prefetchers markov prefetchers see repeat predict 
stride prefetchers limitation 
better utilization pair cache achievable stride filtered 
workloads find insufficient stride applications examined scheme offer improvement prefetch coverage 
case workloads clear approach advantageous 
alexander kedem proposed mechanism similar correlation prefetching distributed prediction table 
variation correlation table predict bit line accesses enhanced dram prefetch individual bit lines dram sram array 
experimental design simulation study important factors influence performance prefetching scheme coverage accuracy timeliness 
prefetch coverage accuracy dependent specific memory system configuration timeliness depends greatly memory penalties particular system 
study single metric fraction level cache misses characterize coverage accuracy 
application record number demand cache misses encountered prefetching normalize values 
measure normalize number cache misses different prefetching schemes 
define coverage fraction satisfied prefetch mechanism 
prefetch mechanism fetch data memory simple mechanism may mispredicted fetches 
record additional relative normalized demand fetch define measure inaccuracy prefetcher 
measured timeliness simulating non speculative processor detailed memory model comparing memory cycles instruction mcpi 
represents average number cpu stalls attributed memory subsystem 
simulation studies cache configurations described 
assume non memory instructions execute cycle 
model processor single cycle chip kb data cache kb instruction cache 
cache entry single cycle victim buffers byte lines 
data cache single cycle entry write buffer uses write policy 
second level cache multi cycle multi bank direct mapped lockup free mb caches byte lines 
data cache uses write back write allocate policy entry address request queue bank 
model synchronous sram cache banks baseline model 
address data busses latency cycles 
combined cycle memory latency cycles return data processor total cache penalty twelve cycles new requests pipelined cycles 
cache bank separate address bus cache bank just data bus shared banks 
address bus contention may considerable contention data bus 
bus bandwidth bytes cycle 
multi cycle multi memory model entry address request queue bank memory banks 
address data busses latency cycles 
access latency bank cycles bus bandwidth bytes cycle 
simulating stream buffers entry stream buffers associative lookup non overlapping stream allocation allocation filters 
stream buffer single cycle access 
simulating stride prefetchers stride prefetcher entry fully associative stride detection table 
access single cycle 
experimented larger stride tables way set associative tables kbytes storage 
advantage tables larger kbytes traces considered little difference way associative table larger kbytes entry fully associative table 
earlier stream buffers farkas entry stream buffers 
small noticable improvement coverage point 
varied number entries stream buffer reached stated entries prefetches insufficient lead time entries accuracy begins fall rapidly 
correlation markov prefetchers combined second level cache 
modeling correlation markov prefetchers bounded constant amount storage combined prefetch cache subsystem possible continuing direct mapped caches 
configured markov correlation prefetchers memory corresponding demand fetch cache stride stream prefetchers 
markov correlation prefetchers mbyte data prefetch table mbyte data cache 
demand fetch model mbyte data cache 
words prefetch implementations require thirds memory stride stream prefetchers implemented 
data cache mb instruction cache mbyte prefetch table prefetch configurations instruction cache demand fetch model 
benchmark applications deficiency widely benchmarks simulation studies spec programs suite 
past research indicated operating system activity multi programming significantly effect cache performance 
little reported impact factors prefetching strategies 
time cache performance workloads tends significantly worse workloads making need latency reducing methods prefetching important 
important technical commercial applications give rise unstructured workloads 
technical applications involving large sparse arrays data store data compressed format access data indirection arrays 
usually organization sparse arrays known till run time may evolve execution 
common source unstructured access patterns technical commercial workloads pointer connected structures 
large graphs trees structures dynamically generated may evolve execution 
algorithms application may jump part tree graph 
consequently pointers accurate indicators access patterns 
unstructured technical workloads include important examples event driven simulators wire routing tools vlsi design unstructured grid algorithms computational fluid dynamics modeling molecular dynamics dram device level simulation structural dynamics analysis 
commercial environments tend unstructured high process switch rates high random rates typically involve large number user processes 
transaction processing utilize searching sorting algorithms give rise unstructured access patterns 
examples commercial workloads include transaction processing multi user software development environments network file server kernels desktop publishing tools compilers 
simulations address traces technical commercial industry standard benchmarks 
captured ibm rs running aix proprietary tracing tool developed ibm 
cache performance characteristics traces maynard 
traces include instruction data obtained execution multiple processes containing kernel user shared library activity 
traces generated unstructured technical codes commercially oriented workloads 
table provides brief summary 
information benchmarks 
table shows statistics indicative impact differences 
columns show percentage instructions branches percentage taken branches 
analysis branching behavior helps explain reasons cache rates tend higher commercial workloads technical workloads 
typical technical workloads dominated short medium length loops 
workload branch instructions return control head loop percentage taken branches higher 
longest instruction loops fit cache cache rate low 
contrast percentage taken branches commercial workloads relatively low indicating workloads execute relatively iterations loop 
lack dominant loops commercial workloads lower probability re executing instructions leading higher rates 
note spice anomalous trend low rate 
spec spec benchmarks instruction working set spice small fits comfortably cache 
average sequential block size shows nature commercial workloads 
block sizes commercial workloads tend shorter technical codes 
column shows fraction total number instructions executed operating system 
numbers indicate commercial workloads done operating system 
reason relatively high usage frequent movement small amounts data different levels system arithmetic operations data 
technical workloads operating system brings data application space application performs extensive arithmetic manipulation handing back operating system store 
table provides summary number instruction data trace rates obtained included information appendix simplify review happy include information final version space permits 
benchmark description sdet multi user software development environment spec sdm benchmark suite 
laddis nfs file server basis spec system level file server sfs benchmark suite 
netperf tcp ip benchmark system communication performance 
transaction processing performance council benchmark users connected client server configurations data base server traced 
structural dynamics analysis tool computation chemistry code gaussian mm local weather model noaa spice electronic circuit simulation spec kernel activity traced 
table workloads focus benchmark branches avg seq instrs branches taken block size os sdet laddis netperf mm spice table workload characteristics program refs refs mil 
mil sdet laddis netperf mm spice table kb cache counts rates predicted predicted predicted number prefetch predictors la net tc aba mm spi sdet laddis netperf mm spice changes prefetch accuracy coverage markov prefetcher number prefetch addresses increased 
bar shows normalized percentage data cache misses prefetch address predictors 
memory subsystem described 
performance comparison physical parameters influencing performance markov predictor specified simulation environment 
important parameters simplify implementation pure markov model 
node state pure markov model arbitrary fan outgoing edge transition probability indicates likelihood moving particular state 
transition probability prioritize memory prefetches 
choose limit fanout state prediction table approximate transition probabilities lru mechanism 
shows effect varying maximal fanout prefetch address predictors 
graph shows prediction accuracy coverage single axis 
vertical axis percentage cache misses normalized application cache organization demand fetch organization 
bars indicating different configurations considered shown application 
bar components 
lower component represents coverage fraction prefetched processor 
larger values better exceed normalized 
middle component represents fraction satisfied prefetch demand fetched 
upper component represents fraction incorrectly predicted result wasted bandwidth 
smaller values better component 
upper component indicates accuracy prefetcher accurate prefetcher fetch fewer 
clearly accuracy decreases coverage increases prefetch address predictors added predicted address fetched fetched matching parent key prefetch table 
larger number prefetch addresses results considerably decreased accuracy little improvement coverage 
example benchmark simulated prefetcher fetches twice wasted cache lines prefetch predictors increases coverage 
applications ex benchmark address predictor sdet laddis netperf mm spice table frequency data cache prediction address entry different applications benchmark address predictor sdet laddis netperf mm spice table lead time data cache prediction address entry different applications 
prefetch predictors provided reasonable balance coverage accuracy data cache configuration remainder 
prefetchers needed instruction cache fewer successors instruction 
performance markov prefetcher dependent size prefetcher tables investigated tiered allocation strategy records initially allocated table prefetch address predictors 
addresses needed entry moved separate entry table 
report results design afforded better performance table size expense slightly complex design 
mentioned predicted prefetch address fetched parent key prefetcher 
improve performance individual prioritized likelihood satisfying request 
considered methods prioritize 
true transition probability second simple lru algorithm 
lru algorithm performed true transition probability application easier implement 
table shows frequency predictor address markov prefetcher prediction addresses 
columns represent relative age predictor predictor address predictor 
example correctly prefetched sdet application predicted predictor address 
large difference frequency second predictors indicates lru policy effective 
lru prioritization orders data improve lead time time data needed model memory simulator 
table shows average number cycles time prefetch issued prefetch requests 
larger values indicate longer lead fraction cache misses mispredicted predicted predicted sdet laddis netperf mm spice simulation study comparing prefetcher accuracy 
left right column shows performance stream prefetching stride prefetching correlation prefetching markov prefetching stride stream markov parallel stride stream markov series 
time meaning time provide data processor 
case lead time increases column column 
indicates lru prioritization method requests frequently needed items requests needed 
compared performance markov prefetcher prefetch addresses lru prioritization policy prefetch mechanisms 
shows normalized memory transfers different methods 
left right column shows performance stream prefetching stride prefetching correlation prefetching markov prefetching stride stream markov parallel stride stream markov series 
stride stream correlation prefetchers simulated described 
shows stream prefetchers provide better coverage stride prefetchers considerably bandwidth 
correlation prefetcher column provides better coverage markov prefetcher column provides best coverage set applications 
stream buffers increase coverage comes price increased memory bandwidth demands fold stream buffers 
columns indicate alternative designs attempt improve coverage improve accuracy 
fifth column combination stride stream markov prefetchers parallel 
typically results large increase mispredicted waste bandwidth modest improvement coverage 
times coverage decreases 
predictors series coverage improves smaller amount 
likewise mispredictions increase wasting bandwidth 
part occurs handled stream stride mechanisms train markov predictor misses 
important factor parallel configuration interference contention prefetch buffers 
example stream buffer markov prefetcher may predict specific different address 
differing contend small number prefetch buffers limited memory bandwidth 
memory overhead different bandwidth reduction techniques 
vertical axis shows fraction words transfered cache lines 
group program bar demand fetching prefetched followed markov prefetching markov prefetching noise rejection accuracy adaptivity cna adaptivity 
bar shows memory cache queried prior prefetching 
limiting prefetch bandwidth general increased need bandwidth provides greatest limitation effective prefetcher implementations 
additional demand increases contention processor bus possibly blocking demand fetches prefetched requests 
examined techniques improve accuracy markov prefetcher reduce bandwidth consumed prefetching 
shows performance techniques variant previous presentations 
show fraction subblock cache misses sub block byte word 
group program bar demand fetching prefetching 
indicates bandwidth demand fetched wasted part data stored byte cache line 
second bar shows information markov prefetching addresses predictors lru prioritization policy 
third bar shows bandwidth reduction occurs simple noise rejection filter 
filter proposed examined reeves 
similar filter kessler improve accuracy stream buffers prefetch request dispatched prefetch pattern seen twice 
fourth column shows performance accuracy adaptivity 
scheme bit saturation counters added prediction address link back prediction address prefetch added prefetch buffer entry 
prefetch discarded prefetch buffer corresponding counter prediction address incremented 
prefetch prefetch buffer corresponding counter decremented 
sign bit counter set associated prefetch address disabled 
prefetch requests disabled prefetches placed pseudo prefetch buffer predictions compared actual prefetch requests allowing disabled prefetcher enabled 
fifth column shows effect automatic cache cna policy reduce portion cache line placed prefetch buffer 
design cna mechanism tyson 
table bit counters determine specific memory instruction cna policy 
prefetcher records information predicted address instruction predicted issue memory 
instruction marked cna requested word full cache line prefetched 
bar group shows effect querying cache prior prefetching 
number prefetch requests valid cache external prefetch mechanism may know external shadow tag table 
techniques querying cache implemented completely external subsystem 
require modifications prefetch mechanisms interaction prefetcher processor 
general cna adaptivity provides greatest performance improvement cna mechanism isn directly applicable instruction cache 
instruction tend cache line techniques accept reject entire cache lines accuracy adaptivity better cna method prefetches portion cache line 
shows cna mechanism changes number cache misses baseline programs mm spice 
cna mechanism improves basic cache performance mm decreases performance spice 
artifact cna bandwidth reduction technique 
comparison memory system simulator seen markov predictor provides better prefetch coverage prefetchers examined 
memory level simulator determine improved coverage resulted better performance memory subsystem 
memory simulator models contention system resources shown 
particular wanted insure additional bandwidth consumed markov prefetcher reduce memory performance 
shows memory cpi different prefetchers data cache shows memory cpi prefetchers instruction data caches 
case accuracy adaptivity bandwidth filter markov prefetcher 
correlation prefetcher uses filter suggested reeves stream buffers filter proposed kessler 
left right individual bars indicate performance stream stride correlation markov prefetchers 
segments bar indicate fraction mcpi attributed individual parts memory system 
bottom top segments indicates delay due fetching instructions cache fetching data cache fetching data cache additional delay incurred pure demand fetch model 
markov prefetcher provides best performance applications particularly applied instruction data caches 
recall correlation markov prefetchers require fewer resources stream stride buffers markov prefetcher mbyte cache mbyte prefetch table vs mbyte cache demand fetch model 
markov prefetcher simulated performance number significant applications 
markov prefetcher characterized extension correlation prefetcher previously described cycles instruction due memory system base sdet laddis netperf mm spice cycles instruction due memory system base sdet laddis netperf mm spice data data instruction average number cpu stalls attributed memory subsystem different prefetchers just data data instructions 
application vertical bars indicate mcpi demand fetch model 
left right individual bars indicate performance stream stride correlation markov prefetchers 
segments bar indicate fraction mcpi attributed individual parts memory system 
reeves number design issues markov prefetcher 
prefetcher able launch multiple prefetch requests prioritize prefetcher consider mechanism limit bandwidth devoted prefetching 
study shown simple effective realizable markov prefetcher built chip component memory subsystem 
design takes fewer resources normal demand fetch cache reduced memory cpi average applications examined 
markov prefetchers examined provide better prefetch coverage timely prefetches prefetchers expense reduce accuracy 
combination bandwidth reduction filters prioritization prefetch requests markov prefetcher effective prefetcher examined 
number ways markov prefetcher improved 
particular markov correlation prefetchers observe sequence prior predicting 
desirable pattern instances sequences 
likewise useful mechanism issue prefetch requests patterns sequences occurred eliminating training period needed markov predictor 
alexander kedem 
distributed predictive cache design high performance memory system 
second international symposium high performance computer architecture feb 
baer 
dynamic improvements locality virtual memory systems 
ieee transactions software engineering march 
reeves 
generalized correlation hardware prefetching 
technical report ee cornell university feb 
chen 
modified approach data cache management 
proceedings th annual international symposium microarchitecture pages nov 
chen baer 
reducing memory latency nonblocking prefetching caches 
asplos pages oct 

characterization alpha axp tp spec workloads 
th annual international symposium computer architecture pages april 
farkas jouppi 
complexity performance tradeoffs non blocking loads 
th annual international symposium computer architecture pages april 
jouppi 
improving direct mapped cache performance addition small fully associative cache prefetch buffers 
th international symposium computer architecture may 
levy 
architecture software controlled data prefetching 
th international symposium computer architecture may 
lipasti software prefetching pointer call intensive 
proceedings th annual international symposium microarchitecture pages nov 
maynard donnelly 
contrasting characteristics cache performance technical multi user commercial workloads 
vi apr 

examination memory access classification scheme pointer intensive numeric programs 
technical report tech report university illinois dec 
mowry lam gupta 
design evaluation compiler algorithm prefetching 
asplos pages oct 
cache heuristics preloading techniques general purpose programs 
proceedings th annual international symposium microarchitecture pages nov 
palacharla kessler 
evaluating stream buffers secondary cache replacement 
th annual international symposium computer architecture pages april 
prefetching system cache having second directory sequentially accessed blocks 
technical report patent office feb 
zhang torrellas 
speeding irregular applications shared memory multiprocessors memory binding group prefetching 
th annual international symposium computer architecture pages june 
benchmark descriptions system evaluation spec sdm suite includes benchmarks sdet 
sdet intended represent general purpose computing environment 
large number users run concurrently commands separated little think time 
commands executed users include edit compile bind ksh grep ls cp rm mkdir 
benchmark run multiple user levels show throughput function number users 
benchmark traced consists users includes kernel user shared library activity 
laddis benchmark measures capacity file server running nfs 
essentially re packaging benchmark created systems 
runs having client workstations submit nfs requests server 
requests completed average response time milliseconds 
benchmark run single client load transactions second processes submitting requests 
netperf benchmark measures performance system communications 
important mimics interactive network environments telnet response time critical 
currently implemented netperf provides simple representation interactive workloads data 
benchmark traced consisted single machine sending packets self 
scenario single byte packet ping processes times 
metric latency transfers elapsed time 
transaction processing performance council tpc benchmarks best known frequently cited measures commercial performance 
tpc collection representatives system database vendors 
time writing tpc released benchmarks called tpc tpc tpc rules running reporting results 
simulations thesis research executed benchmarks selected test case tpc 
tpc simulates banking application users submit simple debit credit transactions automated teller machines 
system expected respond seconds 
number users size database scaled reported performance metric transactions second tps 
example system reports tps rating database roughly times large system rating 
benchmark thesis traced users client server configuration 
structural dynamics analysis product 
particular application set timing runs provided 
described large linear analysis consisting mesh shell elements type 
total number degrees freedom maximum wavefront rms wavefront 
time spent inner loop solver computers fast vector processors run problem quickly 
computational chemistry application gaussian application test suite gaussian runtime test minutes consists processes running concurrently tracing effort concentrated user mode top processes exe exe 
processes accounted total runtime total user mode runtime 
mm local weather model owned noaa national oceanic atmospheric administration 
uses navier stokes equations solve finite element grid 
spice floating point intensive electronic circuit simulation benchmark spec suite 
user activity traced benchmark 
