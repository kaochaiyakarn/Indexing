appear proceedings icslp sydney better integration semantic predictors statistical language modeling noah daniel jurafsky department computer science department linguistics university colorado boulder noah jurafsky colorado edu introduce number techniques designed help integrate semantic knowledge gram language models automatic speech recognition 
techniques allow integrate latent semantic analysis lsa word similarity algorithm word occurrence information gram models 
lsa predicting content words coherent rest text bad predictor frequent words low dynamic range inaccurate combined linearly grams 
show modifying dynamic range applying word confidence metric geometric linear combinations grams produces robust language model lower perplexity wall street journal testset baseline gram model 

lot augmenting gram language models information sources longer distance syntactic semantic constraints 
previous suggested latent semantic analysis lsa model semantic knowledge applied asr 
lsa model word semantic similarity word occurrence tendencies successful ir nlp applications spelling correction 
lsa predicting presence words domain text predicting exact location 
gram model complements lsa model filling missing information exactly content words go 
discovered lsa bad predictor word reasons 
lsa predicting words closely tied semantic domain 
second lsa produces cosines tend narrow dynamic range trivial map cosine distances probability estimation 
furthermore optimal combination lsa gram probabilities difficult unsolved problem 
solution problems involves techniques combine estimators confidence metric lsa modification dynamic range lsa probabilities conservative evidence combination function 
algorithm achieves significant reduction perplexity wall street journal corpus 

lsa language model lsa vector model model semantics word occurrences 
words tend occur similar words considered semantically similar 
lsa algorithm trained corpus documents 
documents semantically cohesive set words paragraphs articles newspapers newsgroup articles structure documents maintained referred bag words model 
build lsa model experiments articles wall street journal years containing total word tokens 
vocabulary set wsj distribution vocabulary set 
term document matrix created rows corresponding words vocabulary columns documents 
entry matrix weighted frequency corresponding term corresponding document 
weighting chosen reduce influence frequently occurring terms function words described detail section 
step reduce large sparse matrix compressed matrix singular value decomposition svd 
original matrix decomposed reduced rank term matrix diagonal matrix singular values document matrix decreasing number dimensions retained reduces accuracy recreated component matrixes importantly reduces noise original matrix 
chose value experiments empirical results ir find value 
task interested term matrix row vector representation semantics particular word dimensional space 
compare semantic distance words looking cosine angle normalized dot product corresponding rows vectors matrix shows closest farthest words word fishing lsa 
furthermore compare single word set words newspaper article arbitrary context speech recognition system 
vectors word set combined form vector represents centroid set point dimensional semantic arizona taxi boat maine salmon fish fishing sample words angular distances fishing mapped space plotted angular distances words word fishing mapped 
closest words fishing fish farthest words taxi shown 
states roughly ordered terms importance fishing industry 
space 
distance vector word centroid computed yielding semantic distance word document 
comparison get measure semantic distance hypothesis word comes word previous article currently recognized 

deriving lsa probabilities previous section showed lsa predictive powers subsequent words semantically cohesive text newspaper article 
show convert probability estimate relatively little computation 
simplest method distance directly normalize word probability probabilities sum see equation 
approach simplistic performs poorly probabilities generated limited dynamic range smaller grams 
probabilities derived hover differing impact 
technique overcomes problem 
increase dynamic range lsa probabilities raising cosines power normalizing 
selected appropriate power empirical testing best held dev test set 
context implementation defined words wsj article current word 
allows draw natural semantic coherence newspaper article allowing line predictability word 
centroid vectors corresponding words context computed cosine computed lsa vector ith word document centroid vectors words context cos find smallest cosine context word ranges vocabulary items 
computed normalization purposes 
min cos estimate lsa probability computed cosine word context subtracting calibrate lowest value zero normalizing sum cosines words vocabulary context cos cos probability reestimated raising power renormalizing 
introducing greatly improved results 
baseline perplexity computed solely bigram model held development test set 
adding lsa model power factor showed small decrease performance 
decreased perplexity improvement 
see table details experiment 

lsa confidence metric problem lsa compared gram poor predictor function words common words uniform distribution contexts 
lsa predicting presence content words specific context occurred document soft soft drink context wsj story coca cola 
introduce confidence metric associated word helps determine degree lsa model effective predicting word 
confidence metric global term weighting useful ir applications entropy frequency word documents training corpus 
lsa confidence term calculated lsa confidence log log number documents corpus likelihood document term occurs count term document count term corpus lsa lsa word grm lsa conf 
word grm lsa conf coca investigation cola enterprises alleged incorporated said violations atlanta coca soft cola drink industry unit federal grand target jury atlanta table sample perplexities assigned lsa bigram models lsa confidences sentence wsj bold faced values indicate places lsa better predictor correct word gram model 
lsa confidence ranges low 
words promiscuous occurring documents regard content high entropy uniform distribution get low lsa confidence value words promiscuous usually occurring family words get higher confidence value 
table see lsa incorrectly assigns low probability high perplexity words lower ngram model 
lsa outperforms bigram model see term predicted relatively high lsa confidence 
section show lsa confidence measure discount lsa probability combining gram predictor 
words context lsa uses base judgment nearly discriminative previous word gram model uses predict words 
see lsa confidence model gave lower perplexity wsj development set baseline bigram model 
table shows perplexity computed variable confidence improvement bigram baseline 
suspected part improvement lsa confidence model due general discounting lsa probability word effect 
ran experiment test factors 
computed average lsa confidence terms development test set constant factor weight 
factor held 
perplexity test demonstrating variable lsa confidence improve results 

combining lsa grams gram model predictor words want guarantee contributes half probability mass predicting word 
divide lsa confidence half ranges 
words lsa confident predicting lsa confidence lsa gram models equally considered 
lsa confidence address problem combining modified lsa estimator gram probability 
simple linear combination inadequate equation partly lsa estimator predicts words syntactically disallowed 
need non linear combination function gives higher probability models agree predicted word syntactically semantically gives low probability estimator believes word 
chose geometric mean non linear combination function equation 
lsa confidence high function forces gram lsa model agree word order get high resulting probability 
lsa confidence low need agreement reduced 
consider case bigram coca atlanta bigram 
want lsa increase likelihood bigram just term atlanta semantically related rest context 
bigram model assigns low probability word sequence 
geometric mean case yield low probability arithmetic mean incorrectly give significant probability bigram due lsa influence 
equation shows sub optimal linear equation tested final equation includes lsa dynamic range adjustment lsa confidence geometric combination lsa gram probabilities shown earlier equations ith word recognized jth term vocabulary number words vocabulary word probability lsa word probability bigram model 
ngram probability geometric mean lambda lsa probability resulting probability ngram probability arithmetic mean lambda lsa probability resulting probability ngram probability geometric mean lambda lsa probability resulting probability ngram probability arithmetic mean lambda lsa probability resulting probability ngram probability geometric mean lambda lsa probability resulting probability ngram probability arithmetic mean lambda lsa probability resulting probability shown probability spaces combining lsa gram probabilities different values lsa confidence metric graphs demonstrate resulting probability space geometric arithmetic means 
consider point graphs lsa probability high gram probability nearly zero 
note geometric mean assigns case low probability arithmetic mean gives relatively high probability 
left geometric mean right arithmetic mean increasing lsa confidence goes 
testing arithmetic mean showed significantly worse results testing geometric mean yielding perplexity vs development test 
tested final model separately held test set 
trained words wsj tested words wsj years nab news corpus 
showed decrease test set perplexity lsa model going perplexity perplexity 
table shows result intermediate dev test results 

shown methods require relatively little computation significantly increase performance language model incorporates semantic information 
semantic confidence measure improved performance accurately predicting semantic model beneficial 
test method ppl bigram bigram lsa confidence geometric mean bigram lsa confidence arithmetic mean bigram lsa geometric mean bigram lsa confidence geometric mean held test set method ppl bigram bigram lsa confidence geometric mean table resulting perplexities development test set separately held test set 
ing influence metric low dynamic range proved important 
geometric combination evidence favors situations orthogonal models agree 
modification system add ngram confidence metric 
help cases lsa prediction wrong despite high lsa confidence word cola table 
currently investigating factor entropy gram distributions prefix 

acknowledgments tom landauer inspiring conversations led ideas apple computer funding internship started implementation andreas stolcke comments providing sri lm toolkit jim martin wayne ward useful feedback nsf nsf iri nsf iis second author 


bellegarda chow 
novel word clustering algorithm latent semantic analysis 
proceedings icassp 

jerome bellegarda 
latent semantic analysis framework large span language modeling 
eurospeech 

deerwester dumais furnas landauer harshman 
indexing latent semantic analysis 
journal american society information science 

susan 
dumais 
improving retrieval information external sources 
behavior methods instruments computers 

gotoh renals 
document space models latent semantic analysis 
eurospeech 

iyer 
improving predicting performance statistical language models sparse domains 
phd thesis boston university 

jones martin 
contextual spelling correction latent semantic analysis 
proceedings fifth conference applied natural language processing 

rosenfeld 
maximum entropy approach adaptive statistical language modeling 
computer speech language 
