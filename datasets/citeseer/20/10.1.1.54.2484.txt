process oriented evaluation step pedro domingos artificial intelligence group instituto superior lisbon portugal gia ist utl pt www gia ist utl pt methods avoid overfitting fall broad categories data oriented separate data validation penalizing complexity model 
limitations hard overcome 
argue fully adequate model evaluation possible search process models obtained taken account 
proposed method process oriented evaluation poe successfully applied rule induction domingos 
sake simplicity treatment artificial assumptions 
assumptions removed simple formula model evaluation obtained 
empirical trials show new better founded form poe accurate previous reducing theory sizes 
overfitting avoidance central problem machine learning statistics cheeseman 
learner sufficiently powerful representation search methods uses guard selecting model fits training data captures underlying phenomenon poorly 
current methods address fall broad categories 
data oriented evaluation uses separate data learn validate models includes methods cross validation breiman friedman olshen stone stone bootstrap efron tibshirani reduced error pruning brunk pazzani 
disadvantages computationally intensive reduces data available learning unreliable validation set small prone overfitting large number models compared ng 
representation oriented evaluation seeks avoid problems data training validation priori penalizing models 
bayesian approaches general fall category cheeseman mackay chickering heckerman 
representation oriented measures typically contain terms reflecting fit data penalizing model complexity akaike schwarz wallace boulton rissanen moody 
approach appropriate simpler models truly accurate ones mounting evidence typically case domingos schuurmans ungar foster lawrence giles tsoi webb schaffer murphy pazzani 
structural risk minimization vapnik shawe taylor bartlett williamson anthony scheffer joachims pac learning kearns vazirani methods seek bound difference training generalization error function model space effective dimension 
typically produces bounds overly broad requires severely restricting model space 
believe limitations representation oriented evaluation stem ignoring search process candidate models obtained 
learner unlimited model space avoid overfitting long attempts limited number models possible priori predict 
intuitively search performed obtain model higher expected generalization error training set error 
domingos intuition precise applied resulting formulas cn rule learner clark niblett obtaining systematic improvements model mean model structure parameter values 
generalization error theory size 
sake simplicity treatment domingos artificial assumptions error rates priori equally model generalization error roughly estimated treating previously generated models having similar generalization errors 
remove assumptions interpret result successfully apply cn 
process oriented evaluation suppose learner lm consists drawing hypotheses random independently model space returning lowest error training sample ith hypothesis generated lm true error rate ffl consists independently drawn examples number errors committed binomially distributed variable parameters ffl jn ffl jn ffl ffl em gamma ffl gammae jn ffl probability number errors greater jn ffl em ijn ffl notice notation opposite usual notation cumulative distribution function ffl gamma binomial cdf ffl 
convenient follows 
probability lm returning hypothesis hm misclassifies training examples probability hypotheses errors errors 
equivalently probability hypotheses gamma errors minus probability errors jn ffl em gamma jn ffl gamma em jn ffl ffl ffl ffl ffl 
bayes theorem ffl jn ffl jn ffl hypothesis lowest error chosen hypothesis learner lm returns 
goal predict hm true error rate ffl ffl purpose marginalize equation save ffl jn ffl mnc ffl jn ffl ffl mnc integral multiple components ffl save ffl expected value ffl computed integration ffl jn ffl ffl jn dffl ffl jn dffl ffl jn ffl dffl ffl em jn ffl dffl ffl ffl ffl jn ffl dffl ffl jn ffl dffl eb ffl ffl ffl em jn ffl dffl ffl em jn ffl dffl substituting equation assumption independent hypotheses assuming prior ffl hypotheses obtain expression previously assumed ffl ffl dropped prior ffl 
ffl jn gamma gamma ffl gamma gamma gamma gamma gamma eb ffl smallest ae equations 
binomial expansion obtain gamma mff gamma gamma gamma gamma gamma ff gamma gamma gamma substituting equation simplifying obtain ffl jn ffl gamma eb ffl ffl ml maximum likelihood estimate ffl sufficiently large ffl ffl ml equation behaved prior ffl long ffl neighborhood ffl 
ffl prior ffl ffl dffl prior expected value ffl suppose beta similarly bell shaped prior bernardo smith intuitive sense error rates 
general inflection point em jn ffl function ffl fall ffl prior peak prior tend zero hypotheses generated lowest error selected 
sufficiently large em jn ffl entire range ffl significantly greater zero leaving left tail distribution eb ffl ffl prior equation 
making substitutions obtain omitting indexes ffl ffl ffl jn ffl ml gamma ffl prior formula quite similar known laplace correction estimate cestnik 
role number hypotheses similar estimate role number examples 
estimate gradually changes maximum likelihood estimate prior number examples decreases similarly equation gradually uncovers prior number hypotheses generated increases 
intuitive meaning equation clear learner generates series hypotheses returns lowest training set error hypotheses generates sure observed error corresponds true error weight priori expected error 
result intuitively satisfying gives mathematical basis increasing model uncertainty amount search performed increases 
equation stands limited practical converges rapidly ffl prior independent hypotheses generated 
result earliest hypotheses error estimate ffl jn quite insensitive empirical error ffl ml effect partly due fact hypothesis dependences ignored result empirical error hypothesis carries information true error 
particular empirical error chosen hypothesis carries information true error resulting chosen hypothesis expected error prior priori possible situations minimum empirical error chosen hypothesis equation 
practical learners hand hypotheses generated typically strongly dependent 
general empirical errors observed carry information true error chosen hypothesis equation converge correspondingly slower prior term ffl prior propose model replacing equation slower growing function thought effective number independent hypotheses attempted 
example attempting hypotheses dependences may equivalent respect convergence equation ffl prior attempting independent hypotheses 
equation provides simple way combining data oriented representation oriented process oriented information estimating generalization error ffl ml data oriented component model empirical error ffl prior component function model form process oriented component function search process led model 
application rule induction rule induction systems employ set covering separate conquer search strategy michalski clark niblett 
rules induced time rule starts training set composed examples covered previous rules 
rule induced adding conditions time starting rule initially covers entire instance space 
condition add chosen attempting possible conditions 
conditions symbolic attributes typically form ij ij possible value attribute conditions numeric attributes typically form ij ij thresholds ij usually values attribute appear training set 
beam search process rule learners step best versions rule evaluation function selected specialization 
aq michalski continues adding conditions rule pure covers examples class 
lead severe overfitting 
latest version cn system clark niblett clark boswell uses simple effective bayesian method combat induction rule stops specialization improves error rate computed laplace correction estimate 
number examples covered rule number examples misclassifies conventional estimate rule error rate estimate ffl ffl rule priori error cn takes error obtained random guessing classes equally ffl gamma number classes 
prior value weight examples behavior equation equivalent having additional examples covered rule class 
cn uses conditions added rule covers fewer fewer examples ffl tends ffl rule making misclassifications may preferred covers examples causing induction earlier reducing overfitting 
clark boswell version cn accurate quinlan benchmark datasets testing 
scheme ignores conditions attempted probability finding appears reduce rule error merely chance increases 
lead estimate underestimate chosen condition true error cn overfit 
upward correction ffl increase number conditions attempted 
process oriented evaluation framework described previous section allows systematic way follows 
equation compare hypotheses returned learners lm choose lowest predicted error 
compare successive stages learner lm result continuing search learner lm gamma hypotheses 
particular successive stages successive versions rule returned cn similar separate conquer rule learner 
natural choice prior expected error ffl prior rule versions default error rate obtained predicting frequent class training set 
choice slower growing function obvious 
possibility log analogy decision tree induction 
learning tree algorithm cart breiman id quinlan quinlan new hypothesis obtained modifying previous fraction instance space fraction corresponding node currently expanded fraction exponentially smaller induction progresses 
entire new level decision tree corresponds entirely new hypothesis 
depth tree grows average logarithm number nodes take equivalent number independent hypotheses attempted proportional logarithm total number hypotheses attempted rule corresponds path decision tree content way induced system cn apply similar line reasoning number rules attempted 
hypothesis version rule attempted beam search 
equation need computed rule version generated beam search 
introduce preference adding conditions produce results domain knowledge supporting preferences 
equation computed round 
round consists generating possible step specialization rule version beam selecting best 
attributes maximum number values attribute worst case numeric attributes round corresponds rule versions 
total number rule versions generated including round round consists initial rule conditions 
induction stops ffl mk jn mk mk ffl mk gamma jn mk gamma mk gamma 
empirical study order test effectiveness process oriented evaluation default process oriented versions experiments described results sensitive base logarithms 
base base base yielded practically indistinguishable error rates theory sizes 
results reported base 
table empirical results error rates theory sizes default cn cn versions evaluation cn poe cn poe 
dataset error rate theory size cn cn poe cn poe cn cn poe cn poe breast sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma glass sigma sigma sigma sigma sigma sigma heartc sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma hepatitis sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma soybean sigma sigma sigma sigma sigma sigma thyroid sigma sigma sigma sigma sigma sigma tumor sigma sigma sigma sigma sigma sigma voting sigma sigma sigma sigma sigma sigma cn compared benchmark datasets previously clark boswell 
versions implemented adding necessary facilities cn source code 
details earlier version poe implementation domingos 
cn laplace estimates choose best specializations round 
preferable uncorrected estimates implemented poe preference hypotheses round factor avoiding overfitting 
laplace correction distorts value ffl ml equation 
particularly pronounced classes cn uses order minimize problem poe 
experimental procedure clark boswell followed 
dataset randomly divided training testing error rate theory size total number conditions measured default cn cn poe earlier version cn poe version described 
repeated times 
average results standard deviations shown table results cn cn poe domingos 
compared cn poe cn poe roughly main exception pole cart available uci repository blake keogh merz 
simply changing default cn change performance datasets 
differences cn results reported clark boswell 
may due fact default version cn uses beam size clark boswell 
distribution version cn may differ clark boswell 
tains accuracy lower error datasets higher lower error average reducing theory size datasets lower higher fewer conditions average 
indicates equation successfully deleting unnecessary conditions previous method retained 
closed form equation efficient evaluate integrals domingos 
results obviously preliminary 
version poe takes cn search process account detail currently developed 
plan apply datasets study behavior detail datasets synthetic ones 
related literature model selection error estimation large attempt review 
incompleteness representation oriented evaluation noted years ago pearl appropriate connect credibility nature selection procedure properties final product 
explicitly known simplicity merely serves rough indicator type processing took place prior discovery 
huber st cohen huber expresses need process oriented evaluation data analysis different example word processing batch programming correctness product checked inspecting path leading 
pieces previous take account number hypotheses compared considered early steps process oriented evaluation 
includes notably systems bonferroni corrections testing significance kass gaines jensen schmill see miller sax 
key difference systems proposed require somewhat arbitrary choice significance threshold directly attempts optimize goal expected generalization error 
bonferroni correction take hypothesis dependencies account framework offers principle way doing 
quinlan cameron jones layered search method automatically selecting cn beam width considered form process oriented evaluation 
layered search approach proposed similar aims biases differ layered search limits search width method limits length 
may effective reducing fragmentation small disjuncts problems pagallo haussler holte acker porter 
assumptions clearer implicit quinlan measure 
freund proposed form evaluation closer pac learning framework 
extension statistical query model kearns attempts obtain tighter bounds generalization error considering tree queries learner 
general algorithm obtain bounds exponential computational cost number queries freund proposes specialized version algorithms local search cn efficient price bounds 
tight bounds case open question empirical testing freund method carried far 
bounds model selection preferring model lowest upper bound parameters 
bonferroni corrections result general depend choice parameters clear criterion 
approach proposed directly obtains estimate generalization error useful confidence interval freund method may path 
evaluating models result search process just fitting parameters predetermined structure traditionally concern statisticians 
change chatfield 
arguments account number hypotheses attempted greater detail jensen cohen ng 
goes proposing method dependences hypotheses account proposing principled way combining search process information traditional representation factors 
main types model selection currently available 
data oriented evaluation hypothesis score depend form hypothesis performance data 
representation oriented evaluation score depends data hypothesis form search process led 
domingos argued ignored proposed process oriented evaluation poe 
domingos assumed models searched similar true error rates error rates equally priori 
removed assumptions derived simple approximation generalization error returned hypothesis function number hypotheses searched 
approximation weighted average maximum likelihood estimate error prior expected error increasingly favors prior models attempted 
approximation gives mathematical basis intuition model uncertainty increase amount search conducted 
plan study statistical properties equation particular sample size large approximate equation compare method proposed forms process oriented evaluation bonferroni corrections jensen schmill layered search quinlan cameron jones apply learners study methods accurately estimating growth effective number hypotheses learners 
akaike 

bayesian analysis minimum aic procedure 
annals institute statistical mathematics 
bernardo smith 

bayesian theory 
new york ny wiley 
blake keogh merz 

uci repository machine learning databases 
irvine ca department information computer science university california irvine 
www ics uci edu mlearn mlrepository html breiman friedman olshen stone 

classification regression trees 
belmont ca wadsworth 
brunk pazzani 

investigation noise tolerant relational concept learning algorithms 
proceedings eighth international workshop machine learning pp 
evanston il morgan kaufmann 
cestnik 

estimating probabilities crucial task machine learning 
proceedings ninth european conference artificial intelligence pp 

stockholm sweden pitman 
chatfield 

model uncertainty data mining statistical inference 
journal royal statistical society 
cheeseman 

finding probable model 
langley eds computational models scientific discovery theory formation pp 

san mateo ca morgan kaufmann 
cheeseman 

preface 
cheeseman eds selecting models data artificial intelligence statistics iv 
new york ny springer verlag 
chickering heckerman 

approximations marginal likelihood bayesian networks hidden variables 
machine learning 
clark boswell 

rule induction cn improvements 
proceedings sixth european working session learning pp 

porto portugal springerverlag 
clark niblett 

cn induction algorithm 
machine learning 
domingos 

bagging 
bayesian account implications 
proceedings third international conference knowledge discovery data mining pp 

newport beach ca aaai press 
domingos 

process oriented heuristic model selection 
proceedings fifteenth international conference machine learning pp 

madison wi morgan kaufmann 
domingos 

occam sharp blunt 
proceedings fourth international conference knowledge discovery data mining pp 

new york ny aaai press 
efron tibshirani 

bootstrap 
new york ny chapman hall 
freund 

self bounding learning algorithms 
proceedings eleventh annual conference computational learning theory 
madison wi morgan kaufmann 
gaines 

knowledge worth ton data 
proceedings sixth international workshop machine learning pp 

ithaca ny morgan kaufmann 


estimation probabilities essay modern bayesian methods 
cambridge ma mit press 
holte acker porter 

concept learning problem small disjuncts 
proceedings eleventh international joint conference artificial intelligence pp 

detroit mi morgan kaufmann 
huber 

languages statistics data analysis 
ostermann eds computational statistics papers collected occasion fifth conference statistical computing schloss 
heidelberg physica verlag 
jensen cohen 

multiple comparisons induction algorithms 
machine learning 
appear jensen schmill 

adjusting multiple comparisons decision tree pruning 
proceedings third international conference knowledge discovery data mining pp 

newport beach ca aaai press 
kass 

exploratory technique investigating large quantities categorical data 
applied statistics 
kearns 

efficient noise tolerant learning statistical queries 
proceedings fifth acm symposium theory computing pp 

new york ny acm press 
kearns vazirani 

computational learning theory 
cambridge ma mit press 
sax 

multiple comparisons 
beverly hills ca sage 
lawrence giles tsoi 

lessons neural network training overfitting may harder expected 
proceedings fourteenth national conference artificial intelligence pp 

providence ri aaai press 
mackay 

bayesian interpolation 
neural computation 
michalski 

theory methodology inductive learning 
artificial intelligence 
miller jr 

simultaneous statistical inference nd ed 
new york ny springer verlag 
moody 

effective number parameters analysis generalization regularization nonlinear learning systems 
moody hanson lippmann eds advances neural information processing systems pp 

san mateo ca morgan kaufmann 
murphy pazzani 

exploring decision forest empirical investigation occam razor decision tree induction 
journal artificial intelligence research 
ng 

preventing overfitting crossvalidation data 
proceedings fourteenth international conference machine learning pp 

nashville tn morgan kaufmann 
pagallo haussler 

boolean feature discovery empirical learning 
machine learning 
pearl 

connection complexity credibility inferred models 
international journal general systems 
quinlan 

induction decision trees 
machine learning 
quinlan 

programs machine learning 
san mateo ca morgan kaufmann 
quinlan cameron jones 

oversearching layered search empirical learning 
proceedings fourteenth international joint conference artificial intelligence pp 

montr eal canada morgan kaufmann 
rissanen 

modeling shortest data description 
automatica 
schaffer 

overfitting avoidance bias 
machine learning 
scheffer joachims 

estimating expected error empirical minimizers model selection tech 
rep 
tr 
berlin germany computer science department technical university berlin 
schuurmans ungar foster 

characterizing generalization performance model selection strategies 
proceedings fourteenth international conference machine learning pp 

nashville tn morgan kaufmann 
schwarz 

estimating dimension model 
annals statistics 
shawe taylor bartlett williamson anthony 

structural risk minimization data dependent hierarchies tech 
rep 
nc tr 
egham uk department computer science royal holloway university london 
st cohen 

building eda assistant progress report 
preliminary papers sixth international workshop artificial intelligence statistics pp 
ft lauderdale fl society artificial intelligence statistics 
stone 

cross choice assessment statistical predictions 
journal royal statistical society 
vapnik 

nature statistical learning theory 
new york ny springer verlag 
wallace boulton 

information measure classification 
computer journal 
webb 

experimental evidence utility occam razor 
journal artificial intelligence research 


multiple tests discrete distributions 
american statistician 
