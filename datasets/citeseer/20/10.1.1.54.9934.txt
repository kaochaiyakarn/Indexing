simplified support vector decision rules chris burges bell laboratories lucent technologies room crawford corner road holmdel nj big att com support vector machine svm universal learning machine decision surface parameterized set support vectors set corresponding weights 
svm characterized kernel function 
choice kernel determines resulting svm polynomial classifier layer neural network radial basis function machine learning machine 
svms currently considerably slower test phase approaches similar generalization performance 
address general method significantly decrease complexity decision rule obtained svm 
proposed method computes approximation decision rule terms reduced set vectors 
reduced set vectors support vectors cases computed analytically 
give experimental results pattern recognition problems 
results show method decrease computational complexity decision rule factor loss generalization performance making svm test speed competitive methods 
method allows generalization performance complexity trade directly controlled 
proposed method specific pattern recognition applied problem support vector algorithm example regression 
support vector machines consider class classifier decision rule takes form theta ns ff ff theta step function ff ns parameters vector classified 
decision rule large family classifiers cast functional form example delta implements polynomial classifier exp gamma oe implements radial basis function machine tanh fl delta ffi implements layer neural network support vector algorithm principled method training learning machine decision rule takes form condition required kernel satisfy general positivity constraint contrast techniques svm training process determines entire parameter set fff ns resulting subset training set called support vectors 
support vector machines number striking properties 
training procedure amounts solving constrained quadratic optimization problem solution guaranteed unique global minimum objective function 
svms directly implement structural risk minimization capacity learning machine controlled minimize bound generalization error support vector decision surface linear separating hyperplane high dimensional space similarly svms construct regression linear high dimensional space support vector learning machines successfully applied pattern recognition problems ocr text independent speaker identification object recognition investigated problems example regression 
reduced set vectors complexity computation scales number support vectors expectation number support vectors bounded gamma expectation probability error test vector number training samples ns expected approximately scale 
practical pattern recognition problems results machine considerably slower test phase systems similar generalization performance fact motivated reported method approximate svm decision rule smaller number reduced set vectors 
reduced set vectors properties ffl appear approximate svm decision rule way support vectors appear full svm decision rule ffl support vectors necessarily lie separating margin support vectors training samples ffl computed trained svm ffl number reduced set vectors speed resulting svm test phase chosen priori ffl reduced set method applicable support vector method example regression 
consider pattern recognition case 
data quote results ocr data sets containing grey level images digits set training test patterns refer postal set set training test patterns nist special database nist test data refer nist set postal images pixels nist images pixels 
nist set restricted classifiers separate digit digits 
reduced set training data elements dl svm performs implicit mapping phi dh dh 
vectors denoted bar 
mapping phi determined choice kernel fact satisfies mercer positivity constraint exists pair phi hg delta svm decision rule simply linear separating hyperplane 
mapping phi usually explicitly computed dimension dh usually large example homogeneous map delta dh dl gamma dl degree polynomials dl dh approximately 
basic svm pattern recognition algorithm solves class problem training data corresponding class labels gamma svm algorithm constructs decision surface psi separates classes psi delta gamma psi delta gamma slack variables introduced handle non separable case separable case svm algorithm constructs separating hyperplane margin positive negative examples maximized 
test vector assigned class label gamma depending psi delta phi greater 
support vector defined training sample equations equality 
name support vectors distinguish rest training data 
psi psi ns ff phi ff weights determined training gamma class labels order classify test point computes psi delta ns ff delta ns ff consider set nz corresponding weights fl psi nz fl phi minimizes fixed nz distance measure ae psi gamma psi call ffl nz reduced set 
classify test point expansion equation replaced approximation psi delta nz fl delta nz fl goal choose smallest nz ns corresponding reduced set resulting loss generalization performance remains acceptable 
clearly allowing nz ns ae zero non trivial cases nz ns ae section 
cases reduced set leads reduction decision rule complexity loss generalization performance 
nz computes corresponding reduced set ae may viewed monotonic decreasing function nz generalization performance function nz empirical results regarding dependence generalization performance nz section remarks mapping phi 
image phi general linear space 
phi general surjective may example homogeneous polynomial degree 
phi map linearly dependent vectors linearly independent vectors example inhomogeneous polynomial linearly independent vectors linearly dependent vectors 
general scale coefficients fl unity scaling homogeneous polynomial example homogeneous degree fl scaled gamma unity 
exact solutions section consider problem computing minimum ae analytically 
start simple non trivial case 
homogeneous quadratic polynomials homogeneous degree polynomials delta normalization factor 
simplify exposition start computing order approximation nz 
introducing symmetric tensor ns ff find ae psi gamma fl zk minimized ffl zg satisfying repeated indices assumed summed 
choice ffl zg ae ae gamma fl largest drop ae achieved ffl zg chosen eigenvector eigenvalue largest absolute size 
note choose fl scale extending order nz similarly shown set ffl minimize ae psi gamma nz fl eigenvectors eigenvalue fl kz gives ae gamma nz fl kz drop ae maximized chosen nz eigenvectors eigenvectors ordered absolute size eigenvalues 
note trace sum squared eigenvalues choosing nz dl approximation exact ae 
number support vectors larger dl shows size reduced set smaller number support vectors loss generalization performance 
general case order compute reduced set ae minimized ffl nz simultaneously 
convenient consider incremental approach ith step ffl held fixed ffl computed 
case quadratic polynomials series minima generated incremental approach generates minimum full problem 
result particular second degree polynomials consequence fact orthogonal chosen 
experiments table shows reduced set size nz necessary attain number errors ez test set ez differs number errors full set support vectors error quadratic polynomial svm trained postal set 
clearly quadratic case reduced set offer significant reduction complexity little loss accuracy 
note digits numbers support vectors larger dl presenting case opportunity speed loss accuracy 
table reduced set generalization performance quadratic case 
support vectors reduced set digit nz ez general kernels apply reduced set method arbitrary support vector machine analysis extended general kernel 
example homogeneous polynomial delta setting ae find pair ffl incremental approach gives equation analogous equation delta delta delta delta delta delta fl kz gamma delta delta delta ns ff ym delta delta delta case varying ae respect fl gives new conditions 
having solved equation order solution ffl ae ae delta delta delta delta delta delta gamma fl kz define delta delta delta delta delta delta gamma fl delta delta delta terms incremental equation second order solution takes form equation fl replaced fl respectively 
note polynomials degree greater general orthogonal 
incremental solutions needs solve coupled equations ffl allowed vary simultaneously 
equations multiple solutions lead local minima ae 
furthermore choices lead fixed point equations 
purposes described decided take computational approach 
solutions equation iterating starting arbitrary computing new equation repeating method described section proved flexible powerful 
unconstrained optimization approach provided kernel derivatives defined gradients objective function ae respect unknowns ffl computed 
example assuming sm function scalar delta fl gamma ns ff sm delta nz fl delta gamma ns fl ff delta nz fl fl delta possibly local minimum unconstrained optimization techniques 
algorithm start summarizing algorithm 
desired order approximation nz chosen 
ffl phase approach 
phase computed incrementally keeping fixed 
phase allowed vary 
phase gradient equation zero fl zero 
fact lead severe numerical instabilities 
order circumvent problem phase relies simple level crossing theorem 
fl initialized gamma initialized random values 
allowed vary keeping fl fixed 
optimal value fl fixed computed analytically 
minimized respect fl simultaneously 
optimal fl computed analytically gamma gamma delta vectors delta gamma gamma fl delta ns ff jk positive definite symmetric inverted efficiently decomposition 
numerical instabilities avoided preventing fl approaching zero 
algorithm ensures automatically step varied fl kept fixed results decrease objective function fl subsequently allowed vary pass zero doing require increase phase repeated times different initial values determined heuristically number different minima 
data usually chose 
sufficiently small sophisticated techniques example simulated annealing pursued 
phase phase vectors phase concatenated single vector unconstrained minimization process applied 
phase results roughly factor reduction objective function order unconstrained optimization method phases 
search direction conjugate gradients 
bracketing points search direction 
bracket balanced minimum quadratic fit points starting point iteration 
conjugate gradient process restarted fixed chosen number iterations process stops rate decrease falls threshold 
checked general approach gave results analytic approach applied quadratic polynomial case 
experiments approach applied svm gave best performance postal set degree inhomogeneous polynomial machine order approximation nz chosen give factor speed test phase class classifier 
results table 
reduced set method achieved speed essentially loss accuracy 
classifiers class classifier gave error full support set opposed reduced set 
note combined case reduced set gives factor speed different class classifiers support vectors common allowing possibility caching 
address question techniques scaled larger problems repeated study class classifier separating digit digits nist set training test patterns 
classifier chosen gave best accuracy full support set degree polynomial 
full set support vectors gave test errors reduced set size gave test errors 
table postal set degree polynomial 
third fifth columns give number errors test mode support vector reduced set systems respectively 
support vectors reduced set digit ns nz totals comparison techniques terms combined speed test phase generalization performance lenet series best performing systems postal set optimal lenet architecture lenet requires approximately multiply adds forward pass 
number reduced approximately architecture specific optimization generalization performance similar class case factor speed described gives svm approach similar test speed lenet neural network data set 
experiment nist set designed check reduced set method scaled larger problems 
factor speed resulting svm considerably slower best performing lenet lenet open question reduction complexity svm decision rule achieved loss generalization performance 
table size reduced set nz needed attain various levels generalization performance phase 
full set support vectors gives raw error 
reduced set gives speed factor ns nz note nz summed digits 
nz nz ns error rate reduced set size versus generalization performance interesting question generalization performance varies size reduced set 
particular number parameters reduced set classifier support vector classifier suspect method may provide means capacity control 
effective form capacity control control empirical risk vc dimension set decision functions 
gain empirical view computed reduced sets different sizes postal set measured generalization performance 
reduced set computed incremental approach phase error rates quoted reduced applying phases 
results shown table shows generalization performance combined class classifiers postal set 
error rate quoted zero rejection 
introduced reduced set method means approximating vector psi appearing decision rule support vector machine shown case ocr digit recognition reduced set give factor speed full support set essentially loss accuracy 
approach described size reduced set specified resulting accuracy loss determined experimentally 
support vector method extremely general applications expect approach described applicable different areas 
choosing nz approach allows direct control speed accuracy trade support vector machines 
wish vapnik valuable discussions commenting manuscript 
wish advanced information systems engineering group bell laboratories lucent technologies yoon arpa support 
funded arpa contract 
vapnik estimation dependencies empirical data springer verlag 
vapnik nature statistical learning theory springer verlag 
boser guyon vapnik training algorithm optimal margin classifiers fifth annual workshop computational learning theory pittsburgh acm 
scholkopf burges vapnik extracting support data task proceedings international conference knowledge discovery data mining aaai press menlo park ca 
cortes vapnik support vector networks machine learning vol pp 
bottou cortes drucker jackel lecun muller sackinger simard vapnik comparison classifier methods case study handwritten digit recognition proceedings th iapr international conference pattern recognition vol 
ieee computer society press los alamos ca pp 

lecun jackel bottou cortes denker drucker guyon muller sackinger simard vapnik comparison learning algorithms handwritten digit recognition international conference artificial neural networks ed 
fogelman gallinari pp 

lecun private communication 
schmidt bbn private communication 
blanz scholkopf bulthoff burges vapnik vetter comparison view object recognition algorithms realistic models submitted international conference artificial neural networks 
lecun boser denker henderson howard hubbard jackel backpropagation applied handwritten zip code recognition neural computation pp 

wilkinson geist janet burges hammond hull larsen wilson census optical character recognition system conference department commerce nist august 
press teukolsky vetterling flannery numerical recipes second edition cambridge university press 
