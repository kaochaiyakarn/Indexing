cs efficient parallel algorithms closest point problems peter su mathematics computer science dartmouth college hanover new hampshire department computer science duke university durham north carolina june efficient parallel algorithms closest point problems thesis submitted faculty partial fulfillment requirements degree doctor philosophy peter su dartmouth college hanover new hampshire june examining committee chairman scot john reif tom cormen david kotz dean graduate studies efficient parallel algorithms closest point problems peter su ph thesis advisor scot department mathematics computer science dartmouth college ii thesis dissertation develops studies fast algorithms solving closest point problems 
algorithms problems applications areas including statistical classification crystallography data compression finite element analysis 
addition comprehensive empirical study known sequential methods introduce new parallel algorithms problems efficient practical 
simple flexible designing analyzing parallel algorithms 
describe fast parallel algorithms nearest neighbor searching constructing voronoi diagrams 
demonstrate algorithms obtain performance wide variety machine architectures 
key algorithmic ideas examine exploiting spatial locality random sampling 
spatial decomposition provides allows concurrent threads independently local areas shared data structure 
random sampling provides simple way adaptively decompose irregular problems balance workload threads 
techniques result effective algorithms wide range geometric problems 
key experimental ideas thesis simulation animation 
algorithm animation validate algorithms gain intuition behavior 
model expected performance algorithms simulation experiments knowledge critical primitive operations cost machine 
addition burden esoteric computational models attempt cover possible variable design computer system 
iterative process design validation simulation delays actual implementation details possible accounted 
experiments tune implementations better performance 
part department computer science duke university durham nc supported arpa isto subcontract ki arpa isto prime contract 
views contained document author interpreted representing official policies expressed implied advanced research agency nsf onr government 
iii noticed years reading people dissertations best ones interesting entertaining 
hope quality mine close read learned 
family especially father pursue ideas activities interested bought magazines telescope introduced science 
taught gave ambition continue education graduate school tools succeed 
distinct pleasure able entirely separate groups people eventual success graduate student 
dartmouth scot advisor taught know voronoi diagrams 
eye detail interest real answers comprehensive 
finding clear simple explanations complicated algorithms improved presentation section thesis 
fellow students dartmouth graduated included barry deb john joe jerry quinn course larry 
guys parties office sharing rooms inn help bent problem sets heated generally worthless political discussions putting abuse asked unix questions 
speaking unix wayne great kept rubber bands glue room running 
duke university john reif provided advising endless pool algorithmic ideas monetary machine support needed finish projects 
john interest combining theoretical experimental ideas computational geometry parallel algorithms materialized 
addition folks north carolina supercomputer center especially greg byrd keeping running putting constant stream novice questions 
alternative happy hour crowd quickly part duke community relieving initial suspicion theory 
provided numerous unix toys fun play helped finish 
owen wasted hours debating various religious issues especially programming languages 
steve tate put constant theoreticians clueless theory questions 
mike landis eric anderson chris connelly provided friendship great conversation 
thirdly old friends cmu got started undergrad gave place finish final stretch thesis writing 
david garlan rob past help guy blelloch sid chatterjee scandal group interested great library cray magic 
net hackers provided programming tools day get done 
world emacs ftp mode mh vm groff iv gcc perl rest 
lastly importantly karen years love understanding support 
hear delaunay triangulations 
table contents iii iv table contents vi list tables ix list figures 

preliminaries 
workload 
models 
machines 
notation terminology 
related 
theory 
practice 
contributions 
outline thesis 
practical sequential voronoi algorithms 
divide conquer 
sweepline algorithms 
incremental algorithms 
faster incremental construction algorithm 
incremental search 
site search 
discussion 
empirical results 
performance incremental algorithm 
incremental algorithm quad tree 
incremental search algorithm 
fortune algorithm 
bottom line 
nonuniform point sets 
notes discussion 
algorithms 
principles vi 
models machines 
popular programming models 
wrong pram 
model 
models 
machine descriptions 
cray mp 
ksr 
benchmarking 
cray 
ksr 
summary 
concurrent local search 
problem 
algorithm 
nearest neighbors cray 
measurements 
extensions applications 
discussion 
implementation ksr 
summary 
parallel delaunay triangulation algorithms 
parallel divide conquer 
optimal expected time algorithms 
randomized algorithms 
preliminaries general framework 
randomized divide conquer 
practical randomized parallel algorithm 

analysis 
experiments 
summary 
practical parallel algorithms delaunay triangulations 
randomized incremental construction 
concurrent incremental construction 
simulations 
implementation 
concurrent incremental search 
experiments 
summary 
summary 
algorithms 
models experimental analysis vii 
implementations benchmarks 

algorithms 

programming parallel algorithms 

simulation performance analysis 
viii list tables 
nonuniform distributions 

costs vector operations processor cray mp 
cost elementwise addition ksr 

comparison synthetic benchmark values actual runtime 
ix list figures 
quad edge data structure 

quad edge representation simple subdivision 

merge procedure guibas stolfi divide conquer algorithm 

rising bubble 

merge loop 

code processing events fortune algorithm 
invalidating circle events 

fortune algorithm 

pseudo code incremental algorithm 

incremental algorithm 

incremental algorithm spiral search point location 

spiral search near neighbor 

algorithm 
code site search 

example bounding boxes site search algorithm examines 

site search algorithm 
comparison point location costs 

circle tests site algorithms 

performance quad tree algorithm 

performance algorithm 
cost fortune algorithm 

circle events cluster near sweepline 

cost fortune algorithm heap 

delaunay edges imply empty regions plane 

comparison runtimes 

incremental algorithm non uniform inputs 

fortune algorithm non uniform inputs 

dwyer algorithm non uniform inputs 

algorithm sensitive bad inputs 

algorithm cluster corners distributions 

runtimes non uniform inputs 

schematic diagram cray mp 

schematic diagram ksr 
performance elementwise vector addition ksr 

performance parallel prefix sum ksr 

performance unstructured permute ksr 

performance reverse ksr 

parallel loop bucket points 

bucket data structure 

handling write collisions 

spiral search find near neighbor 

outer loop spiral search algorithm 

second outer loop spiral search algorithm 

inner loop spiral search algorithm 

distance computations site algorithm 

runtime spiral search algorithm 

runtime cray 
cray vectorized runtime clusters 
threads code bucketing points 

inner loop spiral search 

performance synthetic benchmark 

performance concurrent local search algorithm ksr 

speedup parallel spiral search algorithm 

parallel algorithm compared sparcstation 

objects regions conflict relation 

divide step goodrich algorithm 

routine find circles 

triangle abc fails circle test goes subproblem 

voronoi region point flower circles 

load balancing results processors 

load balancing processors 

bucket expansion bad distributions processors 

total subproblem size algorithm 

total subproblem size distribution processors 

concurrent incremental algorithm 

load balancing concurrent incremental algorithm 

concurrency profiles various machine problem sizes 

circle tests site algorithm cric 

edge tests site algorithm cric 

interchanging loops gss 

effect large batch sizes processors 

circle tests site algorithm cric 

edge tests site algorithm cric 

speedup algorithm cric vs ksr node 

algorithm cric vs sparc 

code concurrent edge dictionary 

code concurrent edge queue 

distance calculations site concurrent incremental search algorithm 

buckets examined site concurrent incremental search algorithm 

speedup algorithm cis relative ksr node 

speedup algorithm cis relative sparcstation 

algorithm cis vs sparc 
xi 
speedup algorithm cis relative algorithm cric procs 
xii chapter creating hard thing grass blade easier oak 
james russell lowell thesis develops parallel algorithms closest point problems computational geometry 
specifically concentrate nearest neighbors problem constructing voronoi diagram 
problems easy state nearest neighbors 
set points space set query points find point gamma fpg closest voronoi diagram 
set points sites 
space 
define collection points gamma fsg dist dist 
voronoi diagram collection fv sg 
straight line dual voronoi diagram called delaunay triangulation 
picture shows voronoi diagram delaunay triangulation points plane ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl dimensions algorithms assume set sites spherical 
case delaunay triangulation partitions convex hull simplices simplex contains site 
example planar case assume sites circular convex hull point set divided triangles triangle empty circumcircle 
dissertation case studies study performance sequential parallel algorithms planar version problems 
case studies provide experimental comparison known sequential algorithms new parallel algorithms problems 
algorithms closest point problems large part literature computational geometry 
preparata shamos ps provide excellent overview field textbook 
theoretical research area produced algorithms optimal theoretical sense efficient practice 
problems interested outside field theoretical computer science 
voronoi diagrams applications statistical classification crystallography data compression finite element analysis 
nearest neighbor search appears problem database systems clustering algorithms image processing combinatorial optimization 
aurenhammer okabe obs survey algorithms applications related problems 
parallel computational geometry branch computational geometry concerned developing geometric algorithms suitable computers multiple processors 
parallel computers theory allow users solve large problems quickly conventional machines parallel algorithms geometric problems interest potential applications get larger 
decade dozens parallel algorithms various kinds machines proposed successfully implemented 
implementations delivered reasonable performance 
primarily due unrealistic computational models high constant factors hidden asymptotic analysis nat 
relative lack concern practical issues parallel algorithms opened gap researchers theoretical computer science practicing programmers 
theoretical computer scientists tend argue incorporating architecture specific parameters models general theory difficult obtain 
practitioners parallel computing argue computational models pram theoretical computer science unrealistic irrelevant 
thesis motivating dissertation views correct 
understanding theoretical insights machine details necessary develop efficient practical parallel algorithms important problems 
dissertation show tools theory analysis algorithms probability theory systems compilers programming languages operating systems architecture design parallel algorithms closest point problems variety architectures 
preliminaries goal dissertation efficient algorithms finding nearest neighbors constructing voronoi diagrams conventional parallel architectures 
efficient mean asymptotically efficient model parallel computation mean efficient terms rate algorithms perform floating point operations simply speedup achieved parallel algorithms multiple processors 
ideally algorithms achieve runtimes competitive known practical solutions problems time effective multiple processors available 
order achieve goal broad range tools areas algorithm design sequential parallel programming methodology performance analysis experimental computer science 
specific techniques show course dissertation include ffl study algorithms efficient expected runtimes input program assumed come known probability distribution 
ffl randomization sequential parallel algorithms design 
ffl high level programming models data structures algorithms away machine architectures 
ffl combination high level programming models explicit cost models keep algorithm designs realistic 
ffl simulation animation sequential parallel algorithms systematically analyze implementation behavior time consuming programming takes place large scale machine 
chapter discusses principles detail context sequential algorithms chapters apply techniques context parallel algorithms design 
key idea dissertation tries emphasize systematic methodology study parallel algorithms theoretical experimental viewpoint 
bentley ben mcgeoch mcg similar methodologies experimental study sequential algorithms 
thesis case studies combine careful experiments new known theoretical results coherent understanding performance proximity algorithms sequential vector parallel architectures 
experimental framework consists parts workloads models machines 
workload workload term experimental computer science describe suite programs real synthetic test behavior existing systems guide design new ones 
context algorithms analysis term refer model expected input algorithms 
terminology describe inputs algorithms results algorithms produce ffl site point plane 
generally terms point site interchangeably 
ffl definitions set sites set sites 
problem assume set case general 
ffl set sites vor denote voronoi diagram dt denote delaunay triangulation site denote voronoi region dn denote set sites connected dt 
algorithms designed efficient sites independently chosen probability distribution 
example experiments thesis done sites generated uniform distribution unit square 
generic term uniform inputs describe class input sets 
inputs include uniform distribution unit square circle distributions values bounded constants entire domain 
say algorithms runs expected time mean things 
algorithm deterministic random choices average runtime distribution inputs input equally 
hand may allow algorithm random choices 
case average runtime distribution sequences random choices sequence equally 
sequential quicksort algorithm provides example terminologies 
original form algorithm expected runtime log long assume input permutation equally 
modify algorithm choose partition elements random algorithm run log expected time input distribution long sequence splitter choices equally 
say randomized algorithm runs time high probability probability algorithm fails run time bound 
thesis concentrate primarily algorithms provably efficient uniform inputs 
inputs usually accurate model data sets occurring actual applications 
addition conduct tests study robustness algorithms non uniform inputs 
distributions outlined chapter 
models main difficulties facing designer parallel algorithms choosing appropriate model computation 
hand model allow algorithm designers analyze reason algorithms straightforward machine independent way 
hand realistic accurately model performance target machines 
current parallel architectures vary machine machine task difficult 
ram model standard model complexity sequential algorithms 
assumes machine central processor infinite random access memory 
single machine cycle ram machine fetch word perform operation processor write result back memory 
operations allowed vary machine machine assume real arithmetic bit wise operations floor function constant time 
practical problem sizes machines provide functionality 
people design sequential algorithms long ram model basis analysis 
model ignores relevant machine details size memory paging number registers asymptotic analysis ram algorithms largely representative real performance 
asymptotics abused time time part careful analysis yielded answers 
natural extension ram model parallel computing just replicate ram processors single machine processors share common memory 
result replication pram model 
pram machine processors share central memory 
cycle processor allowed perform functions normal ram processor 
different pram machines allow different amount concurrent memory access 
erew machine allow concurrent access crew machines allow concurrent reads crcw machines allow concurrent writes 
addition various versions crcw model specify different ways resolve multiple writes memory location 
example arbitrary crcw model picks arbitrary processor winner cases priority crcw model picks winner pre assigned set priorities 
efficiency pram algorithms measured metrics 
number parallel steps needed solve problem hand 
second number processors needed achieve parallel runtime 
total done algorithm delta 
pram algorithm efficient delta runtime best possible best known sequential algorithm 
pram algorithm optimal log positive integer polylog time efficient 
nc nick class class problems solutions running polylog time polynomial number processors 
theoretical computer science problems considered efficient parallel solutions 
algorithms run polylog time polylog extra thought efficient 
pram model weaknesses ram model ignores parameters machine size memory bandwidth crucial performance algorithm 
algorithms designed pram model successful practice designed ram machine 
fact programmers parallel machines simply write pram algorithms theoretical 
problem pram model really isn model 
fact pram model remarkably similar data parallel programming models sh come wide 
data parallel models primitive operations operate large aggregate data structures vectors arrays 
examples primitives include elementwise arithmetic permutation routing parallel prefix sums called scans 
thesis blelloch ble showed pram algorithms translated model 
shown blelloch set vector primitives efficiently implemented wide variety machines cbf 
apparent pram model basis designing parallel algorithms efficient practice 
real problem pram algorithms developed analyzed 
part goal pram algorithm provide practical method solving problem 
pram algorithms vehicles studying questions complexity theory 
result asymptotic analysis abused unrealistic importance placed obtaining polylogarithmic runtimes 
practice asymptotic analysis hides important details working hard achieve polylog time encourages overly esoteric expensive algorithms data structures simpler methods sufficient practical problems 
kruskal rudolph snir krs criticized traditional notions efficiency parallel algorithms propose realistic approach parallel complexity theory 
thesis pram data parallel models express algorithms 
concerned practical computational methods augment models explicit costs primitive operation 
algorithms analyzed terms costs easily different machines 
explicit cost models allow algorithms designed high level retaining contact realistic machine constraints 
addition interested performance algorithms size problem large size machine fixed 
question interest take advantage machine parallelism exploit inherent parallelism particular algorithm 
machines practical study algorithms complete machines run programs 
thesis principal machines experiments sparcstation cray ksr 
baseline machine sparcstation workstation mhz clock rate mb memory 
simulation preliminary program development thesis done machine 
addition provides main cost performance comparison point parallel algorithm implementations study 
cray mp represents traditional large scale parallel architecture 
uses combination large heavily pipelined cpus large high bandwidth memory provide effective support program written vector data parallel style 
ksr represents newer style parallel architecture 
supports powerful cpus relatively low bandwidth distributed memory system 
memory system enhanced local caches 
caches kept globally coherent extra hardware ring communications network 
ksr provides shared memory programming model top message passing architecture 
chapter cover machines machine models detail 
notation terminology list notation terminology rest dissertation ffl log mean natural logarithm lg mean base logarithm 
ln 
denote th harmonic number ffl thread program counter runtime environment address space 
traditional operating systems provide process abstraction usually address space various operating system data structures thread 
operating systems parallel machines allow processes hold threads control address space 
allows applications exploit parallelism fine grain threads cheap create processes expensive 
notion similar idea virtual processors vps simd machines multiplex physical hardware 
terms synonymously 
related section survey literature sequential parallel algorithms closest point problems 
section divided parts devoted theoretical results practical implementations 
theory researchers computational geometry successful designing algorithms closest point problems elegant theoretically efficient practical 
bentley weide yao describe cell algorithms nearest neighbor search voronoi diagram construction buckets main data structure run expected time input points chosen uniform distribution unit square 
algorithms technique called spiral search show simple efficient 
clarkson thesis cla presents results closest point problems higher dimensions 
developed algorithms various kinds nearest neighbor searching constructing minimum spanning trees 
algorithms tested practical situation relatively simple generalizations quad tree subdivide space searching efficient 
bentley tree data structure solve nearest neighbors closest point problems higher dimensions ben 
bentley uses data structure plane answer fixed radius nearest neighbor queries traveling salesman programs ben 
constructing planar voronoi diagrams received large amount attention 
guibas stolfi gs unified framework ideal data structure computing representing diagram dual 
algorithms construct delaunay triangulations voronoi diagrams 
approach resulting algorithms simpler elegant 
guibas stolfi show implement algorithms constructing delaunay triangulation 
divide conquer algorithm second constructs diagram incrementally 
algorithms previously literature gs ls sh guibas stolfi implementations simpler elegant earlier algorithms 
algorithms run log worst case time respectively 
fortune examines construct optimal sweepline algorithm constructing voronoi diagram 
algorithm uses transformation plane allow straightforward sweepline algorithm correctly compute voronoi diagram delaunay triangulation input 
proposed algorithms expected case runtimes 
algorithms come flavors 
algorithms expected runtime depends input distribution 
algorithms tend sort bucketing scheme take advantage smooth input distributions 
bentley weide yao cell algorithms fall class 
maus mau presents simple algorithm constructing delaunay triangulation similar gift wrapping algorithms convex hulls ps 
algorithm repeated discovers new delaunay triangles search process similar spiral search 
algorithm uses bucketing speed inner loop runs expected time 
dwyer thesis shows generalization algorithm higher dimensions runs linear expected time uniform inputs 
dwyer shows modified version divide conquer runs log log expected time 
ohya iri describe incremental algorithm uses combination bucketing quad trees achieve linear expected time 
second class fast expected time algorithms uses randomization achieve expected performance input 
clarkson shor cs describe incremental algorithm inserts points random order runs log expected time 
knuth guibas sharir gks give refined analysis modified randomized incremental algorithm sharir yaniv sy refine analysis discuss implementation algorithm 
algorithm persistent tree structure perform point location current diagram 
devillers similar data structures dynamic algorithms 
clarkson mehlhorn seidel cms show extend clarkson original results develop dynamic algorithms higher dimensional convex hulls construct voronoi diagram 
studies produced algorithms theoretically efficient practical 
chapter describe incremental divide conquer sweepline algorithms detail detailed performance analysis implementations 
discussion important parallel algorithms ideas derived serial algorithms 
thorough understanding known conventional algorithms needed design parallel methods 
parallel computational geometry lagged sequential counterpart design practical algorithms 
large amount theoretical done years 
form algorithms pram models kr 
recall pram machines come flavors depending control concurrent access memory locations 
parallel computational geometry crew pram allows concurrent reads concurrent writes popular 
algorithms crcw pram allows concurrent reads writes 
aggarwal acg summarize 
particular describe parallel algorithm constructing planar voronoi diagrams runs log parallel time crew processors 
algorithm parallelization standard divide conquer algorithm main problem executing merge step parallel 
goodrich cole complicated scheme achieves log time log processors 
order achieve time bounds algorithms elegant complex data structures 
algorithms efficiently implemented current machines 
algorithms literature achieve optimal expected run times 
varadarajan log expected time processor divide conquer algorithm 
uses combination bucketing parallel dividing chain construction achieve runtime 
crcw algorithm levcopoulos lkl uses hierarchical bucketing scheme runs logn expected time log processors 
algorithm parallelization bentley weide yao scheme 
mackenzie stout ms version algorithm runs log log expected time log log processors 
methods mainly theoretical interest constants inside big large 
algorithms achieve optimal performance high probability kind randomized divide conquer 
reif sen rs show randomized divide conquer construct convex hull set points dimensions 
results algorithm constructing voronoi diagram runs logn time crcw processors high probability 
published algorithm extremely complex simplified version constructing voronoi diagram practical 
chapter discuss algorithm detail 
goodrich gg random sampling construct dimensional convex hulls 
algorithm parallel version convex hull algorithm edelsbrunner shi es 
goodrich additional probabilistic machinery improve confidence bounds run time 
reif sen algorithm subroutine method practical published 
algorithms proposed dimensional mesh connected computers 
dehne presents simple iterative algorithm construct voronoi diagram raster screen 
lu lu describes divide conquer algorithm runs log time processor mesh 
jeong lee jl improve time theta processor mesh 
practice researchers scientific computation done practical implementing parallel algorithms 
discusses parallel algorithms various numerical problems solving linear systems 
hundreds papers document performance codes dominant style presentation compare different methods benchmarks various metrics speedup mflops 
fox book fjl describes done scientific computing group caltech gives flavor kind research 
comparatively little implementations combinatorial geometric algorithms 
sorting received attention area 
fox fjl describes hypercube implementations quicksort shellsort bitonic sort 
compares hypercube version sample sort 
blelloch blm describe sorting algorithms connection machine surveys known theoretical practical parallel sorting algorithms 
analyzes cm implementations bitonic sort radix sort sample sort 
feature carefully constructed analytical model accurately predicts performance sorting algorithms large range input sizes 
style analysis blelloch zagha zb vectorized radix sort hightower prins reif phr sample sort mesh connected machines 
parallel computational geometry blelloch thesis ble describes algorithms hulls building trees finding closest pair simple geometric problems 
give detailed performance analysis algorithms 
cohen miller stout report hypercube algorithms various problems 
addition practical done algorithms constructing dimensional delaunay triangulations finite element meshes 
merriam mer teng describe algorithms intel machines connection machine respectively 
implementation comes application areas image processing computer graphics scope thesis survey literature 
dissertation implementations ways 
demonstrate proposed algorithms achieve performance real machine 
results standard benchmark style 
addition implementations simulations gain detailed understanding algorithms 
mcgeoch thesis experimental algorithms analysis mcg bentley traveling salesman problem ben presents framework experimental analysis algorithms 
thesis apply methods design analysis parallel algorithms 
chapter presents ideas detail 
contributions dissertation describe case studies design analysis implementation parallel algorithms computational geometry 
case studies concentrate closest point problems try outline general principals applicable problem 
contributions methodology 
describe apply systematic methodology implementation experimental analysis algorithms sequential parallel 
method utilizes high level models designing analyzing parallel algorithms similar pram kr blelloch vector models ble 
models primitive operations routing element wise arithmetic parallel prefix sums efficient implementations current parallel architectures 
order keep analysis realistic augment models ways 
models parameterized cost primitives target machines 
second models may modified include higher level operations long primitives chosen carefully cost implementation excessive 
methodology depends pragmatic mix mathematical experimental analysis 
mathematical analysis provides high level view performance experimental analysis effectively guides design process bottlenecks implementation 
algorithms 
case studies describe design analysis algorithms nearest neighbors problem constructing voronoi diagrams dimensions 
case studies provide practical guide efficient sequential parallel algorithms voronoi diagram construction 
addition illustrate parameterized models careful experimentation accurately predict performance parallel programs machine 
parallel algorithms novel results 
efficient realistic models computation exhibit performance carefully implemented real parallel machines 
combination theoretical analysis pragmatic machine models careful programming main focus dissertation 
benchmarks 
algorithms implemented additional benchmarks study performance characteristics parallel architectures 
algorithms closest point problems dynamic irregular flavor benchmark programs normally analyze machine performance 
addition algorithms sophisticated irregular benchmarks cha 
dissertation address concerns full applications 
algorithms study fall class kernel programs 
results reflect performance small percentage large application 
experience numerical computing suggests having high performance libraries kernel algorithms available valuable parallel algorithms dissertation attempt take advantage disk arrays parallel devices 
assume algorithms operate data sets loaded main memory results stored main memory 
optimizing algorithms deal issues scope 
aggarwal vitter acg vitter shriver nodine vs nv obtained basic theoretical results area 
cormen thesis cor examine systems issues including implementation efficient algorithms virtual memory data parallel computation 
compilers 
compilers generate efficient machine code relatively high level description algorithms 
restructuring compilers parallelizing compilers subject current research 
wolfe wol gives overview basic methods provides detailed 
dissertation address effect sophisticated compilers algorithm development 
degeneracy roundoff error 
design robust geometric algorithms subject theoretical research ds mil yap 
handling details important general discussion issues scope thesis 
algorithms assume non degenerate input part naive numerical issues 
generalized models 
address extension standard theoretical models deal asynchrony memory latency hierarchical memory systems acf acs acs cz gib hr 
thesis concerned issues real bottlenecks performance algorithms 
incorporating low level machine parameters network bandwidth cache size cost synchronization analysis parallel algorithm appropriate incorporating parameters processor speed number available registers cache size bus bandwidth analysis sequential algorithms 
parameters potentially important situation 
outline thesis chapter compares performance large number known sequential algorithms constructing delaunay triangulation 
addition discusses new incremental algorithm constructing planar delaunay triangulations 
algorithm uses randomization bucketing achieve speed retaining simplicity 
discussion parallel programming models chapter chapter investigates application bucketing techniques parallel algorithms 
result investigation high performance parallel algorithm nearest neighbors problem 
chapters discuss ideas fit fast parallel algorithms constructing delaunay triangulation 
chapter concludes thesis outlines areas 
chapter practical sequential voronoi algorithms man center circle fatal circumference pass 
john james ingalls sequential algorithms constructing voronoi diagram come basic flavors conquer gs sweepline incremental cs gs mau 
chapter presents experimental comparison algorithms 
addition describes new incremental algorithm simple understand implement competitive sophisticated methods wide range problems 
algorithm uses combination dynamic bucketing randomization achieve simplicity performance 
experiments chapter illustrate basic principles design experimental analysis algorithms 
provide detailed characterization behavior algorithm wide class problem instances 
achieve combination high level primitives abstraction explicit cost models algorithm animation careful design experimentation 
chapters apply methods study parallel algorithms 
divide conquer divide conquer algorithms dividing original input set smaller subsets solving sub problems recursively 
answers sub problems merged form final answer entire problem 
guibas stolfi gs conceptually simple method implementing idea elegant data structure representing delaunay triangulation voronoi diagram time 
quad edge data structure represents subdivision plane terms edges subdivision 
edge collection pointers neighboring edges subdivision dual 
solid edges subdivision dotted edges dual 
edge origin org destination dest left right face 
edge sym directed opposite way 
edge rot edge dual directed right left 
edges edges counterclockwise origin left face respectively 
similarly edges clockwise org right face defined analogously destination right face guibas stolfi show edges computable constant number sym rot 
proof algebraic properties abstractions scope chapter 
edges represented record pointers data fields 
data represents edges rot sym rot sym pointers represent edge rot sym rot sym quad edge data structure 
edges 
general edge record represents edge rot example shows triangle quad edge representation 
faces vertices subdivision represented implicitly circular linked lists edges 
picture bold lines connect nodes vertex ring thin lines connect nodes face ring 
add delete edges subdivision operator called splice similar linked list insertion 
call splice effect splicing ring edge ring edge part ring splice splits ring pieces 
splice connect routine connects edges share common left face 
guibas stolfi give detailed description works discuss topological algebraic foundations representation great detail 
understand algorithms chapter sufficient just keep picture mind reading pseudo code 
guibas stolfi algorithm uses additional geometric primitives 
uses dimensional orientation test ccw returns points form counter clockwise turn 
second circle determines lies circle formed assuming ccw true 
primitives defined terms dimensional determinants 
fortune shows compute accurately finite precision 
primitives place guibas stolfi algorithm proceeds standard conquer scheme 
points sorted coordinate split fewer points remain sub problem 
sub problems merged routine shown merge procedure accomplish tasks 
delete edges valid 
second create set valid delaunay edges called cross edges connecting process begins base convex hull merge procedure walks upward vertical line finding cross edges goes 
code variable bl keeps track current cross edge 
cross edge connect left point bl point bl connect right point bl point conceptually algorithm finds new cross edge expanding empty circle incident bl half plane bl 
portion bubble lies bl stays empty shrink circle expands half plane 
portion bubble bl eventually encounter new site endpoints bl new site define new delaunay triangle circle growing bl empty triangle contain cross edge 
algorithm repeats process inductively new cross edge 
process effect iteratively weaving ffl ffl ffl quad edge representation simple subdivision 
block left represents edge right 
example pointer indicates edge counterclockwise edge symmetric copy vertex represented implicitly ring merge find lower common tangent bl base edge bl lower tangent upper tangent bl sym ccw dest bl dest bl org remove bad edges circle dest dest org bl org delete bl ccw dest bl dest bl org remove bad edges circle dest dest org bl dest delete upper tangent 
return connect new cross edge 
circle dest org org dest bl connect bl sym bl connect bl sym sym bl bl sym merge procedure guibas stolfi divide conquer algorithm 
rising bubble 
circular bubble finding cross edges time bubble encounters site see 
code loops perform task finding cross edge 
loop examines edges incident left point bl counterclockwise fashion 
loop uses circle find delete edges invalidated bl org 
second loop performs symmetric operation right endpoint bl 
variables keep track rising bubble fact hit edge respectively 
loops finished cases hold valid candidate edge algorithm connect new cross edge edge available 
case final circle test determines new cross edge connected right left endpoint bl test needed 
process continues reaches upper convex hull edge shows operation merge loop starting lower common tangent left right diagrams 
frames bold circles show successful circle tests dotted edges tested validity 
current value bl cross edge computed called base edge shown bold 
cross hairs mark candidate points connecting new cross edge 
reading frames left right top bottom frame shows merge process proceeded bold edge 
frames show action loops 
edges left diagram deleted right diagram eighth frame shows circle test determines way connect new cross edge 
circle incident left edge empty cross edge connected right endpoint base left subproblem 
new edge shown bold frame new base edge iteration merge 
guibas stolfi provide proof scheme deletes invalid edges adds valid ones final diagram delaunay triangulation set 
show merge step takes steps worst case algorithm runs log worst case time 
comparison models computation optimal runtime algorithm constructs voronoi diagram problem reduced sorting 
merge loop 
site find site circle inscribed tangent sweepline empty 
add edge frontier update event queue new circle events delete invalid circle events event queue sites lying circle remove edges frontier add edge frontier update event queue code processing events fortune algorithm computational model includes unit time floor function better algorithms possible special cases 
dwyer showed simple modification algorithm runs log log expected time uniformly distributed points 
dwyer algorithm splits point set vertical strips width log constructs dt strip merging horizontal lines merges strips vertical lines 
experiments indicate practice algorithm runs linear expected time 
version algorithm due kk merges square buckets quad tree order 
show algorithm runs linear expected time uniform points 
fact experiments show performance algorithm nearly identical dwyer 
sweepline algorithms fortune discovered optimal scheme constructing delaunay triangulation sweepline algorithm 
algorithm keeps track sets state 
list edges called frontier diagram 
edges subset delaunay diagram form tour outside incomplete triangulation 
algorithm keeps track queue events containing site events circle events 
site events happen sweepline reaches site circle events happen reaches top circle formed adjacent vertices frontier 
discussion say circle event associated triple incident frontier edges 
events ordered direction event time event queue minimum coordinate 
algorithm sweeps line direction processing event encounters see 
conceptually fortune algorithm constructs valid delaunay edges time upward sweep point set 
site event algorithm searches valid delaunay edge new site points frontier 
finding site frontier circle inscribed tangent sweepline empty 
accomplished binary search edges frontier 
edges guaranteed delaunay edges added frontier 
new site creates new frontier edge bold edge new circle event bold circle invalidating existing circle event light circle 
case new circle incident created shown picture clear 
circle event bold circle creates new edge bold line invalidates existing circle event light circle 
situation new circle event incident created 
new edge added frontier may generate new circle events 
circle events added event queue new frontier edge externally convex angle neighboring edges edge edge delaunay triangle completed 
priority new event defined coordinate top circle 
addition adding new edge frontier may invalidate existing circle event 
suppose sites form edges currently frontier circle event associated 
edges form externally convex angle edges delaunay triangle 
new site swept connected event invalid replaced circle event associated edges see 
sweepline reaches circle event reached coordinate top circle sites adjacent edges frontier form externally convex angle 
circle guaranteed empty site circle encountered sweepline earlier time invalidated event 
process event algorithm removes frontier diagram replaces edge 
associated circle event invalidated replaced circle incident 
edge forms externally convex angle neighbors new circle event generated added event queue see 
fortune shows scheme adds valid delaunay edges diagram edges added processing site events valid circle events reached sweepline represent empty circles 
standard tree data structures priority queue frontier new event processed logn time algorithm uses log time worst case 
fortune implementation frontier simple linked list half edges point location performed bucketing scheme 
coordinate point located hash value get close correct frontier edge algorithm walks left right reaches correct edge 
edge placed bucket point landed points nearby located quickly 
method works long query points edges distributed buckets 
bucketing scheme represent priority queue 
members queue bucketed priorities finding minimum involves searching non empty bucket pulling minimum element 
works long priorities distributed 
shows operation algorithm small point set 
pictures sweepline shown dotted horizontal line 
sites large dots circle events shown signs marking top corresponding circle 
reading frames left right top bottom frames show circle event processed 
frame sweepline reached top bold circle 
algorithm removes event priority queue 
removes frontier edges incident empty circle frames 
frame shows algorithm completing new triangle 
bold circle shown frame invalid point incident edges just removed frontier rightmost 
frame shows replacement 
frame sweepline reaches new site 
frames show search correct frontier site frame shows new frontier edge 
frames algorithm adds new circle event incident new frontier edge event line processed 
incremental algorithms third simplest class algorithms constructing delaunay triangulation incremental algorithms 
study styles incremental algorithms incremental construction incremental search 
incremental construction algorithms add sites diagram update diagram site added 
guibas stolfi gs basic incremental construction algorithm 
algorithm consists main subroutines locate locates new site current diagram finding triangle point lies insert inserts new site diagram calling locate iteratively updating diagram edges invalidated new site removed see 
locate routine works starting random edge current diagram walking line direction new site right triangle 
algorithm simpler assuming points enclosed large triangle 
insert routine works calling locate connecting new point diagram edge vertex triangle locate 
insert updates diagram finding invalid triangles outside polygon containing new site 
triangles circle test 
abc triangle point opposite new site edge flipping procedure replaces edge bc edge creating new triangles apc abp 
loop adds edges ac ab queue edges checked 
keeps track queue implicitly links quad edge data structure 
loop comes back original starting edge queue empty routine stops 
guibas stolfi show loop terminates edges diagram guaranteed valid 
shows operation insert routine 
frame inserting site marked symbol 
bold edges show path locate takes diagram find new site 
second frames shows new edges connect fortune algorithm 
locate start start true dest org return org dest sym ccw org dest ccw org dest return insert locate org dest return ison org dest delete base org org splice base connect new point base sym base connect base dest base true flip invalid edges done ccw dest org dest circle org dest dest org return pseudo code incremental algorithm 
incremental algorithm 
diagram 
start test validity edges surrounding dotted edges ones tested 
circles empty drawn thin lines 
circle contains drawn bold 
frame shows flipped edge dotted corresponding new circle empty 
algorithm moves edges neighboring edge just flipped 
frames show edge flip 
edges valid 
frame algorithm reached starting edge insertion complete 
worst case runtime algorithm possible construct point set insertion order inserting th point diagram causes updates 
points inserted random order clarkson shor analysis cs shows design algorithm expected runtime log operations 
addition performance algorithm independent distribution input depends random choices algorithm 
algorithm works maintaining current diagram auxiliary data structure called conflict graph 
conflict graph bipartite graph edges connecting sites triangles current diagram 
edge exists means site lies circumcircle triangle new site added diagram algorithm deletes triangles conflicts adds new triangles place updates conflict graph 
standard incremental algorithm modified adding code update conflict graph edge flip 
clarkson shor random sampling results show total expected runtime algorithm log steps 
guibas knuth sharir gks propose similar algorithm conflict graph provide simpler analysis runtime 
particular show random insertion order expected number edge flips standard incremental algorithm performs linear 
bottleneck algorithm locate routine 
guibas knuth sharir propose tree data structure internal nodes triangles deleted subdivided point construction current triangulation stored leaves 
triangle deleted marks creates pointers triangles replaced diagram 
modify locate search tree doing standard edge walk 
hard show expected cost locate log time 
sharir yaniv sy prove bound nh 
modifying existing incremental algorithm maintain point location structure difficult 
easiest way keep separate tree structure links back current quad edge representation delaunay triangulation 
resulting algorithm somewhat unsatisfactory uses log expected time point location maintaining delaunay triangulation costs expected time 
remedy situation assume input consists points probability distribution 
example distribution uniform distribution unit square known algorithms run linear expected time kk mau 
motivation studying case comes fact applications involve data fairly uniform 
section see modify randomized incremental algorithm run linear expected time data 
faster incremental construction algorithm goal speed point location randomized incremental algorithm input uniformly distributed 
concentrate point location assumption random insertion order dominant cost incremental algorithm 
simple bucketing algorithm similar bentley yao finding nearest neighbor query point 
idea point location triangulation equivalent nearest neighbor searching 
bucketing algorithm finds nearest neighbor point constant expected time certain distributions points reduce total expected cost point location steps log time time maintaining relative simplicity incremental algorithm 
bucketing scheme places sites uniform grid adds diagram 
points fall bucket inserted kept discarded 
find near neighbor point location algorithm finds bucket query point lies searches outward spiral non empty bucket 
uses edge incident point bucket starting edge normal point location routine 
spiral search fails find point certain number layers point location routine continues arbitrary edge current triangulation 
bucketing scheme uses dynamic hash table deal fact sites processed line fashion 
scheme bother store points fall particular bucket just stores point seen 
bucket structure provide insertion algorithm true nearest neighbor query find point close query 
sense extra space information need 
small constant choose 
point location maintains bucket table average sites fall bucket 
line maxl site insert diagram bucket current grid new grid size re bucket points maxl spiral bucket layer layer maxl bucket layer point bucket return edge layer return null edge spiral edge null edge arbitrary edge locate edge incremental algorithm spiral search point location 
spiral search near neighbor 
expanding size bucket grid periodically see 
modify locate search bucket structure near neighbor edge incident neighbor starting edge standard edge walk 
search procedure starts bucket new site lies searches spiral pattern searched maxl layers see 
hasn non empty bucket point gives locate proceeds arbitrary edge 
assuming point keeps track incident edge edge modified point location routine shown 
bucketing procedure sure expected number points bucket 
variable maxl sure routine looks log layers buckets 
hard show expected cost point location constant points uniformly distributed 
probability point falls particular bucket probability particular bucket empty gamma gamma gammac cost point location algorithm query point proportional number buckets examined spiral search number edges examined locate step spiral search 
cases consider 
spiral search may succeed cutoff point examining buckets log layers 
referring picture analysis bentley weide yao says expected number points dashed circle bounded 
expected number edges area bounded times expected number points 
expected number edges locate examines 
total cost procedure case bounded log gammac gamma spiral search fails cost locate procedure operations 
algorithm look log buckets spiral search fails probability happening bounded gammac log expected cost operations 
cost maintaining buckets floor function assumed constant time 
insert points random order analysis guibas knuth sharir gks shows expected number edge flips algorithm 
total expected run time algorithm time points uniformly distributed unit square 
important principles bottom search bucket structure current triangulation takes advantage local nature nearest neighbor search dictionary facet half space pairs 
find initial cell facet half space defined include interior insert iteratively advance front empty 
empty element site search search site complete unknown cell 
new cell 
delete vertex facet defined half space defined containing facet search finished insert mark finished algorithm cost adapting bucket grid points added amortized construction process 
resulting algorithm slightly complicated original incremental algorithm see practical situations nearly fast methods seen far 
incremental search class incremental algorithms construct delaunay triangulation incrementally discovering valid delaunay triangles time 
dealing dimensional problems convenient generic terminology dimension 
dwyer dimensions algorithms assume set sites spherical 
case delaunay triangulation partitions convex hull simplices call cells cell contains site 
dimensional simplices called facets 
plane cells triangles facets edges circumcircle triangle empty sites circular 
shows pseudo code incremental search algorithm call algorithm algorithm constructs simplex diagram proceeds extend diagram adding new cells adjacent known facets time 
algorithm works keeping queue facets half spaces 
facet joins cells call facet finished cells discovered algorithm 
algorithm stores facets dictionary flag indicating finished 
time new cell discovered unfinished facets belong cell placed queue 
queue represents front advances point set new cells discovered 
plane facets edges cells triangles 
algorithm starts triangle incrementally finds new triangles 
advancing front just queue unfinished edges 
dimensions algorithm basically manipulates tetrahedron faces 
performance algorithm determined time needed facet search time needed site search 
facet dictionary stored hash table expected cost maintaining dictionary time 
data structure critical performance algorithm supports site searches 
section describe reasonable solution problem case site uniformly distributed unit square 
order generalize algorithm classes inputs higher dimensional problems need replace site search routine appropriate 
site search dwyer algorithm uses relatively sophisticated algorithm implement site searching 
sites placed bucket grid covering unit square 
search begins bucket containing midpoint 
bucket searched neighbors placed priority queue controls order buckets examined 
buckets ordered queue measure distance initial bucket 
buckets placed queue intersect half plane right intersect unit circle 
correct site closer edge buckets queue queue empty algorithm terminates 
dwyer analysis shows total number buckets algorithm consider 
site search algorithm variant spiral search 
new site searching outward unfinished edge spiral pattern 
assume sites chosen uniform distribution unit square dwyer bucket grid unit square help speed search process 
points data structure uniform grid theta buckets constant 
generally choose tune performance particular implementation 
simplify analysis assume 
shows high level description site search routine 
routine computes mid point edge begins looking new site bucket midpoint falls 
algorithm checks site picks forming smallest circle 
algorithm expands range search include boxes surrounding expansion continues range boxes searched algorithm contains bounding box current best circle contains intersection half plane right unit square 
correct site connect shown edge boundary convex hull summarizes details algorithm 
site search routine differs dwyer ways 
main difference lack priority queue control action search routine 
dwyer algorithm careful inspect buckets left outside unit circle 
case prove algorithm 
site search somewhat looking extra buckets 
algorithm examines bucket intersection unit square bounding box final circle discovered 
possible search larger area intermediate circle larger final 
site searches near center unit square search area expands uniformly directions 
site searches near edge unit square search half space right new site 
circle circle centered radius sqrt radius square circle dist square distance center right right segment new circle circle box box centered extending layers left layers right site search queue empty midpoint bucket squared distance circle half plane right plane box intersection unit square nil search box done foreach unseen bucket bucket intersects foreach point bucket right dist new circle nil done contained plane box done bounding box contained done expand code site search 
example bounding boxes site search algorithm examines 
ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl site search algorithm algorithm needlessly expand area non profitable direction see 
advantage scheme tuned case sites distributed unit square avoids extra overhead managing priority queue especially avoiding duplicate insertions 
illustrates operation site search 
top left frame site search algorithm initialized find unknown site bold edge 
site lies left edge 
top right frame shows candidate site resulting circle circle site search 
box frame area searched far 
bottom left frame site search expands search area finds second candidate site circle 
box shows algorithm searched area large prove second circle empty 
new edges shown bold bottom right frame added queue 
operation algorithm reminiscent fortune algorithm totally different methods control evolution expanding front 
discussion algorithms literature including ones due dwyer maus mau basic idea algorithm differ details dwyer algorithm formally analyzed 
dwyer analysis assumed input uniformly distributed unit ball extending analysis unit cube appears non trivial 
plane difference great experiments section show empirical runtime algorithm appears 
empirical results order evaluate effectiveness algorithms described study implementations algorithm 
rex dwyer provided code divide conquer algorithm steve fortune provided code sweepline algorithm 
implemented incremental search construction algorithms 
implementations tuned machine dependent way compiled gnu compiler timed standard unix tm timers 
performance data section gathered instrumenting programs count certain costs 
understanding algorithm profiling information test runs determined important monitor 
algorithm tested set sizes sites 
trials sites uniformly distributed unit square run size graphs show median sample runs box plot summary samples size 
box plots dot indicates median value trials vertical lines connect th minimum th percentile maximum 
programs include code generate simple animations operation 
code outputs simple graphics commands interpreted interactive tool windows perl program generates input various typesetting systems 
movies helpful gaining intuition detail operation various algorithms debugging easier 
movies chapter generated automatically animation scripts 
performance incremental algorithm performance incremental algorithm determined cost point location number circle tests algorithm performs 
standard incremental algorithm spends time doing point location bucket point location routine effectively removes bottleneck 
compares performance algorithms uniformly distributed sites 
plots show standard incremental algorithm uses average comparisons point location step 
simple regression analysis indicates number comparisons point grows plotted curve data 
curve close fit experimental data 
adding point location heuristic improves performance incremental algorithm substantially 
apparent number comparisons site bounded constant near 
comparisons tests done second phase algorithm spiral search 
tests take longer simple comparisons needed spiral search 
spiral search accounts total number comparisons contribution runtime point location algorithm significant 
cost point location depends log odd 
easy explain remember algorithm re buckets sites power 
power average search time dips extra re bucket step reduces average comps site buckets number sites ffl ffl ffl ffl ffl ffl ffl ffl ffl comps site buckets number sites site location vs bucket density comps site bucket density ffl ffl ffl ffl ffl ffl ffl comparison point location costs 
circle tests site number sites incremental dwyer circle tests site algorithms 
bucket density phases construction process 
input size moves power bucket density increases steadily 
cost point location see 
number comparisons needed point location depends average density sites bucket grid 
density points buckets high point location routine waste time examining useless edges 
hand low algorithm waste time examining empty buckets 
shows dependence density cost point location 
graph shows average cost point location trials ranging 
data timing runs 
graph shows large variation number comparisons needed site actual effect runtime algorithm 
runtime incremental algorithm depends circle tests performs 
algorithm inserts points random order analysis guibas knuth sharir gks shows total number circle tests 
sharir yaniv sy tightened bound 
shows analysis remarkably accurate 
shown plot cost dwyer divide conquer algorithm 
algorithm primarily circle testing sense compare way 
plot shows dwyer algorithm performs fewer circle tests incremental algorithm 
profiling programs shows circle testing roughly half runtime dwyer algorithm run roughly percent faster mine 
incremental algorithm quad tree ohya iri describe modification incremental algorithm buckets points algorithm inserts points breadth traversal quad tree 
authors claim algorithm runs expected linear time sites uniformly distributed provide experimental evidence fact 
order show algorithm competitive better similar methods implemented comps site site location number sites circle tests site edge flipping number sites performance quad tree algorithm 
algorithm compared performance mine 
profile program showed algorithm spends time testing circles locating points 
experiments monitored operations detail 
results show quad tree algorithm performs identically algorithm 
algorithm performance depends parameter tests set just provided best performance 
shows quad tree algorithm performs bucket incremental algorithm 
uniform case conclude algorithm performs slightly better complicated alternative 
incremental search algorithm determining factor runtime algorithm cost site search 
factored number distance calculations performed number buckets examined 
examined mean bucket tested edge see algorithm search contents new site 
summarizes behavior parameters tests 
figures show performance algorithm sites chosen uniform distribution unit square show show performance algorithm sites chosen uniform distribution unit circle 
reason looking distributions detail behavior algorithm heavily influenced nature convex hull input 
square distribution expected number convex hull edges logn san 
graph shows number distance calculations site stays constant test range number buckets examined decreases 
reflects fact number edges near convex hull point sets relatively small algorithm examines large number useless buckets site searches near convex hull 
apparent isn case sites distributed unit circle expected size convex hull san 
larger number edges near convex hull reflected fact number buckets site distance calculations number sites boxes site boxes examined number sites site distance calculations unit circle number sites boxes site boxes examined unit circle number sites performance algorithm comps site sweepline maintenance number sites comps site priority queue maintenance number sites cost fortune algorithm 
main factors determining performance algorithm needed maintain heap sweepline data structures 
algorithm examines increases dramatically compared earlier case 
sensitivity distribution input major weakness algorithm indicates adaptive data structure needed support site search routine 
pre computing convex hull point set searching hull edges inward may help minimize effect problem 
bucket data structure sensitive clustering point set non uniform distribution triangle sizes shapes 
best way fix problems replace buckets nearest neighbor search structure sensitive distribution sites perform non local searches efficiently 
bentley adaptive tree ben example data structure 
fortune algorithm runtime fortune algorithm proportional cost searching updating data structures representing event queue state sweepline 
fortune implementation uses hash tables purpose 
expect data structures perform uniform inputs 
fact small input sets algorithm run linear time 
shows performance sweepline priority queue data structures fortune implementation 
sites uniformly distributed direction bucket structure representing frontier performs exactly expect 
indicates search procedure performs comparisons point average 
main bottleneck fortune algorithm ends maintenance priority queue 
priority queue represented uniform array buckets direction 
events hashed coordinate placed appropriate bucket 
addition important realize circle events explicitly placed event queue 
site events stored implicitly initially sorting sites 
problem sites uniformly distributed resulting priorities 
circle events tend cluster close current position sweepline 
clustering increases cost inserting deleting events fortune bucket structure 
operation circle events cluster close sweepline fortune algorithm 
frame early run algorithm second frame run 
note cloud circle events signs moves sweepline 
requires linear time search list events particular bucket 
large buckets expensive 
regression analysis shows number comparisons point grows see 
watching animations large runs fortune algorithm provides heuristic explanation behavior 
sites uniformly distributed new site events tend occur close frontier 
new site causes circle event added priority queue chances circle large coordinate top circle priority new event close current position sweepline 
circle large priority resulting event far sweepline event invalid large circles contain sites 
eventually site circle event invalidate large circle replace smaller lies closer sweepline 
result clustering clearly observable animations 
behavior bucket data structure natural speculate different data structure provide better performance larger problems 
investigate possibility re implemented fortune algorithm array heap represent priority queue 
guarantees operation priority queue costs log time worst case array representation minimizes additional overhead 
test effectiveness new implementation performed extensive experiment 
algorithm tested uniform inputs sizes ranging sites 
shows performance heap data structure experiment 
line lg shows number comparisons maintain heap growing logarithmically plot shows detailed profile comparisons 
profile indicates performed extract min routine 
comparison insert delete relatively cheap 
behavior exactly opposite bucket structure 
actual heap significantly improve runtime algorithm 
compares runtime algorithms range inputs 
graph data point ratio runtime bucketing algorithm runtime heap base algorithm 
graph shows trials input sizes sites evenly spaced comps site cost heap number sites lg insert delete extract min time buckets time heap comparison heap buckets number sites cost fortune algorithm heap 
part shows breakdown cost heap represent event queue 
part plots ratio runtime old algorithm new 
intervals 
timings taken machine configuration section 
plot shows algorithms pretty identical performance points heap version starts dominate 
points heap version roughly better 
main reason improvement greater maintaining heap incur data movement overhead maintaining bucket structure 
bucket structure appears workstation cache effectively stays competitive doing 
interesting feature graph fact number comparisons periodically jumps new level stays relatively constant jumps 
due fact heap represented implicit binary tree 
number comparisons jumps periodically size heap near powers 
graph jumps occur powers average size heap run fortune algorithm 
prove start definition 
definition 
set sites suppose sweepline fortune algorithm height site coordinate say unfinished algorithm computed edges final delaunay triangulation incident site incident circle event queue unfinished convex hull sites uniformly distributed unit square expected size convex hull log bound expected size priority queue need estimate number unfinished sites 
lemma special case result due 
proof repeated slightly simpler form sake completeness kk 
lemma 
assume consists points chosen uniform distribution unit square 
site distance sweepline probability unfinished sectors empty delaunay edges imply empty regions plane 
gamma gamma proof 
uf denote condition site unfinished 
exists point sweepline final delaunay triangulation 
consider circle centered radius 
divide sectors internal angle 
consider square side diagonal 
endpoint diagonal 
dwyer proved exists site free circle contains triangles incident see 
circle contains sectors 
area sectors 
exists region area site free 
pr uf gamma gamma bounded gamma result estimate expected number unfinished sites sweepline reached height theorem 
assumptions expected number unfinished sites 
proof 
lemma show expected number unfinished sites rectangle height width bounded kk 
sweepline reached height number unfinished sites bounded 
course constant higher constant see practice 
unfinished site contribute circle events expected number circle events 
bottom line point course develop algorithm fastest runtime 
benchmarks algorithm run sets sites generated random uniform distribution unit square 
trial random number generator initial seed times identical point sets 
run times measured sparcstation mechanism unix 
graphs show user time real time inputs sets fit main memory generated main memory paging affect results 
graph fortune algorithm shows version heap buckets algorithm better larger problems worse smaller ones 
shows results timing experiments 
graph shows dwyer algorithm gives best performance 
fortune algorithm incremental algorithm large problems incremental algorithm better 
fortune algorithm worst largest problems due overhead priority queue data structure 
algorithm competitive simple bucket data structure uses site searching efficient 
dwyer algorithm faster fewer circle tests incremental algorithm 
profiling information shows algorithm spends half total time circle testing accounts fact dwyer program fifteen percent faster incremental 
incremental algorithm marginally slower dwyer simpler code 
addition line nature incremental algorithm useful applications 
quad tree order incremental algorithm slower algorithm dwyer algorithm somewhat faster 
recall algorithm performs roughly edge tests point location circle tests randomized incremental algorithm 
factors account consistently slower runtimes 
incremental search algorithm clearly slower algorithms 
clear constant factors runtime site search somewhat higher algorithms 
main problem bucket grid data structure efficient searching small ranges 
ideal role played randomized incremental algorithm local information needed 
algorithm data structure called search larger areas unit square performance degrades 
nonuniform point sets algorithms studied uses uniform distribution points advantage slightly different way 
incremental algorithm uses fact nearest neighbor search fast uniform point sets speed point location 
dwyer algorithm uses fact delaunay sites near merge boundary tend effected merge step 
fortune implementation uses bucketing search sweepline 
algorithm depends uniform bucket grid support site searching 
analysis experimental experience gained previously expect log operations dwyer algorithm sensitive pathological input 
line incremental algorithm perform operations average distribution sites totally defeated bucketing strategy 
seen fortune implementation efficient large problems input uniform 
input set extremely irregular priorities fell just buckets expect performance implementation worse incremental algorithm 
expect algorithm sensitive bad inputs performance closely coupled distribution sites bucket grid 
time site dwyer algorithm number sites time site incremental algorithm number sites time site fortune number sites time site quad tree number sites time site algorithm number sites comparison expected runtimes different algorithms sites chosen random uniform distribution unit square 
times microseconds 
name description unif uniform unit square 
ball uniform unit circle 
corners corner unit square 
diam gamma gamma cross points gamma gamma 
norm dimensions chosen 
clus points unit square 
arc circular arc width table nonuniform distributions 
order see algorithm adapts input study tests inputs nonuniform distributions 
table notation refers normal distribution mean standard deviation uniform distribution interval 
graphs show algorithm running different inputs sites distribution 
uniform distribution serves benchmark 
shows effect point distributions incremental algorithm 
expected point distributions heavy clustering corners normal stress point location data structures algorithm increasing point location costs factor 
represent worst case inputs data structures 
comparison tests point location routine substantially faster circle tests worst cases runtime didn increase factor 
sampling build adaptive data structure nearest neighbor search ben wei reduce problems substantially 
distribution input little effect number circle tests algorithm performs 
performance quad tree algorithm slightly erratic bucket incremental algorithm effect relatively minor 
summarizes performance fortune algorithm experiment 
graph shows bucket implementation event queue sensitive site distributions cause distribution priorities extremely nonuniform 
cross distribution happens near line 
point circle events associated sites near line cluster buckets near position 
corners distribution causes similar problem lesser degree 
events associated circles event queue tend stay clustered bucket 
cases non uniform distribution sites direction slows site searches frontier effect bad behavior event queue 
second graph shows performance heap erratic buckets 
small jumps appear due fact event queue larger smaller expected size distributions 
cost heap logarithmic size queue cause large degradation performance 
shows performance dwyer algorithm experiment 
dwyer algorithm slowed distributions cause algorithm create invalid edges subproblems delete merge steps 
effect particularly pronounced unif ball corn diam clus arc comps site site location buckets xxx xx xx xx unif ball corn diam clus arc circle tests site edge flipping buckets xxx incremental algorithm non uniform inputs 
unif ball corn 
diam clus arc comps site buckets fortune buckets uni ball corn diam clus arc comps site heap fortune heap fortune algorithm non uniform inputs 
unif ball corner diam cross norm clus arc circle tests site dwyer algorithm dwyer algorithm non uniform inputs 
run cross distribution group sites near line tall skinny creating worst case merge routine 
shows bad inputs affect algorithm figures leave worst inputs algorithm corners normal algorithm taken hours finish benchmark 
behavior algorithm inputs shown 
algorithm easily sensitive distribution input 
surprising depends essentially routine incremental algorithm uses point location seen point location subroutine performed badly bad inputs 
handicap incremental algorithm large degree point location routine major bottleneck algorithm 
performance site search largely determines runtime algorithm understand measures effect performance shows average runtime trials algorithm algorithm runtimes graph greater algorithm performance uniform case didn feel including graph add useful information 
graph reflects fact primitives algorithm uses incur wide range actual runtime costs 
looping buckets faster restructuring trees ccw tests cheaper circle tests 
costs algorithm vary widely different inputs actual runtimes 
notes discussion experiments chapter led important observations performance serial algorithms constructing planar delaunay triangulations 
observations summarized unif ball corn diam clus arc 
site distance computation unif ball corn diam clus arc boxes site boxes examined algorithm sensitive bad inputs 
run time secs worst case algorithm number sites corners clus algorithm cluster corners distributions 
unif ball corn diam clus arc secs site runtime comparison dw dw dw dw dw dw dw dw quad quad quad quad quad quad quad quad forb forb forb forb forb forb forb forb runtimes non uniform inputs 
quad incremental algorithms dw dwyer algorithm forb fortune algorithm buckets heap respectively 
ffl simple enhancement incremental algorithm results line algorithm competitive known algorithms constructing planar delaunay triangulations 
algorithm optimal expected runtime time simple implement runtime constants small 
particular algorithm simpler faster previous improvements incremental algorithm 
ffl performance new incremental algorithm dependent distribution sites extremely nonuniform point sets degraded performance significant degree 
ffl incremental search appear somewhat slower methods examined 
improved data structures site searching may help matters 
data structures definitely help decrease sensitivity algorithm non uniform inputs 
ffl sets uniformly distributed sites frontier fortune algorithm expected size edges 
expected size event queue algorithm 
ffl uniformly distributed sites circle events fortune algorithm cluster near move sweepline 
causes hashing priority queue implementation perform badly large inputs 
possible better hash function alleviate problem isn clear exactly implement idea 
ffl heap represent event queue fortune algorithm improves performance large problems small amount 
cost maintaining heap fortune algorithm incurred extract min 
reducing overhead heap algorithm efficient bucket clear achieve 
ffl dwyer algorithm strongest range problems 
consistently faster incremental algorithms arguably harder program 
addition incremental algorithm online extended handle dynamic problems efficiently 
algorithms experiments chapter cover available algorithms problem quite comprehensive 
consider divide conquer algorithm discussed guibas stolfi dwyer algorithm fast 
implement incremental algorithm guibas knuth sharir gks point location scheme slower mine inputs considered 
convex hull algorithm developed barber bar available soon full analysis chapter 
algorithm appears practical efficient robust subject studies 
principles design implementation incremental algorithm illustrates principles algorithm design general interest 
bottom search 
point location algorithm speeds near neighbor searches current delaunay triangulation augmenting data structure array buckets 
dynamic hashing 
bucket array point location dynamically adjusts size problem 
amortized cost adjustments linear size final diagram 
randomization analysis 
incremental algorithm takes advantage results analysis randomized incremental algorithms resilient poor point distributions 
performance incremental algorithm distributions experimental verification utility randomized incremental framework 
adaptive data structures 
randomizing input order incremental algorithm update procedure adapt gracefully non uniform inputs 
pathological input insertion procedure perform edge edge flips average 
performance incremental algorithm improved making point location routine adaptive highly clustered point sets 
variants bentley tree data structure ben effective way 
experiments studies chapter tools experimental analysis algorithms 
heavily influenced writings bentley ben ben ph dissertation mcgeoch mcg 
thesis new ideas context parallel algorithms design 
abstraction 
algorithms analyzed terms reasonably high level operations opposed low level machine operations 
maintaining abstraction easier obtain results independent particular machine implementation provides better insight basic behavior algorithm 
explicit cost models 
maintain abstraction unreasonable cost 
choose primitives carefully reasonably sure efficient target machines 
algorithms chapter depended arithmetic primitives circle easily implemented serial machines 
chapters parallel algorithms see low level cost models model performance simple algorithms accurately higher level cost models analysis complicated algorithms manageable 
pragmatic algorithms design 
worst case asymptotic complexity algorithm measure algorithmic efficiency 
especially true parallel algorithms constant factors play larger role 
studies chapter concentrated analyzing measuring expected complexity algorithm various input models uniformly distributed sites random insertion orders 
primitives cost models algorithms designer know parts algorithm critical bottlenecks analyze possibly optimize parts appropriately 
algorithm animation 
animations algorithms valuable tool gaining intuition behavior 
movies chapter illustrate basic ideas algorithm 
addition pictures frontier fortune algorithm motivated analysis size event queue 
simulation experiments 
chapter shown careful experiments data analysis visualization lead important observations performance algorithms 
mcgeoch outlined techniques thesis mcg 
chapter simulations took form instrumented programs implemented algorithms study 
provided way precisely characterize performance algorithms specific types inputs problem sizes 
similar techniques study implementations parallel algorithms 
thesis mcgeoch points simulation program need implement algorithm study long measure cost algorithm 
simulation shortcuts useful study parallel algorithms 
particular serial simulations parallel algorithms allows algorithm designer evaluate multiple design choices cost developing full parallel implementations target machines 
chapter models machines copying ancient models 
ho free blue clouds keen beauty great machine rupert chapter presents machines models design implement evaluate parallel algorithms 
section discusses parallel programming models currently popular 
section discusses weaknesses pram model section outlines programming model 
descriptions models informal operational 
formal discussions models semantics parallel computation outside scope thesis 
chapter presents implementations different machines cray mp vector multiprocessor ksr distributed shared memory multiprocessor 
machines represent major design points high performance computer architecture today 
cray conventional machine relatively small number large highly pipelined cpus connected high bandwidth centralized memory 
ksr relatively large number smaller cpus distributed memory system 
section give overview architecture programming systems 
multiple architectures illustrate conflicting points programming parallel algorithms 
show parallel algorithms certain extent independent machine implemented 
analysis implementation simple algorithm nearest neighbor search chapter illustrate explicit cost models accurately predict performance algorithms multiple architectures programming systems 
carefully matching algorithm architecture programming studying algorithm straightforward machines chosen blindly 
example chapter algorithms study constructing delaunay triangulation complex high level mechanisms concurrency control shared data management great utility 
mechanisms implementation algorithms straightforward concentrate algorithmic questions arcane programming tricks 
algorithms sense initial studies ksr provides mechanisms cray variety reasons multiprocessor programming difficult 
popular programming models main difficulties facing designer parallel algorithms wide variety programming models exist parallel architectures 
contrast conventional algorithms design ram model generally people think model reasonable approximation reality 
ram model succeeds models aspects algorithm usually dominate performance 
recall ram model processor connected infinite memory 
time step processor fetches data memory executes instruction writes result back memory 
clearly simplification reality ignores register usage cache misses virtual memory multiprogrammed operating systems host systems issues 
part efficient ram algorithms proven efficient actual machines 
theoretical computer science model choice studying parallel algorithms pram natural extension ram model 
pram model replicate single ram processor times postulate processors share infinite memory 
unit time processor execute step ram program strongest version model memory system resolves concurrent reads writes pre defined way 
practitioners parallel computing long pram model unrealistic pram machines 
point view somewhat misguided 
building shared memory abstraction top distributed memory system fertile area research architecture operating systems language design 
research commercial systems currently support shared memory programming model cf ek gw ksr wl addition data parallel sh programming models similar pram popular 
models single thread control combined parallel primitives specify operations large aggregate data structures vectors 
primitives fall classes elementwise arithmetic permutation vectors routing computing parallel prefix operations scans 
thesis ble blelloch argues model vector primitives results simpler algorithms easier analyze efficient pram algorithms kinds problems 
blelloch students shown algorithms primitives efficiently implemented simd vector architectures :10.1.1.44.9578
addition chatterjee shown suitable compilers programs primitives efficiently translated run shared memory mimd machines cha 
similar done languages don explicitly vector models 
projects fortran dino kmr crystal ccl proteus mnp ram algorithms asymptotically efficient simple low constant factors 
examples include quicksort delaunay triangulation algorithms previous chapter 
people call single program multiple data spmd programming model 
mimd concerned compiling data parallel programs 
fortran document gives summary research compiler systems 
compilers take shared memory programs possibly annotations generate code variety machines simd mimd shared memory distributed memory 
theory practice popular programming model parallel algorithms basically pram 
wrong pram pram similar popular parallel programming models pram results pram algorithms generally theoretical value 
answer lies pram algorithms seek achieve pram algorithms analyzed 
designers pram algorithms interested mathematical question determining asymptotic complexity problem opposed finding practical solutions problems existing machines 
designers aren interested practical solutions problems isn surprising analysis address questions relevant performance real machines 
lack interest manifests ways ffl importance placed nc 
particular algorithms fast runtimes example processors simply 
theoretical algorithms attempt concurrency assumption real machine simply run algorithm slowly simulating threads real processor 
problem approach simulation multiple threads come free added costs dominate runtime algorithm 
ffl complicated data structures scheduling techniques reduce parallel runtime basic algorithms 
results complicated algorithms simple things scans histogramming sorting load balancing 
canonical example cole pipelined parallel mergesort acg 
tour de force elegant algorithm design analysis contains beautiful ideas algorithm complicated uses global data movement simply compete simpler algorithms radix sort ble nat 
problem asymptotics restricted pram analysis 
theoretical algorithms mesh machines hypercubes network models ram model equally unrealistic 
doesn mean ideas algorithms useless 
contrary examined pragmatically papers theoretical algorithms hiding simple elegant highly practical ideas beneath asymptotics 
user parallel machine interested practical question solve problem faster exploiting concurrency available machine problem sizes relatively small asymptotic standards 
situation sequential algorithms design real world algorithms deal machine constraints don exist ram model 
careful analysis implementation experimentation needed obtain best performance 
researchers theoretical computer science content study questions pram model pram algorithms seen touch reality 
model data parallel model design new algorithms geometric problems outlined chapter 
model mixes blelloch vector model pram model machine pram augmented special vector instructions 
vector instructions specify aggregate operations scans conventional pram code local computation 
pseudocode mix conventional code parallel loops parallel primitives 
parallel loops marked foreach 
appropriate code blelloch vector primitives 
particular scan plus stands prefix sum computation pack moves flagged items source array front destination array returns number items destination array ble 
formally model assume processors machine synchronize parallel loop vector instruction 
informally know routing scans may require synchronization local computation 
assume processors loosely coupled execute independently tightly coupled synchronize fairly quickly 
assumption matches characteristics currently available machines 
complexity algorithm model measured statistics 
step complexity algorithm denoted number parallel steps algorithm needs solve problem size complexity measures total performed algorithm 
calculate complexity keeping track length vector arguments vector operation done active threads executing parallel loop 
sum values history program execution 
goal design parallel algorithms low step complexity complexity runtime sequential algorithm 
cases concentrate worst case time part deal expected case runtimes 
problems interested solved linear expected time find parallel algorithms run time processors linear expected 
chapters describe algorithms 
order achieve goal performance model assigns explicit costs class primitive operations 
example machine level parameters cost local arithmetic 
cost routing 
cost scans number cpus available constants model element time needed perform vector operations particular machine 
measure costs experimentally reasonable implementation primitives machine 
chapter gives concrete example define low level models 
complicated algorithms may appropriate higher level cost model 
example algorithms previous chapter analyzed respect calls point location primitives 
operations broken sequences low level calls higher level abstraction algorithm easier analyze sacrificing accuracy provided pick primitives carefully 
chapter uses style analysis design parallel algorithms constructing delaunay triangulation 
cost models keep algorithm designer aware machine dependent issues forcing address stages implementation 
algorithms tuned evaluated substituting costs real machine analysis transforming code needed obtain better performance 
parallel sorting approach develop high performance sorting algorithms architectures blelloch connection machine cm blm blelloch zagha cray mp zb hightower prins reif maspar mp phr show algorithms described high level maintaining accurate estimate performance real machines 
papers algorithms analyzed terms model assigns explicit costs primitive operations 
straightforward analysis algorithms resulted performance models accurately predicted speed algorithms practical problem sizes 
algorithms thesis extend applicability methods parallel geometric algorithms heavily studied practical setting 
models number proposed programming models parallel algorithms huge 
section briefly survey attempts define realistic models parallel computation consider detail dissertation 
survey meant comprehensive look available literature 
meant illustrate general directions researchers taken tackling problem 
consider network models suitable practical study algorithms inherently machine dependent 
evidence tuning algorithms specific network architectures necessarily result better performance 
example fastest sorting algorithms blelloch study uses hypercube network connection machine cm direct way blm 
addition new commercial machines allow programmer directly control routing messages communications network 
structure communications network largely hidden programmer 
extensions pram model rest literature area 
models extends pram model deal potential bottleneck performance parallel algorithms 
leiserson maggs consider limited network bandwidth lm 
aggarwal chandra snir study communication latency locality acs pram computations 
alpern carter consider hierarchical memory systems uniprocessors acf multiprocessors hr 
aggarwal vitter acg vitter shriver nodine vs nv studied design basic algorithms level memory disk hierarchies 
discusses implementation basic scientific libraries memory disk hierarchies cormen thesis cor presents strategies implementing virtual memory systems data parallel computers 
cole cz consider cost synchronization pram algorithms gibbons gib 
valiant val culler break away pram model propose programming models point point communication primitives costs 
valiant bsp model allows processors execute asynchronously models communication latency limited bandwidth 
culler logp attempts reflect current trends technology construct parallel machines 
model asynchronous uses parameters model network latency communication overhead communication bandwidth size machine 
models remove pram model simplifying assumptions 
theory models reflective practical situations 
analyzing algorithms models complicated 
reflected fact basic primitives summation permutation list ranking ffts matrix arithmetic received attention models 
acf gib vs 
addition study graph algorithms acs sorting nv vs 
purposes extra complexity main reason models analysis 
algorithms study complicated irregular dynamic understood 
addition models accurately reflect mechanisms available programmers current parallel machines 
asynchronous pram atomic operations fetch add compare swap queue locks available major commercial multiprocessor mechanism 
logp models point point communication remote data access consider effects caching protocols 
result addition making analysis difficult models difficult program machines meant abstractions 
high level data parallel model chosen gives freedom ignore machine details important keeping conscious machine level costs primitives algorithm 
find algorithm performance severely hindered particular bottleneck turn detailed models study problem isolation 
machine descriptions sections short descriptions machines studying parallel algorithms thesis 
study architectures chosen representing major design point current architectures 
cray represents traditional modestly parallel heavily pipelined architecture large centralized memory 
ksr represents new breed machines available parallelism physically distributed memory system 
motivation choices illustrate utility able design high level algorithms machine independent way practical issues involved translating high level algorithms efficient machine dependent code 
purposely avoided machines force programmers message passing programming style 
current message passing systems difficult define utilize large distributed data structures 
software hardware mechanisms available help programmer manage data shared multiple processors 
machines centralized memory don need mechanisms cache coherent machines ksr provide hardware 
addition large cache coherent systems built top message passing hardware provide higher level programming model reasonable cost 
programmer really wishes construct message passing programs possible shared memory constructs 
memory memory registers schematic diagram cray mp 
cray mp cray mp vector multiprocessor 
head mp large pipelined cpu 
cpu execute kinds instructions 
scalar instructions correspond instruction set conventional architecture vector instructions take maximum advantage cpu pipelined functional units 
functional units floating point operations logical operations memory operations 
functional units take input write results vector registers hold words data 
fixed startup cost arithmetic logical functional units return result clock tick see 
performance memory unit depends access pattern particular instruction 
cray memory system divided banks 
banks grouped sections containing subsystems hold banks 
section total banks memory 
memory system interleaved consecutive words memory live different banks 
long access pattern avoids strides particular strides multiples memory system deliver word clock tick initial startup time 
addition memory unit supports general routing scatter gather instructions 
instructions efficient access pattern causes bank conflicts 
compilers cray accept programs written fortran compile time analysis directives find loops iterations executed independently 
loops called vectorizable transforming program vectorizable form called vectorizing program 
vectorizable loops partitioned multiple cpus mp 
cray provides automatic tools achieving 
long careful write cray code certain style cray compilers reasonable job obtaining machine performance 
vectorize class parallel instructions model 
consider vector register element virtual processor thread control 
effect virtual processor provide way principle translate shared memory program vector form 
course translations may efficient 
full cray mp cpus vector registers elements long 
gives virtual processors 
machine cpus total virtual processors 
local arithmetic instructions vectorize local mem 
data cache cpu schematic diagram ksr trivially 
handle routing scatter gather instructions 
scans vectorize clever algorithm blelloch chatterjee zagha ble zb 
ksr ksr distributed shared memory multiprocessor 
physical memory machine distributed processors ksr provides programmer globally shared virtual address space 
uses sophisticated cache coherency hardware support programming model 
thread generates virtual address data currently live processor thread executing ksr memory system fetch data transparently remote processor 
algorithm locality cost fetches amortized local operations 
performance algorithm limited speed memory system 
critical design algorithms access memory localized low contention patterns achieve performance machine see 
node ksr fairly conventional cpu mb memory 
processor processing units executes memory control instructions integer instructions fpu floating point instructions connections network 
ksr network hierarchy ring networks 
ring nodes top level ring sub rings total processors 
ksr virtual address space broken byte units units called sub pages 
term block mean sub page 
process generates virtual address ksr memory system searches local cpu data cache cpu local memory correct block 
block available locally cache controller puts request level ring 
processor ring block fetched brought back requesting processor 
level hierarchy searched 
memory hardware uses invalidate protocol keep cache blocks consistent provides atomic operations locking unlocking blocks 
fortran compilers available ksr 
comes runtime library vector operation clocks elem elements add multiply divide add scan random permute reverse perm identity perm table costs vector operations processor cray mp supports multiprocessing multiple threads 
libraries provide placement scheduling threads mutual exclusion condition variables barriers 
fortran system facilities automatically parallelizing loops isn flexible useful algorithms 
implementations threads library directly depending parallelizing compiler 
benchmarking section presents simple benchmark results low level vector primitives 
results detailed analysis chapter 
cray low level primitives model corresponds simple loop benchmark primitives cray study performance code processed cray vectorizing compiler 
appropriate directives compiler vectorize primitives scans 
operations set library routines developed cmu blelloch group :10.1.1.44.9578
loops tested input vectors elements 
timings taken library function keeps track user time clock ticks 
results reported cpu cray mp 
table shows performance primitives 
parameter asymptotic cost vector element vector length half asymptotic performance achieved 
values derived experimentally squares fit benchmark data 
slope line absolute value coordinate intercept line 
standard benchmark metrics vector computers 
table shows representative values classes instructions model 
division expensive forms arithmetic avoid algorithms 
performance permute operation depends structure permutation 
table shows cost identity reverse random permutations 
algorithms assume permutations random cost routing clocks element 
algorithms discuss valid assumption may general 
time element secs elementwise add number elements performance elementwise vector addition ksr 
ksr test performance primitives ksr implemented versions primitive operations ksr threads library 
code ported runtime library chatterjee vcode compiler encore multimax cha 
code compiled optimization optimization introduced bugs parallel programs difficult track 
turning optimization probably improve absolute performance somewhat relative speedups remain 
primitive tested vector lengths threads 
test run times collecting sample values thread 
results reported mean sample set 
take approach ksr performance monitor registers replicated processor machine lacks central clock 
sense collect sample time thread 
graphs show confidence interval left graphs unreadable 
shows performance elementwise addition ksr 
graph shows mean runtime samples thread 
number threads experiment plot symbol shown confidence interval sample set 
graph cleanly split regions depending vectors fit local data cache processor 
local cache overflows runtime test increases dramatically 
table shows time element elements curves 
table shows cache overflow effect pronounced increase number processors participating computation 
sense processors total cache 
shows performance add scan ksr 
plot split separate scales relative performance large vectors easier read 
addition confidence intervals left hand plot provided useful information 
cost scan twice cost elementwise addition consistent number threads table cost elementwise addition ksr 
time element secs add scan small vectors number elements time element secs add scan large vectors number elements performance parallel prefix sum ksr 
time element secs unstructured permute number elements performance unstructured permute ksr 
earlier results cray 
scan routine show cache overflow effect involves vectors source destination 
ksr data cache way set associative avoids interference effects saw simple arithmetic benchmark 
scan routine structured local vector sums global sum 
program global sum performed sequentially master thread incurs large amount overhead large numbers threads 
overhead inhibits speedup somewhat threads example 
replacing sequential routine parallel tree sum allow scan algorithm scale gracefully cha 
examine performance permute 
cray runtime routine depends heavily structure permutation 
test reversal random permutations 
performance ksr identity permutation nearly addition benchmark benchmark memory bound 
reversal permutation representative relatively structured communication pattern random permutation places highest stress ksr memory system 
shows cost performing unstructured permutations ksr 
graph readable times threads left 
lack locality communication pattern inhibits effectiveness multiple threads benchmark 
small numbers threads overhead passing cache blocks processors dominates runtime program little advantage gained 
larger numbers threads eventually pay efficiency stays low 
performance ksr reversal permutation better permutation locality random 
shows operation expensive elementwise addition scans 
linear regression estimate asymptotic values data benchmark experiments obtain equations units microseconds time element secs vector reversal number elements performance reverse ksr 
may values caution valid specific configuration machine relatively large problems 
useful analysis chapter useful general parameters accurately modelling programs 
particular small problems startup costs system overhead routines incurs distort timing results produce inaccurate answers 
performance ksr benchmarks shows primitives model efficiently implemented architectures different traditional vector simd machines 
performance primitives predictable cray 
furthermore natural program ksr terms large memory memory vector operations see relating performance primitives real programs machine requires care cray vector style programming fits machine naturally 
summary chapter programming complexity model parallel algorithms 
model set primitive operations act large aggregate data structures vectors 
addition discussed alternatives model rationale choosing data parallel models available 
chapter closed presenting benchmarks results show primitives model implemented target machines 
chapter demonstrate benchmark results analyze implementations relatively simple algorithms accurately 
chapter concurrent local search toy problem ai deliberately oversimplified case challenging problem investigate prototype test algorithms real problem 
jargon file chapter saw simple bucket data structure speed sequential algorithms constructing delaunay triangulation 
buckets capture locality inherent delaunay triangulation construction problem provide simple way exploit locality obtain performance 
chapter show exploit locality parallel algorithms concurrent threads searching local area shared data structure 
illustrate technique call concurrent local search parallelizing spiral search algorithm chapter solve nearest neighbors problem 
sections describe algorithm show implemented target machines 
analysis implementation algorithm illustrate usefulness parameterized cost models developing simple algorithms multiple architectures 
problem recall set points 
wish find point gamma fsg closest euclidean metric 
corresponds original definition problem algorithms assume input set points uniformly distributed plane 
discuss adapt algorithms handle non uniform inputs 
algorithm algorithm parallelization bentley yao spiral search algorithm 
algorithm starts bucketing points uniform grid cells 
call parameter cell density 
bucketing phase expected number points fall cell different cell densities tune performance algorithm particular environment chapter 
simplify analysis chapter assume 
find nearest neighbor query point algorithm finds bucket lies searches outward spiral non empty bucket searches buckets intersect circle radius distance point query point processed way queries answered 
bentley yao show inputs large class distributions algorithm searches constant number buckets point average 
algorithm solves nearest neighbors problem expected time 
solve problem parallel parallelize spiral search 
build bucket structure parallel execute search steps parallel 
locality inherent spiral search allows algorithm exploit high level concurrency 
search task examines small part shared data structure tasks parallel fear conflict 
section describe parallel vector version algorithm 
addition provide running analysis implementation cray mp 
see algorithm minor modifications exhibits performance ksr 
illustrate original algorithm designed shared memory vector machine mind doesn depend features performance 
nearest neighbors cray cray algorithm runtime expressed terms clock ticks vector element cpu 
large vectors random permutations 
cray vector machine add model parameter reflect number elements vector register 
measure amount parallelism cpu emulate 
time algorithm take advantage multiple cpus 
algorithm fully vectorizable take advantage multiple cpus automatic parallelization facilities cray compilers 
step vectorize bucketing phase algorithm 
isn hard 
compute histogram keys know list nodes allocate ship points correct places bucket structure see 
parallel procedure array keeps track sites fallen bucket 
scan operation computes element nodes falls bucket 
second loop fills nodes pointers sites belong bucket 
second loop buckets contain index site bucket contain index site bucket 
sites contained bucket stored nodes buckets nodes gamma see 
main problem implementing code dealing write collisions elements array 
detect collisions manner 
initially virtual processor holding value write array dest 
virtual processor writes processor number array called test 
part 
vp examines test compares value id virtual processor reads id back test safely write dest part 
routine processes remaining vps sequentially part 
analysis follows show uniformly distributed sites average number collisions small serial part bucketing loop bottleneck 
see cost code tc arithmetic operations compute array indices incremental histogram lasts write table entry 
routing operations come sites array sites nodes array list nodes bucket index buckets array buckets histogram foreach index sites buckets scan plus loop assumes write collisions array taken care processors get correct index nodes 
foreach nodes parallel loop bucket points 
loop point hashed coordinates size bucket computed 
scan plus operation computes starting position bucket nodes array 
points routed destinations second loop 
sites nodes buckets bucket data structure 
fetches fetches fetch nodes 
extra counts cycles needed deal possible collisions 
term accounts cost pack append operations 
append operation implemented scan plus block copy statically allocated array holding write queue 
accounts cycles 
accounts pack 
conservatively assume need pack append iteration 
tc average cost sequential code routine 
collision arithmetic memory write take roughly cycles cray expression cycles total number collisions 
long small compared routine run cycles site average 
birthday paradox see expected number collisions group sites gamma clr 
average total number collisions gamma gamma conclude long small compared bucketing routine perform sites uniformly distributed 
bucketed points bentley weide yao spiral search technique nearest neighbor searching see 
query point spiral search divided phases 
phase find bucket point lies search outward spiral find non empty bucket 
calculate distance query point point bucket 
second phase compute nearest neighbor query point examining point lies bucket intersecting circle radius centered simplicity algorithm checks bucket intersects square side centered algorithm searches layers buckets centered bucket falls 
lies bucket layer buckets away defined buckets sigma gamma sigma gamma main obstacle vectorizing algorithm searches may take longer careful load balancing keep virtual processors busy 
straightforward way create virtual task query point 
task isn really task operating systems sense keeps state necessary perform spiral searching 
search algorithm task search step check see finished 
uses call pack delete finished tasks iterates process tasks finished 
scheme uses small amount overhead ensure search task extra 
parallel algorithm original sequential algorithm overhead due pack 
algorithms expected number iterations loop constant total overhead due pack 
total expected done parallel algorithm algorithm 
load balancing scheme complicated fact spiral search loop nested structure 
outermost loop iterates layers buckets spiral search examining 
second level loop iterates buckets lie layer inner loop iterates points lie bucket 
inner loop actual distance computations 
dest dest virtual processors numbered foreach offset write queue offset part virtual processor writes id reads back check collisions foreach offset dest test dest part detect collisions go ahead foreach test dest dest part put indices queue sequential processing 
element test zero num zero pack test append write queue batch writes 
offset offset length write queue dest write queue dest write queue phase loop handle implement permute operation allows collisions destination vector 
loop processor writes unique id destination looks see write successful 
finds processor written location writes result value location 
case increments destination 
third loop processors find id destination handled sequentially collided processor 
spiral search find near neighbor 
layer foreach prepare layer 
layer foreach done task finished 
pack done outer loop spiral search algorithm 
outer loop search walks layers buckets 
layer algorithm delete tasks finished 
call pack iteration see 
time algorithm moves new layer task checks see done pre calculates information bucket loop 
cost calculations 
general task survives layer point initial stage spiral search algorithm gamma layers away query point 
means gamma layers buckets empty 
probability bucket empty gamma gammac probability gamma layers buckets empty gammak 
algorithm deletes half current tasks time moves new layer total expected time needed bookkeeping cycles point 
loop level walks buckets layer see 
time algorithm moves new bucket virtual processor updates bucket index boundary check participates call pack 
task walked buckets deleted point 
takes time 
algorithm distance calculation bucket expected loop overhead walking buckets nd time average number distance calculations point performs 
calculate value 
inner loop algorithm distance calculation query point layer bucket layer foreach temp boundary check bucket 
done boundary check failed 
bucket done done bucket empty 
pack done pack temp temp done temp second outer loop spiral search algorithm 
bi foreach check nodes buckets bi done bi pack done pack done bi inner loop spiral search algorithm 
member bucket 
distance computation algorithm looks tasks finished bucket packs away 
bentley weide yao show express sum number cells searched stage 
exact estimate number 
coefficients exponentially decreasing suffices consider just terms sum accurate approximation 
cell empty algorithm searches cell cells 
cell empty search cells layers point 
expected number distance computations gamma gammac ce order show negligible consider integral gammax dx gammax gamma gamma fi fi set consider function gamma gamma gammab converges gamma 
conclude small compared terms sum describing sum tells algorithm performs approximately distance calculations site 
distance calculation increment check index load appropriate site perform inner product calculation update running minimum 
takes add overhead load balancing total expected time works cray cycles site site 
value conclude outer loop algorithm uses cycles point 
putting algorithm cycles point average solve nearest neighbor problem 
measurements implemented algorithm mixed calls blelloch chatterjee zagha assembly language routines pack scan plus :10.1.1.44.9578
code runs sun workstation proved invaluable aid modifying debugging line cray 
addition implemented version bentley yao original algorithm 
comparing vector algorithm sequential obtain realistic idea efficient vector algorithm algorithm spends time doing distance computations logical parameter check number distance calculations algorithm performs 
shows number distance computations point implementations performed function size problem 
graph shows summary runs inputs size 
dots graph represent median values trials vertical lines connect third quartile values minimum maximum values respectively 
shows average number distance calculations performed point bounded constant near 
close value predicted analysis 
sequential algorithm performed fewer calculations computes distance point vectorized algorithm avoids extra conditional check expense comps site serial number sites comps site vectorized number sites distance computations site algorithm 
time site secs number sites time point spiral search algorithm sparcstation microseconds 
clocks site serial number sites clocks site vectorized number sites time point spiral search algorithm cray ns clocks 
extra computations 
cray tradeoff conditionals expensive inside vectorized loops 
shows runtime scalar algorithm fast workstation 
timings taken sparcstation mb memory gcc 
graph shows run time algorithm tops point problem sizes tested 
obtaining asymptotic performance model code proved difficult due cache effects 
note runs smaller problem sizes display linear run time problem big effective workstation cache 
fact large problem sizes regression model runtime data indicates expected cost code roughly proportional log shows fit accurate large small values shows performance scalar vector algorithms cray mp 
code compiled cray standard compiler highest available scalar vector optimization levels multitasking 
cray run times units ns clock cycles 
shows scalar code cray uses cycles point 
shows performance vectorized algorithm processor cray mp 
squares regression indicates asymptotic run time algorithm cycles point point 
times faster scalar time cray times faster run time workstation times faster time originally reported bentley weide yao algorithm implemented pdp 
extensions applications current algorithm performs uniformly distributed point sets performance non uniform sets suspect 
bentley weide yao show sequential implementation performs points non uniform 
check results examine effect non uniformity vectorized algorithm ran set trials clustered inputs 
clusters generated formulas comps site comparisons number sites clocks site run time number sites performance vectorized algorithm clustered inputs 
normally distributed chosen random delta delta delta 
set scale normal model mild clustering 
shows results experiments 
figures indicate mild clustering slows algorithm factor consistent earlier results reported bentley 
severe clustering degrade performance way similar behavior point location algorithm chapter 
point sets extremely non uniform add pre processing phase algorithm attempts adapt bucket grid distribution input 
phase small sample input define non uniform bucket grid subdivide buckets uniform way 
sample predictor real distribution points distribution points new bucket structure smooth 
weide shows technique context conventional algorithms parallel algorithms random sampling similar way blm gg rs 
discussion analysis algorithm accurate upper bound real performance program 
actual run time algorithm better analysis predicted 
trace discrepancy incorrect assumptions analysis 
assumed call pack cost scan routing operation cycles element 
fact assembly language routine pack overlaps scan routing operation costs cycles element 
second analysis predicts number iterations inner loop point 
actual constant 
re calculate cost algorithm new values get cycles point little fast 
cray compiler didn really vectorize bucketing code 
fully vectorize loop clear loop vectorized collision resolution technique shown 
bucketing routine run factor slower analysis predicted small effect routine large percentage total run time 
discrepancies illustrate inherent difficulty exactly modeling behavior complete cray system compiler hardware 
result parts resulting program somewhat faster analysis predicted parts somewhat slower 
performance cray implementation shows analysis accurate 
satisfying constants analysis dependent cray architecture reassigning costs primitives easily redesign algorithm machines 
section discuss detail 
critical assumption necessary algorithm perform size input larger amount parallelism available machine 
critical keeping processors busy guaranteeing load balance 
algorithm needs large processor memories hold problems big effective 
really problem current generation parallel architectures processor memories large handle big problems 
performance algorithm compares performance linear time sequential algorithm better 
analysis experimental data provided cray performance measurement tools show inner loop memory bound doing indirect memory operations order perform arithmetic operations 
algorithm uses memory bandwidth unit useful 
key speeding algorithm possible simplify data structure uses indirection simplify structure parallel loops remove overhead possible 
implementation ksr ksr implementation algorithm follows basic outline cray uses concurrent threads perform parallel vector pipelines 
example bucketing stage algorithm constructs linked lists points bucket directly simple locking protocol histogram scan routine cray see 
thread local pool nodes allocate list elements 
private counter keeps track free node 
algorithm frees nodes sophisticated memory management routine needed 
private counters avoids large amount contention shared counter incur 
code somewhat different style traditional data parallel code 
particular uses asynchronous programming style generally associated data parallelism 
implementations ksr characteristic see perfectly possible analyze algorithms data parallel framework defined chapter 
need reasonable performance drives choice programming style 
global synchronization expensive operation mimd multiprocessor perform reasonable execute traditional synchronous data parallel program directly ksr 
program transformed larger asynchronous pieces 
transformation may done automatically compiler see example chatterjee thesis cha time compiler exists production environment 
ksr programs may thought sophisticated compiler produce asked translate data parallel programs efficient code mimd multiprocessor 
model performance code benchmark numbers local arithmetic permutation routing chapter 
obtain lock thread fetch cache block shared global counter private int foreach point idx bucket index lock buckets idx head buckets idx new nodes new head nodes new buckets idx new unlock buckets idx threads code bucketing points 
containing lock variable obtain exclusive ownership 
involve remote fetch cost operation modeled permute benchmark 
call cost rest loop performs simple arithmetic updates local data structures 
cost modeled elementwise add benchmark 
call cost processors microseconds point processor approximately microseconds point processor 
cost loop point processor 
bucketing points thread executes algorithm serial spiral search algorithm 
describes structure inner loop cost iteration loop bounded remote fetches bucket point data bucket index distance computations conditional update nearest neighbor array 
total iteration site 
inaccurate upper bound real performance algorithm 
problem parameters implicitly charge memory operations occurring loop 
remedy note distance calculation structure load data distance calc load data locally calculate dist conditionally update nearest neighbor easily write small synthetic program benchmarks chapter simulate behavior performance analyze inner loop spiral search algorithm 
shows performance ksr benchmark 
mean plotted number threads trial vertical line indicates confidence query point 
number layers need search 
bucket query falls 
buckets 
minx layer minx minx miny layer miny miny maxx layer maxx maxx maxy layer maxy maxy min minx maxx miny maxy point bucket dist min min nn inner loop spiral search 
procs 
benchmark delta benchmark actual error table comparison synthetic benchmark values actual runtime 
interval sample 
performance synthetic program indicates cost distance calculation dominated time takes fetch information isn surprising algorithm distributes point data basically random fashion 
data runs processors linear regression model predicts cost benchmark loop microseconds 
true constant cost inner loop algorithm approximately distance computation 
cost inner loop point processor search costs point processor 
comparing estimate actual performance program shows performance model accurate large problems 
shows performance concurrent spiral search algorithm ksr processors 
figures show synthetic program cost model actual application asymptotically 
earlier experiments know spiral search algorithm performs roughly distance calculations 
average values shown figures machine size shown table 
remember takes total runtime algorithm apparent table runtime parallel algorithm times runtime benchmark 
time element secs distance calculation loop number elements performance synthetic benchmark 
time site secs parallel nearest neighbor search number sites performance concurrent local search algorithm ksr 
speedup parallel nearest neighbor search number sites parallel algorithm compared ksr node 
plot symbols number threads test 
profile shows 
spiral search program spends time inner loop modeled benchmark cumulative self self total time seconds seconds calls ms call ms call name profile entire execution time needed generate points included 
routines executed phase 
search phase performs bucketing loop performs spiral search 
detailed profile shows inner loop spiral search accounts runtime 
need compare performance parallel algorithm performance original algorithm 
compare parallel algorithm single processor ksr sparc workstation 
comparison provides information algorithm scale parallel architecture second evaluates algorithm cost performance basis 
shows speedup parallel algorithm serial running ksr processor 
mean plotted number threads trial vertical line indicates confidence interval sample 
graph shows algorithm scales add processors 
main sources overhead lock contention bucket phase communication overhead search phase 
contribute total runtime algorithm small needed cache blocks replicated 
algorithm obtains close optimal speedup 
compares parallel spiral search algorithm serial spiral search running sparc 
relative speedups better small problems sizes speedup parallel nearest neighbor search number sites parallel algorithm compared sparcstation 
problems parallel algorithm better local data cache 
ksr processors depend large main memories data relative performance parallel algorithm degrades 
caused overhead associated maintaining virtual address space shared processors including cache directory lookups fetching remote cache blocks 
memory overhead fact ksr processor relatively slow compared sparc cpu main obstacles obtaining better speedups algorithm 
summary chapter design analysis simple parallel algorithm neighbors problem 
algorithm executes efficiently centralized distributed memory systems may basis solving proximity problems points independently chosen smooth probability distributions 
addition constructed accurate performance models implementations combining wellknown techniques algorithms analysis simple cost models derived benchmarking primitive operations vector programming model target machines 
saw additional synthetic benchmarks model cost inner loop algorithm bounds standard cost parameters inaccurate 
main purpose chapter illustrate possible design effective algorithms implemented accurately analyzed 
concurrent local search idea works context allows concurrent threads process nearest neighbor queries contention main data structure 
general solution problem 
particular points uniform performance algorithm degraded drastically 
interesting avenue investigate design methods sensitive worst case inputs simple spiral search 
promising techniques sampling construct level bucket structures bentley semi dynamic trees wei ben 
chapter parallel delaunay triangulation algorithms highest probability streets capitol place literally 
lem lem chapter begins study parallel algorithms constructing delaunay triangulation 
sections survey current theoretical literature problem provide basic overview randomized divide conquer emerged useful technique parallel algorithms design 
basis fast sorting algorithms blm phr technique extends geometric algorithms 
section presents framework design algorithms sections propose analyze parallel randomized divide conquer algorithm constructing voronoi diagram 
theoretical analysis suggests algorithm able effective multiprocessor experimental analysis algorithm indicates constant factors high effective practice 
useful study algorithm provides necessary background randomized algorithms probabilistic analysis illustrates careful experiments evaluate algorithms embarking time consuming implementation project 
parallel divide conquer thesis chow cho presents pram algorithm constructs voronoi diagram points log time processors 
algorithm known relationship dimensional voronoi diagram hull gs 
set sites plane imagine projecting dimensions function 
sites projected paraboloid revolution space 
suppose site lies inside circle centered radius gamma gamma true gamma gamma px gamma qy lies circle projection lies hyperplane defined right hand side equation 
follows circle empty corresponding halfspace contain projection site 
empty circles plane correspond empty half spaces space computing convex hull projected point set equivalent computing delaunay triangulation voronoi diagram algorithms discuss chapter relationship computing dimensional convex hulls directly computing voronoi diagram 
aggarwal refine chow computing voronoi diagram directly parallel version guibas stolfi algorithm acg 
parallelize algorithm determine way implement merge step parallel 
non trivial guibas stolfi algorithm iteration merge loop depends cross edge iteration 
parallel find way find cross edges 
algorithm accomplishes indirectly constructing contour voronoi edges passing edges dual cross edge 
key finding edges parallel find edges vor vor cross contour 
follows edge vor crosses contour nearest neighbor endpoints find cross edges sufficient find nearest neighbor voronoi vertex vice versa 
information algorithm determine connect cross edges merged diagram 
processing nearest neighbor queries voronoi diagram equivalent planar point location time best known worst case bound problem log processors 
aggarwal algorithm clever 
builds specialized point location data structures region plane allows algorithm locate points voronoi regions intersect boundary convex hull allows algorithm quickly determine vertex voronoi diagram closer vice versa 
data structures allow algorithm find cross edges log time algorithm cost log processors 
goodrich cole refine method sophisticated data structures non trivial scheduling techniques obtain algorithm runs log time log processors matching optimal bound 
algorithms particularly practical depend complex expensive data structures 
addition designed processor pram machines simulating environment current multiprocessors practical 
investigation divide conquer techniques outside pram community 
lu lu jeong lee jl describe mesh algorithms problem 
lu uses dual algorithm obtain runtime log time theta mesh jeong lee compute diagram directly time optimal mesh 
addition stojmenovic sto describes algorithm designed hypercube architectures 
algorithms basically techniques aggarwal algorithm variation actual details 
results primarily theoretical interest designed real world architectural constraints mind 
optimal expected time algorithms possible obtain efficient algorithms input assumed uniformly distributed 
levcopoulos lingas lkl combination bucketing aggarwal algorithm obtain algorithm runs logn expected time log processors 
algorithm works phases 
points placed buckets algorithm computes inner diagram intersection voronoi diagram unit square 
algorithm computes outer diagram contains voronoi edges lie outside unit square 
recall point set site denotes voronoi polygon phase algorithm uses hierarchical bucketing scheme find site rectangle influence called ri containing points set may contribute 
show total expected size rectangles inner diagram computed log expected time log processors 
show expected number sites contribute outer diagram small brute force algorithm compute rest diagram logn expected time log processors 
structure algorithm similar serial algorithm bentley weide yao hierarchical bucket structure general single level data structure 
mackenzie stout ms algorithm lines faster theoretical runtime 
sophisticated complicated bucketing scheme algorithm able construct voronoi diagram loglog expected time loglog processors 
algorithm uses primitive mackenzie stout call padded sort able bucket sites log log time 
algorithm computes inner diagram spiral search outer diagram brute force 
analysis similar levcopoulos lingas bound cost brute force construction log log 
varadarajan shows modify standard divide conquer algorithm run log expected time processors 
algorithm divides original problem vertical strips constructs voronoi diagram strip constructs dividing chains strips parallel 
algorithm takes advantage fact dividing chains intersect need processing 
recursively merging pairs subproblems algorithm constructs rest diagram examining current set dividing chains intersection points continuing constructing process vertices 
intersections occur algorithm finished 
techniques similar dwyer thesis authors show expected complexity scheme log time uniform distribution assumption probability dividing chain intersects large number chains low 
algorithms ideas basically fast expected time sequential algorithms 
doing spiral search combining bucketing divide conquer way dwyer algorithm chapter 
sake achieving optimal asymptotic runtimes pram model algorithm complex really practical 
algorithms esoteric unnecessary data structures scheduling techniques runtime analysis hides huge constant factors confines asymptotic bounds 
example mackenzie stout bucketing technique call padded sort fill buckets spiral search algorithm 
algorithm runs stages 
alternate attempting place items bins items failed 
sort bins 
reallocation procedure similar pack operation previous chapter complicated fact run theta log log time 
amounts algorithm similar vectorized bucketing scheme chapter 
algorithm attempts bucket item processor items collide iteratively pack away finished items retry failed ones 
seen simple idea pram complexity analysis works extremely vector machines cray locking algorithm achieving effect works mimd machines ksr 
addition analysis algorithms showed bucketing step main bottleneck program searching steps 
pram results unnecessary optimizations optimize relatively minor part algorithm 
course criticism seen attempt question integrity taste authors 
stated goal study basic questions complexity parallel algorithms address problem constructing efficient parallel programs problems 
example illustrate case elegant sophisticated proofs techniques theoreticians prove complexity results simple practical ideas 
fact practical done algorithms similar ones described 
merriam mer implemented parallel algorithm dimensional delaunay triangulation construction loosely ideas papers 
algorithm uses tree iterated nearest neighbor queries construct edges delaunay triangulation time 
method similar algorithms maus mau dwyer 
merriam implementation intel paragon uses message passing maintain distributed data structures coordinate searches 
experiments indicate algorithm runtime log 
addition teng implemented similar algorithm dimensional problem vector style connection machine cm cm 
algorithm uses buckets perform searches experiments indicate runtime algorithm sites processors 
randomized algorithms chapter saw randomizing order processed insertions incremental delaunay triangulation algorithm bound expected number updates algorithm performed regard distribution point set 
intuitively reason behavior random insertion delaunay triangulation random sample point set cause edge flips 
result special case general framework designing randomized geometric algorithms sequential parallel 
motivate formal machinery consider problem sorting set real keys 
insertion sort simple way 
key added set order kept sorted order key inserted 
problem algorithm finding correct position place new key expensive 
alleviate key gamma store interval sorted set belongs 
alternatively pair adjacent keys store list keys belong interval insert new key look keys lies split list keys belonging interval 
resulting algorithm just version quicksort result shares quicksort worst case 
alleviate may insert keys random order case lists interval roughly size resulting algorithm expected runtime log 
idea create divide conquer algorithm 
pick subset ae random sort 
key gamma binary search find interval belongs add subproblem interval 
sort subproblems 
sorting algorithms parallel distributed architectures scheme sample provides convenient way split original problem subproblems solvable parallel 
blelloch survey algorithms describe high performance algorithm designed connection machine cm blm 
prins hightower reif presents similar study maspar mp phr 
key idea algorithms explore context geometric algorithms oversampling obtain load balancing high probability 
processors available choosing sample size choose sample size kp sort form elements ranks delta delta delta gamma blelloch show scheme probability largest subproblem ff times big mean ne gamma gamma ff ffk ff 
practice results implementation obtains somewhat better 
preliminaries general framework clarkson shor cs cms generalized formal framework describing analyzing randomized geometric algorithms 
framework motivate fact principal applied design wide range parallel algorithms 
keep discussion relevant running example show apply ideas construction delaunay triangulation 
set jsj elements called objects 
multiset elements non empty subsets elements called regions 
size largest element 
elements size say uniform 

say relies supports ae define ff ae rg 
particular problem define conflict relation 
ae denote set conflict object 
nature conflict relation depends problem hand term conflicts different meaning problem 
concrete example convex hull problem set points space half spaces defined sets points 
assuming degeneracies uniform set points defines hyperplane assuming translate origin inside convex hull associate halfspace opposite side origin 
point conflicts contained corresponding half space 
regions define convex hull 
illustrates case 
similarly delaunay triangulation problem set sites element triple defines possible delaunay triangle 
assuming degeneracies uniform 
abuse notation identify triples circumcircle define 
conflict relation specify site conflicts region lies disk defined goal algorithm constructs delaunay triangulation build 
illustrates case 
objects regions conflict relation 
points conflict half space 
site conflicts circle lies 
randomized divide conquer clarkson shor framework develop divide conquer algorithms random sampling 
generalization technique sample sorting algorithms 
idea random sample divide independently solvable subproblems 
choosing construct region con denote number objects gamma conflict result characterizes performance algorithms 
define expected value random subset jrj addition max zr theorem 
jrj assume uniform size gamma delta constant follows exists constant max probability gamma gammaq conditions hold con max con max log somewhat restricted version corollary clarkson shor original cs 
result analyze algorithms fit generic framework 
algorithm rd 
randomized divide conquer 

set objects goal construct 
choose subset size random 

construct 

subproblem 
gamma find region conflicts place 
recursively solve results compute 
theorem tells average sample split original problem equally sized subproblems total size larger original 
context parallel algorithms result especially interesting problems interested subproblems independent solved concurrently 
reif sen rs independently developed refinement idea design efficient pram algorithm constructing convex hulls 
algorithm solves dual problem constructing intersection half spaces 
case half spaces 
sample ae intersection half spaces assume know point interior 
partition cones way take arbitrary hyperplane cut faces trapezoids 
triangulate trapezoidal faces connect vertices triangles call resulting set cones delta 
delta half space conflicts bounding plane intersects result follows directly theorem lemma 
terminology exist constants total max conditions hold probability delta con total delta max delta con max log sample ae satisfies conditions call call bad 
reif sen algorithm follows basic outline algorithm rd order achieve optimal pram runtime address issues come serial algorithm ffl sampling algorithm able choose sample high probability order bound max guarantee load balancing recursive calls 
deal problem reif sen sampling scheme called polling processes logn samples parallel logn time 
checking sample dividing convex hull facet 
just schematic drawing isn exact representation execution algorithm 
random sample input entire set chernoff bounds che hr show give accurate estimate actual quality sample 
testing logn samples guarantees algorithm fine high probability 
polling generalization oversampling schemes implementations randomized sorting algorithms guarantee load balance blm phr 
see practice similar somewhat simpler scheme provides result 
ffl algorithm able compute intersections cones logn time plane may cut ffl cones ffl 
reif sen take advantage fact random sample small compared trade time processors brute force approach 
ffl naive algorithm total size subproblems increases factor total level recursion 
reif sen scheme filter redundant half spaces recursive calls show guarantees total number processors needed process call tree 
goodrich gg somewhat different algorithm compute convex hull 
algorithm picks splitting point random uses linear programming find facet hull point 
algorithm uses facet split original problem parts see 
partitioning routine works projecting sites xz yz planes computing dimensional convex hulls 
edges edges hull projected xy plane split sites subproblems 
algorithm solves subproblems recursively 
scheduling tricks similar ones mackenzie authors prove time bound log bound min log log number points number hull facets 
high probability bound 
parallel randomized version hull algorithm due edelsbrunner shi es extension planar hull algorithm kirkpatrick seidel ks 
algorithms follow familiar pattern 
ideas underlying relatively simple large amount complexity overhead added order obtain efficient optimal time bounds pram model 
seriously consider implementing techniques directly basic ideas design evaluate simpler possibly practical algorithms 
practical randomized parallel algorithm 
turn question apply theoretical ideas previous section practical solutions problem 
goal able construct parallel algorithm constructing planar delaunay triangulation exhibits substantial performance advantage compared fast sequential algorithms inputs 
algorithm follow style reif sen algorithm potential advantages algorithms discussed ffl algorithm guaranteed achieve expected performance high probability independently distribution input 
ffl algorithm simple framework 
unclear simplify divide conquer algorithms substantial performance hit 
addition spiral search algorithms tend awkward separate algorithms needed inner outer diagrams 
ffl principle advantage convex hull algorithm goodrich bound output sensitive 
advantage computing delaunay triangulation know projected site convex hull 
extra complexity linear programming point location contours adaptive scheduling gain 
ffl reif sen algorithm easily adapted compute delaunay triangulation directly 
ffl results shown randomized divide conquer extremely effective constructing fast sorting algorithms 
expect technique equally effective geometric problems 
algorithm follow generic outline algorithm rd compute delaunay triangulation directly concentrating computing convex hulls general algorithm carries extra overhead waste time 
addition comparing algorithm incremental algorithm chapter need comparison fair 
objects sites plane regions disks associated delaunay triangles equivalently voronoi vertices current diagram 
convenient describe algorithm terms voronoi diagram delaunay triangulation 
set sites vor denote voronoi diagram dt denote delaunay triangulation site denote voronoi region dn denote set sites connected dt 
algorithm stages algorithm 
randomized divide conquer constructing delaunay triangulation 

constant choose 
assuming jsj processors choose sample ae size kp having processor choose sites random construct vor brute force parallel method small just processor build diagram sequentially 
idea picking sample processor comes experience randomized sorting algorithms blm phr 
idea size subproblem may differ greatly expected value cn kp total size subproblems chosen random far away cn 
subproblem contain site gamma change structure 
site gamma search vor circles conflict circle find associated sites algorithm places subproblems sites 

construct vor efficient sequential algorithm 
remove edges vertices vor lie outside voronoi polygon vor 
see vertices redundant belong final answer 
prove algorithm correct need show correctly construct subproblems step step reports valid vertices vor 
algorithm step easily explained terms delaunay triangulation 
assume constructed dt need find triangles diagram conflict site gamma simple modification incremental insert procedure chapter 
shows modified procedure 
procedure find circles starts locating dt 
obviously conflicts circumcircle triangle lies 
goes subproblems site supporting triangle 
routine finds rest conflicting circles search loop edge flipping loop chapter 
difference flipping edges walking modified diagram find edges keeps queue edges suspect keeps walking queue empty 
edges making initial triangle original members queue 
abc triangle point opposite new site abc fails circle test goes subproblem edges ab ac go queue 
don need add subproblems belong triangle failed circle test see result hard prove 
lemma 
routine find circles visits exactly set edges dt incremental insertion procedure 
queue element edges 
point searched 
find circles locate org dest return ison org dest org conflicts initial triangle add subproblem org enqueue org look conflicting circles done dequeue ccw dest org dest circle org dest dest add subproblem dest enqueue enqueue empty return routine find circles conflict site routine just incremental insertion procedure don change current diagram 
queue keeps track edges need look 
triangle abc fails circle test goes subproblem 
proof 
incremental algorithm checks edges initial triangle edge triangle fails circle test 
find circles search begins edges initial triangle time triangle fails circle test routine adds far edges queue 
conclude check edges incremental algorithm 
lemma implies find circles find regions conflict particular site 
conclude may routine correctly construct needed subproblems 
prove correctness step switch back looking things perspective voronoi diagram 
subproblem vr voronoi polygon vor 
denote circle associated vertex define union delta delta delta vertex vr see 
basic result allows show algorithm correct 
lemma 
vr 
circle centered passing included 
proof 
vr 
follows empty 
neighbor closest vertices vr circles pass totally included circles done 
grow moving directly away growing circle passes follows new circle contained 
conclude ae 
see 
lemma 
vertex vor 
vr vertex vor 
proof 
empty 
lemma implies expand touches staying inside 
ae 
follows ae site fall 
vertex vor 
result implies valid vertex vor appear voronoi diagram subproblem 
lemma 
vertex vor exists vor vr 
voronoi region point flower circles surrounding 
circle centered region passing contained flower 
proof 

passes sites may choose vr 
lemma circle centered passing subset 
included circle ae follows 
sites vertex vor 
lemmas imply algorithm correctly compute vertices vor 
suffices compute vor report vertices diagram fall inside vr 
vertices appear final list easily filtered 
analysis factors critical success algorithm 
average size subproblem small cn processors 
second ratio average subproblem size maximum subproblem size small 
condition implies expected total done algorithm cn constant hope compare runtime constants serial algorithm 
second condition implies efficiency compromised processor 
conditions imply fast parallel runtime low parallel overhead result parallel speedups 
notation section 
objects sites regions triples sites defining circles plane 
site conflicts lies circle defined ae random sample jrj condition theorem implies expected size subproblem algorithm 
remaining problem show sampling scheme produce evenly sized subproblems 
run technical problems 
saw earlier randomized divide conquer algorithms constructing planar voronoi diagram generally framed terms constructing dimensional intersection halfspaces 
reif sen algorithm divide step algorithms partitions faces convex polyhedron set cones creates subproblem cone 
case voronoi diagram action equivalent dividing voronoi region sample point triangular regions 
sake simplicity algorithm create subproblems way 
routine find circles creates subproblem sample point 
algorithm easier program easier understand 
addition algorithm advantage computes voronoi diagram directly going intermediate step constructing intersection halfspaces 
potential performance advantage important standpoint numerical stability 
problem scheme theory algorithm sensitive distribution input certain worst case inputs create subproblems size 
result condition theorem help algorithm fit conditions theorem 
remedy situation change algorithm theorem analyze leave algorithm experiments give confidence find circles behaves expected 
section series experiments suggests algorithm sensitive distribution input partitioning procedure produces equally sized subproblems 
results interesting try improve analysis account behavior modifying algorithm fit current theory 
main reason modifications algorithm complex increase runtime constants 
experiments theoretical analysis suggests algorithm effective current multiprocessor architectures 
analysis exact tell exactly runtime constants 
order study method detail gather experimental data serial simulation bucketing process 
simulations long predict performance real systems 
simulation somewhat different measure opposed real system costs 
mcgeoch mcg uses simulations type study performance algorithms 
notes possible construct simulation programs accurately measure cost algorithm executing algorithm question 
example cost partition step quicksort determined rank splitter 
measure cost sorting keys 
section simulation type measure performance algorithm 
performance algorithm determined size subproblems created step algorithm 
simulator execute steps algorithm output resulting problem sizes 
predict performance phase algorithm experiments analysis chapter 
implementation simulator follows outline algorithm fairly closely 
assuming picks sample input set jrj kp constructs dt incremental algorithm 
uses routine find circles place remaining site appropriate subproblems 
assume subproblems numbered sp sp delta delta delta sp kp processor simulator adds sizes sp ik sp ik delta delta delta sp gamma reports result total needed processor examine sets experiments observe expected value mean min mean subproblem vs min oversampling ratio max mean mean subproblem vs max oversampling ratio load balancing results processors 
complexity algorithm observe expected load imbalance 
set experiments run assuming machine processors machine processors 
matches size target machine second give idea method scale larger machines 
examine effect oversampling variance subproblem sizes 
machine configuration trials bucketing procedure executed input size sites sample sizes site processor samples processor 
set tests sites generated uniform distribution unit square 
noted plot sample mean confidence interval 
look sets results 
shows ratio mean minimum subproblem sizes range inputs 
practice ratio important ratio maximum mean processor holding small subproblem wastes cycles waiting rest machine finish 
shows small sample sizes behave badly regard oversampling scheme smoothes performance bucketing structure significantly 
oversampling factor ratio min mean factor 
shows ratio maximum bucket size mean 
call ratio bucket expansion 
bucket expansion determines efficiency algorithm terms running time processor utilization 
variance subproblem sizes large small oversampling factors samples processor average bucket expansion 
shows results larger simulated machine 
graphs show little change experiments processors 
results provide evidence performance sampling scheme part independent machine size 
remaining question address scheme perform equally poor point distributions 
study question additional set trials conducted group bad distributions chapter 
distribution trials run simulated processors problem size sites oversampling factor 
mean min mean subproblem vs min oversampling ratio max mean mean subproblem vs max oversampling ratio load balancing processors 
unif ball corner diam cross norm clus arc max mean bucket expansion distribution bucket expansion bad distributions processors 
summarizes results box plot set trials 
bucket expansion incurred algorithm show sensitivity distribution input 
particular notice behavior arc diameter distributions somewhat erratic 
cases subproblems associated sites infinity added input incremental algorithm somewhat large 
cases models particular worst case algorithm isolated sample point disproportionately high degree delaunay triangulation sample 
experiments indicate sampling scheme effective way manage load imbalances algorithm 
experiments provide evidence support conjecture conjecture 
set sites identically independently chosen distribution density function satisfies 
bucket expansion algorithm high probability 
call distributions satisfy conditions quasi uniform 
bad distributions experiments fit category pathological small amount randomness 
addition mentioned oversampling scheme directly related scheme blelloch sorting algorithm blm similar polling technique introduced reif sen rs 
theorem experiments fact algorithms provide load balanced provably high probability strong evidence conjecture conjecture proved 
move total size subproblems created bucketing phase 
measure expected total subproblem size trials run various problem sizes sites 
trial bucketing procedure executed total size subproblems tallied 
results experiments conducted oversampling factor 
shows results processor machine summarizes experiments processors 
graphs show small problem sizes total subproblem size fairly small large problems total problem size approaches 
summarizes additional data earlier trials various input distributions 
distribution graph shows boxplot summarizing distribution total problem size trials 
trials run sites distribution simulated processors oversampling ratio 
experiments show total size subproblems comes close large variety inputs 
actual done algorithm small constant times total subproblem size 
bucketing pass expensive expected bucket expansion processors mean speedups processors expect speedup 
addition total subproblem size bucket expansion overly sensitive distribution sites 
expect runtime algorithm stay close expected value types problems 
apparent expected time constant high compared sequential algorithms studied chapter 
main problem algorithm creates subproblems containing large amount total total subproblem size processors number sites total total subproblem size processors number sites total subproblem size algorithm 
unif ball corner diam cross norm clus arc total total problem size distribution total subproblem size distribution processors 
redundant information 
constant size blowup total problem size necessary guarantee parallel subproblems solved independently 
importantly algorithm large amount space store solve subproblem 
parallel machines tend solve larger problems extra space cost may concern large memory sizes prevalent modern machines 
obvious way reduce overhead algorithm phases 
constructing diagram sample add rest sites small groups running steps algorithm times 
problem scheme phase algorithm need run fix procedure filter invalid voronoi vertices 
filtering procedure add cost implementation complexity final algorithm 
way reduce problem size blowup place site subproblem find circles move neighboring subproblems information initial correctly construct 
expect small fraction sites need examine subproblem happen site lie close boundary voronoi region vor 
relatively complex costly task determine solutions subproblems exactly sites need processing exactly proceed sites 
summary chapter starting point study parallel algorithms delaunay triangulation construction 
survey literature introduced framework describing analyzing randomized algorithms computational geometry 
addition studied possible parallel algorithm constructing delaunay triangulations randomized divide conquer 
algorithm substantially simplified version existing theoretical results 
theoretical analysis algorithm promising experiments suggest constant factors runtime high effective 
studied number different classes sequential parallel algorithms closest point problems ffl classical divide conquer 
ffl sweepline algorithms 
ffl randomized incremental divide conquer algorithms 
ffl incremental construction algorithms 
ffl incremental search algorithms 
classic divide conquer sweepline schemes difficult implement way take advantage large amount parallelism 
machines modest number processors straightforward parallelization divide conquer algorithm obtain reasonable performance large problems 
processors algorithm divide points tree subproblems leaves 
log stages algorithm merge problems subproblems processors problems processors 
long large algorithm obtain high levels efficiency 
chapter produced experimental evidence randomized divide conquer high runtime overhead compared current sequential algorithms 
high constants relative complexity algorithm kept main reasons studying actual implementation method 
method remains strong possibility applications inputs extremely unpredictable experiments chapter shown performance algorithm generally uniform wide range possible workloads 
chapter analyze compare possibilities incremental construction incremental search 
motivation studying obvious 
concurrent local search proved simple scheme performance incremental search algorithms constructing delaunay triangulations straightforward extensions idea 
interest incremental construction obvious incremental algorithm appears inherently sequential 
fact saw chapter random insertion order incremental updates tend effect small portion current diagram 
updates performed concurrently assuming devise reasonable way manage concurrent access data structure representing diagram 
chapter practical parallel algorithms delaunay triangulations asymptotic analysis keeps student head clouds attention implementation details keeps feet ground 
jon bentley shortest distance points construction 
incremental algorithm largely ignored theoretical studies parallel methods constructing delaunay triangulation 
clear way modify algorithm obtain levels concurrency needed results popular theoretical models parallel computation 
practice obtaining high levels concurrency usually main concern 
issues managing data movement synchronization obtaining bound close possible sequential time bounds generally important able utilize processors problem size 
saw chapter simple data structures coarse grained parallel model careful engineering implementations led parallel programs performance 
setting incremental algorithm stands natural extension concurrent local search method chapter 
looking radically new methods asymptotically efficient examine practical issues involved building concurrent versions simplest algorithms constructing delaunay triangulation 
algorithms come flavors incremental construction incremental search 
sequential algorithm chapter constructs diagram adding site time 
nearest neighbor algorithm chapter constructs diagram delaunay triangle time variant spiral search 
remaining sections chapter describe analyze compare performance algorithms 
section section sum chapter lessons learned implementing algorithms 
randomized incremental construction randomized incremental construction way apply random sampling design geometric algorithms 
randomized incremental algorithms chapter variant quicksort construct adding objects time set maintaining 
new object added algorithm adds new regions created removes regions conflict original framework clarkson shor algorithms data structures represent regions represent conflict relation 
conflict relation represented bipartite graph edges regions objects 
main cost algorithms updating graph insertion 
correctly algorithm finds deletes regions new object conflicts creates new set regions replace 
edge object deleted region replaced edge appropriate new region example case delaunay triangulation conflict graph stores edge site triangle circumcircle contains new site inserted diagram algorithm delete triangles conflicts replace new triangle contain addition site conflicts deleted triangle examine new triangle add edge conflict graph falls circumcircle bound expected cost incremental algorithm suffices bound expected cost insertion 
assume chosen subset random jrj built delaunay triangulation sites expected size max zr know delaunay triangulation problem 
lemma provides bound expected cost insertion 
just special case theorem clarkson shor cs 
lemma 
expected cost insertion step number 
proof 
site inserted step triangle 
random variable conflicts inserted sites vertices sites conflict number sites conflict cost update step probability sites conflicting gamma 
sum gamma theorem clarkson shor shows gamma expression gamma 
incremental algorithm chapter conflict graph keep track conflicts sites triangles 
combination point location circle testing job 
implementation incremental algorithm chapter cost update proportional number triangles conflicting new site 
equivalently update time proportional number new triangles site created 
fsg cost vertex tg probability vertex triangle sum expected number updates new site incurs constant 
result section design algorithm constructs delaunay triangulation incrementally concurrently 
intuition algorithm chapter 
insertions existing diagram tend cause small number local updates insertions tend independent 
parallel algorithm able perform independent insertions concurrently 
long current diagram large concurrent incremental algorithm effective parallelism available target machine 
concurrent incremental construction concurrent incremental algorithm able process independent insertions concurrently maintaining consistent history intermediate diagrams 
formalize mean ll borrow terminology study concurrent database processing bhg 
transaction defined partial order 
ae fread write data item 
abort commit 
abort commit 
read write write read read write 
condition specifies transaction sequence machine instructions read write data central shared database 
conditions say transaction tentative updates database commits updates permanently aborts permanent changes 
condition requires reads writes data item ordered 
say operations collide operate data item 
normally term conflict case term context randomized incremental construction confusing overload meaning 
complete history set transactions ft partial order properties 

oe 
collide histories formalize notion defining execution order operations set transactions 
general operations set transactions interleaved arbitrary fashion 
interleavings may produce results consistent sequential execution 
complete history serial transactions operations come vice versa 
completion history denoted obtained deleting operations belonging transactions committed 
history serializable equivalent serial history sense orders operations collide way 
serializable histories capture intuitive notion happen concurrent program produces execution history consistent sequential program algorithm 
implement transactions guarantee serializability locks synchronize access shared data 
known set transactions utilizes phase locking history transactions serializable 
phase locking specifies transaction divided phases obtains locks updates database releases locks 
concurrent incremental algorithm structure insertion process set transactions update dt serializable manner 
insertion step act centralized database representing current triangulation assume shared memory programming model multiple threads types program statements lock obtain lock object region conflict list 
update update region corresponding entries conflict graph 
unlock unlock region code insert site shown 
routine split phases 
phase looks bucketing loop previous chapter 
assuming inserted sites routine searches dt marks edges triangle tests conflict unique identifier declares thread attempt obtain lock edge 
addition phase algorithm places edges examined search loop cache avoid repeated circle tests 
entry cache pointer edge record flag indicating edge needs flipped 
algorithm puts edges examined edge cache check second phase locking scheme 
necessary algorithm explicitly maintain conflict graph uses current state diagram locate conflicts new sites existing triangles 
thread lock edges plans flip edges needs read updates invalidate pre computed circle tests 
second phase insert site walks edge cache checks see thread lower priority number marked edges 
insertion wait try round 
edges cache marked safe flip happens phase algorithm 
insert site 
delaunay triangulation construction algorithm composed number insertion phases 
phase thread picks site insert calls insert site 
assume sites randomly permuted insertion phase begins 
isn case permuting input straightforward task 
insert site variable tid holds thread id current thread 
phase locate edge directly left sequential algorithm cache init cache conflicts initial triangle enqueue org look conflicting circles done dequeue enqueue cache put edge cache lock system lock tid mark tag edge id mark tid sym mark tid lock unlock system unlock ccw dest org dest circle org dest dest enqueue enqueue set flip true empty break phase check see threads updating edges barrier private flag elt cache elt mark tid flag phase flag updates connect new point base sym base connect base dest edge cache mark sym mark infinity unlock flip barrier abort concurrent incremental algorithm 
assigning priorities carefully guarantee algorithm progress thread tries repeatedly insert site succeeding 
algorithm assigns priorities manner original set sites fp 
permutation integers permutation defines order sequential randomized incremental insert sites 
thread attempting insert site priority scheme locking protocol ensures algorithm perform insertions order consistent original sequential algorithm assuming algorithm permutation 
addition threads scheme ensures thread wait phases worst case insertion succeeds 
assuming fixed number processors algorithm worst case runtime standard incremental algorithm 
consider situation sites placed axis sites placed circular arc intersects axis gamma 
insert sites arc put sites right left new insertion cause edge flips 
addition sites axis need processed sequentially update disjoint sets existing edges 
case parallel algorithm reduced worst case performance sequential algorithm 
luckily refer theoretical results show average case performance algorithm reasonable 
saw earlier diagram dt jrj insert site number random expected number updates insertion cause constant 
ideally able prove probability insertion causing significantly average number edge flips low 
probabilistic analysis provide exact result need 
crux problem bounding sum derived equation 
knew probabilities sum independent simple tail bound suffice provide needed result 
case general proving bound difficult 
useful bounds available literature 
bound condition theorem implies high probability site inserted conflicts logr existing triangles 
addition clarkson show probability total number updates performed randomized incremental algorithm greater times average bounded gammak cms 
bound gives rough ceiling maximum number updates needed insertion isn exact 
second bound tighter holds amortized sense insertion 
outside realm randomized algorithms bern show restricted model input expected maximum degree vertex triangulation sites theta log log log bey 
point set modelled unit intensity poisson process plane 
authors restrict attention portion delaunay triangulation set points lies square side length model avoid dealing anomalies near edges square correspond point sets generated finite distribution 
result evidence input algorithm generated uniform distribution unit square sites cause expensive insertions take place 
incremental algorithm cost insertion proportional degree new site modified diagram 
supplement current theoretical thinking experiments 
purpose depend simulation parallel algorithm executes conventional workstation 
simulation provide relatively precise answers wide range questions interested large range problem instances 
experimental results current set similar identical theoretical results listed provide evidence result want fact true 
simulations collect data experiment simulator constructed program incremental algorithm chapter 
simulator designed measure cost algorithm cric try estimate runtime algorithm real system 
simulator mimics multiple threads control executing synchronously loops 
statement algorithm cric executed parallel multiple threads simulator wraps statement loop iteration thread 
threads move forward program lock step 
course gross simplification real system simulation attempting provide accurate estimation real performance concern 
simulator instrumented keep track maximum number edge flips block insertions 
size block parameter simulation 
results blocks insertions 
simulator runs sequential algorithm keeps track maximum average number edge flips needed block insertions 
processor simulation ran inputs sites processor simulation ran inputs sites 
experiment inputs input distributions chapter 
shows results experiment 
run distribution box plot summarizes ratio maximum number edge flips block mean block 
processor case plot summarizes trials processor plot summarizes trials 
case see maximum number edge flips small constant times mean 
small dependency problem size block size values uniformly higher processor plot 
practical problem sizes small factor appear significant 
experiments give confidence range problem sizes studying say sites machines processors maximum number edges flipped insertion small constant times average 
case jrj probability conflicts triangle current diagram constant 
suppose sites wish insert say insertions collide update sets intersect 
analysis section see average number insertions collide 
long expected number collisions insertion phase constant 
sites phases 
point location structure chapter expected runtime phase constant expected cost processing insertions time 
remaining cost algorithm managing insertion queue managing locks memory system overhead cost barrier synchronization 
factors machine dependent constants fixed affect asymptotic runtime algorithm 
play large role determining actual performance implementation unif ball corn 
diam clus arc max mean processors sites unif ball corn 
diam clus arc max mean processors sites load balancing concurrent incremental algorithm 
deal issues stage discussion 
look closely expected cost processing insertions 
examine aspects algorithm cric concurrency available total terms circle tests edge tests algorithm perform 
question consider algorithm achieves low contention high concurrency 
examine simulator programmed keep track number insertions succeeded iteration 
concurrency profiles collected wide range problem sizes sites simulated machines processors 
inputs generated uniform distribution unit square 
simulation run initial sample sizes sample samples processor 
examine results sample processor larger initial samples increase potential concurrency algorithm 
shows results preliminary experiment 
machine size processors graph shows simulated run algorithm sites generated uniformly unit square 
time axis plot represents forward motion algorithm insertion phase 
simulation breaks run algorithm groups insertion phases 
phase records number threads phase successfully inserted site 
number reflects number threads doing useful processor utilization phase 
group insertion phases simulation stops computes average number active threads group 
value shown graph 
profiles display general pattern 
sufficient number sites inserted diagram algorithm support active threads needed 
encouraging results hand take detailed look algorithm performance 
relevant measures total number circle tests algorithm performs number edge tests algorithm needs point location 
statistic trials problem sizes sites simulated machine sizes processors 
inputs generated uniform distribution unit square 
size initial sample chosen sample processor 
graphs show box plots summarize distribution trials active threads procs sites samples time active threads procs sites samples time active threads procs sites samples time active threads procs sites samples time active threads procs sites samples time active threads procs sites samples time concurrency profiles various machine problem sizes 
problem size 
shows large problems parallel incremental algorithm slightly circle tests average serial algorithm 
small problems contention transactions aborted retried 
causes extra circle tests performed 
results consistent analysis said algorithm perform long shows performance point location algorithm 
graphs show pattern results circle testing 
large problem sizes performance algorithm close serial algorithm 
smaller problem sizes algorithm somewhat worse long expected cost phase algorithm measured simulator factor worse serial algorithm 
results imply absence system overhead algorithm cric perform compared serial incremental algorithm 
simulations idealized provide evidence runtime constants algorithm low effect performance implementation 
algorithm candidate practical solution problem constructing delaunay triangulations parallel 
implementation course real implementation need contend overhead real systems 
examine issues discuss implementation algorithm cric ksr 
choose ksr cray reasons ffl algorithm suited vector style programming 
saw earlier vectorizing spiral search considerably simpler algorithm cric intricate process 
principle possible translate algorithm vector style natural convenient take advantage multithreaded shared memory programming model ksr provides 
ffl ksr provides potential concurrency processor level better study algorithm scale larger numbers processors 
cray high level potential instruction level parallelism programs vectorize 
processors available cray mp access 
ffl irregular dynamic nature algorithm cric put maximum stress ksr memory system 
performance memory system test provide insight practicality large distributed memory cache coherent multiprocessors 
implementation algorithm cric follows fairly closely 
thread repeatedly calls insert site retrying aborted insertions needed 
sites inserted kept centralized queue idle threads fetch new 
addition shared version point location data structure maintained updated parallel 
insertion phases algorithm occur synchronously phase algorithm checks see bucket table needs expanded 
master thread performs expansion threads sites inserted 
implementation main concern reducing cost data management synchronization 
ksr parallel runtime libraries ksr contain routines barrier synchronization circle tests site processors samples number sites circle tests site processors samples number sites circle tests site processors samples number sites circle tests site processors samples number sites circle tests site processors samples number sites circle tests site processors samples number sites circle tests site algorithm cric 
edge tests site processors samples number sites edge tests site processors samples number sites edge tests site processors samples number sites edge tests site processors samples number sites edge tests site processors samples number sites edge tests site processors samples number sites edge tests site algorithm cric 
queue locks overhead routine high 
simple locks needed implemented instruction 
instruction attempts obtain exclusive access cache block halts processor block available 
mechanism similar instruction described goodman 
rsp instruction release lock 
barriers implemented mellor crummey scott arrival tree barrier mcs 
barrier algorithm attractive uses communication standard counting barriers 
important consideration ksr communication expensive compared local processing 
routine ported ksr original implementation sequent symmetry 
mechanisms place main source overhead cost managing distributed quad edge data structure 
algorithm update diagram insertion phase incurs network traffic fetch data remote memories invalidate copies cache blocks may replicated 
chapter saw programs exhibit pattern memory access introduced significant runtime overhead 
unstructured permutation benchmark chapter representative sort code 
benchmark processors running parallel achieved little speedup sequential code large vectors 
results expect direct cost maintaining shared data structure limit absolute speedup attainable algorithm 
non uniform memory access times produce unexpected indirect effect program performance 
unpredictable performance memory system introduces large amount variance relative runtimes threads parallel algorithm 
result arrival times threads barrier points algorithm unpredictable 
performance barrier algorithms threads reach roughly time arrival times spread large range fast threads spend significant amount time spinning inside barriers 
order combat effect ad hoc devices 
process insertions batches amortize overhead barriers larger number transactions 
idea motivated fact main goals data parallel language compilers mimd machines reduce synchronization overhead data parallel programs combining individual instructions larger blocks code run asynchronously multiple threads cha 
similarly algorithm cric defined set synchronous phases phase site processor 
concurrency profiles indicate moderately sized problems support large amount virtual concurrency 
sense take advantage simulate insertions processor 
effect increase insertion phase spread cost synchronization insertions 
second device implementation motivated guided gss technique polychronopoulos kuck pk 
guided self scheduling designed facilitate scheduling loop iterations multiple processors sharedmemory environment 
information iterations needing service stored central queue 
thread idle examines queue pulls fraction remaining 
polychronopoulos kuck describe threads set local independent scheduling decisions way minimize spread arrival times barrier marking loop 
original thread site current batch insert site site grab new sites changes place new batch points pq thread pq empty pull set sites gss run phase insert site barrier place current batch points pq thread pq empty pull set sites gss run phase insert site barrier place current batch points pq thread pq empty pull set sites gss run phase insert site barrier interchanging loops gss 
restructuring algorithm cric gss required major surgery existing program 
scheme need interchange inner loops algorithm scheme 
new code interchanges order loops bringing internal phases insert site outer level loop nest pushing loop batch sites inner level 
allows gss schedule phases insert site way minimizes overhead barriers 
change magnitude undertaken looking simpler method 
profiles program showed major source overhead barrier phase insert site 
algorithm modified thread reach barrier global flag inform threads finish 
threads detect flag set break phase loop enter barrier soon possible 
batch size time secs processors batch size time secs processors phase phase barriers effect large batch sizes processors 
shows combined effect schemes processors 
tests batch size varied site iteration sites iteration 
batch size trials run average value component total runtime calculated 
graph summarizes results trials batch size problem uniformly distributed sites 
larger contribution barrier overhead total runtime program shrinks significantly resulting larger improvement performance 
simple scheduling trick works relatively small numbers processors global flag clearly limits usefulness algorithm relatively small machines 
large machines expense broadcast operation eventually outweigh gains better scheduling may 
making implementation sophisticated scheduling techniques gss examining scalability context promising avenue research 
extra refinements implementation algorithm cric behaves simulation 
figures show cost algorithm terms circle edge tests 
measure algorithm run processors problem sizes sites generated uniform distribution unit square 
simulations small problem sizes contention shared database causes retries 
problems larger algorithm cric close original incremental algorithm 
actual values somewhat different simulation simulation mimic batched insertions loop scheduling final implementation performed 
mechanisms machine dependent optimizations appropriate study simulation concerned costs 
weaknesses comparing output simulations measurements implementation shows original experiments provided reasonably accurate upper bounds actual performance algorithm 
circle tests site processors number sites circle tests site processors number sites circle tests site processors number sites circle tests site processors number sites circle tests site algorithm cric 
edge tests site processors number sites edge tests site processors number sites edge tests site processors number sites edge tests site processors number sites edge tests site algorithm cric 
speedup speedup cric algorithm number sites speedup algorithm cric vs standard incremental algorithm ksr processor 
speedup speedup cric algorithm processors number sites speedup algorithm cric threads vs standard incremental algorithm sparc 
final performance measure interest algorithm cric course speedup 
speedup plots show average speedup trials parallel algorithm problem sizes sites 
inputs generated uniform distribution distribution unit square 
compares runtime concurrent algorithm original sequential algorithm running ksr processor 
graph indicates takes processors running algorithm cric match performance single sequential node algorithm achieve reasonable levels relative speedup add processors threads 
factors handicap performance algorithm cric compared original incremental method 
ksr memory system worst dealing unstructured read write access patterns 
observed behavior study permute benchmark chapter 
second algorithm cric passes current diagram insertion checking conflicts flipping edges 
original incremental algorithm needs pass perform tasks 
algorithm cric caches results circle tests point location extra related managing queues checking locks 
factors fact ksr node processor slower current shelf parts result speedup curve shown 
runtime algorithm cric processors compared runtime serial algorithm fast workstation 
range problem sizes workstation clear advantage cost performance 
hand problems larger parallel algorithm able take advantage memory workstation 
indication effect small upward jump speedup curves take largest problem sizes 
general cost maintaining intermediate diagrams shared memory high algorithm cric compete current workstations 
face performance algorithm absolute terms somewhat disappointing 
experiments showed algorithm effective multiple processors available 
relative speedup exhibited algorithm processors typical irregular applications high communication requirements 
chatterjee thesis contains benchmarks type cha complex construction delaunay triangulation 
real advantage parallel algorithm fact processors available able solve problems large fit memory current workstations 
multiple cpus memory management hardware available machines ksr possible large data sets efficiently 
concurrent incremental search turn parallel implementation algorithm chapter 
call new algorithm algorithm cis concurrent incremental search 
recall algorithm constructs diagram triangle time search process spiral search 
parallel version algorithm perform site searches parallel time 
order need construct concurrent versions data structures bucket grid edge dictionary edge queue 
chapter showed construct bucket grid parallel simple locking protocol 
implementation edge dictionary similar 
edge dictionary represented hash table external chaining 
inserting new edge record dictionary thread locks bucket record falls adds record bucket unlocks bucket 
bucket grid edge dictionary modified algorithm progresses 
searching dictionary threads lock buckets scan search edge record 
edges mapped buckets uniform manner contention bucket locks problem edge dictionary major bottleneck performance parallel algorithm 
shows pseudo code summarizes implementation concurrent edge dictionary 
handling edge queue slightly subtle problem 
edge queue implemented circular queue embedded array 
straightforward way queue concurrently just threads lock head tail queueing dequeuing edge 
early runs program showed contention limiting performance parallel algorithm 
final implementation removes contention having thread keep local queue edges threads place edges global queue run local space 
addition sure thread terminates prematurely local queue empties sure thread puts fixed fraction edges global queue keep busy 
algorithm tuned adjusting local queue size frequency global inserts optimize load balance contention 
experiments chapter local queue size set extra global inserts performed 
practice premature termination threads happen significant impact performance program 
shows pseudocode queue handling functions implementation 
code create threads synchronize bucketing searching phases algorithm program concurrent incremental search algorithm identical sequential counterpart 
experiments test effectiveness parallel algorithm ran standard set experiments implementation ksr 
tested program point sets generated random node org dest edge record corresponding org dest link bucket pointer edge record chain 
insert edge org dest bucket hash org dest lock link bucket node org dest exists bucket unlock link bucket return node org dest newnode construct node org dest newnode link bucket link bucket newnode unlock link bucket return newnode find edge org dest bucket hash org dest answer nil lock link bucket node org dest exists bucket answer node org dest unlock link bucket return answer code concurrent edge dictionary 
array global queue array local queue head global queue tail global queue head local queue head local queue temp temp temp put return local queue full lock fprintf stderr queue overflowed try exit put unlock check local queue return look global queue lock unlock return nil unlock return code concurrent edge queue 
dist site processors number sites dist site processors number sites dist site processors number sites dist site processors number sites distance calculations site concurrent incremental search algorithm 
uniform distribution unit square 
program tested processors 
trials executed point sizes sites 
comparison sequential version program run ksr node sparcstation earlier tests 
usual sequential incremental algorithm running sparc baseline absolute performance comparison fast workstation 
mainly consistency comparisons 
true dwyer algorithm somewhat faster difference sparc 
algorithm cis performs exactly searches sequential 
situation perform redundant search thread mark edge finished just thread pulled global event queue called site search 
local queues possibility relatively remote 
figures provide experimental evidence fact 
recall chapter saw performance algorithm largely determined number buckets examined number distance calculations performed 
figures show box plots summarizing behavior metrics set experiments 
apparent trials parallel algorithm performs little extra compared sequential code 
saw chapter significantly slower incremental algorithm 
boxes site processors number sites boxes site processors number sites boxes site processors number sites boxes site processors number sites buckets examined site concurrent incremental search algorithm 
speedup number sites speedup algorithm cis relative ksr node 
speedup number sites speedup algorithm cis relative sparcstation 
concurrent incarnation situation reversed 
combination simplicity lack global synchronization lack contention shared data structures allows algorithm cis effectively utilize parallelism algorithm cric 
figures summarize speedup different ways 
shows average speedup obtained trials compares runtime algorithm ksr processor runtime algorithm cis multiple ksr processors 
clearly shows algorithm cis making better extra processors algorithm cric 
algorithm cis behaves parallel nearest neighbors code chapter 
mainly depends searching bucket grid parallel searching updating ksr memory system supports concurrent reads local cache processor 
algorithm cis quite scalable earlier nearest neighbors algorithm searches cover larger area making cache effective algorithm needs maintain shared edge dictionary 
compares algorithm cis running ksr algorithm running sparcstation 
evident ksr significant performance advantage workstation case 
advantage remains compare algorithm cis original incremental algorithm see 
graphs provide solid evidence limitations noted chapter algorithm cis shows possible parallelism effectively construction delaunay triangulation 
shows direct comparison algorithm cis algorithm cric uniformly distributed sites 
algorithms run processors problems sites 
graph shows ratio mean runtimes algorithm 
results easily derivable previous graphs helps see directly 
graphs show smaller problems algorithm cis times faster cric 
large problems algorithm cric improves algorithm cis twice fast 
summary chapter practical algorithms parallel construction delaunay triangulation extensive experimental analysis performance 
algorithm constructs speedup number sites speedup algorithm cis relative sequential incremental algorithm running sparcstation 
speedup number sites speedup algorithm cis relative algorithm cric procs 
delaunay triangulation concurrent incremental fashion 
algorithm cric concurrent version randomized incremental algorithm studied chapter 
algorithm cis concurrent version incremental search algorithm studied chapter 
experiments chapter chapter led important observations practical behavior parallel algorithms delaunay triangulation construction 
summarize observations 
ffl random sample divide conquer produces independent subproblems appear roughly equal sizes 
experiments uniform inputs showed technique adds large multiplicative constant bound algorithm 
ffl incremental algorithms discussed chapter provide practical basis parallel algorithms constructing delaunay triangulation 
ffl simulations showed uniform inputs incremental construction algorithm potential support high level concurrency assuming synchronization bottleneck 
ffl implementation concurrent incremental construction algorithm showed performance poor due high synchronization memory management overhead 
ffl incremental search algorithm avoids overhead incremental construction algorithm exhibits better performance parallel implementation 
experiments chapter experiments parallel algorithms illustrate principles general interest 
covered chapter repeat new context parallel algorithms design 
keep simple stupid kiss 
studying elegant complicated methods constructing delaunay triangulation parallel simplest methods proved practical 
algorithms implemented oldest simplest available 
abstraction 
designing analyzing parallel algorithms high level possible theory practice 
chapter able derive accurate performance models combination high level analysis machine benchmarking 
chapter earlier analysis sequential algorithms high level primitives obtain qualitative feel performance final programs 
successful algorithm cis algorithm cric core algorithm cis nearly identical sequential counterpart new analysis needed 
concurrent data structures 
able utilize concurrent versions common data structures implementation data parallel algorithms easier 
algorithms chapter algorithm cis chapter concurrent hash tables queues 
algorithm cric uses concurrent version quad edge data structure 
availability efficient data structures allows programmer high level abstraction maintaining efficiency 
easier develop programs prove correct 
experimental analysis 
depended heavily experiments characterize expected runtime algorithms wide range possible input distributions 
crucial cases mathematical characterization performance available exact practical purposes 
algorithm animation 
animation parallel algorithms just useful debugging visualization sequential algorithms 
concurrent read easier 
current parallel architectures deal read shared data structures effectively read write shared data structures 
reflected target architectures 
cray mp memory system supports write port cpu cpu read ports 
poor performance algorithm cric part due large amount read write sharing utilized 
contrast nearest neighbors algorithm algorithm cis performed ksr utilize read sharing write sharing 
chapter summary difficult part summarize 

hoare thesis examines construction efficient programs solving proximity problems 
novel results area parallel algorithms real focus combine theoretical experimental results create practical solutions 
contributions thesis areas algorithms experimental analysis implementations 
algorithms dissertation case studies sequential parallel algorithms solving nearest neighbors problem constructing voronoi diagrams delaunay triangulations see chapter 
case studies novel algorithms mathematical experimental analysis performance ffl faster version randomized incremental construction algorithm clarkson combines randomization buckets obtain average runtime steps 
algorithm simple implement performance competitive known methods see chapter 
ffl parallel algorithm nearest neighbors problem demonstrably efficient traditional vector processors newer cache coherent multiprocessors 
machine algorithm achieves speedups sequential code current workstations see chapter 
ffl concurrent incremental algorithm constructing delaunay triangulation uses transactional abstraction dynamically maintain subdivision 
algorithm advantage line extended support concurrent deletion 
overhead maintaining complicated shared data structure limits algorithm performance see chapter 
ffl concurrent incremental search algorithm constructs delaunay triangulation valid triangle time series nearest neighbor queries 
algorithm uses bucket grid accelerate search process obtains reasonable speedups fast sequential algorithms running current generation workstations see chapter 
models experimental analysis case studies thesis illustrate utility models high level machine independent flexible tailored particular applications 
models study parallel sorting extends usefulness realm computational geometry 
models data parallel sense focus performing concurrent operations aggregate data structures specifying large systems independent processes 
lowest level models classes vector operations elementwise arithmetic routing parallel prefix operations scans 
chapter arguments favor style programming model ffl models portable sense low level primitives easy implement efficiently wide variety machines 
chapter benchmarks example implementations primitives cray mp ksr 
ffl models simple expressive 
providing powerful primitives scans models allow programmers express analyze algorithms high level terms 
ffl models realistic 
models lose sight real world costs easily parameterized account cost primitives architecture 
ffl models flexible 
need arises new higher level primitives substituted low level ones long clear new primitives implemented efficiently 
chapter saw combine expressiveness vector models realistic machine parameters obtain accurate estimates expected performance non trivial algorithm 
case studies utilize higher level version basic models vector primitives augmented operations circle test orientation tests distance computations 
concentrating high level operations experiments general observations behavior algorithm 
observations follows ffl incremental construction simple effective method building delaunay triangulation combined dynamic bucket data structure accelerating point location see chapter 
ffl uniform inputs incremental search algorithm appears run expected time somewhat slower algorithms due higher constant factors see chapter 
ffl sets uniformly distributed sites frontier fortune algorithm expected size edges 
expected size event queue algorithm see chapter 
ffl uniformly distributed sites circle events fortune algorithm cluster near move sweepline see chapter 
ffl heap represent event queue fortune algorithm improves performance large problems small amount 
cost maintaining heap fortune algorithm incurred extract min see chapter 
ffl dwyer algorithm strongest range problems tested see chapter 
ffl random samples divide conquer viable basis parallel algorithm construct delaunay triangulation suffers large constant factors empirical runtime see chapter 
ffl randomized incremental construction algorithm insertions update independent areas current diagram processed concurrently see chapter 
ffl parallel incremental search algorithm performs better constants higher avoids high overhead operations synchronization finegrained read write sharing data structures see chapter 
data parallel models allowed implement algorithm terms high level data structures appropriate primitive operations 
cost data structures incorporated parameters model 
sequential algorithms chapter structures buckets heaps queues dictionaries quad edge data structure 
parallel algorithms utilized concurrent versions structures 
data abstraction greatly aided implementation analysis sequential parallel algorithms 
implementations benchmarks final contribution dissertation suite implementations sequential parallel algorithms closest point problems 
sequential suite collects code sources adds implementations constructed 
parallel suite programs nearest neighbors delaunay triangulation construction experiments chapters 
parallel programs form basis new set benchmarks studying performance parallel computers programs solve proximity problems 
benchmark suite useful evaluating possible large machines applications algorithms 
addition algorithms exhibit behaviors typical irregular dynamic programs 
study architectural language support programs 
experience gained designing constructing implementations algorithms leads questions consideration 
algorithms 
chapter chapter able match experiments results known analytical results literature computational geometry 
experimental results chapters fairly conclusive formal proofs back heuristic arguments 
interesting open problem match mathematical analysis algorithms performance practice 
addition improvements extensions algorithms chapter possible 
main bottleneck algorithm cric memory management synchronization overhead resulting algorithm need maintain complicated shared data structure 
particular algorithm displays large amount read write sharing ksr means generates large amount invalidate traffic network 
newer shared memory machines faster cpus higher performance networks cache protocols improve absolute performance algorithm substantially 
experience gained algorithm suggests algorithmic refinements larger impact 
modifying algorithm better scheduling scheme guided self scheduling potentially reduce overhead insertion loops 
addition sophisticated locking protocol remove need barriers inner loops algorithm 
reduce runtime algorithm 
unclear construct protocol way avoid deadlock starvation especially algorithm accesses quad edge structure unpredictable fashion making deadlock avoidance difficult 
herlihy moss hm proposed extension standard cache coherency schemes called transactional memory large extent alleviate problems 
transactional memory allows programmer construct custom multi word read modify write operations act atomically shared data structures 
mechanism allow define insert object barriers complicated locking schemes 
restructured version algorithm cric takes advantage mechanism currently construction simulator transactional memory 
incremental search algorithm improved incorporating better data structure support site searches 
bentley proposed novel data structures supporting types queries higher dimensions ben 
interesting explore structures concurrent environment 
addition techniques chapter incremental search algorithm vectorized resulting better performance machines cray new cm hardware take advantage vector code 
particular am investigating implementation algorithms blelloch nesl language ble allow vector style implementation run platforms 
possible apply methods algorithms problems applications 
randomized incremental framework applied wide variety problems 
particular explore segment intersection higher dimensional convex hull delaunay triangulation construction algorithms 
problems applications potentially large support multiprocessor 
addition exploring algorithms applications closest point problems tsp problem minimum spanning trees clustering lead insights practical nature parallel algorithms 
programming parallel algorithms 
parallel algorithms need easier program 
algorithm thesis built hand 
limited scope investigation small number algorithms restricted class problems 
comprehensive studies practical parallel algorithms feasible portable high level library implementing basic parallel primitives built 
current trend machines difficult program current architectures making library necessary 
data parallelism data abstraction 
packaging large set useful data oriented parallel primitives providing efficient access data parallel languages provide users high level abstraction modest cost 
currently data parallel style associated aggregate operations collection oriented structures vectors 
study mechanisms necessary expand data abstractions available programmer parallel architectures 
particular developing transactional style programming similar chapter promising avenue 
style critical characteristics shared memory programming single address space easier programming multiple address spaces explicitly placing moving data 
complicated data structures irregular dynamic patterns shared memory models provide large gain programmer efficiency exchange modest loss runtime efficiency 
atomic operations herlihy moss hm show simple extensions current multiprocessor memory cache management schemes provide programmer extendible set atomic operations build transactional data abstractions complex locking protocols 
order idea usable practice needs accomplished 
machines built suitable cache management primitives 
notations specifying concurrency concurrent operations data structures designed incorporated languages 
languages notations translated new compilers runtime environment suitable task 
algorithms utilize transactional style developed test tune primitives offered system 
high level tools available workstations migrate multiprocessor machines 
high level programming languages program instrumentation profilers data analysis tools algorithm animators debugging tools needed new machines 
thesis concentrated kinds tools ad hoc fashion usually line actual target machine 
challenge integrating tools coherent efficient systematic programming environment multiprocessors 
simulation performance analysis 
thesis simulations played large role design evaluation algorithms 
addition execution driven machine simulation main vehicle studying questions parallel architecture 
simulation systems potential allow algorithms designers easily observe behavior programs relatively realistic setting 
importantly simulators free designers needing develop machine dependent versions algorithms order evaluate performance wide range architectures 
parallel architectures run simulations allow large applications prototyped tested simulated environment giving realistic results 
having new applications available simulators help architecture systems community 
study novel parallel algorithms large impact design machines architectural decisions studying behavior standard suites benchmarks 
suites need expanded include diverse set algorithms especially algorithms cric cis exhibit irregular dynamic patterns read write sharing 
ideally accurately characterize bottlenecks irregular programs design evaluate new mechanisms relieving bottlenecks 
simulation systems available problems possible areas research 
include managing data produced large experiments querying large databases useful information visualizing performance trends dynamic time dependent program behavior feeding performance information back machine models accurately predict performance related algorithms machines 
simulations concert high level performance models compilers help programmers tune applications better performance 
combination static flow analysis dynamic information collected runtime simulation example tune data placement increase locality restructure programs decrease synchronization costs 
profiler compiler feedback loops restructure sequential program various ways 
acf alpern carter 
uniform memory hierarchies 
acm symposium theory computing pages 
acg aggarwal chazelle guibas yap 
parallel computational geometry 
algorithmica 
acg cole goodrich 
cascading divide conquer technique designing parallel algorithms 
siam journal computing 
acs aggarwal chandra snir 
communication latency pram computations 
annual acm symposium parallel algorithms architectures pages 
acs aggarwal chandra snir 
communication complexity prams 
theoretical computer science 
aggarwal guibas saxe shor 
linear time algorithm computing voronoi diagram convex polygon 
discrete computational geometry 
aggarwal fields 
limitless directories scalable cache coherence scheme 
proceedings fourth asplos pages 
aurenhammer 
voronoi diagrams survey geometric data structure 
computing surveys 
bar brad barber 
computational geometry imprecise data arithmetic 
phd thesis princeton 
ben bentley 
multi dimensional divide conquer 
communications acm 
ben bentley 
writing efficient programs 
prentice hall 
ben bentley 
experiments travel salesman heuristics 
acm siam symposium discrete algorithms pages 
ben bentley 
trees point sets 
proc 
th annual acm symposium computational geometry pages 
ben bentley 
tools experiments algorithms 
rashid editor cmu computer science th anniversary chapter 
acm press 
bey bern eppstein yao 
expected extremes delaunay triangulation 
international journal computational geometry applications 
bhg bernstein hadzilacos goodman 
concurrency control recovery database systems 
addison wesley 
ble blelloch 
scans primitive parallel operators 
ieee transactions computers 
ble blelloch 
vector models data parallel computing 
mit press 
ble blelloch 
nesl nested data parallel language version 
technical report cmu cs cmu 
blm blelloch leiserson maggs plaxton smith zagha 
comparison sorting algorithms connection machine cm 
annual acm symposium parallel algorithms architectures 
bentley weide yao 
optimal expected time algorithms closest point problems 
acm transactions mathematical software 
cbf chatterjee blelloch fisher 
size access inference data parallel programs 
technical report cmu 
chatterjee blelloch zagha :10.1.1.44.9578
scan primitives vector computers 
proceedings supercomputing pages 
ccl chen choo li 
parallel programs optimizing performance 
journal supercomputing 
cf cox fowler 
implementation coherent memory abstraction numa multiprocessor 
platinum 
proceedings twelfth sosp pages 
cole goodrich 
merging free trees parallel efficient voronoi diagram construction 
lncs pages 
cha chatterjee 
compiling data parallel programs efficient execution sharedmemory multiprocessors 
phd thesis school computer science cmu 
che chernoff 
measure asymptotic efficiency tests hypothesis sum observations 
annals mathematical statistics 
cho chow 
parallel algorithms computational geometry 
phd thesis univ illinois 
culler karp patterson schauser santos von eicken 
logp realistic model parallel computation 
technical report ucb cds berkeley 
cla clarkson 
algorithms closest point problems 
phd thesis stanford univeristy 
clr cormen leiserson rivest 
algorithms 
mit press mcgraw hill 
cms clarkson mehlhorn seidel 
results randomized incremental construction 
annual symposium theoretical aspects computer science 
cohen miller stout 
efficient domination algorithms fine medium grain hypercube computers 
algorithmica 
cor cormen 
virtual memory data parallel computing 
phd thesis department electrical engineering computer science massachusetts institute technology 
available technical report mit lcs tr 
cs clarkson shor 
applications random sampling computational geometry ii 
discrete computational geometry 
cz cole ofer 
incorporating asynchrony pram model 
annual acm symposium parallel algorithms architectures pages 
dehne 
computing digitized voronoi diagrams systolic screen applications clustering 
optimal algorithms pages 
devillers 
fully dynamic delaunay triangulation logarithmic expected time operation 
technical report inria 
ds dobkin silver 
applied computational geometry robust solutions basic problems 
journal computer system sciences 
dwyer 
faster divide conquer algorithm constructing delaunay triangulations 
algorithmica 
dwyer 
average case analysis algorithms convex hulls voronoi diagrams 
phd thesis cmu 
dwyer 
higher dimensional voronoi diagrams linear expected time 
discrete computational geometry 
ek eggers katz 
evaluating performance snooping cache coherency protocols 
proceedings sixteenth isca pages 
es edelsbrunner shi 
time algorithm threedimensional convex hull problem 
siam journal computing 
fjl fox johnson otto salmon walker 
solving problems concurrent processors 
prentice hall 
fortune 
sweepline algorithm voronoi diagrams 
algorithmica 
fortune 
stable maintenance point set triangulations dimensions 
ieee symposium foundations computer science pages 
fortune 
numerical stability algorithms delaunay triangulations voronoi diagrams 
annual acm symposium computational geometry 
gg goodrich 
place techniques parallel convex hull algorithms 
annual acm symposium parallel algorithms architectures pages 
gib gibbons 
practical pram model 
annual acm symposium parallel algorithms architectures pages 
gks guibas knuth sharir 
randomized incremental construction delaunay voronoi diagrams 
algorithmica 
gs green sibson 
computing dirichlet tessellations plane 
computing journal 
gs guibas stolfi 
primitives manipulation general subdivisions computation voronoi diagrams 
acm transactions graphics 
goodman vernon 
efficient primitives large scale cache coherent multiprocessors 
proceedings third asplos pages 
gw goodman 
wisconsin new large scale multiprocessor 
proceedings fifteenth isca pages 
kennedy tseng 
compiler support machineindependent parallel programming fortran technical report rice univ 
hm herlihy moss 
transactional memory architectural support lock free data structures 
technical report dec cambridge research lab 
hr hagerup rub 
guided tour chernoff bounds 
information processing letters 
hr ranka 
practical hierachical model parallel computation model 
technical report syracuse university 
jl jeong lee 
parallel geometric algorithms mesh connected computer 
algorithmica 
kk 
constructing delaunay triangulations merging buckets quad tree order 
unpublished manuscript 
kmr koelbel mehrotra van 
supporting shared data structures distributed memory architectures 
second acm sigplan symposium principles practice parallel programming pages 
kr karp ramachandran 
parallel algorithms shared memory machines 
van leeuwen editor handbook theoretical computer science 
elsevier science publishers 
krs kruskal rudolph snir 
complexity theory efficient parallel algorithms 
theoretical computer science 
ks kirkpatrick seidel 
ultimate planar convex hull algorithm 
th annual allerton conference communication pages 
ksr kendall square research 
ksr programming 
lem lem 
cybernetic age 
harvest 
translated michael kandel 
lawson hanson kincaid krogh 
basic linear algebra subprograms fortran usage 
acm transactions mathematical software 
lkl levcopoulos lingas 
optimal expected time parallel algorithm voronoi diagrams 
swat pages 
lenoski laudon gharachorloo gupta 
directory protocol dash multiprocessor 
proceedings seventeenth isca pages 
lenoski laudon joe stevens gupta hennessy 
dash prototype implementation performance 
proceedings isca pages 
lm leiserson maggs 
communication efficient parallel algorithms distributed random access machines 
algorithmica 
ls lee 
algorithms constructing delaunay triangulation 
int 
information science 
lu lu 
constructing voronoi diagram mesh connected computer 
ieee conference parallel processing pages 
mau maus 
delaunay triangulation convex hull points expected linear time 
bit 
mcg mcgeoch 
experimental analysis algorithms 
phd thesis school computer science cmu 
mcs mellor crummey scott 
algorithms scalable synchronization shared memory multiprocessors 
acm transactions computer systems 
mer merriam 
parallel implementation algorithm delaunay triangulation 
european computational fluid dynamics conference pages 
mil milenkovic 
verifiable implementations geometric algorithms finite precision arithmetic 
technical report school computer science cmu 
mnp mills prins reif wagner 
prototyping parallel distributed algorithms proteus 
technical report duke university 
ms mackenzie stout 
ultra fast expected time parallel algorithms 
symposium discrete algorithms pages 
nat 
logarithmic time cost optimal parallel sorting fast practice 
supercomputing pages 
nv nodine vitter 
greed sort optimal external sorting algorithm multiple disks 
technical report cs department computer science brown university 
obs okabe barry boots sugihara 
spatial tessellations concepts applications voronoi diagrams 
john wiley sons 
ohya iri 
improvements incremental method voronoi diagram computational comparison various algorithms 
journal fo operations research society japan 
phr prins hightower reif 
implementations randomized sorting large parallel machines 
annual acm symposium parallel algorithms architectures 
pk polychronopoulos kuck 
guided self scheduling practical scheduling scheme parallel supercomputers 
ieee transactions computers 
ps preparata shamos 
computational geometry 
springerverlag 
quinn hatcher 
compiling programs hypercube multicomputer 
proceedings acm sigplan pages 
rs reif sen polling new randomized sampling technique computational geometry 
symposium theory computation 
rs reif sen optimal parallel randomized algorithms convex hulls related problems 
siam journal computing 
schnabel weaver 
dino parallel progamming language 
technical report univeristy colorado 
san 
integral geometry geometric probability 
addison wesley reading ma 
sh shamos hoey 
closest point problems 
proc 
sixteenth focs pages 
sh steele hillis 
data parallel algorithms 
communications acm 
sto stojmenovic 
computational geometry hypercube 
international conference parallel processing pages 
sy sharir yaniv 
randomized incremental construction delaunay diagrams theory practice 
annual acm symposium computational geometry 
ogawa 
new algorithm tessellation 
journal computational physics 
teng sullivan 
data parallel algorithm threedimensional delaunay triangulation implementation 
submitted manuscript 
val valiant 
bridging model parallel computation 
communications acm august 
vs vitter shriver 
algorithms parallel memory level memories 
technical report cs department computer science brown university august 
revised version technical report cs 
varadarajan 
efficient expected time parallel algorithm voronoi construction 
annual acm symposium parallel algorithms architectures pages 
wei weide 
statistical methods analysis algorithms 
phd thesis school computer science cmu 
greenberg wheat 
core making parallel computer practical 
dags june 
wl wilson 
hiding shared memory latency net distributed shared memory architecture 
journal parallel distributed computing august 
wol michael wolfe 
optimizing supercompilers supercomputers 
mit 
yap yap 
geometric consistency theorem symbolic scheme 
journal computer system sciences 
zb zagha blelloch 
radix sort vector multiprocessors 
proceedings supercomputing pages november 
