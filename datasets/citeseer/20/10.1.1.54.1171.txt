improving accuracy speed support vector machines chris burges bell laboratories lucent technologies room crawford corner road holmdel nj burges bell labs com bernhard scholkopf max planck institut fur kybernetik 
tubingen germany bs mpg de support vector learning machines svm finding application pattern recognition regression estimation operator inversion ill posed problems 
general backdrop methods improving generalization performance improving speed test phase svms increasing interest 
combine techniques pattern recognition problem 
method improving generalization performance virtual support vector method incorporating known invariances problem 
method achieves drop error rate nist test digit images 
method improving speed reduced set method approximating support vector decision surface 
apply method achieve factor speedup test phase virtual support vector machine 
combined approach yields machine times faster original machine better generalization performance achieving error 
virtual support vector method applicable svm problem known invariances 
reduced set method applicable support vector machine 
support vector machines known give results pattern recognition problems despite fact incorporate problem domain knowledge 
part done research holmdel nj 
exhibit classification speeds substantially slower neural networks lecun 
study motivated observations 
shall improve accuracy incorporating knowledge invariances problem hand 
second shall increase classification speed reducing complexity decision function representation 
brings threads explored year scholkopf burges vapnik burges 
method incorporating invariances applicable problem data expected known symmetries 
method improving speed applicable support vector machine 
expect methods widely applicable problems pattern recognition example regression estimation problem vapnik smola 
brief overview support vector machines section describe problem domain knowledge improve generalization performance section 
section contains overview general method improving classification speed support vector machines 
results collected section 
conclude discussion 
support vector learning machines section summarizes properties support vector machines svm relevant discussion 
details basic svm approach reader referred boser guyon vapnik cortes vapnik vapnik 
noting physical analogy 
training data elements corresponding class labels sigma 
svm performs mapping phi 
high possibly infinite dimensional hilbert space vectors denoted bar 
svm decision rule simply separating hyperplane algorithm constructs decision surface normal psi separates classes psi delta gamma psi delta gamma positive slack variables introduced handle non separable case cortes vapnik typically defined gamma respectively 
psi computed minimizing objective function psi delta psi subject constant choose 
separable case svm algorithm constructs separating hyperplane margin positive negative examples maximized 
test vector assigned class label gamma depending psi delta phi greater 
support vectors defined training samples equations equality 
name support vectors distinguish rest training data 
solution psi may expressed psi ns ff phi ff positive weights determined training sigma class labels number support vectors 
order classify test point compute psi delta ns ff delta ns ff phi delta phi ns ff key properties support vector machines kernel compute dot products having explicitly compute mapping phi 
interesting note solution simple physical interpretation high dimensional space assume support vector exerts perpendicular force size ff sign solid plane sheet lying hyperplane psi delta solution satisfies requirements mechanical stability 
solution ff shown satisfy ns ff translates forces sheet summing zero equation implies torques sum zero 
improving accuracy section follows reasoning scholkopf burges vapnik 
problem domain knowledge incorporated different ways knowledge directly built algorithm generate artificial training examples virtual examples 
significantly slows training times due correlations artificial data increased training set size simard advantage readily implemented learning machine invariances 
instance lie groups symmetry transformations dealing discrete symmetries bilateral symmetries vetter poggio bulthoff derivative methods simard applicable 
support vector machines intermediate method combines advantages approaches possible 
support vectors characterize solution problem sense training data removed system retrained solution unchanged 
furthermore support vectors errors close decision boundary sense lie exactly margin close 
different types svm built different kernels tend produce set support vectors scholkopf burges vapnik 
suggests algorithm train svm generate set support vectors fs ns generate artificial examples virtual support vectors applying desired invariance transformations fs ns train svm new set 
build class classifier procedure carried separately binary classifiers 
apart increase training time factor experiments technique disadvantage virtual support vectors support vectors second machine increasing number summands equation decreasing classification speed 
problem solved reduced set method describe 
improving classification speed discussion section follows burges 
consider set vectors nz corresponding weights fl psi nz fl phi minimizes fixed nz euclidean distance original solution ae psi gamma psi note ae expressed terms vectors expressed entirely terms functions kernel vectors input space fl nz called reduced set 
classify test point expansion equation replaced approximation psi delta nz fl delta nz fl goal choose smallest nz ns corresponding reduced set resulting loss generalization performance remains acceptable 
clearly allowing nz ns ae zero 
interestingly nontrivial cases nz ae case reduced set leads increase classification speed loss generalization performance 
note reduced set vectors support vectors necessarily lie separating margin support vectors training samples 
reduced set exactly cases general unconstrained conjugate gradient method find corresponding optimal fl exactly 
method finding reduced set computationally expensive final phase constitutes conjugate gradient descent space delta nz variables case typically order 
experimental results section accuracy mean generalization performance speed mean classification speed 
experiments mnist database handwritten digits comparison investigation lecun 
study error rate record held boosted convolutional neural network lenet 
start summarizing results virtual support vector method 
trained binary classifiers equation 
polynomial kernel delta combining classifiers gave error test set system referred orig 
generated new training data translating resulting support vectors pixel directions trained new machine parameters 
machine referred vsv achieved error test set 
results digit table 
note improvement accuracy comes cost speed approximately factor 
furthermore speed orig comparatively slow start lecun requiring approximately multiply adds table generalization performance improvement incorporating invariances 
ne nsv number errors number support vectors respectively orig refers original support vector machine vsv machine trained virtual support vectors 
digit ne orig ne vsv sv orig sv vsv table dependence performance reduced set system threshold 
numbers parentheses give corresponding number errors test set 
note test gives lower bound numbers 
digit vsv bayes test classification reduced caching results repeated support vectors burges 
order competitive systems comparable accuracy need approximately factor improvement speed 
approximated vsv reduced set system rs factor fewer vectors number support vectors vsv 
reduced set method computes approximation decision surface high dimensional space accuracy rs improved choosing different threshold equations 
computed threshold gave empirical bayes error rs system measured training set 
done easily finding maximum difference un normalized cumulative distributions values dot products psi delta original training data 
note effects bias reduced fact vsv rs trained shifted data original data 
absence validation set original training data provides reasonable means estimating bayes threshold 
serendipitous bonus vsv approach 
table compares results obtained threshold generated training procedure vsv system estimated bayes threshold rs system comparison table speed improvement reduced set method 
second fourth columns give numbers errors test set original system virtual support vector system reduced set system 
columns give system number vectors dot product computed test phase 
digit orig err vsv err rs err orig sv vsv sv rsv purposes see maximum possible effect varying threshold bayes error computed test set 
table compares results test set systems bayes threshold computed training set rs 
results digits combined error orig vsv roughly twice multiply adds rs factor fewer multiply adds orig 
reduced set conjugate gradient algorithm reduce objective function ae equation zero 
example digits ae reduced average factor algorithm stopped progress slow 
striking results achieved 
discussion systems lecun better error lenet error approximately multiply adds boosted lenet error approximately multiply adds 
clearly svms league rs system described requires approximately multiply adds 
svms clear opportunities improvement 
fact trained vsv system error choosing different kernel 
invariances example pattern recognition case small rotations varying ink thickness added virtual support vector approach 
virtual support vectors provide new information decision boundary measure information keep important vectors 
known invariances built directly svm objective function 
viewed approach function approximation reduced set method currently restricted assumes decision function functional form original svm 
case quadratic kernels reduced set computed analytically efficiently burges 
conjugate gradient descent computation general kernel inefficient 
re restriction lead analytical methods apply complex kernels 
wish vapnik smola drucker discussions 
burges supported arpa contract 
scholkopf supported des deutschen 
boser guyon vapnik training algorithm optimal margin classifiers fifth annual workshop computational learning theory pittsburgh acm 
bottou cortes denker drucker guyon jackel le cun muller sackinger simard vapnik comparison classifier methods case study handwritten digit recognition proceedings th international conference pattern recognition neural networks jerusalem burges simplified support vector decision rules th international conference machine learning pp 

cortes vapnik support vector networks machine learning pp 
lecun jackel bottou cortes denker drucker guyon muller sackinger simard vapnik comparison learning algorithms handwritten digit recognition international conference artificial neural networks ed 
fogelman gallinari pp 

scholkopf burges vapnik extracting support data task fayyad uthurusamy 
eds proceedings international conference knowledge discovery data mining aaai press menlo park ca scholkopf burges vapnik incorporating invariances support vector learning machines proceedings icann international conference artificial neural networks 
springer verlag berlin simard le cun denker tangent prop formalism specifying selected invariances adaptive network moody hanson lippmann advances neural information processing systems morgan kaufmann san mateo ca vapnik estimation dependences empirical data russian nauka moscow english translation springer verlag new york vapnik nature statistical learning theory springer verlag new york vapnik smola support vector method function approximation regression estimation signal processing submitted advances neural information processing systems vetter poggio bulthoff importance symmetry virtual views dimensional object recognition current biology 
