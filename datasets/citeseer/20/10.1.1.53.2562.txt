hq learning adaptive behavior press marco wiering jurgen schmidhuber marco idsia ch juergen idsia ch idsia corso ch lugano switzerland www idsia ch hq learning hierarchical extension learning designed solve certain types partially observable markov decision problems pomdps 
hq automatically decomposes pomdps sequences simpler subtasks solved memoryless policies learnable reactive 
hq solve partially observable mazes states previous pomdp 
keywords reinforcement learning hierarchical learning pomdps non markov subgoal learning 
problem 
sensory information usually insufficient infer environment state ceptual aliasing whitehead 
complicates goal directed behavior 
instance suppose instructions way station follow road traffic light turn left follow road traffic light turn right 
suppose reach traffic lights 
right thing need know second 
requires bit memory current environmental input sufficient 
widely reinforcement learning rl algorithms learning watkins watkins dayan td sutton fail task requires creating shortterm memories relevant events disambiguate identical sensory inputs observed different states 
limited markov decision problems mdps time probabilities possible states depend current state action 
partially observable markov decision problems pomdps littman traffic light problem difficult simple mdps observation may occur state environment different action responses may required 
pomdps generally considered difficult particularly nasty temporal credit assignment problem usually hard prior observations relevant affect short term memory contents 
deterministic finite horizon pomdps np complete littman general exact algorithms feasible small problems 
explains interest heuristic methods finding necessarily optimal solutions schmidhuber mccallum ring cliff ross jaakkola singh jordan 
unfortunately previous methods scale littman cassandra kaelbling 
presents hq learning novel approach finite state memory implemented sequence agents 
hq need model pomdp appears scale reasonably approaches 
alternative approaches larger scale pomdps see schmidhuber zhao wiering wiering schmidhuber 
inspiration 
select optimal action necessary memorize entire past general infeasible 
memories corresponding important previously achieved subgoals sufficient 
see recall traffic light scenario 
way memories relevant passed traffic light 
subgoals memory independent reactive policy rp carry safely 
overview 
hq learning attempts exploit situations 
divide conquer strategy discovers subgoal sequence decomposing pomdp sequence reactive policy problems 
solved rps states causing identical inputs require optimal action 
critical points corresponding transitions rp 
deal transitions hq uses multiple rpp solving 
agent rp adaptive mapping observations actions 
time agent active system type short term memory embodied pointer indicating 
bits information conveyed limited kind short term memory 
answer logarithm number agents additional information conveyed system rps subgoal generators tends require bits course 
rps different agents combined way learned agents 
active agent uses subgoal table hq table generate subgoal subgoals represented desired inputs 
follows policy embodied function achieves subgoal 
control passed agent procedure repeats 
goal achieved time limit exceeded agent adjusts rp subgoal 
done learning rules interact explicit communication table adaptation slight modifications learning 
hq table adaptation tracing successful subgoal sequences learning higher subgoal level 
effectively subgoal rp combinations leading higher rewards chosen 
agent rp represented memoryless lookup table system solve non markovian tasks impossible learn single lookup tables 
singh system lin hierarchical learning method depend external teacher provides priori information subtasks 
jaakkola method limited finding suboptimal memoryless stochastic policies pomdps optimal memory deterministic solutions 
outline 
section describes hq learning details including learning rules hq tables 
section describes experiments relatively complex partially observable mazes world states 
demonstrate hq ability decompose pomdps appropriate 
section reviews related 
section summarizes hq advantages limitations 
section concludes lists directions research 
hq learning pomdp specification 
system life separable trials 
trial consists tmax discrete time steps tmax agent solves problem fewer tmax time steps 
pomdp specified 
fl finite set environmental states initial state finite set observations function mapping states ambiguous observations finite set actions theta ir maps state action pairs scalar reinforcement signals fl discount factor trades immediate rewards rewards theta theta ir state transition function js denotes probability transition state environmental state time action executed time system goal obtain maximal discounted cumulative reinforcement trial 
pomdps rpp sequences 
optimal policy deterministic pomdp final goal state decomposable finite sequence optimal policies appropriate subgoals determining transitions rpp 
trivial decomposition consists single state corresponding subgoals 
general pomdps decomposition trivial hard efficient algorithm solving 
hq aimed situations require transitions 
hq architecture implements transitions passing control rpp solving 
architecture 
ordered sequence agents 
cm equipped table hq table control transfer unit cm table see 
agent responsible learning part system policy 
table represents local policy executing action input 
matrix size joj theta jaj joj number different possible observations jaj number possible actions 
denotes value utility action observation agent current subgoal generated help hq table vector joj elements 
possible observation hq table entry representing estimated value subgoal 
hq denotes hq value utility selecting subgoal 
system current policy policy currently active agent 
active time step denote active variable active represents kind short term memory system 
architecture limitations 
sequential architecture restricts pomdp types hq solve 
see consider difference rl goals achievement maintenance 
refer single state goals find exit maze maintaining desirable state time keeping pole balanced 
current hq variant handles achievement goals 
case maintenance goals eventually run agents explicit final desired state restriction may overcome different agent topologies scope 
selecting subgoal 
active 
active hq table select subgoal explore different subgoal sequences max random rule subgoal maximal hq value selected probability pmax random subgoal selected probability gamma pmax conflicts multiple subgoals maximal hq values solved randomly selecting 
denotes subgoal selected agent subgoal transfer control defined confused observation 
selecting action 
action choice depends current observation learning time active agent select actions max boltzmann distribution probability pmax take action maximal value probability gamma pmax select action traditional boltzmann gibbs distribution probability selecting action observation jo ak ak temperature adjusts degree randomness involved agent action selection case boltzmann rule 
conflicts multiple actions maximal values solved randomly selecting 
transfer control 
control transferred active agent follows 
time executed action control transfer unit checks reached goal 
checks solved subgoal decide control passed denote time agent active system start set 
transfer control transfer control table table table agent agent agent hq table hq table basic architecture 
agents connected sequential way 
agent table hq table control transfer unit agent table 
table stores estimates actual observation action values select action 
hq table stores estimated subgoal values generate subgoal agent active 
solid box indicates second agent currently active agent 
agent achieved subgoal control transfer unit passes control successor 
goal state reached current subgoal active active active learning rules line learning updating tables means storing experiences postponing learning trial intra trial parameter adaptation 
principle online learning applicable see 
describe hq variants learning learning overcomes inability solve certain 
learning rules appear similar conventional 
major difference agent prospects achieving subgoal tend vary various agents try various subgoals 
learning values 
want approximate system expected discounted reward executing action step lookahead case jo theta fl sk active jo theta denotes probability system state time observation architecture parameters denoted theta information active 
hq learning depend estimating probability belief vectors littman world model moore help speed learning 
utility observation agent equal value best action maxa fq value updates generated different situations tmax denotes total number executed actions current trial ff learning rate denote agents active times possibly gamma ff ff 
agent active time final action executed ot gamma ff ot ff qr st 
note final reward reaching goal state tmax main difference standard step learning agents trained values see 
learning hq values intuition 
recall traffic light task 
traffic light subgoal 
want system discover exploring initially random subgoals learning hq values 
traffic light hq value instance converge expected discounted cumulative reinforcement obtained chosen subgoal 

traffic light reached agent passes control expectation reward update hq values 
expectations originate 
reflect experience final reward obtained station 
formally 
optimal case hq fl gammat hv denotes average possible trajectories 
gamma fl gammat discounted cumulative reinforcement time active note time interval states encountered depend subtask hv estimated discounted cumulative reinforcement received adjust hq values agents active trial denotes number agents active trial ff hq denotes learning rate chosen subgoal agent hq invoked agent cn gamma update hq gamma ff hq hq ff hq fl gammat hv 
hq cn gamma hq gamma ff hq hq ff hq fl gammat rn 
hq cn hq gamma ff hq hq ff third rules resemble traditional learning rules 
second rule necessary agent cn learned possibly high value subgoal unachievable due subgoals selected previous agents 
hq learning motivation 
learning lookahead capability restricted step 
solve properly assign credit different actions leading identical states whitehead 
instance suppose walk wall looks middle picture 
goal reach left corner reward 
rpp solvable rp 
picture input learning step look ahead assign equal values actions go left go right yield identical wall observations 
consequently hq learning may suffer learning inability solve certain 
overcome problem augment hq td methods evaluating improving policies manner analogous lin offline method 
td methods integrate experiences successive steps disambiguate identical short term effects different actions 
experiments indicate solvable learning sufficiently high tables compute desired values ot st fl gamma active update values qn gamma ff ff qq hq hq tables compute desired hq values hq hq rn hq gamma gamma rn gamma fl gammat rn hq fl gammat gamma hv hq hq update hq values agents min gamma hq gamma ff hq hq ff principle online may 
see peng williams wiering schmidhuber fast implementation 
online action penalty koenig simmons punishing varying actions response ambiguous inputs trap agent cyclic behavior 
combined dynamics 
table policies reactive learn solve 
hq table policies composing rpp sequences 
tables hq tables explicitly communicate influence simultaneous learning 
cooperation results complex dynamics quite different conventional learning 
utilities subgoals rps estimated tracking part successful subgoal rp combinations 
subgoals rarely occur solutions chosen 
certain sense subtasks compete assigned subgoal choices evolve rps 
maximizing expected utility agent implicitly takes account frequent decisions agents 
agent eventually settles particular rpp solvable rp ceases adapt 
illustrated experiment section 
estimation average reward choosing particular subgoal ignores dependencies previous subgoals 
local minima possible 
rewarding suboptimal subgoal sequences close subgoal space optimal may probable suboptimal ones 
show experiments happen 
exploration issues 
initial choices subgoals rps may influence final result may local minimum traps 
exploration partial remedy encourages alternative competitive strategies similar current 
little exploration may prevent system discovering goal 
exploration prevents reliable estimates current policy quality reuse previous successful rps 
avoid exploration max boltzmann max uniform distribution values hq values discussions exploration issues see fedorov schmidhuber thrun cohn dorigo hochreiter schmidhuber wilson schmidhuber 
distributions easy reduce relative weight exploration opposed exploitation obtain deterministic policy learning process increase pmax learning achieves maximum value 
selecting actions traditional boltzmann distribution causes problems hard find values temperature parameter 
degree exploration depends values actions identical values certain input executed equally 
instance suppose sequence different states maze leads observation sequence gamma gamma gamma gamma represents single observation 
suppose equal values going west east response updates hardly change differences values 
resulting random walk behavior cost lot simulation time 
rp training prefer max boltzmann rule 
focuses greedy policy explores actions competitive optimal actions 
subgoal exploration critical partially observable maze pom 
task find path leading start goal optimal solution requires steps reactive agents 
shows possible sub optimal solution costs steps 
asterisks mark appropriate subgoals 

max random subgoal exploration rule may replaced max boltzmann rule 
experiments tested system tasks partially observable environments 
task comparatively simple serve exemplify hq discovers stabilizes appropriate subgoal combinations 
requires finding path start goal partially observable theta maze collectively solved agents 
study system performance agents added 
second quite complex task involves finding key opens door blocking path goal 
optimal solution requires agents costs steps 
learning solve partially observable maze task 
experiment involves partially observable maze shown 
system discover path leading start position goal actions obvious semantics go west go north go east go south 
possible observations computed adding field values blocked fields agent position field value west north east south field respectively agent see adjacent fields blocked 
possible agent positions highly ambiguous inputs 
possible observations occur maze 
means system may occasionally generate unsolvable subgoals control transferred agent 
deterministic memory free policy solving task 
stochastic memory free policies perform poorly 
instance input stands fields left right agent blocked 
optimal action response input depends subtask trial go north go south near go north 
reactive agents necessary solve agents agents agents agents agents optimal agents agents agents optimal left hq learning results partially observable maze agents 
plot average test run length trial numbers means simulations 
system converges near optimal solutions 
required agents tends improve performance 
right results agents actions corrupted noise 
cases find goal noisy actions decrease performance 
pomdp 
reward function 
system hits goal receives reward 
reward zero 
discount factor fl 
parameters experimental set 
compare systems agents noise free actions 
compare systems agents actions selected learning testing replaced random actions probability 
experiment consists simulations system 
simulation consists trials 
tmax 
th trial test run actions subgoals maximal table entries selected max set 
system find goal test run trial outcome counted steps 
coarse search parameter space parameters experiments ff ff hq hq tables tables 
pmax set linearly increased 
table entries initialized 
purposes comparison ran trials actions picked randomly 
tried learning augmented follows current input cartesian product current observation observation different current observation 
theory variant able solve problem 
results 
augmented failed find stable solutions 
hq worked 
plots average test run length trial numbers 
trials systems find near optimal deterministic policies 
consider table 
largest systems able decompose pomdp sequence 
average number steps close optimal 
approximately cases optimal step path 
cases step solutions 
number step solutions larger number step solutions appropriate subgoal combinations result surprising 
systems agents performing better system profits having free parameters 
agents help 
systems perform significantly table hq learning results random actions replacing selected actions probability 
table entries refer final test trial 
nd column lists average trial lengths 
rd column lists goal hit percentages 
th column lists average path lengths solutions 
th column lists percentages simulations optimal path 
system av 
steps goal av 
sol 
optimal agents agents agents agents agents agents noise agents noise agents noise random better random system finds goal step trials 
case noisy actions probability replacing selected action random action systems reach goal simulations see 
final trial simulation systems agents find goal probabilities percent respectively 
significant difference smaller larger systems 
studied system adds agents learning process 
agent system solutions agents simulations 
agents tends things easier 
trials agents average final trials 
agents tend give better results 

systems fail solve task subgoals start subgoals successful 
subgoals possibilities compose paths lower probability finding shortest path maze 
experimental analysis 
system discover stabilize subgoal combinations scs 
optimal step solution uses observation subgoal th top field observation second southwest inner corner 
step solutions scs 
shows scs evolve plotting trials observation stands unsolved subgoal 
scs quite random second agent able achieve subgoal 
system gradually focuses successful scs 
useful scs occasionally lost due exploration alternative scs near simulation system converges sc 
goal hardly prior trial show 
sudden jump performance trials cost just steps 
moment observation second subgoal cases goal 
subgoal tends vary observations 
trials subgoal settles observation observation 
reasons faster stabilization second subgoal may proximity final goal larger number successful subgoal combinations participates 
second third agents learned rps leading second subgoal goal second subgoal hq value increased dramatically dominate alternatives 
second subgoal firmly established similar effect help stabilize 
subgoals tend get fixed reverse order online generation 
agent system subgoal subgoal trials subgoal combinations scs generated agent system sampled intervals trials 
initially different scs tried 
trials hq explores scs converges sc 
key door task 
second experiment involves theta maze shown 
starting system fetch key position move door shaded area normally behaves wall open disappear agent possession key proceed goal different highly ambiguous inputs key door observed free field wall 
optimal path takes steps 
reward function 
system hits goal receives reward 
actions reward 
additional intermediate reward key going door 
discount factor fl 
parameters 
experimental set analogous section 
systems agents systems agents actions corrupted different amounts noise 
ff ff hq 
pmax linearly increased 
hq tables tables table entries zero initialized 
simulation consists trials 
results 
ran step trials system executing random actions 
goal 
ran random system step trials 
shortest path took steps 
observe goal discovery steps action penalty negative reinforcement signals executed action partially observable maze containing key door grey area 
starting system find key open door proceed goal shortest path costs steps 
optimal solution requires reactive agents 
number possible world states 
agents agents agents agents optimal noise noise noise optimal left hq learning results key door problem 
plot average test run length trial number means simulations 
trials systems agents find deterministic policies simulations 
right hq learning results agent system actions replaced random actions probability 
happen 
table show hq learning results noise free actions 
trials deterministic policies simulations 
optimal step paths agents simulations 
runs lead solutions goal rarely 
reflects general problem pomdp case exploration issues trickier mdp case remains done better understand 
table results hq learning simulations key door task 
table entries refer final test trial 
second column lists average trial lengths 
third lists goal hit percentages 
fourth lists average path lengths solutions 
fifth lists percentages simulations optimal step path 
hq learning solve task limit steps trial 
random search needed step limit 
system av 
steps goal av 
sol 
optimal agents agents agents agents agents noise agents noise agents noise random random actions taken cases agent system finds goal final trials see table 
cases long paths steps 
best solutions steps 
interestingly little noise decrease performance noise lead worse results 
previous authors proposed hierarchical reinforcement learning techniques schmidhuber dayan hinton moore atkeson tham sutton 
methods designed mdps 
focus pomdps section limited brief summary previous pomdp approaches specific advantages disadvantages 
recurrent neural networks 
schmidhuber uses interacting gradient recurrent networks 
model network serves model predict environment uses model net compute gradients maximizing reinforcement predicted model extends ideas nguyen widrow jordan rumelhart 
knowledge presents successful reinforcement learning application simple nonmarkovian tasks learning flipflop 
lin uses combinations controllers recurrent nets 
compares time delay neural networks recurrent neural networks 
despite theoretical power standard recurrent nets run practical problems case long time lags relevant input events 
attempts overcoming problem schmidhuber bengio hochreiter schmidhuber reinforcement learning applications 
belief vectors 
kaelbling littman cassandra hierarchically build policy trees calculate optimal policies stochastic partially observable environments 
possible environmental state belief vector provides agent estimate probability currently state 
observation belief vector updated action observation models bayes formula 
compresses history previous events probability distribution 
belief state optimal action chosen sondik 
dynamic programming algorithms compute optimal policies belief states 
problems approach nature underlying mdp needs known computationally expensive 
methods speeding focus constructing compact policy trees 
instance littman uses witness algorithm accelerate policy tree construction 
zhang liu propose scheme speeds dynamic programming updates uses oracle providing additional state information decrease uncertainty 
boutilier poole bayesian networks represent pomdps compact models accelerate policy computation 
parr russell gradient descent methods continuous representation value function 
experiments show significant speed ups certain small problems 
littman 
compare different pomdp algorithms belief vectors 
report small pomdps states actions pose big problem methods 
larger pomdps states cause major problems 
indicates problems current involve states hardly solved methods 
hq learning contrast computationally complex requires knowledge underlying mdp 
absence prior knowledge significant advantage 
advantage methods deal noisy perceptions actions 
possible hq extension belief vectors assign selection probabilities agent weigh values 
noise environments may better simple hq 
hidden markov models 
mccallum utile distinction memory extension chrisman perceptual distinctions approach combines hidden markov models hmms learning 
system able solve simple pomdps maze tasks fields splitting inconsistent hmm states agent fails predict utilities experiences quite different returns states 
problem approach solve problems conjunctions successive perceptions useful predicting reward independent perceptions irrelevant 
hq learning problem deals perceptive conjunctions multiple agents necessary 
memory bits 
littman uses branch bound heuristics find suboptimal memoryless policies extremely quickly 
handle mazes safe deterministic memoryless policy replaces conventional action actions having additional effect switching memory bit 
results obtained toy problem 
method scale due search space explosion caused adding memory bits 
cliff ross wilson classifier system pomdps 
memory bits set reset actions 
trained bucket brigade genetic algorithms 
system reported small problems unstable case memory bit 
usually able find optimal deterministic policies 
wilson described sophisticated classifier system uses prediction accuracy calculating fitness genetic algorithm working environmental niches 
study shows classifiers general accurate 
interesting see system memory solving pomdps 
problem memory bits tasks section require switching memory bits precisely right moment keeping switched long times 
learning exploration memory bit unstable change time algorithms incremental solution refinement usually great difficulties finding set reset 
probability changing memory bit response particular observation low eventually change observation frequently 
hq learning problems 
memory embodied solely active agent number rarely incremented trial 
stable 
program evolution memory cells mcs 
certain techniques automatic program synthesis evolutionary principles evolve short term memorizing programs read write mcs runtime teller 
method probabilistic incremental program evolution pipe schmidhuber 
pipe iteratively generates successive populations functional programs adaptive probability distribution possible programs 
iteration uses best program refine distribution 
stochastically generates better better programs 
mc pipe variant successfully solve tasks partially observable mazes 
memory bit approach mentioned previous paragraph population approaches easily programs memory 
serial machines evaluation tends computationally expensive hq 
learning control hierarchies 
ring system constructs bottom control hierarchy 
lowest level nodes primitive perceptual control actions 
nodes higher levels represent sequences lower level nodes 
disambiguate inconsistent states new higher level nodes added incorporate information hidden deeper past necessary 
system able quickly learn certain non markovian maze problems able generalize previous experience additional learning optimal policies old new task identical 
hq learning reuse policy generalize previous similar problems 
mccallum tree quite similar ring system 
uses prediction suffix trees see ron singer tishby branches reflect decisions current previous inputs actions 
values stored leaves correspond clusters instances collected stored entire learning phase 
statistical tests decide instances cluster correspond significantly different utility estimates 
cluster split 
mccallum experiments demonstrate algorithm ability learn reasonable policies large state spaces 
problem ring mccallum approaches depend creation th order markov model size time window sampling observations 
large approach suffer curse dimensionality 
consistent representations 
whitehead uses consistent representation cr method deal inconsistent internal states result perceptual aliasing due ambiguous input information 
cr uses identification stage execute perceptual actions collect information needed define consistent internal state 
consistent internal state identified single action generated maximize discounted reward 
identifier controller adaptive 
limitation method system means remembering information immediately perceivable 
hq learning profit remembering previous events long time periods 
levin search 
wiering schmidhuber levin search ls program space levin discover programs computing solutions large pomdps 
ls interest amazing theoretical properties broad class search problems optimal order computational complexity 
instance suppose algorithm solves certain type maze task steps positive integer representing problem size 
ls solve task steps 
wiering schmidhuber show ls may substantial advantages reinforcement learning techniques provided algorithmic complexity solutions low 
success story algorithm 
wiering schmidhuber extend ls obtain incremental method generalizing previous experience adaptive ls 
guarantee lifelong history policy changes corresponds lifelong history reinforcement accelerations success story algorithm ssa schmidhuber zhao schraudolph zhao schmidhuber schmidhuber zhao wiering 
lead significant speed ups 
ssa ls specific general approach allows plugging great variety learning algorithms 
instance additional experiments self referential system embeds policy modifying method policy ssa able solve huge pomdps states schmidhuber 
may possible combine ssa hq learning advantageous way 
multiple learners 
hq learning learning uses multiple qlearning agents 
major difference agents skills different agents focus different input features receive different rewards 
reward functions genetic algorithms 
important goal learn agent select part input space 
different learning methods implementing cooperative competitive strategies tested complex dynamic environment lead reasonable results 
describes nested learning technique multiple agents learning independent reusable skills 
generate quite arbitrary control hierarchies simple actions skills composed form complex skills 
learning rules selecting skills selecting actions 
may hard deal long reinforcement delays 
experiments system reliably learns solve simple maze task 
remains seen system reliably learn decompose solutions complex problems stable skills 
hq advantages limitations hq advantages 

pomdp algorithms need priori information pomdp total number environmental states observation function action model 
hq 

history windows hq learning principle handle arbitrary time lags events worth memorizing 
focus power really needed short history windows may included agent inputs take care shorter time lags 
orthogonal hq basic ideas 

reduce memory requirements hq explicitly store experiences different subgoal combinations 
estimates average reward choosing particular subgoal rp combinations stores experiences single sequence hq tables 
successful subgoal rp combinations 
hq approach advantageous case pomdp exhibits certain regular structure agent tends receive achievable similar rps reuse previous rpp solutions 

hq learning immediately generalize solved pomdps similar pomdps containing states requiring identical actions response inputs observed subtasks remain invariant 

learning hq learning allows representing rps subgoal evaluations function approximators look tables 
hq current limitations 

agent current subgoal uniquely represent previous subgoal histories 
means hq learning really get rid hidden state problem hsp 
hq hsp bad 
impossible build policy reacts differently identical observations may occur frequently 
appropriate hq policies exist 
hq remaining hsp may prevent hq learning optimal policy 
deal hsp think subgoal trees sequences 
possible subgoal sequences representable tree branches labeled subgoals nodes contain rps solving 
node stands particular history subgoals previously solved subtasks hsp 
tree grows exponentially number possible subgoals practically infeasible case large scale pomdps 
possible find reasonable compromise simple linear sequences full fledged trees 

case noisy observations transfer control may happen inappropriate times 
remedy may reliable inputs combining successive observations 

case noisy actions inappropriate action may executed right passing control 
resulting new subtask may solvable agent rp 
remedy similar mentioned may represent subgoals pairs successive observations 

possible observations subgoals tested infrequently 
may delay convergence 
overcome problem try function approximators look tables agent generate set multiple alternative subgoals 
instance subgoal set reached control transferred agent 

parameters maximal number agents maximal runtime need set advance 
critical may large storage requirements low 
low possible avoid wasting time cycling states 
summary 
hq learning novel method reinforcement learning partially observable environments 
non markovian tasks automatically decomposed subtasks solvable memoryless policies intermediate external reinforcement subgoals 
done ordered sequence agents discovering local control policy appropriate subgoal 
time step type memory carried name agent active 
experiments involve model free deterministic pomdps states pomdps literature 
results demonstrate hq learning ability quickly learn optimal near optimal policies 

current hq version restricted learning single linearly ordered subgoal sequences 
complex pomdps generalized hq architectures directed acyclic recurrent graphs may turn useful 
point view challenging problem exploration destructive exploration rules subgoal sequences 
improve pomdp exploration open question 
acknowledgments valuable comments discussions marco dorigo nic schraudolph luca gambardella sa zhao cristina stewart wilson anonymous referees 
boutilier poole 

computing optimal policies partially observable decision processes compact representations 
aaai proceedings thirteenth national conference artificial intelligence pages portland 
dorigo 

training agents 
technical report iridia universit libre de bruxelles 
chrisman 

reinforcement learning perceptual aliasing perceptual distinctions approach 
proceedings tenth international conference artificial intelligence pages 
aaai press san jose california 
cliff ross 

adding temporary memory 
adaptive behavior 
cohn 

neural network exploration optimal experiment design 
cowan tesauro alspector editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 
dayan hinton 

feudal reinforcement learning 
lippman moody touretzky editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 


emergent hierarchical control structures learning reactive hierarchical relationships reinforcement environments 
maes mataric meyer pollack wilson editors animals animats proceedings fourth international conference simulation adaptive behavior cambridge ma pages 
mit press bradford books 
fedorov 

theory optimal experiments 
academic press 
bengio 

hierarchical recurrent neural networks long term dependencies 
touretzky mozer hasselmo editors advances neural information processing systems pages 
mit press cambridge ma 
hochreiter schmidhuber 

long short term memory 
neural computation 


action selection methods reinforcement learning 
maes mataric meyer pollack wilson editors animals animats proceedings fourth international conference simulation adaptive behavior cambridge ma pages 
mit press bradford books 
jaakkola singh jordan 

reinforcement learning algorithm partially observable markov decision problems 
tesauro touretzky leen editors advances neural information processing systems pages 
mit press cambridge ma 
jordan rumelhart 

supervised learning distal teacher 
technical report occasional center cog 
sci massachusetts institute technology 
kaelbling littman cassandra 

planning acting partially observable stochastic domains 
technical report brown university providence ri 
koenig simmons 

effect representation goal directed exploration reinforcement algorithm 
machine learning 
levin 

universal sequential search problems 
problems information transmission 
lin 

reinforcement learning robots neural networks 
phd thesis carnegie mellon university pittsburgh 
littman 

memoryless policies theoretical limitations practical results 
cliff husbands wilson editors proc 
international conference simulation adaptive behavior animals animats pages 
mit press bradford books 
littman 

algorithms sequential decision making 
phd thesis brown university providence ri 
littman cassandra kaelbling 

learning policies partially observable environments scaling 
prieditis russell editors machine learning proceedings twelfth international conference pages 
morgan kaufmann publishers san francisco ca 
mccallum 

overcoming incomplete perception utile distinction memory 
machine learning proceedings tenth international conference 
morgan kaufmann amherst ma 
mccallum 

learning selective attention short term memory sequential tasks 
maes mataric meyer pollack wilson editors animals animats proceedings fourth international conference simulation adaptive behavior cambridge ma pages 
mit press bradford books 
moore atkeson 

prioritized sweeping reinforcement learning data time 
machine learning 
nguyen widrow 

truck backer upper example self learning neural networks 
proceedings international joint conference neural networks pages 
ieee press 
parr russell 

approximating optimal policies partially observable stochastic domains 
proceedings international joint conference artificial intelligence ijcai pages 
morgan kaufmann 
peng williams 

incremental multi step learning 
machine learning 
ring 

continual learning reinforcement environments 
phd thesis university texas austin austin texas 
ron singer tishby 

learning probabilistic automata variable memory length 
aleksander taylor editors proceedings computational learning theory 
acm press 
sa schmidhuber 

probabilistic incremental program evolution 
evolutionary computation 
see ftp ftp idsia ch pub pipe ps gz 
schmidhuber 

curious model building control systems 
proc 
international joint conference neural networks singapore volume pages 
ieee 
schmidhuber 

learning generate sub goals action sequences 
kohonen simula kangas editors artificial neural networks pages 
elsevier science publishers north holland 
schmidhuber 

reinforcement learning markovian non markovian environments 
lippman moody touretzky editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 
schmidhuber 

learning complex extended sequences principle history compression 
neural computation 
schmidhuber 

interesting 
technical report idsia idsia 
schmidhuber zhao schraudolph 

reinforcement learning policies 
thrun pratt editors learning learn 
kluwer 
press 
schmidhuber zhao wiering 

shifting inductive bias success story algorithm adaptive levin search incremental self improvement 
machine learning 
singh 

efficient learning multiple task sequences 
moody hanson lippman editors advances neural information processing systems pages san mateo ca 
morgan kaufmann 
sondik 

optimal control partially observable markov decision processes 
phd thesis unpublished doctoral thesis stanford university ca 
hochreiter schmidhuber 

reinforcement driven information acquisition nondeterministic environments 
proceedings international conference artificial neural networks volume pages 
ec cie paris 
sutton 

learning predict methods temporal differences 
machine learning 
sutton 

td models modeling world mixture time scales 
prieditis russell editors machine learning proceedings twelfth international conference pages 
morgan kaufmann publishers san francisco ca 
teller 

evolution mental models 
kenneth kinnear editor advances genetic programming pages 
mit press 
tham 

reinforcement learning multiple tasks hierarchical cmac architecture 
robotics autonomous systems 
thrun 

efficient exploration reinforcement learning 
technical report cmu cs carnegie mellon university 
watkins 

learning delayed rewards 
phd thesis king college oxford 
watkins dayan 

learning 
machine learning 
whitehead 

reinforcement learning adaptive control perception action 
phd thesis university rochester 
wiering schmidhuber 

solving pomdps levin search eira 
saitta editor machine learning proceedings thirteenth international conference pages 
morgan kaufmann publishers san francisco ca 
wiering schmidhuber 

fast online 
technical report idsia idsia 
preparation 
wilson 

zeroth level classifier system 
evolutionary computation 
wilson 

classifier fitness accuracy 
evolutionary computation 
wilson 

explore exploit strategies autonomy 
meyer wilson editors proc 
fourth international conference simulation adaptive behavior animals animats pages 
mit press bradford books 
zhang liu 

planning stochastic domains problem characteristics approximation 
technical report cs hong kong university science technology 
zhao schmidhuber 

incremental self improvement life time multi agent reinforcement learning 
maes mataric meyer pollack wilson editors animals animats proceedings fourth international conference simulation adaptive behavior cambridge ma pages 
mit press bradford books 
