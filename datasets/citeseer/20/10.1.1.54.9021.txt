artificial neural networks tutorial anil jain mao computer science department ibm almaden research center michigan state university harry road east lansing mi san jose ca accepted appear ieee computer special issue neural computing march 
numerous efforts developing intelligent programs von neumann centralized architecture 
efforts successful building general purpose intelligent systems 
inspired biological neural networks researchers number scientific disciplines designing artificial neural networks anns solve variety problems decision making optimization prediction control 
artificial neural networks viewed parallel distributed processing systems consist huge number simple massively connected processors 
resurgence interest field anns years 
article intends serve tutorial readers little knowledge anns enable understand remaining articles special issue 
discuss motivations developing anns basic network models main issues designing anns network architecture learning process 
successful application anns automatic character recognition 
artificial neural networks anns 
excitement anns 
basic models designing anns 
tasks anns perform efficiently 
main questions addressed tutorial article 
consider classes challenging problems interest computer scientists engineers pattern classification ii clustering categorization iii function approximation iv prediction forecasting optimization vi retrieval content vii control 
number successful attempts solve problems variety ann models 
successes anns popular tool problem solving 
pattern classification task pattern classification assign input pattern speech waveform handwritten symbol represented feature vector prespecified classes 
discriminant functions decision boundaries constructed set training patterns known class labels separate patterns different classes 
decision boundaries linear piece wise linear arbitrary shape see 
important issues pattern classification task feature representation extraction decision making 
known applications pattern classification character recognition speech recognition eeg waveform classification blood cell classification printed circuit board inspection 
clustering categorization clustering known unsupervised pattern classification training data known class labels 
clustering algorithm explores similarity patterns places similar patterns cluster see 
number clusters known priori 
clustering difficult problem pattern classification 
known clustering applications include data mining data compression exploratory data analysis 
function approximation set labeled training patterns input output pairs delta delta delta generated unknown function subject noise task function approximation find estimate say unknown function 
statistical literature problem referred regression 
estimated function fit training data arbitrary accuracy adjusting complexity 
important issue avoid fitting noisy training data see 
pattern classification posed function approximation problem 
various engineering scientific modeling problems require function approximation 
prediction forecasting set samples time sequence fy delta delta delta jt delta delta delta task predict sample time prediction forecasting significant impact decision making business science engineering daily life 
stock market prediction weather forecasting typical applications prediction forecasting techniques see 
optimization wide variety problems mathematics statistics engineering science medicine economics posed optimization problems 
optimization problem usually involves components set independent variables parameters referred state process ii objective function cost error function optimized iii set constraints exist 
goal optimization algorithm find state satisfying constraints objective function maximized minimized 
combinatorial optimization problem refers problem state variables discrete finite number possible values 
classical combinatorial optimization problem traveling salesperson problem tsp np complete problem 
content addressable memory von neumann model computation entry memory accessed address physical meaning terms content memory 
small error calculating address completely different item retrieved 
associative memory memory name implies accessed content 
content memory recalled partial input distorted content see 
associative memory extremely desirable building multimedia information databases 
control consider dynamic system defined tuple fu control input resulting output system time model adaptive control goal generate control input system follows desired trajectory determined model 
example model adaptive control engine idle speed control 
example throttle angle control input engine speed output system 
input throttle angle sets engine desired idle speed load torque zero 
adaptive control system various load torque values set engine different idle speeds 
goal engine idle speed control system adaptively generate throttle angle engine runs desired idle speed load torques 
engineering systems require adaptive control 
large number approaches proposed solving problems described 
successful applications approaches certain environments flexible perform outside domain designed 
field artificial neural networks provided alternative approaches solving problems 
established large number applications benefit anns 
artificial neural networks referred neural computation network computation connectionist models parallel distributed processing pdp massively parallel computing systems consisting extremely large number simple processors interconnections 
anns designed goal building intelligent machines solve complex problems pattern recognition optimization mimicking network real neurons human brain biological computation 
goal anns help understand brain simulating testing hypotheses network architecture learning 
purpose article serve tutorial readers little knowledge artificial neural networks 
rest article organized follows 
section provides brief biological neurons neural networks motivations developing anns relationship anns scientific disciplines brief note 
section basic neuron model discuss main issues designing anns network architecture ii learning process 
various ann models organized architecture learning process 
sections provide details known ann models multilayer perceptron kohonen self organizing maps art models hopfield network 
section discuss character recognition popular domain applying ann models 
concluding remarks section 
abnormal normal pattern classifier fitting noisy training data stock value associative memory airplane partially occluded clouds retrieved airplane engine controller load torque throttle angle idle speed tasks neural networks perform 
pattern classification clustering categorization function approximation prediction forecasting optimization tsp problem retrieval content engine idle speed control 
motivations tutorial article fundamentals artificial neural networks 
need provide brief biological neural networks reasons anns inspired biological neural networks ii network massively connected simple processors pdp model exhibits powerful computational capabilities iii biological neural network provides benchmark evaluating performance anns iv biological neural networks existence proof goal building intelligent machines 
biological neuron neural networks neuron nerve cell special biological cell essence life information processing ability 
neurons basic structural constituents brain credited ramon won nobel prize physiology medicine shared crucial discovery extensive interconnections cerebral cortex portion brain approximately neurons human located 
axon synapse nucleus dendrites cell body sketch biological neuron 
schematic drawing neuron shown 
neuron composed cell body soma types reaching tree branches axon dendrites 
cell body nucleus contains information hereditary traits plasma containing molecular equipment production material needed neuron 
cell membrane contains various types pumps maintain imbalances charge concentrations inside outside cell 
neuron receives signals impulses neurons dendrites receivers transmits signals generated cell body axon transmitter eventually branches strands 
terminals strands synapses 
synapse place contact neurons axon strand neuron dendrite neuron 
impulse reaches synapse terminal certain chemicals called neurotransmitters released 
neurotransmitters diffuse synaptic gap effect enhance inhibit depending type synapse receptor neuron tendency emit electrical impulses 
effectiveness synapse adjusted signals passing synapses learn activities participate 
dependence past history acts memory possibly responsible human ability remember 
cerebral cortex humans large flat sheet neurons mm thick surface area cm twice area standard computer keyboard 
amazing creation nature sphere volume typical size human brain surface area cm walnut appearance human brain provides cerebral cortex surface area times larger simple smooth spherical surface 
cerebral cortex contains neurons approximately number stars way 
different types neurons solely shape types functionally different neurons 
neurons massively connected complex denser today telephone networks 
neuron connected gamma neurons 
number interconnections depends location neuron brain type neuron 
total human brain contains approximately gamma interconnections 
neurons communicate short train pulses typically milliseconds duration 
message modulated frequency pulses transmitted 
frequency vary hertz times slower fastest switching speed electronic circuits 
complex perceptual decisions face recognition human brain quickly typically milliseconds 
decisions network neurons operational speed milliseconds 
implies computation involved take serial stages 
words brain runs parallel programs steps long perceptual tasks 
known step rule 
timing considerations show amount information sent neuron small bits 
implies critical information transmitted directly captured distributed interconnections comes name connectionist model 
magic permits slow computing elements perform extremely complex tasks rapidly 
key parallel distributed representation computation 
interested readers find introductory easily comprehensible material biological neurons neural networks 
artificial neural networks 
modern digital computers outperformed humans domain numeric computation related symbol manipulation 
humans effortlessly solve complex perceptual problems recognizing person crowd mere glimpse face fast speed extent dwarf world fastest computer 
exist remarkable difference performance 
biological computer employs completely different architecture von neumann architecture see table 
difference significantly affects type functions computational model best able perform 
numerous efforts developing intelligent programs von neumann centralized architecture 
efforts resulted generalpurpose intelligent programs 
anns inspired biological evidence attempt organizational principles believed human brain 
course implies achievement anns largely dependent depth understanding human brain comprehension 
von neumann computer biological computer complex simple processor high speed low speed large number separate processor integrated processor memory localized distributed non content addressable content addressable centralized distributed computing sequential parallel stored programs self learning reliability vulnerable robust expertise numerical symbolic perceptual problems manipulations operating defined poorly defined environment constrained unconstrained table von neumann computer versus biological computer 
hand successful ann may resemblance biological system 
ability model biological nervous system anns increase understanding biological functions 
example experimental psychologists neural networks model classical conditioning animal learning data years 
state theart computer hardware technology vlsi optical modeling simulation feasible 
long course evolution resulted human brain possess desirable characteristics von neumann computer modern parallel computers 
characteristics include massive parallelism distributed representation computation learning ability generalization ability adaptivity inherent contextual information processing fault tolerance low energy consumption 
hoped anns motivated biological neural networks possess desirable character human brain 
relationship disciplines field artificial neural networks interdisciplinary area research 
thorough study artificial neural networks requires knowledge neurophysiology cognitive science psychology physics statistical mechanics control theory computer science artificial intelligence statistics mathematics pattern recognition computer vision parallel processing hardware digital analog vlsi optical 
illustrates interaction artificial neural networks disciplines 
artificial neural networks receive inputs disciplines 
new developments disciplines continuously field anns 
hand artificial neural networks provide impetus disciplines terms new tools representations 
symbiosis necessary vitality neural network research 
communications disciplines ought encouraged 
neurophysiology psychology physics control theory artificial intelligence statistics mathematics pattern recognition computer vision parallel processing hardware neurophysiology physics statistics mathematics pattern recognition hardware computer science computer vision computer science control theory parallel processing artificial intelligence artificial neural networks cognitive science psychology cognitive science ann relationship disciplines 
shall discuss interactions anns disciplines 
shall primarily focus relationship anns pattern recognition artificial intelligence 
pattern recognition systems expected automatically classify describe cluster complex patterns objects measured properties features 
design pattern recognition system involves main steps data acquisition preprocessing ii representation feature extraction iii decision making clustering 
jain mao addressed number common links anns statistical pattern recognition spr 
close correspondence popular ann models traditional pattern recognition approaches 
quite relationships fully exploited build hybrid systems 
examples perceptron versus linear classifier vector quantization learning anns versus means clustering radial basis function network versus parzen window density estimation classifier 
spite close resemblance ann spr anns provided variety novel supplementary approaches pattern recognition tasks 
noticeably anns provided architectures known statistical pattern recognition algorithms mapped facilitate hardware implementation 
adaptivity anns crucial design pattern recognition systems terms generalization capability terms performance dynamic environments presence incomplete information training 
time anns derive benefit wellknown results statistical pattern recognition 
example generalization ability network related curse dimensionality problem statistical classifiers radial basis function networks share design issues parzen window classifiers 
efforts ann research related pattern recognition directed designing classifier generalization ability 
little devoted designing representation scheme problem neural networks 
artificial intelligence ai aims building intelligent machines perform tasks require cognition performed humans 
typical ai system major components representation reasoning learning see 
motivation derived psychology cognitive science mental representation natural language symbols sequential processing logic symbol manipulation traditional ai systems adopt symbolic representation ii searching reasoning rules logic knowledge database iii expert learning expert systems 
ai takes top strategy solve prob lems level commonsense psychology hypothesize processes solve problem 
problem solved single step break problem subproblems 
procedure continues solution obtained 
contrast anns distinct traditional ai employing parallel distributed processing pdp model uses network massively connected simple processing units connectionist model 
problems knowledge solutions represented coded numeric weights outputs distributed network 
motivated neurophysiology anns take bottom strategy start simple processing units move upward complexity studying interconnections collective behavior 
ai ann paradigms virtues deficiencies symbolic versus connectionist top versus bottom 
importantly virtues approach compensate deficiencies 
exclude approaches bias 
useful approach combine approaches building structured connectionist models 
brief historical review humans creatures long interested exploring mind originates brain computes 
efforts may traced back aristotle 
modern era computational neural modeling began pioneering mcculloch pitts introduced computational model neuron logical calculus neural networks 
mcculloch pitts classic widely read time read generating considerable interest years detailed logic networks consisting simple neurons 
networks proved capable universal computation boolean function 
major milestone anns rosenblatt perceptron 
achievement rosenblatt proof perceptron convergence theorem 
widrow hoff introduced mean square lms algorithm adaptive linear element 
nilsson book machine learning best written exposition linearly separable patterns hypersurfaces 
anns generated great deal enthusiasm 
appeared machine type computation 
enthusiasm minsky papert book demonstrated fundamental limitations computing power layer perceptrons 
showed certain simple computations xor problem solved layer perceptron 
believed limitations overcome multilayer perceptrons employ intermediate layers units hidden units input layer output layer 
difficult problem encountered designing multilayer perceptron credit assignment problem problem assigning credit hidden units network 
learning algorithm known time solve problem 
minsky papert thought profitable explore approaches artificial intelligence 
reasons research neural networks went 
neural network field completely abandoned 
number dedicated researchers continued develop neural network models 
important themes emerged associative content addressable memory selforganizing networks competitive learning 
number important publications appeared changed course ann research 
publication hopfield volume book rumelhart mcclelland influential publications 
hopfield introduced idea energy function statistical physics formulate new way understanding computation recurrent networks symmetric synaptic connections 
formulation explicit principle storing information dynamically stable attractors 
combinatorial optimization problems classical traveling salesperson problem formulated terms network energy function minimized network reaches stable state 
rumelhart hinton williams reported development backpropagation algorithm popularized multilayer perceptron solve wide variety pattern recognition problems 
fact development back propagation algorithm colorful history 
developed werbos ph thesis rediscovered independently places parker lecun 
years thousands researchers diverse fields neuroscience psychology medicine mathematics physics computer science engineering involved developing neural network models implementing models hardware vlsi optics software solving number important applications 
activities continue grow result successful applications ann models 
artificial neurons neural networks section provides overview anns 
computational models neurons introduced 
important issues network architecture learning discussed 
various ann models organized architecture learning algorithm involved 
computational models neurons mcculloch pitts proposed binary threshold unit computational model neuron 
schematic diagram mcculloch pitts neuron shown 
mcculloch pitts model neuron 
mathematical neuron computes weighted sum input signals delta delta delta generates output sum certain threshold output 
mathematically gamma delta unit step function synapse weight associated th input 
simplicity notation consider threshold weight gamma attached neuron constant input 
positive weights correspond excitatory synapses negative weights model inhibitory synapses 
mcculloch pitts proved suitably chosen weights synchronous arrangement neurons principle capable universal computation 
crude analogy table biological neuron wires interconnections model axons dendrites connection weights represent synapses threshold function approximates activity soma 
model mcculloch pitts contains number simplifying assumptions reflect true behavior biological neurons 
differences ffl biological neurons threshold devices graded response essentially nonlinear function inputs ffl biological neurons perform nonlinear summation inputs perform logical processing ffl biological neurons produce sequence pulses simple output value ffl biological neurons updated asynchronously 
mcculloch pitts neuron model started new era computational neural modeling 
biological neurons artificial neurons synapses connection weights axons output wires dendrites input wires soma activation function table analogy biological neurons artificial neurons 
mcculloch pitts neuron generalized ways 
obvious generalization activation functions threshold function piecewise linear sigmoid gaussian shown 
sigmoid function far frequently function anns 
strictly increasing function exhibits smoothness asymptotic properties 
standard sigmoid function logistic function defined exp gammafi fi slope parameter 
threshold piecewise linear sigmoid gaussian different types activation functions 
network architecture topology assembly artificial neurons called artificial neural network 
anns viewed weighted directed graphs nodes artificial neurons directed edges weights connections outputs neurons inputs neurons 
neural networks feedforward networks feedback recurrent networks single layer perceptron multilayer perceptron radial basis hopfield network kohonen som function nets art models competitive networks taxonomy network architectures 
connection pattern architecture various anns grouped major categories shown feedforward networks loop exists graph ii feedback recurrent networks loops exist feedback connections 
common family feedforward networks layered network neurons organized layers connections strictly direction layer 
fact networks loops rearranged form layered feedforward networks possible skip layer connections 
shows typical networks category 
discuss article networks radial basis function rbf networks see employ network architecture multilayer perceptrons different activation functions 
different connectivities exhibit different network behaviors 
generally speaking feedforward networks static networks input produce set output values sequence values 
feedforward networks memoryless sense response feedforward network input independent previous state network 
exception time delay feedforward network dynamics occurs different delay factors neurons network 
recurrent networks dynamic systems 
presenting new input pattern outputs neurons computed 
feedback paths inputs neuron modified leads network enter new state 
process repeated convergence 
obviously different mathematical tools employed treat different types networks 
dynamic systems described differential equations 
network architectures simulated software implemented hardware vlsi optical 
software simulation network necessary implementing hardware 
number public commercial software ann simulators available 
researchers recognized importance hardware implementation probably way take full advantage capacities anns 
difficulty vlsi implementation anns massive connections 
fully connected network neurons requires connections 
factor limits number neurons typically build single chip state art vlsi technology 
alternative optical implementation anns 
early stages 
different network architectures require different learning algorithms 
section provide general overview learning processes 
learning ability learn fundamental trait intelligence 
meant learning difficult describe learning process context artificial neural networks viewed problem updating network architecture connection weights network efficiently perform specific task 
typically learning anns performed ways 
weights set priori network designer proper formulation problem 
time network learn connection weights training patterns 
improvement performance achieved time iteratively updating weights network 
ability neural networks automatically learn examples artificial neural networks attractive exciting 
having specify set rules anns appear learn collection representative examples 
major advantages neural networks traditional expert systems 
order understand design learning process model environment neural network operates information available neural network 
refer model learning paradigm 
second understand weights network updated learning rules govern updating process 
learning algorithm refers procedure learning rules adjusting weights network 
important investigate network learn examples capacity training samples required sample complexity fast system learn time complexity 
study capacity sample complexity time complexity learning theory deal 
illustrates aspects learning process 
main learning paradigms supervised ii unsupervised iii hybrid learning 
supervised learning network provided correct answer input pattern 
weights determined network produce answers learning learning paradigms learning theory supervised unsupervised error correction hebbian competitive capacity sample complexity time complexity learning learning learning process type learning rules hybrid boltzmann learning issues 
close possible known correct answers 
referred learning teacher 
reinforcement learning special case supervised learning network provided critiques correctness network outputs correct answers outputs 
contrast unsupervised learning require correct answer associated input pattern training data set 
explores underlying structure data correlations patterns data organizes patterns categories correlations 
hybrid learning combines supervised learning unsupervised learning 
typically portion weights network determined supervised learning obtained unsupervised learning 
learning theory address fundamental practical issues associated learning samples capacity ii sample complexity iii time complexity 
issue concerns true solution contained set solutions network deliver 
hope obtain optimal solution 
remains difficult open problem 
approximation capabilities feedforward neural networks investigated researchers see 
fundamental result studies layer layer feedforward networks arbitrarily large number nonlinear hidden units capable implementing continuous mapping prespecified accuracy certain mild conditions 
unfortunately theoretical studies ignore learnability problem concerned exist methods learn network weights empirical observations mappings 
furthermore theoretical analyses introduced new practical learning methods 
second issue sample complexity determines number training patterns needed train network order guarantee valid generalization 
patterns may cause fitting problem network performs training data set poorly independent test patterns drawn distribution training patterns 
third issue computational complexity learning algorithm estimate solution training patterns 
existing learning algorithms high computational complexity 
example popular backpropagation learning algorithm feedforward networks computationally demanding slow convergence 
designing efficient algorithms neural network learning active research topic 
basic types learning rules shown error correction ii boltzmann iii hebbian iv competitive learning 
described subsections 
error correction rules supervised learning paradigm network desired output input pattern 
learning process actual output generated network may equal desired output basic principle error correction learning rules error signal gamma modify connection weights error gradually reduced 
known perceptron learning rule error correction principle 
perceptron consists single neuron adjustable weights delta delta delta threshold shown 
input vector delta delta delta net input neuron applying threshold function gamma output perceptron 
class classification problem perceptron assigns input pattern class class 
linear equation gamma defines decision boundary hyperplane dimensional input space divides space halves 
rosenblatt developed learning procedure determine weights threshold perceptron set training patterns 
perceptron learning procedure described follows 

initialize weights threshold small random numbers 

pattern vector delta delta delta evaluate output neuron 

update weights gamma desired output iteration number gain step size 
note learning occurs error perceptron 
rosenblatt proved training patterns drawn linearly separable classes perceptron learning procedure converge finite number iterations 
known perceptron convergence theorem 
practice know patterns linearly separable 
variations learning algorithm proposed literature 
activation functions lead different learning characteristics 
single layer perceptron separate linearly separable patterns long monotonous activation function 
note non monotonous activation functions gaussian function form non linear decision boundaries 
shows trajectory decision boundary learned modified perceptron learning algorithm classifying logical problem linearly separable 
initial final convergence modified perceptron learning algorithm logical problem 
known backpropagation learning algorithm section error correction principle 
boltzmann learning boltzmann machines symmetric recurrent networks consisting binary units 
symmetric mean weight connection unit unit equal weight connection unit unit ij ji 
portion neurons visible neurons interact environment rest hidden neurons invisible 
neuron stochastic unit generates output state boltzmann distribution statistical mechanics 
boltzmann machines operate modes clamped mode visible neurons clamped specific states determined environment ii free running mode visible hidden neurons allowed operate freely 
boltzmann learning stochastic learning rule derived information theoretic thermodynamic principles see 
objective boltzmann learning adjust connection weights states visible units satisfy particular desired probability distribution 
boltzmann learning rule change connection weight ij deltaw ij ae ij gamma ae ij ae ij ae ij correlations states unit unit network operates clamped mode free running mode respectively 
values ae ij ae ij usually estimated monte carlo experiments extremely slow 
boltzmann learning viewed special case error correction rule error measured direct difference desired output actual output difference correlations outputs neurons operating conditions clamped free running 
hebbian rule oldest learning rule hebb postulate learning 
proposed hebb observation neurobiological experiments axon cell near excite cell repeatedly persistently takes part firing growth process metabolic changes take place cells efficiency cells firing increased 
words neurons side synapse activated synchronously repeatedly strength synapse selectively increased 
synapse referred hebb synapse correlational synapse change synapse strength depends correlation presynaptic postsynaptic activities 
mathematically update hebb synapse described ij ij jy output values neurons respectively connected synapse ij learning rate 
note input synapse 
important property rule learning done locally change synapse weight depends activities neurons connected 
significantly simplifies complexity learning circuit vlsi implementation 
problem learning rule connection weights grow unboundedly learning proceeds 
deal problem modifications basic hebbian rule proposed 
example oja rule adds weight decay proportional basic hebbian rule ij ij jy gamma ij interesting note rule similar reverse error correction rule deltaw ij depends difference actual input back propagated output 
single neuron trained hebbian rule exhibits orientation selectivity 
demonstrates property 
points depicted drawn dimensional gaussian distribution training neuron 
weight vector neuron initialized shown 
learning proceeds weight vector moves closer closer direction maximal variance data 
fact eigenvector covariance matrix data corresponding largest eigenvalue 
orientation selectivity single neuron 
straightforward generalize behavior single unit conclude layer feedforward network output units extract principal components dimensional data due orthogonality eigenvectors gamma principal components lie subspace perpendicular principal component corresponding largest eigenvalue 
gamma principal components determined recursively subspaces way similar computing component 
elegant methods proposed computing principal components simultaneously imposing constraints activities output units 
competitive learning rules hebbian learning multiple output units fired simultaneously competitive learning output units compete activated 
result competition output unit group active time 
phenomenon known winner take 
competitive learning exist biological neural networks 
neurobiological experiments shown competitive learning plays important role formation topographic maps brain self organization orientation sensitive nerve cells striate cortex 
outcome competitive learning clustering categorization input data 
similar patterns grouped network represented single unit 
grouping process done network automatically correlations data 
simplest competitive learning network consists single layer output units shown 
output unit network connects input units simple competitive learning architecture 
weights ij delta delta delta output unit connects output units inhibitory weights self feedback excitatory weight 
result competition unit largest smallest net input winner delta delta kw gamma xk kw gamma xk weight vectors normalized inequalities equivalent 
simple competitive learning rule stated follows 
deltaw ij gamma note weights winner unit get updated 
effect learning rule move stored pattern winner unit weights little bit closer input pattern 
geometric interpretation competitive learning demonstrated 
example assume input vectors normalized unit length 
depicted black dots 
weight vectors units randomly initialized 
initial positions final positions sphere competitive learning shown crosses figures respectively 
see natural groups patterns discovered output unit weight vector points center gravity discovered group 
example competitive learning learning learning 
see competitive learning rule network learning updating weights learning rate zero 
possible particular pattern may fire different output units change categories forever learning 
brings stability issue learning system 
learning system said stable pattern training data changes category finite number learning iterations 
way achieving stability force learning rate decrease gradually learning process proceeds eventually approaches zero 
artificial freezing learning causes problem plasticity defined ability adapt new data 
known grossberg stability plasticity dilemma competitive learning 
known example competitive learning vector quantization data compression 
vector quantization widely speech image processing efficient storage transmission modeling 
goal vector quantization represent set distribution input vectors relatively small number prototype vectors weight vectors codebook 
codebook constructed agreed transmit store index corresponding prototype input vector 
input vector corresponding prototype searching nearest prototype codebook 
euclidean distance divides input space voronoi tessellation 
competitive learning rule equation generating codebook set input vectors 
codebook voronoi tessellation generated unsupervised competitive learning rule may best pattern classification purposes see 
learning vector quantization lvq supervised competitive learning technique uses pattern class information adjust voronoi vectors slightly improve classification accuracy 
lvq weight updating rule replaced gamma pattern correctly classified winning unit gamma gamma demonstrates effect lvq 
moves prototypes learned vq algorithm slightly left order classify patterns correctly 
summary learning algorithms various learning algorithms associated network architectures summarized table 
means exhaustive list learning algorithms available literature 
notice supervised unsupervised learning paradigms vector quantization versus learning vector quantization 
patterns classes labeled triangles circles respectively 
solid patterns learned prototypes 
employ learning rules error correction hebbian competitive learning 
learning rules error correction training feedforward networks hebbian learning rules types network architectures 
learning algorithm designed training specific network architecture 
talk learning algorithm implied particular network architecture associated 
learning algorithm designed performing specific tasks 
column table lists number tasks learning algorithm perform 
due space limitation discuss algorithms including madaline linear discriminant analysis see art artmap sammon projection see principal component analysis see rbf learning algorithm see 
interested readers read corresponding 
note order reduce size bibliography article cite proposed particular algorithm 
learning paradigm learning rule architecture learning algorithm task single perceptron learning algorithms pattern classification error correction multi layer backpropagation function approximation perceptron madaline control supervised boltzmann recurrent boltzmann learning algorithm pattern classification hebbian multi layer linear discriminant analysis data analysis feedforward pattern classification competitive learning vector quantization class categorization competitive data compression art network artmap pattern classification class categorization error correction multi layer sammon projection data analysis feedforward feedforward principal component analysis data analysis unsupervised hebbian competitive data compression hopfield net associative memory learning associative memory competitive vector quantization categorization data compression competitive kohonen som kohonen som categorization data analysis art networks art art categorization hybrid error correction rbf network rbf learning algorithm pattern classification competitive function approximation control table known learning algorithms 
multilayer perceptron recognized multilayer feedforward networks capable forming arbitrarily complex decision boundaries represent boolean function 
development back propagation learning algorithm determining weights multi layer feedforward network networks popular networks 
qi ij jk input layer hidden layers output layer typical layer feedforward network architecture 
shows typical layer perceptron 
general standard layer feedforward network consists input stage gamma hidden layers output layer units successively connected fully locally feedforward fashion connections units layer feedback connections layers 
denote ij weight connection th unit layer gamma th unit layer recall task learning algorithm automatically determine weights network certain cost function minimized 
delta delta delta set training patterns input output pairs input vector dimensional pattern space desired output vector dimensional hyper cube 
classification purposes set number classes 
squared error cost function frequently ann literature defined ky gamma adopt convention input nodes counted layer 
back propagation algorithm gradient descent method minimize squared error cost function equation 
described follows 

initialize weights small random values 
randomly choose input pattern 
propagate signal forward network 
compute ffi output layer ffi gamma represents net input th unit th layer 

compute deltas preceding layers propagating errors backwards ffi ij ffi gamma delta delta delta 
update weights deltaw ji jffi gamma 
go step repeat pattern error output layer pre specified threshold maximum number iterations reached 
geometric interpretation adopted modified shown help understand role hidden units threshold activation function 
unit hidden layer forms hyper plane pattern space boundaries pattern classes approximated hyper planes 
unit second hidden layer forms hyper region outputs layer units decision region obtained performing operation hyperplanes 
arrange output layer units perform operation second layer units 
remember scenario depicted help understand role hidden units 
actual behavior train network different 
multilayer feedforward networks sigmoid activation functions form smooth decision boundaries piece wise linear boundaries 
issues designing feedforward networks 
issues include single layer layer layer structure description decision regions exclusive problem classes meshed regions general region shapes general region shapes half plane bounded hyperplane arbitrary complexity limited number hidden units arbitrary complexity limited number hidden units geometric interpretation role hidden units 
layers needed task ii units layer iii expect network generalize data included training set iv large training set generalization 
multilayer feedforward networks backpropagation algorithm widely classification function approximation see design parameters determined error method 
existing theoretical results provide loose guidelines selecting parameters practice 
kohonen self organizing maps kohonen self organizing map som desirable property topology preserving captures important aspect feature maps cortex developed animal brains 
topology preserving mapping mean nearby input patterns activate nearby output units map 
basic network architecture kohonen som shown 
basically consists dimensional array units connected input nodes 
ij denote dimensional vector associated unit location array 
neuron computes euclidean distance 
kohonen self organizing map 
input vector stored weight vector ij ij jjx gamma ij jj kohonen som special type competitive learning network defines spatial neighborhood output unit 
shape local neighborhood square rectangle circle 
initial neighborhood size set network size 
neighborhood shrinks time schedule exponentially decreasing function 
competitive learning weight vectors associated winner neighboring units updated 
kohonen som learning algorithm described follows 

initialize weights small random numbers set initial learning rate neighborhood 
pattern evaluate network outputs 
select unit minimum output jjx gamma jj min ij jjx gamma ij jj 
update weights learning rule ij ij ff gamma ij ij neighborhood unit time ff learning rate 

decrease value ff shrink neighborhood 
repeat steps change weight values pre specified threshold maximum number iterations reached 
kohonen som projection multivariate data density approximation clustering 
successful applications kohonen som areas speech recognition image processing robotics process control 
design parameters include dimensionality neuron array number neurons dimension shape neighborhood shrinking schedule neighborhood learning rate 
adaptive resonance theory models recall important issue competitive learning stability plasticity dilemma 
brain learn new things plasticity retain stability ensures existing knowledge erased corrupted 
carpenter grossberg adaptive resonance theory models art art artmap developed attempt overcome dilemma 
basic idea models follows 
network sufficient supply output units deemed necessary 
unit said committed uncommitted 
learning algorithm updates stored prototypes category input vector sufficiently similar 
input stored prototype said resonate sufficiently similar 
sufficient extent similarity controlled vigilance parameter ae ae determines number categories 
input vector sufficiently similar existing prototype network new category created uncommitted unit assigned new category input vector initial prototype 
uncommitted unit exists novel input generates response 
art takes binary input art designed continuous valued input 
newer version artmap pattern label information 
art illustrate model 
ij ij competitive output layer comparison input layer art network 
shows simplified diagram art architecture see 
consists layers units fully connected 
top weight vector associated unit input layer bottom weight vector associated output unit normalized version ji small number breaking ties selecting winner 
bit input vector output auxiliary unit sgn gamma gamma outputs input units sgn ji gamma output ji reset signal generated similarity vigilance level 
art learning algorithm described 

initialize ij enable output units 

new pattern 
find winner unit enabled output units delta delta 
vigilance test delta ae resonance goto step 
disable unit goto step output units disabled 

update winning weight vector enable output units goto step deltaw ji gamma ji 
output units disabled select uncommitted output units set weight vector uncommitted output unit capacity reached network rejects input pattern 
art model runs entirely autonomously 
able create new categories reject input pattern network reaches capacity 
number categories input data discovered art sensitive vigilance parameter 
hopfield network hopfield network special type recurrent network uses network energy function tool designing recurrent networks understanding dynamic behavior 
hopfield formulation explicit principle storing information dynamically stable attractors popularized recurrent networks associative memory solving combinatorial optimization problems 
hopfield network units versions binary continuous valued networks 
state output th unit 
binary networks continuous networks value 
ij synapse weight connection unit unit hopfield network ij ji symmetric network ii self feedback connections 
network dynamics binary hopfield network sgn ij gamma sgn signum function produces gamma 
network dynamics continuous hopfield network du dt gammau ij gamma net input potential th unit sigmoid function constants 
equilibrium point ij gamma dynamic update network states equation carried ways synchronously versus asynchronously 
synchronous updating scheme units updated simultaneously time step 
central clock required synchronize process 
hand asynchronous updating scheme selects unit time updates state 
unit updating chosen randomly 
asynchronous updating scheme natural biological networks 
continuous hopfield network addition synchronous asynchronous updating schemes equation provides continuous updating scheme particularly desirable circuit implementation 
energy function binary hopfield network state delta delta delta vn gamma ij network energy continuous hopfield network defined gamma ij gamma dx central property energy functions state network evolves network dynamics eqs 
network energy decreases eventually reaches local minimum point network stays constant energy 
local minimum points state space referred attractors due fact starting point state neighborhood attractor network evolve attractor 
neighborhood called basin attraction attractor 
sequence state changes named trajectory 
schematically explains terminologies 
state space trajectories attractors basin attraction schematic plot attractors trajectories basin attraction 
suppose set patterns stored attractors network 
network associative memory 
stored pattern retrieved pattern represented network state basin attraction attractor corresponding stored pattern 
principle hopfield network associative memory 
attractors network encode solutions combinatorial optimization problem cost objective function formulated network energy 
optimal sub optimal solution obtained network evolves local minimum point 
basic idea hopfield network solving combinatorial optimization problems 
subsections discuss applications hopfield network 
associative memory fundamental property associative content addressable memory ability store set patterns way new pattern incomplete noisy version stored pattern network retrieve stored patterns closely resembles input pattern 
property fold meaning 
memory accessed content 
second memory error correcting item memory reliably retrieved noisy incomplete information long information sufficient 
associative memory usually operates phases storage retrieval 
storage phase weights network determined learned way attractors network memorize set dimensional patterns fx delta delta delta stored 
generalization hebbian learning rule setting connection weights ij note number units network equals ij values thresholds set zero 
retrieval phase input pattern initial state network network evolves network dynamics 
pattern produced retrieved network reaches equilibrium state 
patterns stored network binary units 
words memory capacity network 
note capacity finite network binary units maximum distinct states states attractors 
attractors stable states store useful patterns 
exist spurious attractors store patterns different patterns training set 
shown random patterns hopfield network store pmax number stored patterns perfect recall achieved 
memory patterns orthogonal vectors random patterns patterns stored 
number spurious attractors increases reaches capacity limit 
hardware efficiency hopfield network extremely low require connections network store bit patterns 
learning rules proposed increasing memory capacity hopfield networks see 
combinatorial optimization hopfield networks evolve direction leads lower network energy 
implies combinatorial optimization problem formulated minimizing network energy hopfield network find optimal suboptimal solution letting network evolve freely 
fact quadratic objective function rewritten form hopfield network energy 
classical traveling salesperson problem example network constructed 
units network organized dimensional theta array total number cities 
row index unit represent city column index index tour 
output unit row column means city visited th 
solution tsp problem theta permutation matrix 
dxy distance city city construct cost function tsp problem follows 
want minimize total distance dxy vx gamma employ periodic boundary condition th column th column column column respectively 
problem constraints follows city visited vx vx ii contain city vx iii matrix contain entries vx gamma positive constants parameters problem 
total cost tour defined note equation quadratic network outputs 
manipulating equation obtain quadratic function terms 
coefficients quadratic terms cost function define connection weights network xy gamma ffi ij gamma ffi ij gamma ffi xy gamma gamma ffi ffi gamma ffi ij coefficients linear terms specify thresholds units 
xi constant term total cost change solution ignored 
solution tsp problem set cities distances running physical network weights thresholds determined equations computer simulation 
continuous network performs better binary network 
easily gets stuck local minimum poor tour length 
simulated annealing technique employed deal problem time consuming 
performance network crucially dependent choice parameters 
balance parameter values produces valid tours satisfying constraints 
various modifications hopfield tank architecture solving tsp explored see 
applications various ann models learning algorithms successfully applied large variety problems belonging tasks shown 
pointed section important applications ann pattern classification 
pattern classification problem high commercial importance optical character recognition ocr 
ocr deals problem processing scanned image text transcribing machine readable form example ascii 
text may machine printed handwritten 
term ocr misnomer optical processing involved transcription process 
ocr important eliminating minimizing human labor involved capturing information documents 
major application areas ocr forms readers text conversion digital libraries 
section outline basic components ocr explain anns pattern classification 
basic processing steps ocr system shown 
document scanned produce gray level binary black white image scanning resolution pixels inch typically 
preprocessing stage filtering applied remove noise text areas located converted binary black white image globally locally adaptive method 
segmentation step text image separated individual character patterns 
particularly difficult task handwritten text proliferation touching characters 
difficult machine printed text techniques employed 
noise cause separated characters touching 
various techniques split composite patterns 
effective technique break composite pattern smaller patterns segmentation find correct character segmentation points output pattern classifier 
shows size normalized character bitmaps sample set nist character database 
see substantial intra class variations 
goal feature extraction extract relevant measurements sensed data minimize class variability increasing class variability 
various feature scanning preprocessing segmentation feature extraction classification contextual postprocessing anns feature vector classified characters recognized ascii text document binary image single character image gray level binary image diagram typical ocr system 
extraction methods employed character recognition including projection histograms contour profiles zoning geometric moment invariants spline curve approximation fourier descriptors 
clear evidence feature set best application 
shows typical scheme extracting zone features contour direction bending points 
contour direction features generated dividing binary image array rectangular diagonal zones computing histograms chain codes zones results features 
bending point features represent high curvature points terminal points fork points 
special geometrical mapping bending points attributes fixed length feature vector designed 
bending points normalized image coded positions quantized theta regions curvature orientations quantized directions convex concave 
value bending point magnitude corresponding component feature vector 
pattern classification stage extracted features passed input stage ann 
number input units equal dimensionality feature vector 
number output units equal number character categories example sample set characters nist data 
classification numeral digits output units required alphanumeric classifier units may necessary digits upper case lower case special symbols punctuation marks 
number units intermediate layer usually determined experimentally get maximum recognition accuracy independent test set 
ocr system described layer feedforward network hidden units produce generalization ability 
ocr systems extract features raw data 
typical example network developed le cun zip code recognition 
network architecture shown 
theta normalized gray level image feedforward network hidden layers 
feature extraction implicitly takes place intermediate stages ann 
units hidden layer form theta feature maps 
unit feature map locally connected theta neighborhood input image 
units feature map share weight vector 
constructed similar way hidden layer second hidden layer forms theta feature maps 
unit second hidden layer combines local information coming feature maps hidden layer 
third hidden layer consists hidden units 
output units correspond classes digits 
sub network black pixel white pixel contour direction features directions slices convex concave zone features contour direction bending points 
second layer output layer standard fully connected feedforward network 
activation level output unit interpreted approximation posteriori probability belonging particular class input pattern 
output categories ordered activation levels passed post processing stage 
post processing stage contextual information exploited update output classifier 
examples looking dictionary admissible words applying syntactic constraints phone numbers social security numbers 
anns ocr 
anns practice 
conclusive evidence ann superiority conventional statistical pattern classifiers 
census optical character recognition system conference different handwritten character recognition systems tested database 
top performers type multilayer feedforward network nearest neighbor classifier 
anns tend superior speed smaller memory requirements compared nearest neighbor methods 
nearest neighbor methods classification speed ann independent size training set 
recognition accuracies top ocr systems nist isolated pre segmented character data digits upper case characters lower case characters 
drawn test recognition performance ocr systems comparable human performance isolated characters 
network recognizing numeric digits 
humans outperform ocr systems unconstrained cursive handwritten documents 
concluding remarks developments anns experienced lot enthusiasm criticism 
comparative studies provide optimistic outlook anns offer pessimistic view 
tasks pattern recognition single approach dominates 
choice best technique driven nature application 
try understand capacities assumptions applicability various approaches developed various disciplines maximally exploit complementary advantages approaches order develop better intelligent systems 
effort may lead synergistic approach combines strengths anns disciplines order achieve significantly better performance challenging problems 
minsky recognized time come build systems diverse components 
synergistic approach individual modules important methodology integrating various modules key success 
clear communication cooperative anns disciplines avoid importantly stimulate motivate individual disciplines 
acknowledgment authors richard casey dorai useful suggestions 
darpa neural network study 
international press 
james anderson edward rosenfeld 
neurocomputing foundations research 
mit press cambridge massachusetts 
brunak 
neural networks intuition 
world scientific singapore 
carpenter grossberg 
pattern recognition self organizing neural networks 
mit press cambridge ma 
casey 
character segmentation document ocr progress hope 
th annual symposium document analysis information retrieval pages las vegas 
feldman goddard 
computing structured neural networks 
ieee computer pages march 
haykin 
neural networks comprehensive foundation 
macmillan college publishing new york 
hebb 
organization behavior 
wiley new york 
hertz krogh palmer 
theory neural computation 
addison wesley redwood city 
hopfield 
neural networks physical systems emergent collective computational abilities 
proc 
natl 
acad 
sci 
usa pages 
jain mao 
neural networks pattern recognition 
marks ii robinson editors computational intelligence imitating life pages 
ieee press new york 
kohonen 
self organization associative memory 
third edition springer verlag 
le cun boser denker henderson howard hubbard jackel 
back propagation applied handwritten zipcode recognition 
neural computation 
lippmann 
computing neural nets 
ieee assp magazine apr 
mcculloch pitts 
logical calculus ideas nervous activity 
bulletin mathematical biophysics 
minsky 
logical versus analogical symbolic versus connectionist neat versus 
ai magazine 
minsky papert 
perceptrons computational geometry 
mit press cambridge ma 
mao 
comparative study different classifiers character recognition 
gelsema kanal editors pattern recognition practice iv pages 
elsevier science netherlands 
nilsson 
learning machines foundations trainable pattern classifying systems 
mcgraw hill new york 
rosenblatt 
perceptron probabilistic model information storage organization brain 
psychological review 
rumelhart mcclelland 
parallel distributed processing exploration microstructure cognition 
mit press cambridge ma 
widrow 
madaline 
ieee st intl 
conf 
neural networks pages san diego ca june 
wilkinson geist 
eds 
census optical character recognition system conference 
technical report nistir department commerce nist md 
