experimental comparison neural ica algorithms xavier juha karhunen erkki oja lab 
computer information science helsinki univ technology box hut espoo finland neural algorithms independent component analysis ica introduced lately computational properties systematically studied 
compare accuracy convergence speed computational load properties prominent neural semi neural ica algorithms 
comparison reveals interesting differences algorithms 
independent component analysis ica unsupervised technique tries represent data terms statistically independent variables 
efficient new neural learning algorithms developed ica applied closely related blind source separation bss problems 
serious experimental comparison ica algorithms lacking 
results comparison reported detail 
consider standard linear data model ica bss components column vector unknown mutually statistically independent components source signals time index value simplicity assumed zero mean stationary 
components dimensional data vector linear mixtures independent components sources 
theta mixing matrix unknown full rank constant matrix 
columns basis vectors ica 
independent components allowed gaussian 
learning ica expansion theta inverse separating matrix updated vector estimate independent components 
estimate th independent component may appear component 
amplitudes scaled unit variance 
neural ica bss algorithms neural ica bss algorithms data vectors preprocessed whitening sphering 
denotes th whitened vector satisfying unit matrix theta whitening matrix 
whitening done ways 
subsequent separating matrix taken orthogonal improves convergence 
whitening approaches total separating matrix 
limited space describe algorithms included study briefly 
details see 
fixed point fp algorithms 
iteration generalized fixed point algorithm finding row vector gamma efg gw kw suitable nonlinearity typically tanh derivative 
expectations practice replaced sample means 
fixed point algorithm truly neural adaptive algorithm 
algorithm requires data 
vectors orthogonalized done sequentially symmetrically 
usually algorithm converges iterations 
natural gradient algorithm 
originally proposed heuristic grounds popular simple neural gradient algorithm derived information theoretic criteria 
algorithm require 
update rule separating matrix deltab gamma notation means nonlinearity applied component vector bx 
learning parameter usually small constant 
practice updated small batches data 
extended bell sejnowski algorithm 
update rule whitening improve convergence properties deltaw gamma wv 
extended form kurtosis estimated line handling super gaussian sub gaussian sources learning parameter optimized momentum term simulated annealing 
algorithm 
introduced adaptive signal processing algorithm applied neural learning algorithm 
general update formula contains extra terms compared deltab gamma yy gamma yg jy comparison normalized version 
unnormalized version terms denominator left may cause stability problems especially nonlinearities growing faster linearly 
rls algorithm nonlinear pca criterion rls 
basic symmetric version adapted bss problem data vectors gamma gamma fi fi tri theta gamma gamma gamma gamma gamma forgetting constant fi close unity 
notation tri means upper triangular part argument computed transpose copied lower triangular part 
rls algorithm recursive squares version nonlinear pca algorithm 
learning parameter determined roughly optimal 
experimental results artificially generated data far simulations mainly artificially generated data accuracy convergence speed algorithms measured reliably 
real world data true independent components best approximations unknown 
experimental setup algorithm order comparison fair see 
original matlab codes provided authors possible fixed point algorithms 
experiments sub gaussian super gaussian sources generated mixing matrix consisted uniformly distributed random numbers 
accuracy measured performance indexes 
defined jp ij max jp ik gamma jp ij max jp kj gamma ij ba permutation matrix sources separated perfectly 
second performance index absolute values replaced squares 
indices greater value poorer performance 
minimum zero 
fp flops general view power requirements rls bs power requirements flops vs error index boxes typically contain trials 
schematic diagram results basic experiments measuring accuracy computational load floating point operations tested algorithms 
number sources independent components 
clearly fixed point algorithms require smallest amount computation 
symmetric version tanh nonlinearity accurate accuracy basic fp algorithm sequential orthogonalization poorer algorithms 
adaptive algorithms rls converges fastest 
natural gradient algorithm extended bell sejnowski algorithm achieve final accuracy computational load higher 
error square root index plotted function number super gaussian sources algorithms worked 
generally natural gradient algorithm various modifications bs best accuracy behaving similarly expected 
fixed point algorithm fp poorest accuracy error increases slightly sources 
unknown reason error rls algorithm peak sources 
error algorithms tolerable practical purposes 
experiments number sources increased necessary replace cubic nonlinearity stable tanh algorithm tanh gamma algorithm converge sources 
number sources supergaussian problem values trials fp rls bs error function number sources 
gaussian noise added data degradation results smooth noise power increases db signal power 
observation little noise data error strongly depends condition number mixing matrix 
holds equivariant algorithms compute separating matrix directly algorithms employing rls behaves differently 
general experiments artificial data designing efficient ica algorithm split problem different parts 
include choices algorithm nonlinearity control structure 
course dependencies constituent parts taken account 
design allows practical compromise efficiency robustness precision relevant requirements problem hand 
real world data extended comparisons real world data trying find projection pursuit directions ica algorithms 
projection pursuit goal find visualization purposes dimensional projections multidimensional data containing interesting structural information possible 
ica tool projection pursuit provides non gaussian projections containing meaningful structural information suggested 
example considered dimensional data set consisting measurements species crabs 
ica algorithms ica basis vectors separating species certain extent males females species see 
results clearly better standard pca 
main experimental comparison ica algorithms follows 
semi neural fixed point algorithm converges fastest deal sub gaussian super gaussian sources line estimation kurtosis required algorithms 
adaptive neural algorithms recursive squares algorithm usually smallest computational load 
accuracy bell sejnowski algorithm natural gradient extensions 
main factor affecting final accuracy algorithms choice nonlinearity 
comon signal processing vol 
pp 

jutten herault signal processing vol 
pp 
july 
oja 
amari eds brain computing intelligent information systems springer singapore pp 

yang 
amari neural computation vol 
pp 

bell sejnowski neural computation vol 
pp 

cichocki unbehauen ieee trans 
circuits systems vol 
pp 
nov 
karhunen oja wang ieee trans 
neural networks vol 
pp 
may 
karhunen pajunen proc 
int 
conf 
neural networks houston texas june pp 

comparison adaptive independent component analysis algorithms dipl eng thesis epfl switzerland helsinki univ technology finland available www cis hut fi 

cardoso laheld ieee trans 
signal processing vol 
pp 
dec 
hyvarinen proc 
ieee int 
conf 
acoustics speech signal processing munich germany april pp 

girolami fyfe proc 
int 
conf 
neural networks houston texas june pp 

mckeown proc 
natl 
acad 
sci 
usa vol 
pp 

fast ica matlab package 
available www cis hut fi projects ica fastica 
friedman amer 
stat 
assoc vol 
pp 
march 
