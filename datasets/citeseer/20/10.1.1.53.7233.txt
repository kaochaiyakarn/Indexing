acting optimally partially observable stochastic domains anthony cassandra leslie pack kaelbling michael littman department computer science brown university providence ri cs brown edu describe partially observable markov decision process pomdp approach finding optimal near optimal control strategies partially observable stochastic environments complete model environment 
pomdp approach originally developed operations research community provides formal basis planning problems interest ai community 
existing algorithms computing optimal control strategies highly computationally inefficient developed new algorithm empirically efficient 
sketch algorithm preliminary results small problems illustrate important properties pomdp approach 
agents act real environments physical virtual rarely complete information state environment working 
necessary choose actions partial ignorance helpful take explicit steps gain information achieve goals efficiently 
problem addressed artificial intelligence ai community formalisms epistemic logic incorporating knowledge preconditions effects planners moore 
solutions applicable fairly high level problems environment assumed completely deterministic assumption fails low level control problems 
domains actions probabilistic results agent direct access state environment formalized markov decision anthony cassandra supported part national science foundation award iri 
leslie kaelbling supported part national science foundation national young investigator award iri part onr contract arpa order 
michael littman supported bellcore 
processes mdps howard 
important aspect mdp model provides basis algorithms provably find optimal policies mappings environmental states actions stochastic model environment goal 
mdp models play important role current ai research planning dean sutton learning barto bradtke singh watkins dayan assumption complete observability provides significant obstacle application real world problems 
explores extension mdp model partially observable markov decision processes pomdps monahan lovejoy mdps developed context operations research 
pomdp model provides elegant solution problem acting partially observable domains treating actions affect environment actions affect agent state information uniformly 
explaining basic pomdp formalism algorithm finding arbitrarily approximations optimal policies method compact representation policies conclude examples illustrate generalization policy representation action gain information 
partially observable markov decision processes markov decision processes mdp defined tuple hs ri finite set environmental states reliably identified agent finite set actions state transition model environment function mapping elements thetaa discrete probability distributions reward function mapping theta real numbers specify instantaneous reward agent derives action state 
write probability environment transition state state action taken write immediate reward agent action state policy mapping specifying se controller pomdp action taken situation 
adding partial observability state completely observable add model observation 
includes finite set possible observations observation function mapping theta discrete probability distributions write probability making observation state having taken action simply take set observations set states treat pomdp mdp 
problem process necessarily markov multiple states environment require different actions appear identical 
result optimal policy form arbitrarily poor performance 
introduce kind internal state agent 
belief state discrete probability distribution set environmental states representing state probability environment currently state 
set belief states 
write probability assigned state agent belief state decompose problem acting partially observable environment shown 
component labeled se state estimator 
takes input belief state action observation returns updated belief state 
second component policy maps belief states actions 
state estimator constructed straightforward application bayes rule 
output state estimator belief state represented vector probabilities environmental state sums 
component corresponding state written se determined previous belief state previous action current observation follows se pr pr pr pr pr normalizing factor defined pr simple pomdp environment resulting function ensure current belief accurately summarizes available information 
example simple example pomdp shown 
states state designated goal state 
agent states times actions left right move state direction 
moves wall stays state 
agent reaches goal state matter action takes moved equal probability state receives reward 
problem trivial agent observe state difficult observe currently goal state 
agent observe true state represent belief probability vector 
example leaving goal agent moves states equal probability 
represented belief state action right observing goal states agent moved 
agent new belief vector moves right seeing goal agent sure state belief state 
actions deterministic example agent uncertainty shrinks step general actions situations decrease uncertainty increase 
constructing optimal policies constructing optimal policy quite difficult 
specifying policy point uncountable state space challenging 
simple method find optimal state action value function completely observable mdp hs ri watkins dayan belief state input generate action argmax 
act uncertainty action step environment completely observable 
approach similar chrisman chrisman leads policies take actions gain information suboptimal environments 
key finding truly optimal policies partially observable case cast problem completely observable continuous space mdp 
state set belief mdp action set current belief state action joj possible successor belief states new state transition function defined fo pr pr defined 
new belief state generated state estimator observation probability transition 
reward function ae constructed expectations belief state ae may strange appears agent rewarded simply believing states 
way state estimation module constructed possible agent purposely believing state 
belief mdp markov astrom having information previous belief states improve choice action 
importantly agent adopts optimal policy belief mdp resulting behavior optimal partially observable process 
remaining difficulty belief space continuous established algorithms finding optimal policies mdps finite state spaces 
sections discuss method value iteration finding optimal policies 
value iteration value iteration howard developed finding optimal policies mdps 
formulated partially observable problem mdp belief states find optimal policies pomdps analogous manner 
agent moves world policy collecting reward 
criterion possible choosing policy focus policies maximize infinite expected sum discounted rewards states 
infinite horizon problems seek maximize fl fl discount factor reward received time fl zero agent seeks maximize reward time step regard consequences 
fl increases rewards play larger role decision process 
optimal value belief state infinite expected sum discounted rewards starting state executing optimal policy 
value function expressed system simultaneous equations follows max ae fl value state instantaneous reward plus discounted value state action maximizes value 
consider policy maximizes reward finite number time steps essence value iteration optimal horizon solutions approach optimal infinite horizon solution tends infinity 
precisely shown maximum difference value function optimal infinite horizon policy analogously defined value function optimal horizon policy goes zero goes infinity 
property leads value iteration algorithm loop max ae fl gamma jv gamma gamma ffl algorithm guaranteed converge finite number iterations results policy gamma fl optimal policy bellman lovejoy 
finite state mdps value functions represented tables 
continuous space need special properties belief mdp represent finitely 
finite horizon value function piecewise linear convex sondik smallwood sondik 
addition infinite horizon value function approximated arbitrarily closely convex function sondik 
representation properties introduced sondik sondik 
set jsj dimensional vectors real numbers 
optimal horizon value function written max ff delta ff set piecewise linear convex function expressed way particular vectors viewed values associated different choices optimal policy analogous watkins values watkins dayan see cassandra kaelbling littman sondik 
witness algorithm task step value iteration algorithm find set represents gamma detailed algorithms developed problem smallwood sondik monahan cheng extremely inefficient 
describe new algorithm inspired cheng linear support algorithm cheng theory practice efficient 
algorithms smallwood sondik cheng construct approximate value function max ff delta ff successively improved adding vectors set built key insight 
gamma particular belief state determine ff added 
algorithmic challenge find prove exists approximation perfect 
witness algorithm cassandra kaelbling littman defines linear program returns single point witness fact process begins initial populated vectors needed represent value function corners belief space jsj belief states consisting single 
linear program constructed jsj variables represent components belief state auxiliary variables constraints define max ae fl max ff gamma ff delta max ff ff delta final constraint insists program returns witness fails witness determine new vector include process repeats 
linear program solved vector current formulation tolerance factor ffi defined linear program effective 
algorithm terminate long difference point ffi differentiates witness algorithm approaches mentioned find exact solutions 
witness algorithm constructs approximations conjunction value iteration construct policies arbitrarily close optimal making ffi small 
unfortunately extremely small values ffi result numerically unstable linear programs quite challenging linear programming implementations 
shown finding optimal policy finite horizon pomdp pspace complete papadimitriou tsitsiklis algorithms mentioned take time exponential problem size specific pomdp parameters require exponential number vectors represent main advantage witness algorithm appears algorithms running time guaranteed exponential number vectors required 
practice resulted vastly improved running times ability run larger example problems existing pomdp algorithms 
details algorithm outlined technical report cassandra kaelbling littman 
representing policies value iteration converges left set vectors final constitutes approximation optimal value function vectors defines region belief space belief state vector region dot product vector maximum 
vectors define partition belief space 
shown smallwood sondik state vectors share partition share optimal action policy specified set pairs hff ff delta ff delta ff final problems partitions important property leads particularly useful representation optimal policy 
optimal action resulting observation belief states partition transformed belief states occupying partition step 
set partitions corresponding transitions constitute policy graph summarizes action choices optimal policy 
shows policy graph optimal policy simple pomdp environment 
node picture corresponds set belief states vector final largest dot product labeled optimal action set belief states 
observations label arcs graph specifying incoming information affects agent choice actions 
agent initial belief state node marked extra arrow 
example chose uniform belief distribution indicate agent initially knowledge situation 
policy graph computed policy graph representation optimal policy 
agent chooses action associated start node depending observation agent transition appropriate node 
executes associated action follows arc observation 
reactive agent representation policy ideal 
current node policy graph sufficient summarize agent past experience decisions 
arcs graph dictate new information form observations incorporated agent decision making 
graph executed simply efficiently 
returning give concrete demonstration policy graph 
policy graph summarized execute pattern right right left stopping goal encountered 
execute action left reset 
repeat 
straightforward verify strategy optimal pattern performs better 
note state estimator se longer necessary agent choose actions optimally 
policy graph information needs 
right right left sample policy graph simple pomdp environment right small unobservable grid policy graph results experimenting algorithms solving pomdps devised implemented witness algorithm heuristic method constructing policy graph final size optimal policy graph arbitrarily large problems tried policy graph included nodes 
largest problem looked consisted states actions observations algorithm converged policy graph nodes half hour 
generalization policy graph correspondence nodes belief states encountered agent 
environments decisions different belief states captured small number nodes 
constitutes form generalization continuum belief states including distinct environmental states handled identically 
shows extremely simple environment consisting grid cells goal lower right hand corner indistinguishable 
optimal policy graph environment consists just nodes moving grid moving right 
start node agent execute right pattern reaches goal point start 
note time node different beliefs environmental state 
acting gain information real world problems agent take specific actions gain information allow informed decisions achieve increased performance 
planning systems kinds actions handled differently actions change state listen go right go left listen listen right left left left right right policy graph tiger problem environment 
uniform treatment actions kinds desirable simplicity actions material informational consequences 
illustrate treatment information gathering actions pomdp model introduce modified version classic problem 
stand front doors door tiger vast reward know 
may open door receiving large penalty chose tiger large reward chose 
additional option simply listening 
tiger left probability hear tiger left probability hear right symmetrically case tiger right 
listen pay small penalty 
problem iterated immediately choose doors faced problem choosing door course tiger randomly repositioned 
problem long stand listen choose door 
witness algorithm solution shown 
information enter center node listen 
hear tiger left enter lower right node encodes roughly heard tiger left heard tiger right hear tiger right move back center node encodes heard tiger left times heard right 
listen 
continue listening heard tiger twice side point choose 
consequences meeting tiger dire witness algorithm finds strategies listen choosing ones bother listen 
reliability listening worse strategies listen 
related extensive discussion pomdps operations research literature 
surveys monahan monahan lovejoy lovejoy starting points 
ai community issues addressed examined researchers working reinforcement learning 
white head ballard whitehead ballard solve problems partial observability access extra perceptual data 
chrisman chrisman mccallum mccallum describe algorithms inducing pomdp interactions environment relatively simple approximations resulting optimal value function 
relevant ai community includes tan tan inducing decision trees performing lowcost identification objects selecting appropriate sensory tests 
results preliminary 
intend short term extend algorithm perform policy iteration efficient 
solve larger examples including tracking surveillance problems 
addition hope extend number directions applying stochastic dynamic programming barto bradtke singh function approximation derive optimal value function solving analytically 
expect approximate policies may quickly way 
aim integrate pomdp framework methods dean 
dean finding approximately optimal policies quickly considering small regions search space 
acknowledgments chrisman tom dean indirectly ross introducing pomdps 
astrom 
optimal control markov decision processes incomplete state estimation 
math 
anal 
appl 

barto bradtke singh 
real time learning control asynchronous dynamic programming 
technical report department computer information science university massachusetts amherst massachusetts 
bellman 
dynamic programming 
princeton new jersey princeton university press 
cassandra kaelbling littman 
algorithms partially observable markov decision processes 
technical report forthcoming brown university providence rhode island 
cheng 

algorithms partially observable markov decision processes 
ph dissertation university british columbia british columbia canada 
chrisman 
reinforcement learning perceptual aliasing perceptual distinctions approach 
proceedings tenth national conference artificial intelligence 
san jose california aaai press 
dean kaelbling kirman nicholson 
planning deadlines stochastic domains 
proceedings eleventh national conference artificial intelligence 
howard 
dynamic programming markov processes 
cambridge massachusetts mit press 
lovejoy 
survey algorithmic methods partially observed markov decision processes 
annals operations research 
mccallum 
overcoming incomplete perception utile distinction memory 
proceedings tenth international conference machine learning 
amherst massachusetts morgan kaufmann 
monahan 
survey partially observable markov decision processes theory models algorithms 
management science 
moore 
formal theory knowledge action 
hobbs moore eds formal theories commonsense world 
norwood new jersey ablex publishing 
papadimitriou tsitsiklis 
complexity markov decision processes 
mathematics operations research 
smallwood sondik 
optimal control partially observable markov processes finite horizon 
operations research 
sondik 
optimal control partially observable markov processes 
ph dissertation stanford university stanford california 
sutton 
integrated architectures learning planning reacting approximating dynamic programming 
proceedings seventh international conference machine learning 
austin texas morgan kaufmann 
tan 
cost sensitive reinforcement learning adaptive classification control 
proceedings ninth national conference artificial intelligence 
watkins dayan 
learning 
machine learning 
whitehead ballard 
learning perceive act trial error 
machine learning 
