biologically plausible error driven learning local activation differences generalized recirculation algorithm randall reilly department psychology carnegie mellon university pittsburgh pa cmu edu july neural computation error backpropagation learning algorithm bp generally considered biologically implausible locally available activation variables 
version bp computed locally bi directional activation recirculation hinton mcclelland backpropagated error derivatives biologically plausible 
presents generalized version recirculation algorithm generec overcomes limitations earlier algorithm generic recurrent network sigmoidal units learn arbitrary input output mappings 
learning algorithm chl dbm mean field learning uses local variables perform error driven learning sigmoidal recurrent network 
chl derived stochastic framework boltzmann machine extended deterministic case various ways rely problematic assumptions leading conclude fundamentally flawed 
shows chl derived bp framework generec algorithm 
chl symmetry preserving version generec uses simple approximation midpoint second order accurate runge kutta method numerical integration explains generally faster learning speed chl compared bp 
known fully general error driven learning algorithms local activation deterministic considered variations generec algorithm indirectly backpropagation algorithm 
generec provides promising framework thinking brain perform error driven learning 
goal explicit biological mechanism proposed capable implementing generec style learning 
mechanism consistent available evidence regarding synaptic modification neurons neocortex hippocampus predictions 
reilly long standing objection error backpropagation learning algorithm bp rumelhart hinton williams biologically implausible crick zipser andersen principally requires error propagation occur mechanism different activation propagation 
learning appear non local error terms locally available result propagation activation network 
remedies problem suggested fully satisfactory 
approach involves additional error network job send error signals original network activation mechanism zipser rumelhart tesauro merely replaces kind non locality activation kind problem maintaining sets weights 
approach uses global reinforcement signal specific error signals andersen jordan powerful standard backpropagation 
approach proposed hinton mcclelland bi directional activation recirculation single recurrently connected network symmetric weights convey error signals 
order get somewhat unwieldy stage activation update process works auto encoder networks 
presents generalized version recirculation algorithm generec overcomes limitations earlier algorithm generic recurrent network sigmoidal units learn arbitrary input output mappings 
generec algorithm shows general form error backpropagation computes essentially error derivatives almeida pineda ap algorithm almeida pineda recurrent networks certain conditions performed biologically plausible fashion locally available activation variables 
generec uses recurrent activation flow communicate error signals output layer hidden layer symmetric weights 
weight symmetry important condition computing correct error derivatives 
catch generec preserve symmetry weights modified longer follows learning trajectory ap computing essentially error gradient 
empirical relationship derivatives computed generec ap backpropagation explored simulations reported 
generec algorithm common contrastive hebbian learning algorithm chl mean field deterministic boltzmann machine dbm learning algorithm uses locally available activation variables perform error driven learning recurrently connected networks 
algorithm derived originally stochastic networks activation states described boltzmann distribution ackley hinton sejnowski 
context chl amounts reducing distance probability distributions arise phases settling network 
algorithm extended deterministic case approximations restricted cases probabilistic hinton peterson anderson derived boltzmann distribution continuous hopfield energy function movellan 
derivations require problematic assumptions approximations leading conclude chl fundamentally flawed deterministic networks hinton 
shown chl algorithm derived variant generec algorithm establishes general formal relationship bp framework deterministic chl rule previous attempts peterson movellan lecun denker 
relationship means known fully general error driven learning algorithms local activation variables deterministic networks considered variations generec algorithm indirectly backpropagation algorithm 
important feature generec derivation chl relationship learning properties bp generec chl clearly understood 
feature derivation completely general respect activation function generalized recirculation algorithm allowing chl learning rules derived different cases 
chl equivalent generec simple approximation second order accurate numerical integration technique known midpoint second order runge kutta method plus additional symmetry preservation constraint 
implementation midpoint method generec simply amounts incorporation sending unit plus phase activation state error derivative amounts line pattern integration technique 
method results faster learning reducing amount interference due independently computed weight changes pattern 
explain chl networks generally learn faster equivalent bp networks peterson hartman movellan 
comparison optimal learning speeds variants generec feedforward ap recurrent backprop different problems reported 
results comparison consistent derived relationship generec ap backpropagation interpretation chl symmetric midpoint version generec provide empirical support theoretical claims 
finding chl perform networks multiple hidden layers deep networks reported appear problematic claim chl performing fully general form backpropagation learn deep networks 
unable replicate failure learn family trees problem hinton chl 
simulations reported show simply increasing number hidden units chl networks learn problem success rate number epochs order backpropagation 
existing simulation evidence support idea chl performing form backpropagation fundamentally flawed approximation boltzmann machine argued hinton 
generec family algorithms encompasses known ways performing error driven learning locally available activation variables provides promising framework thinking error driven learning implemented brain 
goal explicit biological mechanism capable implementing generec style learning proposed 
mechanism consistent available evidence regarding synaptic modification neurons neocortex hippocampus predictions 
algorithms notation original recirculation algorithm hinton mcclelland generec depends ideas standard learning algorithms including feedforward error backpropagation bp rumelhart cross entropy error term hinton almeida pineda ap algorithm error backpropagation recurrent network almeida pineda contrastive hebbian learning algorithm chl boltzmann machine deterministic variants ackley hinton peterson anderson 
notation equations algorithms summarized section followed brief overview recirculation algorithm section 
provides basis development generalized recirculation algorithm subsequent sections 
feedforward error backpropagation notation layer feedforward backprop network uses symbols shown table 
target values labeled output unit pattern wise sum dropped subsequent derivations depend 
cross entropy error formulation hinton eliminates activation derivative term learning rule recirculation algorithm 
reilly layer index net input activation input stimulus input hidden ij oe output jk oe table variables layer backprop network 
oe standard sigmoidal activation function oe gammaj 
cross entropy error defined log gamma log gamma derivative respect weight output unit jk de dj jk gamma gamma gamma oe gamma oe derivative sigmoidal activation function respect net input gamma canceled denominator error term 
order train weights hidden units impact hidden unit error term needs determined de dj gamma gamma jk take derivative error function respect input hidden unit weights ij dh dj ij gamma gamma jk oe provides basis adapting weights 
almeida pineda recurrent backpropagation ap version backpropagation essentially feedforward described allows network recurrent bidirectional connectivity 
network trained settle stable activation state output units target state input pattern clamped input units 
version bp generec algorithm uses recurrent connectivity approximates closely 
basic notation cross entropy error term described feedforward bp describe ap algorithm net input terms include input unit network lower layers 
generalized recirculation algorithm layer phase net input activation input stimulus input hidden gamma ij kj gamma gamma oe gamma ij kj oe output gamma jk gamma gamma oe gamma table equilibrium network variables layer network having reciprocal connectivity hidden output layers symmetric weights jk kj phases output units target clamped plus phase minus phase 
oe standard sigmoidal activation function 
activation states ap updated discrete time approximation differential equation integrated time respect net input terms dj dt gammaj ij oe equation iteratively applied network settles stable equilibrium state change activation state goes small threshold value provably weights symmetric hopfield hinton 
way activations iteratively updated allow recurrent activation dynamics error propagation ap algorithm performed iteratively 
iterative error propagation ap operates new variable represents current estimate derivative error respect net input unit variable updated dy dt gammay oe kj externally injected error output units target activations 
equation iteratively applied variables settle equilibrium state change falls small threshold value 
weights adjusted feedforward bp providing term 
contrastive hebbian learning contrastive hebbian learning algorithm chl stochastic boltzmann machine deterministic variants differences activation states different phases 
ap algorithm connectivity recurrent activation states deterministic version computed 
discussed locally computable activation differences non local error backpropagation bp ap algorithms biologically plausible 
generec algorithm chl algorithm derived special case uses notion activation phases 
phases activation states chl plus phase states result input target network provide training signal compared minus phase activations result just input pattern 
equilibrium network variables states iterative updating procedure applied phase system labeled table 
possible incrementally update activations net inputs limits ability units change state rapidly largest activation value net inputs bounded 
reilly chl learning rule deterministic recurrent networks expressed terms generic activation states layer network follows ffl deltaw ij gamma gamma gamma sending unit receiving unit 
chl simply difference pre post synaptic activation coproducts plus minus phases 
coproduct term equivalent derivative energy harmony function network respect weight ij intuitive interpretation rule decreases energy increases harmony state vice versa minus phase state see ackley peterson anderson hinton movellan details 
recirculation algorithm original hinton mcclelland recirculation algorithm feedforward bp algorithm 
generec algorithm closely related ap recurrent version backpropagation borrows key insights recirculation algorithm 
insights provide means overcoming main problem standard backprop formulation neural plausibility standpoint manner hidden unit computes error contribution 
shown 
problem hidden unit required access computed quantity depends variables output unit 
crux non locality error information backpropagation 
key insight extracted recirculation algorithm expressed difference terms look net input hidden unit gamma jk gamma jk having separate error backpropagation phase communicate error signals think terms standard activation propagation occurring reciprocal symmetric weights come output units hidden units 
error contribution hidden unit expressed terms difference net input terms 
net input term just received output units target activations clamped received outputs feed forward activation values order take advantage net input error signal hinton mcclelland autoencoder framework pools units visible hidden 
visible units play role input layer output layer 
training pattern target visible units project set hidden units feed back visible units 
input hidden units changes state visible units new state fed system name recirculation see 
result visible units activation states equivalent hidden units activation states function kj labeled second corresponds second key insight recirculation algorithm computing difference difference activations approximation ij gamma gamma oe gamma gamma generalized recirculation algorithm target pattern reconstructed pattern recirculation hinton mcclelland recirculation algorithm proposed hinton mcclelland 
activation propagated steps gamma 
target pattern clamped visible units 
hidden units computing activation function target inputs 
visible units computing activations function hidden unit state 
hidden unit state computed function reconstructed visible unit pattern 
difference activation values net inputs hinton mcclelland imposed additional constraint difference reconstructed target visible difference kept small regression function updating visible units 
function assigns output state computed time weighted average target output activation computed current net input hidden units gamma difference hidden unit activation values approximately equivalent difference times slope activation function net input values oe long linear approximation activation function slope reasonably valid 
case long activation function monotonic error approximation affect sign resulting error derivative magnitude 
errors magnitude lead errors sign pattern wise sum 
difference activations computes derivative activation function implicitly resulting learning rule reasonable monotonic activation function important cases derivative activation function difficult compute 
activation variable easier map biological neuron avoids need neuron compute activation derivative 
note due simplification learning rule recirculation algorithm output units essentially delta rule 
means locally available activation states pre postsynaptic units perform error driven learning avoids note hinton mcclelland linear output units avoid activation function derivative output units cross entropy avoid derivative 
function jk linear sigmoid depending assumptions 
note output units sigmoidal function order cross entropy function cancel derivative 
reilly need biologically troublesome error backpropagation mechanism different normal propagation activation network 
phase learning generalized recirculation recirculation algorithm biologically plausible standard feedforward error backpropagation limited learning auto encoder problems 
recirculation activation propagation sequence requires detailed level control flow activation network interaction learning 
critical insights computing error signals differences net input activation terms applied general case standard layer network learning input output mappings 
section presents generalized recirculation generec algorithm uses standard recurrent activation dynamics ap chl algorithms communicate error signals recirculation technique 
stages activations recirculation generec uses activation phases chl algorithm described 
terms activation states generec identical deterministic chl algorithm notation describe 
learning rule generec simply application key insights recirculation algorithm ap recurrent backpropagation algorithm feedforward bp algorithm basis recirculation algorithm 
recurrent connections hidden output units ignored error output layer held constant easy show fixed point solution iterative ap error updating equation dy dt hidden unit error form feedforward backpropagation 
means recirculation trick computing error signal difference net input activation terms oe kj oe kj gamma oe kj gamma kj assuming constant error output layer equilibrium error gradient computed hidden units ap equivalent difference generec equilibrium net input states plus minus phases 
note minus phase activations generec identical ap activation states 
difference activation states substituted net input differences times derivative activation function approximation introduced recirculation resulting equilibrium unit error gradient gamma gamma note hidden unit states generec reflect constant net input input layer addition output layer activations communicate necessary error gradient information cancels difference computation 
constant input hidden units input layer phases play role regression update equation recirculation 
extent input reasonably large biases hidden units sigmoid bias tend moderate differences gamma making difference reasonable approximation differences respective net inputs times slope activation function 
generalized recirculation algorithm analysis useful seeing generec equilibrium activation states approximate equilibrium error gradient computed ap ap algorithm performs iterative updating error variable 
case iterative updating single variable equivalent iterative updating activation states plus minus generec difference 
relationship expressed writing generec ap notation 
define components error variable effectively generec plus minus phase net inputs ignoring net input input units subtracted anyway dy dt gammay kj dy gamma dt gammay gamma kj oe approximate fixed point difference fixed points variables oe gamma gamma gamma gamma approximated subtraction generec equilibrium activation states discussed previously 
unfortunately validity approximation guaranteed proof author aware 
properties equations lend credibility 
part function output units effectively constant part gamma just activation updating procedure generec ap common 
fixed point solutions generec ap equations recurrent influences ignored pattern recurrent influences set weights additional effects recurrence direction generec ap 
short proof arguments require simulation results comparing differences error derivatives computed generec ap 
results confirm generec computes essentially error derivatives ap layer network long weights symmetric 
approximation deteriorates slightly networks multiple hidden layers effects recurrence considerably greater 
recirculation algorithm important approximation weights hidden units output units values corresponding weights output units receive hidden units 
familiar symmetric weight constraint necessary prove network settle stable equilibrium hopfield 
revisit constraint times 
time assume weights symmetric 
virtually deterministic recurrent networks including generec suffer problem changes weights gradient information computed equilibrium activations result network settling activation state lower error time 
due fact small weight changes affect settling trajectory unpredictable ways resulting entirely different equilibrium activation state settled time 
important keep mind possibility discontinuities progression activation states learning basis optimism issue 
example justification deterministic version boltzmann machine dbm hinton supplies arguments substantiated number empirical findings justifying assumption small weight changes generally lead contiguous equilibrium state unit activities recurrent network 
reilly summarize learning rule generec computes error backpropagation gradient locally recurrent activation propagation recirculation having form delta rule 
stated follows terms sending unit activation receiving unit activation ffl deltaw ij gamma gamma gamma shown learning rule compute error derivatives ap recurrent backpropagation procedure conditions ffl iterative updating error term approximated separate iterative updating activation terms gamma difference 
ffl reciprocal weights symmetric jk kj 
ffl differences net inputs times activation function derivative approximated differences activation values 
relationship generec chl generec learning rule chl learning rule simple expressions involve difference plus minus phase activations 
raises possibility related 
described different ways generec modified combined yield 
generec learning rule divided parts represents derivative error respect unit difference unit plus minus activations gamma gamma represents contribution particular weight error term sending unit activation gamma 
phase term source modification 
standard feedforward ap recurrent backpropagation activation term associated unit equivalent minus phase activation generec phase framework 
contribution sending unit naturally evaluated respect activation gamma appears generec learning rule 
generec activation term corresponding plus phase state units wonder derivative weight evaluated respect activation 
plus phase activation value accurate reflection eventual contribution unit weights network updated 
sense value anticipates weight changes lead having correct target values activated learning avoid interference 
hand minus phase activation reflects actual contribution sending unit current error signal reasonable credit assignment 
arguments favor phases approach simply average 
doing results weight update rule ffl deltaw ij gamma gamma gamma way generec needs modified equivalent chl 
discussed detail modification generec corresponds simple approximation midpoint second order accurate runge kutta integration technique 
consequences midpoint method learning speed explored simulations reported 
generalized recirculation algorithm second way generec needs modified concerns issue symmetric weights 
order generec compute error gradient reciprocal weights weights need value relative magnitudes signs forward going weights 
basic generec learning rule preserve symmetry gamma gamma gamma gamma gamma gamma simulations reported indicate generec learn settle stable attractors explicitly preserving weight symmetry symmetry preserving version generec guarantee computed error derivatives correct 
straightforward way ensuring weight symmetry simply average simply sum weight changes computed reciprocal weights separately apply change weights 
symmetric generec learning rule ffl deltaw ij gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma note rule result weights updated way ap backpropagation error derivatives computed hidden units 
symmetry preserving version generec identical ap backpropagation 
issue explored simulations reported 
midpoint method symmetry preservation versions generec combined result chl algorithm ffl deltaw ij gamma gamma gamma gamma gamma gamma gamma gamma gamma note lecun denker pointed chl related symmetric version delta rule generec incorporate midpoint method able show approximation ignored aspect relationship chl generec 
derivation chl interesting reasons 
error backpropagation generec kind approximation stochastic system 
eliminates problems associated considering graded activations units deterministic system expected values underlying probability distribution 
example order compute probability activation state needs assume units statistically independent see hinton peterson anderson 
movellan showed chl derived independent boltzmann concomitant mean field assumptions derivation apply hidden units network 
consequences relationship chl derived generec standard error backpropagation chl uses faster midpoint integration method imposes constraint apparent relative learning properties algorithms 
derivation explain chl networks tend learn faster equivalent backprop networks 
advantage derivation bp framework sufficiently general allow learning rules derived variety different activation functions network properties 
midpoint method generec approximation mentioned average minus plus phase activations sending unit generec learning rule corresponds approximation simple numerical integration reilly midpoint trial step actual step midpoint method 
point trial step taken derivative point derivative re computed midpoint point trial step 
derivative take actual step point 
technique differential equations known midpoint second order accurate runge kutta method press flannery teukolsky vetterling 
midpoint method attains second order accuracy explicit computation second derivatives evaluating derivatives twice combining results minimize integration error 
illustrated simple differential equation dy dt simplest way value variable integrated difference equation approximation continuous differential equation fflf ffl step size ffl accuracy order taylor series expansion error term order ffl 
integration technique known forward euler method commonly neural network gradient descent algorithms bp 
comparison midpoint method takes trial step forward euler method resulting estimate function value denoted fflf estimate compute actual step derivative computed point halfway current estimated values see fflf ffl terms taylor series expansion function point evaluating derivative midpoint cancels order error term ffl resulting method second order generalized recirculation algorithm accuracy press 
intuitively midpoint method able anticipate curvature gradient avoid going far wrong direction 
number ways midpoint method applied error backpropagation 
correct way doing run entire batch training patterns compute trial step weight derivative run batch patterns weights half way current trial step values get actual weight changes 
require roughly twice number computations weight update standard batch mode backpropagation passes batch mode learning particularly biologically plausible 
generec version approximation correct version respects 
plus phase activation value line estimate activations result forward euler step weights 
estimate advantages available additional computation line weight updating solving major problems correct version 
relationship plus phase activation forward euler step error gradient sense plus phase activation target state direction reducing error 
appendix gives formal analysis relationship 
analysis shows plus phase activation depend learning rate parameter typically estimates size trial euler step 
plus phase activation means precise midpoint computed 
anticipatory function method served trial step exaggerated 
simulation results described indicate advantageous certain tasks take larger trial step 

midpoint method applied portion derivative distributes unit error term incoming weights computation error term 
error term gamma gamma standard forward euler integration method sending activations evaluated midpoint current trial step gamma 
selective application midpoint method particularly efficient case line backpropagation midpoint value unit error term especially single pattern basis typically smaller original error value trial step direction reducing error 
midpoint error value slow learning reducing effective learning rate 
summarize advantage approximate midpoint method represented simple compute appears reliably speed learning line learning 
sophisticated integration techniques developed batch mode bp see battiti review typically require considerable additional computation step biologically plausible 
generec approximate midpoint method backpropagation order validate idea chl equivalent generec approximation midpoint method described approximation implemented standard backpropagation network relative learning speed advantage method compared different algorithms 
similar kinds speedups generec backpropagation support derivation chl 
comparisons described simulation section 
versions generec approximate midpoint method relevant consider 
weight method computes sending unit trial step activation trial step reilly weights simpler approximation uses unit error derivative estimate trial step activation 
cases resulting trial step activation state averaged current activation value obtain midpoint activation value sending activation state backpropagation weight updating ffl deltaw jk gamma corresponds generec version midpoint method 
note layer network hidden output weights affected trial step activation value input units 
bp weight midpoint approximation trial step activation computed weights updated trial step weight error derivatives follows ij ffl ts gamma ij oe ffl ts learning rate constant determines size trial step taken 
note keeping generec approximation actual learning rate ffl included equation 
depending relative sizes ffl ts ffl estimated trial step activation estimate size actual trial step activation 
order evaluate effects estimation range ffl ts values explored 
bp unit error method uses fact weight changed proportion derivative error respect unit net input avoid additional traversal weights ffl ts gamma oe number receiving weights fan 
case trial step size parameter ffl ts reflects average activity level input layer input weight changed amount proportional activity sending unit 
comments regarding ffl ts apply case 
note unit error version midpoint method closely corresponds version chl error derivative respect unit weights unit 
simulations reported indicate midpoint method speed line bp learning nearly factor 
unit error version quite simple requires little extra computation implement 
unit error version applied directly almeida pineda backpropagation true weight version require additional activation settling trial step weights 
order compare ways implementing approximate midpoint method results feedforward backprop networks 
simulation experiments set simulations reported section comparison learning speed varieties generec including symmetric midpoint chl bp midpoint integration method 
gives general sense comparative learning properties different algorithms provides empirical evidence support predicted relationships algorithms generalized recirculation algorithm investigated 
second set simulations detailed comparison weight derivatives computed almeida pineda version backpropagation generec performed showing compute error derivatives certain conditions 
learning speed comparisons perform useful comparisons different learning algorithms comparisons provide empirical evidence necessary evaluating theoretical claims derivation generec algorithm relationship chl 
note intent comparison promote algorithm require broader sample commonly speedup techniques backpropagation 
derivation generec ap backpropagation relationship chl approximate midpoint method specific predictions algorithms learn faster reliably extent empirical results consistent predictions provides support analysis 
particular predicted generec able solve difficult problems roughly order epochs ap algorithm weight symmetry play important role ability generec solve problems 
predicted midpoint versions generec backprop learn faster standard versions 
results consistent predictions 
apparent generec networks learn difficult tasks midpoint integration method appears speed learning generec backpropagation networks 
consistent idea chl equivalent generec midpoint method 
adding symmetry preservation constraint generec generally increases number networks solve task case encoder reasons explained 
consistent idea symmetry important computing correct error derivatives 
different simulation tasks studied xor hidden units encoder shifter task hinton family trees task hinton units hidden encoding layer 
networks valued sigmoidal units 
backpropagation networks cross entropy error function error tolerance output activation target unit error 
generec networks activation values bounded 
generec ap backpropagation networks initial activation values set step size dt update activations 
settling stopped maximum change activation multiplying dt 
networks random initial weights symmetric generec networks run xor encoder shifter family trees problems 
training criterion xor encoder total sum squares error criterion shifter family trees problems units right side patterns 
networks stopped epochs solved xor encoder shifter problems epochs family trees 
simple dimensional grid search performed learning rate parameter order determine fastest average learning speed algorithm problem 
xor encoder shifter tasks grid increments grid family trees problem 
momentum modifications generic backpropagation weights updated pattern patterns randomly permuted order epoch 
results fastest networks reached training criterion 
really important xor problem algorithms typically get stuck problems 
algorithms compared follows see table bp standard feedforward error backpropagation cross entropy error function 
reilly euler midpoint err method ff vs rec sym sym bp ff bp bp mid rec ap act diff ff rec gr gr sym gr mid chl table relationship algorithms tested respect local activations vs explicit backpropagation bp act diff compute error derivatives feedforward vs recurrent ff rec forward euler vs euler midpoint weight symmetrization sym 
gr generec 
ap almeida pineda backpropagation recurrent network cross entropy error function 
bp mid wt feedforward error backpropagation weight version approximate midpoint 
different values trial step size parameter ffl ts order determine effects overestimating trial step case generec 
values xor encoder shifter problem family trees problem 
large trial step sizes resulted faster learning small networks progressively smaller step sizes necessary larger problems 
bp mid un feedforward error backpropagation unit error version midpoint integration method 
trial step size parameters bp mid wt 
gr basic generec algorithm 
gr sym generec symmetry preservation constraint 
gr mid generec approximate midpoint method 
chl generec symmetry approximate midpoint method equivalent chl 
xor encoder results xor problem shown table encoder shown table 
results largely consistent predictions exception apparent interaction encoder problem weight symmetrization generec 
apparent plain generec algorithm successful fast weight symmetrization necessary improve success rate xor task learning speed encoder 
shown detail symmetrization constraint essential computing correct error derivatives generec 
symmetry constraint effectively limits range weight space searched learning algorithm symmetric weight configurations learned affect ability get bad initial weight configurations 
effect may compounded encoder problem input tendency symmetric hidden 
symmetry constraint important able compute correct error derivatives introduces additional constraint impair learning dramatically case encoder 
note larger complicated tasks shifter family trees described advantages computing correct derivatives outweigh disadvantages additional symmetry constraint 
generalized recirculation algorithm algorithm ffl epcs sem bp ap bp mid wt bp mid wt bp mid wt bp mid wt bp mid un bp mid un bp mid un bp mid un gr gr sym gr mid chl table results xor problem 
ffl optimal learning rate number networks successfully solved problem minimum epcs mean number epochs required reach criterion sem standard error mean 
algorithms described text 
note best performance gr networks 
algorithm ffl epcs sem bp ap bp mid wt bp mid wt bp mid wt bp mid wt bp mid un bp mid un bp mid un bp mid un gr gr sym gr mid chl table results encoder problem 
ffl optimal learning rate number networks successfully solved problem minimum epcs mean number epochs required reach criterion sem standard error mean 
algorithms described text 
reilly main prediction analysis approximate midpoint method result faster learning bp generec 
appears case speedup relative regular backprop nearly fold unit error version trial step size 
general advantage unit error weight midpoint method bp interesting considering corresponds generec version midpoint method 
speedup generec chl vs gr sym gr mid vs gr comparisons substantial general 
interesting approximate midpoint method symmetrization constraint enable generec algorithm successfully solve problems 
tasks gr mid performed better gr sym 
attributable ability midpoint method compute better weight derivatives affected inaccuracies introduced lack weight symmetry 
note hold layer networks studied breaks family trees task requires error derivatives passed back multiple hidden layers 
encoder gr mid perform better chl indicating generally advantage having correct error derivatives weight symmetrization addition midpoint method 
additional finding appears advantage recurrent feedforward comparison ap vs bp results 
explained fact small weight changes recurrent network lead dramatic activation state differences feedforward network 
effect recurrent network achieve set activation states feedforward network 
advantage generec probably partially offset additional weight symmetry constraint 
appears liability networks multiple hidden layers family trees results 
shifter task shifter problem larger task xor encoder provide realistic performance typical tasks 
version shifter problem bit input patterns shifted version 
values shift corresponding bit left bit right wrap 
possible binary patterns bits unsuitable result pattern shifted right left 
training patterns bit patterns shifted directions 
task classify shift direction activating output units 
larger versions task levels shift bits input explored configuration proved difficult terms epochs standard bp network solve 
results shown table provide clearer support predicted relationships previous tasks 
particular midpoint speedup comparable bp generec cases role symmetry generec unambiguously important solving task evident complete failure non symmetric version learn problem 
interesting complicated problem approximate midpoint method additional constraint enables generec networks learn problem 
combination approximate midpoint method constraint chl algorithm performs better 
previous tasks appears advantage recurrent network evidenced faster learning ap compared bp 
family trees task mentioned family trees task hinton particular interest reported unable train chl solve problem 
unable note common tasks digit recognition classification tasks easily solved standard bp network epochs provide useful dynamic range desired comparisons 
generalized recirculation algorithm algorithm ffl epcs sem bp ap bp mid wt bp mid wt bp mid wt bp mid un bp mid un bp mid un gr gr sym gr mid chl table results shifter task 
ffl optimal learning rate number networks successfully solved problem epcs mean number epochs required reach criterion sem standard error mean 
algorithms described text 
algorithm ffl epcs sem bp ap bp mid wt bp mid wt bp mid wt bp mid un bp mid un bp mid un gr gr sym gr mid chl table results family trees problem 
ffl optimal learning rate number networks successfully solved problem minimum epcs mean number epochs required reach criterion sem standard error mean 
algorithms described text 
reilly get chl network learn problem number hidden units original backprop version task encoding units input output layer central hidden units simply increasing number encoding units allow chl learn task reliability 
learning rate search performed networks encoding hidden units ensure networks capable learning 
seen results shown table chl networks able reliably solve task roughly comparable number epochs ap networks 
note recurrent networks generec ap appear disadvantage relative feedforward bp task probably due difficulty shaping appropriate attractors multiple hidden layers 
symmetry preservation appears critical generec learning deep networks generec networks unable solve task midpoint method 
comparable performance ap chl supports derivation chl generec algorithm essentially form backpropagation calls question analyses regarding limitations chl deterministic approximation boltzmann machine 
difficult determine responsible failure learn family trees problem reported differences way networks run compared ones described including annealing schedule activation cutoff activations range batch mode line weight updating activation opposed net settling 
unit error learning speed advantage task 
consistent trend previous results 
advantage unit error midpoint method due reliance derivative error respect hidden unit reliable indication curvature derivative weight derivatives method 
generec approximation ap bp analysis earlier shows generec error derivatives almeida pineda version error backpropagation recurrent network conditions hold ffl difference plus minus phase activation terms generec updated separate iterative activation settling phases compute unit error term iterative update difference almeida pineda uses 
ffl reciprocal weights symmetric 
enables activation signals output hidden units recurrent weights reflect contribution hidden units output error forward going weights 
ffl difference activations plus minus phases reasonable approximation difference net inputs times derivative sigmoidal activation function 
note affects magnitude weight derivatives direction 
order evaluate extent violated effect learning generec identical networks run side side sequence training patterns noted performance feedforward bp task faster previously reported results 
due line learning momentum enables network take advantage noise due random order training patterns break symmetry error signals generated problem distinguish different training patterns 
generalized recirculation algorithm epochs training dot product norm sse generec vs almeida pineda stuck net generec brute force symmetrization dwt dot product sse normalized epochs training dot product norm sse generec vs almeida pineda fast net generec brute force symmetrization dwt dot product sse normalized correspondence average normalized dot product weight derivatives computed generec almeida pineda algorithms random initial weight configurations 
weights computed generec symmetry preserved brute force method 
correspondence nearly perfect late training stuck network point network developed large weights appear affect accuracy computed weight derivatives 
network ap cross entropy error function compute error derivatives generec 
standard encoder problem 
extent generec error derivatives computed ap measured normalized dot product weight derivative vectors computed algorithms 
comparison input hidden weights reflect error derivatives computed hidden units 
hidden output weights driven error signal output units environment derivatives identical networks 
order control weight differences accumulate time networks weights copied generec network ap network weight update 
networks run weights order determine different learning trajectory algorithms initial weight values 
weights initialized symmetric 
noted basic generec algorithm preserve symmetry weights undoubtedly affect computation error gradients 
extent symmetry measured normalized dot product reciprocal hidden output weights 
predicted symmetry measure determine large part extent generec computes error derivatives ap 
order test hypothesis methods preserving symmetry weights learning 
method symmetry preserving learning rule shown brute force method reciprocal weights set average values updated 
advantage method rule change computed weight changes 
parameters networks activation step size dt initial activations set settling cutoff maximum change activation learning rate initial random sigma 
reilly epochs training generec vs almeida pineda medium net generec symmetrization dwt dot product recip wts dot product sse normalized epochs training dot product norm sse generec vs almeida pineda fast net generec symmetrization dwt dot product recip wts dot product sse normalized correspondence average normalized dot product weight derivatives computed generec almeida pineda algorithms random initial weight configurations 
weights computed generec 
symmetry imposed weights 
correspondence appears roughly correlated extent weights symmetric 
main result analysis generec algorithm typically computes essentially error derivatives ap weights symmetric 
seen shows different networks running weights brute force method 
weight derivatives computed generec normalized dot product computed ap nearly weights got large network stuck local minimum 
result shows generec usually computes appropriate backpropagation error gradient difference equilibrium activation states plus minus phases supporting approximation 
contrast weight enforced correspondence generec ap weight derivatives appears correlated extent weights symmetric seen 
results runs shown ability generec network solve task appeared correlated extent weights remain symmetric 
note explicit weight symmetrization symmetry preserving learning rule weights symmetric due fortuitous correspondence weight changes reciprocal sets weights 
symmetry preserving rule resulted weight changes typically different computed ap results show error derivatives hidden unit correct 
simply due fact symmetric generec additional symmetry preserving term ap 
symmetric generec algorithm resulted non learning trajectories mirrored ap algorithm remarkably closely 
representative example shown 
difficult certain source correspondence occur networks brute force symmetry preservation method 
question generec compute correct error derivatives network multiple hidden layers differences way generec ap compute error terms apparent due greater influence recurrent setting minus plus phases 
kinds approximations deriving chl deterministic boltzmann generalized recirculation algorithm epochs training dot product norm sse generec vs almeida pineda medium net learning rule symmetrization dwt dot product recip wts dot product ap sse normalized gr sse normalized epochs training dot product norm sse generec vs almeida pineda medium net symmetrization dwt dot product recip wts dot product ap sse normalized gr sse normalized learning trajectories error derivative correspondence non ap generec networks initial weights 
shows standard generec weight symmetrization 
shows generec symmetry preserving learning rule 
rule result networks computing weight updates follow remarkably similar learning trajectory 
case regular generec 
epochs training almeida pineda learning trajectory family trees network epochs training dot product norm sse generec vs almeida pineda family trees network generec brute force symmetrization dwt dot product dwt dot product dwt dot product sse normalized correspondence average normalized dot product weight derivatives computed generec almeida pineda algorithms family trees network 
weights computed generec symmetry preserved brute force method 
agent input agent encoding hidden layer weights agent encoding central hidden layer weights hidden layer patient encoding layer weights 
correspondence layer network remains largely 
shows learning trajectory just ap algorithm smooth feedforward bp 
reilly machine results simulations concluded limitations chl apparent number hidden layers increased deep networks 
address performance generec deep network analysis described performed family trees network hinton brute force symmetrization weight 
network layers hidden units 
normalized dot product measurements error derivatives computed weights agent input agent encoding hidden layer agent encoding central hidden layer hidden layer patient encoding layer weights hidden layers respectively removed output layer 
shows generec computes largely error derivatives ap backpropagation case 
normalized dot product measures usually greater went 
discrepancy generec ap tended increase training proceeded deeper weights 
shows weights got larger differences generec ap due way error computed recurrent settling magnified 
primary problems chl emphasized character error function learning argued provide useful guidance learning 
shows ap algorithm suffers bumpy error surface 
frequency ap bumps bit lower amplitude higher 
indicates due part recurrent nature network small weight changes lead different activation states deficiency learning algorithm se 
possible biological implementation generec learning preceding analysis simulations show generec family phase error driven learning rules approximate error backpropagation locally available activation variables 
fact variables available locally plausible learning rule employed real neurons 
activation signals opposed error variables increases plausibility relatively straightforward map unit activation neural variables membrane potential spiking rate 
main features generec algorithm potentially problematic biological perspective weight symmetry origin plus minus phase activation states ability activation states influence synaptic modification learning rule 
issues addressed context chl version generec best performer simpler form generec versions 
neocortex single important brain area majority cognitive phenomena focus discussion 
weight symmetry cortex ways biological plausibility weight symmetry requirement generec shown important computing correct error gradient addressed 
show exact symmetry critical proper functioning algorithm rough form symmetry required biology 
show rough form symmetry cortex 
data consistent arguments summarized briefly 
order point hinton noted symmetry preserving learning algorithm chl combined weight decay automatically lead symmetric weights start way 
assumes units connected place 
difficult case connection asymmetry investigated hinton chl learning algorithm 
algorithm effective connectivity generalized recirculation algorithm asymmetric pair non input units possible connections existed 
robustness attributed redundancy ways error signal information obtained hidden unit obtain error signal directly output units indirectly connections hidden units 
note absence connection different presence connection non symmetric weight value form asymmetry problematic analysis 
case subset error gradient information available case result specifically wrong gradient information due influence non symmetric weight 
due automatic symmetrization property chl case problem 
terms biological evidence symmetric connectivity indication cortex roughly symmetrically connected 
level identifiable anatomical subregions cortex vast majority areas visual cortex symmetrically connected 
area projects area area receives projection area van essen 
level cortical columns stripes prefrontal cortex monkey levitt lewis lund showed connectivity symmetric interconnected stripes 
neuron received projections neurons stripe projected neurons stripe 
detailed level individual neuron symmetric connectivity difficult assess empirically evidence exist 
evidence rough symmetry detailed symmetry may critical demonstrated hinton chl asymmetrical connectivity long way obtaining reciprocal information subset symmetric connections indirectly neurons area 
phase activations cortex origin phase activations central generec algorithm touches heart controversial aspect error driven learning cortex teaching signal come 
generec teaching signal just plus phase activation state 
standard backpropagation generec suggests teaching signal just state experience network 
interpret state experiencing actual outcome previous conditions 
minus phase thought expectation outcome conditions 
example hearing words sentence expectation develop word come 
state neurons generating expectation minus phase 
experience hearing reading actual word comes establishes subsequent locomotive state activation serves plus phase 
idea brain constantly generating expectations subsequent events discrepancies expectations subsequent outcomes error driven learning suggested mcclelland psychological interpretation backpropagation learning procedure 
particularly attractive generec version backpropagation uses activation states requires additional mechanisms providing specific teaching signals effects experience neural activation states manner widely believed place cortex anyway 
evidence erp recordings electrical activity scalp behavioral tasks cortical activation states reflect expectations sensitive differential outcomes 
example widely studied wave positive going wave occurs msec stimulus onset considered measure violation subjective expectancy determined preceding experience short long term 
formal terms sutton john showed amplitude determined amount prior uncertainty resolved just demonstrate expectations generated salient violated 
reilly minus phase variables plus phase variables gamma gamma ca near gamma gamma ca elevated deltaw ij deltaw ij gamma deltaw ij ltp deltaw ij table directions weight change chl rule qualitative conditions consisting combinations qualitative levels minus plus phase activation coproduct values 
minus phase activation coproduct thought correspond ca increases synaptic efficacy correspond long term potentiation ltp decreases long term depression 
cell consistent biological mechanism ca synaptic activity lead ltp absence ltp 
see text discussion point 
processing event 
nature consistent idea represents plus phase wave activation relatively short time frame development minus phase expectations 
specific properties due specialized neural mechanisms monitoring discrepancies expectations outcomes presence suggests possibility neurons mammalian neocortex experience states activation relatively rapid succession corresponding expectation corresponding outcome 
note generec variants neuron needs plus minus phase activation signals reasonably close temporal proximity order adjust synapses values 
consistent relatively rapid expectation outcome interpretation 
chl special case simply difference coproduct activations potentially computed performing simple hebbian associative learning plus phase point point performing anti hebbian learning minus phase activations hinton sejnowski 
leaves open problems brain change sign weight change kind global switch implemented 
people capable learning things relatively quickly seconds minutes phase switching function difference rem sleep waking behavior suggested phase learning algorithms hinton sejnowski linsker crick mitchison 
possible come answers problems temporally local mechanism suggested plausible 
synaptic modification mechanisms having suggested minus plus phase activations follow rapid succession remains shown activation states influence synaptic modification manner largely consistent chl version generec 
turns biological mechanism proposed accounts different qualitative ranges sign weight change required chl see table 
specifically proposed mechanism predicts weight increase occur pre postsynaptic neurons active plus minus phases chl predicts weight change condition zero 
proposed mechanism corresponds combination chl hebbian style learning rule computational implications subject reilly shows combination error driven associative learning generally beneficial solving different kinds tasks 
briefly summarize findings reilly hebbian component thought imposing additional constraints learning way weight decay conjunction standard error backpropagation 
constraints imposed hebbian learning capable producing useful representations weight decay simply reduce weights zero left devices 
combination chl hebbian learning results networks generalized recirculation algorithm weight decay learn faster especially deep networks family trees task generalize better due effects additional constraints plain chl networks 
purposes crucial aspect mechanism provides error correction term occurs synaptic coproduct larger minus phase plus phase 
defining aspect error driven learning performed chl qualitative ranges chl learning rule similar standard hebbian learning evident table 
generec style learning occur cellular synaptic level neuron needs able retain trace minus phase activation state time neuron experiences plus phase activation state 
reasoning erp data described time period milliseconds 
candidate minus phase trace intracellular ca ca enters postsynaptic area channels pre postsynaptic neurons active 
implement generec style learning rule minus phase ca trace needs interact subsequent plus phase activity determine synapse ltp depressed 
follows term synaptic activity denote activation coproduct term effectively determines amount ca enters channel bliss 
basic categories mechanism provide crucial error correcting modulation sign synaptic modification required chl 
mechanism involves interaction membrane potential synaptic activity ca depends level ca ways signals timing affect various second messenger systems cell provide necessary modulation 
favor mechanism evidence mere presence postsynaptic ca insufficient cause ltp davies unclear exactly additional factor necessary bear 
hypothesis ltp depends activation receptors activated presynaptic activity trigger various mechanisms postsynaptic synaptic compartment 
hand proposed mechanism depends level postsynaptic ca received empirical support reviewed bear 
proposal stipulates increased moderate concentrations postsynaptic ca lead higher concentrations lead ltp 
singer argue mechanism consistent abs learning rule hancock smith phillips singer bienenstock cooper munro stipulates thresholds synaptic modification theta theta gamma level ca higher high threshold theta leads ltp level lower high threshold lower theta gamma threshold leads mechanisms capable pattern synaptic modification shown table context proposed mechanism defined properties 
minimal level ca necessary form synaptic modification ltp 

ca changes relatively slowly persists milliseconds 
allows ca represent prior minus phase activity synapse subsequently active plus phase 

synaptic modification occurs postsynaptic state plus phase activity 
happen locally synaptic modification occurs milliseconds entry ca plus phase activity states long 
alternatively relatively global signal corresponding plus phase triggers synaptic modification provided modulation triggered systems sensitive experience outcomes expectations 

ca initially minus phase due synaptic activity synaptic reilly activity diminished ceased plus phase occur 
expected mechanisms described explicit interaction synaptic activity time modification plus phase trace ca minus phase minus phase ca decayed range time modification occurs plus phase 

synaptic activity place plus phase state sufficient ca ltp occurs 
note means time plus phase activation coproduct reasonably large regardless prior minus phase activity weights increased 
leads combined chl hebbian learning rule discussed 
direct evidence support aspects proposed mechanism discussed indirect evidence support remainder 
empirical literature ltp vast brief summary see singer bear linden reviews 
noted findings described neocortex appear quite general singer linden 
note receptor subject potentiation current value synaptic weight included learning rules accordance generec 
respect point importance ca ltp known bliss clear critical singer hirsh 
support point time course ca concentration measured studies jaffe johnston ross ross connor appears relatively long lasting order seconds clear results reflect happen invasive conditions 
point lancaster zucker significant time period seconds enhanced postsynaptic ca necessary ltp induction 
typical ltp induction regimes involve constant stimulation frequency time periods longer second 
precise time course synaptic potentiation needs studied greater detail evaluate issue fully 
respect existence global learning signal erp data described earlier role systems dopamine suggest monitoring systems exist brain 
example schultz ljungberg describe important role dopamine plays learning responding salient environmental stimuli 
effects probably nature ltp induced direct electrical stimulation individual neurons learning completely dependent global signal 
summarize proposed synaptic modification mechanism consistent findings requires mechanisms 
proposal outlined constitutes set predictions regarding additional factors determine sign magnitude synaptic modification 
analysis simulation results support idea generec family learning algorithms performing variations error backpropagation recurrent network locally available activation variables 
single generec algorithm exactly equivalent almeida pineda algorithm backpropagation recurrent networks generec requires symmetric weights symmetry preserving 
generalized recirculation algorithm idea chl algorithm equivalent symmetry preserving version generec midpoint integration method supported pattern learning speed results different versions generec learning speed increases obtained approximate midpoint integration method backpropagation networks 
shown chl symmetric generec midpoint method reliably learn family trees problem calling question idea chl fundamentally flawed learning algorithm deterministic networks argued 
weight evidence suggests chl viewed variation recurrent backpropagation poor approximation boltzmann machine learning algorithm 
consequence differences generec ap backpropagation mainly symmetry constraint expect generec somewhat different characteristics compared standard backpropagation algorithms may turn differences implications psychological computational models 
analysis imply just exists biologically plausible form backpropagation forms backpropagation biologically plausible 
chl gave best performance generec networks tasks studied symmetry preservation constraint ended liability encoder task 
generec derivation chl practical consequences selection appropriate algorithm task 
derivation allows derive chl algorithms different activation functions network parameters 
important contribution provides unified computational approach understanding error driven learning occur brain 
generec learning rules quite possibly simple local way performing general powerful form learning plausible brain 
specific biological mechanism proposed consistent empirical findings provides starting point exploring hypothesis 
reilly appendix plus phase approximation trial step activations relationship plus phase activation generec hidden unit result euler weight update step taken reduce error denoted formally established 
done simply re computing activation hidden unit weights current error derivatives 
basic generec algorithm difference net input terms activation terms computation easier trial step starred weights follows ij ij ffls gamma gamma oe gamma kj kj gamma gamma gamma oe gamma note original value gamma exact computation midpoint method recurrent network output activation value change weights changed 
impossible express closed form value result settling process recurrent network original value approximation actual value 
sense analysis approximate 
trial step weights compute net input unit receive weight changes denoted follows fact gamma ij gamma kj ij gamma kj ij gamma kj gamma gamma gamma gamma simplify gamma gamma gives gamma gamma plus phase net input activation equivalent forward euler step learning rate ffl set meet conditions ffl oe gamma gamma fixed learning rate smaller result starred trial step activation value direction plus phase activation value term bounded zero quite different minus phase value 
acknowledgments people useful comments earlier drafts manuscript peter dayan jay mcclelland javier movellan rich zemel 
generalized recirculation algorithm ackley hinton sejnowski 

learning algorithm boltzmann machines 
cognitive science 
almeida 

learning rule asynchronous perceptrons feedback combinatorial environment 
butler eds proceedings ieee international conference neural networks san diego ca pp 

singer 

different voltage dependent thresholds inducing long term depression long term potentiation slices rat visual cortex 
nature 
singer 

long term depression excitatory synaptic transmission relationship long term potentiation 
trends neurosciences 
davies 

induction ltp hippocampus needs synaptic activation receptors 
nature 
battiti 

second order methods learning steepest descent newton method 
neural computation 
bear 

synaptic plasticity ltp current opinion neurobiology 
bienenstock cooper munro 

theory development neuron selectivity orientation specificity binocular interaction visual cortex 
journal neuroscience 
singer 

intracellular injection ca blocks induction long term depression rat visual cortex 
proceedings national academy sciences 
bliss 

receptors role long term potentiation 
trends neurosciences 
crick 

excitement neural networks 
nature 
crick mitchison 

function dream sleep 
nature 
van essen 

processing primate cerebral cortex 
cerebral cortex 


limitations deterministic boltzmann machine learning 
network 
hinton 

discovering high order features mean field modules 
touretzky ed advances neural information processing systems 
san mateo ca morgan kaufmann 
hinton 

deterministic boltzmann learning networks asymmetric connectivity 
touretzky elman sejnowski hinton eds connectionist models proceedings summer school pp 

san mateo ca morgan kaufmann publishers hancock smith phillips 

biologically supported error correcting learning rule 
neural computation 


cognition 
plum ed handbook physiology section neurophysiology volume higher functions brain pp 

american physiological society 
hinton 

learning distributed representations concepts 
proceedings th conference cognitive science society pp 

hillsdale nj lawrence associates reilly hinton 

connectionist learning procedures 
artificial intelligence 
hinton 

deterministic boltzmann learning performs steepest descent weight space 
neural computation 
hinton mcclelland 

learning representations recirculation 
anderson ed neural information processing systems pp 

new york american institute physics 
hinton sejnowski 

learning relearning boltzmann machines 
rumelhart mcclelland pdp research group chap 
pp 

hirsh 

postsynaptic ca necessary induction ltp monosynaptic prefrontal neurons vitro study rat 
synapse 
hopfield 

neurons graded response collective computational properties state neurons 
proc 
natl 
acad 
sci 
usa 
jaffe johnston ross ross 

spread na spikes determines pattern dendritic ca entry hippocampal neurons 
nature 


ca entry postsynaptic ca channels transiently excitatory synaptic transmission hippocampus 
neuron 
lecun denker 

new learning rule recurrent networks 
proceedings conference neural networks computing snowbird ut 
levitt lewis lund 

topography pyramidal neuron intrinsic connections macaque monkey prefrontal cortex areas 
journal comparative neurology 
linden 

long term synaptic depression mammalian brain 
neuron 
linsker 

local synaptic learning rules suffice maximize mutual information linear network 
neural computation 


cam kinase ii hypothesis storage synaptic memory 
trends neurosciences 


mechanism hebb anti hebb processes underlying learning memory 
proc 
natl 
acad 
sci 
usa 
lancaster zucker 

temporal limits rise calcium required induction long term potentiation 
neuron 


receptor dependent synaptic plasticity multiple forms mechanisms 
trends 
andersen jordan 

biologically plausible learning rule neural networks 
proc 
natl 
acad 
sci 
usa 
mcclelland 

interaction nature development parallel distributed processing perspective 
eds current advances psychological science ongoing research pp 

hillsdale nj erlbaum 
movellan 

contrastive hebbian learning continuous hopfield model 
touretzky hinton sejnowski eds proceedings connectionist models summer school pp 

san mateo ca morgan kaufmann 
generalized recirculation algorithm 

underlying induction long term depression area ca hippocampus 
science 
reilly 

model neural interactions learning neocortex 
phd thesis carnegie mellon university pittsburgh pa usa 
connor 

role ca entry activated receptors induction long term potentiation 
neuron 
peterson 

mean field theory neural networks feature recognition content addressable memory optimization 
connection science 
peterson anderson 

mean field theory learning algorithm neural networks 
complex systems 
peterson hartman 

explorations mean field theory learning algorithm 
neural networks 
pineda 

generalization backpropagation recurrent higher order neural networks 
anderson ed proceedings ieee conference neural information processing systems denver pp 

new york ieee 
pineda 

generalization backpropagation recurrent neural networks 
physical review letters 
pineda 

dynamics architecture neural computation 
journal complexity 
press flannery teukolsky vetterling 

numerical art scientific computing 
cambridge cambridge university press 
rumelhart hinton williams 

learning internal representations error propagation 
rumelhart chap 
pp 

rumelhart mcclelland pdp research group eds 

parallel 
volume foundations 
cambridge ma mit press 
schultz ljungberg 

responses monkey dopamine neurons reward conditioned stimuli successive steps learning delayed response task 
journal neuroscience 
sutton john 

evoked potential correlates stimulus uncertainty 
science 
tesauro 

neural models classical conditioning theoretical viewpoint 
hanson olson eds connectionist modelling brain function 
cambridge ma mit press 
zipser andersen 

back propagation programmed network simulates response properties subset posterior parietal neurons 
nature 
zipser rumelhart 

significance new learning models 
schwartz ed computational neuroscience pp 

cambridge ma mit press 
