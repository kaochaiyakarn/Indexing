ensemble learning hidden markov models david mackay cavendish laboratory cambridge cb 
uk mackay cam ac uk standard method training hidden markov models optimizes point estimate model parameters 
estimate viewed maximum posterior probability density model parameters may susceptible overfitting contains indication parameter uncertainty 
unrepresentative posterior probability distribution 
study method optimize ensemble approximates entire posterior probability distribution 
ensemble learning algorithm requires resources traditional baum welch algorithm 
traditional training algorithm hidden markov models expectation maximization em algorithm dempster known baum welch algorithm 
maximum likelihood method simple modification penalized maximum likelihood method viewed maximizing posterior probability density model parameters 
hinton van camp developed technique known ensemble learning see mackay review 
maximum posteriori methods optimize point estimate parameters ensemble learning ensemble optimized approximates entire posterior probability distribution parameters 
objective function optimized variational free energy feynman measures relative entropy approximating ensemble true distribution 
derive test ensemble learning algorithm hidden markov models building neal hinton observation expectation maximization algorithms viewed variational free energy minimization methods 
model similar notation rabiner juang 
ffl fs hidden state sequence 
ffl fx observed sequence 
ffl fa ij ij jjs state transition probability matrix ffl fb im im emission probabilities 
ffl initial state distribution 
ffl fa model parameters 
ffl fu hyperparameters define prior 
parameters probability hidden state sequence observed data sj gamma posterior probability hidden variables sjx xj gamma xj sj assume prior probability parameters product dirichlet distributions dirichlet gamma ffi gamma ffi delta function ensures normalized gamma gamma defined hyperparameters positive larger values corresponding stronger priors 
set prior dirichlet gamma fa ii delta similar priors 
reason choosing priors give direct correspondence standard penalized maximum likelihood method initial counts offsets placed bins baum welch algorithm maximum posteriori method posterior density maximized softmax basis mackay probability vector represented parameters standard baum welch optimization baum welch algorithm penalty terms iterative algorithm increases jx iteration maximum reached 
iteration forward backward step computes probabilities state sequences conditioned current parameters step updates parameters 
forward backward probabilities ff fi ff ix fi ff ff ij jx fi ix ij fi step expressed terms ij posterior probability transition state state timestep ij sjx ffi zn ff ij fi zn normalizing constant ij 
step ij gamma ij gamma ij io similar expressions updates 
ensemble learning hmms posterior distribution parameters fa hidden state sequence observation sequence fixed hyperparameters jx approximated ensemble 
constrain approximating distribution separable qa qb constraining assumptions functional forms constituent distributions qa qb 
term ij gamma log ij ij gamma log ij gamma log gamma log log log table dependencies terms equation 
measure closeness ensemble posterior define free energy gamma log ju iterative strategy optimizing sequentially optimize qa qb qs keeping distributions fixed 
looking ahead turn optimized distributions qa qb dirichlet distributions optimized distribution distribution similar posterior distribution equation optimizations performed computational resources baum welch algorithm 
bounded gamma log xju individual minimization decreases ensemble learning algorithm guaranteed converge 
dissect log probability appearing free energy note dependence parameters hyperparameters table 
log ju gamma log ij gamma log im gamma log gamma log log log const derive optimization steps qa qb qs respectively 
optimization qa functional qa qb qs fixed expressed follows qa gamma qa gamma gamma log ij qs gamma log gamma log qa const defining quantity baum welch algorithm ij ffi qa qa log qa ij gamma ij const ij gamma ij gibbs inequality expression log minimized respect appropriate normalizing constant 
minimize fa qa choose distribution qa product dirichlet distributions qa dirichlet gamma fa ij fw ij delta similarly optimal distributions qb products dirichlet distributions defined terms im qs ffi qs ffi optimization qs functional qa qb fixed expressed follows qs gamma qs qa gamma log qb log log gamma log qs const 
defining ij exp qa log ij ik exp thetar qb log ik exp thetar log write qs qs log qs gamma ih const 
optimal distribution qs minimizes qs qs zs gamma zs normalizing constant 
note resemblance distribution posterior distribution sjx equation 
difference normalized probability vectors 
fact geometric means ij ik distribution satisfy 
circle complete 
section qa qb set products dirichlet distributions dirichlet distribution equation obtain dirichlet log gamma log gamma calculate relevant properties optimal distribution quantities forward backward algorithm just obtained equations 
forward backward algorithm affected fact 
ensemble learning computationally inexpensive modification baum welch algorithm 
progress ensemble learning method currently applied toy problems hidden markov models 
train hidden markov models models distinct sources example forwards english text backwards english text test discriminative performance models unseen test data sources 
compare results models trained traditional baum welch algorithm trained ensemble learning 
done issues 

question get predictions optimized ensemble simple approach extract single representative hmm selecting mean value parameters 

option simultaneous optimization hyperparameters mackay peto simplicity initial investigations fixed hyperparameters 
discussion ensemble learning useful method training hidden markov models 
hope takes account parameter uncertainty optimization may cases example data poor problems optimized ensemble gives better representation posterior distribution mode posterior 
note standard penalized maximum likelihood method special case ensemble learning constrain approximating distribution product distribution product adaptable delta functions map qs ffi gamma radford neal geoff hinton helpful discussions 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society 
feynman 
statistical mechanics 
benjamin hinton van camp 
keeping neural networks simple minimizing description length weights 
proc 
th annu 
workshop comput 
learning theory pp 

acm press new york ny 
mackay 
developments probabilistic modelling neural networks ensemble learning 
neural networks artificial intelligence industrial applications 
proceedings rd annual symposium neural networks nijmegen netherlands september pp 
berlin 
springer 
mackay choice basis laplace approximation 
submitted machine learning 
mackay peto 
hierarchical dirichlet language model 
natural language engineering 
neal hinton 
new view em algorithm justifies incremental variants 
biometrika 
submitted 
rabiner juang 
hidden markov models 
ieee assp magazine pp 

