interpreting images propagating bayesian beliefs yair weiss dept brain cognitive sciences massachusetts institute technology cambridge ma usa psyche mit edu central theme computational vision research realization reliable estimation local scene properties requires propagating measurements image 
authors suggested solving vision problems architectures locally connected units updating activity parallel 
unfortunately convergence traditional relaxation methods architectures proven slow general guarantee stable point global minimum 
show architecture bayesian beliefs image properties propagated neighboring units yields convergence times orders magnitude faster traditional methods avoids local minima 
particular architecture non iterative sense marr time step local estimates location optimal information propagated location 
illustrate algorithm performance real images compare existing methods 
mozer jordan petsche editors advances neural information processing systems 
theory essence approach shown 
shows prototypical ill posed problem interpolation function sparse data 
shows traditional relaxation approach problem dense array units represents prototypical ill posed problem traditional relaxation approach dense array units represent value interpolated function 
units update activity local information activity neighboring units 
bayesian belief propagation bbp approach 
units transmit probabilities combine probability calculus non interacting streams 
value interpolated function discretely sampled points 
activity unit updated local data points data available activity neighboring points 
discussed local update rule defined network converges state activity unit corresponds value globally optimal interpolating function 
shows bayesian belief propagation bbp approach problem 
traditional approach function represented activity dense array units 
units transmit probabilities single estimates neighbors combine probabilities probability calculus 
formalize discussion represent activity unit location noisy samples true function 
typical interpolation problem minimize gamma gamma defined grid points data points data 
quadratic local update direction gradient converge optimal estimate 
yields updates sort gamma gamma gamma relaxation algorithms differ choice corresponds gauss seidel relaxation corresponds successive relaxation sor method choice problems 
derive bbp update rule problem note minimizing equivalent maximizing posterior probability assuming generative model oe oe 
ratio oe oe plays role similar original cost functional 
advantage considering cost functional posterior enables methods hidden markov models bayesian belief nets optimal estimation derive local update rules cf 

denote posterior markovian property allows factor terms depending local data depending data left third depending data right cff fi ff gamma fi jy denotes normalizing constant 
denoting conditional gamma ff written terms ff gamma ff ff gamma gamma denotes normalizing constant 
symmetric equation written fi 
suggests propagation scheme units represent probabilities left hand side equations updates right hand side activities neighboring units 
specifically gaussian generating process probabilities represented mean variance 
denote oe similarly ff ff oe ff fi fi oe fi 
performing integration gives kalman filter update parameters oe oe ff ff oe fi fi oe oe ff oe fi ff oe ff gamma ff gamma gamma oe gamma oe ff gamma gamma oe oe ff oe oe ff gamma gamma oe gamma update rules parameters fi analogous far considered continuous estimation problems identical issues arise labeling problems task estimate label take discrete values 
denote label takes value zero 
typically minimizes functionals form gamma traditional relaxation labeling algorithms minimize cost functional updates form gamma different relaxation labeling algorithms differ choice linear sum followed threshold gives discrete hopfield network updates linear sum followed soft threshold gives continuous mean field hopfield updates form gives relaxation labeling algorithm rosenfeld 
see review relaxation labeling methods 
derive bbp algorithm case rewrite posterior markov generating process calculate process 
gives expressions equations integral replaced linear sum 
probabilities gaussian ff fi represented mean variances vector length update rule ff ff ff gamma gamma similarly fi 
convergence equations mathematical identities 
possible show iterations activity units converge correct posteriors maximal distance units architecture iteration refers update units 
furthermore able show iterations activity unit guaranteed represent probability hidden state location data distance guarantee significant light distinction marr regarding local propagation rules 
scheme units communicate neighbors obvious limit fast information reach unit iterations unit know information distance minimal number iterations required data reach units 
marr distinguished types iterations needed allow information reach units versus refine estimate information arrived 
significance guarantee shows bbp uses type iteration iterations allow information reach units 
information arrived represents correct posterior information iterations needed refine estimate 
able show propagations schemes propagate probabilities equations general represent optimal estimate information arrived 
summarize traditional relaxation updates equation bbp updates equations give simple rules updating unit activity local data activities neighboring units 
fact bbp updates probability calculus guarantees unit activity optimal information arrived gives rise qualitative difference convergence types schemes 
section demonstrate difference image interpretation problems 
frame sequence 
hand translated left 
contour extracted standard methods results shows frame sequence hand translated left 
shows bounding contour hand extracted standard techniques 
motion propagation contours local measurements contour insufficient determine motion 
hildreth suggested overcome local ambiguity minimizing cost functional dx dt kv gamma dx dt denote spatial temporal image derivatives denotes velocity point contour 
functional analogous interpolation functional eq 
derivation relaxation bbp updates analogous 
shows estimate motion solely local information 
estimates wrong due aperture problem 
shows performance propagation schemes gradient descent sor bbp 
gradient descent converges slowly improvement estimate discerned plot 
sor converges faster gradient descent significant error iterations 
bbp gets correct estimate iterations 
subsequent plots iteration refers update units network 
due fact iterations estimate location optimal data interval gamma 
case data interval contour correctly estimate motion 
shows estimate produced sor iterations 
simple visual inspection evident estimate quite wrong 
shows correct estimate produced bbp iterations 
direction propagation extracted contour bounds dark light region 
direction dof refers regions certain special cases knowing lk sufficient choosing sequence labels minimizes cases belief revision propagation bbp sor gradient iterations local estimate velocity contour 
performance sor gradient descent bbp function time 
bbp converges orders magnitude faster sor 
motion estimate sor iterations 
motion estimate bbp iterations 
ground 
local cue dof convexity neighboring points contour prefer dof angle defined points acute obtuse 
shows results local cue hand contour 
local cue sufficient 
overcome local ambiguity minimizing cost functional takes account dof neighboring points addition local convexity 
denote dof point contour define gamma determined angle location shows performance propagation algorithms task traditional relaxation labeling algorithms mf hopfield rosenfeld constrained gradient descent bbp 
traditional algorithms converge local minimum bbp converges global minimum 
shows local minimum reached hopfield network shows correct solution reached bbp algorithm 
recall section bbp guaranteed converge correct posterior data 
extensions previous examples ambiguity reduced combining information points contour 
exist cases information propagated points image 
unfortunately propagation problems correspond markov random field mrf generative models bbp mfa relax label relax gradient iterations error local estimate dof contour 
performance hopfield gradient descent relaxation labeling bbp function time 
bbp method converges global minimum 
dof estimate hopfield net convergence 
dof estimate bbp convergence 
calculation posterior done efficiently 
willsky colleagues shown mrfs approximated hierarchical multi resolution models 
current multi resolution generative model derive local bbp rules 
case bayesian beliefs propagated neighboring units pyramidal representation image 
preliminary stages find encouraging results comparison traditional relaxation schemes 
discussion update rules equations differ slightly derived pearl quantities ff fi conditional probabilities constantly normalized sum unity 
pearl original algorithm sequences long ones considering lead messages vanishingly small 
likewise update rules differ slightly forward backward algorithm hmms assumption states equally updates symmetric ff fi 
equation seen variant riccati equation 
addition minor notational differences context update rules different 
hmms kalman filters updates seen interim calculations calculating posterior updates parallel network local units interested estimates units network improve function iteration 
shown architecture propagates bayesian beliefs probability calculus yields orders magnitude improvements convergence traditional schemes propagate probabilities 
image interpretation provides important example task pays bayesian 
acknowledgments adelson dayan tenenbaum comments versions manuscript jordan stimulating discussions introducing bayesian nets 
supported training 
arthur gelb editor 
applied optimal estimation 
mit press 
hildreth 
measurement visual motion 
mit press 
li 
markov random field modeling computer vision 
springer verlag 
mark karl allan willsky 
efficient multiscale regularization application computation optical flow 
ieee transactions image processing 
marr 
vision 
freeman 
judea pearl 
probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann 
lawrence rabiner hwang juang 
fundamentals speech recognition 
ptr prentice hall 
rosenfeld hummel zucker 
scene labeling relaxation operations 
ieee transactions systems man cybernetics 
finkel 
intermediate level visual representations construction surface perception 
journal cognitive neuroscience 
gilbert strang 
applied mathematics 
wellesley cambridge 
