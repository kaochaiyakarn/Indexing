instance utile distinctions reinforcement learning hidden state andrew mccallum department computer science university rochester rochester ny mccallum cs rochester edu utile suffix memory reinforcement learning algorithm uses short term memory overcome state aliasing results hidden state 
combining advantages previous instance memorybased learning previous statistical tests separating noise task structure method learns quickly creates memory needed task hand handles noise 
utile suffix memory uses tree structured representation related prediction suffix trees ron parti game moore algorithm chapman kaelbling variable resolution dynamic programming moore sensory systems embedded agents inherently limited 
reinforcement learning agent sensory limitations hide features environment agent say agent suffers hidden state 
reasons important features hidden robot perception sensors noise limited range limited field view occlusions hide areas sensing space prevent robot desired sensors power supply robot sensors time robot limited computational resources turning raw sensor data usable percepts 
hidden state problem arises case perceptual aliasing mapping states world sensations agent whitehead ballard perceptual limitations allow agent perceive portion world different world states produce percept 
agent active perceptual system meaning redirect sensors different parts surroundings reverse true different percepts result world state 
perceptual aliasing blessing curse 
blessing provide useful invariants representing equivalent world states action required 
curse confound world states different actions required 
perceptual aliasing provides powerful generalization generalize 
trick selectively remove hidden state uncover hidden state impedes task performance leave ambiguous hidden state irrelevant agent task 
distinguishing states difference irrelevant current task causes agent increase storage requirements learning time requiring agent re learn policy needlessly distinguished states 
state identification techniques history information uncover hidden state bertsekas shreve defining agent internal state percepts agent defines internal state space combination percepts short term memory past percepts actions 
agent uses short term memory right places agent uncover non markovian dependencies caused task impeding hidden state 
predefined fixed sized memory representations undesirable 
memory size number internal state variables needed exponentially increases number agent internal states policy learned stored size memory needed agent reverts disadvantages hidden state 
agent designer understands task know maximal memory requirements agent disadvantage fixed sized memory tasks different amounts memory needed different steps task 
conclude agent learn line memory needed different parts state space 
described addresses issue hidden state conjunction principles ffl agents learn tasks 
prefer agents learn behaviors hard coded programming details hand tedious may know environment ahead time want robot adapt behavior environment changes 
ffl agents learn trials possible 
learning experience expensive terms wall clock time dangerous terms potential damage robot surroundings 
experience relatively expensive storage computation 
furthermore expense computation storage decrease advances computing hardware continue cost experience 
ffl agents able handle noisy perceptions actions rewards 
ffl agents short term memory uncover hidden state 
working internal state representation non markovian respect rewards actions agent definition able predict rewards choose actions markovian state representation 
markovian state representation obtainable memory 
furthermore agent learn memory reasons cited principle agent distinctions making distinctions necessary wastes learning time explained earlier 
previous reinforcement learning hidden state solutions hidden state problem short term memory best nonmarkovian state representation 
lion algorithm example simply avoids passing aliased states 
agent finds state delivers inconsistent reward sets state utility low policy visit 
success algorithm depends deterministic world existence path goal consists states 
solutions avoid aliased states involve learning deterministic policies execute incorrect actions aliased states learning stochastic policies aliased states may execute incorrect actions probability 
littman jaakkola reinforcement learning algorithms adopted approach learns memory defining agent internal state space 
perceptual distinctions approach chrisman utile distinction memory mccallum splitting states finite state machine doing batched analysis statistics gathered steps 
recurrent lin training recurrent neural networks 
genetically programmed indexed memory teller evolves agents load store instructions register bank 
chief disadvantage techniques require large number steps training 
memory algorithms perceptual distinctions approach utile distinction memory augment memory capacity line learning 
complementary algorithms utile distinction memory additional interesting feature adds new memory capacity doing increase agent ability predict reward 
schemes predicting percepts udm uses discounted reward build agent internal state space large needed perform task hand large needed represent entire perceived world including irrelevant details 
agent task simple world complex agent internal state space simple 
udm attains task dependent representation robust statistical test distinguish reward variations predicted new memory added reward variations due unpredictable noise 
udm problems 
addition learning extremely slowly trouble discovering utility memories longer time step statistical test examines benefit making single additional state split time 
concerns addressed algorithm called nearest sequence memory nsm mccallum comes family techniques dubbed instance state identification techniques instance memory learning uncovering hidden state 
nsm specifically nearest neighbor 
key idea instance state identification recognition recording raw experience particularly advantageous learning partition state space case agent trying determine history significant uncovering hidden state 
learning agent incorporates experience merely averaging current flawed state space partitions bound attribute experience wrong states experience attributed wrong state turns garbage wasted 
recorded raw experience reinterpreted state space boundaries shift learning 
faced evolving state space keeping raw previous experience path commitment cautious losing information 
nsm learns order magnitude faster hidden state reinforcement learning algorithms mccallum nsm advantage udm easily create memories multiple steps long 
algorithm records raw experience sequence steps leading reward available examination 
observation time action time observation time observation time action time usm style suffix tree 
percepts indicated integers actions letters 
fringe nodes drawn dashed lines 
nodes labeled nodes hold values 
ironically nsm falls severely short exactly areas udm 
nearest neighbor nsm explicitly separate variations due noise variations due task structure nsm handle noise 
furthermore nsm concept utile distinctions amount short term memory nsm creates depends erroneously regional sample density requirements task 
utile suffix memory describes new method dynamically adding short term memory state representation reinforcement learning agent 
call utile suffix memory usm 
result effort combine advantages instance learning utile distinctions 
nearest sequence memory raw experience utile distinction memory algorithm determines memory consider significant statistical test discounted reward 
technique efficient raw experience order learn quickly discover multi step memories easily uses statistical technique order separate noise task structure build task dependent state space 
instance algorithms usm records raw experiences 
reinforcement learning agent transitions consisting action percept reward triples connected time ordered chain 
call individual raw experiences instances 
nsm usm organizes clusters instances way explicitly control history consider significant 
structure controls clustering kind finite state machine called prediction suffix tree ron structure thought order markov model varying different parts state space 
leaf tree acts bucket grouping instances matching history certain length 
deeper leaf tree history instances leaf share 
branches root tree represent distinctions current percept second level branches add distinctions immediately previous action branches previous percept backwards time 
add new instance tree examine percept follow corresponding root branch look action new instance predecessor instance chain follow corresponding second level branch examine percept new instance predecessor proceed likewise reach leaf deposit new instance 
agent builds tree line training short term memory selectively adding branches additional memory needed 
order calculate statistics value additional distinctions tree includes fringe additional branches normally consider leaves tree 
instances fringe nodes tested statistically significant differences expected discounted reward action basis 
kolmogorov smirnov test indicates instances came different distributions fringe nodes official leaves fringe extended 
deeper nodes added populated instances parent node instances properly distributed new children additional distinction 
depth fringe configurable parameter 
multi step fringe usm successfully discover useful multi step memories 
details algorithm interaction agent environment described actions observations rewards 
finite set possible actions fa jaj finite set possible observations fo joj scalar range possible rewards 
time step agent executes action result receives observation reward subscript indicate arbitrary time 
agent aims learn action choosing policy maximizes expected discounted sum reward called return written flr fl fl instance algorithms utile suffix memory records raw experiences 
experience associated time captured transition instance dimensional space written instance tuple consisting available information associated transition time previous instance section detail mccallum discusses natural extension handling multi dimensional continuous spaces 
chain action gamma resulting observation resulting reward 
gamma gamma 
write gamma indicate predecessor instance chain indicate successor 
addition organizing instances time ordered chain utile suffix memory clusters instances nodes suffix tree 
tree nodes labeled deeper layers tree add distinctions alternately previous observations actions 
nodes odd depths labeled observation nodes depths labeled action children parent label 
node tree uniquely identified string labels path node root 
string called node suffix 
interchangeably indicate suffix tree node suffix specifies 
instance deposited leaf node suffix matches suffix actions observations transition instances precede time 
transition belongs leaf label suffix gamma gamma gamma gamma gamma 
set instances associated leaf labeled written 
suffix tree leaf instance belongs written 
official leaves suffix tree additional layers nodes called fringe 
fringe nodes labeled scheme non fringe nodes contain transitions suffix criterion non fringe nodes 
purpose fringe provide agent hypothesis distinctions performing tests distinctions introduced fringe agent decide promote fringe distinctions status official distinction partition state space 
fringe may deep shallow desired different branches fringe may different depths 
deeper fringe help agent discover conjunctions longer multi step distinctions 
term leaf refer non fringe official leaves 
theoretically number fringe nodes grow exponentially depth fringe practice growth fringe extreme fringe non fringe nodes need created agent experiences transition sequences suffixes match nodes 
practice sequences experienced impossible environment reinforcement learner line balance exploitation versus exploration visits 
example possible sequences leading bumping wall explored 
case fewer tree nodes instances 
usm leaves suffix tree internal states reinforcement learning agent 
deep branches tree correspond regions agent internal state space finely distinguished specific long memories shallow branches tree correspond regions agent internal state space broadly distinguished little memory 
agent maintains learned estimates expected discounted reward state action pair 
estimate value choosing action leaf suffix written 
note deeper layers tree correspond earlier time steps values indicate expected values step 
steps usm algorithm 
agent begins tree represents history information tree distinctions agent current percept 
tree root node child percept child nodes leaves 
leaves tree fringe desired depth 
tree nodes empty 
time ordered chain instances empty 

agent step environment 
records transition instance puts instance chain instances 
create transition instance 
gamma gamma 
concerned size instance chain limit growth simply discarding oldest instance adding new instance reasonably sized limit reached 
additionally help deal changing environment 
agent associates new instance leaf fringe nodes suffixes matching suffix new instance 
nodes set nodes path suffix matching fringe leaf node leaf node 
transition deposited nodes fringe deep 
fringe leaf node suffix ft 
step world agent step value iteration bellman leaves tree states 
computational limitations number states full value iteration expensive prioritized sweeping moore atkeson dyna peng williams learning watkins small number agent internal states created usm utile distinction nature value iteration feasible previously possible typically exponential fixed granularity state space divisions 
value iteration consists performing step dynamic programming values fl pr js estimated immediate reward executing action state pr js estimated probability agent arrives state executed action state utility state calculated maxa 
pr js calculated directly recorded instances 
set transition instances node record having executed action remember recorded element jt pr js jt efficiency value iteration calculation obviously benefit caching values pr js strategy incrementally updates values new instance added relevant tree node 
note usm explicitly represents reward noise action noise calculation pr js explicitly represent perception noise 
imply usm handle noisy percepts handle perception noise techniques learning simply average outcomes matching percepts 
usm handle perception noise better explicitly represented partially observable markov decision processes pomdp example 
pomdp find optimal policy observation transition reward model environment example white scherer uses fixed length suffix percept action sequence uses variablelength finite suffix cassandra learns policy finite suffix 
investigating combination methods learn model line usm methods learn policies pomdp subject 

adding new instance tree agent determines new information warrants adding new history distinctions agent internal state space 
agent examines fringe nodes new instance leaf node uses kolmogorov smirnov test compare distributions discounted reward associated action different nodes 
test indicates distributions statistically significant difference implies promoting relevant fringe nodes non fringe leaf nodes help agent predict reward 
agent introduces new history distinctions doing help agent predict reward 
kolmogorov smirnov test answers question distributions significantly different test kolmogorov smirnov test works feature agent comparing distributions real valued numbers expected discounted rewards 
distribution expected discounted reward associated particular node composed set expected discounted reward values associated individual instances node 
expected discounted reward instance written defined flu comparing distributions fringe nodes process result comparisons number fringe nodes new instance leaf current implementation compares distributions fringe nodes distribution leaf node resulting comparisons 
tests result promoting fringe nodes leaves fringe extended new leaves necessary preserve desired fringe depth 
note instance algorithm new distinctions added previous experience properly partitioned new distinctions new nodes added tree instances parent correctly distributed new children looking extra time step back instance history 
concerned computation time agent perform test instance additions leaf node testing addition 

agent chooses action values leaf corresponding history actions observations 
chooses argmax alternatively probability agent explores choosing random action 
increment return step 
experimental results utile suffix memory tested environments show results hallway navigation task states perceptually aliased agent suffers noisy rewards actions percepts space docking task previously test performance perceptual distinctions approach chrisman nearest sequence memory mccallum blocks world hand eye coordination task whitehead ballard hidden state 
hallway navigation agent task navigate goal location actions moving north south east west 
key difficulty robot sensors provide local information binary indications presence absence barrier immediately adjacent robot compass directions 
top shows map environment note different actions required perceptually aliased states 
agent receives reward reaching goal reward attempting move barrier reward 
reaching goal agent executes action begins new trial randomly chosen corner world 
graph performance learning shown bottom 
agent temporal discount factor fl constant exploration probability fringe depth 
usm learns quickly handles noise 
action noise added consisted executing random action probability action steps task completion trials performance training optimal noise noisy reward noisy reward action noisy perception noisy perception action reward top hallway navigation task limited sensors 
world locations labeled integers encode bits perception 
bottom performance learning noise 
plot shows median runs 
chosen agent policy perceptual noise consisted seeing random observation probability reward noise consisted adding values uniformly chosen range gamma normal reward 
combinations noise agent learned policy optimal action perception noise interfere 
graph suboptimal trial lengths continue occur late learning due exploration steps action noise perception noise 
smarter exploration techniques various counter techniques thrun caused exploration decay appropriate amount experience apply straightforwardly tasks hidden state 
efficient exploration hidden state extremely difficult problem remains area requiring exploration issues addressed 
problems regarding exploration hidden state discussed mccallum shows suffix tree learned utile suffix memory 
usm successfully separates noise environment non markov structure environment building memory needed perform task hand needed model entire environment definitely needed represent uniformly deep memories 
deep branches created needed provide multi step memory required disambiguating various state shallower branches created step memory needed observation time action time observation time action time observation time tree learned utile suffix memory navigating environment 
deeper branches tree correspond longer detailed memories 
usm created memory needed solve task hand 
disambiguate state critical path task deeper branches needed disambiguate state sides environment created needed solve task memory created states aliased 
performance chrisman spaceship docking task perceptual distinctions approach chrisman demonstrated spaceship docking application hidden state 
task difficult noisy sensors unreliable actions 
sensors returned incorrect values percent time 
various actions failed percent time failed resulted random states 
see chrisman full description 
nearest sequence memory mccallum tested environment 
graph comparing performance learning algorithms shown 
usm temporal discount factor fl constant exploration probability fringe depth 
sum discounted reward plotted vertical axis number steps taken learning horizontal axis 
perceptual distinctions approach takes steps learn task order maintain resolution graph steps shown 
nearest sequence memory learns task order magnitude time steps 
utile suffix memory learns task approximately half time taken nearest sequence memory steps 
reasons better performance discussed briefly section fully mccallum utile suffix memory require computation time 
difficult compare time complexity algorithms directly different underlying constants analysis may give insights 
perceptual distinctions approach takes time jsj step propagate forward state utility steps performance training utile suffix memory nearest sequence memory perceptual distinctions approach performance learning chrisman spaceship docking task 
plot shows median runs 
tion probabilities plus calculate values addition steps agent performs calculation requires time space jsj performs jsj jaj chi square tests chrisman experiments 
nearest sequence memory requires time step number instances chain experiments limited 
usm takes time jsj jaj step agent uses full dynamic programming take prioritized sweeping plus time tests number fringe nodes single leaf node 
important feature usm concerned computation time number means gracefully reduce computation time exchange increased learning steps required 
prioritized sweeping occasional kolmogorov smirnov tests mentioned previously examples 
inefficient implementation utile suffix memory caching pr js values dynamic programming fair amount extraneous full dynamic programming tests step learns spaceship docking task minute user cpu time minutes wall clock time sgi workstation 
whitehead blocks world revisited agent task pick place red blue blocks order uncover lift green block 
agent available actions consist overt actions picking putting blocks perceptual actions form visual routines ullman agre chapman directing visual attention selectively 
visual routine manipulated region attention called marker 
agent perception consists bit hand table observation gce attention rue ruf rce rcf gue gce gcf buf bce bcf lr pd lb pd observation time ls ls action time top hand eye coordination task blocks visual routines 
agent uncover green block pick 
bottom suffix tree learned usm blocks task 
red green blue uncovered covered hand empty hand full lr look red lg look green lb look blue lt look table ls look top stack pu pick pd put 
indicates hand empty full plus bits describe features object marked 
top depicts task environment 
solution task consists action sequences look green look top stack pick look table putdown look green pick 
agent marker suffer perceptual aliasing states critical path 
state looking top stack state just placed block table perceptually identical 
agent look table second agent look back green block 
whitehead lion algorithm handle hidden state avoiding aliased states way solve task passing aliased state whitehead added second marker agent perception agent perception path start state state consisting solely states 
result additional marker agent doubled number perceptual bits increasing size perceptual state space exponentially number additional bits 
utile suffix memory allows take different approach 
need explode state space size task extra information needed step 
start smaller state space marker usm augment state space short term memory part state space needed 
result smaller state space obvious advantages overcoming ubiquitous problems curse dimensionality 
furthermore avoid need know ahead time markers required perform task hidden state calculation requires knowing precise sequence actions agent solve task 
usm tested marker blocks task whitehead learning watching technique teacher intervention probability 
teacher serves bias exploration tell agent distinctions 
extra bias task usm trouble finding sequences lead reward 
whitehead lion algorithm may done better teacher state aliased visited strict schedule perceptual overt actions perceptual actions overt action usm distinguish overt perceptual actions 
mentioned section finding smarter exploration hidden state remains area requiring 
usm temporal discount factor fl constant exploration probability fringe depth 
tree solves task built usm trials shown bottom 
related utile suffix memory inherits technique desired features udm nsm ideas come combination algorithms 
ideas algorithms particular inspired workings usm probabilistic suffix tree learning ron parti game moore algorithm chapman kaelbling variable resolution dynamic programming moore algorithms trees represent distinctions grow trees order learn finer distinctions 
probabilistic suffix tree 
usm common probabilistic suffix tree learning algorithm 
usm borrows directly tree represent variable amounts memory fringe test statistically significant predictions 
common base usm adds features specific reinforcement learning 
add notion actions layers tree alternately add distinctions previous actions observations 
add reward model set statistical test add distinctions discounted reward observations 
add dynamic programming order implement value iteration 
note training sequences usm ahead time usm generate training sequences line current suffix tree 
parti game 
key feature usm inherits instance foundation 
algorithms take advantage memorized instances order speed learning 
usm parti game generate instances line 
parti game differs usm ways 
parti game agent available greedy controller moves agent global goal state new distinctions added greedy controller fails 
usm agent greedy controller works traditional local rewards environment new distinctions added statistics indicate doing help predict return 
usm handles noisy rewards actions parti game requires deterministic environments 
algorithm 
algorithm usm inherits robust statistical test reward values parti game usm algorithm handle noise 
usm parti game algorithm instance 
results significant inefficiency distinction added time algorithm splits state algorithm forced reset child state values zero 
algorithm raw record previous experience know distribute experience parent state new children 
time state split throws away accumulated experience region effectively re learn region state space scratch 
variable resolution dynamic programming 
parti game algorithm variable resolution dynamic programming usm dynamic programming directly learned model relevant parts environment 
advantages separating model learning requires exploration non learning dynamic programming requires computation 
instance scheme reward easily propagated crucial regions state space difficult visit multiple times 
learning algorithms reward propagation requires multiple runs propagation path 
path play back schemes experience replay lin agent visit reward state multiple times order propagate reward multiple different paths may available 
addition usm reinforcement learning algorithms take advantage model approach sutton barto moore atkeson disadvantage partitions state space high resolution agent visits potentially making irrelevant distinctions 
usm hand creates utile distinctions 
discussion utile suffix memory successfully combines instancebased learning statistical techniques robust noise dynamic learned model environment variable length short memory overcoming hidden state utile memory distinctions 
algorithm fit disparate technical ideas smoothly preliminary experimental results look promising 
interesting possibilities development tree hold perceptual distinctions history distinctions forming framework generalized utile distinctions 
fringe branches test sophisticated distinctions complex tests supplied agent designer providing straightforward means domain knowledge agent 
modify tree utile distinctions continuous perception action spaces 
handle noisy perception better explicitly representing techniques probabilistic decision trees better applying lessons usm learning partially observable markov decision processes 
detailed explanation discussion ideas see mccallum acknowledgments benefited discussions colleagues including dana ballard andrew moore jonas karlsson 
material nsf 
iri nih phs rr 
agre chapman philip agre david chapman 
pengi implementation theory activity 
aaai pages 
barto barto bradtke singh 
real time learning control asynchronous dynamic programming 
technical report university massachusetts amherst ma 
bellman bellman 
dynamic programming 
princeton university press princeton nj 
bertsekas shreve dimitri 
bertsekas steven shreve 
stochastic optimal control 
academic press 
cassandra anthony cassandra leslie pack kaelbling michael littman 
acting optimally partially observable stochastic domains 
proceedings twelfth national conferenceon artificial intelligence seattle wa 
chapman kaelbling david chapman leslie pack kaelbling 
learning delayed reinforcement complex domain 
twelfth international joint conference artificial intelligence 
chrisman chrisman 
reinforcement learning perceptual aliasing perceptual distinctions approach 
tenth national conference ai 
jaakkola tommi jaakkola satinder pal singh michael jordan 
reinforcement learning algorithm partially observable markov decision problems 
advances neural information processing systems nips 
morgan kaufmann 
lin long ji lin 
programming robots reinforcement learning teaching 
ninth national conference artificial intelligence 
lin long ji lin 
robots neural networks 
phd thesis carnegie mellon school computer science january 
littman michael littman 
memoryless policies theoretical limitations practical results 
proceedings third international conference simulation adaptive behavior animals animats 
mccallum andrew mccallum 
overcoming incomplete perception utile distinction memory 
proceedings tenth international machine learning conference 
morgan kaufmann publishers 
mccallum andrew mccallum 
utile suffix memory reinforcement learning hidden state 
technical report university rochester computer science dept december 
mccallum andrew mccallum 
instance state identification reinforcement learning 
advances neural information processing systems nips 
moore atkeson andrew moore christopher atkeson 
memory reinforcement learning efficient computation prioritized sweeping 
advances neural information processing systems nips 
morgan kaufmann publishers 
moore andrew moore 
variable resolution dynamic programming efficiently learning action maps multivariate real valued state spaces 
proceedings eighth international workshop machine learning pages 
moore andrew moore 
parti game algorithm variable resolution reinforcement learning multidimensional state spaces 
advances neural information processing systems nips pages 
morgan kaufmann 
peng williams jing peng williams 
efficient learning planning dyna framework 
proceedings second international conference simulation adaptive behavior animals animats 
kerry 
finite memory estimation control finite probabilistic systems 
phd thesis department electrical engineering computer science mit january 
ron dana ron yoram singer naftali tishby 
learning probabilistic automata variable memory length 
proceedings computational learning theory 
morgan kaufmann publishers 
sutton richard sutton 
integrated architectures learning planning reacting approximating dynamic programming 
proceedings seventh international conference machine learning june 
teller astro teller 
evolution mental models 
kim kinnear editor genetic programming chapter 
mit press 
thrun sebastian thrun 
efficient exploration reinforcement learning 
technical report cmu cs cmu comp 
sci 
dept january 
ullman shimon ullman 
visual routines 
cognition 
watkins chris watkins 
learning delayed rewards 
phd thesis cambridge university 
white scherer chelsea white william scherer 
finite memory suboptimal design partially observed markov decision processes 
operations research 
whitehead ballard steven whitehead dana ballard 
learning perceive act trial error 
machine learning 
