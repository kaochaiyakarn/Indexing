supervised learning incomplete data em approach zoubin ghahramani michael jordan department brain cognitive sciences massachusetts institute technology cambridge ma cowan tesauro alspector 
eds 
advances neural information processing systems 
morgan kaufmann publishers san francisco ca 
email zoubin psyche mit edu 
real world learning tasks may involve high dimensional data sets arbitrary patterns missing data 
framework maximum likelihood density estimation learning data sets 
mixture models density estimates distinct appeals expectationmaximization em principle dempster deriving learning algorithm em estimation mixture components coping missing data 
resulting algorithm applicable wide range supervised unsupervised learning problems 
results classification benchmark iris data set 
adaptive systems generally operate environments fraught imperfections cope imperfections learn extract relevant information needed particular goals 
form imperfection incompleteness sensing information 
incompleteness arise data generation process intrinsically failures system sensors 
example object recognition system able learn classify images occlusions robotic controller able integrate multiple sensors fraction may operate time 
framework derived parametric statistics learn ing data sets arbitrary patterns incompleteness 
learning framework classical estimation problem requiring explicit probabilistic model algorithm estimating parameters model 
possible disadvantage parametric methods lack flexibility compared nonparametric methods 
problem largely circumvented mixture models mclachlan basford 
mixture models combine flexibility nonparametric methods certain analytic advantages parametric methods 
mixture models utilized supervised learning problems form mixtures experts architecture jacobs jordan jacobs 
architecture parametric regression model modular structure similar nonparametric decision tree adaptive spline models breiman friedman 
approach differs regression approaches goal learning estimate density data 
distinction input output variables joint density estimated estimate form input output map 
similar approaches discussed specht tresp 

estimate vector function joint density estimated particular input conditional density yjx formed 
obtain single estimate full conditional density evaluate yjx expectation density approach learning exploited ways 
having estimate joint density allows representation relation variables 
estimate inverse gamma relation subsets elements concatenated vector 
second density approach applicable supervised learning unsupervised learning exactly way 
distinction supervised unsupervised learning framework portion data vector denoted input portion target 
third discuss density approach deals naturally incomplete data missing values data set 
problem estimating mixture densities viewed missing data problem labels component densities missing expectation maximization em algorithm dempster developed handle kinds missing data 
density estimation em section outlines basic learning algorithm finding maximum likelihood parameters mixture model dempster duda hart nowlan 
assume data fx xn generated independently mixture density 

component mixture denoted parametrized equation independence assumption see log likelihood parameters data set jx log 

maximum likelihood principle best model data parameters maximize jx 
function easily maximized numerically involves log sum 
intuitively credit assignment problem clear component mixture generated data point parameters adjust fit data point 
em algorithm mixture models iterative method solving credit assignment problem 
intuition access hidden random variable indicated data point generated component maximization problem decouple set simple 
indicator variable complete data log likelihood function written jx ij log jz involve log summation 
unknown utilized directly expectation denoted 
shown dempster jx maximized iterating steps step jx jx step arg max expectation step computes expected complete data log likelihood maximization step finds parameters maximize likelihood 
steps form basis em algorithm sections outline real discrete density estimation 
real valued data mixture gaussians real valued data modeled mixture gaussians 
model step simplifies computing ij ij jx probability gaussian defined parameters estimated time step generated data point ij sigma gamma expf gamma gamma sigma gamma gamma sigma gamma expf gamma gamma sigma gamma gamma step re estimates means covariances gaussians data set weighted ij ij ij sigma ij gamma gamma ij derivation assumes equal priors gaussians priors viewed mixing parameters learned maximization step 
discrete valued data mixture dimensional binary data xd modeled mixture bernoulli densities 
xj 
xd jd gamma jd gammax model step involves computing ij id jd gamma jd gammax id id ld gamma ld gammax id step re estimates parameters ij ij generally discrete categorical data modeled generated mixture multinomial densities similar derivations learning algorithm applied 
extension data mixed real binary categorical dimensions readily derived assuming joint density mixed components types 
learning incomplete data previous section aspect em algorithm learning mixture models 
important application em learning data sets missing values little rubin dempster 
application pursued statistics literature non mixture density estimation problems combine application em learning mixture parameters 
assume data set fx xn divided observed component missing component similarly data vector divided data vector different missing components denoted superscript simplified notation sake clarity 
handle missing data rewrite em algorithm follows step jx jx step arg max comparing equation see aside indicator variables added second form incomplete data corresponding missing values data set 
step algorithm estimates forms missing information essence uses current estimate data density complete missing values 
real valued data mixture gaussians start writing log likelihood complete data jx ij log jz ij log ignore second term estimating parameters jz 
equation mixture gaussians note indicator variables missing step reduced estimating ij jx 
case interested types missing data expand equation superscripts denote subvectors submatrices parameters matching missing observed components data jx ij log log sigma gamma gamma sigma gamma oo gamma gamma gamma sigma gamma om gamma gamma gamma sigma gamma mm gamma note expectation sufficient statistics parameters involve unknown terms ij ij ij compute ij jx ij jx ij jx intuitive approach dealing missing data current estimate data density compute expectation missing data step complete data expectations completed data reestimate parameters step 
intuition fails dealing single dimensional gaussian expectation missing data lies line biases estimate covariance 
hand approach arising application em algorithm specifies current density estimate compute expectation incomplete terms appear likelihood maximization 
mixture gaussians incomplete terms involve interactions indicator variable ij second moments simply computing expectation missing data model substituting values step sufficient guarantee increase likelihood parameters 
terms computed follows ij jx ij probability defined measured observed dimensions ij jx ij jz ij ij sigma mo sigma oo gamma gamma defining ij jz ij regression gaussian ij jx ij sigma mm gamma sigma mo sigma oo gamma sigma mo ij mt ij step uses expectations substituted equations re estimate means covariances 
re estimate mean vector substitute values jz ij missing components equation re estimate covariance matrix substitute values jz ij outer product matrices involving missing components equation discrete valued data mixture bernoulli mixture sufficient statistics step involve incomplete terms ij jx ij jx 
equal ij calculated observed subvector second assume class individual dimensions bernoulli variable independent simply ij step uses expectations substituted equation 
supervised learning vector data set composed input subvector target output subvector learning joint density input target form supervised learning 
supervised learning generally wish predict output variables input variables 
section outline achieved estimated density 
function approximation real valued function approximation assumed density estimated mixture gaussians 
input vector extract relevant information density jx 
single gaussian conditional density normal mixture gaussians jx 
principle conditional density final output density estimator 
particular input network returns complete conditional density output 
applications require single estimate output note ways obtain estimates squares estimate lse takes jx stochastic sampling stoch samples distribution jx single component lse takes jx arg max jx 
input picks gaussian highest posterior approximates output lse estimator gaussian 
conditional expectation lse estimator gaussian mixture ij sigma oi sigma ii gamma gamma ij convex sum linear approximations weights ij vary nonlinearly equation input space 
lse estimator gaussian mixture interesting relations algorithms cart breiman mars friedman mixtures experts jacobs jordan jacobs mixture gaussians competitively partitions input space learns linear regression surface partition 
similarity noted tresp 
stochastic estimator stoch single component estimator better suited squares method learning non convex inverse maps mean solutions inverse solution 
classification iris data set 
data points training testing 
data point consisted real valued attributes class labels 
shows classification performance sigma standard error function proportion missing features em algorithm mean imputation mi common heuristic missing values replaced unconditional means 
classification missing inputs missing features em correct classification mi estimators take advantage explicit representation input output density selecting solutions inverse 
classification classification problems involve learning mapping input space set discrete class labels 
density estimation framework lends solving classification problems estimating joint density input class label mixture model 
example inputs realvalued attributes class labels mixture model gaussian multinomial components dj 
jd sigma expf gamma gamma sigma gamma gamma denoting joint probability data point belongs class jd parameters multinomial 
density estimated maximum likelihood label particular input may obtained computing 
similarly class conditional densities derived evaluating xjc 
classes way yields class conditional densities turn mixtures gaussians 
shows performance em algorithm example classification problem varying proportions missing features 
applied algorithms problems clustering dimensional greyscale images approximating kinematics joint planar arm incomplete data 
discussion density estimation high dimensions generally considered difficult requiring parameters function approximation 
density approach learning advantages 
permits ready incorporation results statistical literature missing data yield flexible supervised unsupervised learning architectures 
achieved combining branches application em algorithm yielding set learning rules mixtures incomplete sampling 
second estimating density explicitly enables represent relation variables 
density estimation fundamentally general function approximation generality needed large class learning problems arising inverting causal systems ghahramani 
problems solved easily traditional function approximation techniques data generated noisy samples function relation 
titterington david cohn helpful comments 
project supported part mcdonnell pew foundation atr auditory visual perception research laboratories siemens national science foundation office naval research 
iris data set obtained uci repository machine learning databases 
breiman friedman olshen stone 

classification regression trees 
wadsworth international group belmont ca 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
royal statistical society series 
duda hart 

pattern classification scene analysis 
wiley new york 
friedman 

multivariate adaptive regression splines 
annals statistics 
ghahramani 

solving inverse problems em approach density estimation 
proceedings connectionist models summer school 
erlbaum hillsdale nj 
jacobs jordan nowlan hinton 

adaptive mixture local experts 
neural computation 
jordan jacobs 

hierarchical mixtures experts em algorithm 
neural computation 
little rubin 

statistical analysis missing data 
wiley new york 
mclachlan basford 

mixture models inference applications clustering 
marcel dekker 
nowlan 

soft competitive adaptation neural network learning algorithms fitting statistical mixtures 
cmu cs school computer science carnegie mellon university pittsburgh pa specht 

general regression neural network 
ieee trans 
neural networks 
tresp ahmad 

network structuring training rule knowledge 
hanson cowan giles editors advances neural information processing systems 
morgan kaufman publishers san mateo ca 
