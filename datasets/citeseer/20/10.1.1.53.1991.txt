survey continuous time computation theory department mathematics university motivated partly resurgence neural computation research partly advances device technology increase interest analog continuous time computation 
special case algorithms devices developed relatively little exists general theory continuous time models computation 
survey existing models results area point open research questions 
long period interest analog computation rise 
immediate cause new wave activity surely success neural networks revolution provided hardware designers new numerically computationally interesting models structurally sufficiently simple implemented directly silicon 
designs actual implementations neural models vlsi see 
fundamental explanation development hardware technology advanced generally easier experiment manufacture individualized special purpose computational devices opposed mass production general purpose processors memory chips 
trend continue noticeable making manners special purpose computational models practically theoretically interesting objects study 
increasing need theoretical understanding capabilities limitations effectiveness various kinds special purpose computational models challenge directed right heart computational complexity research 
concentrates theoretical issues analog computing similar situation exists foreseen cellular automata complex systems models computation molecular computing optical computing somewhat quantum computation 
analog computation theory goes back claude shannon papers early literature extensive answer questions appear interesting day perspective 
instance years quite number papers appeared computational aspects discrete time analog models number papers significant continuous time models far fewer 
box fin finland 
mail math fi 
amount practically implementable models close nil 
particular papers mentioned ignore effects computing process imprecision noise pervasive practical problems analog computation 
exists continuous time systems practically computational complexity aspects basic notions computation time input size system size waiting proper reasonably general implementation independent definitions 
survey existing research general computability complexity aspects continuous time computation models 
section outline unconstrained models 
staying level individual concretely specified models 
particular important collection results cover properly belong context pour el richards general computability theory common analytical physical operators 
section move models arisen real idealized physical implementations mechanical electrical differential analyzers electronically implemented neural networks 
section discuss existing papers computational complexity issues 
conclude section listing main open research directions 
unconstrained models continuous time analog system generally mean dimensional system autonomous ordinary differential equations ode form dx dt field defining system 
field sufficiently smooth continuously differentiable system determines unique flow function oe oe dt oe oe precise existence flow may guaranteed contained interval 
different ways defining notion computation context shall discuss 
means obvious inputs system read outputs discrete time analog system defined similarly iterated map form 
pointing sufficiently regular discrete time analog system theoretically embedded poincar section snapshot sequence higher dimensional continuous time analog system 
specifically system representable diffeomorphic map interval embedded section smooth flow dimensional manifold 
instance turing machine simulations iterated piecewise linear maps moore koiran principle extended smooth locally dimensional systems 
technically needs require turing machines simulated invertible known turing machine converted invertible 
moore fact discusses issue continuous time embedding length presents quasi physical billiard ball model implementing resulting continuous time system 
survey shall concentrate explicitly continuous time systems discuss discrete time systems 
mention passing simulation turing machines dimensional continuous time systems piecewise constant derivatives asarin maler 
technical point worth noting general proof strategy asarin maler authors turing machine simulations dynamical systems implicitly 
takes starting point standard correspondence turing machines stack pushdown automata 
turing machine tape represented opposing stacks contents stacks encoded manner real numbers leading representation system state point usually constrained 
needs additional dimension connect states way corresponds turing machine transition function 
fact asarin maler koiran prove class systems consider dimensions sufficient continuous time simulation 
basic idea stack machine simulation starting point straightforward robust quite elegant simulation turing machines continuous time systems continuous vector fields branicky 
technique obtain simulation lipschitz continuous systems non lipschitz systems briefly outline branicky construction turing machine tape symbols states represents instantaneous configuration pair integers xr encoding tape contents left right tape head respectively adic encoding 
current state encoded say lowest order digit xr xr mod 
transition function determines finite gain discrete time mapping xl xr finite gain mean constant branicky reason integer real number encodings gives system degree robustness small perturbations 
attempt continous time simulation discrete time system define system state variables xl xr system equations dt gammax xr dt gammax xr approach updating state variables variables xl xr maintain old values new values computed 
branicky solves dilemma defining phase continuous time system introducing extra pair state variables xl xr explicit time variable time variable defines periodic clock functions gamma values oscillate alternatingly 
state variables coupled clock high new values xr xl xr computed gamma clock high new values simply copied xr xr 
formally ignoring technical details continuous time system constructed follows 
defines clock functions sigma sigma sin sigma gamma connects state variables roughly follows dt gamma xl xr delta dt gamma xr xr delta dt gammax xl delta gamma dt gammax xr delta gamma dt constant aspect construction ignoring time run slowly choose appropriate constant state variables complete updates clock period 
refer reader original papers details 
branicky credits brockett introducing phase trick specialized context 
construction technically achieves purpose conceptually somewhat unsatisfactory basically analog system 
fundamental level digital computers clocked analog systems potentially infinite number state variables 
interest understand computational capabilities continuous time systems clocks notion may difficult precise 
possible restrictive condition require system possess function value bounded decreases time system trajectory 
trick go dimensional turing machine simulation dimensional simulation simply encode configuration pair xr single integer xl continue 
unfortunately prime power encoding destroys finite gain property discrete time transition mapping consequently resulting continuous time system non 
open question turing machines simulated dimensions robust lipschitz continuous systems 
continuous embeddings piecewise linear maps ought satisfy lipschitz condition systems sensitive arbitrarily small perturbations 
computability partial differential equations studied omohundro 
arbitrary dimensional cellular automaton moore neighbor neighborhood omohundro constructs system coupled nonlinear pde space variables time variable simulating dimensional cellular automata simulate turing machines omohundro construction establishes continuous time pde universal computational systems 
simulation digital idea evolve localized bumps xy space height indicates state simulated automaton cell position 
hand extended analog computer eac interested production real functions solutions systems differential equations set certain systematic quasi effective manner 
starts general purpose analog computer systems defined shannon pour el see extends capability solve boundary value problems systems pde defined lower order versions eac 
proves eac quite powerful generation model able produce functions model examples see 
constrained models move analog computation models correspond idealized versions existing devices course mean arbitrary computations models precisely implemented real physical hardware 
earliest theoretical study computational capabilities analog devices shannon generative power bush differential analyzer 
differential analyzers built mechanically rotating gears shafts connecting electronically resistors capacitors 
bush original machine electro mechanical electronic differential analyzers developed world war ii initially fire control purposes widely engineering 
shall discuss briefly electronic version device details refer reader large literature solution engineering problems differential analyzers textbooks 
electronic differential analyzers constructed interconnecting resistors capacitors high gain operational amplifiers systematic manner 
recall timevarying input voltage resistor resistance creates passes current capacitor capacitance creates current du dt operational amplifiers act simple large constant voltage multipliers 
particular voltage differences capacitors correspond integrals currents help resistors amplifiers perform integration input voltages 
illustrates design integrator input voltage gives response gamma rc dt assuming gain amplifier large 
abstractly electronic differential analyzer viewed consisting kinds computational devices interconnected possibly cyclic network 
integrator 
input output device producing input functions output function dv constant value depends initial settings device 

constant multiplier 
input output device producing input output cu arbitrarily chosen real constant 
fi fi fi fi fi fi electronic integrator 

adder 
input output device producing inputs output 

variable multiplier 
input output device producing inputs output delta 

constant function 
input output device producing input output 
devices variable multiplier fact redundant implemented delta dv du pour el shannon context mechanical differential analyzer observes real function generated input interval network devices types system ode form du dt ijk du dt initial conditions system unique solution interval 
addition requires system possess domain generation meaning sequence initial values sufficiently close gives rise locally unique solution show class functions fact coincides class differentially algebraic functions satisfy algebraic differential equation form nonzero polynomial variables 
result claimed shannon pour el argues shannon proof seriously incomplete claim discovered gap pour el reasoning 
corollary characterization functions interesting functions known differentially algebraic shown differential analyzer type analog computers 
include gamma function gamma gamma gammat dt riemann zeta function functions generated eac model discussed 
fi fi fi fi fi fi fi fi fi fi fi sigma sigma oe ae ij ik oe gammaoe hopfield electronic neuron 
different approach continuous time computation taken electronically neural network model proposed hopfield 
basic computational unit electronic neuron schematically shown 
hopfield type neurons constructed resistors capacitors amplifiers time amplifiers assumed saturating response characteristic oe oe ff tanh fi 
continuous time hopfield network consists finite number interconnected units type 
consider behavior neuron network neurons 
ae input resistance capacitance respectively amplifier neuron denote input voltage amplifier output voltage order establish inhibitory interconnections neurons inverted output voltages gammav needed 
simplicity indicate voltages relative level neuron shown draws input neurons indicated resistors resistances denoted ij ik voltages sigma sigma obtained appropriate output terminals neurons depending interconnections excitatory inhibitory 
circuit equations network neurons written du dt ae ij sigma gamma choosing circuit parameters appropriately normalizing rc constants network implement system order nonlinear differential equations form du dt gammau ij oe essentially chooses ij ij normalizes details see 
hopfield proved function argument interconnections neurons symmetric ij ji pair system globally asymptotically stable initial voltage state network relaxes stable equilibrium state 
view network performing input output mapping initial states respective equilibrium states 
note point view completely ignores time evolution system central topic interest study differential analyzers 
convergence behavior particular type function proof hopfield proposed various special case uses networks associative memory combinatorial optimization applications 
general computational power hopfield network model studied shown arbitrary polynomially space bounded turing machines simulated polynomial size networks piecewise linear amplifier response function oe gamma gamma gamma networks constructed asymmetric computational power polynomial size symmetric networks remains open question 
hand shown corresponding discrete time model asymmetric symmetric networks computationally equivalent 
simulation uses similar trick branicky construction way somewhat unsatisfactory 
result lower bound computational power possibility remains polynomial size hopfield networks powerful polynomial space turing machines conceivably finite networks universal power case models discussed section 
computational complexity little done potentially fruitful field computational complexity analysis continuous time systems 
basic definitions fixed universally manner 
apparently published area authors study possibility type systems cf 
equation generally lipschitz continuous systems ode solving combinatorial problems faster possible digital means 
standard numerical integration argument come analog computation simulated integrated interval arbitrary precision digital computer number steps polynomial maximum magnitude second derivative simulated system 
intended implication analog systems efficient digital computers solving limited precision problems 
looking carefully argument notices number steps digital simulation fact exponential length analog time interval assumed predetermined proof 
course analog computation artificially sped occur time interval maximum second derivative system interval increases 
length interval appear parameter result argument inconclusive 
promising approach defining general notion analog computation time suggested discussions author 
assume system field dx dt relaxes initial state stable equilibrium state define computation time input precision smallest ffl ffl gamma 
obtain natural time scale linearizes field stable state obtain stability matrix close dy dt gamma 
eigenvalues negative real parts gamma largest state system approaches locally jx gamma gammat 
eigenvalues nonpositive real parts system stable increase state gets closer factor natural choose local unit time 
obviously approach preliminary time scales obtained valid locally vicinity individual stable state argue simple systems usually converge quickly neighborhoods clear define time scales systems stability matrix eigenvalues vanish 
question define notion input size model 
authors propose considering computation relative grid defining input size log largest computation performed correctly input output observed precision 
open problems surveyed far sparsely researched field continuoustime computation theory 
apparent interesting research problems open cases proper definitions established 
significant unexplored area surely computational complexity theory continuoustime systems find correct definitions basic notions computation time input size develop techniques global analysis interesting concrete systems spirit traditional discrete algorithm analysis 
initial step look local analysis hopfield associative memory 
parallel develop appropriate notions complexity classes reductions hard problems continuous time computation 
interesting specialized problems remain open 
section surveyed finite dimensional systems capable simulating turing machines 
simulations little concern implementability 
implementation finite dimensional systems finite hopfield networks computationally universal 
turing machine simulations unsatisfactory explicitly constructed system clock 
computational power continuous time systems clock systems possess functions interesting concrete example class hopfield networks symmetric neuron interconnections 
adleman molecular computation solutions combinatorial problems 
science nov 
anderson rosenfeld eds neurocomputing foundations research 
mit press cambridge ma 
asarin maler relations dynamical systems transition systems 
proc 
st internat 
colloq 
automata languages programming 
lecture notes computer science springer verlag berlin 
bennett logical reversibility computation 
ibm res 
develop 

bennett time space trade offs reversible computation 
siam comput 

bernstein vazirani quantum complexity theory 
proc 
th acm symp 
theory computation 
acm press new york ny 
blum shub smale theory computation real numbers npcompleteness recursive functions universal machines 
bulletin amer 
math 
soc 

branicky analog computation continuous odes 
proc 
workshop physics computation 
ieee computer society press los alamitos ca 
branicky universal computation capabilities hybrid continuous dynamical systems 
theoret 
comput 
sci 

brockett smooth dynamical systems realize arithmetical logical operations 
decades mathematical system theory schumacher eds 
lecture notes control information sciences springer verlag berlin 
brockett dynamical systems sort lists diagonalize matrices solve linear programming problems 
linear algebra applications 
bush differential analyzer new machine solving differential equations 
franklin inst 

casey dynamics discrete time computation application recurrent neural networks finite state machine extraction 
neural computation 
cichocki unbehauen neural networks optimization signal processing 
wiley teubner stuttgart 
hopfield tank neural computation decisions optimization problems 
biological cybernetics 
feitelson optical computing survey computer scientists 
mit press cambridge ma 
garzon models massive parallelism analysis cellular automata neural networks 
springer verlag berlin 
analog analog hybrid computer programming 
prentice hall englewood cliffs nj 
hirsch smale differential equations dynamical systems linear algebra 
academic press san diego ca 
hopfield neurons graded response collective computational properties state neurons 
proc 
nat 
acad 
sci 
usa 
reprinted pp 

hopfield tank neural computation decisions optimization problems 
biological cybernetics 
jackson analog computation 
mcgraw hill new york ny 
johnson analog computer techniques nd ed 
mcgraw hill new york ny 
koiran dynamics discrete time continuous state hopfield networks 
neural computation 
koiran cosnard garzon computability low dimensional dynamical systems 
theoret 
comput 
sci 

korn korn electronic analog hybrid computers rd ed 
mcgraw hill new york ny 
differentially algebraic replacement theorem analog computability 
proc 
amer 
math 
soc 

lipton dna solution hard computational problems 
science apr 
maass effect analog noise discrete time analog computation 
proc 
neural information processing systems appear 
mead analog vlsi neural systems 
addison wesley reading ma 
moore unpredictability undecidability physical systems 
phys 
review letters 
moore generalized shifts unpredictability undecidability dynamical systems 
nonlinearity 
omohundro modelling cellular automata partial differential equations 
physica 
computational power continuous time neural networks 
project neurocolt report nc tr royal holloway college univ london dept computer science 
pp 
computational power discrete hopfield nets hidden units 
neural computation 
universal computation finite dimensional coupled map lattices 
proc 
workshop physics computation appear 
jr de melo geometric theory dynamical systems 
springer verlag new york ny 
pour el computability relation general purpose analog computer connections logic differential equations analog computers 
trans 
amer 
math 
soc 

pour el richards computability analysis physics 
springer verlag berlin 
ak complexity theory genetics 
proc 
th ann 
ieee conf 
structure complexity theory 
ieee computer society press los alamitos ca 
reif tygar yoshida computability complexity optical beam tracing 
proc 
st ann 
ieee symp 
foundations computer science 
ieee computer society press los alamitos ca 
wagner power bio computers 
technical report universitat inst 
fur informatik feb 
mathematical limitations general purpose analog computer 
adv 
appl 
math 

extended analog computer 
adv 
appl 
math 

anchez lau artificial neural networks paradigms applications hardware implementations 
ieee press new york 
shannon mathematical theory differential analyzer 
math 
phys 
mit 
reprinted 
shannon theory design linear differential equation machines 
report national defense research council january 
reprinted pp 

shannon collected papers sloane wyner eds 
ieee press piscataway nj 
shor algorithms quantum computation discrete logarithms factoring 
proc 
th ann 
ieee symp 
foundations computer science 
ieee computer society press los alamitos ca 
siegelmann analog computing dynamical systems 
manuscript april 
pp 
siegelmann sontag computational power neural nets 
comput 
system sciences 
steiglitz dickinson complexity analog computation 
math 
computers simulation 
