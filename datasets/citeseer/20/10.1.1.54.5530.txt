electronics institute hardware learning analogue vlsi neural networks thesis torsten lehmann partial fulfillment requirements degree doctor philosophy september technical university denmark dk lyngby denmark page ii typeset plain format ver 
ver 
tl edition copyright fl torsten lehmann rights reserved page iii english thesis concerned hardware implementation learning algorithms analogue vlsi artificial neural networks 
artificial neural networks anns successfully applied problems algorithmic solution exist described examples 
anns fault tolerant parallel nature analogue vlsi technology efficiently exploit properties providing high performance systems 
analogue vlsi implementations recall mode anns maturing equally important problem implementing programming learning hardware infancy 
shall analogue vlsi implementation supervised gradient descent learning algorithms anns error back propagation learning algorithm bpl layered feed forward anns real time recurrent learning algorithm rtrl general recurrent networks 
algorithms teach analogue vlsi chip set anns shall describe 
chip set simple capacitive weight storage digital ram back memory 
bpl algorithm implemented chip novel bidirectional principle resulting modest hardware increase compared recall mode system 
rtrl algorithm implemented add hardware recall mode system compromise computational speed hardware consumption 
implementations algorithmic variations considered 
weight decay momentum 
results measurements fabricated chips measurements recall mode system 
display novel category gradient descent algorithms non linear gradient descent better suited hardware implementations ordinary gradient descent terms accuracy hardware consumption 
argue ann ensembles improve performance analogue neural systems 
included novel considerations analogue computing accuracy offset compensation derivative computation analogue memories network topologies process parameter dependency canceling learning systems ram back things 
conclude technology promising implementing learning algorithms research needed algorithmic level implementation level 
page iv vi os med hardware af til vlsi 
artificial neural networks anns er med held pa der men som kan ved af 
anns er af og analog vlsi er en som kan til af med stor 
analog vlsi af fast anns er ved men det problem hardware til er sin 
vi vlsi af gradient til anns error back propagation bpl til real time recurrent learning rtrl til 
vlsi til anns som vi ogsa 
med en digital ram til 
bpl er pa nyt og pa ann ved af hardware 
rtrl med extern hardware og som og hardware 
af af ogsa fx 
og moment 
fra pa de chips som pa system pa ann 
vi den ny af gradient gradient som er til hardware gradient med til og hardware 
vi ann ensembles af 
ogsa af offset af proces parameter og med ram 
vi er af er pa niveau pa niveau 
page preface thesis partial fulfillment requirements degree doctor philosophy doctor ph 
carried electronics institute technical university denmark funded scholarship technical university denmark 
professor erik electronics institute supervisor 
tried thesis coherent presentation hardware learning analogue vlsi neural networks means exhaustive 
necessary include entirely state clearly case 
particular fellow ph student john lansner responsible large parts implementation neural networks 
thomas responsible op amps current chips 
masters student mine jesper schultz sparse input synapse chip architecture 
john hertz benny anders krogh responsible development non linear back propagation 
tried refer authors possible refered apart known matters parts 
people undoubtedly find parts text provocative 
argue refrain floating gate memories synapse strengths please take means mark peoples merely personal views deliberate attempt provoke think sound development field research 
regarding layout thesis say habit parenthesis quite 
note act subordinate clauses contents important 
equation numbers globally enumerated carry chapter labels superscripts ease location 
places peoples anchez lau incomplete sense relevant displayed cover material related text usually preceded cf se 
italics font emphasis concepts index 
personal pronoun thesis think eases reading 
thesis organized follows field analogue vlsi neural networks including hardware learning briefly introduced 
motivations research preface page vi objective thesis defined 
implementation neural network chapter neural network architecture basis rest thesis 
results chip loop training serves standard chip learning implementations 
preliminary conceptions hardware learning chapter choices learning algorithms implementation considered elaborate general considerations hardware learning 
implementation chip back propagation chapter hardware learning system described 
simple elegant idea meant just minor part thesis initial implementing rtrl 
turned lot hard involved verifying simple idea worth years 
borne fruit invitations 
second hardware learning system described implementation rtrl hardware chapter 
system architecture developed studies masters degree 
system lot hard involved implementation testing experimental system 
complete time writing 
thoughts analogue vlsi neural networks chapter collected odds ends field fit chapters ph study lot ideas come mind network architectures learning algorithms formed personal opinion matters 
thoughts mature find important propagate information scientific community order scientists benefit ideas 
chapter drawn 
section tried emphasize contributions science denoted 
appendices hold material interest reader 
enclosures hold material interest reader want carry serves documentation 
organized project oriented way considerations placed bulk text readability organized logical manner 
hope index prove adequate locating considerations 
lyngby september torsten lehmann page vii analogue integrated electronics group electronics institute valuable discussions particular erik thomas john lansner peter shah 
members connect discussions neural networks especially lars kai hansen anders krogh 
ole hansen ready answer helping hand 
mogens petersen electronics institute layout soldering numerous pcbs 
dtu staff questions pushed deadlines chip manufacturing 
peter john lansner lars kai hansen thomas valuable criticism thesis 
due danish technical research council danish natural science council analog devices denmark financial support 
page viii contents iii preface vii contents viii abbreviations xii symbols xiv list figures xvii chapter implementing anns analogue hardware implementing learning algorithms analogue hardware chapter implementation neural network artificial neural network model neurons network mapping algorithm vlsi architecture signalling memories multipliers activation functions chip design neuron chip synapse chip sparse input synapse chip chip measurements neuron chip synapse chips chip compound system design system measurements contents page ix process parameter dependency canceling temperature compensation improvements summary chapter preliminary conceptions hardware learning hardware consumption choice learning algorithms gradient descent learning error back propagation real time recurrent learning hardware considerations chapter implementation chip back propagation back propagation algorithm basics variations mapping algorithm vlsi hardware efficient approach chip design synapse chip neuron chip chip measurements synapse chip neuron chip improving derivative computation system design asic interconnection weight updating hardware non linear back propagation derivation algorithm hardware implementation chopper stabilizing including algorithmic improvements improvements summary chapter implementation rtrl hardware rtrl algorithm basics variations mapping algorithm vlsi system simulations chip design width data path module signal slice auto offset compensation contents page chip measurements system design asic interconnection width data path module interface algorithm variations non linear rtrl derivation algorithm hardware implementation continuous time rtrl system improvements summary chapter thoughts analogue vlsi neural networks gradient descent learning 
neuron clustering self refreshing system neural network ensembles hard soft hybrid synapses chapter bibliography index appendix definitions appendix artificial neural networks ann model applications motivations teaching anns gradient descent algorithms performance evaluation appendix integrated circuit issues mos transistors bipolar transistors analogue computing accuracy integrated circuit layout appendix system design aspects scalable ann chip set synapse chips chip set improvements chip back propagation chip set back propagation synapse chips back propagation neuron chips scaled back propagation synapse chips contents page xi back propagation chip set improvements rtrl chip rtrl chip improvements rtrl back propagation system appendix building block components op amp mos resistive circuit enclosure published papers enclosure ii chip enclosure iii test pcb schematics enclosure iv rtrl back propagation system interface page xii abbreviations ac alternating current analogue digital adc analogue digital converter ann artificial neural network asic application specific integrated circuit bipolar complementary mos integrated technology bjt bipolar junction transistor bpl back propagation learning current conveyor second generation cco current controlled oscillator cmos complementary mos integrated technology cps connections second cups connection updates second digital analogue dac digital analogue converter dc direct current dna acid dsp digital signal processor eeprom electrical programmable rom flops floating point operations second fsm finite state machine gcps giga cps giga cups ipm inner product multiplier input scale isa industry standard architecture lbm lateral bipolar mode lsb significant bit mega cps mega cups multiplying dac mflops mega flops mlp multi layer perceptron mos metal oxide semiconductor mos field effect transistor abbreviations page xiii mos transistor mpc multi project chip mpw multi project wafer mrc mos resistive cell mrna messenger rna msb significant bit mvm matrix vector multiplier normalized average relative variance nlbp non linear back propagation non linear rtrl non linear synapse multiplier optimal brain damage output range pc personal computer pc ibm pc type compatible computer pcb printed circuit board pfm pulse frequency modulation pla programmable logic array power supply rejection ratio pwm pulse width modulation recurrent artificial neural network ram random access memory regulated gain rna acid rom read memory rtrl real time recurrent learning sar successive approximation register ttl transistor transistor logic uv ultra violet vlsi large scale integration wsi wafer scale integration page xiv symbols list commonly symbols appearing thesis 
standard symbols refer literature 
geiger hertz anchez lau 
index index runs possible values 
omega vector multiplication coordinates omega 
convolution operator 
superscript bit number number bits bit discretization resolution 
bandwidth 
bm memory necessary store 
ox mos gate oxide capacitance unit area 
neuron target value 
non linearity 
da accuracy 
normalized average relative variance error measure 
neuron activation function vector scalar functions 
ann weight neuron index set ann input indices 
sm maximal signal current 
ann input output index cost function instantaneous 
quadratic jq entropic je 
tot total cost function 
ann neuron index ij index rtrl chip access variable 
process parameter ox 
number layers ann 
channel length 
non unit bit absolute measures 
ann layer number indexing superscript input output deltal ofs geometric device size offset 

ann input index number inputs ann 
symbols page xv number inputs neuron number neurons ann 
na number letters input alphabet epc number epochs ann trained 
places implicitly order 
ne number networks ann ensemble 
number neurons layer ann 
ofs index offset error 
ij neuron derivative variables rtrl 
nij neuron derivative variables non linear rtrl 
res index resolution 
rd dynamic range 
neuron net input vector time 
unit 
pd propagation delay calculating 
set neuron indices target exist 
cyc learning cycle time 
epc training epoch epc training 
indicates 
set neuron indices 
vsm maximal signal voltage 
thermal voltage kt vt threshold voltage 
ij ij connection strength input neuron layer gamma neuron layer matrix columns corresponding inputs 

connection strengths arranged vector 
deltaw ij connection strength change 
deltaw min connection strength change threshold 
channel width 
xm input ann vector neuron activation vector neuron input ann input neuron activation vector ff fc bjt forward emitter collector current gain 
ff mtm learning momentum parameter 
ff nlbp domain parameter 
fi parameter fi delta ox fi activation function steepness fi fi fi 
fl derivative fahlman perturbation 
ffi weight strength error back propagation vector ffi symbols page xvi ffi nk nlbp weight strength error 
ffi rate sampled signal 
weight drift mv 
neuron activation error vector 
ffl dec weight decay parameter 
general quantity random variable free running index 
ann learning rate 
neuron specific theta neuron threshold 
weight restoration efficiency 
carrier surface mobility 
general quantity random variable free running index 
oe current mismatch standard deviation compared device 
oe ij neuron net input derivative variables rtrl 
oe clock phase 
generally rules obeyed deviations appear 
different kinds signals distinguished rules context symbol appear supply lack information 
lack consistent usable standard necessitates unfortunate definition 
electrical signals case symbol indicates dc signal ac signal 
case bias voltages quiescent currents upper case letters 
bias second small signal quantities instantaneous values lower case letters 
signal font subscript indicates subscript refer symbol italics 
subscript descriptive roman 

case descriptive index usually indicates compound abbreviation upper case 
vgs lower case 
ofs 
page xvii list figures expandable neural network expandable recurrent neural network reconfigurable neural network typical electronic synapse capacitive storage floating gate mos gilbert multiplier mos resistive circuit multiplier mrc resistive equivalent multiplying dac synapse simple non linear synapse multiplier weight output characteristic pulse frequency neuron distributed neuron hyperbolic tangent neuron inner product multiplier synapse schematic current conveyor nucleotide sequence sparse input synapse chip column measured neuron transfer function measured synapse characteristics measured synapse neuron transfer characteristics measured synapse neuron step response layer test perceptron test perceptron system architecture sunspot prediction sunspot learning error sunspot prediction error non unity current gain canceling general process parameter canceling circuit schematic back propagation synapse schematic back propagation neuron mrc operated forward mode mrc operated reverse mode list figures page xviii back propagation system second generation synapse chip second generation hyperbolic tangent neuron back propagation neuron forward mode synapse characteristics reverse mode synapse characteristics forward mode weight offsets reverse mode weight offsets forward mode neuron characteristics computed neuron derivative different neuron transfer functions different neuron non linearities different parabola transfer functions different parabola non linearities neuron sampler rate differential quotient derivative approximation back propagation ann architecture digital weight updating hardware principle nlbp training error continuous time non linear back propagation neuron discrete time non linear back propagation neuron neuron activation block schematic neuron transfer function chopper stabilized weight updating discrete time system discrete time rtrl system order signal slice current auto zeroing principle double resolution conversion sar bit slice weight change ipm element characteristics tanh derivative computing block characteristics edge sampler sampling auto zeroing simulation rtrl ann basic architecture non linear rtrl system neuron clustering self refreshing ann system general neural network model layered feed forward neural network channel mos transistor symbols channel mos transistor short channel snap back npn bipolar transistor symbol npn bipolar transistor list figures page xix lateral bipolar mode symbol lateral bipolar mode current subtraction synapse current subtraction row layout matched transistors digital level shifter synapse layout table ann chip set characteristics table row column element control back propagation synapse column row element forward mode bpl synapse row element forward mode bpl synapse column element route mode bpl synapse row element route mode bpl synapse column element reverse mode bpl synapse row element reverse mode bpl synapse column element back propagation neuron schematic back propagation weight update schematic table back propagation chip set characteristics table scaled bpl synapse chip characteristics scaled synapse chip characteristics sar start signal gating transmission gate symbol rtrl signal slice schematic rtrl weight change schematic clock generator table rtrl chip characteristics operational amplifier regulated gain current mirror current conveyor op amp frequency response typical mrc layout page chapter thesis describes analogue vlsi implementation supervised learning algorithms artificial neural networks 
error back propagation learning algorithm layered feed forward network real time recurrent learning algorithm general recurrent network 
operate discrete time analogue vlsi neural network digital random access backup memory weights 
included thesis implementation analogue vlsi neural network general conceptions hardware learning thoughts analogue vlsi neural networks 
decade field artificial neural networks anns see appendix matured 
artificial neural networks longer magic devices powerful tools right manner classification problems similar tasks algorithmic solution known 
ann foundation theoretically application level growing increasingly solid hardware implementations primarily analogue digital vlsi see appendix high performance systems begun emerge 
vlsi implementations recall mode anns maturing field ready step fully adaptive vlsi anns 
including learning 
objective thesis study analogue vlsi implementations computational neural networks emphasis learning hardware implementations 
motivations analogue vlsi implement anns learning algorithms described objective defined 
chapter page implementing anns analogue hardware analogue hardware 
artificial neural networks easily simulated standard digital von neumann computers 
general purpose computers rapidly moving imaginable part society 
subject intense research world wide competition manufactures hard 
computational performance growing exponentially time 
possibly hope compete 

niches analogue integrated neural networks advantage 
section examine 
ffl parallelism pursuit faster data processing approaches possible faster systems 
higher clock frequencies years procedure exploited utmost limit interfacing communication fast systems difficult seitz 
way speed data processing parallel data processing elements 
trivial task problems parallelized gottlieb leighton 
neural networks inherently parallel easily mapped parallel hardware 
analogue data processing elements inherently slower digital equivalents analogue versions neural network computing primitives multiplication addition smaller digital equivalents 
massively parallel neural systems efficiently implemented analogue vlsi giving potential fast data processing 
murray ismail graf jackel 
claim requires couple remarks exact mapping parallel hardware depends heavily network topology application dependent massively parallel neural systems application specific general purpose lehmann jackel mead 
analogue neural network thought accelerator von neumann computer serial computer supply data massively parallel neural network cases severely limit performance 
note high precision computations digital circuits required generally believed precision offered analogue components sufficient neural systems cf 
chapters edwards murray hollis tarassenko 
massively parallel analogue neural networks reported arima castro 
ffl neural networks asynchronous nature 
efficiently exploited 
asynchronous self timed systems number distinct advantages systems governed clock seitz sutherland murray chapter page ffi synchronous systems designed run conservative clock frequency ensure functionality worst case situations asynchronous systems run maximum speed hardware 
furthermore certain component bottleneck system replaced faster component immediate improvement performance 
ffi increasing system clock frequency communication components problem difficult distribute system clock skew large area 
asynchronous communication usually solution 
ffi synchronous systems components change states draw current power supply simultaneously clock edges 
puts heavy demands tolerable power supply peak currents capacitive decoupling heavy peak currents introduce lot noise 
asynchronous systems power averaging 
ffi real world interfacing basically asynchronous task asynchronous systems natural approach eliminates instance problems associated seitz 
spite attractive features asynchronous systems conventional digital data processing systems successfully implemented hardware overhead needed handshaking 
pure analogue systems need handshaking suited implement asynchronous systems 
systems feed back stability considered 
asynchronously operated analogue neural networks reported alspector hollis mead 
ffl fault tolerance trained ann insensitive small weight changes kj equilibrium error cost kj weight cf 

insensitive complete loss connection due short circuit ram fault radiation network simple architecture possible ensure generalization ability 
ensure fault tolerance hardware level necessary introduce redundant hardware 
favour hardware implementations neural networks particular analogue hardware cost extra synapse relative low 
context new emerging technology wafer scale integration wsi deserves mentioning fault tolerant systems crucial applicability wsi 
technology tailored implementation massively parallel neural networks 
ffl low power applications sub threshold operated offer possibility extremely low power systems 
digital systems function sub threshold analogue systems carry information wire fewer transistors operation inherently power cf 

ismail 
low power analogue neural networks reported leong chapter page jabri mead 
ffl real world interfacing asynchronous real world interfaces required analogue 
analogue neural networks obviously eliminate need converters attractive feature 
paramount importance data applied massive parallelism 
hundreds thousands high speed converters seldom justified 
low power systems real world interfaces great importance power wasted processes converting 
analogue neural networks real world interfaces reported leong jabri mead 
ffl regularity regularity artificial neural networks suited massively parallel implementations design effort put designing efficient components repeatedly interconnected regular way 
conclude niches analogue integrated neural systems exist possibly asynchronous redundant hardware ffl massively parallel application specific systems having parallel real world interface 
ffl small low power application specific systems real world interface 
predominantly interested massively parallel analogue neural networks low power ones 
today applications analogue integrated neural networks reported 
instance high energy particle detector track reconstruction devices heart leong jabri silicon retinas motion sensors mead park cao 
systems reported embrace principles general purpose analogue neural systems reported mueller van der spiegel 
systems embrace principles ismail corso eberhardt hollis lansner lehmann mead moon murray neugebauer 
chapter page implementing learning algorithms analogue hardware non ideal characteristics analogue integrated neural networks taught chip loop training castro eberhardt loading predetermined weights application individual chip system trained applying input pattern chip ii compute network error basis target values actual chip outputs iii adjust weights chip learning algorithm network error decreases 
accommodate offset errors non linearities wealth different training approaches quite easily programmed host computer chip loop training 
question arising sacrifice flexibility chip loop training host computer favour implementing learning algorithms analogue hardware reasons similar reasons implementing anns analogue hardware place performing similar operations synapses neurons regular system composed synapses neurons learning algorithms properties neural networks implemented analogue hardware ffl parallelism learning computationally heavy task 
typically order system neurons 
compare task order assuming system synapses 
terms speed greater importance utilize inherent parallelism learning algorithm neural network 
fortunately learning algorithms parallelized great extent 
arguments analogue hardware utilize parallelism 
massively parallel implementations learning algorithms reported arima 
ffl adaptability adaptive neural systems continuously taught 
situations exist learning algorithm embedded system hardware large adaptive systems crucial utilize inherent parallelism learning algorithm 
ii small adaptive low power systems host computer available 
application areas neural network analogue arguments advocated analogue hardware neural network holds implementation learning algorithm 
ffl learning algorithms formulated continuous time enables asynchronous analogue hardware implementations learning algorithms 
asynchronous analogue neural network asynchronous analogue interfaces naturally taught learning algorithm 
ffl fault tolerance learning algorithms formulated operate local data way learning algorithm embedded chapter page extended synapse neuron hardware 
fault tolerant analogue neural network inclusion learning algorithms sacrifice fault tolerance system 
ffl low power applications case analogue neural networks analogue implementations learning algorithms operated sub threshold extremely low power applications 
ffl data conversion learning algorithm needs access inputs outputs intermediate variable neural network 
neural network analogue analogue hardware learning algorithm eliminates need converters cf 

ffl regularity case neural networks learning algorithms regular structure design equivalent hardware inexpensive 
combining properties section conclude niches analogue hardware implementations learning algorithms exist possibly asynchronous redundant hardware ffl massively parallel possibly adaptive application specific systems having parallel real world interface 
ffl small adaptive low power application specific systems real world interface 
neural networks believed certain circles limited precision analogue hardware sufficient implementation certain learning algorithms feed back 
argue limiting effects noise murray edwards improve learning ability 
certain offset errors completely destructive learning scheme lehmann generally accepted prohibits analogue implementations learning algorithms systems reported 
alspector valle 
research implementing analogue learning hardware wealth unexplored areas 
commit chapters 
architectures analogue hardware learning alspector arima card hollis jabri flower lehmann matsumoto koga murray schneider card tarassenko valle wang 
page chapter implementation neural network implementing learning system chosen sequential approach implementing acting neural system second implementing learning hardware system 
chapter implementation artificial neural network core systems thesis 
chapter includes reflections choice network models topologies suitable analogue vlsi implementation 
choices hardware topologies essential memories multipliers thresholding hardware discussed implementation hardware learning mind 
examples literature 
presentation design measurements ann solution presentation system level measurements reflections inclusion process parameter dependency canceling temperature compensation 
summary concludes chapter 
chapter implementation neural network page artificial neural network model thing decide model artificial neural network 
properties model possess ffl general purpose ffl simple ffl suitable technology argued chapter analogue anns application specific 
particular application mind point shall deviate slightly principle violating object design set general purpose building block components modules eberhardt see mueller 
application specific systems composed number 
analogue computational hardware typically limited relative precision 
leary see appendix 
reason reason limiting hardware cost preferable simple ann model 
researchers try model biological mechanisms neural networks closely see macgregor complicated network models 
necessary computational neural networks properties owing structure non linearity system 
paramount importance ann model compatible restrictions imposed analogue vlsi technology 
mead 
advantages technology place lost 
absolutely clear justify implementation arbitrary model just justify analogue integrated anns arbitrary application argued previous chapter model easy map hardware terms topology computation primitives 
objective implement acting system refined necessary 
neurons stochastic neurons gives possibility implementing powerful networks boltzmann machines hertz 
activation stochastic neuron typically value gamma probabilistically determined pr gamma pr gamma neuron net input activation function cf 
appendix 
stochastic systems efficiently explore state space system free parameters learning process 
somewhat slow outputs averaged time find probability distributions outputs stochastic processes suited analogue signal processing see alspector vlsi implementation come problem different kinds annealing processes 
chapter implementation neural network page general deterministic network model uses higher order neurons giles kj kj kj delta delta delta kj kj kj connection strengths neuron cf 
appendix 
highest number factors gives order neuron 
higher order networks efficient compared conventional 
order networks map poorly vlsi high structural dimensionality dth order network dimensional structure 
order deterministic neurons preferable vlsi implementation point view 
theoretically studied significant 
choice neuron transfer function 
simplest possible choice setting sign hard limiter hopfield networks suited analogue vlsi implementation hollis anchez lau 
sacrifice generality system obviously continuous valued outputs impossible learning algorithms rely smooth transition low high neuron output 
choice set sigmoid function sufficiently general solution networks kind neurons approximate limited function lapedes farber 
network ideally put constraints network topology 
shall see sparse interconnections neurons difficult implement general 
choice fully interconnected groups neurons general topology 
unconstrained topology imposes problem feedback instability 
unknown number neurons feedback loop cause unknown phase shift high frequencies lead oscillations 
problem complicated fact signs magnitude gains weights system change learning 
solution place feedback shown ensuring single dominating pole loop refer hollis graf jackel network input output activation neuron see sections 
chapter implementation neural network page mueller systems 
non relaxation network time constants way match time constants input data possible learning hardware 
case easier discrete time feedback 
sampler general system sacrificed 
discrete time systems wealth problems apply analogue anns systems thesis designed discrete time 
mapping algorithm vlsi presenting analogue integrated ann solution shall look different aspects implementations 
specifically shall discuss different architectures signalling methods memories multiplication thresholding circuits implementation learning algorithms mind 
architecture architecture small low power application specific system cf 
chapter tailored application 
building block components systems atomic parts neural networks synapses neurons form say cell library cmos process 
discussions thesis meant massively parallel systems considerations apply equally small low power systems 
circuits replaced low power ones 
shall strong inversion circuits subthreshold ones inherently faster 
massively parallel application specific systems level integration building block components preferably high reduces design time 
unfortunately puts constraints architecture minimizing constraints objects vlsi neural networks design 
reformulating vector order neurons cf 
corresponding say layer vector sigmoid functions 
assuming parallel operated matrix vector multiplier mvm gives output multiplication input vector stored matrix number rows multiplier increased systems network allowed settle steady state input pattern applied williams zipser time sequence data processing :10.1.1.52.9724
chapter implementation neural network page simply adding multiplier input vector 
number columns increased adding output vector multiplier 
dimensions easily added vector functions implementation ann fully interconnected layers layered scaled arbitrary size feasible building block components lansner lehmann 
shown assumed adding outputs multipliers done simply connecting outputs cf 
section 
important 
shall terms synapse chip multiplier neuron chip squashing functions modules 
shall refer rows columns rows synapses columns synapses 
recurrent network elegant approach place synapses neuron chip illustrated wy 
module interconnection easier 
neuron synapse expandable neural network 
topology implement systems arbitrary size fully connected layers 
expect routing problems systems rigorously interconnected units 
distributed regular placement synapses systems practically eliminates problem massive inter chip communication inconvenient 
sparse random connectivity routing consume considerably area synapse 
obviously order ann topology mapped systems setting connection strengths equal zero feedback extra layers added 
system known sparse chapter implementation neural network page expandable recurrent neural network 
topology implement systems arbitrary size 
neurons larger synapses order waste area 
connectivity preferable waste hardware 
accomplished folding synapse matrix way similar folding sparse plas structure network known advance 
certainly implementing general neural architectures 
solving problem unknown properties typically arrive sparse architecture pruning 
removing unnecessary connections fully connected network 
optimal brain damage le cun see larsen :10.1.1.32.7223
preferably reconfigurable neural network able emulate fully connected pre pruning phase 
depending remove connections simple synapses small care taken interconnections routing switches take area left free reduced number synapses 
way avoid wasting hardware pruned network null connections fully connected architecture introduce redundancy system 
believe fully connected building block topology simple capable 
shall 
number systems reconfigurable network topologies proposed literature mueller graf henderson 
questioned random connected neural network mapped efficiently systems provide gen chapter implementation neural network page eral problem solving environment 
reconfigurability alter ann topology training map defective blocks 
routing switches neuron synapses reconfigurable neural network 
philosophy kind topology implement general neural computer 
particularly interesting reconfigurable ann see 
implementation lumped synapses neurons replaced distributed neuron synapses neuron squashing circuit distributed connected synapses connected parallel neuron synapses ensuring routing switch area kept reasonably low indicated signalling domains various signals carried closely related needs matrix multiplier needs synapse ffl output neuron network input easily distributed column synapses 
ffl outputs row synapses easily accumulated 
distributing signal easily done voltage detected high impedance sensors parallel 
mos gates 
current domain addition analogue signals simply done connecting input wires output wire 
synapses voltage inputs current outputs satisfies requirements fortunate multipliers typically voltage inputs current outputs 
illustrated variation current output scheme charge packages accumulated integrator 
chapter implementation neural network page kj typical electronic synapse 
multiplier voltage inputs current output ensure synapses 
analogue signals carried voltage current domains sensitive noise instance coupled power supply capacitive inductive 
pulse stream neural network noise sensitivity neuron outputs efficiently reduced moving information voltage domain time domain instance pulse frequency modulation pfm pulse width modulation pwm murray digital voltage signal easily distributed regenerated temporal information insensitive noise sources 
noise sensitivity synapse outputs easily reduced requirement easy accumulation 
synapse outputs typically charge packages connections strengths multiplied stream input pulses 
get full advantage noise insensitive neuron outputs important synapse neuron connections kept minimum local area 
neuron outputs inter chip communication 
disadvantages pulsed neural networks reduction speed bandwidth system process data points pure analogue pfm neural network dynamic range db 
commonly signalling methods integrated neural network contexts methods exist neugebauer murray mead webb 
shall continuous valued signalling voltage current domains inherently fastest signalling method compatible simple synapse architecture 
seen typical electronic synapse consists components fundamental nyquist upper limit assuming sinc sinc def sin pulses linear system 
realistic measure instance data points second assuming system single dominating pole frequency db output corresponding step input settle bit accuracy time db lehmann 
process data points 
chapter implementation neural network page multiplier connection strength memory cell 
number synapses ann scale number neurons reducing synapse area major objects integrated neural network research 
discussion memory cells multipliers subject sections 
memories storing analogue signals means simple true efficient analogue electronic memory exists today 
storage synaptic strengths major concern analogue anns research solutions literature compromises kind put categories ffl capacitive storage ffl storage special process facilities ffl digital storage capacitive storage simplest method storing analogue signal put charge capacitor reading high impedance gate terminal 
drawback method leakage current primarily sampling switch weight changing device eventually exhausts weight 
approaches reduce leakage current possible 
instance differential scheme shown cancels predominant source bulk reverse biased junction current 
scheme cancels offset error due charge injection 
alternatively low offset voltage buffer ensure voltage drop source bulk diode efficiently eliminating leakage see shah nakamura 
method employed weight decay totally eliminated kind refresh necessary 
ss write multiplier capacitive storage 
approaches refreshing charge reducing leakage differential scheme shown possible 
chapter implementation neural network page refreshing schemes rely quantizing weights discrete valued weights recharge weight capacitors 
obvious approaches digital ram backup memory weights stored digitally ram capacitors periodically refreshed converter lansner lehmann eberhardt jackson 
typically serial access weight capacitors words ram count converters 
digital ram cheap accountable solution large systems 
serial access updating weights severe limitation system hardware learning necessarily order slower system parallel weight access 
discussed issue coarse weight discretizing learning see 
hollis tarassenko chapters pronounced architecture converters needed accountable high precision converters second ram backup words arbitrary widths number bits small weight changes accumulated cf 
chapters 
possible employ quantize regenerate refreshing scheme voltage weight capacitor periodically compared discrete number voltages 
form staircase ramp capacitor voltage regenerated closest nakamura 
high precision weights compared weight rate necessary place regeneration circuit synapse sites allow parallel weight refresh 
lower precision weights column say share circuit voltage buffer placed synapse site drive capacitance wire connecting column regeneration circuit 
case quantize regenerate technique area demanding simple capacitive storage ram backup 
systems chip learning quantize regenerate refreshing scheme particularly suited required high resolution weights learning scheme fast weight updates accumulated successive weight refreshes compare analogue adjustment 
altogether different approach refreshing relies presence learning scheme refresh relearning idle phase neural network trained epoch say original training data restoring weights cf 

valle see arima 
obvious disadvantages approach network run continuously ii training set needs stored system see chapter 
training scheme employed unsupervised learning scheme refresh relearning employed elegant way learning applied input pattern eliminating need idle phase storing training data schneider card 
weight refresh applicable learning critic employed system neurons 
chapter implementation neural network page ment signal usually extracted environment acting system minimum cost 
trouble approach network tend forget classification scarcely occurring input patterns 
acceptable advantage strongly dependent application 
situations necessary able read contents weight matrix backup purposes example transferring network state network retraining necessary 
possible direct weight access outputs matrix vector multiplier accessible applying ffi inputs matrix vector multiplier yields outputs ij ij ofs ij ofs mvm output offset error order approximation 
special process facility storage non volatile analogue memories overcome leakage problems capacitive storage 
popular floating gate storage charge trapped completely insulated floating gate programming threshold voltage cf 
sze see nakamura compare 
input transistor multiplier exists numerous ways trapping charge floating gate compatible standard cmos processes requiring special process steps eeprom processes example 
programming usually carried applying high voltage gate oxide forcing tunneling current charge gate carley lee ii exposing gate uv light inducing parallel conductance caused generation holes electron pairs oxide benson 
floating gate pn bulk source drain control gate fox floating gate 
schematic drawing physical floating gate 
control gate floating gate overlap need top channel 
completely different approach amorphous silicon storage 
resistance amorphous silicon sandwich programmed applying high voltage pulses way floating gate programmed electrically 
kronecker delta ffi ae note offset error small readout accurate learning rule 
chapter implementation neural network page quite small special process facility memories true analogue memories existing today matters weigh analogue vlsi neural networks writing analogue memories usually wear devices 
typical floating gate device endure order full scale changes example 
sufficient programmable recall mode systems castro adaptive systems weight changes accumulated short term memory reduce number device systems general taught example line learning batch learning continuously time 
number weight changes scale quickly exhausting number device 
driving force vlsi processes digital electronics primarily ram microprocessors 
state art process tuned digital requirement 
single poly triple metal cmos process precision components 
get access state art processes analogue circuit designer submit potentials offered digital processes 
argue special analogue devices high resistive polysilicon floating capacitors precision components processes eventually cease available average designer role analogue circuits interfaces digital signal processors dsps mixed analogue digital integrated circuits near see 
reason reason special process steps analogue circuits especially vlsi analogue circuits exceedingly expensive 
context important note systems minimizing synapse cost power dissipation synapse area objective 
non volatile analogue memories compatible standard cmos processes caution rely un documented features process characterized experimentally designer ii probably subjects immense process variations iii possibly changed notice vendor 
important characteristic instance memory life time fairly idea considering production floating gate devices degrade time scale years seconds capacitive storage 
need high voltages uv light programming inconvenient 
see murray 
digital storage problems weight degrading weight wearing special processing steps overcome willing refrain analogue storage 
high resolution synapse strengths needed digital memories consume area simple analogue memories size digital memory scale log res analogue scale res res resolution synapse strength limited noise cf 

geiger 
typical ann system require weight resolution bit analogue solution usually smallest far 
chapter implementation neural network page severe problem embedding digital circuitry analogue system need data converters area monotonous converters typically scale res cf 
geiger 
digital synapse strength storage require digital analogue converter dac synapse site multiplying dac eliminates need synapse multiplier area consuming part synapse 
spite area penalties digital weight storage application areas useful especially small systems tolerate need support hardware ram 
flower jabri digital synapse strength storage heart instance 
analogue systems include hardware learning obstruction connected digital storage need analogue digital converters adc write synapse strengths 
parallel weight updating system area inefficient see hollis necessary high weight resolution learning typically bit hollis lehmann tarassenko fahlman brunak hansen 
inspired fact weight changes learning usually smaller necessary resolution recall mode network say bit proposed analogue adjustment analogue bit digital memory active learning see lehmann lansner weight changes determined chip learning algorithm accumulated analogue memory 
equivalent lsb accumulated digital word decreased increased analogue adjustment reset 
operation basically requires bit digital adder shared column synapses 
synapse memories read memories mentioned see mead ismail mead instance programmed transistor sizing 
read memories interesting context hardware learning 
chosen simple capacitive storage method ram backup particularly suited learning simple reliable concept allow dense synapse packaging 
learning scheme submit storage method 
chapter implementation neural network page multipliers analogue memories analogue multipliers easily implemented say cmos provided inherent offset non linearity acceptable 
different multiplier architectures proposed literature see neugebauer ismail hollis saxena clark anchez lau shall restrict examination 
desired synapse multiplier key characteristics cf 
ffl small ffl current output ffl voltage inputs 
high input impedance 
mos gate capacitance node synapse strength storage 
prior implementing synapse multiplier questions need answer 
need quadrant multiplier 
synapse strengths general neural network needs bipolar need quadrant synapse 
neuron activation function 
tanh yields output bipolar value indicating need quadrant synapse multiplication 
doing simple linear transformation activation function bipolar units superscript sigma sigma gamma units superscript sigma kj sigma kj theta theta sigma sigma kj theta neuron threshold cf 
see mathematical sense need bipolar activation function 
expected pulse stream networks brain operational 
comes learning advantageous bipolar neuron outputs learning algorithms typically factor ffl weight updating rule kj ffl neuron error 
neurons tanh say weight change negligible close lower extreme value regardless neuron error 
reason anns bipolar neurons tend learn faster stornetta huberman haykin see le cun 
noted transformation applied learning algorithms networks 
yield slightly complicated weight adding constant prior multiplication subtraction wz gamma quadrant multiplication strictly necessary johnson see 
method bound introduce additional offset errors turns major problem analogue vlsi neural networks cf 
text 
chapter implementation neural network page updating rules different rules kj theta undesirable 
quite different motivation quadrant synapse multiplier needed implementation learning hardware cf 
chapters reduces design time error probability reuse building blocks 
second question need linear multiplier 
doing gradient descent learning needs know ij derivative sum synapse outputs 
fulfill need multiplier linear compliance simple ann model computable transfer function derivative 
requirement somewhat strict needed inherent fault tolerance anns relaxes requirement ann chips taught chip loop training hardware training inaccuracies eliminated great extent learning algorithm eberhardt lehmann castro valle card leong jabri see section 
greater importance multiplier linearity dynamic range usually restricted db puts constraints networks mapped topology 
connection multiplier output offset error important connecting outputs synapse multipliers output offsets accumulate easily giving resulting offset greater maximum output single synapse multiplier systematic offset probable 
principle resulting neuron input offset error canceled adjusting bias dynamic range bias synapse easily exceeded steps prevent taken 
offset canceling 
chip specific offset errors process variation related inaccuracies reasons analogue anns general need taught chip loop training 
analogue systems chip learning small multiplier offsets cause severe problems cf 
chapters 
gilbert multiplier popular quadrant multiplier gilbert multiplier shown schneider card 
assuming square law approximation saturated show output current wz wz gamma wz gamma fi fi delta vw vw small sense differential output current upper differential pairs linear vw fi fi parameters upper lower differential pairs respectively 
synapse multiplier row synapses share current mirror needed take wz gamma wz gamma difference call circuit current saving current mirror synapse fi gs gamma vt precisely vw ib fi gamma fi ib gamma fi chapter implementation neural network page ss wz bias wz mos gilbert multiplier 
transistors saturation 
wide range version possible adding number current mirrors 
accuracy difference total transistor area devoted task decreased cf 
appendix 
note local current differencing multiplier output directly compatible neuron input utmost care taken design required signal converter loose accuracy component 
design restricted supply voltage wide range version multiplier easily implemented addition number current mirrors 
folded version multiplier possibility 
mos resistive circuit linear multiplier mos resistive circuit mrc shown ismail 
ensuring virtual short circuit output terminals differential output current wz wz gamma wz gamma mrc transistors operate region 
requiring matched transistors circuit nice properties cancels non linearities mos transistors making difference current linear vw difference current independent threshold voltage making insensitive bulk effect substrate noise 
circuit quite fast high frequency effects parasitic capacitances tend cancel 
disadvantage circuit mode operation require somewhat large power supply voltage 
differential mode signals terminals circuit acts controlled resistors cf 
resistances mrc defined observation convenient analyzing circuits 
need virtual short circuit prevents feasibility local current subtraction circuit synapse multiplier circuit usually exhibit relative large output offset errors 
consisting transistors synapse density high multiplier 
higher synapse density possible single ended signalling employed chapter implementation neural network page wz wz mos resistive circuit multiplier 
transistors mode output potentials equal 
mode operation restricts dynamic range 
mrc mrc wz wz mrc resistive equivalent 
convenient equivalent circuit valid differential mode signals 
terminals constant forcing output potentials potential transistors zero drain source voltage eliminated conduct current flower jabri 
problem approach low impedance summing nodes necessary outputs capable sinking current synapses 
multiplying dac digital synapse memory analogue system multiplying dac synapse multiplier 
disadvantage approach excessive area consumption 
expense reduced accuracy cf 
appendix scaling summing circuit dacs shared say row synapses 
way identical voltage controlled current sources needed synapse sites bit resolution see dietrich see van der spiegel 
diode coupled transistor common column ensures current proportional input voltage synapse current sources corresponding weight bit kj set 
currents output lines scaled summed current adder common row producing resulting output current 
accuracy solution primarily determined diode coupled transistor match synapse current sources column 
resolution determined local current source matching accuracy scaling current adder 
modest area increase circuit handle bipolar inputs dietrich lehmann lansner 
theta synapse chip designed institute fabricated standard cmos process giving accuracy proving applicability scheme dietrich lehmann 
doing chip learning system digital synapse weights weight resolution temporary increased learning addition analogue adjustment noted section 
get accurate weight chapter implementation neural network page algorithm learning analogue adjustment ss ss kj kj kj multiplying dac synapse 
simplified schematic positive inputs 
switch transistors controlled local weight register bits kj shown 
value optional analogue adjustment controlled chip learning algorithm 
multiplication learning analogue multiplier added multiplying dac indicated 
improved learning result omission multiplier network errors calculated actual resulting network intermediate network higher weight resolution lansner 
see section 
input neuron activation value network input binary multiplier architecture simple murray graf jackel johnson 
basically weight controlled current source switched multiplier output depending state input requiring transistors neuron activation 
see section 
synapse multiplier non linear vw 
sinh reduce problem limited dynamic range van der spiegel hollis valle kwan tang 
magnitude sign actual value weight importance ann performance inevitably reduced resolution large weights concern 
mos transistors operated threshold achieving exponential characteristic easy 
square law non linearity easily achieved indicated particular circuit advantage offset free zero weight ensured ignoring subthreshold currents proper biasing see flat plateau zero output current tend chapter implementation neural network page trap small weights efficiently zero value multiplier said self pruning improve ann generalization ability 
dd ss wz bias bias simple non linear synapse multiplier 
leftmost transistors act level ensure saturated middle weight transistors turned 
multiplication performed rightmost switch transistor 
vw vo ua ua ua ua non linear input binary date time run temperature weight output characteristic 
simple double asymmetric square law characteristic 
notice null range ensure zero output offset error 
chosen mos resistive circuit multiplier 
reasons mrc small fast multiplier important applicability massively parallel analogue anns 
ii learning algorithm add stage determined advance reject possibility implementing gradient descent algorithm 
extent unknown synapse non linearity canceled learning scheme problem algorithm dependent reasonably linear multiplier safe choice 
iii mrc versatile component 
instance implement voltage voltage multiplier divider op amp independent process variations cf 
chapters 
needed learning scheme reuse multiplier cell reduces possibility design errors 
connection mentioned possible choices process variation insensitive voltage voltage multipliers 
wang see sakurai ismail allen 
chapter implementation neural network page activation functions thing needs considered implementing analogue neural network threshold function 
binary valued neuron transfer function sufficient application hand neuron circuit simple ismail hollis 
hand continuous valued non linearity sought case circuit somewhat complex 
complexity usually major concern neurons compared order synapses 
exact shape neuron transfer function usually irrelevant cf 

anchez lau 
important qualitative shape 
monotonous saturates numerically large inputs 
attractive feature means transfer functions easily implemented technology chosen 
pulse frequency neuron neuron schematic course strongly dependent network signalling domains 
pulse frequency neural networks instance neuron non linear current controlled oscillator cco 
sample implementation neuron seen murray 
dep int ref ref pulse frequency neuron 
output voltage alternates digital high low frequency determined non linear way input current 
pulse frequency range act dep int ref gamma ref gamma 
particular circuit particularly low input impedance compatible synapse multipliers 
distributed neuron continuous valued current voltage signalling neurons neuron non linearity achieved simply applying non linear load summed synapse outputs wire neuron input output assume loads neuron output high impedance 
approach easy chapter implementation neural network page distribute neuron hardware synapses conventional lumped neurons see 
distributed approach obvious advantage current range distributed elements single synapse making system truly scalable arbitrary size 
function implemented distributed neuron synapse approach kj number inputs resulting neuron argue typical weight magnitudes neuron proportional see section case factor improve effective dynamic range synapses factor see eberhardt 
note distributed neuron elements kept small number scale 
kj synapse synapse kj synapse dd ss dd ss dd ss distributed neuron 
input current output voltage carried wire 
transfer function particular neuron saturates synapse contribute transfer characteristic 
distributed approach network truly scalable 
hyperbolic tangent neuron add learning hardware acting neural network probably access neuron net inputs implementation gradient descent algorithm needs computed 
choice hyperbolic tangent transfer function transfer function bipolar differential pair tailored situation tanh gamma tanh chapter implementation neural network page unfortunately differential pair voltage input current output way input output necessary 
hyperbolic tangent neuron implementation seen 
bipolar transistors available standard cmos processes 
mos transistors operated lateral bipolar mode lbm turns reasonably somewhat slow bipolar transistor see appendix 
philosophy section devices severe case fairly simple regulating circuit primary parameter lbm current gain measured adapted 
see section 
shall discuss necessity computing transfer function derivative chapters problems related calculation 
shall examine neuron circuits 
constraining implementation learning hardware motivates choice transfer function hyperbolic tangent implemented lbm 
neuron architectures literature refer interested reader 
mueller schneider card lau mead 
chip design section describe chip set developed institute 
considerations choices central components previous section 
description chip set published lansner lehmann see lehmann schultz 
chip set consist neuron chip synapse chip having topologies shown chip set designed primarily test ann functionality little hardware possible included chips reduce possibility malfunctioning chips 
design methodology proven successful designed chips worked processing errors occurred 
price reduced error probability basically chips need large number biases voltages currents severely complicates 
unfortunately sufficient time design complete volume set chips problems related self biasing temperature compensation experimentally covered thesis important vlsi design 
integrated cmos process shall standard analogue double poly double metal cmos process 
order put constraints possible analogue building blocks chosen large sigma power supply 
convenient different components chip set chapter implementation neural network page developed concurrently different designers john lansner thomas 
implementation building blocks redesigned standard digital process 
design strategy employed reuse components possible order reduce possibility design errors design time 
true chip micro components op amp macro components matrix vector multiplier shall see chapters 
preliminary general system aspect considerations needed actual chip designs 
appendix 
neuron chip neuron chip design done john lansner see lansner 
schematic neuron neuron chip shown core circuit bipolar differential pair implemented lbm 
differential output current pair converted single ended voltage output range mrc op amp 
op amp schematic see appendix buffered neuron drive relative low impedance input synapse chip cf 

input neuron input scale mrc op amp likewise placed acting input converts input current voltage needed drive differential pair 
resulting transfer function yk fi ff fc tanh sk fi ror ff fc tanh sk bias current ff fc emitter collector current gain 
undesired vertical collector lbm connected substrate ff fc gammai 
output voltage yk referred ref gamma compatible synapse inputs 
input impedance non linear strongly dependent smaller resulting non ideal load current source tanh saturated severe non linearities synapse output stage caused non linear finite load indifferent input current sk error indistinguishable neuron output 
differential pair saturates differential input voltage tanh voltage shift input tolerated synapse chip regardless 
easily accomplished 
input adjustable dynamic range db db 
words neuron transfer function steepness fi adjustable range effective maximum synapse weight jwj max prefers think transfer function fixed steepness fi 
clearly chapter implementation neural network page dd ss ref hyperbolic tangent neuron 
basically bjt differential pair parasitic components embedded 
system arbitrary large number synapses connected neuron dynamic range allow neuron saturated synapses exiting case steepness scaled number connected synapses 
dynamic range neuron steepness sufficient cancel process variations 
classification problems seen neurons trained network output hidden ones saturated see 
brunak krogh see williams zipser act hard clearly possible steepness scales ensure adjustable omega gamma extremes embraced eberhardt see mueller lumped neuron approach prove difficult lumped neuron able sink arbitrarily large current 
system able handle situation synapses agree decision turn overruled synapse situation uncommon brain rumelhart 
unfortunate order absolutely general dynamic range synapse scale incompatible analogue vlsi 
chapter implementation neural network page stress importance making dynamic range synapse large possible 
order reasonably simple neurons learning algorithms chosen fixed neuron steepness approach 
steepness selected compatible typical weight magnitudes reported systems simulations 
noted non linearity introduced non linear neuron input impedance acceptable neuron steepness reduced simply adding parallel external resistance input 
objectives research enhance neuron steepness dynamic range steepness governed learning algorithm 
synapse chip synapse chip consists number inner product multipliers ipm multiply input vector 
row stored matrix wk 
inner product multiplier shown see 
ismail 
difference summed synapse mrc outputs taken op amp mrc feedback ensures required virtual short circuit synapse outputs 
resulting voltage transformed mk output current sk 
op amp schematics see appendix resulting transfer function sk delta vc delta mrc width length ratios vc controls total 
control voltage adjust effective maximum synapse weight dynamic range allow small adjustments compensating process variations case neuron steepness adjustments 
schematic single synapse shown synapse strength stored differential manner capacitors synapse site way offset due charge injection wegmann canceled differential charge leakage due reversed biased drain bulk diodes sampling switches provided components match 
ensure random synapse access sampling switches controlled nand gate directly row column select signals provided row column decoders lee 
minimum geometry transistors gate area overhead acceptable 
see appendix 
meaning index runs possible values 
def zm chapter implementation neural network page kj ref mk inner product multiplier 
mrc input vector dimension required upper part 
mrc feed back ensures virtual mrc outputs converts output voltage current 
second generation synapse chip op amp mrc feedback synapse followed obtain desired output current somewhat indirect 
simpler accurate approach current conveyor see appendix take difference ensuring virtual short circuit shown see chapter 
sk input lead directly output sk gamma negated current conveyor added output 
voltage follower ensures virtual short circuit 
avoid dc common mode currents synapses output potential current conveyor close ref requires slightly changed neuron schematic cf 
chapter 
solution allow effective maximum synapse weight tuned 
process variations canceled scaling weights reduces dynamic range synapse weights slightly 
adjustments carried automatically learning chapter implementation neural network page wz wz syn sw kj row column vw ref kj synapse schematic 
addition mrc multiplier differential capacitive access switch transistors nand gate circuit placed synapse sites 
weight scaling learning mechanism 
current conveyor 
circuit gives output sk sk gamma sk gamma buffering node voltage node 
input voltage determined output load 
possible current mode op amp resistive feedback 
way effective maximum synapse weight adjusted preserving accuracy simple 
current mode operational devices current mode signal processing superior choice considering speed accuracy circuit 
chapter implementation neural network page sparse input synapse chip inputs neural network taken discrete input alphabet consisting number na symbols letters ff ff ff na avoid false distance relations different letters unary coding letters usually employed na network input lines assigned logical input letter input na na na na na gamma ff delta delta delta ff delta delta delta 
ff na delta delta delta notice network inputs sparsely 
gamma inactive value discussed section 
particular case choice improve learning weights related input letter letter input modified typical algorithm cf 

sample applications examples applications unary input coding shall mention prediction splice sites word hyphenation human project aim map human genes prediction splice sites pre mrna molecules copy part information dna molecule important task 
dna information junk code proteins junk dna scattered dna sequences code proteins 
prior protein synthesis body cells cut junk sequences pre mrna molecules 
junk code boundaries splice sites see brunak 
splice sites difficult predict dependent context nucleotide sequence 
applying neural networks problem proven successful brunak 
input alphabet ann application na letters corresponding nucleotide bases dna rna 
brunak layer perceptrons order letter inputs 
network inputs hidden output sigmoid neurons classification task 
application uses discrete input alphabet word hyphenation 
ignoring context dependent hyphenation de vs des ert solution ann input window letters brunak layer perceptron letter inputs hidden output sigmoid neurons danish words 
input alphabet classification task na letters corresponding letters danish alphabet ae plus null letter 
chapter implementation neural network page pre mrna splice site nucleotide sequence 
splice site shown sample schematic pre mrna molecule rna composed sequence bases 
sparse unary coding letter inputs say applying zero inputs input clearly exploits input bandwidth standard synapse chip poorly 
systems say hundreds letter inputs small reduction required input bandwidth reduce system cost considerably 
motivation develop special input layer synapse chip networks discrete input alphabet sparse input synapse chip 
basic idea synapse chip simply binary coding input alphabet decode unary coding chip 
standard digital cmos design techniques decoding alternatively controlled letter input redirect current shown shows current bit letter input synapse columns connected 
discussed section synapse multiplier simple input binary 
current derive bias currents synapse shown columns synapse bias currents 
wx vw network inputs synapse column letter input dd ss ss ss sparse input synapse chip column 
binary coded letter input corresponds say unary coded ann inputs bias current left synapse columns shown right corresponding letter input 
turning synapse bias currents act multiplication 
chapter implementation neural network page implement non application specific sparse input synapse chip necessary reconfigurable able change number outputs control lines fit applications 
design reconfigurable sparse input synapse chip done jesper schultz see schultz 
required input bandwidth drive number synapse columns scale log na na exploit input bandwidth chip area efficiently na input signals time multiplexed na low improve exploitation chip area usually paramount importance fully exploit input bandwidth na high 
necessary select ideal na number synapse rows tuned input bandwidth 
number letters input alphabet power binary input coding prohibits efficient exploitation input bandwidth chip area 
reconfigurable sparse input synapse chip non application specific general purpose 
chip measurements neuron neuron chip theta synapse synapse chip fabricated standard cmos process 
section shall measurements chips published lansner lehmann 
table important chip characteristics appendix 
neuron chip measurements neuron chip done john lansner see lansner 
measured neuron transfer characteristics different values input scale voltage seen 
maximum transfer function nonlinearity compared ideal tanh output range non linearity derivative dg 
neuron function input current ua output voltage measured neuron transfer function 
characteristics different input scale control voltages dotted lines ideal tanh curves 
input offset canceled 
chapter implementation neural network page low non linearity proves applicability lbm differential pair possibility accurately computing derivative basis neuron output see section 
synapse chips measured synapse transfer characteristics single synapse seen characteristics showed linearity bits accuracy exception case negative values positive values 
necessary lower ss ensure reasonable output current swing due layout error 
non linearity means prohibitive application synapse chip chip loop training chip learning mechanism easily compensate cf 

castro valle see section 
synapse characteristic input voltage output current ua 

measured synapse characteristics 
characteristics different stored weight voltages vw output offset canceled 
weight matrix resolution measured mv bit range matrix voltages 
smaller changes possible lies noise floor synapse chip output 
recall mode system resolution sufficient wide range applications cf 

sections 
second generation synapse chip measurements synapse chip current conveyor synapse differencing refer chapter 
output offset currents synapse chip input offset current neuron chip quite large approaching magnitude maximum synapse output current 
reason addition component mismatch low gains 
db offset voltages chapter implementation neural network page mv give measured current offsets 
necessarily major problem provided network trained chips offset currents just displaces neuron biases 
likewise matrix offset voltages relative small small random initial weights network trained 
noted offset errors non systematic 
large current offsets chip set characteristics compatible ann applications general ideal simulated network course 
primary limitation network limited dynamic range synapses 
enlarging application area ensemble methods employed cf 
section 
chip compound interconnecting synapse neuron chip combined transfer characteristics measured 
shown different values synapse strength verifying synapse neuron chip compatibility 
step response synapse neuron combination shown delay layer ann chips measured curve bit output accuracy lpd corresponding synapse chip 
synapse chip propagation delay largely independent number synapses full size theta synapses synapse chip expected 
synapse neuron function input voltage output voltage 
measured synapse neuron transfer characteristics 
characteristics combined chips ann layer different stored weight voltages vw synapse neuron step time voltages measured synapse neuron step response 
combined chips 
dashed line input signal 
effect cascaded seen evidently high order transfer function 
comparison typical workstation able mflops hp mhz hp direct orders magnitude computational power single chip 
chapter implementation neural network page system design system design done john lansner see lansner 
verify functionality chip set layer test perceptron implemented institute 
synapse chips neuron chips architecture implemented shown 
layout error caused neurons neuron chip disconnected inputs synapse chip malfunctioning 
architecture 
linear output neurons implemented simple resistors 
layer test perceptron 
simple architecture capable solving large range non trivial tasks 
standard pc interface added test teach system cf 

synapse strengths stored bit ram periodically refreshed bit dac 
output hidden neurons accessible pc bit 
inputs driven bit dacs 
dac dac adc counter ram pc test perceptron system architecture 
test ann system embedded digital system 
real world application weight backup digital 
chapter implementation neural network page system measurements system measurements done john lansner see lansner 
realistic performance evaluation known real world data set applied hardware system sunspot time series weigend 
semi periodic time series yearly average dark sun see data normalized range 
tapped delay line feed ann sunspot activity latest years ann predict activity year 
note data set complexity approximately matches network architecture essential obtaining generalization ability 
ann output 
sunspot prediction time series year activity training set sunspot prediction 
classic regression problem 
actual sunspot time series solid sunspot activity predicted hardware ann dotted 
ann trained standard chip loop back propagation algorithm 
calculation neuron transfer function derivative tanh characteristic exploited fi gamma ff fi ff constants actual hardware ann neuron activation 
ensure learning take place cf 
chapters lehmann neuron activations scaled ff compensate neuron output offset scale errors 
ff ff closely resembles procedure shall apply learning hardware 
prior learning neuron output ranges measured determine optimal ff ff performance hardware ann compared ideal software ann identical architecture non analogue hardware 
coarse weight discretization offset errors non linearities 
normalized average relative variance error training set training progresses see appendix seen noticed performance hardware ann somewhat noisy slightly worse software ann expected limited accuracy analogue hardware 
output accuracy hardware ann approximately bit see lansner 
error test sets seen curves high atypical chapter implementation neural network page test set sunspot count period resemble rest data set closely 
error lower training set test set exhibits minimum training leads fitting 
notice hardware ann test error noisy higher compared training error software test error 
caused limited accuracy analogue vlsi 
sunspot learning errors epoch normalized error sunspot learning error 
function learning epoch hardware ann dotted ideal software ann solid 
error approaches asymptotically minimum 
sunspot test errors epoch sunspot prediction error 
function learning epoch hardware ann dotted ideal software ann solid different test sets 
notice minima training leads fitting 
successful system evaluation indicates implementing hardware anns proposed architecture feasible claimed authors noted different analogue hardware anns 
step implement learning hardware ann system 
chapter implementation neural network page related hardware implementation artificial neural network issues need considered volume production possible 
shall display section 
process parameter dependency canceling noted section eliminate undocumented process parameters parasitic effects semiconductor process operated lateral bipolar mode 
case hyperbolic tangent neuron undocumented process parameter question forward emitter collector current gain ff fc cf 
differential pair 
simple regulating circuit seen single lbm emulates differential pair driven saturation tanh 
non substrate collector current lbm effective bias current ff fc subtracting current dumping difference high impedance node gives voltage vab drive bias transistor 
vab distributed neuron lbm differential pairs effective bias currents approximately loop gain gammag mb ff fc dsi gamma easily implemented simple mos transistors indicated 
corresponds effective bias current error gamma gamma 
vab meant distributed large number neurons 
bias transistors differential pairs ff fc distributed large area required match point increasing gain better accuracy 
sufficiently high number attached neurons gs bias transistors act compensation capacitance cc 
simple regulating loop cancel unknown global process parameters principle general applicability 
principle illustrated chip internal instance temperature compensated cancel temperature dependency unknown block 
chip signal input output needs absolute range 
critical inter chip signals multi chip systems 
hyperbolic tangent neuron output range cf 
evident component apply current factor dependency canceling 
wish calculate neuron derivative chip gamma ensure neuron output absolute range gamma output 
input lbm differential pair output current corresponds neuron output 
chapter implementation neural network page fc mb dsi dsi dd ss ref ab non unity current gain canceling 
circuit lbm differential pair 
saturated bjt differential pair output current 
effective tail current compared current amplified high impedance ab node fed back tail current source 
single bjt bjt pair transistor turned bjt pair 
ctrl ctrl ctrl unknown transfer function output input desired output general process parameter canceling circuit 
principal schematic method applied cancel non unity current gain 
unknown transfer function blocks matched 
temperature compensation excessive outside feedback loops analogue neural networks 
synapse multipliers temperature drift major concern problem experimental ann system solving sunspot regression problem 
temperature dependence mrc instance primarily determined mobility proportional gamma low substrate room temperature sze 
high substrate room temperature 
temperature range corresponds synapse strength drift 
real world applications large temperature drift unacceptable temperature known constant implanted devices temperature compensation employed 
regard temperature time varying undocumented process pa chapter implementation neural network page rameter 
temperature compensation implemented provided temperature independent available 
primary interest relation temperature drift effective neuron activation slope equivalently effective maximum synapse weight assuming output range defined 
noted section process variation influences effective maximum synapse weight canceled learning algorithm assuming constant temperature important special case classification neurons usually saturated learning phase 
case relative absolute synapse strengths describes network assume constant system wide temperature system function independently 
regression problems hand analogue outputs required temperature compensation 
improvements addition important process parameter temperature variance compensation issues subjects improvement developed ann chip set 
appendix 
summary chapter designed analogue vlsi artificial neural network 
network models topologies literature displayed 
deterministic order neurons continuous valued currents voltages signalling chosen architecture placing neurons chip synapses selected generality 
essential building block components memories multipliers reviewed 
analogue memories presently available 
adaptive systems simple capacitive storage best choice weight refresh problem 
chosen digital ram weight back memory puts severe restrictions efficiency learning scheme 
possibility digital storage combination analogue adjustment 
quadrant multipliers strictly necessary system temperature dependency synapse multipliers cancels neuron input scale 
resulting temperature dependency effective neuron activation slope kt varies temperature range 
absolutely true linear output neurons assuming hidden neurons saturated situation regression networks see 
krogh pedersen hansen temperature dependencies canceled 
chapter implementation neural network page probably advantageous compact mrc 
noted order absolutely general dynamic range synapse multiplier scalable system infinite output offset error important 
propose shall employ highly non linear multiplier improve dynamic range relative output offset error 
neuron activation function chose hyperbolic tangent function restrict implementations learning hardware derivative easily computed 
design ann chip set displayed 
sparse input synapse chip proposed chip architecture exploits limited input bandwidth efficiently problems discrete input alphabet 
measurements fabricated chip set displayed indicating possible gcps chip chip set 
offset errors primarily synapse output neuron input large tolerable training results test perceptron implemented test chips trained pc displayed hardware network learning error slightly worse ideal software net 
importance process parameter dependency canceling real systems stressed including temperature compensation 
sample circuit eliminating unknown forward emitter current gain lbm displayed 
page chapter preliminary conceptions hardware learning basic analogue artificial neural network architecture defined tested shall turn attention implementation analogue learning hardware ann 
general conceptions hardware learning chapter firstly shall consider tolerable amount hardware spent learning implementation 
secondly choices learning algorithms implement discussed 
give general considerations implementation ann learning algorithms analogue vlsi relation limited precision technology 
chapter preliminary conceptions hardware learning page hardware consumption chapter argued niches analogue hardware implementations learning algorithms exist ffl massively parallel possibly adaptive application specific systems having parallel real world interface 
ffl small adaptive low power application specific systems real world interface 
niches tolerable amount hardware put learning algorithm application dependent 
extreme crucial exploit inherent parallelism speed cost important learning scheme excessively upper bound amount learning hardware realistic implement 
massively parallel implementation learning algorithm choice 
vast amount learning hardware severely limit applicability learning scheme certain applications learning hardware lies idle system recall mode reduces number integrated synapses silicon area 
extreme cost power speed important learning scheme employed occasionally amount learning hardware kept small possible 
shall implement learning algorithms belonging categories hardware efficient implementation back propagation parallel fully parallel implementation real time recurrent learning 
learning algorithm amount energy learning hardware process single input output pattern independent parallelism implementation ideally reality serial implementation probably consume energy parallel implementation 
power consumption concern learning algorithm employed occasionally fully parallel implementation power circuit solution 
chapter preliminary conceptions hardware learning page choice learning algorithms choice learning algorithm highly dependent application hand 
different learning algorithms operate different network architectures different optimization procedures different goals 
addition new learning algorithms arise continuously argued vlsi learning hardware adaptable changing learning algorithms cf 


reconfigurability analogue anns high degree programmability analogue learning hardware comply technology hardware lie idle configure algorithm participate computations 
massively parallel low power implementations acceptable compromise advantages analogue technology algorithmic variations included possible appropriate cf 
inclusion entropic quadratic cost functions chapter 
possible principle ann implement general purpose analogue ann learning machine 
learning algorithm implemented specifically application hand 
ideally 
application specific learning algorithm comply general purpose building block ann chip set sacrifice generality ann learning chip set particular application matter course 
specific application mind choose learning algorithm carefully order applied large range applications murray 
learning algorithm possess properties neural network model ffl general purpose ffl simple ffl suitable technology simplicity important 
partly complex learning model requires hardware primarily limited precision technology things equal hardware participating calculations larger accumulated errors finer points complex learning scheme quickly insignificant compared errors 
needless say learning algorithm map simple way silicon rely local communication consume area memory 
choosing learning algorithm means choosing application area 
artificial neural networks applied wide range applications see 
lau 
instance ffl classification ffl pattern recognition ffl regression ffl example described problems ffl function approximation ffl associative memories ffl feature mapping chapter preliminary conceptions hardware learning page ffl optimization ffl control ffl data compression shall predominantly interested applications classified classification regression problems 
important classes problems successfully solved artificial neural networks 
instance prediction splice sites human pre mrna molecules brunak pig grading heart jabri high energy particle detector track reconstruction devices 
problems supervised learning usually employed 
choosing learning algorithm system applicable broad range applications general purpose shall commit text implementation supervised learning algorithms 
implementation unsupervised learning interesting story 
gradient descent learning simple approach supervised learning ann differentiable neuron activation functions gradient descent cf 
appendix 
defining cost function measures cost network error adjust free parameters ann 
synapse strengths neuron thresholds slopes decreases rapidly 
gradient deltaw kj gammaj kj learning rate small positive constant 
real gradient descent requires cost function function training patterns tot ptn batch learning 
weights changed basis instantaneous cost function evaluated pattern line learning 
small learning rates methods equivalent compare gauss seidel method solving linear equations numerically press 
gradient descent optimization technique cf 
hertz 
notably ffl tendency get stuck local minima cost function ffl converges slowly improved conversion time algorithms conjugate gradient method quasi newton employed see press 
methods advantage order cost function derivatives gradient descent computed quite efficiently cf 
computing weight changes 
second order derivatives full hessian matrix approximations likewise improve learning time significantly hertz buntine weigend pedersen hansen 
methods simulated annealing quite interesting 
simulated annealing searches chapter preliminary conceptions hardware learning page weight space occasional uphill moves gradient methods get stuck local minimum 
expense algorithms compared simple gradient descent increased computational cost 
spite poor performance gradient descent vigorous interest neural network society 
thoroughly analysed tested connection artificial neural networks wealth improvements clean gradient descent emerged 
popular application people quite impressive results obtained method 
brunak see williams zipser 
gradient descent central state neural network art 
method quite simple maps topologically nicely vlsi suggest analogue hardware implementation possible 
motivation gradient descent shall fair chance implementation limited precision technology cf 
section chapters users application people know expect implementation buy solution 
noted supervised learning system 
gradient descent extended straight forward manner implement learning critic prediction control 
authors looked implementation learning analogue hardware 
gradient descent algorithms 
alspector instance simulated annealing scheme boltzmann machine card uses hebbian learning unsupervised learning kohonen feature mapping 
error back propagation usual network architecture applied classification regression problems multi layer perceptron mlp 
error back propagation learning algorithm rumelhart hertz chapter formulation gradient descent maps feed forward ann architectures 
multi layer perceptrons analogue vlsi point view number draw backs addition drawbacks gradient descent learning example ffl neuron derivative needs computed 
ffl sensitive offsets various signals notably weight changes 
ffl learning rate small convergence 
noted problems particular back propagation apply algorithms see section 
problems shall addressed chapters 
analogue vlsi point view back propagation number advantages applies algorithms example chapter preliminary conceptions hardware learning page ffl fully parallelizable 
ffl uses local signalling 
ffl relatively simple 
implementation back propagation analogue vlsi considered authors 
valle wang cho lehmann 
authors chosen derive new algorithms having implementation analogue vlsi mind 
alternatives back propagation include weight perturbation virtual targets weight perturbation considered standard algorithm analogue vlsi opposed standard algorithm back propagation simulated networks weight perturbation jabri flower difference quotient approximation gradient descent learning 
weight perturbation inspired fact back propagation usually requires times synapse hardware recall mode system cf 
chapter ii requires computation neuron activation function derivatives 
instantaneous cost function weight perturbation prescribes weight changes deltaw kj gammaj gamma kj deltaw pert delta gamma gamma kj delta deltaw pert deltaw pert weight perturbation constant possibly weight dependent 
direct way approximating cost function derivative additional advantage kind non linearity offset error recall mode network transparent algorithm impact cost function derivative included algorithm contrast implementations backpropagation usually requires reasonably linear synapses neuron derivatives compute cost function derivative 
assuming weights externally accessible little hardware extra signal routing required hardware implementation weight perturbation 
drawback algorithm computationally expensive training pattern time step fully parallelizable serial weight update 
problem addressed flower jabri summed weight neuron perturbation speed achieved expense storage requirement 
matsumoto koga algorithm similar weight perturbation oscillating weights concurrently different frequencies weight derivatives weight changes computed simultaneously 
procedure introduces new problems related accurate band pass filtering system bandwidth requirements 
noted weight perturbation applied network topology just mlps 
chapter preliminary conceptions hardware learning page virtual targets exactly gradient descent learning algorithm virtual targets mlp learning algorithm murray uses gradient descent layer locally 
weight change rule back propagation cf 
assuming quadratic cost function deltaw kj jg neuron error computed differently back propagation virtual targets neurons assigned target values training patterns 
targets hidden neurons initialized random values developed training deltad tgt jk tgt target learning rate 
pattern successfully learned algorithm cease react pattern hidden neuron activations drift causing pattern unlearned classification pattern forgotten training reaction pattern resumed 
explicitly target values hidden neurons improve learning speed algorithm possess ability jump local minima 
disadvantage algorithm requirements hidden neuron access target storage furthermore scheme complicated instance back propagation weight perturbation 
central state neural network art implementation gradient descent back propagation learning algorithm analogue vlsi important issue integrated ann research 
problem shall address chapter emphasize issues hardware cost derivative computation weight updating schemes 
real time recurrent learning popular pattern recognition feed forward ann architectures limitations 
general set architectures recurrent networks recurrent artificial neural networks 
constraints put connections instance symmetric connections architectures recurrent network architectures potentially powerful 
ability example deal ffl temporal information ffl storing data ffl attractor dynamics chapter preliminary conceptions hardware learning page supervised learning read gradient descent train recurrent neural networks taught recognize sequential structures 
grammars smith zipser imitate finite 
turing machines williams zipser simulate strange attractors 
mackey glass series 
recurrent neural networks say tapped delay line perceptrons 
pedersen hansen temporal pattern recognition task dependent unknown temporarily wide distributed input samples recurrent network solve task fewer connections important especially relatively small training data set available 
application driven part motivation implementing recurrent network learning scheme prediction splice sites pre mrna molecules mentioned section solved neuron rtrl network brunak hansen 
trouble recurrent networks usually hard train 
available users ann system potent possibilities recurrent neural networks shall addition implementation backpropagation learning investigate implementation learning algorithm ann architecture connected recurrent way 
examples gradient descent algorithms recurrent networks exist literature see 
hertz 
compromise generality ann architecture absolutely necessary choose algorithm general applicability 
real time recurrent learning rtrl williams zipser chapter choice 
algorithm number advantages ffl general ann architecture 
rtrl algorithm formulated completely general ann architecture fully interconnected network 
network organize reflect structure application learning 
structure problem solved known priori reflected network architecture algorithm teach constrained architecture 
generality ann learning chip set important possible implement ann learning chip set applicable network topology 
ffl application invariant 
network topology size determined learning scheme determined independent application 
storage requirement learning algorithms back propagation time time dependent recurrent back propagation proportional maximum sequence length memory system needs processed 
application independent learning hardware architecture important general system 
ffl real time training 
flight training 
algorithms rtrl training phase recall phase rtrl functions inthe flight training system 
essential adaptive systems important analogue implementations general storing training patterns batch learning hostile analogue implementation storage probably digital ram compliance real world interface requirement analogue learning hardware 
chapter preliminary conceptions hardware learning page course training patterns held store teaching environment system resides able generate representative learning sequences system 
ffl hardware compatible 
algorithm parallelizable fully partial turns architecture maps nicely hardware 
furthermore computationally fairly simple algorithm suitable analogue hardware 
ffl powerful 
range impressive problems solved rtrl cited examples solved rtrl 
number disadvantages ffl computation requirements 
rtrl requires order computation primitives training example required number examples scale order computation primitives train network 
major drawback rtrl 
rtrl requires parallel computing relatively small systems 
ffl memory requirements 
rtrl requires memory order 
semi parallel implementation limit network size 
ffl 
recurrent networks harder train feed forward networks type suffice 
argue 
tsoi completely general fully connected architecture hard train select general recurrent architectures 
note priori knowledge kind problem hand general architecture selected known solution problem 
chapter shall investigate implementation rtrl algorithm emphasize issues hardware cost derivative computation weight updating schemes back propagation implementation 
usually meant line back propagation employed example similar flight line manner paramount importance hold training patterns store mlp back propagation 
chapter preliminary conceptions hardware learning page hardware considerations implementing artificial neural networks limited precision technologies analogue vlsi considered fairly straight forward matter 
inherent adaptability ann systems accommodate non 
learning algorithms 
authors noted learning algorithms presently available analogue designer typically sensitive certain kinds non displayed instance analogue vlsi 
section shall look important ones 
discussion primarily concerning gradient descent algorithms issues specific kind algorithms derivative computation probably generally applicable weight change offset 
allowable non ideality magnitudes dependent application topology size dependent 
observations qualitative 
weight discretization ann implementations connection strengths discrete number values consequence weight refreshing scheme digital non volatile weights analogue digital hybrids 
discrete number weight values limits problem space solvable network course words degrade performance xie jabri lehmann 
important fact smallest weight change restricted lsb weight values computed gradient descent learning algorithms instance constituted small weight changes 
higher weight resolution required learning recall mode 
meet demand assuming ann weight resolutions tailored recall mode learning hardware access high precision version synapse strengths network hollis morgan see section 
procedure probabilistic rounding computed weight changes smaller lsb lsb weight change carried probability lsb fahlman 
see section 
dynamic range trained gradient descent algorithm instance 
pattern recognition problem synapse strength magnitudes tend grow time 
easily exhaust limited dynamic range synapses see section 
logarithmic coded synapse strengths hollis increase effective dynamic range 
weight decay employed neuron gain increased learning hollis postpone weight exhaustion 
chapter preliminary conceptions hardware learning page derivative computation gradient descent algorithms need neuron derivatives compute cost function gradient 
major concern analogue implementations 
authors reported learning take place approximate neuron derivative calculations 
valle 
learning trajectory follow gradient case course 
property calculated neuron derivative possess right sign 
typically problem saturated sigmoid neurons actual derivative close zero calculated derivative negative gradient descent weight updating rule result hill cost function climb possibly bringing neuron deeper saturation lehmann krogh see 
small positive offset deliberately introduced neuron derivative calculation circuit prevent hazard lehmann 
enable saturated neurons taught gradient descent algorithm prevented zero derivatives 
offset errors possibly problematic issues analogue vlsi implementations ann learning algorithms offset errors 
offset errors signals instance neuron net inputs insignificant completely prevent learning signals 
sensitive offset errors weight change offsets 
weight change offset error deltaw ofs comparable sense typical weight change deltaw weights develop time kj kj deltaw ofs governed learning algorithm sufficiently large deltaw ofs learning impossible lehmann 
neuron error offsets 
offsets errors output neurons just displaces target values serious analogue outputs 
offsets errors hidden neurons severe offsets cause learning take place hidden neurons output error zero 
solution training problem stable state system lehmann see murray 
cost function offsets 
weight perturbation learning controlled computation cost function offset errors quantity cause solution training problem unstable state network degrades learning 
learning rate simulated networks learning rate usually chosen quite small learning 
smaller typically compatible analogue vlsi presence weight discretization weight updating offsets learning rate large effects small compared typical weight changes tarassenko 
learning scheme refresh purely capacitive synapse storage typical weight change large compared memory rate see hansen salamon lehmann hansen chapter preliminary conceptions hardware learning page 
noise analogue systems noisy 
noise doubt nuisance analogue signal processing systems advantage ann learning systems 
limited resolution 
signal noise ratio analogue systems comparable limited resolution number signal processing bits digital systems 
presence noise improve learning system jump local minima occasional random movements improve generalization ability network forced locate underlying structure training data presence noise improve fault tolerance information tend spread evenly synaptic connections ed wards murray jim see hertz qian sejnowski 
page chapter implementation chip backpropagation inclusion back propagation learning ann chip set small amount additional hardware objective chapter 
learning algorithm described shown mapped ann architecture hardware efficient solution 
design experimental vlsi chip set synapse neuron chip measurements 
design complete back propagation system including learning hardware chips 
problems relation derivative computation learning system digital weight backup discussed 
novel non linear back propagation learning algorithm displayed show algorithm nice properties relation analogue hardware implementation hardware low cost implementation 
reflections chopper stabilization technique elimination offset errors proposed inclusion algorithmic variations system outlined 
summary concludes chapter 
back propagation algorithm error back propagation learning bpl algorithm supervised gradient descent algorithm cf 
appendix 
section describe basic algorithm display modifications typically applied 
chapter implementation chip back propagation page basics error back propagation learning algorithm layered feed forward neural network multi layer perceptron mlp cf 
appendix described follows hertz rumelhart input vector time write neuron activation layer kj assume gamma neuron biases implicitly connection strengths constant inputs 
set target values neurons output layer define neuron errors quadratic cost function gamma jk ffi weight errors deltas defined ffi discrete time line learning updating scheme connection strengths changed weight updating rule kj kj deltaw kj kj jffi learning rate 
variations serving new mlp learning algorithms efficiency basic back propagation algorithm questioned authors algorithm slow network gets stuck local minima 
fahlman hertz haykin 
reason wealth back propagation algorithms improvements algorithm emerged 
improvements alter basic topology algorithm easy incorporate vlsi architectures shall describe shortly 
cost incorporation dependent exact implementation 
digital analogue weights parallel serial weight update 
common modifications algorithm chapter implementation chip back propagation page applied learning algorithms include hertz haykin plaut krogh hertz solla fahlman ffl weight decay 
modifying weight updating rule kj gamma kj deltaw kj delta gamma ffl dec ffl dec weight decay parameter discourages large weight magnitudes eliminates small 
unnecessary weights 
improves generalization ability 
analogue vlsi point view discouraging large weights advantageous limited dynamic weight range exceeded 
ffl momentum 
modifying weight change implicitly defined deltaw kj ff mtm deltaw kj gamma jffi ff mtm momentum parameter averages random weight changes magnifies consistent ones 
reduces oscillations learning 
disadvantage momentum need additional memory especially severe vlsi implementation 
ffl cost function 
standard quadratic cost function cf 
appendix leads weight errors ffi 
typical sigmoid activation function weight errors close neuron net input numerically large large neuron errors weight change take place 
problem eliminated entropic cost function perturbation 
resulting heuristically derived weight error ffi gamma fl delta fl derivative perturbation 
entropic cost function ffi 
theoretically weight errors modified output layer 
analogue vlsi implementation derivative perturbations weight errors ensure fl probably calculated non ideal hardware destructive learning process 
adaptive systems especially important learning take place neurons saturated 
confident decision fire system functionality changes time 
case entropic cost function perturbation superior quadratic cost function 
incorporation alternative cost functions vlsi implementation straight forward 
ffl dynamic learning rate 
learning rate important parameter large gradient descent leads oscillations small gradient chapter implementation chip back propagation page descent converges slowly 
adapting learning rate increase cost function set weight changes deltaj gamma gamma reduce problem deltaj deltaj gamma deltaj gamma gamma deltaj constants 
impossible implement analogue vlsi dynamic learning rate scheme somewhat complex requires memory addition cost function simple order computable 
hardware implementation ensure learning rate decrease critical value crit weight changes insignificant compared weight discretization weight errors switch learning 
ffl eta finder 
avoid complexity dynamic learning rate choose optimal learning rate 
propose gammay max ymax gammaz max max max max fi layer learning rate effective number inputs fan layer fi neuron transfer function steepness 
prefer hertz oe combined dynamic learning rate rule 
non reconfigurable network learning rates computed advance current network architecture 
reconfigurable network additional hardware reconfigurable block needed learning rate computation automated 
ffl batch learning 
doing real gradient descent weights updated epoch kj gamma epc delta kj nt epc gamma deltaw kj nt epc epc epoch length cf 
appendix 
usually line learning considered faster batch learning possible weight updates larger chunks scheme potentially sensitive offset weight discretization important analogue vlsi implementation valle 
disadvantage additional memory needed 
note line learning converges constant learning rate weights stir optimal solution white battiti 
chapter implementation chip back propagation page mapping algorithm vlsi mlp recall mode equation written noted section see lehmann widrow lehr 
likewise write neuron error equation gamma ffi notice important properties equations matrix calculate neuron error transposed calculate neuron net input 
ii signal flow reversed 
chip implementation back propagation algorithm means calculate neuron errors signals propagate layer layer gamma matrix vector multiplier topologically identical recall mode synapse chip positions inputs outputs exchanged 
words expanded version synapses 
neuron chip turn able calculate ffi 
schematic back propagation synapse 
additional current output multipliers basically needed compared recall mode synapse 
schematic back propagation neuron 
additional neuron derivative computing block multiplier needed compared neuron 
block diagrams expanded synapse neuron seen figures respectively 
synapse included hardware calculating weight change deltaw kj 
expanded synapse voltage inputs current outputs just original synapse 
mapping algorithm silicon gives order improvement speed compared serial approach weights updated simultaneously 
back propagation silicon systems similar architectures reported lately valle wang cho 
enabling neuron route back signal ffi wire architecture realize hebbian back propagation hebbian hybrid algorithm cho 
placing weight updating hardware synapse sites placed neuron sites reduces amount weight updating plain hebbian learning uses weight changes deltaw kj jy hertz 
chapter implementation chip back propagation page hardware order 
case weights neuron layer gamma neurons layer updated simultaneously procedure give order improvement speed compared serial approach 
noted needed calculating deltaw routed sites weight updating hardware calculated needed 
efficiency scheme highly dependent chosen weight storage method simple capacitive storage back memory weight change typically applied amplifier see card wang correspond amplifier avoid weight degradation destruction charge redistribution bus lines amplifier kind shielding circuit implementation reducing amount saved hardware 
digital memory usually read non destructively similar penalties capacitive storage digital backup memory digital storage objective learning algorithm cases modify digital memory 
saving weight updating hardware particularly important weight modifications digital domain converter needed modification site 
placing converters bit precision synapse unrealistic area consumption large 
systems digital ram backup memory weight access probably serial weight updating hardware performance loss orders placed neuron chips single 
separate module 
case cost weight update converter insignificant weight updating serial order speed improvement 
weight updating module advantage low cost implementations certain algorithmic improvements ones related weights neurons additional synapse hardware needed complex updating scheme 
implementing momentum instance basically requires memory adder leaky integrator continuous time synapse site fully parallel weight updating scheme 
weight updating module adder required 
weight change matrix placed standard digital ram cost far additional synapse hardware 
digital weight backup memory digital weight memory place additional data converters needed 
weight updating scheme implementations potentially better accuracy fully parallel ones see 
chapter implementation chip back propagation page hardware efficient approach architecture major drawbacks silicon area number synapses reduced compared number synapses recall mode system multipliers 
recall mode synapse hardware lies idle course undesirable 
ii number wires synapse neuron chips doubled compared recall mode system 
disadvantages severely restrict applicability adaptive neural network physical size importance 
fortunately possible overcome disadvantages mrc operated forward mode 
standard mode operation shown chapter 
mrc operated reverse mode 
circuit symmetry extra synapse hardware required alternating forward reverse mode operation 
studying synapse multiplier section repeated convenience notice perfectly symmetric 
apply differential voltage ffi output nodes ensure virtual short circuit input nodes difference current injected input nodes gamma gamma ffi mrc ffi illustrated implies enable original matrix vector multiplier cf 
section perform multiplication transposed matrix 
calculate gamma ffi simply exchanging output current input buffers 
see modified matrix vector multipliers cascaded valid multiplication transposed matrix performed inputs outputs exchanged 
weight updating hardware placed neuron sites implement back propagation learning algorithm extra hardware synapse sites 
hardware cost mere order 
routing neuron module possible ffi wires chapter implementation chip back propagation page switches 
significant extra signal routing 
noted time multiplexing ffi wires obviously requires discrete time system 
digital weight backup memory learning algorithm required run discrete time anyway restrict applicability scheme 
neurons corresponds modified synapse chip look quite output sampled cf 
section 
implementation low hardware cost analogue neural network chip back propagation section 
principal operation back propagation system illustrated normal operation chips forward mode response input pattern propagated output certain delay 
synapse weight kj layer updated chips previous layers operate forward mode produce chips layers layer operate reverse mode produce synapse chips layer operate route mode route inputs neuron chips layer turn operate learn mode calculate kj 
noticed newly updated weights back propagating errors reverse mode 
exactly comply learning algorithm small learning rates difference indistinguishable 
expect faster learning synchronous weight update equivalent gauss seidel method solving linear equations numerically press 
back propagation system 
different operation modes back propagation chip sets shown layer network middle layer updated 

order control lines assuming weight updating module weight updating module placed synapse chip segment row neuron chip avoid inter chip routing deltaw chapter implementation chip back propagation page exploiting bidirectional properties mrc synapse multiplier possible placing weight updating hardware synapse sites maximum speed improvement 
weight updating rule gamma needed updating kj ffi distributed simultaneously synapse chip ignoring synapse multipliers multipliers placed synapse sites access appropriate signals calculating deltaw 
simultaneously layer 
words expense discrete time possible eliminate extra inter chip connections synapse learning components compared straight forward fully parallel implementation back propagation algorithm top vlsi mlp 
shall see section possible reduce additional neuron hardware 
additional advantage bidirectional usage mrc process variation insensitivity transistors synapse multiplication recall mode back propagation mode 
assuming input voltage close output voltages figures conduct current 
forward mode differential output current gamma reverse mode gamma matching forward reverse currents necessary match 
ideally conduct current removed requires low neuron input impedance cf 
flower jabri 
chip design basic idea ann chip set chip back propagation developed institute bidirectional properties mrc 
section shall describe chip set 
description chip set published lehmann 
noted chapter intention add learning hardware acting recall mode ann 
described chapter 
shall capacitive storage digital ram backup memory prevents parallel weight updating scheme 
additional hardware cost serial weight updating scheme small replace original ann chip set containing chip back propagation learning algorithm disregarded user wished 
original ann chip set shall hyperbolic tangent neuron activation function 
case implementation original ann back propagation chip set designed test functionality hardware efficient approach 
layout possible design reused corrected improved reused 
design details appendix 
chapter implementation chip back propagation page synapse chip computing elements back propagation synapse chip forward mode seen identical computing elements second generation synapse chip 
writing synapse storage capacitors done way generation synapse chip pre charged row column selectors nand gates synapse sites determines synapse globally distributed weight voltage written 
synapse schematic little different original explicit storage capacitors synapse multiplier gate channel capacitances act memory 
second generation synapse chip 
reverse mode back propagation operation reverse signal flow exchange buffers current 
current implemented supply current sensed op amps op amps voltage buffers reverse mode 
way components needed row column chip op amps current mirrors switch transistors plus row column decoders synapses 
basically increase op amp row column back propagation implementation 
reasonably sized switches voltage drop dozens synapses source current nonnegligible say mv 
order ensure proper input voltage buffering virtual synapse output short circuits necessary put switches inside chapter implementation chip back propagation page high gain loops ensure zero matched switch transistor currents 
case transistors need matched 
switch transistor placement row column elements appendix notice elements identical control signals permuted 
neuron errors computed input layer input layer synapse chips need able run reverse mode 
reason hardware efficient architecture compatible sparse input synapse chip mentioned section 
sparse input synapse chip functionality extended include routing inputs outputs 
neuron chip stated section output voltage second generation synapse chip close voltage ref avoid dc common mode currents synapses 
neuron output referred voltage necessary separate bipolar pair output range mrc current mirrors ensure sufficient emitter collector voltage bipolar pair 
principal neuron schematic unaltered 
schematic second generation hyperbolic tangent neuron shown extra current mirrors inevitably cause increased neuron output offset 
second generation hyperbolic tangent neuron 
simplified schematic 
resistors implemented generation neuron 
extending second generation neuron chip chip back propagation enable neurons compute weight errors ffi serial weight updating scheme selected chosen place weight unfortunately overlooked design time causing synapse weight offsets larger necessary different forward reverse modes 
chapter implementation chip back propagation page updating hardware neuron chip 
strictly speaking unnecessary neuron chips system 
convenient place ffi delta multiplier near physical location ffi signals 
back ram organized parallel accessible banks ram semi parallelism exploited 
block diagram back propagation neuron seen detailed circuit schematic refer appendix 
crucial functionality learning algorithm offset errors weight change signals small cf 
section 
reason hardware offset compensating weight change signal indispensable 
current system uses serial weight updating hardware new weights ultimately written digital backup memory strong motivation keep weight change calculating hardware 
digital hardware part system reduce problem weight change offset cf 
section 
back propagation neuron 
block diagram 
positions switches forward reverse learn mode indicated 
elements dashed line weight updating hardware common neurons 
hyperbolic tangent neuron activation derivative calculated calc gamma operation dimensional inner product multiplier topologically identical ipm recall motivations analogue hardware anns place included size advantage massively parallel system power advantage especially small systems 
scaling hardware important 
maintain purely analogue system cost 
course data converters needed digital circuitry included expense large 
small low power systems fully analogue system solution 
chapter implementation chip back propagation page output 
versatile component core computing circuitry thesis 
power multiplication addition subtraction division tree identical output independently process parameters sk gamma gamma wk gamma gamma gamma gamma wk gamma gamma vc gamma vc gamma sigma sigma sigma sigma vc sigma inputs cf 
gamma gamma 
easily configured compute desired function level inserted sigma vc sigma inputs ensure transistors operate region 
weight errors ffi computed dimensional mrc ipm 
inputs synapse chip buffered neuron chip low output impedance forward reverse mode 
switches redirect signal flow various operating modes simple mos transistors reasonably sized transistor cf 
appendix technology drive pf load bit accuracy ns larger capacitive loads feedback lower switch impedance cf 
synapse chip row column elements 
operating reverse learn mode neuron net input unavailable 
neuron activation sampled forward mode order provide data calculation weight error 
synapse chip provides neuron error current 
delta multiplier needs voltage inputs need neuron error input 
implemented mrc plus op amp neuron input scale see 
neuron errors computed differently output layer preceding layers 
accommodate adding second mrc neuron error 
transforming dimensional ipm activating mrc output layer see 
acting ipm circuit take difference applied input voltage chip neuron activation 
words output layer shall provide target value voltage neuron error current 
chip measurements neuron back propagation neuron chip theta synapse back propagation synapse chip fabricated standard cmos process 
section shall measurements chips published lehmann lehmann 
table important chip characteristics appendix 
chapter implementation chip back propagation page synapse chip measured forward mode synapse transfer characteristics single synapse seen reverse mode characteristics synapse seen non linearity approximately equal generation synapse non linearity expected 
chip output offset current forward reverse mode quite large magnitude comparable maximal synapse output current 
responsible output offset row current presumably current buffer 
appendix expect large output offset current differencing rows synapse 
output current conveyor designed sink current large number synapses gives output offset chip scaled 
synapses added significant increase output offset current 
comparing second generation synapse chip output offset generation chip notice reduced factor simpler current differencing circuit 
reduced synapse maximum output current compared generation chip necessary dedicate synapse row offset canceling extra bias synapse order exhaust neuron bias synapse 
operation reverse mode offset errors probably larger tolerable back propagation algorithm offset canceling employed cf 
section section 
forward mode synapse characteristics 
measured transfer functions single back propagation synapse different synapse strengths 
reverse mode synapse characteristics 
measurements synapse reverse mode 
output offset errors canceled 
synapse chip weight resolution bit sufficient range applications 
effective weight offset somewhat higher strongly correlated synapses connected cf 

offsets caused inability synapse row column elements ensure virtual short circuit synapse output presumably caused mismatch reconfiguring switch transistors 
systematic offset canceled offset chapter implementation chip back propagation page generation synapse chip 
non ideality row column causes synapses different offset forward mode reverse mode 
undesirable magnitude offset probably tolerable learning scheme offset error corresponds bit recall mode weight accuracy 
sc row sc col weight offset lsb forward mode weight offsets 
sample chip 
notice correlation synapses row emphasized contour plot dashed lines 
sc row sc col weight offset lsb reverse mode weight offsets 
sample chip 
notice correlation synapses column dashed lines contour plot 
neuron chip measured forward mode neuron transfer characteristics seen figures increased output offset compared generation neuron chip noticed 
non linearity cf 
offset errors little importance recall mode performance generation chip set 
increased output offset compared generation chip caused additional current mirrors cf 

shows electronically computed neuron derivative calculated neuron chip dy ds gamma cf 

asymmetry caused output offset neuron transfer function input offset derivative computing gamma block 
note neuron input offset affect accuracy computed derivative case derivative computed basis neuron input 
non linearity derivative computing block cf 

total non linearity computed derivative 
offset errors related derivative computation quite large causing computed derivative negative particularly poor specimens neuron destructive learning process 
including derivative perturbation weight error calculation mentioned hope offsets tolerated learning scheme 
experimentally proven course 
note authors employed coarse derivative approximations observed learning progress 
valle see cho 
derivative perturbation easily inserted substituting worst case max calculating derivative dy ds max gamma procedure chapter implementation chip back propagation page generation chip set neuron output variation larger second generation chip degrade performance 
forward mode neuron characteristics 
measured transfer characteristics different input scale voltages computed neuron derivative 
measured derivative computed chip 
asymmetry caused neuron output offset 
neuron net input ua neuron output neuron transfer function different neuron transfer functions 
measured stair case fitted smooth tanh functions indistinguishable 
neuron net input ua non linearity neuron tanh non linearity different neuron non linearities 
measured fitted tanh relative difference 
measurements chip set indicate inclusion offset canceling certain signals chip set able function core discrete time analogue back propagation neural network chip set functions chapter implementation chip back propagation page neuron output derivative derivative calculator transfer function different parabola transfer functions 
measured stair case fitted smooth derivative computing 
neuron output non linearity parabola non linearity different parabola nonlinearities 
measured fitted parabola relative difference 
predicted 
variations gains offset errors hopefully canceled learning process 
neuron sampler rate 
measured rate different starting points sigma 
notice linear decay caused mismatched diode reverse currents 
sign magnitude rate vary neuron neuron 
synapse strength rate similar appearance 
synapse neuron chip propagation delays give layer propagation delay lpd gives recall mode speed synapse chip layer gcps full size theta synapse chip 
note reconfiguring switches neuron output degrade performance compared generation chip current capacitive load 
neuron chip takes calculate new weight giving learning speed approximately back propagation system 
rate neuron activation sampler cf 
limit system size delta connections bit accuracy neuron activations 
applications delta connections known cf 
chapter brunak 
problem digital refresh sampled neuron activation employed neuron sampler extra input chapter implementation chip back propagation page purpose improving derivative computation apart synapse chip output offsets weight change offsets canceled auto offset canceling scheme concerning problem chip set calculation neuron derivative concern authors jabri flower hollis authors take advantage fact derivative need computed accurately learning proceed coarse derivative approximations valle 
ensure non negative result computation derivative perturbation mentioned employed 
alternatively addition clamp output zero negative dimensional ipm second generation synapse chip ipm calculate gamma output current 
mirroring current simple current mirror ensure output negative additional hardware needed ensure reasonably input impedance speed 
possible necessarily considerable amount additional hardware employ auto offset canceling techniques cancel offsets 
elegant approaches come offset related problems derivative calculation possible back propagation chip set stated section probably access neuron net inputs calculating neuron derivatives learning algorithm 
integrating learning algorithm chip add module access neuron net input 
implies choose neuron transfer function freely long derivative computable 
propose approximate derivative differential quotient deltav gamma gamma deltav deltav deltav small voltage 
block diagram circuit general differential voltage differential current neuron shown output current neuron output switches open deltav derivative approximation switches closed 
neuron instance differential pair 
case magnitude possible negative output calculating derivative determined matching tail current sources 
mismatch realistic small derivative perturbation added ensure derivative calculated strictly larger course 
differential pair implement neuron transfer function advantage terms speed compared hyperbolic tangent neuron 
accuracy difference quotient hardware efficient systems parallel weigh update neuron activation deterioration problematic course 
chapter implementation chip back propagation page vref differential quotient derivative approximation 
arbitrary differential voltage differential current transfer function blocks approximating derivative switches closed transfer function switches open 
approach brought reasonably large deltav neuron transfer blocks calculate derivative single block combined switched capacitor shall see section 
reduces transistor count 
approximating circuit calculating derivative inherently positive output uses fewer transistors reported lately 
apparent difficulties computing activation function derivatives needed gradient descent inspired authors derivative information instance substituting derivative completely different optimization techniques 
see 
battiti krogh 
shall elaborate solutions objectives approximate standard algorithms known properties 
assuming monotonous activation function 
knowledge sign derivative sufficient convergence decreasing learning rate compare stochastic approximation gelb 
chapter implementation chip back propagation page system design hardware ann chip back propagation included back propagation chip set 
complete system additional hardware needed 
ffl digital weight backup memory 
ffl order scaling hardware 
converters accessing backup memory weight updating hardware 
including ffl finite automaton control system weight refresh applying inputs controlling learning scheme 
ffl environment place ann 
section shall describe complete system 
ease test embed ann digital pc interface pc master finite automaton finite state machine fsm 
solution allow test speed system pc isa bus necessary converters bottleneck system terms speed 
recall mode learning mode speed performance tested 
reason system run speed indicated measurements individual chips 
circuit level system performance hand easily tested high degree programmability artificial pc environment 
design time important 
real application environment electrical analogue domain finite automaton quite simple digital asic possibly including converters 
system designed rtrl back propagation learning ann hybrid rtrl system cf 
chapter shall second generation back propagation ann chip set 
back propagation included system free eliminating cost pc interface separate backpropagation system 
consequence back propagation ann architecture determined rtrl ann architecture designed specific application 
test acceptable 
complete system schematic enclosure iii see appendix 
scaled back propagation synapse chip back propagation mode system realizes layer perceptron 
architecture require theta synapse chips necessary scaled back propagation synapse chip theta fabricated 
unfortunately particular mpc run process parameters outside specified ranges channel process parameter instance low compared nominal value 
presumably threshold voltages numerically large explain reduced dynamic range components tested previous mpc runs 
impacts chip primarily reduced input range large systematic offset errors see appendix 
raised chapter implementation chip back propagation page voltage external current offset compensation circuits hope system spite poor quality chips 
asic interconnection scaled theta synapse chips synapse chip count reduced interconnected system operates back propagation mode shown synapse chips drawn having architecture convenience 
input lines output lines matrix synapses driven dc voltages reserving rows columns layer neuron thresholds forward mode offset compensation indicated 
strictly speaking reverse mode offset compensation unnecessary input layer 
prior learning matrix vector multiplier outputs measured forward reverse mode offset compensation synapse strengths adjusted minimize offsets 

offset threshold synapses nc sc nulls nc nc nc sc sc bias bias bias bias back propagation ann architecture 
ann chip interconnections system operates back propagation mode 
blocks sc nc synapse neuron chips respectively 
input output lines sc blocks accessible top bottom left right 
synapse strength backup memory bit ram 
authors addressed problem weight discretization anns hollis tarassenko lehmann lansner brunak hansen see section 
bit resolution learning system compliance reported simulations 
subsequent learning necessary weight resolution lower usually relative weight magnitudes exact weight values determines system behavior learning small weight changes need accumulated 
refreshing synapse strengths synapse chip ram bit dac 
actual ann weight discretization reduced bit accumulate weight changes small bit discretization 
ann performance scheme superior system weight discretization reduced bit bit learning train actual final network intermediate network artificial high weight resolution see lansner 
chapter implementation chip back propagation page weight updating hardware offset values weight resolutions typically analogue vlsi systems restrict learning rate range somewhat higher employed software simulations cf 
sections tarassenko krogh hansen salamon hertz weigend see williams zipser 
weight updating scheme high precision weight backup memory reduce influence weight change offsets weight change discretization reduce minimum learning rate adding weight change old weight digital domain 
adc convert analogue weight change signal digital form adding padded zeroes bit weight scale effective weight change offset factor illustrated actual schematic slightly complex bipolar weights weight changes needs handled overflow digital adder prevented 
multiplexor included allow test chip analogue weight updating hardware cf 
enclosure iii 
backup ram kj adc kj kj dac kj digital weight updating hardware principle 
digital part uses higher precision offered data converters reducing effective learning rate 
studying back propagation neuron block diagram see deltaw kj signal directly accessible applying kj ffl dec signals neuron chips zeros gives desired weight change output inevitable larger offset internal deltaw kj 
synapse chip outputs deltaw kj output offset compensated prior learning applying zero inputs learn mode adjusting kj signal output zero 
addition small hardware cost advantage serial weight updating scheme possibility employ advanced accurate 
hardware hungry weight updating scheme extreme hardware 
learning algorithms inherently serial weight updating 
weight perturbation weight change offset large coarser discretization preferably weight change offset lsb 
things equal adding additional hardware analogue domain inclusion advanced updating scheme increase weight chapter implementation chip back propagation page take advantage instance weight updating scheme 
updating scheme requires weight storage digital weight backup course large systems choice serial weight updating scheme 
back propagation system time writing construction 
unfortunately system level experiments 
hopefully available near 
non linear back propagation seen major concerns implementing gradient descent learning algorithms hardware computation neuron derivatives 
different approaches approximate derivative proposed literature difference quotient locally globally computed approximating approaches perturbations reducing offset related errors implementations largely ignoring derivative 
implementation related difficulties motivated development new gradient descent algorithm non linear back propagation nlbp derivative computation avoided hertz 
section shall display algorithm show incorporate existing backpropagation architecture 
derivation algorithm derivation non linear back propagation framework recurrent backpropagation hertz 
feed forward case recall weight updating rule define weight change deltaw kj jffi jg ff ff ing errors 
reducing primary concern learning scheme implementation advocates simple weight updating scheme analogue domain 
argued presently realistic way implement advanced updating schemes order updating scheme digital domain 
chapter implementation chip back propagation page call ff nlbp domain parameter 
basic idea non linear back propagation interpret equation order tailor expansion equation deltaw kj ff gamma ff delta gamma valid small ff 
redefining weight error definition ffi nk ff gamma ff delta gamma ffi nk nlbp weight errors nlbp weight change equation form original back propagation equation deltaw kj jffi nk nlbp domain parameter ff large tailor approximation requires high precision compute 
ff small algorithm numerically stable taken far gradient descent behaviour 
think ff range ff 
numerically stable limit interesting vlsi implementation limited precision technology takes simpler ffi nk gamma ff ordinary back propagation chose ffi nk output layer wish entropic cost function 
training epochs squared error nlbp training error 
training error function learning epoch normal back propagation solid line non linear version ff dashed line ff dot dashed line 
note errors propagated non linear units neuron activations recall mode linearized units ordinary back propagation 
name non linear back propagation 
chapter implementation chip back propagation page training errors nettalk data set sejnowski rosenberg normal back propagation nlbp compared hertz 
performance nlbp similar ordinary back propagation simulations note usual algorithm variations weight decay momentum applicable nlbp 
hardware implementation expected superior ordinary back propagation addition subtraction addition neuron non linearity differentiation multiplication calculation nlbp weight error simpler bound accurate 
favouring hardware implementation indication nlbp superior ordinary back propagation large learning rates cf 
section 
hardware implementation difference ordinary back propagation non linear backpropagation way weight strength errors computed computed locally nlbp maps topologically hardware exactly way ordinary back propagation neuron implementation differ 
section shall show core neuron implementations ann chip nlbp learning reported hertz 
continuous time nlbp neuron bjt differential pair original neuron starting point implementing nlbp hyperbolic tangent neuron activation function leads schematic simplicity differential pairs shown implemented npn actual cmos implementation lateral bipolar mode channel previous designs course 
actual neuron activation function unimportant nlbp differential pairs favour speed 
lbm differential pairs probably favour accuracy salama 
notice circuit requires application negated neuron error gamma synapse chip compute reverse mode requiring simple modification 
interesting notice circuit structure identical cf 
compute neuron activation function derivative substituting gammav small constant voltage gamma deltav ffi output approximate deltav delta yk sk interpret nlbp way exploit implicit multiplication difference eliminates delta multiplier source errors 
small voltage opposed deltav inherent inaccuracies significant relatively 
consequently circuit nlbp gives better accuracy ordinary back propagation 
accuracy circuit 
weight error calculation determined matching differential pairs tail currents 
order output current magnitude see 
leary see chapter implementation chip back propagation page dd ss bias iy continuous time non linear back propagation neuron 
differential pair possible activation function different 
precision determined matching differential pairs 
salama better far accuracy chips probably enhance performance significantly 
linear needed inputs outputs compatibility synapse chip 
degrade performance 
circuit functions continuous time substitute schematic back propagation neuron system uses fully parallel weight updating 
discrete time nlbp neuron actual shape neuron activation function irrelevant non linear back propagation need base implementation differential pairs 
far better approach circuits inherently current inputs voltage outputs needed 
function calculate neuron activations weight errors preferable hardware calculations eliminates need matched 
possible system required function continuous time output sampled introduces errors 
shown simplified schematic discrete time neuron reuses activation function block current inputs voltage outputs 
oe clock phase yk available output sampled capacitor 
oe clock phase ffi available output 
switched capacitor computes difference determines implement bump function radial basis function popular certain authors see 
hertz anchez lau function long implementation time invariant possibly reproducible 
complex activation function extra hardware prohibit implementation activation blocks neuron 
chapter implementation chip back propagation page accuracy circuit 
note output buffer needs linear offset error canceled switched capacitor 
neuron transfer function block arbitrary current voltage circuit static errors input current output voltage offsets block irrelevant 
design techniques reduce charge injection redistribution robert accuracy brought output voltage range 
bias bias dd max ss min neuron activation block discrete time non linear back propagation neuron 
simplified schematic 
time invariant inaccuracies affect performance circuit 
precision determined switched capacitor 
discrete time nlbp neuron directly replace computing elements original back propagation neuron assuming voltage buffer needed recall mode version neuron nlbp hardware overhead potentially extremely small consisting switched capacitor neuron sites addition modest hardware increase original hardware efficient back propagation synapse chip order weight updating hardware finite automaton algorithm controller 
operating output transistors transfer function block mode circuit output voltage exhibit reasonably smooth transition vmax min input current increased giving shaped transfer function 
circuit poor power supply rejection ratio 
realistic circuit shown insertion current mirrors signal paths gives better possibility lower input impedance 
avoid drawing current output range compromise rigidity simple amplifiers buffer nlbp require neuron output range defined large input offset vt amplifiers need match 
neuron steepness controlled input stage bias current transfer function simulation different bias currents seen chapter implementation chip back propagation page dd ss max min rgb ref min buffer rgb min max neuron activation block schematic 
improved 
outputs standard source follower current input stage drives transfer function shaping output transistors current mirrors 
output level buffers thought degenerated regulated 
ua ua ua ua ua ua iin squashing neuron improved date time run temperature neuron transfer function 
transfer function sigma power supply bias currents 
obviously system level experiments need carried order evaluate applicability proposed chip set especially respect derivative computation weight change offsets 
presently carried chapter implementation chip back propagation page institute 
implementation proposed discrete time nlbp neuron chip pin compatible back propagation neuron chip substitute back propagation system evident design task 
recall mode neural network design chapter design issues need consideration prior volume production 
considerations process parameter dependency canceling temperature compensation mentioned section apply back propagation chip set 
addition implementation high accuracy calculations low offset ones needed learning scheme requires investigation 
section shall discuss approaches reduce influence offset critical signals inclusion algorithmic variations 
chopper stabilizing critical signals ann learning scheme respect offset weight change 
straightforward solution mentioned section problem measure offset auto zeroing phase subsequently subtract offset 
solution major drawbacks requires memory ii offset free comparator see chapter 
especially systems fully parallel weight updating scheme drawbacks quite severe 
way eliminate weight change offset apply chopper stabilizing technique known operational amplifier offset cancellation hsieh allen polarity inputs outputs differential op amp synchronously periodically chp reversed moves offset error low frequency noise odd harmonics chopping frequency chp similar way periodically permute inputs output polarities ffi delta multiplier compute weight changes 
illustrated case weight updating multipliers placed synapse sites parallel weight 
assume weight updating multiplier differential inputs differential output multiplier 
gilbert multiplier computes deltaw ffi ffi inserting switch transistors input output reverses polarity corresponding control signal oe high doing successive weight updates oe oe oe ffi pulse width modulation gating oe oe signals contacts connected storage capacitor adjust learning rate shown 
multiplier bias current control learning rate 
chapter implementation chip back propagation page kj ofs kj dw kj ideal ss dd chopper stabilized weight updating 
principal schematic 
parallel weight updating indicated extra minimum switches needed synapse sites 
gives resulting weight change deltaw kj oe oe oe ffi gamma gamma oe ffi ffi ffi delta gamma gamma oe delta gamma oe jffi call multidimensional chopper stabilization 
note output switches placed way order approximation offset errors related switched differencing current mirror canceled 
placing switch transistors indicated see minimum switch transistors addition weight updating multiplier necessary synapse site 
switch transistors common row column synapses 
additional advantage possible offsets input buffers indicated canceled 
add chopper stabilizing weight change signal back propagation system stabilizer neuron chip required course 
exact offset cancellation data frequencies lower half smallest chopper frequencies 
discrete time system probably apply new data set oe triples offset cancellation expected 
chapter implementation chip back propagation page chopper stabilizing technique signals 
probably back propagated error signals 
chopper stabilizing signal back propagation synapse chip unfortunately somewhat difficult current conveyor differencing technique 
basing synapse current mode operational amplifier stabilizing possible switch transistors row column elements 
signals concurrently stabilized ones chopper stabilized say stabilize ffi computed neuron chip chopper frequencies need introduced 
ensure permutations chopper phases oe giving constant resulting sign weight change signal complete cycle 
including algorithmic improvements mentioned section advantage serial weight updating scheme advanced procedures inexpensively employed 
improvements system including relevant algorithm variations displayed section variations important consider 
weight change threshold addition offset compensation chopper stabilizing problem offsets weight changes solved introducing weight change threshold deltaw min proposed 
influence weight change offset severe ideal weight change close zero case offset desired weight change determines actual applied weight change 
effect weight space state drift away cost function minimum solution problem hand ideal weight changes zero 
consequences introduce weight change threshold weight changes ignored 
substituting deltaw kj deltaw kj ae deltaw min jffi quite easily incorporated digital weight updating scheme introducing errors 
momentum popular improvements back propagation momentum 
analogue domain assuming parallel weight updating momentum included adding leaky integrator output ffi delta multiplier 
momentum parameter ff mtm cf 
typically order offset errors associated multiplier integrator increased factor gamma ff mtm 
weight changes small severe problem prohibit inclusion momentum pure analogue systems 
digital weight updating scheme backpropagation system effective weight change offset chapter implementation chip back propagation page reduced compared analogue approach 
system hope increased effective offset error acceptable especially weight change threshold included 
choosing momentum parameter ff mtm enables easy implementation momentum back propagation system 
hardware needed simple digital adder digital ram deltaw bit wide number words weight backup ram 
deltaw bit discretization weight change signal memory resulting weight change signal deltaw training samples 
smaller desired applications 
ff mtm gives times longer memory multiplying digital hardware inconvenient 
way lengthen weight change memory apply ff mtm factor th sample call degenerated momentum deltaw kj ae ff mtm deltaw kj gamma jffi deltaw kj gamma jffi increases memory factor resulting weight change corresponding training data applied time approximately gamma ff mtm delta jffi 
hardware implementation scheme complicated simple choice ff mtm requires addition overflow control adder insertion arithmetic shift left hardware selective multiplication 
weight decay implementing weight decay system simple capacitive storage parallel weight updating scheme principle just matter making storage capacitor leaky 
placing resistor capacitor zero weight voltage 
weight decay small compared typical weight changes order prohibit learning 
krogh hertz example small weight decay parameter learning rate ratio ffl dec delta gamma probably insignificant compared typical weight change offsets 
accept weight decay large compared weight change offsets influence probably reduced addition advantages weight decay 
possible experiments show 
back propagation system order digital weight updating hardware situation different 
ffl dec delta gamma learning rate gives ffl dec gamma negligible weight discretization bit weight backup memory precision easily enhanced bit say accommodate small weight changes 
weight change offset larger lsb weight change adc weight change threshold needed 
inclusion weight decay system requires digital adder 
chapter implementation chip back propagation page improvements case recall mode ann chip set improvements back propagation chip set possible 
list obvious appendix 
summary chapter designed variation ann chip set including chip error back propagation learning 
basic learning algorithm displayed applicability common algorithmic variations implementation analogue vlsi discussed 
shown fully parallel implementation give order improvement speed compared serial solution 
shown exploiting symmetry mrc possible implement back propagation extra hardware synapse sites extra inter chip connections cost order speed solution 
digital ram weight back weight access restrictions reduce learning speed compared serial solution 
design back propagation chip set displayed 
improved current synapse chip neuron chip approximately twice complex generation recall mode 
excessively extra computing circuitry neuron chip 
measurements chip set displayed indicating learning speed 
measurements suggest range offset errors especially weight change signal canceled 
neuron derivative computation problematic certain offset errors 
improvements circuit proposed 
complete back propagation system design chip set displayed 
weights ultimately placed digital ram weight change hardware neuron chips implemented discrete digital hardware 
elaborated virtues solution 
reduced minimum effective learning rate weight change offset easy reliable implementation weight decay momentum 
system presently construction experimental results 
novel non linear back propagation learning algorithm displayed 
needing neuron derivative neuron circuitry algorithm superior original algorithm exact neuron transfer function irrelevant focus design effort electrical characteristics neuron 
possible back propagation neurons proposed continuous time nonlinear back propagation compatible discrete time system 
solution virtually extra hardware needed learning algorithm compared recall mode system 
chopper stabilization technique reducing offset errors proposed 
sample implementation reducing offset errors weight change signals computed chapter implementation chip back propagation page local synapse multiplier circuitry 
inclusion weight change thresholds momentum weight decay back propagation system outlined 
page chapter implementation rtrl hardware implementation add real time recurrent learning hardware ann chip set objective chapter 
learning algorithm briefly described shown mapped hardware compatible ann architecture realistic amount hardware 
results simulations modeling analogue vlsi non architecture displayed 
design experimental vlsi chip implementing learning hardware including offset canceling scheme critical weight change signal 
chip measurements 
design complete rtrl system done shown apply algorithmic variations system 
derive non linear version rtrl algorithm argue algorithm virtues non linear back propagation 
reflections continuous time rtrl system considered 
summary concludes chapter 
rtrl algorithm real time recurrent learning rtrl algorithm supervised gradient descent algorithm cf 
appendix general recurrent artificial neural network architectures 
section shall describe basic algorithm display modifications typically applied 
chapter implementation rtrl hardware page basics rtrl algorithm artificial neural network discrete time feedback discrete time recurrent artificial neural network cf 
appendix described follows williams zipser input vector time write neuron activation kj ae gamma set input indices set neuron indices 
neuron biases implicitly connection strengths constant input 
note discrete time feedback dependency time slightly different definition chapter 
doing usual gradient descent line learning weight updating rule form ij ij gamma ij ij gamma ij instantaneous cost function cf 
appendix 
idea rtrl neuron activation derivatives shown equal ij ij neuron derivative variables ij computed ij ij oe ij oe ij kl ij gamma ffi ik assumed teaching starts time introduced neuron net input derivative variables oe ij quadratic cost function resulting weight change equal deltaw ij ij ffi ik denotes kronecker delta 
chapter implementation rtrl hardware page neuron error defined ae gamma neuron target value time set neurons targets exist time target indices 
entropic cost function hyperbolic tangent activation functions shown weight change equal deltaw ij oe ij new fx dg pair applied time 
call completion computations learning cycle time step 
variations back propagation algorithm rtrl varied numerous ways 
variations alter topology algorithm easily applied vlsi implementation 
addition different cost functions shown variations include williams zipser smith zipser hertz brunak hansen ffl teacher forcing 
network taught dynamic behaviour altered qualitative manner teacher forcing employed instance network taught oscillate 
target values exist network outputs fed back tf gamma gamma gamma gamma correct derivative calculation case sum taken gamma 
incorporation teacher forcing vlsi rtrl implementation straight forward 
ffl relaxation pipelining 
rtrl network architecture cf 
implies delay number time steps input applied corresponding output seen 
instance network implement xor function organize layer perceptron 
compute gamma gamma 
applying target values network implicitly constraining network architecture choosing input output delay terms time steps 
delay just long enable sufficiently complex network organization unnecessary delay insertions degrades learning 
procedures possible introducing input output delay relax network number time steps input chapter implementation rtrl hardware page applied 
set delta delta delta tpd apply corresponding target time tpd tpd desired propagation delay 
exploit pipelined nature network architecture 
new input time step apply corresponding target time tpd gamma 
clearly latest method gives highest throughput 
harder train delays synchronizing may inserted learning algorithm 
choice relaxation pipelining effects algorithm control mechanism vlsi implementation 
true ffl learning subsequence 
interesting apply different independent sequences network regarding inputs continuous stream data 
avoid false correlations different sequences neuron derivative variables ij neuron states reset sequence 
nt seq sequences length seq 
related application priori temporal knowledge output known dependent latest input vectors ij reset enforce limited memory 
note cases tapped delay line feed forward ann solve task 
cases especially seq large recurrent net needs fewer processing elements 
ffl random initial state 
alternative setting ij ij nt seq initial neuron derivative variables set small random numbers ij ij ij uncorrelated noise sources 
white gaussian 
tendency speed learning small sequences 
ffl momentum weight decay standard learning algorithm variations mentioned section applicable rtrl 
notes applicability hardware realizations apply 
noted continuous time formulation algorithm possible cf 
section 
mapping algorithm vlsi topological mapping rtrl algorithm analogue vlsi published lehmann 
mentioned chapter aim implement learning algorithms analogue ann topology described chapter 
adding sample hold circuit feedback layer system recall mode chip set arrive discrete time recurrent network topology rtrl developed 
network architecture shown chapter implementation rtrl hardware page discrete time system 
block diagram 
system composed collection synapse neuron chips forming layer ann sample hold circuit feedback 
calculations training example needed rtrl algorithm performed fully parallel 
unrealistic construct system grows large 
say multipliers chip 
network fit synapse neuron chips need multiplier chips 
fully parallel weight update incompatible ann system serial access weight backup ram 
studying basic equations notice ij gamma gamma omega delta omega oe ij oe ij ij gamma ffi 
synapse weight matrix columns corresponding inputs omega denotes vector multiplication coordinates 
assumed tanh 
weight change equations written deltaw ij delta ij deltaw ij delta oe ij quadratic entropic cost function respectively 
implementing equations parallel divides operations space domain time domain 
division advantages ffl area computing parts learning hardware grow faster 
ffl calculations order determining ij performed matrix vector multiplier identical synapse matrix vector multiplier ann 
ffl additional hardware implemented signal slices 
ffl system 
principle expanded arbitrary size 
area derivative variable memory grows course 
chapter implementation rtrl hardware page ffl signal paths need lines 
ffl weight updating serial gives advantages mentioned chapter 
disadvantage space time division course system order slower fully parallel implementation 
block diagram proposed rtrl system seen matrix vector multipliers synapse weight memory derivative variables memory easily identified 
adders phi multipliers omega working coordinates select block sel chooses outputs target values multiplexor pair computes ffi 
delta vector inner product multiplier 
dash dotted signal path entropic cost function 
controlled digital finite automaton system operation follows learning cycle gamma sampled 
applied computed asynchronously 
fi jg ij gamma read ram ij ij computed stored respective rams 
discrete time rtrl system 
block diagram 
lines carry analogue signal vectors width indicated thickness digital signals 
comments topology indicated derivative variables meant placed digital ram 
digital ram selected physically small cheap reliable 
storage requirement grows size limiting factor system 
small systems feasible analogue storage 
large rams serial word access 
achieve required parallel signal ij necessary chapter implementation rtrl hardware page multiplex ram access 
speed limiting factor system precisely probably connected multiplexors limit speed reason analogue memory small systems 
follow learning algorithm strictly update analogue weight storage matrix vector multipliers full learning cycle completed 
large systems chip weight storage degraded time learning cycle 
come problem ram banks refresh new weights 
weight changes small reason believe periodical weight refresh weights partially updated weight backup memory prohibit learning cf 
section 
elements multiplexor digital weight updating hardware rams matrix vector multipliers operate coordinates vectors width elements placed width data path module 
inner product multiplier distributed signal slice module current output ensure 
system architecture comprised components ffl number synapse chips doing multiplication 
ffl number neuron chips applying tanh nonlinearities 
set chips act layer core neural network 
ffl way multiplexor width data path module 
ffl digital rams corresponding converters 
data converters chip chip components 
ffl width data path module weight updating hardware 
module digital finite automaton controlling learning scheme placed 
ffl number width data path modules total signal slices performing rest calculations 
add learning hardware existing ann control ann neuron output range compute neuron derivative gamma cf 
previous chapters 
implicitly requiring matrix multipliers width signal slices match inter chip matching 
turns matching important cf 

noted sparse input synapse chip mentioned section process network inputs multiplexor capable handling binary coded inputs 
chapter implementation rtrl hardware page system simulations lehmann simulations done influence various system 
non linearities offset errors quantizations selected signals investigated 
restrictions compliance authors learning algorithms 
back propagation weight perturbation hollis see tarassenko 
see section 
qualitative simulations ffl neuron output larger 
way neuron tanh derivative computed strict requirement 
computed derivative wrong sign 
non linearities transfer function tolerated extent 
simulations non linearities range gamma quadratic cost function entropic acceptable range generally smaller 
exact tolerable range depend problem network size number training cycles learning rate non 
qualitative interesting actual figures 
ffl synapse strength discretization sufficiently fine 
simulations bit 
ffl weight change offset small 
simulations delta gamma ffl neuron error offsets small 
applies non targeted neurons 
output neurons neuron error offset merely displace network output offset 
simulations error offset delta gamma addition constraints simulations showed ffl neuron derivative variable discretization coarse 
simulations bit levels gamma adequate situations 
ffl non linearities general large 
simulations 
ffl offset errors general large 
instance derivative variables 
soft requirements computing accuracy indicates instance inter chip matching important 
offset cancellation various signals hand necessary 
lower bound determined target values obviously neuron range 
chapter implementation rtrl hardware page chip design recall mode ann shall design rtrl hardware back propagation chip set chapter 
disregard back propagation operation modes implementation 
important design strategy rtrl system systems thesis reuse hardware 
ij multiplier course implemented synapse chips transistor level width data path module shall reuse hardware 
layout module originates back propagation chip set 
chips rtrl hardware chips designed validate system topology 
little hardware possible put silicon 
section shall design width data path module 
design details appendix 
rest rtrl system implemented discrete components section 
width data path module signal slice compiling learning components operate data paths arrive block diagram single signal slice 
width data path rtrl module rtrl chip basically consists number signal slices 
detailed circuit schematic refer appendix 
chips designed far mrc extensively analogue computing components 
avoid oscillations race neuron activations sampled sample hold circuit edge 
sampler implemented successive simple track hold samplers inverted clock signal second 
gamma block calculating neuron derivative identical back propagation neuron chip 
implemented dimensional mrc ipm 
likewise ffi gamma implemented dimensional ipm back propagation neuron chip output layer 
way multiplexor controlled select neuron target time zero neuron error signal 
implementation ensures negligible error offset originating circuit neurons target values simulations essential learning 
input offset succeeding inner product multiplier course cause offset unavoidable 
ensure ipm dimension number input inherently lowest offset error signal 
controlled multiplexor dimension number input ipm ij oe ij depending cost function chooses 
inner product multiplier calculates weight change distributed signal slices exactly way ipms backpropagation synapse chip distributed matrix columns mrc multiplier placed signal slice differential current outputs chapter implementation rtrl hardware page order signal slice 
block diagram signal slice width data path module 
switches parts distributed de multiplexors 
chip nodes indicated bonding pads 
elements dashed line ij access circuitry 
computed single giving chip desired deltaw current output 
distributed signal slices 
output preceding chip multiplexor voltage necessary distributing signal width data path chips 
chip transformed current easily 
output added current output ij matrix vector multiplier resulting current transferred voltage 
equivalent input backpropagation neuron chip mrc plus op amp input voltage likewise close ref avoid dc common mode currents ij matrix vector multiplier 
value chosen maximum effective synapse strengths range jw ij max 
output level large ensure accuracy small inputs nominally expense quickly saturated output 
delta oe ij multiplier dimensional ipm 
gives total operational amplifiers computing hardware signal slice 
included width data path chip sample chapter implementation rtrl hardware page hold circuits multiplexor accessing derivative variable ram 
components drawn dashed line 
multiplexor distributed signal slices 
sample hold circuits signal slice simple track hold implementations 
placing derivative variable access circuitry width data path chips indicated means converter converter derivative variable ram bank width data path chip 
convenient system speed low signal slices implemented single chip cf 
lehmann 
chip supplied input channels sampling ij gamma channel meant reading ram meant initialization ij gamma connected zero noise generator dependent variation algorithm uses 
auto offset compensation repeatedly noted text necessity low weight change offset 
weight change output offset compensated 
weight change offset compensation circuitry placed width data path module 
offset standard deviation proportional square root number cascaded width data path modules assuming uncorrelated individual module offsets dynamic range offset compensation circuit large principle infinite arbitrary ann size 
reason compensation circuitry placed width data path module addition principal schematic width data path module offset compensation circuit shown auto offset phase deltaw signal disconnected output pin lead current comparator inputs deltaw computing ipm brought state resulting ideal zero deltaw current 
offset compensation current controlled successive approximation register sar adjusted give zero comparator input current 
converter avoid problems charge injection weight drift analogue storage chosen store measured offset digital way indicated 
converter needed subtract measured offset 
summed weight change currents width data path chips converted digital signal width data path module 
effective weight change offset clearly say lsb deltaw converter measures offset errors taken width data path placing compensation circuitry width data path module compliance observations analogue computing accuracy appendix number connected deltaw current outputs doubles resulting expected offset error 
bring offset error certain value offset canceling circuit precision doubled requires area 
chapter implementation rtrl hardware page dac ref dw ij calc dw ij chip auto zero learn output current state zero offset compensation comparator sar cps set bit current auto zeroing principle 
auto zeroing ideally zero output current chip deltaw ij directed current comparator chip output pin 
offset stored sar subtracted cps output 
module 
assuming input range converter corresponds maximum synapse weight allowable offset relative unit output current deltaw ofs jw ij max delta lsb deltaw offset known mrc maximal output delta bit data converter maximum synapse weight jw ij max learning rate need bit current offset canceling converter 
impossible implement standard cmos process salama bit dac quite area consuming 
sacrificing monotonicity fortunately cascade lower precision dacs summed output nl dac input single double double resolution conversion 
deliberately introducing weighted sums outputs increases resolution 
apart slightly scaled mrc unit cell weight change computing ipm identical row back propagation synapse chip offset measurements chips give estimate expect rtrl chip 
value 
chapter implementation rtrl hardware page assume ff fi bit converters ff fi ideal output ranges gamma lsb ff ff max gamma lsb fi fi max maximum relative differential integral nonlinearities ff fi weighted sum fl outputs ff fi way fl ff lsb ff ff fi max ff max fi fl resolution fl res lsb ff ff lsb fi fi ff max ff lsb ff fi lsb fi corresponds ff fi gamma bits 
illustrated sample fl shown ff fi ff fi lsb ff practice fl resolution somewhat coarser accurate scaling needed sure fi scaled factor large ideal 
relative ease convinced successive approximation register works resulting non linear converter 
analogue offset canceling deliberately non linear dac proposed 
width data path chip bit standard cell dacs current offset canceling converter 
voltage outputs control mutually scaled connected input weight change computing ipm 
resolution bit expected 
current comparator high precision offset canceling paramount importance current comparator offset free 
accomplished high input impedance voltage comparator indicated see dom castro 
offset canceling chip output current chip deltaw ij offset zero output current calc deltaw ij minus current value offset canceling current cps integrated input capacitance comparator node capacitance especially small comparator input capacitances current source output capacitances significant case rtrl chip 
time voltage capacitor eventually exceed comparator offset error saturate comparator output desired value 
comparator gain input offset ofs minimum high output high maximum comparison time input current resolution cmp res ofs high ofs ff ofs mv cmp res na 
reduce comparison time input resolution source followers chapter implementation rtrl hardware page placed comparator inputs offset error increase input capacitance reduced 
normal operation weight change output voltage close voltage ref cf 

weight change offset expected dependent output voltage comparator level ref ensures weight change output voltage close ref offset canceling 
prior comparison comparator input voltage reset ref ensure fast comparison 
adding capacitor switches comparator offset error reduced standard auto offset canceling techniques geiger improving comparison time 
sar successive approximation register implemented bit slices straight forward manner cmos multiplexors dynamic delay elements 
bit slice sar shown sake clarity switches shown single transistor switches cmos switches 
circuit needs phase non overlapping clock start conversion signal sc high oe phase prior conversion generation clock signals start signal refer appendix 
current comparator reset oe clock phase active oe phase 
sar consists basically shift register shifts msb lsb conversion memory register 
bit test apply dac current comparator output set bit sb stored register 
bit slices apply stored bit dac 
shift register sc memory register sc dac sr sr sb output msb sar bit slice 
single transistor switches shown simplicity 
bit slices cascaded connecting sr sr just significant slice 
chapter implementation rtrl hardware page chip measurements signal slices width data path module rtrl chip fabricated cmos process 
unfortunately process parameters particular batch mpc run scaled back propagation synapse chip part outside specified ranges 
severe influence chip performance especially signal ranges speed 
hope implement working system spite poor chip performance raise voltage accommodate reduced input voltage range particular current conveyor shall measurement results section 
note chip functions architectural point view indicating correct chip block interconnection possibly applicability 
table chip characteristics appendix 
non linearities offset errors comparable ones backpropagation chip set reduced signal range taken consideration 
typical multiplier characteristic weight change computing ipm seen input large non linearity oe full scale range inputs oe 
non linearities derivative computing gamma block characteristics shown compare typically order magnitude cause trouble learning process 
figures neuron activation input varied different values multiplexed neuron input input considerable offset input observed jv mv 
acceptable experimentally investigated probably upper limit acceptable offset 
large offset caused output offset current conveyor transform input current 
current conveyor multidimensional ipms designed source larger current necessary redesigned 
possibly problematic offset error output offset error gamma block computes neuron derivative 
case back propagation neuron chip offset error magnitude chip dealt way cf 
section 
non chip acceptable learning process 
input output edge neuron activation sampler shown illustrates applicability prohibit race feedback neural network sample time order ns output settling time seen approximately charge injection acceptable 
chips tested weight change output auto offset compensation scheme worked 
note spice simulations circuit shown auto zeroing circuit topology valid 
circuit malfunction caused layout error specified range process parameters able determine layout errors lack internal test points 
measurements indicate current comparator 
chapter implementation rtrl hardware page vz weight change ipm element characteristics 
measured weight change current function neuron activation input different network inputs ipm single element characteristics 
notice reduced input range offset 
vz tanh derivative computing block characteristics 
measured neuron derivative output function neuron activation input different network inputs parabola block characteristics notice offset 
expanded middle section edge sampler sampling 
measured half scale range input sampled output 
effect cascaded samplers clearly seen charge injection occurs sample time half way hold period clock edges 
system design completion rtrl ann system need hardware addition synapse neuron chips width data path rtrl chips 
mentioned section design rtrl back propagation hybrid system save hardware design time 
hardware basically system chapter implementation rtrl hardware page time iin cda ua ua ua sar successive approximation register date time run temperature auto zeroing simulation 
spice simple dac models 
top input output currents 
bottom current comparator input voltage 
notice output current increasingly accurately approximates input current trials 
input voltage indicates output current large small 
ffi digital weight backup memory 
ffi digital serial weight updating hardware including converters interfacing width data path module including ffi finite automaton system control 
ffi ann environment 
complete rtrl system need ffl multiplexor width data path module 
ffl derivative variable ram including converter rtrl chip system 
section shall describe complete system 
complete system schematic refer enclosure iii see appendix 
chapter implementation rtrl hardware page asic interconnection application want apply rtrl system prediction splice sites pre mrna molecules cf 
section 
recurrent network need neurons output neuron tree letter inputs corresponding amino acid code taken letter alphabet brunak hansen 
unary coding gives twelve inputs plus biases mapping topology chip set requires synapse chips neuron chips ann 
assigning inputs offset compensation expected total mvm output offset delta oe oe single chip output offset standard deviation inputs gives rtrl system topology inputs neurons 
custom chip interconnections system operates rtrl mode seen assuming neuron rtrl chips number neurons signal slices 
multiplexor mux implemented standard analogue multiplexor 
derivative variables rams ram implemented bit wide static rams accessed bit data converters 
converters fast flash converters bottle neck system terms speed 
clarity target values target indicators explicitly shown 
signals various control signals supplied environment serial weight updating hardware block 
particular application fit description massively parallel possibly adaptive application specific system having real world interface analogue vlsi learning systems chapter 
toy network application benefit additional neurons 
enhanced performance network ensembles employed cf 
section hundreds neurons exploited system massively parallel 
input output hand consist bit easily supplied say standard bus input data available 
real world interface unnecessary 
connecting synapse chips expected total output offset deltaoe oe single chip output offset standard deviation commit synapses row offset compensation cf 
section 
threshold synapses row seen neuron thresholds larger magnitude typical synapse strengths 
system extra inputs tied zero 
illustrates typical problem standard building block components implement application specific system application exactly match topology building block components hardware wasted 
solution problem available range synapse neuron chips say theta theta theta synapses neurons 
chapter implementation rtrl hardware page offset synapses offset threshold synapses ram ram ij ram mux nc nc nc control unused synapses serial weight updating hardware ment environ mirrored synapse strengths sc sc bias bias sc bias bias rtrl ann basic architecture 
ann rtrl chip interconnections system operates rtrl mode 
blocks rtrl chips 
input output lines sc blocks accessible top bottom left right 
width data path module apart weight strength backup memory system environment components explicitly shown part width data path module rtrl system implementation 
basically identical back propagation learning hardware minor modifications necessary synapses systems unique address fi lg reflecting back propagation topology ij ij mvm rtrl algorithm address space 
offset compensation synapses weight chip ann synapse chip gamma inputs cf 

reason weight backup ram filtered version fi lg address bus mirrors relevant part ann mvm ij mvm 
doing instance weight refresh governed pc simply run fi lgs synapse sites get correct weight 
chapter implementation rtrl hardware page high precision digital weight updating hardware back propagation mode cf 
rtrl mode addition joined rtrl chip current weight change output compatibility 
comments section virtues architecture apply rtrl mode course 
rtrl chip digital signals addressing distributed digital signals access derivative variables ram erroneously combined save pins chip 
error fixed addition digital fi converter see enclosure iii 
error unfortunate lengthy analogue calculation weight change signal take place concurrently derivative variable ram updating degrades performance 
system controlled large number digital handles 
instance sample neuron activation signal oe sy initiate auto zeroing signal sc current synapse signal fi lg 
synapse neuron chips needs signals forward reverse learn signals 
control signals needed including 
combined mutual timing requirements known prototype chip set system design time 
letting system controlling finite automaton control internal timing safe fast designed choice 
noted section low frequency circuit level system performance 
speed tested system 
system controlling finite automaton supply control signals 
placed registers accessible host pc said automaton 
interface host pc interfaced standard bit channel pc isa bus 
addition odd digital handles controls rtrl back propagation system host pc provide various analogue control signals changed user 
instance learning rate neuron activation steepness control signals 
placed bit serial dacs signals offset compensation driven bit dacs 
monitor learning progresses pc access weight backup ram derivative variables rams requires substantial amount extra hardware 
neuron outputs output layer reverse mode synapse chip currents ij mvm current outputs available pc bit 
offset compensate synapse chips high precision 
having neuron outputs available includes possibility pc refresh sampled neuron activations case chip analogue samplers unexpectedly prove inadequate 
acting master finite automaton pc provides environment ann placed supplies input signals target values bit dacs reads network outputs bit 
application need binary inputs targets bit sampling neuron chapter implementation rtrl hardware page activation probably sufficient gradient descent algorithm 
test system prevent ourself analogue input output data data set give additional information system performance 
reason high precision data converters 
algorithm variations rtrl algorithm variations listed section affect algorithm control mechanism 
finite automaton easily incorporated system ffl teacher forcing 
ignoring neuron activation samplers rtrl chips configuring ann mvm inputs back propagation mode cf 
pc supply tf ann mvm inputs teacher forcing 
initialization channel derivative variable ram access circuit connected zero system 
reading ij gamma prior weight calculation initialization channels ram channels sampling ij gamma gamma ensure sum taken gamma 
assuming target neurons neuron chips 
case explicitly write zeros derivative rams 
approach somewhat inelegant designing system include teacher forcing require way analogue multiplexors 
ffl relaxation pipelining 
system designed pipelining 
relaxation scheme just matter updating neuron activations neuron derivative variables couple tpd times updating weights 
note general neuron activation target set known empty omit weight updating step partly speed primarily avoid unnecessary influence weight updating offsets 
ffl learning subsequence 
resetting neuron activation variables subsequence trivial just uses initialization channels ram channels sampling ij gamma ffl random initial states 
system designed random initial derivative variables obtained letting pc write random numbers derivative rams initialization channel initialization including option system design involves placement digital pseudo random generators override derivative variable ram outputs 
ffl momentum weight decay observations implementation weight decay momentum chapter apply system rtrl mode 
note system designed possible read weight changes deltaw ij pc enables weight decay momentum things system chapter implementation rtrl hardware page somewhat laborious way 
real time recurrent learning system time writing construction 
unfortunately system level experiments 
hopefully available near 
non linear rtrl back propagation system gradient descent learning systems hardware implementation neuron activation derivative computation primary error source rtrl system 
inspired non linear back propagation shall derive show implement analogous version rtrl algorithm derivative computation avoided non linear real time recurrent learning 
non linear principle approximating derivative applicable gradient descent learning algorithm uses derivatives rtrl virtual targets back propagation variations 
derivation algorithm derivative variable definition point departure ij oe ij interpret order tailor expansion equation nij gamma oe ij delta gamma defines neuron derivative variables 
nlbp scale oe ij equation factor ff accurate theoretic tailor expansion domain parameter ff large 
interested numeric accuracy theoretic accuracy choose domain parameter small ff nlbp case 
algorithm variations simply arrives substituting nij ij equations section 
simulations algorithm performed experimental verification algorithm functionality done implementation course 
rtrl back propagation chapter implementation rtrl hardware page gradient descent algorithms rtrl ij oe ij plays role similar ffi kj kj respectively back propagation nlbp derived complete analogous ways expect simulated performance similar rtrl 
performance hardware implementation course expected superior rtrl derivative computation omitted signal slice multiplier saved 
hardware implementation mapping algorithm hardware turns advantageous time offset weight updating scheme ij ij deltaw ij gamma assuming discrete time nlbp neurons possibly extra error input assuming quadratic cost function discrete time system takes form shown non linear rtrl system 
block diagram 
topology considerably reduces order data path hardware compared original rtrl system probably accurate 
system uses delayed targets 
weight updating scheme originally proposed williams zipser 
chapter implementation rtrl hardware page system operated follows learning cycle gamma sampled 
gamma applied computed asynchronously 
original system neuron chip samples 
fi jg ij gamma read ram ij multiplier adds oe ij ann mvm output forcing neuron chip calculate ij 
concurrently computed ij stored respective rams 
notes architecture current outputs adding oe ij trivial 
doing oe ij explicitly system refrain entropic cost function 
neuron activations sampled neurons ij computation neuron activation sampler chip need edge 
op amp saved op amps computing hardware saved signal slice counting derivative variable ram access circuit 
back propagation system need carry system level experiments order evaluate applicability proposed chip set 
experiments suitably include simulated 
pc perform digital fixed point computations weight change threshold momentum weight decay verify applicability algorithm variations simulations performed system back propagation mode 
experiments presently carried institute 
obvious tasks include possibly redesign weight change offset canceling circuit simulation implementation proposed algorithm 
considerations process parameter dependency canceling temperature compensation necessary volume production cf 
section just case chips designed far 
considerations implementation high accuracy calculations mentioned section chopper stabilizing weight change threshold apply rtrl chip 
addition tasks vlsi implementations rtrl algorithm include continuous time system version chapter implementation rtrl hardware page continuous time rtrl system nice features analogue signal processing inherently asynchronous functionality 
systems far ignored dealt sampled time systems 
back propagation algorithm fairly easily implemented asynchronous way feed forward architecture 
easy rtrl architecture attractive 
section shall roughly outline continuous time asynchronous version nl rtrl algorithm implemented analogue hardware 
sample hold circuits feedback ann cf 
see continuous time low pass filters having transfer functions yk 
note low pass filter hinder parasitic oscillations caused non ideal electrical components 
signs magnitudes connection strengths synapse matrix unknown cause oscillation 
learning scheme adjust weights prevent oscillation cause 
applies discrete time system matter 
continuous time feedback neuron input definition ae impulse response yk 
input signal frequencies lower filter cutoff frequency instance yk gamma gamma network relaxation network relaxed state oscillation 
input frequencies approaching cutoff frequency network function somewhat pipelined network ascribe filter delay say ln choosing example neuron derivative variable definition nij gamma oe ij delta gamma propose compute neuron net input derivative variables oe ij kl ij nij ffi ik ann substituted low pass filters ij delay elements 
ensure stability low pass filters needed equation 
select ij yk holds ij ij ij nij original algorithm 
denotes convolution def gamma course vastly simplified 
see 
roberts details linear systems 
chapter implementation rtrl hardware page formulate weight updating rule continuous time 
quite trivially generalizes integration ij ij gamma ij ij nij quadratic ij oe ij entropic expanded equation cases quadratic entropic cost functions 
implementing algorithm disadvantage necessary fully parallel 
area 
implementation limiting approach fairly small systems 
needlessly say continuous time incompatible ann architecture uses serial discrete time weight access 
research needed continuous time recurrent learning system implemented things simpler discrete time version algorithm proven operational 
continuous time recurrent learning systems see 
improvements essentially composed components back propagation chip set issues rtrl chip subject improvements 
list appendix 
summary chapter add hardware applying real time recurrent learning ann chip set designed 
basic learning algorithm displayed applicability common algorithmic variations implementation analogue vlsi discussed 
shown doing space time division computational primitives time step choice respect scalability hardware cost ann system compatibility implementation ease 
system level implementation computations order part performed synapse multiplier 
results simulations modeling analogue vlsi non architecture compliance simulations system generally tolerant non exception weight change offsets hidden neuron error offsets neuron derivative computation 
chapter implementation rtrl hardware page design rtrl chip computing part computational primitives displayed 
chip implemented exclusively components back propagation chip set 
weight change offset compensation circuit dac resolution enhancement technique included chip 
unfortunately chip resulted poor computation speed reduced signal range possibly malfunction offset canceling circuit function simulations 
complete rtrl system design various chips displayed 
inputs neurons test system pre mrna splice sites prediction chosen 
learning hardware implemented asics basically weight updating hardware back propagation system possibility digital weight updating hardware adding virtues proposed silicon mapping 
ease test system controlled pc 
various algorithmic variations simulated interface 
system presently construction experimental results 
non linear version real time recurrent learning proposed 
system level implementation algorithm shown saw implementation superior rtrl implementation terms hardware cost computational accuracy 
argued applicability algorithm similar non linear back propagation 
proposed continuous time version non linear real time recurrent learning algorithm analogue vlsi 
page chapter thoughts analogue vlsi neural networks chapter odds ends analogue vlsi neural network design fit chapters collected 
firstly thoughts gradient descent learning analogue vlsi 
secondly propose ann architecture massively parallel systems maps hardware 
thirdly point analogue vlsi neural network ensembles trend propose weight refreshing scheme ensembles 
reflect combining read plastic synapses analogue vlsi computational neural networks 
chapter thoughts analogue vlsi neural networks page gradient descent learning 
investigated possibility implementing supervised learning teacher precisely gradient descent learning analogue vlsi neural networks 
experiments carried far ask question gradient descent learning analogue vlsi 

unsupervised learning learning critic possibly better suited technology lack analogue memory need massively parallel pattern recognition engines evident 
hertz anchez lau strongly encourage development efficient learning teacher algorithms 
rtrl back propagation system prove able solve classification regression tasks expectation small scale system prove applicability learning schemes massively parallel implementations 
believe demonstrate noteworthy points surely ideal solution 
analogue vlsi point view learning teacher schemes serious draw backs 
notably relation offset errors large systems probably relation dynamic range synapse strengths 
eliminating offset errors offset canceling chopper stabilizing proposed thesis solution probably ensuring applicability gradient descent learning 
solution requires precision circuitry neural nature 
neural way deal weight change offsets weight change threshold induced highly non linear weight change multiplier 
additional procedures deal weight change offsets increase learning loop gain corresponding large learning rate ii ac couple learning hardware remove dependency dc offsets 
rate profound belief ultimate implementation hardware learning teacher requires research learning algorithms research electronic implementations 
research course focus limitations analogue vlsi resulting learning algorithms resemble popular simulated algorithms order attract application people 
elegant solutions non linear back propagation weight perturbation come problem computing neuron activation derivatives examples bend algorithm meet technology requirements 
human brain obviously capable doing reliable learning inaccurate 
seek inspiration neuro biology 
computational part brain totally embedded sensors actuators possible learning teacher know 
mean learning algorithms inferior possible information solving problem 
learning teacher 
chapter thoughts analogue vlsi neural networks page neuron clustering scalable ann architecture principle implement ann topology suited applications 
implementation huge massively parallel systems scalable principle hold 
implementation neurons say millions synaptic inputs capabilities technology 
required dynamic range synapses neuron ability sink current put bound neuron fan say impact offset errors electrical degrading speed architecture 
chip input output bottle neck inter chip routing problematic 
imagine learning system conventional algorithms difficult krogh haykin ideal learning machine limited precision technology analogue vlsi learning surely prove hard increased fan requires increased precision cf 
section section section 
applications huge networks robotics instance alternative network topology 
analogue vlsi point view kind neuron clustering advantageous propose architecture consisting sparsely interconnected modules densely connected neurons see individual clusters solve different reasonably complex subproblems possibly problems clusters ensure fault tolerance module level see section global problem solved divide conquer manner 
see jacobs haykin 
note input data structure problems 
visual systems need large fan input layer neurons see 
anchez lau need large fan modules dedicated peripheral tasks 
high level data processing need large fan evident compare models human brain rumelhart miles rogers neuron cluster ann topology applicable general high level computational anns 
cluster size topology obviously tremendous impact system functionality implicit constraints put problem architecture 
efficient system need excessive research areas cluster size topology cluster interconnection architectures 
problem teaching system needs addressed 
questions need type size topology clusters cluster modules reconfigurable modules asked 
learning scheme probably involve supervised unsupervised learning 
convenient single cluster neurons fit chip 
case problem chip input output bottleneck reduced inter module communication efficiently take place robust neuron activation signals 
pulse frequency modulation 
chip communication optimized respect speed power consumption area chapter thoughts analogue vlsi neural networks page neuron clustering 
vlsi point view attractive network architecture 
different blocks individual chips 
possibly kind restricted reconfigurability needed 
local ann topology known priori chip communication intermediate signals synapse outputs takes place 
cmos vlsi technologies today integration neurons synapses including learning hardware realistic 
high level data processing part human brain cerebral cortex organized vertical layers 
communication cerebral cortex layers distinctively local nature long distance connections 
cerebral cortex organized disjoint patches rumelhart see miles rogers 
inspired authors propose modular model cerebral cortex arranging neurons disjoint densely connected columns mutually sparse interconnected see proposed cluster architecture analogue vlsi implementations 
believe topology starting point investigations anns clustered neurons 
proposed neuron clustering architectures inspired hardware friendly sparse connectivity 
noted reported chip architectures resemble proposed neuron clustering architecture sense densely interconnected fixed reconfigurable extend architecture ann integrated chip non expandable way neuron activation inter chip communication 
graf henderson valle castro hamilton serrano 
best knowledge exhaustive investigation chips architectures vs system generality carried cf 
johansen foss problem modeling complex systems ensembles simple models 
chapter thoughts analogue vlsi neural networks page self refreshing system mentioned section major concerns analogue vlsi ann research issue synapse strength storage especially connection chip learning true long term analogue memories floating gate devices tedious program probably expensive suited adaptive systems 
area penalty high resolution chip digital storage kind quantize regenerate scheme compromises advantages analogue vlsi 
systems external components unacceptable parallel weight updating necessary capacitive storage ram back inapplicable 
referring section certain applications unsupervised learning algorithm learning critic elegant way apply refresh relearning systems simple capacitive storage pure analogue signal processing circuitry 
question arises possible storing training patterns apply refresh relearning scheme application areas learning teacher pattern recognition related tasks 
self repair properties neural network ensembles case certain applications 
neural network ensembles neural network ensemble hansen salamon hansen collection neural networks topologically identical trained different initial states solve problem 
training algorithm applied instance back propagation 
applying ensemble problem solution consensus decision individual network outputs 
majority decision binary outputs weighted sum decision stacked regression leblanc tibshirani analogue outputs 
providing ffl individual networks perform reasonably ffl errors different networks degree independent consensus decision superior individual networks 
specifically classification task probability doing misclassification individual network providing network errors independent error probability ensemble ne networks pe ne ne ne gamma ne gammak pe ne 
ensembles ne networks instance hansen reported improvement individual network performance handwritten digit recognition problem 
chapter thoughts analogue vlsi neural networks page improving performance important high cost properties neural network ensembles particularly important relation analogue vlsi ann implementations ffl fault tolerance 
potential fault tolerance neural networks repeatedly emphasized literature networks trained standard approaches back propagation exhibit high degree fault tolerance kock neti insensitive small weight perturbations recall gradient descent solution kj network insensitive complete loss synapse network architecture kept simple possible ensure generalization abilities 
neural network ensembles provides simple way introduce fault tolerance 
analogue vlsi point view fault tolerance important analogue signal processing requires better components digital signal processing sensitive processing errors 
fault tolerant system consensus decider implemented redundant hardware 
ffl enhanced performance 
implementing anns limited accuracy technology analogue vlsi performance ann solutions bound inferior high precision simulated networks see 
section tarassenko lansner 
acceptable application dependent neural network ensembles provides simple way enhance analogue ann performance 
regularly happens ann trained gradient descent gets stuck local minimum cost function 
network solve task hand learning 
recall mode systems fatal incident just repeat training phase initial weights 
adaptive system trained line 
chance learn task 
enhanced performance offered ann ensembles prove crucial systems 
implementation neural network ensemble simple requiring design consensus decider assuming collection acting neural networks 
mentioned previous chapters simplicity important analogue vlsi design 
duplicating ann say ne times implementation ensemble computationally expensive procedure 
moderate size networks parallel computations necessary dedicated hardware implementation ensemble methods hardware hungry potentially small sizes analogue computing elements analogue vlsi ideal technology neural network ensembles 
vice versa 
chapter thoughts analogue vlsi neural networks page consensus decision neural network ensemble superior performance individual networks provides way self repair system degrading weights hansen salamon consensus decision target values networks simply apply supervised line learning algorithm weight updates presentation input patterns system running recall mode call consensus trainer 
certain conditions scheme keep weight deterioration check exploited weight refresh analogue vlsi neural systems simple capacitive storage chip learning teacher see neural network ensemble consensus decider inputs targets error synapse self refreshing ann system 
self repair properties neural network ensemble retain weights stored pure capacitive analogue synapse storage 
occasional external target values read adaptive systems prolong time memory exhaustion 
assume weight perturbation gives proportional network error probability change caused learning algorithm weight deterioration chapter thoughts analogue vlsi neural networks page mechanism assume model discrete time weight change deltaw kj gammaj kj learning scheme gamma ffi delta cyc weight deterioration ffi constant worst case weight rate noise term cyc time needed full weight matrix update learning cycle cost function computed consensus decision target values consensus cost function 
note ffit cyc product equivalent weight change offset 
case turns weight restoration efficiency ffit cyc larger critical value crit system performance remain largely unchanged period time depends noise level 
crit abrupt transition regime system performance degrades rapidly hansen salamon 
ann ensemble consensus trainer sustain weight deterioration rapidly destroy non retrained ensemble say single network life time finite patterns misclassified consensus decider networks trained imitate misclassification probability doing new misclassifications finite partly noise lehmann hansen 
refresh retraining schemes system tend forget classification scarcely occurring input data 
possible occasionally apply external teacher case adaptive systems life time profoundly improved 
systems inherent systems advantage old memories considered irrelevant 
intuitively critical restoration efficiency expected increase system size increases limiting network size apply consensus trainer weight refresh analogue system 
large systems extra hardware ensemble hindrance implementation small systems hand reduction hardware cost complex refreshing scheme easily accommodate extra hardware consensus refresh especially improved performance ensemble taken consideration 
propose consensus trainer refresh relearning small adaptive analogue ann systems chip learning teacher simple capacitive weight storage function changing hostile environment 
applicability scheme subject going research pronounced non linearity anns highly inaccurate conservative approximation 
assume gradient descent learning small learning rate quadratic cost function approximate neuron activation functions jsj sign saturated neurons small weight change alter error probability 
non saturated neurons output error change linear weight change 
chapter thoughts analogue vlsi neural networks page presently carried institute lehmann hansen 
course memory destruction power loss artificial neural systems unacceptable 
need means read volatile synapse strengths backup purposes replication 
alternatively combination read dynamic synapse memories hard soft hybrid synapses convenient manner read synapse strengths ann simple capacitive storage memory restoration power loss synapse strength loading volume manufacturing prove tedious 
cases retraining necessary system back memory site memory restoration necessarily compatible analogue ann system 
short term adaptations needed solution hard soft hybrid synapses consisting pre programmed non volatile possibly read part perturbation stored volatile capacitive memory 
idea pre determined template instance obtained simulations generate non volatile hard parts synapse strengths manufacturing implemented instance scaled transistors floating gate devices ii chip learning algorithm adapting system volatile soft part synapse strengths 
hard memory part reflect behavioural model system soft memory part reflect current working conditions 
consider instance robot walking sand snow earth pavement mud pebbles 
basic locomotive behaviour pre programmed place leg front keep balance temporary adaptations current ground cover determined real time learning 
applications adaptive analogue systems tremendous powerful 
page chapter thesis implementation analogue vlsi neural systems chip set recall mode neural networks ii variation chip set including chip error back propagation learning hardware efficient way iii add hardware doing real time recurrent learning chip set realistic amount hardware time 
recall mode system tested experimentally chip electrical level system level 
learning system tested chip level learning systems time construction 
recall mode chip development reviewed different chip network architectures different basic building block components memories multipliers thresholding functions 
settled chip system synapse chip neuron chip analogue voltage current signalling capable principle implementing ann topology order deterministic neurons 
chose simple capacitive synapse strengths storage digital ram backup mos resistive circuit synapse multiplier hyperbolic tangent neuron activation function parasitic bipolar transistors 
addition basic synapse chip showed special sparse input synapse chip efficiently exploit chip input bandwidth unary coded network inputs 
theta synapses synapse chip neurons neuron chip fabricated standard cmos process 
measurements chips showed bit synapse resolution non linearities quantities offset errors quantities magnitudes compatible september 
chapter page large range applications 
system full size theta synapse chips estimated gcps synapse chip 
measurements experimental perceptron solving sunspot prediction problem chip shown learning error slightly worse ideal simulated network 
classification regression tasks multi layer perceptrons trained error back propagation employed 
fully parallel vlsi implementation algorithm gives improvement speed compared serial usually cost times synapse hardware twice inter module wiring compared recall mode system 
physical size system important learning scheme employed occasionally additional hardware severely restrict system applicability 
showed implement back propagation extra synapse hardware inter module wiring mos resistive synapse multiplier novel configuration exploits bidirectional properties cost discrete time improvement speed compared serial approach 
theta synapses back propagation synapse chip neurons back propagation neuron chip fabricated standard cmos process 
measurements chips showed bit synapse resolution non linearities quantities offset errors quantities magnitudes compatible range applications offset canceling applied critical signals 
system full size theta synapse chips estimated gcps synapse chip serial weight update digital weight back ram puts restriction system 
showed implement back propagation system chip set 
addition chip set basically finite automaton controlling system weight updating hardware implemented digital components 
powerful recurrent neural networks solve larger class problems perceptrons 
real time recurrent learning train completely general network architecture nice properties respect vlsi implementation 
require massive order computational primitives training example 
showed dividing computational primitives space time domain choice respect scalability hardware cost speed implementation ease compatibility acting recall mode neural system 
showed perform learning recall mode system adding synapse multiplier scalable rtrl chip consisting order signal slices plus weight updating hardware order finite automaton controlling system 
signal slices rtrl chip fabricated standard cmos process unfortunately chip resulting reduced signal range speed 
measurements showed topological functionality non linearities offset errors quantities magnitudes system neurons chapter page possibly compatible algorithm offset canceling applied critical signals 
implementation neurons inputs system displayed 
digital ram back synapse strengths obviously restricts learning scheme efficiency serial weight change possible converters needed 
showed implementing weight updating hardware digital electronics exhibits range virtues possibility implementing high accuracy circuits digital components enables applicability advanced weight updating schemes 
momentum function simple analogue system weight change offsets amplified 
turns scheme reduce minimum effective learning rate weight change offset major concern analogue implementations learning algorithms 
algorithmic variations weight decay weight change threshold readily accurately implemented digital domain 
note ram weight updating hardware scale hardware cost important 
placing synapse strengths digital ram convenient back purposes important real applications 
noted applications exist tolerate parameter variances induced temperature drift analogue neural networks general temperature compensated 
implementations back propagation system need process parameter variation canceling various scheme outlined 
relation limited accuracy analogue computing systems noted problems teaching artificial neural networks gradient descent algorithms especially severe offset errors primarily weight change signal neuron derivative calculation 
procedures reduce offset errors dac resolution enhancement technique utilizing chopper stabilizing 
ensure correct sign computed neuron derivative profound importance convergence introduced deliberate offset computing hardware 
suggested clipping computed quantity possibility 
attacking problem algorithmic starting point attractive far proposed non linear gradient descent analogue vlsi novel non linear back propagation learning algorithm displayed 
algorithm important properties hardware implementation activation function derivatives need computed ii back propagation errors non linear network forward propagation 
showed implies electronic implementation model algorithm accurately possible ordinary back propagation design efforts put electrical properties system components 
proposed chapter page hardware implementations continuous time version discrete time version algorithm combining hardware efficient backpropagation synapse chip saw implementation non linear backpropagation possible virtually extra hardware compared recall mode system 
simulations non linear back propagation learning nettalk problem shown performance similar ordinary backpropagation 
derived non linear version real time recurrent learning algorithm argued compared ordinary real time recurrent learning properties similar non linear back propagation application level performance respect hardware implementations 
system level implementation algorithm showed half hardware rtrl chip saved 
proposed continuous time version nonlinear real time recurrent learning exploitation asynchronous properties analogue vlsi 
saw implementation analogue neural network ensembles trend field ensembles introduce seldom implemented fault tolerance analogue neural systems 
importantly ensembles enhance performance limited accuracy technology analogue vlsi neural systems bound inferior performance compared ideal simulated networks 
adaptive systems particular severe training data necessarily reproducible faulty learning intolerable 
neural network ensembles expensive terms hardware analogue vlsi ideal technology neural network ensembles 
vice versa 
solution various chips set functions limited number cascaded devices argued generalization arbitrary size huge neural networks compliance analogue technology systems requires infinite dynamic range instance synapse strengths 
highly non linear synapse multipliers improved limited 
proposed network topology clusters neurons sparsely interconnected clusters come problem research cluster topology vs system generality carried 
addition problem limited accuracy analogue vlsi issue weight storage systems major concern analogue electronic memory presently available 
eliminate need ram back systems simple capacitive storage proposed self repair properties neural network ensembles auto refresh systems chip supervised learning algorithm 
efficiently prolong time exhaustion weights 
adaptive systems prove sufficient especially synapse strengths combination read behavioral plastic adaptive memories 
digital synapse memory acceptable similar manner combine analogue adjustment enabling analogue learning chapter page algorithm train system coarse discretized synapse strengths 
bottom line implementations analogue neural network learning systems begun emerge literature including thesis excessive research field needed field mature 
research learning algorithms needed algorithmic implementation level 
problems need addressed include insensitivity weight change offset analogue memories enhancement system performance reliability 
believe research prove fruitful vlsi supervised learning algorithms 
page bibliography logically placed categories ffl chip learning 
ffl analogue neural networks 
ffl artificial neural networks 
ffl integrated circuits 
ffl miscellaneous 
cited thesis included thorough referring field 
probably authors excluded list apology 
listed alphabetical order bibliography page connectionist mailing list restricted internet mailing list connectionists request cs cmu edu 
hp direct hewlett packard series workstations 
near workshop european analog research september post conference workshop copenhagen 
screening perceptrons december talk th neural information processing systems conference denver 
tor local generation storage voltages cmos technology proc 
th european conference circuit theory design pp 

dreyfus gascuel personnaz roman silicon integration learning algorithms properties digital feedback neural networks vlsi design neural networks ulrich ulrich eds norwell kluwer academic publishers pp 

phillip allen douglas cmos analog circuit design fort worth holt rinehart winston 
george allan gottlieb highly parallel computing redwood city benjamin cummings publishing 
joshua alspector anthony stephan luna experimental evaluation learning neural microsystem proc 
th neural information processing systems conference pp 

vlsi reinforcement learning private communication niels bohr institute 
andersson portable cmos design rules swedish universities lund wallin ab 
hardware realisation neuron transfer function derivative electronics letters vol 
pp 

anne johan analysis modelling implementation analog integrated neural networks ph thesis university twente netherlands 
precision requirements feed forward neural networks proc 
th international conference microelectronics neural networks fuzzy systems turin pp 

anne johan hans learning behaviour temporary minima layer neural networks neural networks vol 

pp 

arima yamada atsushi maeda shinohara analog vlsi neural network chip neurons synapses ieee journal solid state circuits vol 
sc pp 

bibliography page nelson morgan experimental determination precision requirements back propagation training artificial networks proc 
nd international conference microelectronics neural networks pp 

les atlas suzuki digital systems artificial neural networks ieee circuits devices magazine vol 
pp 

roberto battiti learning second derivatives case study high energy physics neurocomputing vol 
pp 

randall beer hillel leon sterling biological perspective autonomous agent design robotics autonomous systems vol 
pp 

ha 
learning multilayer perceptron journal physics math 
gen vol 
pp 

ronald benson douglas uv activated conductances allow multiple time scale learning ieee transaction neural networks vol 
pp 
may 
steven mohammed ismail issues analog vlsi mos techniques neural computing analog vlsi implementation neural systems carver mead mohammed ismail eds norwell kluwer academic publishers pp 

steven mohammed ismail neural network building blocks analog mos vlsi analogue ic design current mode approach haigh eds iee circuits systems series london peter pp 

christian sven multivalued memory standard cmos weight storing neural networks proc 
th european conference circuit theory design vol 
pp 

generation neuron transfer function derivative electronics letters vol 
pp 

borwein dictionary mathematics glasgow collins 
cmos analogue current steering multiplier electronics letters vol 
pp 

ren brunak jacob engelbrecht steen knudsen prediction human mrna donor acceptor sites dna sequence journal molecular biology vol 
pp 

ren brunak benny med og vol 
pp 

ren brunak hans hansen predicting splice sites rtrl private communication technical university denmark 
bibliography page erik john lansner torsten lehmann analog vlsi architectures computational neural networks proc 
th seminar pp 

erik bandwith limitations current mode voltage mode integrated feedback amplifiers ei preprint technical university denmark 
erik analogue signal processing collected papers electronics institute technical university denmark lyngby 
erik thomas john lansner peter shah analogue vlsi private communication technical university denmark 
wray buntine andreas weigend computing second derivatives feed forward networks review ieee transactions neural networks vol 
nn pp 

graham lionel tarassenko learning analogue vlsi mlps proc 
th international conference microelectronics neural networks fuzzy systems turin pp 

yong cao sven christian system new implementation proc 
th european solid state circuits conference pp 

howard card relaxation networks examples analog circuits canada proc 
rd international conference microelectronics neural networks pp 

howard card analog circuits relaxation networks international journal neural systems vol 
pp 

richard carley trimming analog circuits floating gate analog mos memory ieee journal solid state circuits vol 
sc pp 

castro simon tam mark holler implementation performance analog neural network analog integrated circuits signal processing vol 
pp 

thierry method improving real time recurrent learning algorithm neural networks vol 
pp 

daniele maurizio valle giacomo effects weight discretization back propagation learning method algorithm design hardware realization proc 
ieee international joint conference neural networks pp 
ii ii 
yuan chang cheng chi wang jain bean hsu schemes detecting cmos analog faults ieee journal solid state circuits vol 
sc pp 

cho yoon choi soo young lee modular analog neuro chip set chip learning error back propagation hebbian rules proc 
international conference artificial neural networks bibliography page sorrento vol 
pp 

leon chua lin yang cellular neural networks applications ieee transactions systems vol 
cas pp 

leon chua lin yang cellular neural networks theory ieee transactions systems vol 
cas pp 

pe 
allen low voltage quadrant analogue cmos multiplier electronics letters vol 
pp 

cohen mos circuit nonlinear hebbian learning electronics letters vol 
pp 

dean collins andrew considerations neural network hardware implementations proc 
ieee international symposium circuits systems pp 

michael chopper stabilization mos operational amplifiers feed forward techniques ieee journal solid state circuits vol 
sc pp 

del corso artificial neural system coherent pulse edge modulation proc 
rd international conference microelectronics neural networks pp 

roberts high swing mos current mirror arbitrarily high output resistance electronics letters vol 
pp 

yann le cun john denker sara solla optimal brain damage proc :10.1.1.32.7223
neural information processing systems conference san mateo pp 

yann le cun ido sara solla second order properties error surfaces learning time generalization proc 
neural information processing systems conference denver pp 

novel mos resistive circuit synthesis fully integrated continous time filters ieee transactions circuits systems vol 
cas pp 

van zhao shawe taylor real time output derivatives chip learning digital stochastic bit stream neurons electronics letters vol 
pp 

casper dietrich analog vlsi af matrix med sc 
thesis institut lyngby 
card neural learning analogue hardware effects component variation fabrication noise electronics letters vol 
pp 

bibliography page dom castro rodr iguez high resolution cmos current comparators proc 
th european solid state circuits conference pp 

kenji doya adaptive neural continuous time back propagation learning neural networks vol 
pp 

eberhardt tran learning optimization cascaded vlsi neural network building block chips proc 
ieee international joint conference neural networks pp 
june 
scott mohammed ismail high frequency cmos analogue ic design current mode approach haigh eds iee circuits systems series london peter pp 

silvio eberhardt anil design parallel hardware neural network systems custom analog vlsi building block chips proc 
ieee international joint conference neural networks pp 
ii ii 
silvio eberhardt alex anil considerations hardware implementations neural networks proc 
nd asilomar conference signals systems computers pp 

peter edwards alan murray analogue synaptic noise implications learning improvements international journal neural systems vol 
pp 

lewis interfacing ibm personal computer nd ed indianapolis sams 
fahlman fast learning variations back propagation empirical study proc 
connectionist models summer school pittsburgh touretzky hinton sejnowski eds morgan kaufmann pp 

nabil neural networks learning machines ieee circuits devices magazine vol 
pp 

barry flower jabri implementation single dual transistor vlsi analogue synapses proc 
rd international conference microelectronics neural networks pp 

barry flower jabri summed weight neuron perturbation improvement weight perturbation proc 
neural information processing systems conference san mateo pp 

gregory charles stroud cmos master slave flip flops ieee transactions circuits systems pt 
ii vol 
cas pp 

robert richard roberts signals linear systems rd ed new york john wiley sons 
bibliography page franco full stacked layout analogue cells proc 
ieee international symposium circuits systems pp 

accurate cmos sample circuit ieee journal solid state circuits vol 
pp 

randall geiger phillip allen noel vlsi design techniques analog digital circuits singapore mcgraw hill publishing 
arthur gelb joseph kasper jr raymond nash jr charles price arthur sutherland jr science applied optimal estimation cambridge mit press 
giles chen miller chen sun lee grammatical inference second order recurrent neural networks proc 
ieee international joint conference neural networks pp 

karl gustafson kristina johnson connectionist nonlinear relaxation proc 
ieee international joint conference neural networks pp 
iii iii 
malcolm gordon animal physiology principals nd ed new york macmillan publishing pp 

hans graf lawrence jackel analog electronic neural network circuits ieee circuits devices magazine vol 
pp 

graf analog electronic neural networks proc 
th european solid state circuits conference pp 

hans peter graf don henderson reconfigurable cmos neural network artificial neural networks edgar anchez clifford lau eds new york ieee press pp 

david john taylor paul design implementation evaluation high speed integrated hamming neural classifier ieee journal solid state circuits vol 
sc pp 

sten peter wall en lennart anders lansner neuronal network generating locomotor behavior lamprey annual reviews neuroscience vol 
pp 

heng guo saul gelfand analysis gradient decent learning algorithms multilayer feedforward neural networks ieee transactions circuits systems vol 
cas pp 

hamilton stephen churcher peter edwards geoffrey jackson alan murray martin reekie pulse stream vlsi circuits systems epsilon neural network chipset international journal neural systems vol 
pp 

lars kai hansen christian peter salamon ensemble methods bibliography page handwritten digit recognition proc 
ieee workshop neural networks signal processing pp 

lars kai hansen peter salamon neural network ensembles ieee transactions pattern analysis machine intelligence vol 
pami pp 

lars kai hansen peter salamon self repair neural network ensembles conference neural networks san diego 
ole hansen vlsi devices private communication technical university denmark 
simon haykin neural networks comprehensive foundation new york macmillan collage publishing 
john hertz anders krogh benny torsten lehmann nonlinear back propagation doing back propagation derivatives activation function preprint niels bohr institute copenhagen 
john hertz anders krogh richard palmer theory neural computation redwood city addison wesley publishing 
marcus scott fahlman probabilistic rounding neural network learning limited precision proc 
nd international conference microelectronics neural networks pp 

hollis harper effects precision constraints backpropagation learning network neural computation vol 
pp 

paul hollis john artificial neural networks mos analog multipliers ieee journal solid state circuits vol 
sc pp 

paul hollis john christopher costa optimized learning algorithm vlsi implementation proc 
nd international conference microelectronics neural networks pp 

paul hollis john neural network learning algorithm tailored vlsi implementation ieee transactions neural networks vol 
nn pp 

nakamura analog memories vlsi neurocomputing artificial neural networks edgar anchez clifford lau eds new york ieee press pp 

learning modular networks proc 
th yale workshop adaptive learning systems pp 

rong hsieh wen chen neural network model combines unsupervised supervised learning ieee transactions neural networks vol 
nn pp 

kou chiang hsieh paul gray daniel david bibliography page schmitt low noise chopper stabilized differential switched capacitor filtering technique ieee journal solid state circuits vol 
sc pp 

john hurdle erik josephson asynchronous vlsi design neural system implementation proc 
rd international workshop vlsi neural artificial intelligence pp 

mohammed ismail analog vlsi signal information processing electrical computer engineering series new york mcgrawhill 
jabri barry flower weight perturbation optimal architecture learning technique analog vlsi feedforward recurrent multilayer networks ieee transactions neural networks vol 
nn pp 

jabri leong chi flower xie ann classification heart proc 
neural information processing systems conference denver pp 

jabri practical performance credit assignment efficiency analog multi layer perceptron perturbation training algorithms system engineering design automation laboratory sydney university electrical engineering tech 
rep 
lawrence jackel practical issues electronic neural net hardware tutorial notes th neural information processing systems conference 
geoffrey jackson hamilton alan murray pulse stream vlsi neural systems robotics proc 
ieee international symposium circuits systems london vol 
pp 

robert jacobs michael jordan andrew barto task decomposition competition modular connectionist architecture vision tasks cognitive science vol 
pp 

kam jim lee giles bill horne synaptic noise recurrent neural networks convergence generalization institute advanced computer studies university maryland umiacs tr cs tr 
tor johansen bjarne foss constructing models models international journal control vol 
pp 

johnson marsland neural network implementation single synapse appear ieee transactions neural networks 
proposed hardware implementation massively parallel cortical automation networks electronics letters vol 
pp 

yaron yair ery back propagation data bibliography page architectures proc 
rd international conference microelectronics neural networks pp 

thomas angle detector magnetic sensing proc 
ieee international symposium circuits systems london vol 
pp 

brian kernighan dennis ritchie programming language nd ed englewood cliffs prentice hall 
douglas experiments large scale analog computation ph thesis california institute technology pasadena 
donald eric swanson bit mhz step flash adc ieee journal solid state circuits vol 
sc pp 

edwin van sel hans neural hardware performance criteria private communication eindhoven university technology 
nabil mohammed ismail nonlinear cmos analog cell vlsi signal information processing ieee journal solid state circuits vol 
sc pp 

anders krogh john hertz simple weight decay improve generalization proc 
neural information processing systems conference denver pp 

anders krogh lars kai hansen jan larsen neural networks private communication technical university denmark 
francis keith moon ingham mack francis long programmable analog vector matrix multipliers ieee journal circuits vol 
sc pp 

hon kwan systolic architectures hopfield network bam multi layer feed forward networks proc 
ieee international symposium circuits systems pp 

kwan tang designing multilayer feedforward neural networks simplified sigmoid activation functions weights electronics letters vol 
pp 

robert miles copeland modeling mismatch mos transistors precision analog design ieee journal solid state circuits vol 
sc pp 

john lansner torsten lehmann neuron synapse chip artificial neural networks proc 
th european solid state circuits conference pp 

john lansner torsten lehmann analog cmos chip set neural networks arbitrary topologies ieee transaction neural networks vol 
pp 
may 
bibliography page john lansner experimental hardware neural network analog chip set appear international journal electronics technical university denmark 
john lansner analogue vlsi implementation artificial neural networks ph thesis electronics institute technical university denmark lyngby 
alan lapedes robert farber neural nets proc 
neural information processing systems conference anderson eds new york american institute physics pp 

jan larsen design neural network filters ph thesis electronics institute technical university denmark lyngby 
michael leblanc robert tibshirani combining estimates regression classification preprint university toronto 
bang lee bing sheu han yang analog floating gate synapses general purpose vlsi neural computation ieee transactions circuits systems vol 
cas pp 

hae seung lee david hodges paul gray self calibrating bit cmos converter ieee journal solid state circuits vol 
sc pp 

torsten lehmann hardware implementation real time recurrent learning algorithm proc 
th european conference circuit theory design vol 
pp 

torsten lehmann vlsi sc 
thesis institut lyngby 
torsten lehmann chip set ann chip backpropagation proc 
rd international conference microelectronics neural networks pp 

torsten lehmann hardware efficient chip set ann chip back propagation international journal neural systems vol 
pp 

torsten lehmann erik analogue vlsi implementation backpropagation learning artificial neural networks proc 
th european conference circuit theory design pp 

torsten lehmann implementation issues back propagation learning analog vlsi neural networks preparation technical university denmark 
torsten lehmann lars kai hansen analogue vlsi neural network ensemble issues preparation technical university denmark 
torsten lehmann erik casper dietrich analogue digital hybrid vlsi synapses recall learning mode neural networks proc 
th seminar gothenburg pp 

torsten lehmann erik casper dietrich mixed analogue digital bibliography page matrix vector multiplier neural network synapses preparation technical university denmark 
thomson leighton parallel algorithms arrays trees hypercubes san mateo morgan kaufmann publishers 
phillip leong jabri low power analogue neural network proc 
rd international conference microelectronics neural networks pp 

phillip leong jabri low power analogue neural network classifier international journal neural systems vol 
pp 

edgar anchez angel rodr iguez jos cmos analog adaptive bam chip learning weight refreshing ieee transaction neural networks vol 
pp 
may 
richard lippmann computing neural nets ieee assp magazine vol 
pp 

ronald macgregor neural brain modeling san diego academic press 
damien michel verleysen paul jean didier analog implementation kohonen map chip ieee transaction neural networks vol 
pp 
may 
madani de ion temperature effects modelling compensation analysis analogue implementation stochastic artificial neural networks proc 
th international conference microelectronics neural networks fuzzy systems turin pp 

jim mann richard lippmann bob berger jack self organizing neural net chip proc 
ieee custom integrated circuits conference pp 

patterns second analog cmos neural network pattern classifier proc 
th european conference circuit theory design pp 

dynamic cmos multiplier analog neural network cells proc 
ieee custom integrated circuits conference pp 

ofer christopher burges yann le cun john denker multi digit recognition space displacement neural network proc 
th neural information processing systems conference pp 

matsumoto koga high speed learning method analog neural networks proc 
ieee international joint conference neural networks pp 
ii ii 
systematic capacitance bibliography page matching errors corrective layout procedures ieee journal circuits vol 
sc pp 

carver mead analog vlsi neural systems reading addison wesley publishing 
carver mead mohammed ismail analog vlsi implementation neural systems norwell kluwer academic publishers 
christopher michael mohammed ismail statistical modeling device mismatch analog mos integrated circuits ieee journal solid state circuits vol 
sc pp 

jean yves michel high performance analog cells mixed signal vlsi problems practical solutions analog integrated circuits signal processing vol 
pp 

coe miles david rogers associative memory motivated memory architecture ieee transactions neural networks vol 
nn pp 

antonio ronald john building blocks temperature compensated analog vlsi neural network chip learning proc 
ieee international symposium circuits systems london vol 
pp 

antonio paul hollis john chip learning analog domain limited precision circuits proc 
international symposium circuits systems pp 

keith moon francis ingham mack random address programmable analog vector matrix multiplier artificial neural proc 
ieee custom integrated circuits conference pp 

takashi analog expandable neural network lsi chip backpropagation learning ieee journal solid state circuits vol 
sc pp 

alessandro eric communication architecture tailored analog vlsi artificial neural networks intrinsic performance ieee transactions neural networks vol 
nn pp 

organizing principle cerebral function unit module distributed system mindful brain edelman eds mit press pp 

paul mueller jan van der spiegel david blackman timothy chiu thomas clare christopher pu hsieh marc design fabrication vlsi components general purpose analog neural computer analog vlsi implementation neural systems carver mead mohammed ismail eds norwell kluwer academic publishers pp 

alan murray multilayer perceptron learning optimized chip bibliography page implementation noise robust system neural computation vol 
pp 

alan murray del corso lionel tarassenko pulse stream vlsi neural networks mixing analog digital techniques ieee transactions neural networks vol 
pp 
march 
murray tarassenko reekie hamilton churcher baxter pulsed silicon neural networks biological leader vlsi design neural networks ulrich ulrich eds norwell kluwer academic publishers pp 

alan murray peter edwards analogue synaptic noise hardware nuisance aid learning proc 
rd international conference microelectronics neural networks pp 

alan murray peter edwards enhanced mlp performance fault tolerance resulting synaptic weight noise training ieee transactions neural networks vol 
nn pp 

personnaz training recurrent neural networks 
illustration dynamical process modeling ieee transactions neural networks vol 
nn pp 

charles neugebauer amnon parallel analog ccd cmos signal processor proc 
neural information processing systems pp 

neti michael schneider eric young maximally fault neural networks ieee transactions neural networks vol 
nn pp 

paul leary practical aspects mixed analogue digital design analogue digital asic circuit techniques design tools applications franca eds iee circuits systems series london peter pp 

new approach selection initial values weights neural function approximation electronics letters vol 
pp 

palm ultsch knowledge processing neural architecture proc 
rd international workshop vlsi neural artificial intelligence pp 

joshua park christopher abel mohamed ismail design silicon cochlea mos switched current techniques proc 
th european conference circuit theory design pp 

morten pedersen lars kai hansen recurrent networks second order properties pruning ei preprint 
marcel aad anton matching properties mos transistors ieee journal solid state bibliography page circuits vol 
sc pp 

plaut nowlan hinton experiments learning backpropagation department computer science carnegie mellon university pittsburgh tech 
rep cmu cs 
william press brian flannery saul teukolsky william vetterling numerical recipes cambridge cambridge university press 
ning qian terrence sejnowski predicting secondary structure globular proteins neural network models journal molecular biology vol 
pp 

jack electronic implementation systems proc 
ieee custom integrated circuits conference pp 

ulrich guide lines vlsi design neural nets vlsi design neural networks ulrich ulrich eds norwell kluwer academic publishers pp 

ulrich ulrich vlsi design neural networks norwell kluwer academic publishers 
ulrich peter developments neurodynamics impact design neuro chips international journal neural systems vol 
pp 

thomas smith godfrey owen snell murray rose application analogue amorphous silicon memory devices resistive synapses neural networks proc 
nd international conference microelectronics neural networks munich pp 

leonardo analysis performance silicon implementations backpropagation algorithms artificial neural networks ieee transactions computers vol 
pp 

del corso coherent pulse width edge modulations artificial neural systems international journal neural systems vol 
pp 

jacques robert philippe second order high incremental converter offset charge injection compensation ieee journal solid state circuits vol 
sc pp 

rumelhart hinton williams learning internal error propagation parallel distributed processing explorations microstructure cognition vol 
rumelhart mcclelland pdp group eds cambridge mit press chap 

rumelhart mcclelland pdp research group parallel distributed processing explorations microstructure cognition cambridge mit press 
bibliography page eduard sackinger linda placement critical analog integrated circuits ieee journal solid state circuits vol 
sc pp 

eduard sackinger walter high swing high impedance mos circuit ieee journal solid state circuits vol 
sc pp 

hiroshi yamamoto reduction required precision bits back propagation applied pattern recognition ieee transactions neural networks vol 
nn pp 

sakurai ismail high frequency wide range cmos analogue multiplier electronics letters vol 
pp 

andre salama david henry current mode converters analogue ic design current mode approach haigh eds iee circuits systems series london peter pp 

edgar anchez clifford lau artificial neural networks new york ieee press 
yannis hans peter graf reconfigurable vlsi neural network ieee journal solid state circuits vol 
sc pp 

saxena james clark quadrant cmos analog multiplier analog neural networks ieee journal solid state circuits vol 
sc pp 

jurgen schmidhuber learning algorithm fully recurrent networks institut fur informatik technische universitat munchen munchen 
christian schneider howard card analog cmos synaptic learning circuits adapted biology ieee transactions circuits systems vol 
cas pp 

jesper schultz vlsi med sparse inputs sc 
thesis institut lyngby 
analog interface circuits vlsi analogue ic design current mode approach haigh eds iee circuits systems series london peter pp 

charles seitz system timing vlsi systems carver mead lynn conway eds reading addison wesley publishing pp 

sejnowski charles rosenberg parallel networks learn pronounce english text complex systems vol 
pp 

bibliography page gerd kock fault tolerant neuro computing proc 
international conference artificial neural networks sorrento vol 
pp 

serrano cmos vlsi analog current mode high speed art chip proc 
ieee international conference neural networks orlando vol 
pp 

peter shah short term analogue memory proc 
th european solid state circuits conference pp 
september 
samir shah francesco fast local algorithm training feedforward neural networks proc 
ieee international joint conference neural networks pp 
iii iii 
je mahesh patil bing sheu measurement analysis charge injection mos analog ieee journal solid state circuits vol 
sc pp 

kimura itakura fujita iida neuro chips chip back propagation hebbian learning ieee journal solid state circuits vol 
sc pp 

elimination process dependent clock skew cmos vlsi ieee journal solid state circuits vol 
sc pp 

offset compensation phase switched capacitor filters ieee journal solid state circuits vol 
sc pp 

roy berg tor analog neural network chip back propagation learning proc 
th seminar gothenburg pp 

patrick simpson foundations neural networks artificial neural networks edgar anchez clifford lau eds new york ieee press pp 

anthony smith david zipser learning sequential structure real time recurrent learning algorithm international journal neural systems vol 
pp 

solla levin accelerated learning layered neural networks complex systems vol 
pp 

jens christian nielsen lars nielsen rgen design self timed multipliers comparison proc 
ifip tc wg 
working conference asynchronous design methodologies manchester pp 

jan van der spiegel paul mueller david peter chance ralph etienne peter analogue neural computer modular architecture real time dynamic computations ieee journal solid state circuits vol 
sc pp 
bibliography page 
verghese rob richard carley david addressing substrate coupling mixed mode ic simulation power distribution synthesis ieee journal solid state circuits vol 
sc pp 

scott stornetta huberman improved layer back propagation algorithm proc 
ieee international conference neural networks vol 
pp 

biochemistry rd ed new york freeman 
sun moll berger break mechanism short channel mos transistors proc 
ieee technical digest international electron device meeting dc pp 

ivan sutherland communications acm vol 
pp 
june 
hansen larsen design evaluation tapped delay neural network architectures proc 
ieee international conference neural networks vol 
pp 

sze semiconductor devices physics technology new york john wiley sons 
janos dynamic backpropagation algorithm neural network controlled resonator bank architecture ieee transactions circuits systems ii vol 
cas pp 

lionel tarassenko jon chip learning analogue vlsi neural networks proc 
rd international conference microelectronics neural networks pp 

lionel tarassenko jon graham chip learning analogue vlsi neural networks international journal neural systems vol 
pp 

hans henrik neural information processing system pig grading danish danish meat research institute preprint roskilde 
axel thomsen martin floating gate cmos signal conditioning circuit nonlinearity correction analog integrated circuits signal processing vol 
pp 

cris john universal current mode analogue amplifiers analogue ic design current mode approach haigh eds iee circuits systems series london peter pp 

haigh analogue ic design current mode approach iee circuits systems series london peter 
bibliography page makris extending voltage mode op amps current mode performance iee proceedings pt 
vol 
pp 

payne current feedback versus amplifiers history insight relationships proc 
ieee international symposium circuits systems pp 

yannis mihai john continuous time filters vlsi ieee transactions circuits systems vol 
cas pp 

analogue circuits variable synapse electronic neural networks electronics letters vol 
pp 

yannis analog circuits possibilities needed support proc 
th european solid state circuits conference pp 

tsoi tan lawrence financial time series application artificial neural network techniques preprint department electrical computer engineering university queensland st lucia australia 
paul spice guide circuit simulation analysis englewood cliffs prentice hall 
osman data transmission analysis design applications new york mcgraw hill 
maurizio valle daniele giacomo experimental analog vlsi neural chip chip back propagation learning proc 
th european solid state circuits conference pp 

layout comparison large ratios electronics letters vol 
pp 

eric mos transistors operated lateral bipolar mode application cmos technology ieee journal solid state circuits vol 
sc pp 

maher analog storage adjustable synaptic weights vlsi design neural networks ulrich ulrich eds norwell kluwer academic publishers pp 

fong jim wang gabor fast offset free sample hold circuit ieee journal solid state circuits vol 
sc pp 

wang modular analog cmos lsi feedforward neural networks chip bep learning proc 
ieee international symposium circuits systems vol 
pp 

wang cmos quadrant analog multiplier voltage output improved performance ieee jour bibliography page nal solid state circuits vol 
sc pp 

timothy albrecht rau michael statistical mechanics learning rule physics review vol 
pp 

webb implementation neural networks international journal neural systems vol 
pp 

george wegmann eric charge injection analog mos switches ieee journal solid state circuits vol 
sc pp 

andreas weigend bernardo huberman david rumelhart predicting connectionist approach international journal neural systems vol 
pp 

neil principles cmos vlsi design systems perspective reading addison wesley publishing 
chin long jiang gregory build self test bist design large scale analog circuit networks proc 
ieee international symposium circuits systems pp 

white learning artificial neural networks statistical perspective neural computation vol 
pp 

bernard widrow michael lehr years adaptive neural networks perceptrons madaline backpropagation ieee proceedings vol 
pp 

remco wim de jager offset cancelling circuit ieee journal solid state circuits vol 
sc pp 

ronald williams david zipser learning algorithm continually running fully recurrent neural networks neural computation vol :10.1.1.52.9724
pp 

ronald williams david zipser experimental analysis realtime recurrent learning algorithm connection science vol 
pp 

implementing backpropagation analog hardware proc 
international conference neural networks orlando pp 

robin martin reekie alan murray pulse stream circuits chip learning analogue vlsi neural networks proc 
ieee international symposium circuits systems london vol 
pp 

niels holger learning dynamics recurrent networks sc 
thesis institut 
yun xie jabri analysis effects quantization multilayer neural networks statistical model ieee transactions neural networks vol 
nn pp 

masuda asai minoru bibliography page yamada akira design fabrication evaluation inch wafer scale neural network lsi composed digital neurons artificial neural networks edgar anchez clifford lau eds new york ieee press pp 

chong gun yu randall geiger automatic offset compensation scheme ping pong control cmos operational amplifiers ieee journal solid state circuits vol 
sc pp 

index page index priori knowledge abbreviations xii absolute temperature iii ac signal xvi ac couple learning hardware accuracy vii activation function activation functions adaptability adaptive system adaptive systems adaptive adc algorithm variations amorphous silicon storage amount learning hardware analogue adjustment analogue computing accuracy analogue neural networks analogue vlsi ann ensembles analogue vlsi ann properties analogue vlsi learning ann properties ann applications ann model map easily hardware ann fit technology ann application invariant application specific applications motivations architecture artificial neural network artificial neural networks asic interconnection associative memories asynchronous attractor dynamics auto offset compensation auto zeroing simulation back propagation ann architecture back propagation chip computing elements index page back propagation chip set improvements back propagation learning back propagation mode back propagation neuron chip back propagation neuron schematic back propagation neuron back propagation synapse chip back propagation synapse column row element back propagation system hardware back propagation system back propagation weight update schematic back propagation basics batch learning bibliography binary coding inputs bipolar transistors bit absolute measure bit relative measure bjt boltzman constant boltzmann machines boundary effects bpl building block components bulk threshold parameter capacitive storage cco cerebral cortex channel length modulation constant channel length modulation parameter channel length channel width chip compound chip design chip designs chip measurements chip chip set improvements chip loop training choice learning algorithms chopper stabilized weight updating chopper stabilizing clamped derivative output classification clock generator cmos process collector current columns synapses comments topology common centroid layout computation requirements computed neuron derivative computing accuracy conjugate gradient method connection strength connection strengths connection updates second connections second consensus cost function consensus decision consensus trainer contents viii continuous time nlbp neuron continuous time non linear backpropagation neuron continuous time rtrl system control systems convolution cost function offsets cost function cps critic cups current auto zeroing principle current controlled oscillator current conveyor index page current conveyor current current input inherently current levels current mismatch current subtraction row current subtraction synapse cut dac resolution enhancement dac iv data compression data conversion dc signal xvi definitions degenerated momentum derivation algorithm derivative computation avoided derivative computation derivative perturbation design strategy deterministic neurons device orientation different neuron non linearities different neuron transfer functions different parabola nonlinearities different parabola transfer functions differential quotient derivative approximation digital level shifter digital pc interface digital storage digital weight updating hardware principle digital weight updating hardware discrete input alphabet discrete time feedback discrete time nlbp neuron discrete time non linear backpropagation neuron distributed neuron distributed neuron synapse distributed neuron synapses domains signalling double resolution conversion drain current variance drain current driving force vlsi rate dsp dynamic learning rate dynamic range current gain cancelling edge sampler sampling edge sampler effective bias current effective connection primitives second effective maximum synapse weight electronic synapse electronically computed neuron derivative elementary charge eliminate need matched components emitter area energy learning hardware english iii enhanced performance entropic cost function epoch length epoch error back propagation error measure eta finder example described problems exchanging inputs outputs expandable neural network expandable recurrent neural network exploit implicit multiplication fahlman perturbation index page fault tolerance fault tolerant feedback finite automaton finite state machine fires firing rate generation synapse chip floating gate floating gate storage focus electrical properties folding synapse matrix font xvi forward early voltage forward emitter collector current gain forward mode neuron characteristics forward mode synapse characteristics forward mode bpl synapse column element forward mode bpl synapse row element forward mode neuron transfer characteristics forward mode synapse transfer characteristics forward mode weight offsets forward mode quadrant multiplier fsm fully interconnected gate oxide capacitance general ann architecture general high level computational anns general neural network model general process parameter canceling circuit general purpose analogue neural network generalization ability gilbert multiplier global process variations gradient descent algorithms gradient descent learning 
gradient descent learning gradient descent handles hard limiter hard soft hybrid synapses hardware compatible hardware considerations hardware consumption hardware efficient approach hardware efficient learning hardware implementation hardware chip hebbian learning hessian high accuracy calculations higher order neurons hopfield networks huge massively parallel systems human project hyperbolic tangent neuron hyperbolic tangent transfer function implementation chip backpropagation implementation rtrl hardware implementation neural network implementing anns analogue hardware implementing learning algorithms analogue hardware improving derivative computation including algorithmic improvements index inner product multiplier inner product multipliers input bandwidth input indices input vector instant cost function index page integral non linearity integrated circuit issues integrated circuit layout integrated circuits internal logic level flight training ipm kronecker delta large learning rates lateral bipolar mode symbol lateral bipolar mode lateral bipolar mode lateral bipolar mode layer parallel back propagation hardware layer synchronous weight update layered feed forward neural network layout matched transistors layout lbm learn mode learning epoch learning example learning subsequence learning cycle learning hardware learning loop gain learning rate learning speed learning significant bits letter input letters linear operation linear multiplier list figures xvii local process variations low cost algorithmic improvements low power applications majority decision mapping algorithm vlsi massively parallel learning matrix vector multiplier measured neuron transfer function measured synapse characteristics measured synapse neuron step response measured synapse neuron transfer characteristics memories memory requirements miscellaneous mismatch mlp modules momentum inclusion momentum parameter momentum mos gilbert multiplier mos resistive circuit multiplier mos resistive circuit mos transistor symbols mos transistors motivation gradient descent mrc operated forward mode mrc operated reverse mode mrc resistive equivalent mrc multi layer perceptron multidimensional chopper stabilization multi layer perceptron multiplier multipliers multiplying dac synapse multiplying dac 
