hierarchical probability estimation martin bichsel university zurich department computer science multimedia laboratory 
zurich switzerland ifi unizh ch estimating probabilities measured numbers occurrences events provides central link probability theory real world applications 
important class applications probabilistic events correspond digitized outcome analog sensor 
shows theoretically experimentally events governed natural similarity relation imposes subtle highly effective priori constraints meta probability distribution probability distribution probability values 
application constraints significantly improves probability estimates measurements 
results applied estimating order markov model parameters image processing applications 
virtually real world signal processing problem non deterministic component 
adequate description statistical terms joint probability distributions 
numerous examples different areas pattern recognition data compression process control 
powerful techniques theorems hand allowing find optimal solutions soon probability distributions known 
examples bayes rule pattern recognition problems shannon theorem data compression 
difficulties arise comes determining probability distribution admissible patterns 
measure probabilities directly measure number occurrences patterns statistical experiment estimate probabilities measurements 
difficulties statistical probability estimation illustrated example coin flipped twice outcome twice head 
experiment unique correct answer telling probability head supported consortium author swiss national science foundation second author 
british machine vision conference tail 
course reasonable estimation method probability estimates usually get better experiments 
unfortunately coin example quite typical situations image analysis number possible patterns number different events comparable large compared number measurements taken reasonable effort 
experiment patterns occur 
real large extent unsolved problem consists estimating higher order joint probabilities patterns pattern group data values taken data points specific relative positions 
derives important constraint data representing digitized outputs analog sensor 
constraint improves joint conditional probability estimates illustrated probability distributions patterns natural images 
considerations hold data types audio data 
exploiting hierarchy digital sensor outputs subtle differences unrelated events side events governed physically similarity relation side 
section shown cases lead different priori meta probability distributions probability distributions probability values lead different relations measured numbers occurrences probability estimates 
carry classical statistical experiment consisting trials count number occurrences event mg experiment distinguishable events 
counts want estimate probabilities 
denote probability estimate fn nm estimate denotes expectation value ef ng measuring occurrences event independent repeated experiment consisting trials experiment repeated conditions determine numbers unspecific form stochastic experiment events equivalent priori expectation favor particular combination probability values 
equivalence means index label meaning 
equivalently permutation indices 
case best probability estimate counts subsequently called equivalence probability estimate 
result derived applying principle maximum ignorance probability values treated random variables 
principle maximum ignorance priori assumption combination probability values fulfilling equally vsm meta probability distribution probability distribution random field vsm volume dimensional unit simplex 
maximum ignorance principle consistent assumption equivalent random events meta probability distribution symmetric permutation indices 
british machine vision conference surprising consequence maximum ignorance principle best estimate subsequently called direct probability estimate 
ae ae gamma eq 
differs significantly small values 
gamma denote digital sensor output digitized pixel value ccd sensor denotes number different digital values typically 
imposes similarity relation events implies different possible digital sensor outputs 
may priori expect pfi different pfi pfi values treated random variables 
expectation fact digital sensor value corresponds analog variable result numerous correlated physical effects 
sensor element image pixel image sensor camera receives light particular surface element scene 
typical effects contributing pixel value ffl surface properties albedo roughness specularity ffl distribution light falling surface element ffl orientation surface element ffl noise camera digitizer electronics dust air defocus continuous nature macroscopic physical variables lets assume probability distribution contributing effect smooth resulting probability distribution joint effect smooth 
fact uncorrelated gaussian noise causes resulting probability distribution smooth scale noise variance 
similarly contributing effect imposes weak smoothness constraint appropriate scale 
strong priori constraints relate probability value pfi pfi 
different weak constraints apply 
index describes sensor output permutation symmetry priori meta probability distribution expected valid 
priori knowledge principle maximum ignorance led eq 
invalid 
better probability estimation methods exist 
possible strategy derive better probability estimates consist replacing maximum ignorance principle different explicit priori distribution 
far unfortunately find computationally manageable way 
efficient method implicitly uses improved meta probability distribution calculating probabilities hierarchical way 
method mimics hierarchical way various effects play produce digital sensor output 
idea hierarchically subdivide set possible digital sensor outputs pairs subsets division priori knowledge probability subset minimum 
pair subsets british machine vision conference apply principle maximum ignorance 
simplicity reasons assumed power 
illustrate procedure example 
procedure starts dividing complete set digital sensor outputs subsets 
maximum ignorance achieved number elements subsets equal values subset maximally different values second subset 
best choice splits complete set subsets differ significant bit 
splitting maximizes mean squared difference mean absolute difference sets 
elements set similar elements second set elements different 
consequence little priori knowledge probabilities sensor value falls set maximum ignorance principle approximation situation 
treat situation experiment events eq 
applies 
events 
denote number occurrences sensor value 
applying maximum ignorance principle leads pfi pfi denotes fh sensor value lies subset procedure continues splitting maximum ignorance achieved probabilities sensor value falls sub subset 
repeating previous arguments leads splitting groups sensor elements differ second significant bit 
loss generality illustrate set subdivided sets 
division values set maximally different values second preceding arguments repeated principle maximum ignorance applied 
leads pfi hg pfi hg pfi hg describes conditional probability value falls lower quarter lower half measurements combining previous results leads pfi pfi hg delta pfi pfi pfi hg delta pfi priori expectation probability pfi similar pfi pfi implicitly taken account probability estimates common factor 
factor calculated higher precision smaller tendency second factor due fact factor contains summands 
additional effect implicitly takes account priori expectation 
british machine vision conference splitting procedure repeated subdividing subset subset consists element fig 
case final probability pfi consists product terms 
order able express idea compact form define hierarchical sets digital sensor outputs differ significant bits 
delta xi delta xi gamma xi denotes integer division 
example obtain values differ significant bits 
definition hierarchical probability estimate fi value defined fi pi log gamma sk sk hierarchical probability estimate desired property large gamma converges pi log gamma sk sk fig log gamma intermediate terms cancel 
equation shows hierarchical probability estimates calculated simple efficient way 
compared equivalence probability estimation form fi advantages 
maximum ignorance principle applied pairs groups sensory values priori smoothness expectation low maximum ignorance principle appropriate 
term eq 
closer best estimate 

data values close terms high eq 
identical terms low differ 
reflects priori expectation neighbouring values similar probabilities 

terms low priori smoothness expectation highest calculated precision show largest tendency probability 
due fact terms summands terms high hierarchical probability estimates optimum sections demonstrate hierarchical probability estimation leads considerable improvement compared traditional equivalence direct estimation 
testing estimated probabilities quality estimated probabilities tested number experiments investigate predictability joint statistics natural images 
results particularly important adaptive image compression 
general procedure testing estimated probabilities consists carrying subsequent statistical experiments governed statistics 
order test predictability joint statistics natural images british machine vision conference general scheme image divided distinct parts 
joint statistics collected part 
assuming stationary joint statistics statistics second image part predicted part 
measured numbers occurrences part denoted measured numbers second part denoted gamma gamma denote total number measurements parts 
relative number occurrences predicted accurately possible 
quality estimated probabilities tested different methods 
method calculates squared deviation estimated probabilities second part oe gamma gamma quantity subsequently called experimental variance 
second method shannon theorem states minimum expected code length achieved assign exactly gamma log bits symbol context image processing refers single gray value pattern consisting gray values 
shannon optimum mean code length pattern entropy gamma gamma log shannon theorem states gamma gamma log gamma identify estimated probabilities number occurrences pattern independent test series examples 
quantity gamma gamma log subsequently called experimental entropy suited test estimated probabilities 
experimental entropy describes mean code length pattern ideal lossless compression algorithm achieve independent test data estimated probabilities 
experimental entropy takes minimum expectation value estimated probabilities true probabilities coincide 
experimental entropy closely related kullback leibler information directed divergence gamma log gamma log constant independent probability estimation method 
information widely image processing measuring similarity probability distributions 
british machine vision conference application order markov models assume pattern formation process ergodic stationary counting number occurrences patterns sliding sub window provides useful information global image statistics 
assuming additionally probability particular pixel depends neighbourhood directly leads markov models 
markov models important signal processing tools 
parameters markov model conditional probabilities relating value particular data point values neighbouring points 
markov models example texture analysis image compression speech recognition 
verified predictions section order model characterized conditional probabilities pfi ji image processing denotes value pixel pixels enumerated scan line order 
markov models pfi ji independent write pfi ji 
denote number gray levels 
contexts model characterized probability values results sections applied 
weighting quality measures section eqs 
relative number occurrences context generalized quality measures obtained oe gamma tot gamma pfi ji gamma tot gamma log pfi ji gamma gamma tot log pfi ji formulas denotes number occurrences value previous value gamma number occurrences value tot gamma total number pixel pairs write fh numbers calculated large number images 
image calculated quarter image calculated subsequent quarters image 
way large number independent pairs statistics obtained 
pairs compared equivalence probability estimation direct probability estimation hierarchical probability estimation quality measures eq 
eq 

numbers obtained conditional accumulator implemented array actual pixel value previous value 
accumulator new value training set updated lines pseudo code british machine vision conference get value program start element initialized zero 
accumulator values obtained accordingly 
experiment equivalence direct probability estimation equivalence probability estimates calculated fi ji gamma eq 
prior knowledge meta probability distribution generalized conditional probabilities 
experimental variance oe eq 
experimental entropy eq 
eq 
calculated images various sizes including brodatz textures face database set high quality images 
experimental entropies shown table function size training set 
mean experimental entropy image class obtained averaging images specific class 
completeness quality measures calculated direct probability estimate fi ji gamma corresponding experimental variances oe listed table 
experimental entropies listed log fi ji means infinite experimental entropy results 
comparison experimental variances equivalence direct probability estimation shows small differences favor probability estimation 
type faces brodatz high quality size bytes mean experimental variance oe mean experimental entropy mean experimental variance oe table results equivalence direct probability estimation 
experiment hierarchical probability estimation second experiment fact exploited image pixel value result digitizing analog brightness value 
case results section apply probability values estimated fi ji pi log gamma sk sk british machine vision conference type faces brodatz high quality size bytes mean experimental variance oe mean experimental entropy oe oe table results hierarchical probability estimation 
file size relative experimental entropy relative experimental entropy file size generalizing eq 
conditional probabilities 
table shows results obtained calculating experimental variance experimental entropy hierarchical probability estimation 
lines compare results table results table show systematic improvement hierarchical probability estimation 
systematic improvement test images individually 
improvement prominent small images large ones illustrated experimental entropy 
expected hierarchical probability estimation probability estimates precise small image sizes 
relative improvement larger smaller image sizes 
hierarchical probability estimation example lead improvement average compression ratio images ideal lossless coder 
predicted experimentally verified basic property joint probability distribution digital sensor outputs 
showed class random variables hierarchical probability estimates considerably better equivalence direct probability estimates 
better probability estimates improve results application estimates 
results especially important data compression pattern classification 
british machine vision conference fromherz hafner popat helping accomplish manuscript 
papoulis probability random variables stochastic processes mcgraw hill 
shannon mathematical theory communication bell system technical journal pp 
pp 

bichsel pentland human face recognition face image set topology cvgip image processing published 
bichsel subtleties probability estimation university zurich multimedia laboratory technical report 
bichsel benefits noise joint probability estimation university zurich multimedia laboratory technical report 
press numerical recipes art scientific computing cambridge university press 
healey ccd camera calibration noise estimation proceedings ieee conference computer vision pattern recognition cvpr champaign illinois 
encyclopedia statistical sciences kotz johnson eds john wiley sons 
ch 
bouman liu multiple resolution segmentation textured images ieee transactions pattern analysis machine intelligence pp 

witten radford neal john cleary arithmetic coding data compression communications acm pp 

rabiner tutorial hidden markov models selected applications speech recognition proceedings ieee pp 

bruce carlson communication systems signals noise electrical communication third edition mcgraw hill pp 

handbook mathematics th edition verlag deutsch 
probability statistical optics data testing springer 
williams adaptive data compression kluwer academic publ 
