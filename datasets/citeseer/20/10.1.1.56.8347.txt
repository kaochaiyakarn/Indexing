speech recognition dynamic bayesian networks geoffrey zweig stuart russell computer science division uc berkeley berkeley california cs berkeley edu dynamic bayesian networks dbns useful tool representing complex stochastic processes 
developments inference learning dbns allow real world applications 
apply dbns problem speech recognition 
factored state representation enabled dbns allows explicitly represent long term articulatory acoustic context addition phonetic state information maintained hidden markov models hmms 
furthermore enables model short term correlations multiple observation streams single time frames 
dbn structure capable representing long short term correlations applied em algorithm learn models parameters 
structured dbn models decreased error rate large vocabulary isolated word recognition task compared discrete hmm improved significantly published results task 
successful application dbns largescale speech recognition problem 
investigation learned models indicates hidden state variables strongly correlated acoustic properties speech signal 
years probabilistic models emerged method choice large scale speech recognition tasks dominant forms hidden markov models rabiner juang neural networks explicitly probabilistic interpretations bourlard morgan robinson fallside 
despite numerous successes recognition continuous speech recognition methodologies suffer important deficiencies 
hmms single state variable encode state information typically just identity current phonetic unit 
neural networks occupy opposite spectrum hundreds thousands hidden units little intuitive meaning 
copyright american association artificial intelligence www aaai org 
rights reserved 
motivated desire explore probabilistic models expressed terms rich defined set variables dynamic bayesian networks provide ideal framework task single set formulae expressed single program probabilistic models arbitrary sets variables expressed computationally tested 
decomposing state information set variables dbns require fewer parameters hmms represent amount information 
context speech modeling dbns provide convenient method defining models maintain explicit representation lips tongue jaw speech articulators change time 
models expected model speech generation process accurately conventional systems 
particularly important consequence including articulatory model handle coarticulation effects 
main reasons occur inertia speech articulators acquired generation sound modifies pronunciation sounds 
addition dbns able model correlations multiple acoustic features single point time way previously exploited discrete observation hmms 
implemented general system doing speech recognition bayesian network framework including methods representing speech models efficient inference methods computing probabilities models efficient learning algorithms training dbn model parameters observations 
system tested isolated word recognition task 
large improvement results modeling correlations acoustic features single time frame 
increase results modeling temporal correlations acoustic features time frames 
analysis learned parameters shows kinds models capture different aspects speech process 
problem background task statistical speech recognition system learn parametric model large body training data model recognize words previously utterances 
number words natural language large impossible learn specific model word 
words expressed terms small number stereotypical atomic sounds phonemes english example modeled terms phonemes 
models phoneme learned word models created concatenating models word constituent phonemes 
example word cat phonetic transcription ae 
order model effects expanded phonetic alphabets unique symbol phoneme context surrounding phonemes 
left context alphabets phonetic unit phoneme left context possible preceding phoneme 
right context alphabets unit phoneme right context possible phoneme 
triphone modeling particularly common scheme unit phoneme context possible preceding phonemes 
phonetic units alphabets referred phones 
theoretically squares number atomic units triphones cubes number practice commonly occurring combinations modeled 
beneficial break phonetic unit substates 
state phone system example phone broken initial sound final sound doubling total number phonetic units 
precise form phonetic alphabet training data consists collection utterances associated phonetic transcription 
utterance broken sequence overlapping time frames sound processed generate acoustic features acoustic features may extracted frame notation refer features extracted ith frame regardless number 
phonetic transcription word model associated utterance 
statistical speech recognition main goal statistical speech recognition system estimate probability word model sequence acoustic observations 
focus isolated word recognition results generalize connected word recognition 
rewritten bayes rule jo desirable decomposes problem subproblems estimated language model specifies probability occurrence different words estimated model describes sounds generated 
constant respect word models different models compared computing just 
computation straightforward case isolated words focus estimation probability observation sequence word 
probability distribution usually estimated directly 
statistical models typically collection hidden state variables intended represent state speech generation process time 
sjm addition observation generated point usually assumed depend state process sjm ojs refer specification sjm pronunciation model specification ojs acoustic model 
hmms hidden markov model simple representation stochastic process kind described 
hidden state process represented single state variable point time observation represented observation variable 
furthermore markovian assumption decompose probability state sequence follows leaving implicit dependence js js gamma js case speech state variable usually identified phonetic state current phone uttered 
pronunciation model contained probability distribution js gamma designates transition probabilities phones consequently distribution phone sequences particular word 
acoustic model probability distribution ojs independent particular word 
models assumed independent time 
conditional probability parameters hmms usually estimated maximizing likelihood observations em algorithm 
trained hmm recognize words computing word model details reader referred rabiner juang 
concerned discrete observation variables created actual signal process vector quantization rabiner juang 
order allow wide range sounds common generate discrete observation variables point time fairly small range say values 
dbn representation hmm 
distinct state observation variable point time 
node graph represents variable arcs leading node specify variables conditionally dependent 
valid assignment values state variables word shown 
observation variables shaded 
simple picture ignores issues parameter tying phonetic transcriptions 
keep number parameters manageable multiple observation streams conditional independence assumption typically lee js js bayesian networks bayesian network general way representing joint probability distributions chain rule conditional independence assumptions 
advantage bayesian network framework hmms allows arbitrary set hidden variables arbitrary conditional independence assumptions 
conditional independence assumptions result sparse network may result exponential decrease number parameters required represent probability distribution 
concomitant decrease computational load smyth heckerman jordan ghahramani jordan russell 
precisely bayesian network represents probability distribution set random variables variables connected directed acyclic graph arcs specify conditional independence variables joint distribution vn jp parents parents graph 
required conditional probabilities may stored tabular form functional representation 
shows hmm represented bayesian network 
tabular representations conditional probabilities particularly easy straightforward model observation probabilities mixtures gaussians done hmm systems 
variables represent temporal sequence ordered time resulting bayesian network referred dynamic bayesian network dbn dean kanazawa 
networks maintain values set variables point time 
ij represents value ith variable time variables partitioned equivalence sets share time invariant conditional probabilities 
bayesian network algorithms 
hmms standard algorithms computing bayesian networks 
implementation probability set observations computed algorithm derived peot shachter 
conditional probabilities learned gradient methods russell em lauritzen 
adapted algorithms dynamic bayesian networks special techniques handle deterministic variables key feature speech models see 
full treatment algorithms zweig 
dbns speech recognition hmms dbn speech models decompose pronunciation model acoustic model 
acoustic model includes additional state variables call articulatory context variables intent may capture state articulatory apparatus speaker case models 
variables depend current phonetic state previous articulatory context 
mathematically expressed partitioning set hidden variables phonetic articulatory subsets sjm ajm 
bayesian network structure thought consisting layers models models 
illustrates dbn structured speech recognition manner 
sections discuss pronunciation model acoustic model turn 
pronunciation model 
zweig russell zweig shown dbn model structure represent distribution phone sequences represented hmm 
purposes simplifying presentation additional assumptions 
word model consists linear sequence phonetic units example cat assumed pronounced ae variation phonetic units order 
second assumption concerns average durations phones probability transition consecutive phones phone dependent transition probability index node keeps track position phonetic transcription words go sequence values observation word observation deterministic deterministic stochastic stochastic deterministic stochastic index phone context transition dbn speech recognition 
index transition phone word variables encode probability distribution phonetic sequences 
context observation variables encode distribution observations conditioned phonetic sequence 
valid set variable assignments indicated word picture context variable represents 
vowel usually case coarticulation causes occurrence 
number phonetic units transcription 
assignment values index variables specifies time alignment phonetic transcription observations 
specific pronunciation model deterministic mapping index phonetic unit actual phonetic value represented phone variable 
mapping specified word word basis 
binary transition variable conditioned phonetic unit 
transition value index value increases encoded appropriate conditional probabilities 
distinction phonetic index phonetic value required parameter tying 
example consider word digit phonetic transcription ih jh ih 
ih followed jh second ih followed distinction phone occurrences 
hand probability distribution acoustic emissions occurrences distinction 
impossible satisfy constraints single set conditional probabilities refers phonetic values index values 
conditional probabilities associated index variables constrained index value begins stay increase time step 
dummy word observation ensure sequences non zero probability transition phonetic unit 
binary variable observed value conditional probabilities variable adjusted transition probability cases 
conditioning transition variable ensures unbiased distribution durations phonetic unit 
deterministic variables labeled 
advantage deterministic relationships crucial efficient inference 
acoustic model 
reason dbn allows hidden state factored arbitrary way 
enables approaches acoustic modeling awkward conventional hmms 
simplest approach augment phonetic state variable variables represent articulatory acoustic context 
structure shown 
context variable serves purposes dealing long term correlations observations time frames short term correlations time frame 
purpose model variations phonetic pronunciation due effects 
example context variable represents capture vowels 
depending level detail desired multiple context variables represent different articulatory features 
model semantics enforced statistical priors training data articulator positions known 
second purpose model correlations multiple vector quantized observations single time frame 
directly modeling correlations requires prohibitive number parameters auxiliary variable parsimoniously capture important effects 
network structures tested 
experiments tested networks varied acoustic model 
dbn variants single binary context variable differed conditional independence assumptions variable 
model structures see 
articulator network context variable depends phonetic state past value 
directly represent articulatory target positions inertial constraints 
context observations observations context pd correlation phone phone articulator correlation chain acoustic models networks tested 
acoustic observations time frame 
dotted arcs represent links previous time frame 

chain network phonetic dependence removed 
structure directly represent phone independent temporal correlations 

phone dependent correlation network results removing temporal links articulator network 
directly model phone dependent intra frame correlations multiple acoustic features 

correlation network removes phonetic dependence 
capable modeling intra frame observation correlations basic way 
articulator network initialized reflect voicing chain network reflect temporal continuity 
experimental results database task test bed selected phonebook database large vocabulary isolated word database compiled researchers nynex 
words chosen goal incorporating phonemes segmental stress contexts produce variations spanning variety talkers telephone transmission characteristics 
characteristics challenging data set 
data processed ms windows generate mel frequency cepstral coefficients mfccs davis derivatives ms 
mfccs generated computing power spectrum fft total energy different frequency ranges computed 
cosine transform logarithm filterbank outputs computed low order coefficients constitute mfccs 
mfccs represent shape short term power spectrum manner inspired human auditory system 
mfccs vector quantized size codebook 
derivatives quantized second codebook 
delta coefficients quantized separately size codebooks concatenated form third valued data stream 
performed mean cepstral subtraction speaker normalization lee 
effect mean cepstral subtraction remove transmission characteristics telephone lines 
speaker normalization scales power level speaker subtracting maximum value resulting values compared utterances 
experimented dbn models context independent context dependent phonetic units 
cases started phonetic transcriptions provided phonebook ignoring stressed distinction vowels 
case context independent units simple phonemes distinct states phoneme initial final state interior states 
generate context dependent transcriptions replaced phoneme new phonetic units representing phoneme left context preceding phoneme representing phoneme right context unit 
example ae ae gamma ae ae gamma 
prevent proliferation phonetic units context dependent units seen fewer threshold number times training data 
context dependent unit available context independent phoneme initial phoneme final unit 
beneficial repeat occurrence unit twice 
phoneme original transcription broken total substates comparable context independent phonemes 
effect doubling number occurrences phonetic unit increase minimum expected durations state 
report results context dependent phonetic alphabets units occurring times training data units occurring times 
cases alphabet contained context independent units initial final segments original phonemes 
alphabets contained units respectively 
number parameters case comparable alphabet system auxiliary variable number parameters second case comparable number arises auxiliary variable added context dependent system 
note notion context sense context dependent alphabet different rep network parameters error rate baseline hmm correlation pd correlation chain articulator test results basic phoneme alphabet oe 
number independent parameters shown significant figures dbn variants slightly different parameter counts 
resented context variable figures 
context kind expressed alphabet idealized pronunciation template represents context manifested specific utterance 
training subset consisted files tuned various schemes development set consisting files 
test results reported files training tuning phases 
words phonebook vocabulary divided word subsets recognition task consists identifying test word word models subset 
training utterances development utterances test utterances 
overlap training test words training test speakers 
performance shows word error rates basic phoneme alphabet 
results dbns clearly dominate baseline hmm system 
articulatory network performs slightly better chain network networks time continuity arcs perform intermediate levels 
differences augmented networks statistically significant 
results significantly better reported state art systems dupont 
report error rate hybrid neural net hmm system phonetic transcription test set worse results conventional hmm system 
report improved performance transcriptions pronunciation dictionary cmu 
shows word error rates contextdependent alphabets 
context dependent alphabet proved effective way improving performance 
number parameters augmented context independent phoneme network performance slightly better 
augmenting context dependent alphabet auxiliary variable helped 
tested best performing augmentation articulator structure context dependent alphabet obtained significant performance increase 
increasing alphabet size network parameters error rate cda hmm cda articulator cda hmm test results context dependent alphabets cda oe 
systems context dependent unit occurred times training data third threshold 
resulted alphabet sizes respectively 
state hmm phone model top corresponding hmm model binary context distinction bottom 
second hmm states original states representing context values 
shaded nodes represent notional initial final states sound emitted 
phone models concatenated merging final state initial state 
complex model initial final states retain memory context phones 
graphs specify possible transitions hmm states dbn specifications 
attain comparable number parameters help 
terms computational requirements baseline hmm configuration requires ram process single example em iteration faster real time sparc ultra 
articulator network requires ram runs faster real time 
cross product hmm 
acoustic articulatory context incorporated hmm framework creating distinct state possible combination phonetic state context modifying pronunciation model appropriately 
illustrated binary context distinction 
expanded hmm new states original states transition model complex possible transitions point time corresponding possible combinations changing phonetic state changing context value 
number independent transition parameters needed expanded hmm times num ber original phones 
total number independent transition context parameters needed articulatory dbn times number phones 
chain dbn equal number phones 
tested hmm shown basic phoneme alphabet different kinds initialization reflecting continuity context variable analogous chain dbn reflecting voicing analogous articulator dbn 
results word error respectively parameters 
results indicate benefits articulatory acoustic context modeling binary context variable achieved complex hmm model 
expect case number context variables increases 
discussion presence context variable unambiguously improves speech recognition results 
basic phoneme alphabets improvements range 
statistically results highly significant difference baseline articulator network significant level 
context dependent alphabet observed similar effects 
having learned model hidden variables interesting try ascertain exactly variables modeling 
striking patterns parameters associated context variable clearly depend network structure 
ffic observation stream strongly correlated context variable association illustrated articulator network 
graph shows context variable value large values characteristic vowels 
information shown correlation network pattern obviously different easily characterized 
initialized context variable articulator network reflect known linguistic information voicing phonemes assumption significant single bit articulator state information learned model appear reflect voicing directly 
networks time continuity arcs parameters associated context variable indicate characterized high degree continuity 
see 
consistent interpretation representing slowly changing process articulator position 
demonstrate dbns flexible tool applied effectively speech recognition show factored state representation improve speech recognition results 
explicitly model articulatory acoustic context auxiliary variable complements phonetic state delta context association learned context variable acoustic features articulatory network 
indicative energy acoustic frame 
maximum value utterance subtracted value greater 
assuming mel frequency filter bank contributes equally ranges maximum value decibels maximum 
delta association learned context variable acoustic features correlation network 
shows quite different pattern exhibited articulator network 
clarity surface viewed different angle 
phone initial final learning continuity 
solid line shows initial probability auxiliary state value remaining consecutive time frames function phone 
variable initialized reflect voicing low values reflect voiced phones 
dotted line indicates learned parameters 
learned parameters reflect continuity auxiliary variable change regardless phone 
effect observed values auxiliary chain 
generate recognition results initialized parameters extreme values results fewer em iterations somewhat better word recognition 
variable 
context variable initialized reflect voicing results significant improvement recognition 
expect improvements multiple context variables 
natural approach modeling effects arise inertial quasi independent nature speech articulators 
acknowledgments benefited discussions nelson morgan jeff bilmes steve greenberg brian kingsbury dan gildea 
funded nsf iri aro daah 
grateful international computer science institute making available parallel computing facilities possible 
bourlard morgan 
connectionist speech recognition hybrid approach 
dordrecht netherlands kluwer 
davis 
comparison parametric representations monosyllabic word recognition continuously spoken sentences 
ieee transactions acoustics speech signal processing 
dean kanazawa 
model reasoning persistence causation 
computational intelligence 
dupont bourlard fontaine 

hybrid hmm ann systems training independent tasks experiments phonebook related improvements 
icassp 
los alamitos ca ieee computer society press 
ghahramani jordan 
factorial hidden markov models 
machine learning 
lauritzen 
em algorithm graphical association models missing data 
computational statistics data analysis 
lee 

automatic speech recognition development sphinx system 
dordrecht netherlands kluwer 
peot shachter 
fusion propagation multiple observations 
artificial intelligence 
fong wong leung 
phonebook phonetically rich telephone speech database 
icassp 
los alamitos ca ieee computer society press 
rabiner juang 

fundamentals speech recognition 
prentice hall 
robinson fallside 
recurrent error propagation speech recognition system 
computer speech language 
russell binder koller kanazawa 
local learning probabilistic networks hidden variables 
ijcai 
montreal canada morgan kaufmann 
smyth heckerman jordan 
probabilistic independence networks hidden markov probability models 
neural computation 
zweig russell 
compositional modeling dpns 
technical report ucb csd computer science division university california berkeley 
zweig 
speech recognition dynamic bayesian networks 
ph dissertation university california berkeley berkeley california 
