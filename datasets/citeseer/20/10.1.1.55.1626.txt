theory refinement knowledge feature set selection brendan burns andrea department computer science science center williams college ma andrea cs williams edu classification systems depend having best relevant set input features classification decision 
true classifiers inductive learners build 
describes indigent improving known domains genetically engineered features 
indigent utilizes genetic search perform knowledge feature selection 
assumes provided neural network 
performs genetic search better network topologies focusing entirely incorporation deletion input features 
indigent viewed ways theory refinement system feature selection system guided domain knowledge 
conjunction extensive theory refinement systems 
classification systems depend having best relevant set input features classification decision 
true classifiers inductive learners build 
describes indigent improving known domains genetically engineered features 
indigent utilizes genetic search perform knowledge feature selection 
expert classification systems proven effective decision makers types problems 
accuracy systems highly dependent accuracy human expert domain theory 
human experts learn create set rules subject number 
significantly experts greater lesser extent restricted tradition scholarship preceded inability examine large amounts data rigorous fashion effects boredom frustration 
result human theories erroneous incomplete 
russell norvig escape dependency machine learning systems developed automatically refine correct expert domain theory learn classification theory data completely bypassing theory provided expert 
theory revision systems applied expert theories concentrate reformulation knowledge provided reformulation input features 
case inductive learning classifier data system provided fairly small set features expert determined particular task 
set may suboptimal 
case idea considering potential features may simply unwieldy 
indigent system developed perform feature selection classification tasks 
assumes provided neural network 
performs genetic search better network topologies focusing entirely incorporation deletion input features 
indigent viewed ways theory refinement system feature selection system guided expert provided domain knowledge 
selected features input standard inductive learning system extensive theory refinement system 
section discuss background related 
follow description indigent algorithm 
section discusses results applying indigent domains molecular biology 
demonstrate taken theory revision system indigent focus feature selection gives performance comparable greater theory revision systems manipulate theories 
discuss results applying standard inductive learning algorithms features selected indigent 
follow description show feature set selection impact extensive theory revision system 
background related number researchers shown selection feature set dramatically improve classification system performance earlier methods focus selection set relevant features prior application learning algorithm 
space potential feature subsets large algorithms form non deterministic search 
system directly related indigent distal yang honavar 
distal uses genetic search find optimum features constructive neural networks 
constructive networks built greedy search adds single hidden node time network classified entire training set 
method fundamentally different indigent important ways 
construction neural network encode expert information 
secondly greedy search approach create networks single hidden layer 
restrictions limit ability distal capture accurate networks large training sets 
additionally severely curtail interpretability constructed networks 
distal showed ability improve performance constructive neural network algorithm compared accuracies features 
vafaie dejong uses genetic algorithm search optimal feature sets 
designed select features optimize performance decision tree learning system 
binary representation features genetic algorithm similar indigent 
generation produces offspring represent entirety features parents 
distal utilize expert information 
result initial feature sets arbitrarily generated may ultimately hinder accuracy speed algorithm 
significantly different indigent module focuses feature construction combination multiple existing features single new feature 
able increase accuracy task locating eyes images 
exception general non deterministic search feature selection focus almuallim dietterich performs exhaustive search feature subsets finds smallest subset features property data points feature subset agree classification agrees 
focus computationally expensive unable handle noisy data 
relief kira rendell algorithm selects set relevant features filters data 
relief computes relevance examining arbitrary number data points chosen random producing weight represents summed square differ selection feature set performed indigent algorithms differs implicit feature selection performed inductive learning systems goal indigent feature selection eliminate noisy data order improve machine learning algorithm performance 
ence nearest positively nearest negatively classified neighbors 
weights summed feature 
feature weight near zero assumed mean feature equally close average positive negative classes 
feature irrelevant 
feature weight positive indicates connection class features judged relevant 
subset relevant features chosen relevance threshold value set inspection automatically proportional size feature set 
asker maclin demonstrated asker maclin feature set selection crucial success system detected presence exploratory images returned venus 
method focuses creation number neural networks different subset total input features 
individual networks bagged breiman create final classifier 
indigent tnt indigent feature subsets hand picked rigorous time intensive process human selection 
demonstrate importance feature selection system truly autonomous 
devaney ram devaney ram developed system autonomously searches feature set space 
algorithm differs indigent tnt indigent number ways 
importantly search method backward forward sequential selection 
method search starts empty forward full backward feature set adds best feature forward removes worst feature backward 
process continues improvement 
sequential search suffers fact hill climbing algorithm 
unable traverse valley error space order ascend higher point 
genetic search non deterministic indigent tnt indigent able jump peak 
additionally sequential search algorithms unable simultaneously consider radically different feature subsets genetically combine indigent tnt indigent genetic algorithm able 
area constructive induction 
concept indigent system theory refinement system feature selection system goes michalski definition constructive induction phased search best representation space best description space 
see example bloedorn michalski 
knowledge feature selection indigent performs knowledge feature selection 
guided part expert knowledge classification domain 
performs genetic search feature space order select features best classify data domain considered conjunction domain theory 
indigent uses knowledge artificial neural networks towell shavlik noordewier represent expert theories previously written simplified form horn clause logic 
number systems neural network encodings refine variety representation types including finite state automata maclin shavlik certainty factor rule bases mahoney firstorder horn clause logic towell shavlik noordewier 
indigent uses kbann towell shavlik noordewier method encoding domain theories 
done genetically refining neural network topologies 
regent opitz system extension kbann refines domain theories addition rules input output features 
regent explore modification network input set indigent 
towell shavlik earlier extension kbann modify set input features 
system significantly different indigent chooses input nodes add induction erroneous hidden nodes feature set utilizing genetic search 
system mahoney developed genetically refine certainty factor rule bases 
regent genetically refines topology hidden nodes networks 
attempt improve set input features adding relevant judged quinlan quinlan information gain metric nodes network fails reach certain level accuracy 
treatment input set selection significantly weaker indigent adds individual input nodes secondary refinement method doesn explore entire space possible input feature sets 
indigent finding accurate input feature subset indigent focuses evolving input features network chooses 
demonstrated errors corrected simply modifying bottom nodes network towell shavlik 
expanding idea indigent algorithm seeks search space subsets total feature set order discover subset features creates accurate classifier 
discovering optimal subset challenge exhaustive search possible subsets currently intractable 
genetic algorithm ideal search input features easily represented genotype feature subset error space full local maxima 
step indigent algorithm generation initial genotype genetic algorithm 
indigent initial genotype includes input features referenced expert domain theory features included initial domain theory deemed important quinlan decision tree generator initial genotype simply binary string bit represents input feature corresponding value represents presence absence subset 
indigent takes initial genotype representing input features mutates create initial population genotypes 
indigent default population size 
initial population networks created domain theory genotype population networks evaluated fold cross validation 
networks evaluated networks worst best fitness values respectively noted 
parent genotypes selected 
parents chosen manner proportional fitness values parent genotypes crossed create new child genotype 
child genotype created mutated mutation operator applied create initial population 
fitness function applied child genotype 
resulting fitness value higher fitness worst genotype population worst genotype replaced child 
search performed find new best worst fitness values 
process creation evaluation potential replacement repeated arbitrary number generations 
stage best network saved file 
specified number generations reached best network indigent result 
operation indigent summarized 
crossover indigent crossover indigent performed follows input features appearing parent genotypes added child genotype 
feature appears parent genotype chance proportional fitness value parent genotype added child 
merging genotypes indigent genetic algorithms operate splicing fractions parent genotypes form child genotype 
reality features chosen decision tree correctly classify data 
fold cross validation method testing program ability learn memorize data set 
data randomly divided sets 
algorithm performed times single set held network trained remaining 
network tested held unseen data set 
accuracy network reported average accuracy folds 
choice accomplished wheel fortune selection process genotype receives segment wheel size corresponds genotype fitness 
initial genotype parents evaluation mutation initial domain theory best network child initial population decision tree selection crossover indigent algorithm 
methods fairly similar 
produce child logical parent genotypes addition number elements existing solely single parent 
indigent method crossover doesn restrict placement features single parents proximity 
method crossover allows greater possible number genotypes created single pair parents 
mutation indigent indigent mutates genotype follows potential input feature random decision add remove feature 
indigent favors adding input features deleting 
reasoning twofold assumed domain theory largely correct 
nodes mentioned domain theory useful network nodes mentioned domain theory 
adding unknown inputs beneficial network removing known inputs 
shown kbann networks ability backpropagation eliminate detrimental antecedents reducing weight connecting synapses opitz shavlik 
having genetic algorithm performing task best redundant worst detrimental performance 
new feature set selected corresponding network constructed 
networks fully connected new inputs added linked hidden layer 
process allows features domain theory impact network accuracy 
experiments section describes experiments evaluate performance indigent 
domains data sets experiments come biologic domain concerned identification certain features string nucleotides dna rna 
domain important examine research human genome project sequencing far dna effectively examined humans 
automated system extraction features dna useful tool biologists 
additionally domains effective areas test indigent taken real world contain irregularity noise challenge classification system 
domains useful evaluation indigent tnt indigent systems developed classify domains established benchmark accuracies compare indigent tnt indigent results 
promoter domain domain problem deciding promoter exists starting point sequence coli dna initial domain theory obtained dataset machine learning archive university wisconsin domain contains positive classification examples different source negative classification examples generated sliding nucleotide window promoter free sequence nucleotides 
data folds data generated random fold numbers kbann regent experiments university wisconsin benchmark results indigent tnt indigent compared 
dataset contains positive examples negative examples ratio randomly dis promoters sequences allow rna polymerase initiate regulate transcription dna proteins 
ftp cs wisc edu machine learning datasets tributed folds 
ribosome binding site domain second domain examined identification ribosome binding site rbs point sequence rna promoter domain dataset folds domain theory obtained machine learning archive university wisconsin 
rbs domain positive examples negative examples generated sliding nucleotide window rbs free sequence nucleotides 
promoters random folds folds kbann regent experiments 
splice junction domain third domain identification splice junctions task domain determine location marks boundary section retained exon section removed intron boundary falls intron exon location boundary 
splice junction domain differs previous domains classification categories 
classes ei boundary section retained exon section removed intron boundary intron exon location boundary 
splice junction domain consists twelve strands nucleotides 
positive examples negative examples 
random folds created locally 
subset total dataset order maintain ratio positive ei negative classifications 
ratio positive negative rbs promoter data sets 
folds smaller size data set created nearly correlation data points features 
data set differs slightly figures reported kbann regent experiments show performance different data set distribution positive negative examples 
measuring performance goal indigent create neural network string input data case dna returns appropriate classification dna 
measure accuracy percentage examples classifier judges correctly 
important note classification particular position dna strand 
creates slightly ribosome binding sites locations rna binds ribosome transcription protein 
splice junctions points sequence dna form boundaries areas dna retained removed splicing 
artificial classification scheme 
negative examples generated single continuous strand nucleotides positive examples distinct individual strands 
result accuracy may exactly correspond algorithm ability discover positive classifications single long strand dna 
important note indigent tnt indigent run generations numbers reported regent represent runs generations 
experimental setup experiment runs indigent generations conducted data domain 
training training data shuffled prior training iteration order encourage generalization networks 
best network generation reported 
best network final generation result run accuracy evaluated 
accuracy knowledge neural networks slightly variable accuracy reported best networks represent average number evaluations 
accuracies networks averaged form final accuracy indigent domain 
numerical results discussing results experiments described important note prior running experiments 
preliminary runs graphed average performance genetic search runs indigent 
average performance genetic search runs indigent similar expected performance genetic search individual performance run indigent displays steep jumps upward performance improvement 
additionally majority improvements come generations generations tend improve performance significantly 
graphs performance regent show similar results choice regent evaluation arbitrary 
reason indigent run generations came noticing longer run time didn necessarily improve performance manner proportional computational expense 
fact cases performance decreased generations population increasingly homogeneous 
observation interesting commentary concept time algorithm discussed opitz 
stated goal time algorithm algorithm produces steadily increasing performance corresponds length computation 
graphs show primarily goal performance increasing proportion computation possible goal 
generations pass improvements performance increasingly expensive 
time needed increasing accuracy infinite limit practically generations improvement computation generally worth expense 
course design time algorithm computation may halted user discretion observation time approach guidance appropriate number generations run genetic search algorithm 
shows final average accuracy indigent generations compared accuracy neural network classification systems data 
shows feature selection ability improve initial domain theory degree extensive theory revision systems regent shows statistical information indigent feature sets 
promoters rbs splice junctions increase total differences initial indigent feature sets 
discussion results indigent shown effective simple method theory revision 
indigent attains level performance par regent extensive theory revision system 
appealing result number reasons 
foremost feature set revision understandable explainable form theory revision 
traditionally theory revision systems especially neural network systems suffered inability explain accuracy 
numbers showing improvement may document improvement theory hard real life manager telephone system dam power plant relinquish control system programmers promise high accuracy really explain works 
black box indigent feature set revision improves domain theory altering recognition 
doing aids improvements created ease application solution realworld problems 
maximal accuracy goal ultimately useless system 
additionally feature set revision important guide reformulation expert theory 
feature set selected improves performance domain theory extensive theory revision influenced selection features lead greater improvements 
theory revision addressed generation indigent system tnt indigent discussed 
indigent results interesting effect learned feature sets inductive learning systems 
shown voted indigent feature set improved performance showed slight improvements ripper rl promoter domain 
importantly shown classifiers learned smaller sense simpler input features selected indigent 
limitations indigent feature selection algorithm indigent helped limited dependence domain theory 
indigent aware domain theory information necessarily selects feature sets perform conjunction theory 
selected feature sets machine learning systems utilize initial domain theory may restrictive 
machine learning system receive domain theory priori may necessary receive information data order induce theory 
indigent feature selection simply select features relevant domain selects features relevant domain theory 
understandable absence domain theory selected feature sets may appropriate 
taken theory refinement algorithm indigent limited ability correct erroneous initial antecedents 
theory contains errors layer higher input layer indigent way correct 
indigent add remove nodes hidden layer truly explore possible network topologies 
reasonable imagine combining indigent ability select inputs general theory refinement strategy give improvements 
addressed implementation tnt indigent 
find indigent conjunction tnt indigent feature selection adds performance extensive theory revision system 
tnt indigent total neural topology search tnt indigent total neural topology indigent system able modify entire topology input hidden nodes general course tnt indigent algorithm similar assumed designer expert theory clear idea correct categories classification modifying output nodes network doesn sense 
system promoter rbs splice junction indigent regent topgen kbann indigent accuracy compared systems deviation parentheses accuracy rbs data accuracy promoter data system features indigent features features indigent features rl ripper indigent feature set accuracy systems indigent 
significant differences come mutation crossover operators described 
indigent tnt indigent begins initial neural network 
genotype indigent binary string corresponding possible input features tnt indigent maintains population members entire neural networks 
tnt indigent mutation crossover operators performed entire networks 
tnt indigent mutates initial network adds new networks original order create starting population networks default 
networks evaluated fold cross verification order measure generalization memorization 
parents chosen population selection method indigent 
tnt indigent keeps track number times particular network generated child 
number repetitions exceeds set limit parents chosen random effort encourage algorithm produce diverse range children 
having chosen parents networks crossed create child 
child evaluated fitness function 
fitness exceeds worst member population topology child represented population worst member population replaced new child 
indigent process continues set number generations 
mutation tnt indigent tnt indigent stores manipulates entire networks population 
result mutation complicated indigent 
increase complexity mutation directed learning process 
mutation occurs distinct steps 
preventing duplicate networks necessary maintenance diverse population 
hidden nodes network mutated 
node hidden layer chance equivalent mutation rate mutated 
node mutated represents logical logical generated greater number false positives false negatives 
node considered generated false positive error value positive 
conversely node generates false negative error negative 
practice false positives negatives accumulated error greater threshold value 
manner node mutated designed reduce number errors node 
node logical false positives negatives new antecedent node added node bias increased maintain node 
node false negatives positives nodes added 
node mutated antecedent newly created node inherits mutated node outgoing synapses 
node created additional antecedent created node 
node false positives modified similarly false negative node 
consequent node created node 
node false positives may mutated removal antecedent weakest connecting synapse 
node false negatives antecedent node added mutated node 
explanation please see 
mutation rate set manually user 
mutation rate percent 
node considered bias slightly sum incoming positive weights 
node bias slightly greater sum incoming negative weights 
true statement greater distance sum incoming weights bias wins see opitz details 
classifier size rbs classifier size promoters system indigent features features indigent features features pruned pruned pruned pruned rl ripper comparison sizes learned classifiers manner mutation similar regent opitz 
regent unable subtract nodes neural network mutation tnt indigent able 
input nodes mutated separately manner identical method indigent 
binary string representing inputs generated network input features string mutated slight bias adding features 
mutated genotype translated back network input nodes 
final step mutation fully connect network nodes added gain connections nodes layer nodes layer 
crossover tnt indigent tnt indigent crosses parent networks create new child network 
networks encode logical information domain theory important process crossover destroy information 
achieve tnt indigent attempts cross entire logical rules 
step crossover copy output nodes child 
tnt indigent searches nodes exist parent networks copies child network 
see 
process begins hidden layer directly outputs proceeds downward 
result node copied synapse connects node existent child network copied 
node added network value synapses average values synapses parent networks 
step crossover determine nodes parent network significant 
node defined significant strongly connected node added child network 
process repeated input nodes 
results tnt indigent separate experiments run algorithm 
consisted fold cross validated run tnt indigent initial domain theory starting point 
second experiment fold validated run tnt indigent discussed earlier outputs assumed constant 
node strongly connected weight connecting synapse greater minimum weight set user 
voted feature sets indigent runs starting network 
separate experiments run order better understand contributions tnt indigent indigent 
running tnt indigent indigent features effect tnt indigent initial domain theories accurately gauged 
additionally results tnt indigent compared equivalently regent running tnt indigent seeded indigent features comparing results runs effects indigent features genetic theory revision system examined 
experiments run domains described earlier 
accuracies reported averaged way indigent final accuracies 
numerical results shows accuracy separate runs tnt indigent compared knowledgebased neural network systems 
runs tnt indigent started initial expert domain theory features seeded results represent original expert domain theory conjunction best voted features selected indigent 
discussion results indigent shown effective simple method theory revision 
indigent attains level performance par regent extensive theory revision system 
appealing result number reasons 
foremost feature set revision understandable explainable form theory revision 
criticism feature set revision simply adds features correspondence created features data 
order prevent ratio features data points domains approximately 
additionally indigent added average features feature sets 
indigent resulting feature sets contain average thirds features 
clear possibly correspondence features data points 
additionally indigent removes features initial domain theories providing evidence process feature selection blindly add new inputs 
original topology false negatives false positives remove node add node add nodes add node add nodes mutation tnt indigent aa parent parent new child logical crossover tnt indigent system promoter rbs splice junctions tnt indigent tnt indigent regent kbann tnt indigent accuracy compared systems seeded deviations parentheses additionally feature set revision important guide reformulation expert theory 
feature set selected improves performance domain theory extensive theory revision influenced selection features lead greater improvements shown results tnt indigent 
results tnt indigent important reasons 
results runs seeded indigent features show indigent compatible theory revision system modifies neural topologies 
resulting combination shown produce domain theories equally accurate previously existing machine learning methods 
seeding tnt indigent indigent features bears discussion 
rbs domain feature set seeding voted indigent feature set promoters splice junction domains feature set seeding feature set separate indigent runs produced accurate result 
difference occurred voted rbs features resulted accuracy matched accuracy average indigent accuracy voted feature sets promoters splice junctions performed significantly worse average indigent accuracy 
clear reason discrepancy performance 
drawn features unique indigent feature set crucial promoter splice junction performances features important rbs domain consistent feature sets 
may indicate rbs domain single correct feature set promoters splice junction domains 
alternatively may indicate rbs feature sets closer single correct set greater distance error space necessarily disjoint 
regardless reason performance clearly dependent initially seeded feature set 
tnt indigent results interesting level 
important aspect theory revision ultimate accuracy discussed simplicity theory 
tnt indigent results seeded runs significant modifications domain system rbs promoters splice tnt indigent tnt indigent topgen regent tnt indigent nodes added compared systems seeded 
theories smaller previous systems see 
tnt indigent shows performance equal better regent dramatic decrease number added nodes 
discussed simpler changes expert system changes understandable 
changes understandable system trusted 
additionally simple theories result simple implementations quicker train complicated models 
curiously simplification changes result indigent feature sets 
interesting machine learning systems indigent features appear simplify resulting theories 
examined indigent system genetic selection optimum feature sets knowledge neural networks 
explored application feature sets machine learning systems 
indigent shown ability improve accuracy initial domain theory 
additionally shown feature sets selected may contain information generalized machine learning systems 
importantly shown feature set selection important way increase machine learning system accuracy obfuscating clarity 
includes applying indigent number domains order examine capabilities 
useful construct inductive rule learning system take greater advantage information provided indigent selection feature sets tnt indigent currently 
almuallim dietterich 
learning irrelevant features 
proceedings ninth national conference artificial intelligence 
asker maclin 
feature engineering classifier selection case study volcano detection 
proceedings fourteenth international conference machine learning 
bloedorn michalski 
data driven constructive induction 
ieee trans 
intelligent systems 
breiman 
bagging predictors 
machine learning 
devaney ram 
efficient feature selection conceptual clustering 
proceedings fourteenth international conference machine learning 
kira rendell 
feature selection problem traditional methods new algorithm 
proceedings tenth national conference artificial intelligence 
maclin shavlik 
knowledgebased neural networks improve algorithms refining chou algorithm protein folding 
machine learning 
mahoney 
combining symbolic connectionist learning methods refine factor rule bases 
ph dissertation university texas austin 
opitz shavlik 
heuristically expanding knowledge neural networks 
proceedings thirteenth international joint conference artificial intelligence 
opitz 
anytime approach connectionist theory refinement refining topologies knowledge neural networks 
ph dissertation department computer sciences university wisconsin madison 
appears uw technical report 
quinlan 
induction decision trees 
machine learning 
russell norvig 
artifical intelligence modern approach 
prentice hall 
towell shavlik 
symbolic learning improve knowledge neural networks 
proceedings tenth national conference artificial intelligence 
towell shavlik noordewier 
refinement approximate domain theories knowledge neural networks 
proceedings eighth national conference artificial intelligence 
vafaie dejong 
feature space transformation genetic algorithms 
ieee trans 
intelligent systems 
yang honavar 
feature subset selection genetic algorithm 
ieee trans 
intelligent systems 
