electronic notes computer science domain theory learning processes edalat department computing imperial college science technology medicine queen gate london sw bz uk applications domain theory stochastic learning automata neural nets 
show basic probabilistic algorithm called linear reward penalty scheme binary state stochastic learning automata modelled dynamics iterated function system probabilistic power domain compute expected value continuous function learning process 
consider general class called forgetful neural nets pattern learning takes place local iterative scheme domain theoretic framework distribution synaptic couplings networks action iterated function system probabilistic power domain 
obtain algorithms compute decay embedding strength stored patterns 
probabilistic power domain introduced developed studying probabilistic computation order provide semantics probabilistic programming languages 
general framework established application probabilistic power domain computation semantics 
shown probabilistic power domain upper space compact metric space provides domain theoretic framework measure integration theory metric space 
basic idea bounded measure metric space obtained upper bound increasing chain linear combinations point measures valuations upper space 
leads theory generalised riemann integration 
results provided new domain theoretic constructive tech supported epsrc project constructive approximation domain theory 
fl elsevier publishers author 
edalat niques new algorithms computation fields mathematics physics 
deals applications learning algorithms 
consider stochastic learning automata studied regarded precursors neural nets 
show basic probabilistic algorithm called linear reward penalty scheme binary state stochastic learning automata modelled dynamics iterated function system probabilistic power domain 
result obtain precise characterisation distribution states stage learning process 
enables compute expected value continuous function learning process 
consider general class called forgetful neural nets pattern learning takes place local iterative scheme 
nets proposed order overcome phenomenon catastrophic forgetting known hopfield model 
domain theoretic framework distribution synaptic couplings forgetful nets 
framework uses action iterated function system probabilistic power domain enables analyse local behaviour distribution synaptic couplings 
obtain algorithms compute decay embedding strength stored patterns quantity computing storage capacity network 
domain theoretic framework section establish domain theoretic framework measure integration theory need reviewing presenting new results :10.1.1.57.2414
compact metric space ux upper space non empty compact subsets ordered reverse inclusion bounded complete continuous dcpo directed complete partial order bottom scott topology ux basic open sets fc ux ag open set singleton map ux embeds set maximal elements ux 
similarly set normalised measures probability measures probability distributions embedded set normalised measures ux 
done normalised probabilistic power domain aim define 
valuation topological space map omega gamma edalat satisfies ii iii 
normalised valuation satisfies 
scott continuous valuation valuation omega gamma directed set wrt open sets sup point valuation valuation ffi omega gamma defined ffi finite linear combination ffi point valuations ffi constant coefficients scott continuous valuation call simple valuation 
normalised probabilistic power domain topological space consists set normalised scott continuous valuations ordered follows iff open sets 
partial order dcpo lub directed set open subset omega gamma sup continuous dcpo bottom continuous dcpo bottom ffi basis consisting simple valuations 
furthermore extends uniquely borel measure convenience denote unique extension 
simple valuations ffi ffi finite subsets splitting lemma iff exists nonnegative number edalat implies consider source mass sink mass number bc flow mass property regarded conservation total mass ux continuous dcpo bottom ux continuous dcpo bottom ffi borel subset induces singleton map borel subset ux 
valuation ux said supported unique extension borel measure ux satisfies 
ux supported iff open subsets supported support set points scott neighbourhood ux element ux supported maximal element ux proposition denote set valuations supported identify set normalised borel measures follows 

ffi gamma 
ffi theorem theorem maps defined induce isomorphism open subset equation implies furthermore follows theorem exists increasing chain simple valuations ffi gamma words probability measure upper bound chain simple valuations ux 
convenience write ffi gamma edalat converse problem suppose ux directed set simple valuations know lub determines borel measure words conditions proposition gives necessary sufficient condition 
denote diameter compact subset jcj 
proposition conditions equivalent 

ii ff fi exists ffi ff poset sequence ha finite subsets tower vem em ordering 
suppose increasing chain simple valuations ffi ux 
equation easy see hb tower ux call associated tower chain clearly lub ux chain associated tower element ux say intersection shrinking sequence non empty compact subsets non empty compact subset 
proposition konig lemma prove useful result 
proposition suppose ux increasing chain simple valuations 
lub chain associated tower singleton subset lub chain ifs probabilities fx finite number continuous maps compact metric space assigned probability weight 
ifs said hyperbolic maps contracting 
hyperbolic ifs probabilities gives rise unique invariant borel measure 
shown theorem unique invariant measure edalat fixed point transition operator ux ux 
defined gamma fixed point written ffi ffi ffi unique invariant measure hyperbolic ifs probabilities expressed lub canonical chain simple valuations ux 
invariant measure obtained natural way 
start valuation ffi contains information stage iteration obtain information invariant measure 
chain valuations gives rise tree called ifs tree transitional probabilities nth level represents nth valuation chain 
edges nodes nth level st labelled probabilities indicating ratios mass distributed node children cf 
splitting lemma equation 




ifs tree transitional probabilities 
say ifs hyperbolic 
proposition deduce 
proposition ifs probabilities unique invariant measure edalat lub branch associated ifs tree singleton set 
domain theoretic framework provides finite algorithm generating digitised screen invariant measure ifs probabilities satisfying condition proposition 
lub branch singleton subset branch digitised screen terminates leaf pixel size 
total weight assigned pixel sum weights leaves lie algorithm generalises holds hyperbolic ifss 
generalised riemann integral developed gives simple formula compute expected value continuous function respect invariant measure ifs probabilities 
assume ifs satisfies condition proposition real valued function continuous respect invariant measure fix point generalised riemann sum ffi ffi follows ffi 
result refines holds hyperbolic ifs continuous functions 
ifs hyperbolic satisfies lipschitz condition obtain finite algorithm calculate integral accuracy follows 
suppose exist jg gamma call lipschitz constant 
js ffi gamma gd dlog jxj log se contractivity hyperbolic ifs dae integer greater equal edalat stochastic learning automata stochastic learning automata learning systems precursors modern neural nets 
give brief review 
learning automaton operates unknown probabilistic environment connected feedback loop 
action automaton receives simple response environment indicates success failure 
response taken input automaton reevaluate action 
automaton learn actions highest probability success 
learning takes place automaton tends increase chance success 
formally stochastic learning automata consists 
finite set actions convenience denote rg ii state automaton step learning probability vector set states gamma dimensional simplex endowed induced euclidean metric compact metric space 
iii step learning automaton takes action probability current state 
iv environment receives action input responds sending simple success failure signal 
denote success failure respectively 
environment characterised set success probabilities probability success signal action probability failure signal action gamma probabilities known priori course 
simplicity assume time independent environment said stationary edalat automaton takes action environment responds signal pair called event 
set events theta 
automaton state probability event easily seen gamma gamma 
vi learning algorithm automaton set continuous maps ff automaton state event takes place automaton modifies state mapping 
new state 
means dynamical equation updating state automaton step learning event occurs en en selected probability en 
basic learning algorithm linear reward penalty scheme defined follows 
gamma gamma gamma gamma gamma 
action taken rewarded environment increases states decrease 
punishment response decreased states increased 
binary state edalat linear reward penalty automaton extensively studied karlin 
case fs mappings probabilities gamma gamma gamma gamma gamma gamma gamma gamma note similarly dynamics automaton reduced maps state independent transition probabilities 
precisely 
gamma 
gamma dynamics state governed hyperbolic ifs probabilities 
automaton probabilistic really interested probability distribution states stage learning 
domain theory provides proper conceptual technical framework model process learning 
apply domain theoretic setup described section step step generation invariant probability distribution hyperbolic ifs probabilities 
initially learning distribution states ffi edalat stage iteration probabilistic algorithm new learning achieved 
evolution distribution state captured mapping 
ffi gamma ffi gamma mapping 
ffi gamma ffi gamma distributions states refined step learning process technically expressed information relation ffi ffi 
theorem 
theorem nth step learning binary state linear automaton distribution state ffi ffi 
limiting distribution ffi nature limiting distribution depends parameter easily analysed framework 
theorem support cantor set support unit interval 
result confirms uses classical measure theory 
framework allows obtain expected value real valued continuous function states automaton 
function regarded function 
alternatively function 
equation say generalised riemann sum ffi equation obtain proposition lim ffi edalat lipschitz obtain finite algorithm obtain integral threshold accuracy equation 
forgetful neural nets neural networks models computational behaviour resemble brain 
section give brief review order describe class called forgetful nets 
study neural nets began mcculloch pitts introduced notion formal neuron state threshold element firing state state gamma say 
neuron receives input ij neuron state neuron ij synaptic coupling neuron neuron assembly neurons dynamics states neurons follows ij gamma ij threshold neuron mcculloch pitts showed networks elements implement logical function 
important step taken hebb suggested concept represented brain firing activity cell assembly learning takes place modification couplings ij neurons 
variety models associative memory pattern recognition various classification problems developed 
neural network models different types architecture 
feedforward networks neurons layer receive inputs preceding layer 
type multiply connected network lot feedback elements 
important breakthrough came hopfield multiply connected networks 
assumed couplings symmetric ij ji states neurons change time 
theory ising spin spin glasses statistical physics energy function defined gamma ij edalat dynamic equations increase deltas change state change energy easily seen deltae gamma deltas ij gamma states neurons flip energy decreases stable local minimum energy reached 
order capture concept learning memory hopfield final assumption stable local minima wells energy function associated stored key patterns 
stored pattern characterised specifying component state sigma neuron depicts schematic picture energy function stable local minima representing stored patterns 

energy landscape local stable minima representing stored patterns 
order stored key patterns stable local minima thresholds couplings ij suitably determined 
hopfield put ii hebbian prescription ij patterns stored 
easily shown near stable local minimum implies start configuration neuron states close stored pattern network follows hill dynamics ends stable pattern say pattern embedded memory net 
hopfield breakthrough important landmark theory architecture neural nets 
hopfield model suffered serious shortcoming catastrophic forgetting 
long important edalat parameter ff number stored patterns number neurons net remains critical value ff hopfield network works quite practically stored patterns remembered 
ff ff network goes state total confusion negligible amount patterns remembered 
technically model couplings ij arbitrarily large magnitude 
obviously unsatisfactory 
memory constructed go state total confusion overloaded old patterns forgotten order leave room new inputs 
task design networks forget overloaded 
attractive method construct forgetful neural nets model learning local iterative procedure sure couplings ij remain bounded 
proposed hopfield 
ij represents stored information including pattern stipulates ij oe ffl nj gamma ij oe suitable continuous function ij patterns stored 
different choices oe give different models 
original hopfield model fact recovered putting oe ffl 
hopfield chose oe jxj jxj jxj ffl constant 
nadal examined called case oe gamma kn gamma ffl 
hemmen considered called smooth case oe odd function oe gammax gammaoe strictly concave oe oe 
prototype general class oe tanh edalat ffl kn gamma sake exposition convenient prototype smooth model results extended general case 
note models including considered part scalar factor limit smooth case oe 
different models give rise different distributions couplings different storage capacities 
probabilistic power domain forgetful nets iterative equation study distribution couplings ij investigated 
consider equation fixed bond assume key patterns independent random uncorrelated 
means variables independent identically distributed random variables take values sigma equal probabilities 
similarly follows variables hm independent identically distributed random variables take values sigma equal probabilities 
putting xm nj ij convenience ffl ffl equation reduces stochastic equation xm oe 
choices give rise simple distributions xm fact easily seen leads random walk set integers leads finite state markov partition 
studied dynamics stochastic equation smooth cases 
oe sigma oe sigma oe sigma ffl 
cases oe oe gamma unique fixed point sigma dynamics confined gamma 
perron frobenius operator computer simulation inferred stochastic equation limiting distribution multi fractal structure 
domain theory general framework stochastic equation studying dynamics ifs probabilities gamma oe oe gamma domain gamma 
smooth models domain theoretic setting provides simple constructive analysis show existence uniqueness limiting distribution examine properties 
furthermore enables compute expected value physical quantities respect distribution 
edalat particular compute decay embedding strength stored patterns 
quantity computing storage capacity network 
note ifs probabilities gamma oe oe gamma captures stochastic equation specification initial value 
show smooth case ifs unique invariant measure deduce properties 
show invariant measures fact limiting distributions stochastic equation 
convenience put gamma 
case oe sigma sigma sigma sigma gamma ifs hyperbolic contractivity gamma kn gamma furthermore sets oe oe gamma empty intersection theorem support invariant measure cantor set union sets support interval 
smooth case oe sigma tanh sigma ffl depicted 

graphs oe sigma tanh sigma ffl ffl ffl 
edalat ffl ffl ffl root equation ffl tanh ffl ifs hyperbolic oe sigma satisfies oe oe gamma 
ffl ffl support invariant measure cantor set 
ffl ffl ifs hyperbolic oe sigma takes value oe sigma vanishes shown satisfy condition proposition unique invariant measure exists furthermore oe oe gamma implies support invariant measure interval 
note cases invariant measure constructively unique fixed point transition operator ui ui 
ffi oe gamma gamma ffi oe gamma words ffi ffi sigma ffi oe oe oe elton theorem prove 
theorem limiting distribution couplings smooth models invariant measures corresponding ifss probabilities unique fixed points transition operators domain ui 
interesting application result investigate local behaviour limiting distributions basic theoretical practical interest 
fact open subset sup ffi sup oe oe oe equations compute expected value physical quantity respect distribution couplings 
important quantity decay rate stored pattern new patterns learned compute storage capacity net defined follows 
patterns learned encoded couplings ij ij net embedding strength pattern ij edalat forgetful nets embedding strength pattern depends storage ancestry 
gamma gamman 
decay rate exponentially fast exp fln fl average lyapunov exponent stochastic mapping fl ln oe ln oe gamma model immediately finds fl ln 
smooth case fl gamma ln cosh ffl cosh gamma ffl accurately computed generalised riemann integral compared approximation obtained 
note function gamma ln cosh ffl cosh gamma ffl analytic function jg gamma cjx gamma yj max gamma xx jg ffl ffl know associated ifs hyperbolic equation gives formula compute integral threshold 
ffl ffl associated ifs hyperbolic algorithm compute integral accuracy analysed complexity 
describe algorithm show soundness 
ffi proposition ff fi exists number lists hi sigma length diameter set oe oe oe 
generalised riemann sum ffi sigma oe oe oe edalat integral corresponding lower upper sums gamma sigma sup oe oe gamma inf oe oe sup oe oe gamma inf oe oe sup oe oe gamma inf oe oe summation subsets diameters second summation rest subsets 
equation term lipschitz condition second term sum coefficients 
js ffi gamma algorithm finds nth level ifs tree equivalently ffi satisfies equation 
note oe gamma oe monotone diameter oe oe simply oe oe gamma oe oe gamma algorithm computes ffi 
inviting physics department leipzig university summer number informative discussions forgetful neural nets 
barnsley 
fractals 
academic press second edition 
van hemmen kuhn lange 
forgetful memories 
physica 
edalat edalat 
dynamical systems measures fractals domain theory extended 
burn gay ryan editors theory formal methods 
springer verlag 
full appear information computation 
edalat 
domain computation random field statistical physics 
hankin editor proceedings second imperial college department computing theory formal methods workshop 

edalat 
domain theory integration extended 
logic computer science 
ieee computer society press 
ninth annual ieee symposium july paris france 
full appear theoretical computer science 
edalat 
power domains iterated function systems 
technical report doc department computing imperial college 
submitted information computation 
edalat 
domain theory stochastic processes 
proceedings logic computer science tenth annual ieee symposium june san diego 
ieee computer society press 
appear 
elton 
ergodic theorem iterated maps 
journal ergodic theory dynamical systems 
van hemmen keller huber kuhn 
nonlinear neural networks 
journal statistical physics 
van hemmen keller kuhn 
forgetful memories 
letters 
prusinkiewicz saupe 
rendering methods iterated function systems 
proceedings ifip fractals 

hopfield 
proceedings national academy science usa volume page 

hopfield 
neurons graded response collective computational properties state neurons 
proceedings national academy science usa volume pages 

jones plotkin 
probabilistic powerdomain evaluations 
logic computer science pages 
ieee computer society press 
karlin 
random walks arising learning models 
pacific journal mathematics 

learning algorithms theory applications 
springer 
nadal toulouse 
solvable models working memories 
physique 
nadal toulouse dehaene 
networks formal neurons memory 
letters 
edalat norman 
markov processes learning models 
academic press 
plotkin 
powerdomain countable non determinism 
nielsen schmidt editors automata languages programming pages berlin 
eatcs springer verlag 
lecture notes computer science vol 


cpo measures non determinism 
theoretical computer science 

probability 
graduate texts mathematics 
springer 
