appear ieee signal processing letters 
infomax maximum likelihood blind source separation jean fran cois cardoso algorithms blind separation sources derived different principles 
letter shows proposed infomax principle equivalent maximum likelihood 

source separation consists recovering set unobservable signals sources set observed mixtures 
simplest form theta vector observations typically output sensors modeled mixing matrix invertible theta vector independent components probability density function factors assumptions realizations aim estimate matrix equivalently find separating matrix bx ba estimate source signals 
guiding principle source separation optimize function oe called contrast function function distribution bx see comon 
infomax principle apparently new contrast function derived bell sejnowski attracting lot interest 
letter exhibit contrast function associated established maximum likelihood ml principle 
device show regarding source separation infomax principle boils maximum likelihood ml 
infomax 
bell sejnowski application infomax principle source separation consists maximizing output entropy 
delta plain maximization inappropriate entropy bx diverges infinity arbitrarily author cnrs centre national de la recherche scientifique enst ecole nationale sup erieure des el 
paris cardoso sig enst fr letter respect lebesgue measure 
distributions assumed continuous respect 
large separating system infomax principle implemented maximizing respect entropy bx component wise non linear function 
infomax contrast function oe def bx delta shannon differential entropy 
scalar functions gn taken squashing functions mapping real line interval monotonously increasing 
differentiable cumulative distribution function real line dg ds gamma dy denote theta random vector distributed uniformly def distributed uniformly infomax contrast rewritten oe gammak bx gammak bx deltak delta denotes kullback leibler kl divergence 
equality combination second results 
shows identical minimization kl divergence distribution output vector bx distribution ii 
maximum likelihood 
recall maximum likelihood principle associated contrast function 
specialized source separation model 
consider sample independent realizations xt random variable distributed common density 

fp theta parametric model density likelihood sample drawn particular distribution product 
logarithm dividing number observations results normalized log likelihood lt def log log appear ieee signal processing letters 
sample average log converges probability law large numbers expectation lt def elt 
log dx setting 

yields gammak kp gamma note reached assuming exact model assumed 
theta 
result applies source separation follows 
distribution true distribution distribution vector parameter parametric model unknown mixing matrix parameter set theta set invertible theta matrices distributions vector source vector assumed distributed distribution 
setting eq 
gammak ska gamma contrast function associated likelihood appears oe def gammak ska additive constant depending parameter discarded 
iii 
discussion view function oe thetan 
oe def gammak appears play central role 
write oe oe ba oe oe gamma equality stems bx ba second equality applying property eq 
gamma contrasts associated infomax maximum likelihood coincide provided identified gamma interpretation straightforward case contrast optimization corresponds kullback matching hypothesized source distribution distribution cs ba gamma 
correct source model distributed contrast reduces oe gammak 
maximized sks lowest possible value kl divergence non negative 
happens wrong source model equivalently squashing functions source distributions 
sketch answer functions def gamma gamma log ds stationary points likelihood infomax contrast cancel gradient oe 
differentiating oe easily matrices cs verifies ef ffi ij ffi ij 
solution matrix diag stationary point oe source signals zero mean es 
easily seen stationarity condition satisfied due eff eff ef equality independence source signals second zero mean condition 
wrong source model generally diag satisfactory solution respect source separation source signals recovered scaling factors 
unfortunately conclude likelihood infomax contrast completely robust source distributions large mismatch may turn diag unstable stationary point observed 
infomax principle shown coincide maximum likelihood principle case source separation 
principles explicitly implicitly assume source distributions 
consist minimizing kullback divergence distribution output separating matrix hypothesized source distribution 
appears important match source distributions closely possible noted obvious ml standpoint 
wrong source model stationary point contrast obtained separated generally scaled signals stability guaranteed large mismatch 
principles hint general approach involving joint estimation mixing matrix characteristics source distributions 
steps direction taken elegant developed approach 
acknowledgment 
completed author visiting pr amari riken institute japan 
appendix information theoretic quantities 
ffl differential entropy variable denoted kullback leibler divergence denoted pkq 
definitions gamma log dy pkq log dy appear ieee signal processing letters 
integrals exist 
convenient abuse notation pkq resp 
density random vector resp 

ffl pkq equality iff agree 
ffl kl divergence invariant invertible transformation sample space kf gamma kf gamma ffl differential entropy distribution support kl divergence distribution uniform distribution clearly gamma pz log pz dz gammak bell sejnowski information maximization approach blind separation blind deconvolution neural computation vol 
pp 


cardoso equivariant approach source separation proc 
pp 

available ftp sig enst fr pub jfc papers ps gz 
cover thomas elements information theory wiley series telecommunications 
john wiley 
comon independent component analysis new concept signal processing vol 
pp 


pham jutten separation mixture independent sources maximum likelihood approach proc 
eusipco pp 

