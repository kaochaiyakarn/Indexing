journal name number year fl year kluwer academic publishers boston 
manufactured netherlands 
overcoming myopia inductive learning algorithms relieff igor kononenko marko igor kononenko fer uni lj si university ljubljana faculty electrical engineering computer science si ljubljana slovenia tel fax 
current inductive machine learning algorithms typically greedy search limited lookahead 
prevents detect significant conditional dependencies attributes describe training objects 
myopic impurity functions lookahead propose relieff extension relief developed kira rendell heuristic guidance inductive learning algorithms 
reimplemented assistant system top induction decision trees relieff estimator attributes selection step 
algorithm tested artificial real world problems results compared known machine learning algorithms 
excellent results artificial data sets real world problems show advantage approach inductive learning 
keywords learning examples estimating attributes impurity function relieff empirical evaluation 
inductive learning algorithms typically greedy search strategy overcome combinatorial explosion search hypotheses 
heuristic function estimates potential successors current state search space major role greedy search 
current inductive learning algorithms variants impurity functions information gain gain ratio gini index distance measure measure mdl 
measures assume attributes conditionally independent class domains strong conditional dependencies attributes greedy search poor chances revealing hypothesis 
kira rendell developed algorithm called relief powerful estimating quality attributes 
example parity problems various degrees significant number irrelevant random additional attributes relief able correctly estimate relevance attributes time proportional number attributes square number training instances reduced limiting number iterations relief 
original relief deal discrete continuous attributes deal incomplete data limited class problems 
developed extension relief called relieff improves original algorithm estimating probabilities reliably extends handle incomplete multi class data sets complexity remains 
relieff promising heuristic function may overcome myopia current inductive learning algorithms 
kira rendell relief preprocessor eliminate irrelevant attributes data description learning 
relieff general relatively efficient reliable guide search learning process 
reimplementation assistant learning algorithm top induction decision trees described named 
information gain assistant uses relieff heuristic function estimating attributes quality step tree generation 
experiments series artificial authors 
real world data sets described results obtained relieff selection criterion compared results approaches 
approaches compared ffl information gain selection criterion ffl lfc tries overcome myopia information gain limited lookahead ffl naive bayesian classifier assumes conditional independence attributes ffl nearest neighbors algorithm 
organized follows 
section original relief briefly described interpretation extended version relieff 
section reimplementation assistant called assistant 
section briefly describe algorithms experiments 
section describe experimental methodology 
section describes experiments compare results different algorithms 
show assistant performs better 
potential breakthroughs discussed basis excellent results artificial data sets 
integration compared algorithms proposed 

relieff 
relief key idea relief estimate attributes values distinguish instances near 
purpose instance relief 
set weights 


randomly select instance 
find nearest hit nearest 
attributes 
diff 
diff 
basic algorithm relief searches nearest neighbors class called nearest hit different class called nearest 
original algorithm relief randomly selects training instances userdefined parameter 
algorithm 
function diff attribute instance instance calculates difference values attribute instances 
discrete attributes difference values different values equal continuous attributes difference actual difference normalized interval 
normalization guarantees weights interval gamma normalization unnecessary step relative comparison attributes 
weights estimates quality attributes 
rationale formula updating weights attribute value instances class subtracting difference diff differentiate instances different classes adding difference diff 
function diff calculating distance instances find nearest neighbors 
total distance simply sum differences attributes 
fact original relief uses squared difference discrete attributes equivalent diff 
experiments significant difference results diff squared difference 
number training instances complexity algorithm theta theta attributes 

interpretation relief estimates derivation shows relief estimates strongly related impurity functions 
obvious relief estimate attribute approximation difference probabilities different value aj nearest instance different class title 
gammap different value aj nearest instance class eliminate requirement selected instance nearest formula different value class gammap different value class rewrite equal value class value obtain bayes rule gamma gamma gamma sampling replacement strict sense equalities hold theta equalities obtain theta gamma const theta theta theta gamma highly correlated gini index gain classes values attribute difference factor gini index gain uses equation shows strong relation relief weights gini index gain 
probability instances value attribute eq 
kind normalization factor multi valued attributes 
impurity functions tend overestimate multi valued attributes various normalization heuristics needed avoid tendency gain ratio distance measure binarization attributes 
equation shows relief exhibits implicit normalization effect 
deficiency gini index gain values tend decrease increasing number classes 
denominator constant factor equation attribute serves kind normalization relief estimates exhibit strange behavior gini index gain 
derivation eliminated nearest instance condition probabilities 
put back interpret relief estimates average local estimates smaller parts instance space 
enables relief take account context attributes conditional dependencies attributes class value detected context locality 
global point view dependencies hidden due effect averaging training instances exactly impurity functions myopic 
impurity functions correlation attribute class disregarding context attributes 
global point view disregarding local peculiarities 
example data set table illustrates difference myopic estimation functions relief 
attributes training instances 
class value determined xor function attributes third attribute randomly generated 
relief equation correctly estimates attributes important contribution attribute poor 
hand equation authors 
table example data set estimated quality attributes function class relief gini index gain information gain gain ratio distance equation original gini index gain information gain gain ratio distance measure estimate contribution highest attributes estimated completely irrelevant 
hong developed procedure similar relief estimating quality attributes directly emphasizes contextual information 
difference relief approach uses information nearest misses ignores nearest hits 
hong uses normalization penalize contribution nearest misses far away instance 

extensions relief original relief deal discrete continuous attributes 
deal incomplete data limited class problems 
equation crucial importance extensions relief 
turned extensions relief straightforward realized relief fact approximates probabilities 
extensions designed way probabilities reliably approximated 
developed extension relief called relieff improves original algorithm estimating probabilities reliably extends deal incomplete multi class data sets 
brief description extensions follows 
reliable probability approximation parameter algorithm relief described section represents number instances approximating probabilities eq 

larger implies reliable approximation 
obvious choice adopted relieff relatively small number training instances run outer loop relief available training instances 
selection nearest neighbors crucial importance relief 
purpose find nearest neighbors respect important attributes 
redundant noisy attributes may strongly affect selection nearest neighbors estimation probabilities noisy data unreliable 
increase reliability probability approximation relieff searches nearest hits misses near hit averages contribution nearest hits misses 
shown extension significantly improves reliability estimates attributes qualities 
overcome problem parameter tuning experiments set empirically gives satisfactory results 
problems significantly better results obtained tuning typical majority machine learning algorithms 
incomplete data enable relief deal incomplete data sets function diff attribute instance instance relieff extended missing values attributes calculating probability instances different values attribute ffl instance unknown value diff gamma value jclass ffl instances unknown value diff gamma values gamma jclass theta jclass delta conditional probabilities approximated relative frequencies training set 
title 
nearest neighbors correlation coefficient independent atts parity problems 
correlation relieff estimates intended quality attributes data sets conditionally independent strongly dependent attributes 
approach assumes conditional probabilities attribute values class applicable context attribute 
may cases naive including context far inefficient 
multi class problems kira rendell claim relief estimate attributes qualities data sets classes splitting problem series class problems 
solution unsatisfactory section discuss performance approach compare extension described 
practice relief able deal multiclass problems prior changes knowledge representation affect final outcomes 
finding near different class relieff searches near misses different class averages contribution updating estimate 
average weighted prior probability class gamma diff theta class gamma class theta diff theta idea algorithm estimate ability attributes separate pair classes regardless classes closest 
normalization prior probabilities classes necessary near misses different class tend exaggerate influence classes small number cases 
note time complexity relieff theta attributes number training instances 

relieff estimates attribute quality estimate contribution parameter nearest hits misses relieff estimates attribute quality kononenko compared intended information gain attributes estimates generated relieff calculating standard linear correlation coefficient 
correlation coefficient show intended quality estimated quality attributes related 
typical graph data sets conditionally independent attributes strongly dependent attributes parity problems various degrees shown 
conditionally independent attributes quality estimate monotonically increases number nearest neighbors 
dependent attributes quality increases maximum decreases number nearest neighbors exceeds number instances belong peak distribution space class 
note attributes evaluated myopic impurity functions gini index information gain quality estimates high conditionally independent attributes poor strongly dependent attributes 
corresponds estimates relieff large number nearest hits misses 
test effect normalization factor eq 
run relieff known medical data set primary tumor described authors 
section 
major difference estimates impurity functions estimates relieff primary tumor problem estimates significant attributes 
information gain gini index overestimate attribute values opinion physicians specialists 
relieff normalized versions impurity functions correctly estimate attribute important 

assistant assistant reimplementation assistant learning system top induction decision trees 
basic algorithm goes back cls concept learning system developed hunt reimplemented authors see overview 
describe main features assistant 
binarization attributes algorithm generates binary decision trees 
decision step binarized version attribute selected maximizes information gain attribute 
continuous attributes decision point selected maximizes attribute information gain 
discrete attributes heuristic greedy algorithm find locally best split attribute values subsets 
purpose binarization reduce replication problem strengthen statistical support generated rules 
decision tree pruning techniques pruning unreliable parts decision trees 
user defined thresholds provided minimal number training instances minimal attributes information gain maximal probability majority class current node 
method developed niblett bratko uses laplace law succession estimating expected classification error current node pruning pruning subtree 
incomplete data handling learning training instances missing value selected attribute weighted probabilities attribute value conditioned class label 
classification instances missing values weighted unconditional probabilities attribute values 
naive bayesian classifier internal node decision tree eventually third successor appears labeled attribute values training instances available 
null leaves naive bayesian formula calculate probability distribution leaf attributes appear path root leaf cja root leaf cja note calculation done line learning phase 
classification null leaves labeled calculated class probability distribution classification manner ordinary leaves 
main difference assistant reimplementation assistant relieff attribute selection 
addition appropriate relative frequency assistant uses estimate probabilities shown significantly increase performance machine learning algorithms 
prior probabilities laplace law succession possible outcomes number trials number trials outcome prior probabilities estimate conditional probabilities xjy theta parameter trades contributions relative frequency prior probability 
experiments parameter set setting usually default empirically gives satisfactory results title 
tuning problem domains better results may expected 
estimate naive bayesian formula laplace law succession proposed cestnik bratko relieff estimates probabilities 
eq 
probabilities root tree estimate prior probabilities lower internal node corresponding training instances theta diff val diff val root gamma theta diff val hit theta diff val hit root 
experimental environment 
algorithms comparison performed series experiments assistant compared performance algorithms assistant variant assistant relieff uses information gain selection criterion assistant 
differences assistant remain estimate probabilities 
algorithm enables evaluate contribution relieff 
parameters assistant assistant fixed experiments 
lfc limited lookahead lfc lookahead feature construction algorithm top induction decision trees detect significant conditional dependencies attributes constructive induction 
show interesting results data sets 
reimplemented algorithm tested performance 
results show drawbacks experimental comparison described rendell confirm advantage limited lookahead constructive induction 
lfc generates binary decision trees 
node algorithm constructs new binary attributes original attributes logical operators conjunction disjunction negation 
constructed binary attributes best attribute selected process recursively repeated subsets training instances corresponding values selected attribute 
constructive induction limited lookahead 
space possible useful constructs restricted due geometrical representation conditional entropy estimator attributes quality 
reduce search space algorithm limits breadth depth search 
lfc uses lookahead myopic greedy algorithm assistant 
comparison results may show performance greedy search combination relieff versus lookahead strategy 
results comparable assistant equipped lfc pruning probability estimation facilities described section 
tests performed default set parameters depth lookahead beam size domains better results may obtained parameter tuning 
higher values parameters may combinatorially increase search space lfc algorithm impractical 
naive bayesian classifier classifier uses naive bayesian formula calculate probability class values attributes assuming conditional independence attributes 
new instance classified class maximal calculated probability 
estimate probabilities parameter set experiments 
performance naive bayesian classifier serve estimate conditional independence attributes 
authors 
nn nearest neighbor algorithm 
new instance algorithm searches nearest training instances classifies instance frequent class instances 
nn algorithm distance measure relieff see section 
results obtained manhattan distance 
results euclidian distance practically 
best results respect parameter fair comparison parameter tuning allowed training testing sets 
selected naive bayesian classifier nn algorithm comparison known simple perform real world problems 
performance algorithms may show nature classification problems 

experimental methodology experiment data set performed times randomly selecting instances learning testing results averaged 
system subsets instances learning testing order provide experimental conditions 
verify significance differences test ff confidence level null hypothesis stating difference zero 
differences results having value statistic threshold considered significant 
exception methodology experiments finite element mesh design problem experimental methodology dictated previous published results described section 
classification accuracy measured average information score 
measure eliminates influence prior probabilities appropriately treats probabilistic answers classifier 
average information score defined inf testing instances inf testing instances information score classification th testing instance defined inf gamma log cl log cl cl cl inf gamma gamma gamma log gamma cl log gamma cl delta cl cl 
cl class th testing instance cl prior probability class cl cl probability returned classifier 
returned probability correct class greater prior probability information score positive obtained information correct 
interpreted prior information necessary correct classification minus posterior information necessary correct classification 
returned probability correct class lower prior probability information score negative obtained information wrong 
interpreted prior information necessary incorrect classification minus posterior information necessary incorrect classification 
main difference classification accuracy information score illustrated example 
prior distribution classes posterior distribution returned classifier 
correct class information score positive classification accuracy treats posterior distribution wrong answer 
correct class information score negative classification accuracy treats posterior distribution correct answer 
classification accuracy may special cases exhibit high variance information score stable 
special case data set irrelevant attributes exactly instances class instances class testing probabilistic classifier give approximate accuracy default classifier classifies instance majority class accuracy 
slight modification distribution training instances drastically change title 
accuracy approximately 
drastic modification distribution say cases class increase accuracy default classifier accuracy probabilistic classifier approximately theta theta 
classifiers information score scenarios remain approximately bits indicate classifiers unable extract useful information attributes 

experimental results section give results artificial real world data sets 
presentation experiments divided parts groups data sets artificial data sets controlled conditional dependency attributes benchmark artificial data sets medical data sets realworld data sets 
group give brief description data sets followed results 
results tables include averages runs standard errors 

artificial data sets generated data sets order compare performance various algorithms inf domain conditionally independent informative binary attributes classes random binary attributes 
learner detect attributes informative relatively easy task 
algorithms able solve problem 
inf domain obtained inf replacing informative attribute attributes values define value original attribute xor relation 
problem learner detect important attributes fact attributes pairwise strongly conditionally dependent 
fairly complex problem solved myopic heuristics 
data set show advantage lfc assistant 
tree domain instances generated decision tree internal nodes containing different binary attribute 
random binary attributes added description instances 
problem easy greedy decision tree learning algorithms approaches may difficulties due inappropriate knowledge representation target concept 
par parity problem significant binary attributes random binary attributes 
randomly selected instances labeled wrong class 
problem hard lot attributes equal score evaluated myopic evaluation function information gain 
par par significant attributes parity relation problem harder 
par par significant attributes parity relation problem hardest parity problems experiments 
basic characteristics artificial data sets listed table 
characteristics include percentage majority class interpreted default accuracy class entropy gives impression complexity classification problem 
results learning algorithms lfc assistant assistant naive bayesian classifier nn algorithm table classification accuracy table information score 
results expected show ffl classifiers perform relatively simple domain conditionally independent attributes inf 
ffl versions assistant perform problem reconstruction decision tree tree classifiers significantly worse 
ffl assistant lfc able successfully solve problems strong conditional dependencies attributes inf par assistant performs better especially case hardest problem par 
note lfc solve par depth lookahead increased authors 
table basic description artificial data sets domain class atts 
val att 
instances maj class entropy bit inf inf tree par par par table classification accuracy learning systems artificial data sets domain lfc assistant assistant naive bayes nn inf sigma sigma sigma sigma sigma inf sigma sigma sigma sigma sigma tree sigma sigma sigma sigma sigma par sigma sigma sigma sigma sigma par sigma sigma 
sigma sigma sigma par sigma sigma sigma sigma sigma time complexity lookahead increases exponentially depth 
hand assistant solves parity problems equally quickly 
ffl information score naive bayesian classifier problems strong conditional dependencies attributes poor indicates classifier failed find regularity data sets 

benchmark artificial data sets artificial data sets previous subsection benchmark artificial data sets authors note results authors directly compared results experimental conditions training testing splits bool boolean function defined attributes class noise optimal recognition rate 
target function phi data set smyth report sigma classification accuracy naive bayes sigma backpropagation sigma rulebased classifier 
led led digits problem noise attribute values 
optimal recognition rate estimated 
smyth report sigma classification accuracy naive bayes sigma backpropagation sigma rule classifier 
data set obtained irvine database 
krk problem legality king chess endgame positions 
attributes describe relevant relations pieces rank adjacent file 
originally data included sets examples learning testing test inductive logic programming algorithms 
reported classification accuracy sigma 
set examples instances training 
krk krk available attributes coordinates pieces 
data set mladenic 
reported results accuracy system assistant 
basic description data sets provided table results tables 
interesting led domain naive bayesian classifier nn algorithm reach estimated upper bound classification accuracy 
suggests attributes considered optimal classification domain 
problem attributes conditionally independent class performance naive bayesian classifier surprising 
domains performance naive bayesian classifier poor due strong title 
table average information score learning systems artificial data sets domain lfc assistant assistant naive bayes nn inf sigma sigma sigma sigma sigma inf sigma sigma sigma sigma sigma tree sigma sigma sigma sigma sigma par sigma sigma sigma sigma sigma par sigma sigma sigma sigma sigma par sigma sigma sigma sigma sigma table basic description benchmark artificial data sets domain class atts 
val att 
instances maj class entropy bit bool led krk krk table classification accuracy learning systems artificial data sets domain lfc assistant assistant naive bayes nn bool sigma sigma sigma sigma sigma led sigma sigma sigma sigma sigma krk sigma sigma sigma sigma sigma krk sigma sigma sigma sigma sigma table average information score learning systems artificial data sets domain lfc assistant assistant naive bayes nn bool sigma sigma sigma sigma sigma led sigma sigma sigma sigma sigma krk sigma sigma sigma sigma sigma krk sigma sigma sigma sigma sigma conditional dependencies attributes 
information score see table shows naive bayesian classifier provides average information bool krk domains 
performance different variants assistant krk domain performance assistant poor note default accuracy krk 
performance assistant nn algorithm significantly better confidence level 
information score shows assistant nn successful problem 
expected constructive induction possible reveal regularities chess positions described coordinates pieces 
lfc able construct important attributes domain enables achieve significantly better results algorithms 

medical data sets compared performance algorithms medical data sets ffl data sets obtained university medical center ljubljana slovenia problem locating primary tumour patients prim problem predicting recurrence breast cancer years removal tumour problem determining type cancer lymphography diagnosis rheumatology 
ffl survival patients suffering hepatitis 
data provided gail gong carnegie mellon university 
ffl data sets obtained statlog database diagnosis diabetes diab diagnosis heart diseases heart 
diab data set rendell report classification accuracy lfc algorithm 
report poor performance authors 
table basic description medical data sets domain class atts 
val att 
instances maj class entropy bit prim diab heart table classification accuracy learning systems medical data sets domain lfc assistant assistant naive bayes nn prim sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma diab sigma sigma sigma sigma sigma heart sigma sigma sigma sigma sigma table average information score learning systems medical data sets domain lfc assistant assistant naive bayes nn prim sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma diab sigma sigma sigma sigma sigma heart sigma sigma sigma sigma sigma algorithms constructive induction 
results see results statlog project show poor results algorithms domain due lack constructive induction 
experiments diab dataset classifiers perform equally exception naive bayesian classifier significantly better 
basic characteristics medical data sets table 
results experiments data sets provided tables 
medical data sets attributes typically conditionally independent class surprising naive bayesian classifier shows clear advantage data sets 
interesting performance nn algorithm domains worse performance naive bayesian classifier 
information score table data set indicates learning algorithm able solve problem 
suggests attributes relevant 
versions assistant similar performance domain assistant significantly better performance confidence level 
detailed analysis showed problem relieff discovered significant conditional interdependency attributes class 
attributes score poorly considered independently 
assistant able discover regularity data 
hand redundant attributes available contain similar information attributes 
reason naive bayesian classifier performs better 
tried provide naive bayesian classifier additional attribute joining conditionally dependent attributes 
performance remained 
lfc achieved significantly better results inductive algorithms domain constructive induction title 
useful 
lfc performed significantly worse domain domains inductive algorithms perform equally 

non medical real world data sets compared performance algorithms non medical real world data sets iris vote obtained irvine database sat obtained statlog database famous soybean data set michalski 
iris known fisher problem determining type iris flower 
mesh mesh problem determining number elements edges object finite element mesh design problem 
objects experts constructed appropriate meshes 
experiments object testing learning results averaged 
results reported dzeroski various ilp systems classification accuracy foil golem result reported 
description mesh problem appropriate ilp systems 
attribute learners relations arity attributes describe problem 
note domain training testing splits algorithms 
testing methodology special case leave results tables problem standard deviations 
quinlan reports results ilp systems achieved domain testing positive negative instances 
results misleading 
positive instance negative instances average 
copies instance classification instance correct copies gives classification accuracy classifier classifies wrong class 
mesh contains basic attributes original database ignores relational description objects 
mesh domain attribute learners information ilp learners 
mesh contains original attributes attributes derived relational background knowledge 
problem attribute learners advantage provided additional attributes 
provided description objects ilp learners informative 
principle attributes number additional attributes derived extremely ilp learners relational description background knowledge 
fairly complex task 
attribute learners mesh data set better chances ilp learners reveal hypothesis 
sat database consists multi class spectral values pixels theta neighborhoods satellite image classification central pixel neighborhood 
results statlog project classification accuracy nn algorithm backpropagation cn naive bayesian classifier relative frequencies estimate probabilities 
vote voting records session united states congress 
smyth report classification accuracy naive bayesian classifier backpropagation rule classifier 
basic characteristics non medical real world data sets table 
tables give results 
iris data sets classifiers perform equally 
results naive bayesian classifier indicate attributes conditionally relatively independent data sets agreement previously published results 
sat data set nn significantly outperforms algorithms agreement results statlog project 
naive bayesian classifier estimate probabilities reaches classification accuracy inductive learning algorithms 
results naive bayesian classifier authors 
table basic description non medical real world data sets domain class atts 
val att 
instances maj class entropy bit iris mesh mesh sat vote table classification accuracy learning systems non medical real world data sets domain lfc assistant assistant naive bayes nn sigma sigma sigma sigma sigma iris sigma sigma sigma sigma sigma mesh mesh sat sigma sigma sigma sigma sigma vote sigma sigma sigma sigma sigma table average information score learning systems non medical real world data sets domain lfc assistant assistant naive bayes nn sigma sigma sigma sigma sigma iris sigma sigma sigma sigma sigma mesh bit bit bit bit bit mesh bit bit bit bit bit sat sigma sigma sigma sigma sigma vote sigma sigma sigma sigma sigma statlog project worse 
cestnik shown estimate significantly increases performance naive bayesian classifier confirmed experiments 
versions assistant perform data sets sat data set assistant lfc achieve significantly better result confidence level 
result confirms relieff estimates quality attributes better information gain 
vote data set naive bayesian classifier worst versions assistant comparable rule classifier smyth 
interesting results appear mesh domains 
attribute learners mesh information ilp systems outperform results ilp systems reported dzeroski 
additional attributes mesh results inductive learners significantly improved 
inductive learning systems significantly outperform naive bayesian classifier nn algorithm 
detailed analysis showed excellent result versions assistant due naive bayesian formula calculate class probability distribution null leaves see section 
problem happens testing instances fall null leaf training instances values significant attributes testing instances 
naive bayesian classifier efficiently solves problem 
lfc generates null leaves constructed attributes strictly binary values true false 
classification objects different value original attribute training instances proceeds branch labeled false 
effect strategy testing instance corresponding leaf contains training instances similar values attributes appear path root leaf 
strategy works mesh problems 

discussion note null leaves versions assistant influence performance arti title 
data sets missing values data 
mesh problem performance lfc generate null leaves 
null leaves crucial difference assistant lfc 
equation shows interesting relation relief estimates impurity function 
relief efficiently estimate continuous discrete attributes 
implicit normalization eq 
enables relief appropriately deal multivalued attributes 
assistant eq 
information gain myopic 
example par problems eq 
estimate attributes equally non important 
reason success assistant nearest instances heuristic influences estimation probabilities 
heuristic enables relief detect strong conditional dependencies attributes overlooked estimates probabilities done randomly selected instances nearest instances 
relieff efficient heuristic estimator attribute quality able deal data sets conditionally dependent independent attributes 
extensions relieff enable deal noisy incomplete multi class data sets 
increasing number nearest hits misses correlation relieff estimates impurity functions increases greater number instances peak instance space 
study reported showed relieff acceptable bias respect measures estimating attributes different number values 
myopia current inductive learning systems partially overcome replacing existing heuristic functions relieff 
assistant variant top induction decision trees algorithms uses relieff estimating quality attributes significantly outperforms classifiers domains strong conditional dependencies attributes 
myopia inductive learners may cause overlook significant relations 
easily demonstrated artificial data sets shown real world problems sat 
data sets relieff detected significant conditional interdependencies attributes resulted significantly better result assistant result assistant 
feature relief addressed attribute replicated data set replications get estimate 
increasing number replications quality estimates replicated attribute affects distances instances 
constructive induction lfc uses limited lookahead detect significant conditional dependencies attributes 
lfc shows similar advantage algorithms 
artificial problem krk real world problem lfc performs significantly better due constructive induction 
cases constructive induction may spoil results case data set 
lfc performs problems suggests limited lookahead search strategy real world problems 
lookahead reasonable limit time complexity increases lookahead depth 
relieff may overcome myopia useless assistant change representation required 
cases constructive induction applied 
example krk problem assistant achieves result improved constructive induction 
idea constructive induction may relieff combination lookahead 
naive bayesian classifier obvious advantage domains conditionally relatively independent attributes medical diagnostic problems 
domains naive bayesian classifier able reliably estimate conditional probabilities able attributes available information 
interesting appropriately combine power relieff naive bayesian classifier 
current ilp systems able attributes appropriately 
demonstrated mesh domain attribute learn authors 
ers outperformed existing ilp systems 
enable ilp systems deal attribute value representation combination semi naive bayesian classifier useful 
hand current ilp systems greedy search techniques heuristics guide search myopic 
kononenko implemented adapted version relieff foil ilp system called ilp experiments show similar advantages system ilp systems assistant assistant 

relieff efficient heuristic estimator attribute quality able deal data sets conditionally dependent independent attributes noisy incomplete multi class data sets 
myopia current inductive learning systems partially overcome replacing existing heuristic functions relieff 
acceptable increase computational complexity may certain domains payoff eventual discovery strong conditional dependencies attributes detected myopic impurity measure guide greedy search 
experimental results indicate majority real world problems myopia marginal effect 
may wonder myopia really worth attention 
faced new data set unreasonable try myopic algorithm know advance data set strong conditional dependencies attributes 
serious application machine learning new data try discover regularities data possible 
non myopic approaches described indispensable tools analysing data 
estimate equation proposed cestnik 
prim data sets milan gail gong padhraic smyth bool led saso dzeroski krk mesh bob diab heart sat data sets statlog database strathclyde university patrick murphy david aha data sets irvine database 
grateful colleagues saso dzeroski kovacic anonymous reviewers comments earlier drafts significantly improved 
supported ministry science technology 

breiman friedman olshen stone 
classification regression trees 
wadsworth international group 

cestnik 
estimating probabilities crucial task machine learning 
proc 
european conference artificial intelligence 
stockholm august pp 

cestnik bratko 
estimating probabilities tree pruning 
proc 
european working session learning 
porto march kodratoff ed springer verlag 
pp 

cestnik kononenko bratko 
assistant knowledge elicitation tool sophisticated users 
bratko lavrac eds 
progress machine learning 
england sigma press 

chase brown 
general statistics 
john wiley sons 

muggleton 
application inductive logic programming finite element mesh design 
muggleton 
ed 
inductive logic programming 
academic press 

dzeroski 
handling noise inductive logic programming 
sc 
thesis university ljubljana faculty electrical engineering computer science ljubljana slovenia 

hong 
contextual information feature ranking discretization 
technical report ibm rc 
appear ieee trans 
knowledge data engineering 

hunt martin stone 
experiments induction new york academic press 

kira rendell 
practical approach feature selection 
proc 
intern 
conf 
machine learning aberdeen july sleeman edwards eds morgan kaufmann pp 

kira rendell 
feature selection problem traditional methods new algorithm 
proc 
aaai san jose ca july 

kononenko 
inductive bayesian learning medical diagnosis 
applied artificial intelligence 

kononenko 
estimating attributes analysis extensions relief 
proc 
european conf 
machine learning catania april 
de raedt bergadano eds springer verlag pp 

title 

kononenko 
biases estimating multivalued attributes 
proc 
ijcai montreal august 
mellish ed morgan kaufmann pp 


kononenko bratko 
information evaluation criterion classifier performance 
machine learning 

mantaras 
id revisited distance criterion attribute selection 
proc 
int 
symp 
methodologies intelligent systems charlotte north carolina oct 

michalski 
learning told learning examples experimental comparison methods knowledge acquisition context developing expert system soybean disease diagnosis 
international journal policy analysis information systems 

michie spiegelhalter taylor eds 
machine learning neural statistical classification 
ellis horwood limited 

mladenic 
combinatorial optimization inductive concept learning 
proc 
th intern 
conf 
machine learning 
amherst june morgan kaufmann pp 


muggleton ed 
inductive logic programming 
academic press 

murphy aha 
uci repository machine learning databases machine readable data repository 
irvine ca university california department computer science 

niblett bratko 
learning decision rules noisy domains 
proc 
expert systems brighton uk december 

kovacic kononenko 
stochastic approach inductive logic programming 
proc 
conf 
electrical engineering computer science 
slovenia pp 
sept 

kononenko 
linear space induction order logic relieff della kruse eds 
mathematical statistical methods artificial intelligence 
lecture notes springer verlag 

quinlan 
induction decision trees 
machine learning 

quinlan 
minimum description length principle categorical theories 
proc 
th int 
conf 
machine learning university new brunswick july cohen hirsh 
eds morgan kaufmann pp 


rendell 
lookahead feature construction learning hard concepts 
proc 
th intern 
conf 
machine learning 
amherst june morgan kaufmann pp 


rendell shaw 
learning complex real world feature construction 
technical report uiuc bi ai 
beckman institute university illinois 


constructive induction decision trees 
sc 
thesis slovene university ljubljana faculty electrical engineering computer science ljubljana slovenia 

smyth goodman 
rule induction information theory 

piatetsky shapiro frawley eds 
knowledge discovery databases mit press 

smyth goodman higgins 
hybrid rule bayesian classifier 
proc european conf 
artificial intelligence stockholm august pp 

