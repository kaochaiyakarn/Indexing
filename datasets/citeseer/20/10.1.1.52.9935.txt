performance evaluation memory consistency models shared memory multiprocessors kourosh gharachorloo anoop gupta john hennessy computer systems laboratory stanford university ca memory consistency model supported multiprocessor architecture determines amount buffering pipelining may hide reduce latency memory accesses 
different consistency models proposed 
range sequential consistency allowing limited buffering release consistency allowing extensive buffering pipelining 
processor consistency weak consistency models fall 
advantage strict models increased performance potential 
disadvantage increased hardware complexity complex programming model 
informed decision tradeoff requires performance data various models 
addresses issue performance benefits consistency models 
results simulation studies done applications 
results show environment processor reads blocking writes buffered significant performance increase achieved allowing reads bypass previous writes 
pipelining writes determines rate writes retired write buffer secondary importance 
result show sequential consistency model performs poorly relative models processor consistency model provides benefits weak release consistency models 
memory accesses satisfied processor environment experience long latencies large scale shared memory multiprocessors 
powerful techniques help reduce hide latency buffering pipelining memory accesses 
unfortunately distributed memory caches general interconnection networks large scale multiprocessors cause multiple requests issued processor execute order 
depending memory consistency model supported various restrictions placed amount buffering pipelining allowed 
choice memory consistency model directly affects performance machine 
memory consistency models proposed literature 
strictest model sequential consistency sc requires execution parallel program appear interleaving execution parallel processes sequential machine 
conceptually intuitive elegant model imposes severe restrictions outstanding accesses process may restricting buffering pipelining allowed 
strict models release consistency rc allows significant overlap memory accesses synchronization accesses identified classified acquires releases 
models discussed literature processor consistency pc weak consistency wc 
fall sequential release consistency models terms strictness 
strict models provide higher potential performance require complicated hardware result complex programming model 
data indicating performance achieved models essential allowing system designer appropriate decision model support 
far detailed performance data available 
presents simulation results comparing performance consistency models engineering applications 
applications particle simulator aeronautics mp lu decomposition program lu digital logic simulation program pthor 
simulated architecture stanford dash multiprocessor 
results show architecture blocking reads buffered writes typical current commercial microprocessors sequential consistency model significantly worse models 
somewhat expected surprising result processor consistency model release consistency model benchmarks better weak consistency model 
results indicate gain allowing reads bypass pending writes allowed pc wc rc important allowing writes pipelined allowed wc rc 
sections description class multiprocessor architectures simulation environment benchmark applications study 
section presents background information implementation oriented view consistency models 
simulation results section 
section explores effects prefetching models 
general discussion results related sections 
conclude section 
multiprocessor architecture simulator enable meaningful performance comparisons models necessary focus specific class multiprocessor architectures 
reason tradeoffs may vary depending architecture chosen 
example tradeoffs small bus multiprocessor broadcast possible latencies cycles quite different tradeoffs large scale multiprocessor broadcast possible latencies may cycles 
study chosen architecture resembles dash shared memory multiprocessor large scale cache coherent machine currently built stanford 
actual parameters dash prototype possible removed limitations imposed dash prototype due design time constraints 
shows high level organization simulated architecture consists processing nodes connected low latency scalable interconnection network 
physical memory distributed nodes cache coherence maintained distributed directory protocol 
memory block directory keeps track remote nodes caching block point point messages sent invalidate remote copies 
messages inform originating processing node invalidation completed 
dash prototype processors node simulate architecture processor node 
allows isolate effect consistency models clearly 
due distribution memory system general interconnection network architecture accesses issued order may complete different order 
example producer process writes variable sets flag consumer process may see flag set holding stale value variable cache 
preserve desired order setting flag delayed write variable performed 
simulated architecture read considered performed return value bound modified write operations 
similarly write considered performed exclusive ownership acquired copies invalidated 
simplicity assume writes atomic 
architecture provides mechanism keep track multiple outstanding accesses processor allows processor stalled pending accesses performed details see 
notion performed having completed interchangeably rest 
shows organization processor environment 
cpu system contains kbyte data cache 
write cache enables processors single cycle write operations 
level data cache interfaces kbyte second level write back cache 
interface consists read buffer write buffer 
second level caches direct mapped support byte lines 
architecture processor write buffer may stalled due constraints imposed consistency model purely due implementation constraints 
example consistency model may allow multiple outstanding accesses implementation cache may allow 
separate consistency model issues implementation directory memory controller processor cache memory interconnection network directory memory controller processor cache memory architecture processor cache primary secondary cache write buffer stores loads processor environment simulated architecture processor environment 
decisions examine performance models implementations varying aggressiveness 
consider versions implementation lfc aggressive implementation lockup free caches reads bypass write buffer word deep write buffer ii lfc caches longer iii basic reads allowed bypass write buffer 
lockup free caches allow multiple accesses service simultaneously opposed servicing access time 
stated performance comparisons lfc version cpu minimizes shortcomings due implementation 
studies event driven simulator simulate major components dash architecture behavioral level 
simulations processor configuration 
simulator tightly coupled tango generator assure correct interleaving accesses 
example process doing read operation blocked read completes latency read determined architecture simulator 
main memory distributed nodes allocated round robin scheme applications 
latency memory access simulated architecture depends memory hierarchy access serviced 
table shows latency servicing access different levels hierarchy absence contention simulation results include effect contention 
latency shown writes time acquiring exclusive ownership line necessarily include time receiving messages invalidations 
naming convention describing memory hierarchy 
local node node contains processor originating request home node node contains main memory directory physical memory address 
remote node node 
synchronization primitives modeled dash 
queue lock primitive supporting locks 
general locks cached processor spinning locked value 
lock released waiting processors chosen random granted table latency various memory system operations processor clocks 
numbers mhz processors 
read operations hit primary cache fill secondary cache fill local node fill remote node fill dirty remote remote home write operations owned secondary cache owned local node owned remote node owned dirty remote remote home lock update message 
acquiring free lock takes processor cycles local remote locks respectively 
barriers implemented fetch increments gather stage update writes releasing 
total latency perform barrier processors reach barrier time processor cycles 
benchmark applications programs chosen evaluation represent applications engineering computing environment 
applications written synchronization primitives provided argonne national laboratory macro package 
applications study mp lu pthor 
mp dimensional particle simulator 
study pressure temperature profiles created object flies high speed upper atmosphere 
computation mp consists evaluating positions velocities molecules sequence time steps 
time step molecules picked time moved velocity vectors 
particles come close may undergo collision probabilistic model 
collisions object boundaries modeled 
simulator suited parallelization molecule treated independently time step 
main synchronization consists barriers time step 
experiments ran mp particles space array simulated time steps 
lu performs lu decomposition dense matrices 
primary data structure lu matrix decomposed 
working left right column modify columns right 
columns left column modified column modify remaining columns 
columns statically assigned processors interleaved fashion 
processor waits column produced column modify columns processor owns 
processor completes column releases processors waiting column 
experiments performed lu decomposition matrix 
pthor parallel distributed time logic simulator chandy misra simulation algorithm 
primary data structures associated simulator logic elements gates flip flops nets wires linking table general statistics benchmarks 
program busy cycles shared processor ideal mp lu pthor elements task queues contain activated elements 
processor executes loop 
removes activated element task queues determines changes element outputs 
looks net data structure determine elements affected output change schedules newly activated elements task queues 
experiments simulated fifteen clock cycles multiplier circuit consisting equivalent input gates 
table presents general information applications 
minimize architectural dependence measurements latency cycle assumed accesses refer ideal architecture 
busy cycles specify amount useful cycles program shared indicates number read write synchronization accesses issued program 
high processor utilization ideal architecture implies sufficient parallelism algorithms processors 
table presents detailed statistics access behavior applications 
rate read write misses rate synchronization important factors determining relative performance models 
table shows mp pthor relatively high rates rates substantially lower lu 
case lu caches large hold matrix completely application lot communication 
application number read misses higher number write misses 
pthor shown highest rate synchronization compared applications 
consistency models implementation section provides general overview consistency models 
describe implementation restrictions imposed model 
main goal section develop intuition situations strict models expected perform better 
contents section form basis arguments explain simulation data sections 
general overview consistency model imposes restrictions order shared memory accesses initiated process 
strictest model originally proposed lamport sequential consistency sc 
sequential consistency requires execution parallel program appear interleaving execution parallel processes sequential machine 
processor consistency pc proposed goodman relax restrictions imposed sequential consistency 
processor consistency requires writes issued processor may table number shared read write synchronization accesses processor configuration 
numbers parentheses rates cycles ideal architecture 
program reads writes read misses write misses locks unlocks barriers ratio mp lu pthor observed order issued 
order writes processors occur observed third processor need identical 
constraints satisfy processor consistency specified formally 
relaxed consistency model derived relating memory request ordering synchronization points program 
weak consistency model wc proposed dubois idea ensures memory consistent synchronization points 
example consider process updating data structure critical section 
sc access critical section delayed previous access completes 
delays unnecessary programmer sure process rely data structure consistent critical section exited 
weak consistency exploits allowing accesses critical section pipelined 
correctness achieved guaranteeing previous accesses performed critical section 
release consistency rc extension weak consistency exploits information synchronization classifying acquire release accesses 
acquire synchronization access lock operation process spinning flag set performed gain access set shared locations 
release synchronization access unlock operation process setting flag permission 
acquire accomplished reading shared location appropriate value read 
acquire associated read synchronization access see discussion read modify write accesses 
similarly release associated write synchronization access 
contrast wc rc require accesses release delayed release complete purpose release signal previous accesses complete say ordering accesses 
similarly rc require acquire delayed previous accesses 
shows restrictions imposed consistency models studied 
ordering restrictions terms access allowed perform 
shown sequential consistency guaranteed requiring shared accesses perform program order 
processor consistency allows flexibility sc allowing read operations bypass previous write operations 
weak consistency release consistency differ sc pc exploit information synchronization accesses 
wc rc allow accesses synchronization opera weak consistency release consistency models shown models respectively terminology 
load store load store load store load store load store load store weak consistency wc acquire acquire release release load store load store load store load store load store load store release consistency rc acquire release acquire release sequential consistency sc load load store load load store store store processor consistency pc load load store load load store store store perform performed load store load store loads stores perform order long local data control dependences observed ordering restrictions memory accesses 
tions pipelined shown 
numbers blocks denote order accesses occur program order 
shows rc provides flexibility exploiting information type synchronization 
implementation oriented view table summarizes implementation restrictions imposed models processors blocking reads sufficient necessary restrictions satisfying model 
base model added consistency models discussed earlier 
constrained model baseline performance comparisons 
incorporates buffering pipelining waits read write complete proceeding 
implementation restrictions considerably simplified processors block loads typical commercial microprocessors 
consistency model constraints access block preceding load acquire automatically satisfied 
qualitatively compare performance consistency models consider sources idle time processor 
architecture level idle time consists read accesses table implementation consistency models 
operation base sc pc wc rc 
read processor issues read stalls read perform 
note pending writes possible 
see point 
processor stalls pending writes perform aggressive implementations gain ownership 
processor issues read stalls read perform 
processor issues read stalls read perform 
note reads allowed bypass pending writes 
processor issues read stalls read perform 
notes reads allowed bypass pending writes 
ii interaction pending releases see point 
processor issues read stalls read perform 
note reads allowed bypass pending writes releases 

write processor issues write stalls write perform 
note need write buffer 
processor sends write write buffer stalls write buffer full 
note write buffer retires write write performed aggressive implementations ownership gained 
processor sends write write buffer stalls write buffer full 
notes write buffer require ownership gained retiring write 
ii interaction acquires releases see points 
acquire treated read treated read treated read processor stalls pending writes releases perform 
processor issues acquire stalls acquire perform 
processor issues acquire stalls acquire perform 
note processor need stall pending writes releases 

release treated write treated write treated write processor sends release write buffer stalls write buffer full 
notes write buffer retire release previous writes performed 
ii write buffer stalls release perform 
iii processor stalls read release release perform 
processor sends release write buffer stalls write buffer full 
note write buffer retire release previous writes releases performed 
stalling data ii processor stalling previous writes releases complete iii write release accesses stalling write buffer full iv acquire accesses spinning corresponding release observed 
performance particular consistency model depends effective model reducing idle time 
compare base sc models shown table 
models guarantee sequential consistency sc aggressive formulation 
main difference writes treated 
base model processor stalls immediately write write completes 
sc writes put write buffer processor stalled read operation 
result part write latency overlapped computation read 
cases read access occurs soon write latency completing write seen processor 
sc pc models major difference pc allows reads performed waiting pending writes write buffer 
significantly decreases total delay experienced processor increasing performance 
course side effect higher probability write buffer may get full stall processor pc 
shown simulation results section rarely happens practice 
reason intuitive write misses interleaved read misses clustered number read misses usually dominates 
processor blocking reads stall duration read misses provides time write misses retired write buffer preventing getting full 
comparing pc wc models wc model exploits knowledge synchronization accesses 
pc order write accesses preserved 
wc reads writes synchronization accesses performed order provided uniprocessor control data dependences maintained 
result reads bypass writes case pc 
contrast pc writes pipelined 
pipelining allows writes retired write buffer requiring ownership obtained 
writes retired faster rate possible pc 
help performance ways 
chances write buffer filling stalling processor smaller compared pc 
second release operation unlock operation writes write buffer remote processor trying acquire lock variable observe release sooner spinning shorter amount time 
release operation wc wait invalidations completed previous writes allow invalidations multiple writes overlapped 
reasons indicate wc provide performance advantages pc significant clustering writes especially clustering release accesses 
disadvantage wc compared pc 
pc processor stalled read acquire pending writes release performed 
wc processor stalls acquire previous writes releases complete read access release release complete 
seriously degrade performance application high rate synchronization 
show results section programs uses fine grain synchronization pthor wc worse pc 
comparing wc rc rc model removes shortcoming wc described 
similar pc rc stalls processor read acquire previous writes releases complete 
consequently rc offer best performance models 
case wc performance gains pc occur significant clustering writes 
performance gains relaxing consistency model accompanied complex implementation 
implementation difference models arises different number outstanding requests allowed model 
implementation allow multiple outstanding requests cache needs lockup free 
addition needs mechanism keeping track outstanding request know request complete 
base sc require lockup free cache outstanding access allowed 
pc read write access outstanding 
wc rc allow multiple outstanding accesses blocking reads limit number outstanding read accesses 
lockup free cache pc simpler wc rc accesses read write outstanding time 
furthermore wc rc require additional counters keep track outstanding accesses complete see details 
secondary effects discussed 
example strict models allow accesses issued faster rate turn may result contention network memory system increasing latency accesses 
may offset gains strict models 
discuss impact side effects section 
simulation results section comparative analysis performance achieved various consistency models 
section presents simulations lfc architecture described section 
lfc aggressive architecture results primarily affected constraints imposed consistency model limitations implementation 
section presents results relative performance models aggressive implementations 
results lfc architecture shows relative performance models application 
purposes define performance processor utilization achieved execution 
consistency model performance gain base mp pthor lu base sc pc wc rc relative performance models lfc 
reason processor utilization merit provides reasonable results program control path deterministic depends relative timing synchronization accesses 
processor utilization model normalized performance base model program 
results show wide range performance gains due strict models 
moving base sc gains minimal 
largest gains performance arise moving sc pc 
surprisingly wc worse pc pthor 
rc performs better models gains pc small 
maximum gain relaxing consistency model mp pthor lu 
better understand results breakdown execution time applications models 
execution time models normalized execution time base application 
bottom section column represents busy time useful cycles executed processor 
black section represents time processor stalled waiting reads 
time include time read acquire access may stalled waiting previous writes perform 
time represented section 
sections top represent stalls due write buffer full time spent spinning waiting acquires complete time spent waiting barrier 
general observations breakdown latency read misses forms large portion idle time especially move pc wc rc ii major reason base sc worse models stalling processor reads acquires pending writes complete iii write buffer full factor hindering performance pc iv reason wc performing worse pc rc extra processor stalls acquires read release accesses described section 
small variation busy times pthor due non deterministic behavior application input 
look comparative performance models greater detail 
difficult distinguish shade sections small height 
usually small height implies sections important safely ignored 
table statistics write misses including releases 
program write fraction average fraction average rate followed distance followed distance base cycles cycles cycles write read mp lu pthor performance sc versus base performance differences sc base arise fact sc able overlap latency write misses read access 
expected gains depend frequency write misses occur average distance write read 
clustering write misses reads helps sc processor stalled 
data figures show sc model perform significantly better base 
table provides relevant data explain results 
executions base model presents frequency write misses cycles ii fraction write misses followed write intervening reads indicates write clustering average distance iii fraction write misses followed reads average distance 
contents table reasons relative performance gains applications apparent 
mp see frequency write misses relatively high writes immediately followed read access read average cycles away 
complete write penalty cycles observed read 
case lu gains small write rate low read follows write cycles 
pthor write clustering observed distance write misses larger 
consequently gain higher mp lu 
general expect performance gains base sc small applications 
reads expected closely interleaved writes 
significant write clustering may occur example initializing data structures occurrences expected infrequent 
performance pc versus sc base pc model sequential consistency abandoned 
extra complexity programming model benefits 
main benefit comes fact reads stall pending writes perform 
benefits may lost write buffer gets full stalls processor 
show second factor turns issue 
focusing magnitude gains pc gains large frequency write misses high cost write results base slightly pessimistic due extra write hit latency level cache structure assumed simulations 
base stalls processor write including write hits 
sc sensitive effect writes buffered usually computation overlap small latency write hits 
mp normalized execution time barrier base sc pc wc rc lock write buffer full stall read acq read busy lu normalized execution time barrier base sc pc wc rc lock write buffer full stall read acq read busy pthor normalized execution time barrier base sc pc wc rc lock write buffer full stall read acq read busy breakdown execution time lfc 
misses high 
cost write misses applications relative gains depend primarily frequency misses 
table see base architecture frequency write misses including release lu mp pthor number write misses percentage read misses distribution write misses including releases read misses including acquires 
accesses cycles mp lu pthor 
consequently expect large gains mp medium gains pthor small gains lu 
case seen figures 
secondary effect apparent 
time compressed pc number useful instructions executed shorter time compared base congestion memory system increases read latencies go 
example mp fraction normalized cycles spent read misses goes move base pc 
consequently savings due hiding write latencies lost 
return issue write buffer full 
difficult write buffer stall times negligible mp lu pthor 
numbers small primarily multiprocessor blocking reads ii number read misses larger number write misses see table 
result average time write buffer retire writes processor stalled read misses 
course write buffer build short periods time due clustered writes benchmark applications see significant clustering 
indication interleaving read write misses data histogram number write misses read pairs 
data shows read misses write misses interleaved 
show fill depth write buffer encountered write accesses applications 
write buffer depth simulated architecture sufficient clustering applications 
summary see pc model relatively successful hiding latency writes reasonably deep write buffer 
comparison pc wc involved comparison rc examine pc versus rc 
subsequently compare pc wc 
performance pc versus rc rc model provides performance benefits pc model 
addition exploiting information synchronization accesses allows pipelining writes 
writes retired write buffer ownership lu mp pthor write buffer depth percentage writes distribution writes write buffer pc 
obtained 
fact writes retired faster rate implications write buffer full problem ii cases release operation writes write buffer release observed sooner processor waiting acquire 
discussed previous subsection write buffer getting full really problem pc 
gains observed due faster completion synchronization 
shows performance mp comparable pc rc 
seen pc stalled due full write buffer mp idle time corresponding synchronization approximately models 
mp rate synchronization small effect benefits rc negligible 
contrast mp lu pthor exhibit small performance gain pc rc 
lu idle time breakdown shows difference mainly due faster synchronization rc write buffer stalls gain respectively shown 
lu processors synchronize waiting pivot column ready 
processor updating pivot column series writes doing release operation indicating column ready 
allowing pipelining writes rc allows release issued earlier reduces synchronization time 
detailed simulation statistics show average waiting time gaining access column goes cycles pc cycles rc 
large waiting times lu due slight load imbalance application see table 
idle time breakdown pthor shows performance gain due faster synchronization normalized cycles 
simulations show average time obtain lock goes cycles pc cycles rc 
faster time synchronization occurs pthor exploits fine grain synchronization tight producer consumer relationships 
pthor high rate synchronization releases unlocks critical path usually process waiting lock released 
gains making releases visible slightly earlier large expected 
barriers take slightly time rc 
total time read misses gone 
closer look simulation statistics shows increase read idle time due larger number read misses 
probably due fact program took slightly different execution path rc 
performance wc versus pc rc compare wc rc explain wc performs worse rc 
differences wc rc arise wc differentiate acquire release synchronization operations 
consequently synchronization operation conservatively satisfy constraints release acquire 
compared rc wc stalls processor acquire pending writes releases complete 
addition processor stalled pending releases attempts read operation 
results show mp lu perform comparably wc rc 
reason mp lu low synchronization rates cycles ideal architecture 
extra stalls wc substantially increase idle time 
pthor rc performs noticeably better wc pthor higher synchronization rate 
understand performance pthor better gathered data regarding extra stalls introduced wc rc 
frequently write release cycles acquire operation 
similarly read access small number cycles release 
cycles latency write release hidden rest latency visible processor 
detailed data show virtually releases caused delay read acquires delayed due previous write misses 
compare wc pc explain surprising result pc performs better wc 
wc advantage writes retired faster rate write buffer 
disadvantage wc pc disadvantage wc rc wc stalls processor points pending writes releases perform 
shows wc performs slightly better pc lu mp worse pthor 
lu performance better pipelining writes shown effective reducing idle time due synchronization delays 
addition extra stalls introduced wc issue lu 
mp discussion pc rc explained pipelining gain pc 
addition wc hindered due extra stalling described 
explains models perform comparably mp 
pthor shows interesting result 
pthor pipelining important reduce time synchronization 
saw wc causes stalls quite 
disadvantage stalls turns greater advantage pipelining writes case 
see wc performing worse pc 
relative performance wc compared pc rc may affected fact simulated architecture cache locks 
caching locks beneficial processor acquires releases lock times processor accessing lock 
models gain faster acquire lock cache 
reduced latency release pc rc benefit hide write release latencies 
wc potentially benefit 
may reduce difference performance observed lfc basic consistency model performance gain basic base base sc pc wc rc performance mp lfc basic implementations 
wc versus pc rc pthor 
effect implementation variations previous section evaluated performance consistency models aggressive implementation lockup free caches 
goal minimize influence implementation comparison models 
section explore aggressive implementations study impact relative performance models 
specific implementations compare lfc aggressive implementation studied far lockup free caches reads bypass write buffer word deep write buffer ii lfc caches longer lockup free iii basic reads allowed bypass write buffer 
shows relative performance models implementation mp application 
results applications indicated similar trend 
processor utilization merit performance 
performance curves normalized base model basic implementation 
shown base sc affected implementation changes model exploit lockup free caches bypassing reads 
performance pc wc rc experience large decline move lfc smaller decline go basic 
result shows lockup free caches essential realizing full potential strict models 
analyze results detail 
implementation comparing lfc implementations see large performance difference mp pc wc rc models 
examine performance pc model 
difference lfc pc lfc read write outstanding read write outstanding 
read table average latency read cycles 
architecture mp lu pthor lfc follows closely write wait previously initiated write access completes tens cycles 
statistics mp show read misses application write cycles read addition lfc read misses serviced write outstanding large performance difference lfc 
results wc rc show large decrease performance lockup free caches 
lockup free cache provides benefits wc rc read serviced right away regardless previous write misses benefit pc ii write misses retired faster rate write buffer cache allows multiple outstanding accesses 
shown previous subsection pipelining write misses effective increasing performance mp 
detailed simulation results show lfc rarely outstanding requests time wc rc 
similar pc gain lfc comes primarily read access having stall cache servicing previous write table shows average latency read misses implementations rc applications 
results show latency read misses substantially reduced due lockup free cache 
basic implementation comparing basic implementations see relatively smaller decrease performance 
obvious advantage allowing bypassing read wait write buffer empty serviced 
performance gains bypassing depend clustering write misses read greater clustering bigger gains 
detailed simulation statistics mp show read misses write occurs shortly clustering write misses small 
gains moving basic small 
reason performance pc better sc basic 
sc level cache read hits read misses delayed write buffer empty 
pc read hits level cache delayed pending writes level read misses suffer penalty 
effect prefetching consistency models results far architecture blocking reads 
strict consistency models successful hiding latency writes architecture done alleviate latency reads 
consequently see large percentage idle cycles due read misses especially pc rc models 
section explore effects reduced read latency relative performance models 
particular model prefetch mechanism similar provided dash architecture 
general types prefetching binding non binding 
binding prefetch value access bound time prefetch completed 
contrast prefetch simply brings copy location higher level memory hierarchy closer processor 
non binding prefetch location accessible coherency mechanism kept coherent processor binds value regular binding access 
binding prefetching interacts consistency models prefetch affect consistency models 
non binding prefetch flexible easier affect correctness provide benefits consistency models 
prefetch mechanism provided dash non binding assume prefetch mechanism section 
detailed evaluation prefetching see 
results section show pipelining writes allowed rc wc significantly important achieving high performance latency reads reduced prefetching 
section presents simulation results ideal prefetching reads 
section provides realistic assessment case study applications prefetching added program 
ideal prefetching results modeled effects ideal non binding prefetch artificially making read misses take cycle simulations latency writes synchronization changed shows results simulation applications 
performance model normalized base application 
comparing prefetching results see pipelining writes rc important 
shown dramatically case mp 
pc performed comparably rc prefetching rc shown outperform pc significantly prefetching 
detailed simulation results show idle cycles pc caused stalls due write buffer full 
simulation deeper write buffer show gain pc rate write misses large 
pthor shows rc performing better pc prefetching 
comparing performance pc wc pthor shows pipelining writes important pc longer outperforms wc 
results lu show trend results gains due strict models accentuated 
results section pertain prefetching reads 
invalidation coherence scheme prefetches 
read exclusive prefetch invalidates copies location brings exclusively owned copy closer processor issued prefetch reduces latency reads writes 
non binding prefetching technique applicable models 
major performance difference models comes effectiveness reducing hiding write latency reduction latency writes prefetching general difference models pronounced 
results effects read prefetching read exclusive prefetching mp subsection 
consistency model performance gain base mp pthor lu base sc pc wc rc results ideal read prefetching lfc 
case study mp subsection results versions mp explicit read prefetching explicit read read exclusive prefetching 
prefetch instructions inserted appropriate places program 
prefetching simulator modeled way prefetches dash prefetch brings copy location special cache associated node 
shows relative performance models 
performance case normalized performance lfc base model 
results show prefetching benefits performance models 
relative performance models case read prefetching matches results ideal prefetching reads 
clearly reduction latency reads latency due write misses critical allows wc rc perform substantially better pc 
simulation results show occasionally requests outstanding time 
prefetching done difference pc wc rc diminishes considerably 
reducing latency write misses increases rate write buffer retire writes reduces chances full write buffer 
interesting observation read exclusive prefetching help wc rc read prefetching see top right 
reason wc rc hiding latency writes completely anyway reducing latency writes offers extra performance benefits 
summary read read exclusive prefetching substantially improve performance models including base sc 
note prefetching successful reducing latency accesses 
cases read prefetching successful show pipelining writes important 
expect read exclusive prefetching successful read prefetching works 
read exclusive prefetching shown diminish importance write pipelining allowing pc perform comparably rc 
lfc read read exclusive prefetch lfc read prefetch lfc consistency model performance gain base lfc base sc pc wc rc read read exclusive prefetching mp discussion section discuss validity results broader context 
addition briefly point issues tradeoffs concerning consistency models 
results specific set architectural parameters limited set benchmarks 
expect hold broader context 
major reads blocking pc performs rc 
achieve pc relies write buffer getting full 
intuitively write misses distributed evenly case applications studied time write buffer retire writes processor stalls read misses 
formally write buffer fill number read write misses corresponding latencies respectively 
applications including ones studied read misses write misses especially data usually read written 
addition architectures comparable latencies reads writes 
pc expected perform comparably rc wide range architectures applications 
study primarily concerned gains arise relaxing consistency constraints hardware level 
consistency constraints affect performance software level mainly enabling compiler optimization shared variables 
example common compiler optimizations register allocation common subexpression elimination involve changing order accesses program 
aggressive compiler optimizations blocking loops register allocation arrays result reordering accesses 
sc pc optimizations legally performed compiler 
significantly degrade performance system 
contrast rc wc provide considerable flexibility compiler optimizing shared accesses desirable models 
fact gains achieved doing software optimizations exceed gains achievable hardware 
conservative condition time processor busy executing instruction waiting acquire synchronizations contributes amount time write buffer retire writes 
issue concerning consistency models complexity model programmer 
efforts specifying programming restrictions result relaxed models equivalent sequential consistency far correctness concerned 
practice satisfying restrictions problem 
user level applications developed group satisfy restrictions change programs 
porting silicon graphics irix operating system dash require handful simple changes 
research needed help programmer identifying avoiding unwanted race conditions 
simulation results major assumptions 
assumptions limit gain achievable relaxing consistency model 
assumption processors block read access 
conditions wc rc allow multiple read accesses overlapped pipelined implementation blocking reads allow latency reads hidden manner 
design processors allow multiple outstanding reads order execution instructions effectiveness hiding latency reads current topic research 
results dependent assumption invalidation coherence scheme 
tradeoffs update coherence scheme quite different 
programs exhibit larger number read misses write misses invalidation scheme number read misses decrease write updates substantially increase update scheme 
may turn pipelining writes important prevent write buffer filling 
determine tradeoffs described require depth study update schemes 
related section discuss alternative implementations proposed sequential consistency weak consistency briefly describe previous evaluation efforts 
adve hill proposed aggressive implementation sequential consistency 
scheme requires invalidation cache coherence protocol 
points sc implementation stalls full latency pending writes implementation stalls ownership gained 
implementation satisfy sequential consistency new value written visible processors previous writes processor completed 
simulations show latency obtaining ownership slightly smaller latency write complete 
number invalidations caused write usually small 
visibility control mechanism reduces stall time sc slightly expect pc perform significantly better sc 
adve hill proposed implementation weak ordering strict wc 
constraints implementation quite similar constraints imposed rc 
performance model expected comparable rc practice significantly better pc 
dubois provide simple analytical models estimate benefits arising weak consistency lockup free caches 
gains predicted models large close order magnitude gain performance 
compare results models non blocking reads simulation results assume processor stalls reads 
torrellas hennessy detailed analytical model multiprocessor architecture estimate effects relaxing consistency model performance 
maximum gain performance predicted weak consistency sequential consistency 
prediction lower results study due mainly reasons latencies assumed lower ones simulated architecture ii bus bandwidth assumed architecture limiting factor cases high sharing weaker models gain 
concluding remarks enable multiprocessors hide reduce memory latency number memory consistency models proposed literature 
far detailed performance results reported 
study characterizes performance consistency models 
results showed architectures blocking reads sequentially consistent models base sc significantly worse performance strict models pc wc rc 
benchmark applications studied strict models shown improve processor utilization base model 
gains expected increase larger memory latencies seen machines 
showed benefits achieved strict models due buffering writes allowing reads bypass pending writes 
lockup free caches shown essential achieving full potential models 
ability pipeline writes critical performance especially reasonably deep write buffers 
surprising consequence processor consistency performed release consistency model 
shown true architectural variations 
processor consistency simpler implement hardware results suggest choosing processor consistency hardware model may reasonable approach current processors 
acknowledgments hank levy reviewers helpful comments 
people provided useful feedback earlier version sarita adve phillip gibbons james laudon christoph josep torrellas shigeru 
wish todd mowry providing simulator helping changes needed 
simulation results possible generous help 
application writers applications larry soule pthor jeff mcdonald mp todd mowry mp explicit prefetching ed rothberg lu 
rohit chandra helpful discussions 
research supported darpa contract 
kourosh gharachorloo partly supported texas instruments anoop gupta partly supported nsf presidential young investigator award 
sarita adve mark hill 
implementing sequential consistency cache systems 
proceedings international conference parallel processing pages august 
sarita adve mark hill 
weak ordering new definition 
proceedings th annual international symposium computer architecture pages may 
anant agarwal beng hong lim david kranz john kubiatowicz 
april processor architecture multiprocessing 
proceedings th annual international symposium computer architecture pages may 
michel dubois christoph 
memory access dependencies shared memory multiprocessors 
ieee transactions software engineering june 
michel dubois christoph briggs 
memory access buffering multiprocessors 
proceedings th annual international symposium computer architecture pages june 
kourosh gharachorloo dan lenoski james laudon phillip gibbons anoop gupta john hennessy 
memory consistency event ordering scalable sharedmemory multiprocessors 
proceedings th annual international symposium computer architecture pages may 
stephen goldschmidt helen davis 
tango tutorial 
technical report csl tr stanford university 
james goodman 
cache consistency sequential consistency 
technical report sci committee march 

lockup free instruction fetch prefetch cache organization 
proceedings th annual international symposium computer architecture 
leslie lamport 
multiprocessor computer correctly executes multiprocess programs 
ieee transactions computers september 
dan lenoski james laudon kourosh gharachorloo anoop gupta john hennessy 
directory cache coherence protocol dash multiprocessor 
proceedings th annual international symposium computer architecture may 
ewing lusk ross overbeek portable programs parallel processors 
holt rinehart winston 
jeffrey mcdonald donald 
vectorization particle simulation method flow 
aiaa thermodynamics lasers conference june 
todd mowry anoop gupta 
tolerating latency software controlled prefetching scalable shared memory multiprocessors 
journal parallel distributed computing appear june 
pfister george harvey mcauliffe melton norton weiss 
ibm research parallel processor prototype rp architecture 
proceedings international conference parallel processing pages 
christoph 
access ordering coherence shared memory multiprocessors 
phd thesis university southern california may 
christoph michel dubois 
concurrent resolution multiprocessor caches 
proceedings international conference parallel processing pages august 
larry soule anoop gupta 
parallel distributed time logic simulation 
ieee design test computers december 
josep torrellas john hennessy 
estimating performance advantages relaxing consistency sharedmemory multiprocessor 
proceedings international conference parallel processing pages august 
wolf dietrich weber anoop gupta 
analysis cache invalidation patterns multiprocessors 
proceedings third international conference architectural support programming languages operating systems pages april 
