dynamic recurrent neural networks barak pearlmutter december cmu cs supersedes cmu cs school computer science carnegie mellon university pittsburgh pa survey learning algorithms recurrent neural networks hidden units attempt put various techniques common framework 
discuss fixpoint learning algorithms recurrent backpropagation deterministic boltzmann machines non fixpoint algorithms backpropagation time elman history cutoff nets jordan output feedback architecture 
forward propagation online technique uses adjoint equations discussed 
cases unified presentation leads generalizations various sorts 
simulations issues computational complexity addressed 
research sponsored part defense advanced research projects agency information science technology office title research parallel computing arpa order issued darpa contract mda part national science foundation number eet part office naval research contract number 
author held fannie john alexander hertz fellowship 
views contained document author interpreted representing official policies expressed implied hertz foundation government 
keywords learning sequences temporal structure recurrent neural networks fixpoints contents recurrent networks hidden units continuous vs discrete time learning networks fixpoints fixpoint exist 
problems fixpoints recurrent backpropagation simulation associative network deterministic boltzmann machines backpropagation time time constants time delays simulations exclusive circular trajectory rotated stability perturbation experiments leech simulations non fixpoint techniques elman nets moving targets method forward propagation extending online learning time constants delays faster online techniques feedforward networks state teacher forcing continuous time jordan nets continuous time jordan nets summary complexity comparison acknowledgments recurrent networks subject document training recurrent neural networks 
problem training non recurrent layered architectures covered adequately discussed 
motivation exploring recurrent architectures potential dealing sorts temporal behavior 
recurrent networks capable settling solution vision system gradually solve complex set conflicting constraints arrive interpretation 
discussed extent primarily concerned problem causing networks exhibit particular desired detailed temporal behavior modeling central pattern generator insect 
noted real world problems think require recurrent architectures solution soluble layered architectures reason urge engineers try layered architectures resorting big gun recurrence 
hidden units restrict attention training procedures networks hidden units units particular desired behavior directly involved input output network 
biologically inclined thought interneurons 
practical successes backpropagation gratuitous virtues hidden units internal representations 
hidden units possible networks discover exploit regularities task hand symmetries replicated structure training procedures exploiting hidden units backpropagation current excitement neural networks field 
training algorithms operate hidden units widrow hoff lms rule procedure train recurrent networks hidden units recurrent networks hidden units reduce non recurrent networks hidden units need special learning algorithms 
consider neural network governed equations dy dt gammay oe state activation level unit ji total input unit ij strength connection unit unit oe arbitrary differentiable function 
typically function chosen squashing function oe gamma gamma case oe oe gamma oe oe tan gamma case oe oe gamma oe 
symmetric squashing function usually preferable number computational advantages simulations 
initial conditions driving functions inputs system 
defines general dynamic system 
assuming external input terms held constant possible system exhibit wide range asymptotic behaviors 
simplest system reaches stable fixpoint section discuss different techniques modifying fixpoints networks exhibit 
complicated possible asymptotic behaviors include limit cycles chaos 
describe number training procedures applied training networks exhibit desired limit cycles particular detailed temporal behavior 
chaotic dynamics play significant computational role brain training procedures chaotic attractors networks hidden units 
crutchfield lapedes farber success identification chaotic systems models temporally hidden units 
continuous vs discrete time concerned predominantly continuous time networks 
learning procedures discuss equally applied discrete time systems obey equations oe continuous time advantages expository purposes derivative state unit respect time defined allowing calculus tedious explicit temporal indexing making simpler derivations exposition 
continuous time system simulated digital computer usually converted set simple order difference equations formally identical discrete time network 
regarding discrete time network running computer simulation continuous time network number advantages 
sophisticated faster simulation techniques simple order difference equations higher order forward backward techniques 
second simple order equations size time step varied suit changing circumstances instance network signal processing application faster sensors computers available size time step decreased retraining network 
third continuous time units stiff time tend retain information better time 
way putting bias learning theory sense temporally continuous tasks certainly advantageous task performed temporally continuous 
advantage continuous time networks somewhat subtle 
tasks temporal content constraint satisfaction best way recurrent network perform required computation unit represent nearly thing nearby points time 
continuous time units default behavior absence forces units tend retain state time 
contrast discrete time networks priori reason unit state point time special relationship state point time 
pleasant benefit units tending maintain state time helps information past decay slowly speeding learning relationship temporally distant events 
learning networks fixpoints fixpoint learning algorithms discuss assume networks involved converge stable fixpoints 
networks converge fixpoints interesting class things compute constraint satisfaction associative memory tasks 
tasks problem usually network initial conditions constant external input answer state network reached fixpoint 
precisely analogous relaxation algorithms solve things steady state heat equations constraints need spatial structure uniformity 
fixpoint exist 
problem fixpoints recurrent networks converge 
number special cases guarantee converge fixpoint 
ffl simple linear conditions weights zero diagonal symmetry ij ji ii guarantee function gamma ij log gamma log gamma decreases fixpoint reached 
weight symmetry condition arises naturally weights considered bayesian constraints boltzmann machines 
ffl showed unique fixpoint reached regardless initial conditions ij ij max oe practice weaker bounds weights suffice indicated empirical studies dynamics networks random weights 
ffl empirical studies indicate applying fixpoint learning algorithms stabilizes networks causing exhibit asymptotic fixpoint behavior 
theoretical explanation phenomenon 
technically algorithms require fixpoint reached stable 
probability zero network converge unstable fixpoint practice convergence unstable fixpoints ignored 
energy landscape represented curved surface balls representing states network illustrates potential problems fixpoints 
initial conditions differ infinitesimally map different fixpoints mapping initial conditions fixpoints continuous 
likewise infinitesimal change weights change fixpoint system evolves starting point moving boundary watersheds attractors 
similarly point changed fixpoint non fixpoint infinitesimal change weights 
algorithm capable learning fixpoints require network trained settle fixpoint order operate backpropagation time 
nowlan train constraint satisfaction network queens problem shaping gradually train discrete time network hidden units exhibit desired attractors 
fixpoint algorithms consider take advantage special properties fixpoint simplify learning algorithm 
problems fixpoints guaranteed network settles fixpoint fixpoint learning algorithms run trouble 
learning procedures discussed compute derivative error measure respect internal parameters network 
gradient optimization procedure typically variant gradient descent minimize error 
optimization procedures assume mapping network internal parameters consequent error continuous fail spectacularly assumption violated 
consider mapping initial conditions resultant fixpoints 
dynamics network continuous need 
purposes visualization consider symmetric network dynamics cause state network descend energy function equation 
shown schematically infinitesimal change initial conditions location ridge slope intermediate point trajectory change fixpoint system ends 
words continuous 
means learning algorithm changes locations fixpoints changing weights possible cross discontinuity making error jump suddenly remains true matter gradually weights changed 
recurrent backpropagation pineda discovered error backpropagation algorithm special case general error gradient computation procedure 
backpropagation equations oe oe ij ij ordered partial derivative respect error metric simple derivative respect final state unit 
original derivations backpropagation weight matrix assumed triangular zero diagonal elements way saying connections acyclic 
ensures fixpoint reached allows computed efficiently single pass units 
backpropagation equations remain valid recurrent connections assuming fixpoint reached 
assume equation reaches fixpoint equation satisfied 
satisfied find satisfy give derivatives seek presence recurrent connections 
simple task reported reaching precise fixpoint crucial learning 
way compute fixpoint relax solution 
subtracting side get gammay oe fixpoint dy dt equation dy dt gammay oe appropriate fixpoints 
note gammay oe greater zero reduce value increasing circumstances dy dt positive greater zero 
choose giving technique relaxing fixpoint 
equation linear determined solution unique 
technique solving set linear equations 
computing fixpoint associated differential equation tempting dz dt gammaz oe ij equations admit direct analog implementation 
real analog implementation different time constants probably assumption time spend settling negligible compared time spend fixpoints rate weight change slow compared speed presentation new training samples weights updated continuously equation dw ij dt gammaj de dw ij momentum term ff desired ij dt gamma ff dw ij dt jy simulation associative network simulated recurrent backpropagation network learning higher order associative task associating pieces information bit shift registers direction bit equal equal rotated bit right 
task reconstruct pieces information 
architecture network shown 
groups visible units hold undifferentiated group hidden units fully bidirectionally connected visible units 
connections visible units 
extra unit called bias unit implement thresholds 
unit incoming connections forced value constant external input 
connections go unit allowing units biases equivalent negative threshold complicating mathematics 
inputs represented external input bit bit bit completed network 
network trained giving external inputs put randomly chosen patterns visible groups training third group attain correct value 
error metric squared deviation unit desired state units penalized correct 
patterns successfully learned ones ambiguous shown state diagrams 
weights training took epochs shown 
inspection weights large decidedly asymmetric training unit external input pushed bounds 
register register rotate 
bias architecture network solve associative version bit rotation problem 
hinton diagram weights learned network 
instabilities observed 
network consistently settled fixpoint simulated time units 
network tested untrained completion problems reconstructing half partially unambiguously specified performance poor 
redoing training weight symmetry enforced caused network learn training data untrained completions 
pineda recurrent backpropagation learning procedure successfully applied learning weights relaxation procedure dense stereo disparity problems transparent surfaces qian sejnowski 
training examples able learn appropriate weights deriving simplified unrealistic analytical model distribution surfaces encountered usual 
deterministic boltzmann machines mean field form stochastic boltzmann machine learning rule shown descend error functional 
stochastic boltzmann machines scope document give probabilistic interpretation mft boltzmann machines derivation 
network state cases bit rotation problem 
display shows states units arranged 
row shows value register rows 
row diagrams left show network state competing direction bit register register 
right shift 
note completions correct cases rotation bit determined shift registers pattern 
deterministic boltzmann machine transfer function oe gamma gamma temperature starts high value gradually lowered target temperature time network new input loss generality assume target temperature 
weights assumed symmetric zero diagonal 
input handled different way procedures discuss external inputs set zero subset units obeying values set externally 
units said clamped 
learning set input units states index ff clamped values network allowed settle quantities gamma ij ff ff ff ff accumulated delta 
denotes average environmental distribution superscripts denote clamping 
procedure repeated output units states index fi clamped desired values yielding ij ff fi ff ff fi ff fi point case ij ij gamma gamma ij ff fi ff log gamma measure information theoretic difference clamped distribution output units clamped input units 
gamma measures probable network says fi ff definition scope document 
recurrent network shown left representation network unfolded time time steps shown right 
learning rule version hebb rule sign synaptic modification alternated positive waking phase negative phase 
learning rule rigorously justified deterministic boltzmann machines applied success number tasks 
weight symmetry assumed definition energy definition probability fundamental mathematics practice weight asymmetry tolerated large networks 
mft boltzmann machines biologically plausible various learning procedures discuss difficult see possible extend learning complex phenomena limit cycles paths state space 
probably best technique domain application turn attention procedures suitable learning dynamic sorts behaviors 
backpropagation time fixpoint learning procedures discussed unable learn non fixpoint attractors produce desired temporal behavior bounded interval learn reach fixpoints quickly 
turn learning procedures suitable non fixpoint situations 
consider minimizing functional trajectory taken instance gamma dt measures deviation function minimizing teach network imitate derive technique computing ij efficiently allowing gradient descent weights minimize backpropagation time train discrete time networks perform variety tasks 
derive continuous time version backpropagation time couple toy domains 
infinitesimal changes considered 
infinitesimal changes considered 
ii ji ij lattice representation 
derivation take conceptually simple approach unfolding continuous time network discrete time network step deltat applying backpropagation discrete time network limit deltat approaches zero get continuous time learning rule 
derivative approximated dy dt deltat gamma deltat yields order difference approximation deltat gamma deltat temporally discretized versions continuous functions 
define ffie ffiy usual case form dt 
intuitively measures small change time affects left unchanged 
usual backpropagation define denotes ordered derivative variables ordered time unit index 
intuitively measures small change time affects change propagated forward time influences remainder trajectory 
course limit deltat 
ffi standard backpropagation generalized ffi rule 
chain rule ordered derivatives calculate terms deltat 
chain rule add separate influences varying direct contribution comprises term equation 
varying effect deltat gamma deltat giving second term gamma deltat deltat 
weight ij influence deltat compute influence stages 
varying varies ij varies oe ij oe varies deltat ij oe deltat 
gives third final term ij oe deltat deltat 
combining deltat gamma deltat deltat ij oe deltat deltat put form take limit deltat obtain differential equation dz dt gamma gamma ij oe boundary conditions note limit deltat 
consider making infinitesimal change dw ij ij period deltat starting cause corresponding infinitesimal change oe dw ij wish know effect making infinitesimal change ij time integrate entire interval yielding ij oe dt derive calculus variations lagrange multipliers william personal communication optimal control theory 
fact idea gradient descent optimize complex systems explored control theorists late 
mathematical techniques handled hidden units refrained exploring systems degrees freedom fear local minima 
interesting note recurrent backpropagation learning rule section derived 
held constant assume network settles fixpoint integrated time unit reduce recurrent backpropagation equations sense backpropagation time generalization recurrent backpropagation 
time constants add time constant unit modifying dy dt gammay oe carry terms derivation section equations dz dt gamma gamma ij oe ij oe dt order learn time constants just set hand need compute substitute ae gamma find ae derivation similar substitute back get gamma dy dt dt time delays consider network signals take finite time travel link modified ji gamma ji ji time delay connection unit unit include variable time constants section 
surprisingly time delays merely add analogous time delays dz dt gamma gamma ij oe ij ij ij oe ij ij dt remains unchanged 
set ij deltat modified equations alleviate concern time skew simulating networks sort obviating need accurate numerical simulations differential equations allowing simple difference equations fear inaccurate error derivatives 
regarding time delays fixed part architecture imagine modifiable time delays 
modifiable time delays able learn appropriate values accomplished gradient descent ij oe ij dy dt gamma ij dt simulated networks modifiable time delays progress institution 
interesting class architectures state unit modulate time delay arbitrary link network time constant unit 
architectures appropriate tasks time warping issue speech recognition certainly accommodated approach 
presence time delays reasonable connection single pair units different time delays different connections 
time delay neural networks proven useful domain speech recognition 
having connection unit requires modify notation somewhat weights time delays modified take single index introduce external apparatus specify source destination connection 
weight connection unit unit time delay connection 
notation write jl gamma equations general written notation readability suffer translation quite mechanical 
simulations simulations networks time delays mutable time constants 
associative network section extra input unit value held constant external input outgoing connections units implement biases 
order finite difference approximations integrated system forward set boundary conditions integrated system backwards numerically integrating oe dy dt computing ij computing dz dt requires oe stored replayed backwards 
stored replayed expressions numerically integrated 
error functional gamma dt desired state unit time importance unit achieving state time 
oe gamma gamma time constants initialized weights initialized uniformly distributed random values gamma initial values set oe 
simulator order difference equations deltat 
exclusive network trained solve xor problem 
aside addition time constants network topology pineda 
defined gamma dt ranges cases correct output input hidden output xor network 
states output unit input cases plotted epochs learning 
error computed 
state output unit 
inputs net range possible boolean combinations different cases 
suitable choice step size momentum training time comparable standard backpropagation averaging epochs 
task network benefit units attain final values quickly possible tendency lower time constants 
avoid small time constants degrade numerical accuracy simulation introduced term decay time constants 
decay factor simulations described really necessary task suitably small deltat simulation 
easier justifiable approach simply introduce minimum time constant done simulations 
interesting binary task network dynamic behavior 
extensive training network behaved expected saturating output unit correct value 
earlier training occasionally training sessions observed output unit nearly correct value saw move wrong direction stabilizing wildly incorrect value 
dynamic effect run shown 
output unit heads wrong direction initially corrects error window 
minor case diving desired states plotted left actual states plotted epoch center right 
desired states plotted left actual states plotted epoch center right 
correct value moving away seen lower left hand corner 
circular trajectory trained network input units hidden units output units fully connected follow circular trajectory 
required leftmost point circle go circle twice circuit units time 
environment include desired outputs period network moves initial position correct location leftmost point circular trajectory 
network run circuits cycle overlap closely separate circuits visible 
examining network internals devoted hidden units maintaining shaping limit cycle fourth hidden unit decayed away quickly 
decayed pulled units appropriate starting point limit cycle decayed ceased affect rest network 
network different units limit behavior initial behavior appropriate modularization 
unable train network hidden units follow shape shown network hidden units 
trajectory output units crosses units governed order differential equations hidden units necessary task regardless oe function 
training output rotated network trained angles left untrained angles right 
difficult circular trajectory shaping network behavior gradually extending length time simulation proved useful 
network moves short loop initial position ought 
goes shaped cycle units time 
network run circuits cycle produce graph overlap closely separate circuits visible 
rotated simulation network trained generate output units precisely way section rotated center angle input network input units held coordinates unit vector appropriate direction 
different values equally spaced circle generate training data 
experiments hidden units network unable learn task 
increasing number hidden units allowed network learn task shown left 
network run input angles furthest training angles shown right generalization poor 
task simple solve second order connections allow problem decoupled 
units devoted orthogonal oscillations connections form rotation matrix 
poor generalization network shows solving problem straightforward fashion suggests tasks sort better slightly higher order units 
output states plotted time unit run units network perturbed random amount units time 
perturbations circle network left uniform sigma network right sigma 
stability perturbation experiments analytically determine stability network measuring eigenvalues df function maps state network point time state time 
instance network exhibiting limit cycle typically function maps network state time cycle state corresponding time cycle 
attempt judge stability limit cycles exhibited going trouble calculating df simply modified simulator introduce random perturbations observed effects perturbations evolution system 
output units task appear phase locked phase relationship remains invariant face major perturbations 
phase locking solution human create analytically determining weights decoupling output units linearized subnets generate desired oscillatory behavior 
limit cycle right symmetric perturbations introduced right symmetry broken 
portion limit cycle moving upper left hand corner lower right hand corner diverging lines believe indicate high eigenvalues instability 
lines converge rapidly upward stroke right hand side analogous wouldn care eigenvalues df se wouldn care perturbations direction travel effect phase 
reason want project matrix computing eigenvalues 
effect achieved automatically display 
unstable behavior symmetric downward stroke upper right hand corner lower left 
analysis shows instability caused initialization circuitry inappropriately activated initialization circuitry adapted controlling just initial behavior network net delay time cycle moving lower left corner circuitry explicitly symmetric 
diverging lines caused circuitry activated exerting strong influence output units circuitry deactivates 
leech simulations techniques discussed fit low level neurophysiological model leech local bending reflex data sensory motor neuron activity 
modified dynamic equations substantially order model system low level activity levels represent currents voltages 
trained model disagreed human intuition concerning synaptic strengths fact signs qualitatively matched empirical measurements counterintuitive synaptic strengths 
non fixpoint techniques elman nets elman investigated version discrete backpropagation time temporal history cut 
typically timesteps preserved discretion architect 
cutoff backpropagation time online algorithm backpropagation done account error point time done immediately 
computational expense time step scale linearly number timesteps history maintained 
accuracy computed derivative smoothly traded storage computation 
real question elman networks contribution error history cut significant 
question answered relative particular task 
instance elman finds problems amenable history cutoff resorts full fledged backpropagation time tasks 
cleeremans find regular language token prediction task difficult elman nets transition probabilities equal find breaking symmetry allows nets learn task 
moving targets method rohwer proposed moving targets learning algorithm 
algorithms phases alternate 
phase hidden units targets improved targets attained better performance achieved 
phase weights modified unit comes closer attaining target 
error regarded having terms term penalizes units far targets penalizes targets far values attained 
technique appeal decoupling temporally distant actions learning weights disadvantage requiring targets stored updated 
limit learning rates decreased moving targets method equivalent backpropagation time 
primary disadvantage technique pattern learned associated targets hidden units targets learned just weights 
technique inapplicable online learning pattern seen 
forward propagation online exact stable computationally expensive procedure training fully recurrent networks discovered robinson fallside rediscovered independently :10.1.1.52.9724
develop technique follows 
form calculate ij follows 
apply chain rule ij ij dt fl ijk dt fl ijk ij calculated derivative respect ij yielding auxiliary equations dfl ijk dt fl ijk lk fl ijl net integrating simulation making weight change continuously update weights equation dw ij dt gammaj fl ijk integrating expression assumption online weight changes affect trajectory taken see equivalent discrete update equation replaces 
auxiliary quantities fl ijk initial boundary conditions zero start time computations carried forward time 
technique online learning procedure amount time network run need known advance history need stored 
addition stable technique introduce numerical instabilities 
technique poses substantial computational burden computation auxiliary equations dominating 
units weights primary state variables calculations required update 
auxiliary variables require total calculations update 
furthermore primary equations potentially implemented directly analog hardware auxiliary equations weight times making analog hardware implementation difficult 
extending online learning time constants delays easily extend online learning procedure take account time constants 
substitute take partial respect ij substitute fl possible differential equation fl dt gammafl kij oe lk nearly time constant 
derive analogous equations time constants define take partial respect substitute yields dq dt gammaq gamma dy dt oe ki update time constants continuous update rule dt gammaj similarly derive equations modifying time delays section 
define ij ij take partial respect ij arriving differential equations dr ij dt gammar ij oe ij dy dt gamma ij included gamma lk ij gamma lk time delays updated online continuous update equation ij dt gammaj ij faster online techniques way reduce complexity algorithm simply leave auxiliary variables reason believe remain approximately zero simply discarding corresponding terms 
approach particular ignoring coupling terms relate states units module weights explored zipser 
clever transformations adjoint equations derive exact stable variant forward propagation algorithm requires auxiliary variables updated just time 
technique announced just document went press simulated appears quite promising 
verified practice technique appear dominate online algorithm described technique choice online learning 
feedforward networks state noteworthy basic mathematical technique forward propagation applied networks restricted architecture feedforward networks units state 
requiring ij matrix triangular allowing non zero diagonal terms 
fl quantities ordered derivatives standard backpropagation simplified architecture reduces computational burden substantially 
elimination temporal interaction fl ijk leaving auxiliary equations updated computation total update burden conventional backpropagation 
favorable computational complexity practical significance large feedforward recurrent networks 
feedforward networks outside scope 
teacher forcing teacher forcing consists jamming desired output values units network runs teacher forces output units correct states network runs name 
technique applied discrete time clocked networks concept changing state output unit time step sense 
error usual caveat errors measured output units forced 
williams zipser report teacher forcing technique radically reduced training time recurrent networks teacher forcing networks larger number hidden units reported difficulties 
continuous time williams zipser application teacher forcing networks deeply dependent discrete time steps applying teacher forcing temporally continuous networks requires different approach 
approach shall take add knobs control states output units keep output units locked desired states 
error function minimized measure amount control necessary exert zero error coming knobs need twisted 
gammay oe just dy dt add new forcing term dy dt phi denote set units forced trajectory force follow phi set dd dt gamma phi phi consequence phi 
error functional form dt typically phi modify derivation section teacher forced system 
phi change canceled immediately limit deltat yields 
doesn matter phi 
apply calculate phi 
chain rule calculate change effects yielding phi ffie ffif phi gamma oe ij phi unchanged phi remains unchanged 
equations required ij phi phi 
derive consider instantaneous effect small change ij giving ij oe dt analogously phi gamma dy dt dt left system number special cases depending units phi 
interestingly equivalent system results leave unchanged setting phi setting 
open question way defining results simplification 
jordan nets jordan backpropagation network outputs clocked back inputs generate temporal sequences 
networks long teacher forcing perspective jordan nets simply restricted class teacher forced recurrent networks particular discrete time networks recurrent connections emanate output units 
teacher forcing output units real recurrent paths remain simple backpropagation single time step suffices training 
main disadvantage architecture state retained network time manifest desired outputs network new persistent internal representations temporal structures created 
instance impossible train networks perform task section 
usual control theory way difficulty partially alleviated cycling back inputs just previous timestep outputs small number previous timesteps 
tradeoffs hidden units encapsulate temporally hidden structure temporal window values contain desired information problem dependent depends essence long hidden variable remain hidden manifested observable state variables 
continuous time jordan nets easy construct continuous time jordan network units values continuous time output units constantly corrected values external sources recurrent connections outputs back inputs 
done general setting fully recurrent networks note passing natural teacher forced continuous time jordan network state held individual units equivalent simply training layered network produce derivative output signal current value output signal input 
technique time space online 
stable 
local 
storing sn backwards forward propagation nm forward propagation nm table summary complexity learning procedures recurrent networks 
storing technique store time run forwards replay run time backwards computing backwards store recomputing time run backwards 
forward propagation online techniques described section 
times computing gradient respect pattern 
summary complexity comparison consider network units weights run time steps variable grid methods reduce dynamically varying deltat gamma deltat 
additionally assume computation network partitioned 
conditions simulating system takes time time step simulating system 
means technique described section entire simulation takes time time step best hoped 
storing activations weights takes space storing forward simulation replay simulating backwards takes sn space technique entire computation takes sn space 
simulate backwards backwards simulation simulation requires space best hoped 
technique susceptible numeric stability problems 
online technique described section requires time time step nm space 
technique alluded section requires time space retains online advantages appear dominate original technique assuming simulations bear stability 
time complexity results sequential machines summarized table 
note section concerning computation takes obtain gradient information 
generally just inner loop complex algorithm adjust weights uses gradient information gradient descent algorithm gradient descent momentum conjugate gradient 
experience shown learning networks tended stiff sense hessian error respect weights matrix second derivatives tends wide eigenvalue spread 
technique apparently proven useful particular situation robert jacobs applied fang sejnowski problem described section great success 
leech simulations described section apparently substantial reduction number epochs required convergence 
applications identification control explored author thesis research 
signal processing speech generation recognition generative techniques domains type network naturally applied 
domains may lead complex architectures discussed section 
control domains important ways force learning solutions stable control sense term 
fact simard developed technique learning local maximum eigenvalue transfer function optionally projecting directions eigenvalues interest 
technique applied control domain 
hand turn logic section 
consider difficult constraint satisfaction task sort neural networks applied traveling salesman problem 
competing techniques problems simulated annealing mean field theory 
providing network noise source modulated second order connections say see learning algorithm constructs network noise generate networks simulated annealing pure gradient descent techniques evolved 
hybrid network evolves structure may give insight relative advantages different optimization techniques best ways structure annealing schedules 
recurrent networks avoided fear inordinate learning times incomprehensible algorithms mathematics 
clear fears unjustified 
certainly reason recurrent network layered architecture suffices hand recurrence needed plethora learning algorithms available spectrum quiescence vs dynamics spectrum accuracy vs complexity spectrum space vs time storage 
acknowledgments advisor david touretzky 
david ackley geoffrey hinton terry sejnowski 
learning algorithm boltzmann machines 
cognitive science 
robert allen joshua alspector 
learning stable states stochastic asymmetric networks 
technical report tm bell communications research morristown nj november 
almeida 
learning rule asynchronous perceptrons feedback combinatorial environment 
proceedings st international conference neural networks volume pages san diego ca june 
ieee 
amir 
learning general network 
dana anderson editor neural information processing systems pages new york new york 
american institute physics 
blom sanz serna jan 
simple moving grid methods dimensional evolutionary partial differential equations 
stichting mathematisch centrum amsterdam netherlands 
axel cleeremans david servan schreiber james mcclelland 
finite state automata simple recurrent networks 
neural computation 
cohen steven grossberg 
stability global pattern formation parallel memory storage competitive neural networks 
ieee transactions systems man cybernetics 
crutchfield mcnamara 
equations motion data series 
complex systems 
jeffrey elman 
finding structure time 
technical report crl center research language ucsd april 
yan fang terrence sejnowski 
faster learning dynamic recurrent backpropagation 
neural computation 
freeman 
brains chaos order sense world 
brain behavioral science november 
conrad geoffrey hinton 
deterministic boltzmann learning networks asymmetric connectivity 
technical report crg tr university toronto department computer science 
michael 
learning algorithm analog fully recurrent neural networks 
international joint conference neural networks volume pages 
ieee 
marco gori bengio renato de mori 
bps learning algorithm capturing dynamic nature speech 
international joint conference neural networks volume pages 
ieee 
geoffrey hinton 
learning distributed representations concepts 
proceedings eighth annual cognitive science conference 
lawrence erlbaum 
geoffrey hinton 
deterministic boltzmann learning performs steepest descent weight space 
neural computation 
geoffrey hinton terrence sejnowski 
optimal perceptual inference 
proceedings ieee conference computer vision pattern recognition pages washington dc june 
ieee computer society 
geoffrey hinton terrence sejnowski david ackley 
boltzmann machines constraint satisfaction networks learn 
technical report cmu cs carnegie mellon university may 
hopfield tank 
neural computation decisions optimization problems 
biological cybernetics 
robert jacobs 
increased rates convergence learning rate adaptation 
technical report coins university massachusetts amherst ma 
michael jordan 
attractor dynamics parallelism connectionist sequential machine 
proceedings cognitive science conference pages 
lawrence erlbaum 
arthur bryson jr steepest ascent method solving optimum programming problems 
journal applied mechanics 
kirkpatrick gelatt jr vecchi 
optimization simulated annealing 
science 
gary kuhn 
look phonetic discrimination connectionist models recurrent links 
working institute defense analysis princeton new jersey april 
kevin lang geoffrey hinton 
development time delay neural network architecture speech recognition 
technical report cmu cs department computer science carnegie mellon university november 
kevin lang geoffrey hinton alex waibel 
time delay neural network architecture isolated word recognition 
neural networks 
alan lapedes robert farber 
nonlinear signal processing neural networks prediction system modelling 
technical report theoretical division los alamos national laboratory 
shawn yan fang terrence sejnowski 
dynamic neural network model sensorimotor transformations leech 
neural computation 
shawn jr distributed processing sensory information leech input output relations local bending 
journal neuroscience 
shawn jr distributed processing sensory information leech ii identification interneurons contributing local bending reflex 
journal neuroscience 
shawn jr qian sejnowski 
neural network analysis distributed representations sensory information leech 
david touretzky editor advances neural information processing systems ii pages 
morgan kauffman 
steven nowlan 
gain variation recurrent error propagation networks 
complex systems june 
mary patrice simard dana ballard 
fixed point analysis recurrent neural networks 
david touretzky editor advances neural information processing systems morgan kauffman 
david parker 
learning logic 
technical report tr mit center research computational economics management science cambridge ma 
barak pearlmutter 
learning state space trajectories recurrent neural networks 
technical report cmu cs department computer science carnegie mellon university pittsburgh pa 
barak pearlmutter 
learning state space trajectories recurrent neural networks 
neural computation 
peterson james anderson 
mean field theory learning algorithm neural networks 
technical report ei mcc august 
peterson anderson 
mean field theory learning algorithm neural nets 
complex systems 
fernando pineda 
generalization back propagation recurrent neural networks 
physical review letters 
ning qian terrence sejnowski 
learning solve random dot stereograms dense transparent surfaces recurrent backpropagation 
proceedings connectionist models summer school pages san mateo ca 
morgan kaufman 
steve renals richard rohwer 
study network dynamics 
journal statistical physics june 
robinson fallside 
static dynamic error propagation networks application speech coding 
dana anderson editor neural information processing systems pages new york new york 
american institute physics 
richard rohwer 
moving targets training algorithm 
touretzky editor advances neural information processing systems pages san mateo ca 
morgan kaufmann 
david rumelhart geoffrey hinton williams 
learning internal representations error propagation 
parallel distributed processing explorations microstructure cognition volume bradford books cambridge ma 
patrice simard jean pierre bernard 
shaping state space landscape recurrent networks 
touretzky editor advances neural information processing systems 
morgan kaufmann 
appear 

adjoint operators non adiabatic learning algorithms neural networks 
touretzky editor advances neural information processing systems 
morgan kaufmann 
appear 
tokunaga 
modified leaky integrator network temporal pattern recognition 
international joint conference neural networks volume pages 
ieee 
alex waibel hinton shikano lang 
phoneme recognition time delay networks 
ieee transactions acoustics speech signal processing 
paul werbos 
regression new tools prediction analysis behavioral sciences 
phd thesis harvard university 
paul werbos 
generalization backpropagation application recurrent gas market model 
neural networks 
widrow hoff 
adaptive switching circuits 
western electronic show convention convention record volume pages 
institute radio engineers 
ronald williams david zipser 
learning algorithm continually running fully recurrent neural networks 
technical report ics report ucsd la jolla ca november 
david zipser 
reduces complexity speeds learning recurrent networks 
touretzky editor advances neural information processing systems pages san mateo ca 
morgan kaufmann 
