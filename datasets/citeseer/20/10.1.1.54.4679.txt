proceedings ijcnn international joint conference neural networks vol 
pp 
piscataway nj ieee service center 
appear proceedings ijcnn ieee international joint conference neural networks anchorage alaska may 
dimensionality reduction random mapping fast similarity computation clustering samuel kaski helsinki university technology neural networks research centre box fin hut finland samuel kaski hut data vectors high dimensional computationally infeasible data analysis pattern recognition algorithms repeatedly compute similarities distances original data space 
necessary reduce dimensionality example clustering data 
dimensionality high websom method organizes textual document collections self organizing map commonly dimensionality reduction methods principal component analysis may costly 
demonstrated document classi cation accuracy obtained dimensionality reduced random mapping method original accuracy nal dimensionality suoeciently large 
fact shown inner product similarity mapped vectors follows closely inner product original vectors 

exists wealth alternative methods reducing dimensionality data ranging different feature extraction methods multidimensional scaling 
feature extraction methods tailored nature data generally applicable example data mining tasks 
multidimensional scaling methods hand computationally costly dimensionality original data vectors high infeasible linear multidimensional scaling methods principal component analysis dimensionality reduction 
new rapid dimensionality reduction method needed situations impossible original vectors existing dimensionality supported academy finland 
reduction methods costly 
random mapping method provides computationally feasible method reducing dimensionality data mutual similarities data vectors approximately preserved 
motivation method dates back experiments ritter kohonen 
organized words information contexts tend occur 
dimensionality representations contexts reduced replacing dimension original space random direction smaller dimensional space 
may surprising random mapping reduce dimensionality data manner preserves structure original data set useful 
main goal explain random mapping method works high dimensional spaces analytical empirical evidence 

random mapping method linear random mapping method original data vector denoted multiplied random matrix mapping rn results reduced dimensional vector matrix consists random values euclidean length column normalized unity 
way interpreting random mapping consider happens dimensions original space mapping 
ith column denoted random mapping operation expressed ith component denoted original vector components weights orthogonal unit vectors expression dimension original data space replaced random non orthogonal direction reduced dimensional space 

properties random mapping utility random mapping method clustering depends fundamentally mutual similarities data vectors 
clear closer vectors orthonormal better similarities vectors obtained random mapping correspond original similarities 
hint choosing random directions vectors useful provided hecht nielsen exists larger number orthogonal orthogonal directions highdimensional space 
high dimensional space vectors having random directions suoeciently close orthogonal provide approximation basis 
distortions random mapping method causes mutual similarities data vectors characterized statistically 

transformation similarities cosine angle vectors commonly measure similarity 
results restricted vectors unit length case cosine computed inner product vectors 
inner product vectors obtained random mapping vectors respectively expressed follows rm matrix decomposed terms ffl ffl ij ffl ii components diagonal collected identity matrix 
equal unity vectors normalized 
units ooe diagonal collected matrix ffl 
entries ffl equal zero vectors orthogonal matrix equal similarities documents preserved exactly random mapping 
practice entries ffl small equal zero 
statistical properties ffl 
possible analyze statistical properties entries ffl distribution entries random mapping matrix distribution components column vectors assume components initially chosen independent identically normally distributed mean zero length normalized 
result procedure direction distributed uniformly 
evident ffl ij denotes average random choices entries practice speci instance matrix need know distribution ffl ij judge utility random mapping method 
proven cf 
appendix dimensionality space large ffl ij approximately normally distributed 
variance denoted oe ffl approximated oe ffl distribution ffl ij dimensionalities illustrated 
matrix approximate identity matrix better higherdimensional vectors 
statistical properties mutual similarities 
know distribution ffl possible investigate closely similarities original vectors transformed random mapping 
speci cally pair original data vectors possible derive distribution similarity vectors obtained random mapping respectively 
equations inner product mapped vectors expressed ffl kl denote ffi ffl kl expression deviance form original value inner product produced random mapping 
mean ffi zero mean term sum zero 
shown appendix variance ffi denoted oe ffi expressed oe ffi gamma oe ffl gamma gamma gamma gamma gamma distribution ffl ij 
distribution inner products pairs random vectors distribution ffl ij dioeerent dimensionalities increases inner products smaller vectors orthogonal 
orthogonality perfect generally small inner products contribute small distortions similarity computations 
normal distributions variance equal plotted gure curves distinguishable empirical curves demonstrates distribution ffl ij approximates normal distribution fairly small values length original data vectors xed unity inner product oe ffi oe ffl summary distortion inner products produced random mapping zero average variance inverse dimensionality reduced space multiplied 
consider simple instructive setting data vectors constrained certain amount ones rest components zero 
ones occur position vectors inner product oe ffi gamma oe ffl inner product xed variance error smaller sparser input smaller random mapping function better sparser data 
random mapping som consider random mapping data vectors processing data 
self organizing map som algorithm serve instructive example case experiments reported sec 

valid distance clustering algorithms 
som algorithm constructs mapping input space usually dimensional lattice 
lattice position called map unit contains model vector result algorithm model vectors neighboring map units gradually learn represent similar input vectors 
mapping ordered 
resulting map intuitive representation data set 
map example data exploration applications multitude applications cf 

som algorithm consists steps applied iteratively 
winning unit model vector closest current input selected model vectors units neighbors winning unit map lattice updated 
may useful notice random mapping operation linear small neighborhoods original space mapped small neighborhoods smaller dimensional space 
som model vectors neighboring units generally close small neighborhoods original space mapped single map unit set neighboring map units 
som probably sensitive distortions similarities caused random mapping 
considering random mapping inputs learning som consider concept nullspace mapping operator mapping operation considered non orthonormal formed rows rows form set random vectors original space 
nullspace operator subspace original space mapped zero vector 
input vector resides original data space decomposed unique sum orthogonal components gamma belongs nullspace complement 
input vector mapped random mapping operator result parts orthogonal nullspace rn projection eoeect removes parts reside nullspace mapped vector rn input som time step model vectors updated rule ci rn gamma ci called neighborhood kernel decreasing function distance units map lattice 
index unit model vector closest rn 
update occurs mapped space interested comparing results update results obtained som operate inputs original space 
fact possible consider model vectors original space stated exactly virtual images model vectors complement nullspace mapping operator denote pseudoinverse virtual image model vector original space de ned denote virtual image image vector smallest norm vectors maps multiply sides get ci gamma learning rule eoeect corresponds learning original data space complement nullspace may disadvantageous deliberately neglect rest vectors component demonstrated empirically sec 
reduction dimensional space dimensional random mapping produces satisfactory results 
may striking case null space dimensional randomly chosen dimensions taken account 
reason results probably clearly recognizable equation dimensional vectors variance similarity smaller largest possible similarity 

experiments mapping textual documents websom system 
websom system websom method organizing textual documents dimensional map display 
nearby locations display contain similar documents aids browsing document collection 
map content addressable document vector document encoding map document collection mapping function path nntp hut fi news fi news 
fi eu net usenet 
news uk psi net cam ac uk cus cam ac uk newsgroups comp ai neural nets subject efficient computation date feb gmt organization university cambridge england lines message id uk nntp posting host cus cam ac uk tin version pl 
schematic diagram basic building blocks websom system 
websom documents rst encoded numerical vectors mapped dimensional display document collection som algorithm 
search ltering interesting documents incoming document stream 
task encoding documents websom system fig 
case study random mapping method 
noted sake clarity case study include possible ingredients websom system 

encoding documents random mapping simple eoeective document encoding method called vector space model documents represented vectors space dimension corresponds word 
value component equal relative frequency occurrence corresponding word document 
alternatively function frequency occurrence importance word may 
resulting vectors thought representing word histograms documents 
length document vector normalized direction vector contents document 
unfortunately impossible vector space model large document collections dimensionality resulting document vectors high 
dimensions vectors words vocabulary 
vector space model ideal candidate random mapping 
fact random mapping word histograms shown produce promising results preliminary experiments 

results usefulness random mapping method reducing dimensionality document vectors measured index designed measure relative goodness dioeerent document encoding methods websom system 
goal websom produce map location contains set similar articles close locations contain similar sets 
laborious assess success method subtle details task possible measure dioeerent topic areas separated map document collection 
document collection experiments consisted articles usenet newsgroups groups considered represent dioeerent topic areas 
noted similar newsgroups grouped groups highly overlapping 
separability groups relative criterion comparing dioeerent document encoding methods absolute measure goodness websom method 
constructing word histograms documents rarest common words removed 
removals dimensionality document vectors 
histograms word weighted entropy weight 
separability newsgroups document map measured teaching unit som encoded documents inputs labeling map unit group dominated unit 
separability newsgroups measured total number documents groups dominating nodes 
computations text document material corresponds usage websom method real situations 
important construct map certain document collection able generalize result new documents 
separability newsgroups function dimensionality obtained random mapping method depicted results obtained pca 
pca essentially equivalent latent semantic indexing method reduce dimensionality document vectors 
separability obtained pca rises rapidly saturates 
random mapping requires somewhat larger dimensionalities results essentially obtained pca results obtained original vectors 
computational complexity forming random matrix nd negligible computational complexity estimating principal components nn 
dimensionalities separability newsgroups dimensionality mapping random mapping pca 
separability topic areas websom document map function dimensionality document vectors obtained random mapping pca 
bars denote standard deviations experiments 
separability obtained original document vectors 
fore random mapping respectively number data vectors 

discussion random mapping method shown offer promising computationally feasible alternative dimensionality reduction situations reduced dimensional data vectors clustering similar approaches 
especially original dimensionality data large infeasible computationally costly methods pca 
method applied websom document organization system 
dimensionality original data vectors describe documents high order thousands computations required construct self organized map infeasible rapid dimensionality reduction method 
random mapping method demonstrated produce essentially results pca original data vectors dimensionality mapped vectors 
random mapping method produced better separability dioeerent topic areas vs alternative method encoding documents faster 
exist straightforward neural implementations random mapping method emphasis properties mapping implementation 
acknowledgments wish prof teuvo kohonen useful discussions concerning methodology ideas contributed signi cantly results 
am grateful rvi carrying simulations needed fig 
dr timo honkela simulations needed fig 

appendix proofs equation distribution ffl ij derived fairly easily vectors consist independent normally distributed values normalized length vectors equals unity 
inner product fact estimate correlation independent identically normally distributed random variables 
normalization vectors corresponds normalization estimate square roots sums squares instances random variables 
old result due fisher ln ffl ij gamma ffl ij normally distributed variance equal gamma number samples estimate 
equation linearized zero claim follows large equation de nition ffi oe ffi ffl kl ffl pq ffl kl ffl pq straightforward verify ffl kl ffl pq oe ffi oe ffl oe ffl oe ffl gamma gamma oe ffl gamma oe ffl assumption data vectors normalized 
deerwester dumais furnas landauer latent semantic analysis journal american society information science vol 
pp 

golub van loan matrix computations north oxford academic oxford england 
hecht nielsen vectors general purpose approximate meaning representations self organized raw computational intelligence imitating life marks ii robinson eds ieee press piscataway nj pp 

honkela kaski lagus kohonen newsgroup exploration websom method browsing interface tech 
report laboratory computer information science helsinki university technology espoo finland 
kaski idata exploration self organizing maps acta mathematics computing management engineering series march dr tech 
thesis helsinki university technology finland 
kaski honkela lagus kohonen order digital libraries self organizing maps proceedings world congress neural networks lawrence erlbaum inns press mahwah nj pp 

kohonen self organizing maps springer berlin second extended edition 
kohonen kaski lagus honkela large level som browsing newsgroups proceedings icann international conference arti cial neural networks von der malsburg von seelen eds lecture notes computer science vol 
springer berlin pp 

lagus honkela kaski kohonen maps document collections new approach interactive exploration proceedings second international conference knowledge discovery data mining simoudis han fayyad eds aaai press menlo park california pp 

ritter kohonen organizing semantic maps biological cybernetics vol 
pp 

salton mcgill modern information retrieval 
mcgraw hill new york 
