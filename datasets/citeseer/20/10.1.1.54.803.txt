bayesian models non linear peter uller mike west institute statistics decision sciences duke university durham nc steven maceachern department statistics ohio state university columbus oh peter uller partially supported nsf dms mike west dms 
developed steve maceachern visiting duke university 

discuss classes bayesian mixture models nonlinear autoregressive times series developments semi parametric bayesian density estimation years 
development involves formal classes multivariate discrete mixture distributions providing flexibility modelling arbitrary non linearities time series structure formal inferential framework address problems inference prediction 
models relate naturally existing kernel related methods threshold models offer major advances terms parameter estimation predictive calculations 
theoretical computational aspects developed involving efficient simulation posterior predictive distributions 
various examples illustrate perspectives identification inference mixture approach 
keywords autoregressive time series bayesian computations mixture models non linear time series developments bayesian density estimation mixture models produced flexible implementable methods inferring features multivariate distributions estimating non linear regression relationships west uller escobar 
parallels development introducing novel mixture models time series domain presenting approaches non linear modelling analysis autoregressive time series 
real sense development leads encompassing framework non linear offering flexibility represent arbitrary forms non linear conditional expectations generally practically critically conditional distributions 
exclusively context real valued univariate time series conceptual basis anticipates possible developments discrete data models multivariate series 
example non linear predictors threshold non parametric kernel approaches neural networks forth fall framework 
methodological viewpoint computational bayesian development stochastic simulation posterior predictive distributions permits routine implementation model fitting assessment examples final section demonstrate 
models mixtures normal distributions obviously linked terms point estimates conditional ar predictions nonparametric kernel regression approaches 
framework involves complete specification sequences conditional distributions just ad hoc point estimates facility automatic smoothing parameter estimation embedded general bayesian learning framework delivers full probabilistic measures relevant uncertainties point estimates 
addition nice feature model homes sub models small numbers mixture components crudely speaking number distinct kernel components offering automatic reduction model dimension complexity con text realised time series support simpler model forms 
conditional expectations take form locally weighted mixtures linear auto regressions 
model puts hierarchical prior size terms mixture 
similar models albeit naturally hierarchical mixture model prior proposed non bayesian literature 
include threshold autoregressive models hidden markov chain autoregressive models non linear additive ar models threshold smooth threshold ar models general class state dependent models review models basic concepts non linear non stationary time series analysis see example priestley tong 
bayesian approaches models proposed 
geweke develop bayesian inference threshold autoregressive models 
albert chib discuss hidden markov chain autoregressive models 
papers dealing related class non normal non linear state space models carlin polson stoffer jacquier polson rossi carter kohn 
models may express specific cases non linear autoregressive relations considered 
differ models considered defining dynamic parameters evolve time semi parametric framework introducing hierarchical prior models static parameters 
section develops basic mixture modelling framework introducing dirichlet process priors basis defining mechanisms model switching classes standard normal linear 
extensions models including non linear regressions exogenous covariates included 
section deals computational bayesian analysis provides algorithms fitting models stochastic simulation methods roots related algorithms density estimation contexts 
section presents simulation study explore long run performance proposed models examples canadian lynx data old faithful data general summary comments 
mixture modelling framework general background bayesian mixture modelling density estimation regression appears west uller escobar specific regression material uller west 
basic theory mixture models relevant discuss relevant components theory 
non linear ar contexts consider univariate real valued time series observed equally spaced time points suppose interest lies developing models conditional distributions ar assumption focus rests density functions jy gamma note stationarity assumptions data may ultimately turn consistent stationarity 
practically viable approaches modelling non linear structure mixtures normal linear ar models 
true 
assume exist number possible linear models characterised parameters fi fi may generated models 
denote model selected particular assume jy gamma fi fi gamma selected set possible models selection probabilities ij gamma exp gamma gamma gamma additional hyperparameter indicates random variable normal distribution mean variance assume known informative prior available 
model defines locally weighted mixture similar idea threshold autoregression 
threshold autoregression value lagged variables gamma linear submodel applies sudden changes thresholds locally weighted mixture provides smooth change linear submodels star models tong 
submodel receives maximum weight gamma relative weight falling gaussian kernel 
mixture prior set possible models ng induced building hierarchical mixture model structure inherent bayesian mixture models west uller escobar 
ffi denote point mass specifically assume ffi dp ffg random discrete distribution modeled dirichlet process dp prior base measure ffg denoted dp ffg 
probability distribution hyperparameters ff total mass parameter dp describes concentration dp prior prior expectation full mathematical details dp model see example antoniak 
key features results relevant model discussed 
complete model assuming ff 
denote fg corresponding respectively 
dp prior samples random discrete measure generating step function fg centered step sizes close discrete fg determined total mass parameter ff 
large values ff lead random fg close smaller values ff correspond variation assumption dp prior critical model alternative prior models weights locations possibly finite number point masses lead similar inference 
utilize dirichlet process prior formulation straightforward interpretation parameters ff dp fact proven useful applications bush maceachern escobar west maceachern uller west west uller escobar 
computational issues implementation similar mixture models forms prior distributions 
key features results relevant prior model mixture summarised 
set model vectors fi fi contains distinct quadruples fi fi useful introduce configuration indicators fs models arranged different groups group having regression line selection probability parameter count relative numbers distinct set defining fi js jg noting exists prior distribution implicit underlying dirichlet process theory generating number distinct model vectors mechanism configuration indicators chosen 
prior determined single precision parameter ff sample size discussed 
marginalizing configuration indicators follow marginal prior sjk gamma 
see example maceachern uller 
second sample marginally sampled third model produces predictive distribution conditional list model vectors takes form locally weighted mixture normal linear regressions 
jy fi fi exp gamma gamma words generate model picks normal linear regressions respective probabilities depending gaussian kernel weight function 
equation obviously connects traditional non linear non parametric autoregression concepts priestley embedded coherent framework allowing formal parameter estimation bayesian context 
problems underlying autoregression jy gamma irregular non linear non normal possibly multimodal value tend large slope intercept parameters fi fi widely dispersed 
corresponding features local regression lines may quite distinct providing varying structure differing degrees smoothing different regions sample space 
furthermore weights components determined adapt varying concentrations mass typically moderate order ff log 
stressed grow context observed data configurations demand 
formally required bayesian predictive distribution obtained averaging respect posterior distribution data jy jy dp jy including hyperprior ff model allow learning ff uncertain features addition primary uncertainties vectors 
return equation 
computations required evaluate necessary posterior jy perform integration possible methods stochastic simulation developed exemplified west uller escobar maceachern bush maceachern maceachern uller uller west original univariate modelling context escobar west 
important aspect computation stage moved introduced bush maceachern detailed steps iii iv algorithm section 
simulation analysis delivers sequences values parameters representing draws posterior jy 
monte carlo approximation central distribution may deduced replacing integral monte carlo summation 
specifically posterior samples approximated jy gamma jy applications concern estimation independent sampling models 
normal models covariates reduces replacing sigma sigma particular additional mixture 
details different basic principle dirichlet process model parameterize mixture main issues building markov chain monte carlo scheme estimate model 
general autoregressive mixture model model easily extended include higher order autoregression additional covariates 
gamma gammap denote dimensional vector lagged observations additional dimensional exogenous covariate vector write oe dimensional gaussian kernel moments evaluated assume initial values gammaj known informative prior available 
model replaced jx oe delta fi equivalently jx fi ij oe complete model description specifying ff 
assume dp ffg fi fi fi fi delta ga gamma aw delta fi fi gamma fi gamma fi cc gamma gamma gamma qr gamma ff ga ff ff ff delta denotes wishart distribution scalar parameter matrix parameter ga delta denotes gamma distribution shape scale special case non linear ar model predictive distributions take form locally weighted mixture linear regressions jx oe fi dirichlet process model puts prior probability model size individual terms fi mixture process allocating parameters common values computations markov chain monte carlo scheme estimate autoregressive mixture model prior implement markov chain monte carlo mcmc scheme appropriate combination gibbs sampling independence chain metropolis steps 
review markov chain monte carlo schemes see example gelfand smith tierney smith roberts gilks 
listed 
basic rationale markov chain monte carlo posterior inference simulate markov chain defined desired posterior limiting distribution 
ergodic averages evaluate posterior inference 
describe mcmc scheme suited model 
currently imputed values unknown parameters ff fi fi move iteration markov chain replacing parameters steps ix described 
write gamma number distinct elements nfs number indices denotes full data set write yj jx likelihood 
remember set unique elements indicators map vector partitioned fi 
list fi fi fi collects fi similarly step define additional latent parameter vector remember denotes set hyperparameters fi fi 
explicitly describing transition probability give outline schematically listing updating sequence 
item form xjy indicates parameter updated parameters kept fixed 
absence parameter data conditioning set indicates conditionally independent marginalize updating detailed description point marginalization 
fixed hyperparameters covariates listed conditioning set 
possible updating done draw conditional posterior distribution 
parameters resort metropolis independence chain step indicated 
invariance posterior distribution transitions defined markov chain guaranteed gibbs independence chain metropolis steps 
see example tierney 
step ix concerns resampling latent parameter vector defined 
set model vectors ng alternatively parameterized whichever zation notation clearer expression 
sequence ii iii fi js iv jfi gamma vi ffj vii fi jfi fi viii fi jfi fi ix jj note step ii parameter vector augmented latent indicator variables sampling fi condition starting step iv marginalize step ix parameter vector augmented condition 
ii viii marginalize steps ii iii vii viii ix draws complete conditional posterior straightforward gibbs sampling steps 
step iv metropolis step 
step independence chain step 
step vi draws complete conditional posterior appropriate data augmentation 
step described detail 
sampling 
motivate latent variable scheme introduce start considering resampling implicitely conditional posterior ff 
new value set equal probability yj equal new draw yj probability ff yj dg 
appropriate normalizing constant 
note distribution simply posterior prior combining identical defining yj written ff ffi direct simulation unfortunately hindered integral expression maceachern uller mm show scheme possible extend list cluster locations include 
empty clusters mm specify prior distribution augmented parameter vector induces prior distribution model original parameterization 
note definition includes constraint cluster locations indexed non empty clusters come empty ones 
mm derive full posterior conditionals required gibbs sampling model 
rearrange indices correspondingly redefine drawing conditional replaced simulating conditional posterior distribution 
note resampling conditional change 
multinomial probabilities follows 
yj ff yj probability multinomial probabilities notice integral expression replaced sum simple dimensional normal density evaluations required 
ii sampling generating latent variables straightforward multinomial sampling probabilities oe fi 
iii sampling fi fi js 
note conditioning set includes indicators introduced 
indicators allow associate set indices gamma ft jg 
conditional posterior fi simple autoregression gamma giving normal inverse gamma conditional posterior fi 
iv updating implementation straightforward gibbs sampling step precluded complicated form gamma fi allow efficient random variate generation 
metropolis step tierney update generate candidate sigma compute min yj yj denotes currently imputed vector replaced normal hyperprior 
yj jx denotes likelihood 
current implementation initial value covariance matrix sigma normal proposal distribution 
second probability replace leave unchanged 
updating complete conditional posterior allow efficient random variate generation implement gibbs sampling step 
realize independence chain step tierney 
generate candidate gamma gamma gamma qr gamma compute min gamma gamma min yj yj probability replace keep unchanged 
choice candidate generating distribution gamma motivated simple form acceptance probabilities 
vi viii updating ff fi fi conditionals resample fi fi straightforward multivariate normal wishart distributions 
resampling ff done introducing latent beta distributed variable described escobar west west 
ix resampling 
note independent parameters conditional posterior fi identical normal inverse gamma normal prior 
convergence section discuss convergence issues markov chain monte carlo scheme defined earlier 
detailed discussion parameterization convergence issues mixture dirichlet process models refer maceachern uller 
summarize main result 
delta denote transition probability defined iterations gibbs sampler current state chain write posterior distribution 
tierney sufficient condition convergence subset parameter space theta parameter vector theta integer irreducibility 
configuration vector introduces finite partition parameter space subspaces theta equal configurations 
exists configuration theta 
iteration gibbs sampler allows move configuration positive probability initial move configuration positive probability iteration 
sub steps ii ix generate distributions mutually absolutely continuous respect arguments suffice show oe 
decide termination simulated markov chain diagnostic proposed geweke informal graphical methods 
diagnostic indicated practical convergence iterations examples reported sections 
respectively 
actual cpu time dec alpha minutes respectively 
examples simulation experiment simulated example generated data known models ar model exogenous covariate gamma ffl ii threshold autoregression tar ffl gamma ffl iii ar model gamma gamma ffl 
examples assumed ffl simulated observations 
additional data points fitting models generated compute predictive mean squared error 
simulation example estimated ar coefficients true ar model autoregressive mixture model 
example ii estimated coefficients true tar model true threshold ar model threshold model 
example iii fitted true ar model model 
example simulated experiments 
simulated experiment ar threshold ar models estimated maximum likelihood non linear ar model estimated posterior simulation described section 
table reports mean squared errors simulations fitted models 
denote fitted value th observation th experiment denote simulated observation time th experiment 
third column reports mean squared error mse fit gamma examples ii example iii 
columns report mean squared prediction error step ahead forecasts mse gamma true model mse fit gamma delta mse 
smaller values indicate overfit larger values indicate lack fit 
comparison footnote reports marginal variance 
mse conditional stationary fitted values posterior means conditional stationarity 
compute markov chain monte carlo simulation described section dropping posterior simulations corresponding non stationary solutions mcmc averages 
table suggests little loss efficiency mixture model data generated linear ar model examples iii potential gains true model violates linear ar assumption example ii 
example ii loss efficiency mixture model threshold ar small true threshold fitting threshold ar 
role maximum number terms mixture linear ar critical long large explain non linearity example ii 
simulations example ii guidelines emerge choice parameter maximum number distinct linear submodels locally weighted mixture 
choice related expected non linearity autoregressive function 
example allows change regimen allows shaped functions general guideline suggest choose larger values doubt 
simulations reported table give evidence large value example example ii critical small choice example linear ar example ii 
ar model covariate bowman analyzed data set concerning old faithful national park wyoming 
data set records eruption durations intervals subsequent collected continuously august st august th 
original observations removed observations taken night recorded durations short medium long 
plots data 
fit non linear ar model times duration previous eruption additional covariate 
likelihood specified jy gamma fi fi gamma fi ffl ffl ij gamma oe gamma 
figures show elements estimated non linear autoregression 
model allows non linearity non normality 
crucial data set 
illustrates conditional modal trace provide better summary important features conditional distribution jy conditional expectations 
notice lack normality 
lagged variable gamma range data suggest bimodal conditional jy gamma exemplified 
plots step ahead forecasts 
see section details implementation prior choice 
harmonic process model example non linear autoregression estimate generalization harmonic process models west estimates unknown frequencies harmonic models fitting corresponding unit root ar models 
ffl oe 
ar model jy gamma gamma fi fiy gamma gamma gamma defines harmonic model fixed frequency arccos fi time varying amplitude phase subject jfij 
generalization harmonic process model allows frequencies selected conditional covariates 
clearly formalized modifying allow locally weighted mixture auto regressions form 
specifically likelihood takes form jy gamma gamma fi gamma gamma gamma jj gamma gamma oe gamma gamma 
illustrate model estimating data giving annual number lynx mackenzie river district north west canada period priestley 
shows time series log scale 
plots histograms imputed periods fi posterior samples regression coefficients fi priestley observes asymmetry behaviour series 
time spent rising side period rising trough peak slightly longer time spent falling side peak trough 
histograms separated points rising side period gamma gamma points falling side gamma gamma confirming observation asymmetry 
markov chain monte carlo simulation easy consider additional model elaborations simply adding appropriate layers simulation scheme 
example measurement error model assuming autoregressive process lynx population straightforward extension model requiring additional step simulation scheme impute latent variables representing true population numbers 
implementation prior choice examples choices hyperparameters model 
hyperprior fi chosen noninformative gamma matrices diagonal matrices simply reflecting scale problem 
variables centered 
remaining hyperparameters example reported section 
chosen aw 
aw ff ff 
example section 
fi vector dimensional 
replaced wishart hyperprior gamma fi gamma distribution ga gamma fi cc gamma shape parameter 
examples maximum number terms mixture 
initialized gibbs samplers fi fi gamma gamma examples reported section 
respectively 
initial values computed squares fits regression ffl total mass parameter ff hyperprior mean ff 
interest analysis focuses prediction estimation full conditional distribution jy 
empirical evidence suggests inference little sensitive particular hyperprior parameter choice wide range 
larger values maximum number terms weighted mixture tends marginally increase number posteriori estimated distinct sub models 
albert chib 
bayes inference gibbs sampling autoregressive time series subject markov mean variance shifts 
journal business economic statistics pp 

antoniak 
mixtures dirichlet processes applications non parametric problems 
annals statistics pp 

bowman 
look data old faithful 
applied statistics pp 

bush maceachern 
semi parametric bayesian model randomised block designs 
biometrika 
carlin polson stoffer 
monte carlo approach non normal non linear state space modelling 
journal american statistical association pp 

carter kohn 
bayesian methods conditionally gaussian state space models 
technical report australian graduate school management unsw 
escobar west 
bayesian density estimation inference mixtures 
journal american statistical association 
gelfand smith 
sampling approaches calculating marginal densities 
journal american statistical association pp 

geweke 
evaluating accuracy sampling approaches calculation posterior moments 
bayesian statistics ed 
berger bernardo dawid smith pp 

geweke 
bayesian threshold auto regressive models nonlinear time series 
journal time series analysis 
gilks clayton spiegelhalter best mcneil kirby 
modelling complexity applications gibbs sampling medicine 
journal royal statistical society ser 
pp 

jacquier polson rossi 
bayesian analysis stochastic volatility models 
journal business economics statistics pp 

maceachern 
estimating normal means conjugate style dirichlet process prior 
communications statistics simulation computation pp 

maceachern mueller 
estimating mixture dirichlet process models 
discussion isds duke university 
mueller west 
bayesian curve fitting multivariate normal mixtures 
biometrika pp 

priestley 
non linear non stationary time series analysis academic press london 
smith roberts 
bayesian computation gibbs sampler related markov chain monte carlo methods discussion 
journal royal statistical society ser 
pp 

tierney 
markov chains exploring posterior distributions 
annals statistics pp 

tong 
nonlinear time series clarendon press oxford 
west 
hyperparameter estimation dirichlet process mixture models 
discussion isds duke university 
west cao 
assessing mechanism neural synaptic activity 
bayesian statistics science technology case studies ed 
hodges kass new york 
west mueller escobar 
hierarchical priors mixture models application regression density estimation 
aspects uncertainty tribute lindley eds 
smith freeman wiley new york 
west turner 
deconvolution mixtures analysis neural synaptic transmission 
statistician pp 

west 
bayesian inference cyclical component dynamic linear models 
journal american statistical association pp 

table mean squared errors simulation experiments iii 
fitted model mse fit mse mse mse ar mixture ii tar ar mixture mixture mixture iii ar mixture note true sampling models marked theoretically optimal values mse fit delta delta examples ii iii respectively 
optimal value mse 
larger values indicate lack fit smaller values indicate overfit 
values parentheses numerical standard deviations accuracy reported value 
comparison marginal variances examples ii iii respectively 

old faithful data set waiting time eruption versus lagged gamma versus duration eruption 

jy jy estimated non linear autoregression function 
panel shows jy solid curve 
comparison dots plot observed data 
short line segments show elements distinct regression lines imputed iteration 
lines intercept fi slope fi center segments corresponds panel shows dimensional ar regression surface estimated jy 

jy jy addition point estimate unknown autoregression shown model allows inference uncertainty line 
panel shows jy grey shade contour plot 
note show joint bivariate distribution shows conditional distributions 
autoregressive relationship summarized conditional expectations solid white line alternatively conditional modes white stars 
comparison triangles show actual data points 
panel shows jy evidently displaying clear non normality 
beta beta posterior distributions fi jy fi jy old faithful data 
note multimodality fi jy reflecting clustering 
time 
step ahead forecasts jy gamma solid line 
actual time series dashed line dots shown comparison 
noticeably non bell shaped overlayed shows jy 
year lambda fi 
annual number lynx mackenzie river district north west canada period 
panel shows time series logarithms 
panel shows histograms imputed periods fi 
histogram separated points falling side gamma gamma solid line points rising side gamma gamma dashed line 
year 
step ahead forecasts jy gamma solid line 
actual time series dashed line dots shown comparison 
point forecasts model delivers full description uncertainty exemplified plotting predictive distribution jy bell shaped curve overlayed 


example example ii 
example iii simulation example forecasts true model solid line mixture model dashed line ar model panel dotted line 
forecasts specific simulation examples 
curves show fitted values respective model 
example solid line example plots fi fi gamma fi jy 
fi fi fi denote coefficients ar model covariate curves show posterior predictive mean respective model 
example solid line example plots fi fi gamma fi jy 
dots plot comparison simulated data 
