multi agent reinforcement learning independent vs cooperative agents ming tan gte laboratories incorporated road waltham ma tan gte com intelligent human agents exist cooperative social environment facilitates learning 
learn error cooperation sharing instantaneous information episodic experience learned knowledge 
key investigations number reinforcement learning agents cooperative agents outperform independent agents communicate learning price cooperation 
independent agents benchmark cooperative agents studied ways sharing sensation sharing episodes sharing learned policies 
shows additional sensation agent beneficial efficiently sharing learned policies episodes agents speeds learning cost communication joint tasks agents engaging partnership significantly outperform independent agents may learn slowly 
tradeoffs just limited multi agent reinforcement learning 
human society learning essential component intelligent behavior 
individual agent need learn scratch discovery 
exchange information knowledge learn peers teachers 
task big single agent handle may cooperate order accomplish task 
examples common non human societies 
example ants known communicate locations food move objects collectively 
reinforcement learning study intelligent agents mahadevan lin tan 
reinforcement learning agent incrementally learn efficient decision policy state space trial error input environment delayed scalar reward 
task agent maximize long term discounted reward action 
reinforcement learning focused exclusively single agents extend reinforcement learning straightforwardly multiple agents independent 
outperform single agent due fact resources better chance receiving rewards 
whitehead demonstrated potential benefit multiple cooperative agents single agent 
practical study compare performance independent agents cooperative agents identify tradeoffs 
study done previously 
subject 
reinforcement learning agents cooperative identify ways cooperation 
agents communicate instantaneous information sensation actions rewards 
second agents communicate episodes sequences sensation action reward triples experienced agents 
third agents communicate learned decision policies 
presents case studies multiagent reinforcement learning involving cooperation draws related limited multi agent reinforcement learning 
main thesis cooperation done intelligently agent benefit agents instantaneous information episodic experience learned knowledge 
specifically case study investigate ability agent utilize sensation input provided agent 
demonstrate sensory information agent beneficial relevant sufficient learning 
show instance cooperative agents able efficiently learn decision policies compared independent agents due insufficient sensation agents 
case study focuses sharing learned policies episodes 
show cases cooperation speeds learning affect asymptotic performance 
provide upper bounds communication costs incurred cooperation 
sharing policies limited homogeneous agents sharing episodes heterogeneous agents long interpret episodes 
case study concerns joint tasks require agent order accomplished 
demonstrate cooperative agents sense partners communicate sensations learn perform tasks level independent agents reach start slowly 
cooperative agent sense agents size state space increase exponentially terms number involved agents 
ideally intelligent agents learn cooperate cooperative method achieve maximum gain 
starting point examination fundamental open questions 
related multi agent learning systems developed speed accuracy 
gte ils system silver integrates heterogeneous inductive search knowledge learning agents central controller agents critique proposals 
male system sian uses interaction board similar blackboard coordinate different learning agents 
dls shaw sikora adopts distributed problem solving approach rule induction dividing data inductive learning agents 
chan stolfo advocate meta learning distributed learning 
systems deal inductive learning examples autonomous learning agents involve perception action 
exception complexity analysis cooperative mechanisms reinforcement learning whitehead 
main theorem reinforcement learning agents observe decrease required learning time rate omega gamma 
field distributed artificial intelligence dai gasser huhns addressed issues organization coordination cooperation agents multi agent learning 
terms dai case studies explore reinforcement learning collaborative reasoning systems pope concerned coordinating intelligent behavior multiple self sufficient agents case study studies reinforcement learning distributed problem solving systems durfee tan particular problem divided agents cooperate interact develop solution 
dai deal issues communication language agent beliefs resource constraint negotiation 
mainly focus homogeneous agents 
reinforcement learning reinforcement learning line technique approximates conventional optimal control technique known dynamic programming bellman 
external world modeled discrete time finite state markov decision process 
action associated reward 
task reinforcement learning maximize long term discounted reward action 
study reinforcement learning agent uses step learning algorithm watkins 
learned decision policy determined state action value function estimates longterm discounted rewards state action pair 
current state available actions qlearning agent selects action probability boltzmann distribution jx actions ak temperature parameter adjusts randomness decisions 
agent executes action receives immediate reward moves state time step agent updates recursively discounting utilities weighting positive learning rate fi fi gamma fl fl discount parameter max actions note updated action state selecting actions stochastically ensures action evaluated repeatedly 
agent explores state space estimate improves gradually eventually approaches ef fl gamma reward received time due action chosen time gamma 
watkins dayan shown learning algorithm converges optimal decision policy finite markov decision process 
prey hunter grid world 
task description tasks considered study involve hunter agents seeking capture randomly moving prey agents grid world shown 
time step agent hunter prey possible actions choose moving left right boundary 
initially hunters random moves equal values 
agent occupy cell 
prey captured occupies cell hunter case study hunters occupy cell prey prey case study 
capturing prey hunter hunters involved receive reward 
hunters receive gamma reward move capture prey 
hunter limited visual field inside locate prey accurately 
shows visual field depth 
hunter sensation represented relative distance closest prey hunter axis 
example perceptual state closest prey lower left corner hunter visual field see 
prey equally close hunter chosen randomly sensed 
prey sight unique default sensation 
run experiment consisted sequence trials 
trial run agents random location 
trial began rewarded hunters random locations 
trial ended prey captured 
run sufficient number trials decision policies hunters converged performance hunters stabilized 
measured average number time steps trial training actions selected boltzmann distribution intervals trials 
convergence measured average number time steps trial test actions selected highest value trials 
results averaged runs 
learning parameters set fi fl 
values reasonable perceptual state represented visual field depth 
tasks 
task parameters include number prey number hunters hunters depth 
learning hunters move randomly baseline performances different prey hunter tasks table 
table shows average number steps random hunters capture prey trials 
tested performances independently learning hunters corresponding tasks 
table gives average number steps capture prey training calculated sufficient number trials hunters visual field depth 
clearly learning hunters significantly outperform random hunters 
real question cooperation learning hunters improve performance 
case sharing sensation study effect sensation agent 
isolate sensing learning choose prey hunter task add scouting agent capture prey 
extend concept hunters perform scouting hunting 
demonstrate sensory information scouting agent beneficial information relevant sufficient learning 
scout random moves 
step scout send action sensation back hunter 
assume initial relative location scout hunter known 
hunter incrementally update scout relative location compute location prey sensed scout 
example relative locations prey scout known scout hunter sensed respectively relative location prey hunter 
keep dimension state representation combine sensation inputs hunter scout follows hunter sensation hunter sense prey scout sensation 
table shows average numbers steps capture table average number steps capture prey random vs independently learning hunters 
prey hunters joint task joint task random hunters learning hunters table scouting vs scouting 
hunter visual depth scout visual depth average steps capture prey training test scouting sigma sigma sigma sigma sigma sigma sigma sigma prey training trials ones test convergence scout 
confidence intervals calculated test listed parentheses 
hunter scout took fewer steps training test capture prey 
scout depth increases difference performances larger 
observation held hunter visual field depth values 
state representation maximum number perceptual states grid world theta 
introducing scout size state space hunter effectively increased 
increase traded extra sensory information paid 
fact scout depth obvious slowdown observed trials 
establishing benefit additional sensory information scout extended concept prey hunter task hunter acting scout hunter 
table gives similar measures independent agents 
confidence intervals calculated test resulting test comparisons pair parentheses 
visual field depth increases independent mutual scouting agents take fewer fewer steps capture prey mutual scouting agents gradually outperform independent agents advantage mutual scouting agents independent agents shows sooner test training 
average steps hunter training scout visual field depth hunter scout difference significant test 
example visual field depth hunters took average steps test capture prey comparing steps independent hunters 
depth limited sharing sensory information hindered training short sighted scouting hunter stay prey long hunter learn catch prey 
suggests sensory information agent extra insufficient information interfere learning 
scouting incurs communication cost 
information communicated mutual scouting agent agent step bounded size bits sensation action representation 
experiment log depth depth visual field depth 
case sharing policies episodes assume agents share sensation 
agent adequate accomplish task hunter capture prey cooperation agents useful 
studied ways sharing learned policies episodes hunter task 
hunters decision policy exchange individual policies various frequencies 
episodes exchanged peer hunters peer expert hunters 
show cooperative agents speed learning measured average number steps training eventually reach asymptotic performance independent agents 
study presents experimental results hunters visual field depth table independent agents vs mutual scouting agents 
visual depth average steps capture prey training test independent agents sigma sigma mutual scouting agents sigma worse sigma independent agents sigma sigma mutual scouting agents sigma sigma better independent agents sigma sigma mutual scouting agents sigma better sigma better number trials average steps training cumulative independent policy independent agents vs policy agents 

visual field depth similar 
simple way cooperating hunters decision policy 
hunter updates policy independently rate updating policy multiplied number hunters step 
shows hunters policy converged quicker independent hunters 
average information communicated policy hunter step bounded number bits needed describe sensation action reward 
experiment log depth 
assume agent keeps decision policy 
step rest involved agents send current sensation policy keeping agent receive corresponding actions return send rewards actions back policy keeping agent 
number trials average steps training cumulative independent policy averaging policy averaging policy averaging independent agents vs policy averaging agents 
agents perform task decision policies learning differ may explored different parts state space 
hunters complement exchanging policies agent learned benefit 
assume agent simultaneously send current policy agents adopted policy assimilation agents average policies certain frequency 
shows performance results hunters averaged policies steps steps steps 
converged quicker independent hunters 
interesting observation visual field depth best frequency steps see visual field depth best frequency steps shown 
general information communicated policy exchanging hunter step bounded gamma delta delta number number trials average steps training cumulative independent episode peer episode expert independent agents vs episode exchanging agents 
participating hunters size policy number perceptual states theta number actions theta number bits needed represent sensation action value frequency policy exchanging 
large communication costly 
hand policy agents policy exchanging agent selective assimilating agent policy 
example agent adopt agent decision confidence certain actions 
sharing learned knowledge policy agents share episodes 
episode sequence sensation action reward triples experienced agent 
episode exchanging hunter captured prey hunter transferred entire solution episode hunter 
hunter mentally replayed episode forward update policy 
result hunters doubled learning experience 
middle curve shows speedup training hunters exchanging episodes 
average information communicated episode exchanging hunter step bounded gamma delta number bits needed represent sensation action reward log depth experiment addition flexibility assimilating episodes exchanging episodes heterogeneous reinforcement learning agents long interpret episodes hunters different visual field depths 
demonstrate point hunters learn expert hunter moves prey shortest path 
shows significant improvement number trials average steps training cumulative independent policy episode peer policy averaging episode expert summary 
novice hunters episodes received expert hunter see bottom curve 
note expert hunter just hunter learned hunting skills 
result demonstrates benefit learning cooperative society novices learn quickly experts examples lin whitehead 
summarizes experimental results case study 
generally speaking early phase training cooperative learning outperforms independent learning learning expert outperforms 
differences performance statistically significant tests 
different ways cooperation excluding learning expert conclusive evidence performs better 
terms average information communicated number participating agents limited exchanging episodes comparable policy 
exchanging policy plausible size policy small proper frequency policy exchanging determined 
case joint tasks previous case studies hunter capture prey 
study joint tasks prey captured hunters occupy cell prey prey 
hunters cooperate passively observing actively sharing sensations locations 
demonstrate cooperative agents learn perform joint task significantly better independent agents start slowly 
number trials average steps training trails independent passively observing mutual scouting typical runs prey hunter joint task 
assume hunters visual field depth similar visual field depth 
consider prey joint task 
independent hunters task hunter tended learn approach prey directly 
hunters approached prey succeeded received rewards 
chased different prey failed penalized 
training continued performance noticeably level average steps capture prey see top curve 
problem independent hunters ignore 
distinguish situation hunter nearby far away 
hunter sense hunter cooperative behavior emerge greedy learning hunters 
address problem extended sensation hunter pairs prey prey ptn ptn prey prey relative location visual field depth prey hunter ptn ptn partner hunter 
note state space increased exponentially terms number agents 
large state space means state exploration hunter slower learning 
starting slowly passively observing hunters began overtake independent hunters soon trials eventually reduced average number steps see middle curve 
hunters cooperate passively observing addition prey 
encouraging results case study proceeded hunters actively share sensory information 
number trials average steps training trails independent passively observing mutual scouting typical runs prey hunter joint task 
means state space enlarged increase dimension state representation 
enlargement initial learning slower passively observing hunters 
mutual scouting hunters soon outperformed agents trials settled average steps training see bottom curve 
average number steps trial test independent passively observing mutual scouting hunters respectively 
people may wonder happen prey joint task 
independent hunters hunters just learn approach prey directly 
case 
knowing partner hunter learn better approach herding patterns 
shows typical runs types hunters prey 
see independent agents passively observing agents mutual scouting agents settled average steps training respectively 
difficult analyze hunters specific approach patterns fact cooperative hunters outperformed independent hunters steps trial suggests existence patterns 
demonstrates reinforcement learning agents learn cooperative behavior simulated social environment 
results simulated prey hunter tasks believe applied cooperation autonomous learning agents general 
identifies ways agent cooperation communicating instantaneous information episodic experience learned knowledge 
specifically cooperative reinforcement learning agents learn faster converge sooner independent agents sharing learned policies solution episodes 
cooperative agents broaden sensation mutual scouting handle joint tasks sensing partners 
hand shows extra sensory information interfere learning sharing knowledge episodes comes communication cost takes larger state space learn cooperative behavior joint tasks 
tradeoffs taken consideration autonomous cooperative learning agents 
research raises important issues multiagent reinforcement learning 
sensation selective size state space increase exponentially terms number involved agents 
heuristic hunter pays attention nearest prey hunter 
selective sensation strategies learned 
second related issue needs generalization techniques reduce state space improve performance complex noisy tasks 
third learning opportunities hard come nontrivial cooperative behavior 
prey smart know escape take long time hunters get learning experience 
learning focused learning teacher 
fourth information exchanging agents incurs communication costs 
agents learn communicate learning task gets complicated content communication instantaneous information episodic experience learned knowledge 
fifth cooperative methods need explored 
example hunters share action intentions avoid collision share rewards sustain hunger 
homogeneous agents learn job division specialize differently 
heterogeneous agents scouting agents vs blind hunting agents learn cooperate 
directions 
acknowledgments am grateful rich sutton steve whitehead chris matheus useful discussions careful comments 
goyal support research 
bellman 

dynamic programming 
princeton university press princeton nj 
chan stolfo 

parallel distributed learning meta learning proceedings aaai workshop knowledge discovery databases appear 
durfee 

coordination distributed problem solvers kluwer academic publishers boston 
gasser huhns 

distributed artificial intelligence eds 
pitman london 
lin 

programming robots reinforcement learning teaching 
proceedings aaai 
pp 

mahadevan 

automatic programming behavior robots reinforcement learning 
proceedings aaai 
pp 
pope meyer 

distributing planning process dynamic environment 
proceedings th international workshop distributed ai glen arbor mi 
shaw sikora 

distributed problem solving approach rule induction learning distributed artificial intelligence systems 
technical report cmu ri tr robotics institute carnegie mellon university 
sian 

extending learning multiple agents issues model multi agent machine learning 
kodratoff ed machine learning ewsl 
springer verlag pp 

silver iba bradford 

framework multi paradigmatic learning 
proceedings seventh international conference machine learning 
austin texas 
tan 

cost sensitive reinforcement learning adaptive classification control 
proceedings aaai 
pp 

tan 

integrating agentoriented programming planning cooperative problem solving 
proceedings aaai workshop cooperation heterogeneous intelligent agents san jose ca watkins 

learning delayed rewards 
ph thesis cambridge university psychology department 
watkins dayan 
technical note learning 
machine learning kluwer academic publishers 
whitehead 

complexity analysis cooperative mechanisms reinforcement learning 
proceedings aaai 
pp 

