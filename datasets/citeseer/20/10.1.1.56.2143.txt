class constructible neural networks alois heinz institut fur informatik der albert universitat am freiburg germany mail heinz informatik uni freiburg de phone propose new class artificial neural networks regression tasks construction algorithm 
networks different isomorphic layouts 
tree structured layout built construction phase accelerated serial evaluation network 
layer layout derived tree structured layout simple mapping 
implementation network parallel hardware 
network construction algorithm combination ideas statistics neural network theory parallelized easily 
learning represent function set examples considered problems neural network theory 
multi layered feed forward neural networks able approximate continuous function arbitrary degree accuracy lot different training methods known variants gradient descent algorithms problem remains choose right network type size adequate learning algorithm parameters :10.1.1.118.8075
algorithm time may return vague results 
called constructive methods proposed able develop suitable network efficient purpose algorithms 
iteratively add neurons adapt parameters network performance sufficiently 
algorithms applicable classification tasks generate large deep networks take steps evaluate parallel machines 
describe class artificial neural networks able represent continuous functions ir ir efficiently constructed 
networks described different isomorphic views layouts 
construction phase tree structured layout iteratively augmented network performance sufficiently 
procedure parallelized easily 
evaluation recall phase parallel hardware layer layout 
serial evaluation accelerated orders magnitude tree structured layout 
network construction algorithm combines ideas statistics optimize local enhancements network topology paradigms neural network theory minimize global performance error 
certain modifications algorithm lead networks different interesting properties 
tree structured layout section describe tree structured layout neural network derive formula describes network evaluations 
input layer network consists input neurons identity transfer functions inputs tree structured layout example network left transformation layer layout right 
ovals represent set input neurons 
shaded circle represents root output neuron left design big circle depicts summation output neuron right design 
circles decision neurons 
boxes labeled neurons boxes represent leaf neurons 
special neuron provides constant value 
output layer output neuron linear transfer function considered root tree 
receives exactly input signal neuron connection weight decision neuron connection weight 
network may number decision neurons 
decision neuron input connections connections weights input neurons connection weight gammac neuron connections input values left right sub trees connections weight receives input decision neuron weighted connected neuron 
weights form weight vector demand weight vectors normalized length equal 
decision neuron equipped internal parameter non negative radius ir 
output defined gamma gamma decision function defined oe delta gamma 
oe may denote usual logistic transfer function oe gammax continuous non decreasing function properties lim gamma oe lim oe gamma oe oe gammax 
case treated lim oe delta gamma ffl delta gamma ffl step function 
increase efficiency serial evaluation algorithms define oe constant outside limited interval oe ffl jxj gamma jxj 
derivative oe non zero interval jxj gamma jxj 
connection leading neuron left right sub tree input decision output neuron called leaf weight denoted decision neuron directly indirectly receives signal leaf called ancestor indicator defined gamma belongs left right sub tree respectively undefined 
evaluation network input vector described equation leafs ancestors gamma leafs defined ancestors gamma property leafs ir 
network construction consider problem set training vectors associated value ir 
task construct tree structured neural network small possible represents function widely coincides set generalizes inputs distribution problem solved iteratively 
network decision neurons step leaf replaced decision neuron new leafs certain condition satisfied 
network learned training set perfectly leafs leafs equation fulfilled demanding leaf gamma obtained minimizing errors local gamma 
minimal 
network construction process leaf connected root optimal weight eq 
mean values 
optimal weight leaf connected decision neuron eq 
weighted mean fuzzy sub set xg respect grade membership eq 

value serve approximation demand eq 
strong general 
step construction algorithm try minimize remaining global error en gamma 
leaf responsible certain local error rate global error en gamma 
eq 
easily show global error equals sum error rates en leafs en 
local error rate leaf vanish replacing sub tree sufficient height provided contains contradictions vectors 
leaf highest local error rate may candidate replaced 
leaf replaced new decision neuron new leafs parameters chosen order minimize en en lr lr difficult local optimization problem global optimization problem difficult local change affects local error rates network parameters re adapted 
offer simple heuristic procedure parts escape dilemma part sets initial parameter values 
parameters chosen randomly improved parts 
intelligent choice easily efficiently initial decision split set near optimal disjoint sub sets lr hyperplane perpendicular coordinate axes 
exactly component 
index initial determined fold scan time vector changes side hyperplane sum errors local eq 
evaluated lr gamma gamma lr lr lr computed eq 
constant 
search needs jx time provided sorted respect dimensions preprocessing step network construction phase 
second part improves local parameter settings optimizing respect errors local eq 
help neural net training algorithm 
hyperplane fuzzy sets lr turned moved sets overlap grows larger 
important constant number adaptations limit time needed jx 
third part re adapts network parameters order minimize global error eq 

number epochs limited constant jxj jn time serial algorithm special data structures network properties 
neural training algorithm parts variant quickprop algorithm uses small additive real derivative 
helps speed allow learning real derivatives zero appears function eq 

training algorithm uses step size parameter total length gradient vector controls learning parameters 
step size altered changes error values 
parts change information local leaf performed parallel leafs stage 
part uses changes information network pipelined version training vector takes constant time implemented parallel hardware 
serial versions training evaluation algorithm tree data structure top lazy evaluation provides exponential speed respect tree height exploiting special properties oe eq 

network enhancements continued number decision neurons remaining global error reaches limit estimated generalization error begins increase 
layer layout properties ab exp ln ln eq 
re formulated leafs exp ancestors ln gamma leads mapping tree structured layout layer architecture shown 
input layer consists input neurons neuron 
hidden layer contains copies decision neuron negative positive decision gamma 
input connection weights determined parameters associated activation function neurons ln oe 
neuron second hidden layer represents leaf receives input negative positive copy ancestor gamma 
activation function exp gamma activation 
single output neuron linear activation function connected leaf neurons connection weight described transformation yields standard layer neural network takes constant number steps evaluate parallel machine sufficient size 
implemented evaluated versions described network construction algorithm differ certain details way initial fuzzy splits decision neurons determined decisions constrained adaptation process 
decision boundaries constrained perpendicular coordinate axes networks learn intended function grow larger general 
interesting advantage restriction resulting network interpreted fuzzy logic expert system 
decision radii restricted zero network regarded oblique decision tree 
fastest convergence observed restrictions imposed form decisions 
network construction algorithm described ideal combination local enhancement steps motivated statistical considerations global performance optimizations supported neural network theory 
critical point different sights general problem alternate 
sight correct errors neglected sight decisions certain stage modified stage 
example minimum function arguments unit square learned optimal direction weight vector sigma decision neuron 
neuron fed decision neurons optimal weight vector direction sigma gamma means turned 
construction algorithm provides chance weight vectors parameters change stages construction probabilities large changes may decrease sub trees grow larger 
acknowledgments author anonymous referees comments helped improve quality 
breiman friedman olshen stone 
classification regression trees 
wadsworth belmont california 
brent 
fast training algorithms multilayer neural nets 
ieee transactions neural networks 
fahlman 
empirical study learning speed back propagation networks 
technical report cmu cs computer science department carnegie mellon university pittsburgh june 
fahlman lebiere 
cascade correlation learning architecture 
touretzky editor advances neural information processing systems pages san mateo california 
morgan kaufman publishers fang 
network complexity learning efficiency constructive learning algorithms 
proceedings ieee international conference neural networks volume pages orlando florida june 
frean 
algorithm method constructing training feed forward neural networks 
neural computation 
heinz 
efficient neural net ff fi evaluators 
morasso editors proceedings international conference artificial neural networks pages sorrento italy may 
springer verlag 
heinz 
fast bounded smooth regression lazy neural trees 
proceedings ieee international conference neural networks volume iii pages orlando florida june 
heinz 
adaptive fuzzy neural trees 
liu editors advances intelligent data analysis proceedings ida symposium volume pages baden baden germany aug 
international institute advanced studies systems research cybernetics 
isbn 
heinz 
pipelined neural tree learning error forward propagation 
proceedings ieee international conference neural networks perth australia dec 
appear 
hornick stinchcombe 
multilayer feedforward networks universal approximators 
neural networks 
jordan jacobs 
hierarchical mixtures experts em algorithm 
neural computation 

nadal 
learning feedforward layered networks tiling algorithm 
journal physics math 
gen 
quinlan 
programs machine learning 
machine learning 
morgan kaufmann publishers san mateo california 
rumelhart hinton williams 
learning internal representations error propagation 
rumelhart mcclelland editors parallel distributed processing explorations cognition vol 
chapter 
mit press ma 
sethi 
entropy nets decision trees neural networks 
proceedings ieee oct 
