hive operating system fault containment shared memory multiprocessors john chapin technical report 
csl tr january research supported darpa contract dabt 
author acknowledges support fannie john hertz foundation 
ii hive operating system fault containment shared memory multiprocessors john chapin technical report 
csl tr january computer systems laboratory departments electrical engineering computer science stanford university william gates computer science building stanford ca pubs stanford edu reliability scalability major concerns designing general purpose operating systems large scale shared memory multiprocessors 
dissertation describes hive operating system novel kernel architecture addresses issues 
hive structured internal distributed system independent kernels called cells 
architecture improves reliability hardware software error damages cell system 
architecture improves scalability kernel resources shared processes running different cells 
hive prototype complete implementation unix svr targeted run stanford flash multiprocessor 
research described dissertation primary contributions demonstrates distributed system mechanisms provide fault containment inside shared memory multiprocessor provides specification set hardware features implemented stanford flash sufficient support fault containment demonstrates take advantage sharedmemory hardware cell boundaries application kernel levels preserving fault containment 
dissertation analyzes architectural performance tradeoffs multicellular kernels 
fault injection experiments conducted simos machine simulator demonstrate reliability hive prototype 
studies general purpose scientific workloads illustrate performance tradeoffs multicellular kernel architecture 
key words phrases hive stanford flash unix svr multicellular architecture operating system reliability operating system scalability fault containment fault tolerance shared memory multiprocessors cc numa multiprocessors distributed systems 
ii copyright john chapin iii parents 
iv acknowledgments building operating system immense task 
challenge daunting system architecture machine run developed time 
hive operational today hard people 
advisor mendel rosenblum inspiration guidance 
faculty members flash project anoop gupta mark horowitz john hennessy contributed valuable advice system developed 
committee consisting mendel rosenblum anoop gupta mary baker spent substantial time commenting dissertation greatly improved quality 
acknowledge huge design implementation effort members hive team 
dan scott devine lahiri collectively spent man years system 
compared industrial operating system development efforts small team level functionality reached hive superb job done involved 
experiments hive possible simos machine simulator 
steve herrod witchel ed bugnion robert bosch scott devine ben verghese developed simulator willing add unique features required hive 
flash hardware team responded opportunity add new fault containment functionality design 
help particular joel baxter john built custom features support hive jeff dave mark heinrich kapadia modified designs needed 
fannie john hertz foundation graduate research fellowship supported stanford arpa funding flash project contract dabt silicon graphics development machines irix source code hive 
special pinkerton constant companion hiking buddy chef friendship love ph studies parents paul susan chapin love support decades education detailed proofreading significantly helped dissertation 
preface hive joint project 
ph students visitor stanford contributed significantly operating system dan process management rpc subsystem anonymous memory failure recovery 
scott devine virtual memory system heap corruption experiments 
lahiri file system 
reboot process low level hardware management 
memory load balancing 
primary contribution system architecture including problem definition design multicellular architecture choice error model decisions features put hardware 
designed firewall short interprocessor send mechanism memory fault model hardware software interfaces 
implemented initial prototypes multicellular boot process remote procedure call subsystem anonymous memory manager final versions intercell address space duplication page fault handling cells 
nature contribution dissertation focuses system design reliability characteristics 
detailed descriptions evaluations various subsystems appear team members dissertations 
vi vii contents overview stanford flash multiprocessor 
motivation new operating system 
multicellular kernel architecture 
implementing multicellular kernel 
definition fault containment 
usefulness fault containment 
success conditions 
implementation hive 
experimental evaluation 
terminology 
outline dissertation 
smp os limitations reliability issues 
system failures practice 
failure rates large machines 
reliability requirements 
improving reliability 
performance issues 
experimental setup 
behavior 
detailed observations 
improving performance 
hive architecture error model 
fault containment architecture 
application level 
operating system level 
memory system level 
network level 
data link level 
physical level 
summary fault containment stack 
operating system software architecture 
distributed system 
cell isolation 
wild write defense 
error recovery 
resource sharing policies 
resource sharing mechanisms 
cell size tradeoffs 
summary features 
viii error recovery lower levels 
hive prototype implementation status 
multiple cells 
rpc subsystem 
hardware support 
rpc architecture 
evaluation 
file system 
summary 
experimental setup simos simulation environment 
performance experiments 
reliability experiments 
impact simulation 
limited system size 
impact performance experiments 
impact reliability experiments 
fault containment safe remote reads 
careful protocol 
publisher lock 
evaluation 
wild write defense 
firewall management 
preemptive discard 
failure detection 
summary possible improvements 
memory fault model 
specification 
design discussion 
limitations flash implementation 
evaluation 
failure recovery 
fault error injection experiments 
heap corruption 
node failures 
instruction corruption 
effectiveness wild write defense 
summary 
memory sharing application level memory sharing 
irix page cache design 
logical level sharing 
performance logical level sharing 
physical level sharing 
remote firewall management 
logical physical interactions 
memory sharing fault containment 
summary application level memory sharing 
kernel level memory sharing 
ix cell public area 
remote process creation 
anonymous memory manager 
summary kernel level memory sharing 
system performance performance characterization 
increasing kernel 
decreasing kernel overheads 
increasing user time 
decreasing idle time 
evaluation 
architectural evaluation hardware support 
additional operating system functionality 
comparison smp kernels 
limitations architecture 
open questions 
related improving smp kernels 
multiprocessor operating systems 
distributed systems 
error model reliability prediction 
index list tables table 
data cache hotspots irix seconds execution dash 
table 
fault containment stack 
table 
system model performance experiments 
table 
time completion seconds performance experiments 
table 
system model reliability experiments 
table 
percent kernel time spent level cache misses 
table 
recovery subsystem design 
table 
heap corruption experiments 
table 
instruction corruption experiments 
table 
causes failures 
table 
latency error cell enters recovery experiments 
table 
virtual memory system interface memory sharing 
table 
components remote quick fault latency 
table 
wallclock time completion seconds 
table 
total useful kernel time seconds 
table 
total non idle kernel time seconds 
table 
kernel time stalled cache misses percent useful kernel time 
table 
kernel time spinning locks percent useful kernel time 
table 
kernel time waiting rpcs percent useful kernel time 
table 
total application time seconds 
table 
frequent kernel operations cell hive running raytrace 
table 
total idle time seconds 
xi list figures 
stanford flash multiprocessor 

go wrong smp kernel 

partition multiprocessor hive cells 

architecture stanford dash multiprocessor 

operating system view firewall 

implementation spanning tasks 

intercell optimization wax 

types memory sharing cell boundaries 

lightweight process migration spanning tasks 

flash remap region 

firewall support magic 

network packet loss violates memory fault model 

control flow recovery process 

decision tree instruction corruption experiments 

application level memory sharing cell boundaries 

logical level sharing data pages 

physical level sharing page frames 

anonymous memory manager data structures 

time completion workloads 

execution trace ocean 
xii overview chapter overview dissertation describes hive operating system designed improve reliability scalability large general purpose shared memory multiprocessors 
hive targeted run stanford flash multiprocessor machine currently built stanford 
hive prototype implementation novel kernel architecture called multicellular kernel 
running single shared memory program manages machine resources multicellular kernel partitions machine runs internal distributed system multiple kernels called cells 
architecture improves reliability system compared previous multiprocessor kernel architectures hardware software error cell crash machine 
improves scalability application requests serviced cell application running reducing kernel synchronization delays memory system bisection bandwidth requirements 
previous multicellular kernels focused scalability benefits 
research described dissertation primary contributions demonstration distributed system techniques provide fault containment inside shared memory multiprocessor despite possibility wild writes due software errors specification set hardware features flash generalizable multiprocessors sufficient support hardware software fault containment demonstration cells take advantage shared memory hardware cell boundaries application kernel level preserving fault containment 
chapter introduces system experimental evaluation done dissertation 
start description flash machine design stimulated development hive 
stanford flash multiprocessor flash shared memory multiprocessor designed scale thousands processors 
reach gigantic size flash distributes main memory nodes machine uses scalable interconnect network shared bus 
motivation new operating system flash representative cc numa multiprocessor cache coherent non uniform memory access time 
name arises bus machines memory accessible uniform time physically distributed memory cc numa machine addresses slower faster access perspective processor 
cc numa multiprocessors flash uses directory cache coherence protocol koh 
core flash design programmable protocol processor node implements complex algorithms required directory cache coherence 
protocol processor chip called magic memory general interconnect controller 
executes microcode stores data structures main memory node caches reduce instruction data access latency 
magic programmable essentially unlimited code data storage flash flexible previous multiprocessors memory system protocols implemented dedicated logic 
memory system behavior changed new features added recompiling protocol microcode rebooting machine 
flexibility offers opportunity design new hardware operating system software features provide novel system functionality 
motivation new operating system developers flash scalable cc numa machines including manufactured silicon graphics sil sequent loc data general dat hp intend general purpose multiprocessors 
general purpose machine runs standard commercial engineering applications efficiently provides net nd level cache nd level cache dram mp magic processor memory protocol processor 
stanford flash multiprocessor 
magic overview services expected commercial systems multiprogramming networking security system administration 
multiprocessors ways general purpose computers 
supercomputers environment design goals efficient multiprogramming compatibility standard applications sacrificed order improve raw performance 
multiprocessors implement high availability fault tolerant systems multiple processors provide replication order ensure continuous operation 
uses important developers scalable multiprocessors target general purpose market notably managing commercial databases larger supercomputers fault tolerant systems 
unfortunately current general purpose operating systems significant reliability scalability problems run large multiprocessors 
due structure symmetric multiprocessing smp operating systems processors share single copy kernel data structures 
monolithic kernels unix vms microkernels mach chorus windows nt smp kernels definition 
architecture causes reliability problems smp operating system rebooted recover errors 
build intuition observation consider effects error damages core kernel data structure run queue 
case memory module fails causing run queue data inaccessible new processes scheduled machine 
case processor acquired run queue lock ensure mutual exclusion failed releasing lock processor trying access run queue spin forever waiting lock released 
case processor uses mem mem mem mem mem mem proc proc proc proc proc proc 
go wrong smp kernel 
memory module failure run queue lost 
processor failure run queue lock released 
wild write run queue corrupted 
multicellular kernel architecture uninitialized pointer store instruction corrupting data run queue operating system probably fail processor access run queue finds inconsistent state 
variety possible errors smp kernel extremely difficult recover rebooting 
machines grow size error rate increases mean time failure decreases 
size smp operating systems longer reliable general purpose applications 
scalability limited widely shared data structures smp operating system 
improving parallelism smp operating system iterative trial error process identifying fixing bottlenecks 
larger machine sizes bottlenecks subtle false sharing cache lines high conflict rates caused simultaneous access unrelated data structures 
memory system bottlenecks highly workload dependent operating system performs tested developer perform poorly production 
problems cause cost delivering high performance smp operating system increase sharply size multiprocessor limiting size machine economically feasible manufacture 
significant improvements operating system reliability scalability large multiprocessors succeed general purpose computing platforms 
multicellular kernel architecture reliability scalability problems addressed restructuring operating system internal distributed system cells 
cell smp kernel 
independently manages portion processors memory devices machine supports applications running portion machine shares resources cells needed performance 
multicellular kernel architecture improves reliability cells defend failures 
error may cause cells fail terminating applications running cells rest machine unaffected 
kernel software architectures assume operating system correct multicellular kernel architecture provides reliability respect operating system software errors important failures observed field caused software errors gra chb 
multicellular architecture improves scalability kernel data structures shared processes running different cells 
increasing number cells systematically improves overview parallelism operating system increases locality kernel memory accesses reduces hardware bottlenecks cc numa multiprocessor 
scalability problems remain involve explicit communication cells easier control implicit communication shared memory occurs smp operating system 
implementing multicellular kernel multicellular architecture creates implementation challenges arise existing multiprocessor operating systems cell isolation effects errors confined cell occur 
additionally system survive hardware errors hardware provide features support restoring memory system interconnect functional state hardware error 
resource sharing processors memory system resources shared flexibly cell boundaries preserve execution efficiency justifies investing multiprocessor 
cell boundaries add high performance overheads resources shared 
single system image cells cooperate standard smp os interface applications users 
problems faced implementing multicellular kernel arise single distributed systems sprite ocd locus pow 
presence shared memory hardware cells dramatically higher bandwidths lower increasing physical addresses cell trap vectors cell code cell internal data paged memory cell cell cell cell physical view memory layout 
partition multiprocessor hive cells 
definition fault containment latencies multiprocessor interconnect solutions distributed systems inappropriate multicellular kernel 
particular problem affects system design possibility wild writes stores incorrect addresses caused kernel software errors 
novel software mechanisms hive novel hardware mechanisms flash implement cell isolation resource sharing despite possibility wild writes 
definition fault containment application visible reliability model results multicellular design called fault containment 
reliability model familiar previous distributed systems 
informally application may fail error occurs part system application 
useful define fault containment precisely system provides fault containment probability application fail proportional amount resources application total amount resources system 
precise definition better informal reasons 
multiprocessor dynamic fine grained resource sharing part system application frequently poorly defined 
second operating system hardware may need application vulnerable failure part system improve performance simplify implementation 
precise definition gives metric allows comparison different systems design features attempt provide fault containment 
usefulness fault containment fault containment strategy differs significantly traditional fault tolerance model system attempts guarantee applications fail error occurs 
fault containment weaker fault tolerance applications fail error occurs appears lower overheads avoids replication 
question weaker model useful 
fault containment multiprocessor provides reliability benefits workload multiple processes processes continue doing useful fail 
general overview purpose workloads appear property 
examples include engineering computeraided design graphics virtual reality software development general interactive 
fault containment improve reliability large processes resources machine 
applications probably justified purchase machine place database multimedia web servers large engineering scientific simulations 
fortunately programs focus significant software engineering investment 
reasonable assume modified improve reliability systems fault containment 
ways large applications cases decomposed independent smaller tasks benefit fault containment 
examples applications include decision support data mining initial long running query split smaller independent subqueries 
large applications multimedia web servers naturally split multiple processes services client group clients 
applications require high availability restructured process pairs bar sis 
system fault containment support applications exposes sufficient control resource allocation processes pair avoid common points failure 
traditionally applications run fault tolerant systems cases human safety risk general purpose multiprocessor fault containment may cost effective solution 
applications require high availability checkpointing provide roll back recovery failures 
batch processes engineering scientific simulation graphics rendering tend files directly humans graphical output computation steering 
checkpointing applications straightforward various user level checkpointing libraries written require little effort exploit lis 
operating system support useful checkpointing lnp sis easily integrated multicellular kernel 
success conditions fault containment appears useful wide range general purpose applications sufficient justify immense investment required adopt new kernel architecture 
multicellular kernel succeed general purpose operating system achieve goals implementation hive fault containment kernel demonstrate substantial improvements reliability compared smp kernels 
competitive performance kernel add substantial performance overheads compared smp kernels supporting workloads medium sized systems processors dominate market foreseeable 
scalability kernel provide excellent performance scalability operating workloads databases system grows large sizes processors 
binary compatibility kernel execute unmodified legacy applications correctly efficiently 
implementation hive hive prototype built evaluate multicellular kernel capable achieving goals 
intended demonstrate complete implementation commercial multicellular kernel 
particular single system image incomplete file system primitive resource sharing mechanisms 
current prototype includes features required initial evaluation multicellular kernel architecture complete cell isolation failure recovery subsystem virtual memory system capable kinds memory sharing cell boundaries tuned remote procedure call subsystem distributed process management sufficient remote process creation distributed process groups 
innovative parts implementation take advantage defend problems caused shared memory hardware 
wild write defense experiments shared memory cells particularly noteworthy 
interesting observation shared memory useful kernel level communication mechanism cells expected section 
lessons learned prototype relevant possible potential commercial adopters multicellular architecture hive binary compatible widely commercial smp operating system irix silicon graphics version unix system release 
hive implemented extensive modification irix code base 
overview experimental evaluation flash machine operational experiments reported dissertation simos machine simulator wir 
simos offers choice processor memory system models trade speed accuracy simulation 
experiments reliability system high speed mode simos timing experiments slower mode accurate coupled cycle accurate simulation flash memory system 
reliability experiments reliability experiments series fault error injection studies 
studies include corruption data kernel heap corruption kernel instructions failure flash nodes 
workload reliability experiments parallel attractive properties independent subprocesses easily checked output 
experiment considered succeed files produced parallel corrupted cell fails fault error injected 
hive shows data corruption experiments limits effects faults errors cell injected time 
performance experiments performance experiments microbenchmarks highlevel workloads pmake raytrace ocean 
pmake parallel stresses system ways characteristic general purpose 
raytrace ocean parallel scientific applications splash benchmark suite 
simulation limits performance experiments small system sizes 
largest system studied processors megabytes memory 
combined limited set features implemented prototype difficult draw performance multicellular kernels large machines 
prototype show interesting performance trends tradeoffs suggest multicellular design promising larger systems 
processor system cell hive configuration shows slowdown pmake slowdown raytrace performance improvement ocean compared irix baseline 
workloads amount time spent executing operating system highest running cells decreases system increases cells due reductions memory system stall time kernel lock contention 
ocean raytrace suffer effect increases time spent spinning locks user level indicating important spread kernel workload evenly multiple cells supporting parallel applications synchronize frequently 
terminology terminology dissertation follows standard terminology describe dependable computing systems joh 
fault latent problem mistake line code short circuit chip 
error result activating fault executing incorrect code conditions cause give wrong result reading incorrect value shorted line 
failure occurs error causes externally observable behavior device software system deviate specification 
failure fail fast failed component produces incorrect output halting produces output corrupt components receiving output detect failure immediately 
case multicellular kernel software hardware error may cause cell fail system fail successfully confines effects error directly affected cells 
error leads failure cells complete system failure said cause failure 
violation standard terminology 
term fault containment dissertation properly replaced error containment forward error recovery 
term fault containment chosen analogy widely term fault tolerance similarly values clarity conciseness precision 
outline dissertation rest dissertation structured follows chapter provides motivation developing hive analyzing limitations smp operating systems 
chapter describes fault containment architecture system software architecture hive 
chapters describe implementation hive prototype experimental setup dissertation 
chapters focus implementation fault containment resource sharing prototype 
chapter measures performance prototype 
chapter discusses learned tradeoffs multicellular architecture 
chapters conclude dissertation survey related summary main results research 
smp os limitations chapter smp os limitations previous chapter gave overview motivation design system 
turn detailed investigation reasoning design hive 
particular provide evidence current smp operating systems insufficient large multiprocessors 
smp operating systems successful design small scale general purpose multiprocessors 
commonly accepted difficult scale current high machines tens processors 
new problems arise larger multiprocessors scaling difficult 
part chapter focuses reliability issues second studies performance issues 
reliability issues considering reliability larger machines best start data reliability current machines 
unfortunately information failure rate multiprocessors running current smp operating systems available 
published studies combine uniprocessors multiprocessors give hints magnitude rate 
system failures practice tai uses automatically maintained error logs analyze failures vaxcluster systems running vms 
machine years data shows mean time failures mtbf machine days 
rate high due immaturity operating system start data collection period 
mtbf due hardware errors machine days mtbf due software errors improved days year years years 
data collected tightly coupled separate machines standard lan software error rate probably higher observed independent machines 
cbr analyzes problems reported ibm service organization releases large ibm operating system 
data covers software errors hardware errors includes errors lead system failures 
release rate months errors reliability issues year release release rate years errors similar point life cycle 
measurement period rates years years errors 
assuming percent operating system errors lead system failures measured vms study ibm systems year release mtbf year release years release 
failure rate low excludes hardware errors 
studies support hypothesis general purpose machines require mtbf due hardware software faults month year 
case operating system released bottom range manufacturer invested sufficiently improving reach somewhat exceed top range 
studies failure rates field systems examined differ smp operating systems currently commercial multiprocessor vendors investigated studies 
provides survey literature 
failure rates large machines scaling larger systems cause mtbf smp operating systems decrease significantly 
intuitively obvious respect hardware errors larger machines errors error machine causes operating system crash 
note hardware error rate scale directly machine size improved integration hardware generation reduces number components required build larger machines 
obviously scaling size machine increases rate software errors smp operating system 
efficient performance larger machine parallelism smp operating system increased finer grained locking data structures partitioned processors similar techniques 
changes create increased risk software errors operation parallelized code hardest type code analyze test system development 
observation parallelized code common wisdom programmers supported field data 
example study customer problem reports tandem guardian operating system tested thoroughly manufacturer intended high availability applications race conditions timing problems prevalent type software faults crashed systems field 
true number faults code number failures resulting faults lei 
smp os limitations general purpose machine grows size operating system require increased internal parallelism applications run time 
leads increased number dynamic interactions processes increasing potential stimulating latent parallelism related faults operating system code 
relationship system workload software error rate documented cas moa 
put increased rate hardware errors increased parallel constructs operating system code increased dynamic parallelism system suggest mtbf large multiprocessors substantially lower current multiprocessors current smp operating systems 
reliability requirements time technological factors push mtbf downwards market pressures require large multiprocessors provide substantially higher mtbf current small scale systems 
reasons change applications primary applications justify purchase expensive machines transaction processing decision support databases currently run mainframes mtbf expectation higher current small scale systems 
web multimedia servers may important applications large multiprocessors high availability requirements 
greater impact failure sheer size large multiprocessors give failure greater impact current multiprocessors 
consider organization uses large multiprocessor centralize computing resources machine current small scale multiprocessor 
failure interrupt service done people 
machine take longer return steady state performance reboot require refill larger main memory restart larger number processes recover cached knowledge processes maintain workloads state world 
change applications greater impact failure imply cost failure machines substantially higher current small multiprocessors 
users require higher reliability find machines 
mtbf similar current mainframes necessary open mainframe market multiprocessors 
mtbf levels similar current small multiprocessors may leave large multiprocessors sufficient market justify development 
performance issues improving reliability may case improvement necessary better engineering testing techniques operating system 
software error rate dominates hardware error rate current systems gra chb machines times larger largest current multiprocessor 
reducing rate software errors may provide sufficient reliability improvements eliminate need radical changes 
approach require high engineering investments due difficulty finding parallelism related software faults 
larger system sizes hardware error rate significant significant reliability improvements required suggested previous section smp operating systems appear provide acceptable solution 
performance issues reliability important performance primary motivation users invest large multiprocessors 
large multiprocessors achieve near linear scalability system throughput attract users choose clusters smaller machines achieve substantial price performance advantages compared existing mainframes attract users 
larger machines operating systems encounter primary performance problems traditionally ignored smaller machines 
related memory system 
communication latency machines higher relative processor speed small machines algorithm suffers frequent cache misses potential performance bottleneck 
locality memory access important reduce access latency minimize interconnect contention widely shared data structures slower localized data structures frequency cache misses remains constant 
section reports data memory system costs problems smp operating systems face reducing 
data comes study smp operating system irix running processor cc numa machine stanford dash 
irix parallelized run efficiently large bus multiprocessors sgi challenge machines support mips processors memory system behavior skewed bad synchronization behavior system size 
giving complete details study highlight selected points 
interested readers may refer chr details data 
smp os limitations experimental setup detailed trace data collected dash configuration clusters 
cluster slightly modified silicon graphics power station bus multiprocessor mhz mips processors 
processors relatively limited caches kilobyte external data cache backing smaller chip data cache kilobyte instruction cache direct mapped byte lines 
cache fills local memory stall processor minimum processor clock cycles 
remote memory requests take processor clock cycles data clean memory processor clock cycles fetched cache cluster 
remote cache misses satisfied latency local access hit remote access cache kilobyte direct mapped cache byte lines 
workload study features characteristic software development engineering environment 
workload contains way parallel copies microbenchmark suite copies moderately large engineering analysis program 
dash cluster includes hardware monitor traces bus activity affecting timing machine 
disk capacity dash limits total trace collected seconds execution 
data reported comes multiple second samples taken different times separate workload runs 
remote access monitor hw directory main processor st level cache nd level dcache memory interconnection network mesh remote access monitor hw directory cache 
architecture stanford dash multiprocessor 
performance issues behavior workload studied system spends average non idle time running applications non idle time operating system 
components non idle operating system time non idle time executing useful instructions spinning locks stalled cache misses 
words smp operating system spends fifths time stalled cache misses 
roughly half cache time due instruction cache misses higher dash systems small direct mapped instruction cache 
excluding instruction cache misses irix spends times time stalled data cache misses non idle time executing useful instructions 
cache simulations trace data dash show substantial reduction data cache misses megabyte way set associative data cache 
indicates data cache stall time caused communication misses 
clearly relatively modest system size processors smp operating system suffers significant memory system performance bottleneck 
bottleneck significant flash large multiprocessors higher relative remote cache latency dash 
detailed observations half operating system data cache time comes memory block transfer routines page copy page zero similar routines 
stall time routines primarily spent waiting cache misses remote memory 
cause problem smp kernel architecture lack numa aware page allocation processor scheduling irix combined failure take advantage hardware prefetch support provided dash 
proper policies hardware mechanisms reduce block transfer latencies improve memory access locality 
half data cache time dominated interprocessor communication hotspots caused smp kernel architecture 
table shows data structures highest cache times leaving large data structures process table high stall times come large size 
types data structures table functions inherently hot accidentally hot 
structure inherently hot function requires written frequently read written multiple processors 
category table shows fast clock control counters smp os limitations frequently acquired reader writer lock head links free list run queue head pointer 
reducing cost accessing structures requires algorithmic changes example changing globally shared run queue multiple processor cluster run queues 
structure accidentally hot cache time due programmer error false sharing 
example surprising see high cache times memory lock written boot time changed 
read frequently happens cache line frequently written counter number pages available users 
note workload studied innocuous variable accumulated stall time head pointer global run queue 
improving performance reducing communication hotspots memory system performance bottleneck require cycle iterative improvements smp kernels system grows 
particular uncontrolled interprocessor communication inherent smp operating system architecture means increase system size change workload stress new parts system cause new hot spots limit performance 
familiar example architectural characteristic synchronization behavior smp kernels 
example irix running processors global run queue lock substantial contention 
developers significant algorithmic changes table 
data cache hotspots irix seconds execution dash 
structure stall time msec description structure fast clock number pages available bucket fast clock acknowledge dummy variable anon shake lock reader writer counters copy tree buffer free list header owner processor rights execute network code head process run queue memory lock pointer hardware spin lock physical memory management structures performance issues eliminate hotspot time irix released 
growing system processors lock memory lock came high contention 
additionally innocuous locks create significant performance problems workloads behave differently ones tested developers 
ocean workload measured chapter provides excellent example problem 
memory system costs smp kernel reduced modifying algorithms reduce communication processors false sharing cause significant performance problems 
dash byte lines hold integers pointers false sharing caused significant hot spots 
systems longer lines flash bytes expect severe performance impacts 
improved measurement linking tools help reduce false sharing increase system size change workload expected stress system different ways bring new false sharing problems 
improving operating system performance continual reengineering requires systematic way increase operating system parallelism reduce global data sharing 
multicellular kernel architecture offers promising approach ensuring processes running separate cells share kernel locks data structures 
multicellular kernel faces problems reducing communication fundamental communication rate number points communication occur lower making easier detect solve scalability problems 
dissertation chapter returns discussion compares complexity scalability multicellular smp kernels light experience gained hive prototype 
hive architecture chapter hive architecture chapter describes multicellular kernel operating system structured internal distributed system cells 
architecture hive covers wider domain just software architecture implements idea 
reliability goal choice error model part system design 
hive operating system interface feature set exported applications important 
hive integral part hardware project design includes novel hardware features appropriate multicellular kernel 
chapter describes architecture parts 
part fault containment stack layering similar networking protocol stack defines expected errors responsibilities level system 
second part software design hive implements layer fault containment stack 
chapter concludes brief description implementation lower layers stack flash 
design features described chapter implemented prototype 
useful system design despite limitations current implementation design drives ongoing research hive flash explains decisions implementing prototype 
chapter describes current implementation 
error model error model set assumptions types errors occur effects system 
error model drives reliability design system design hardware software mechanisms cell isolation failure recovery 
error occurs model behavior system unpredictable 
just system designed improve reliability choosing error model hive requires making tradeoffs reliability performance 
error left model cause system fail probability low relative complexity performance cost defending effects 
fault containment architecture error model described detail section fault containment architecture 
summarize errors assumed descending order probability operating system software errors power cooling failure portion machine link failure interconnect halt individual node 
link failure relatively probable compared expected smaller machine flash multiple cabinets network links stretched vulnerable human error cable 
hive flash carefully avoid assuming error occur time 
multiprocessor tightly coupled system single problem cooling failure cause errors multiple cells time 
worse multiple errors occur slightly different times boards different rates 
software error cell manages corrupt cell cause similar behavior 
time error occur system recovering previous error 
fault containment architecture table summarizes fault containment stack 
bottom layers part flash hive operating system layer 
fault containment implemented top stack bottom hypothesize 
example having cabinets independent power supplies improve reliability applications loss cabinet crashes operating system 
contrast multicellular kernel run hardware support provide reliability benefits reducing impact operating system software errors 
table 
fault containment stack 
layer example design features application needed checkpointing process pairs useful operating system independent cells wild write defense memory system timeouts cache misses recover consistency hardware errors network drain packets reroute failed components data link error correcting codes completion truncated packets physical multiple fans power supplies hot pluggable boards hive architecture sections fault containment stack describe architecture system 
describe errors occur layer fault containment features provides 
application level error model just smp operating systems reliability design system assumes applications potentially erroneous malicious 
fault containment assume types applications workload large number naive applications unaware fault containment properties system small number sophisticated applications implemented fault containment mind 
naive applications standard unix system interface 
applications require operating system automatically provide fault containment 
naive applications large example parallel scientific engineering simulations run threads processors machine 
assume important provide fault containment workload combines large small naive applications 
acceptable case large applications receive automatic fault containment benefits small applications continue protected 
sophisticated applications seek improve reliability user level process pairs mechanisms described section need control set cells failure damage application 
requires operating system export information configuration system cell layout allow applications limit activity automatic resource sharing load balancing mechanisms 
application currently runs cluster machines shared databases web servers run reliably multiprocessor advantage features provided sophisticated applications 
may possible significantly improve performance applications adding shared memory communication regions independent processes 
operating system support allowing sophisticated process survive loss memory resources 
operating system level error model reliability design system assumes software error cell eventually result deadlock panic halt due failure self check partial halt loss ability schedule processes 
cell reaches point may issue wild writes arbitrary addresses send corrupt messages cells 
fault containment architecture key simplification error model cells assumed exhibit byzantine faults lsp 
particular sanity checks cells apply messages data received cells assumed detect incorrect messages 
furthermore cell appears failed correct cell appear failed correct cells subsequently test 
success fault injection experiments described chapter suggests assumptions reasonable 
assumptions free hive performance impact implementation complexity byzantine fault tolerant distributed algorithms lyn 
assumptions strong intermittent faults cause incorrect behavior turns significant problem practice algorithms known enable weakening assumptions implementing full byzantine fault tolerance see chapter 
fault containment operating system responsible detecting software errors cells recovering operating system data structures consistent state software hardware errors rebooting failed cells hardware resources lost 
implements wild write defense mechanisms cell isolation hardware features provided memory system level 
naive applications hive maximizes fault containment minimizing number cells application vulnerable 
provides automatic resource sharing load balancing required achieve execution efficiency justifies investing sharedmemory multiprocessor 
hive provides mechanism called spanning tasks improves fault containment naive applications 
spanning task unix process threads running multiple cells 
process merely uses memory files multiple cells spanning task mechanism 
spanning task mechanism enables system run small cells providing fault containment small naive applications workload contains large naive applications processors machine 
difference processes spanning tasks visible naive applications 
sophisticated applications hive provides straightforward extensions unix system interface give control resource sharing load balancing mechanisms 
extension system interface management essential dependencies process 
cell fails process may terminated immediately 
alternatively may receive error time accesses resource memory page lost failure 
occur process essential dependency cell 
hive architecture hive allows application discover restrictions modify essential set cells 
approach avoids cluttering system interface controls various internal kernel resources normally visible applications 
memory system level memory system level consists magic chip firmware respond cache misses uncached accesses maintain cache coherence provide features interrupts interprocessor message sends 
error model reliability design assumes memory system level fail fast 
potential errors outside error model include returning wrong memory line cache sending cache writeback wrong address memory 
note fail fast assumption violated easily large scale multiprocessor running complicated cache coherence protocol simpler multiprocessors 
flash memory system managed lines microcode running magic 
code extremely tested presumably contains faults 
self checking assertions added microcode convert software errors fail fast node halts assertions performance critical sections code 
microcode error trigger assertion outside error model may lead data corruption total system failure 
fault containment memory system level provides features software hardware fault containment 
important features firewall memory fault model 
firewall firewall supports software fault containment operating system 
page memory firewall gives cell owns page ability control processors system modify page 
processor attempts modify data firewall write permission receives bus error 
firewall protects devices node control registers firewall state returning bus errors disallowed uncached reads discarding disallowed uncached writes 
memory fault model memory fault model supports hardware fault containment operating system 
memory fault model analogous memory consistency model specifies behavior reads writes multiprocessor 
fault model tells operating system designer assumptions algorithms seek recover hardware errors 
important feature memory fault model assumption operating system sets firewall limit write permission fault containment architecture page set cells data page damaged hardware error outside set cells 
firewall straightforward extension standard cache coherence mechanisms 
contrast memory fault model requires design features memory system level lower levels fault containment stack 
features memory system level include detection hardware errors timeouts initiation network level operating system level recovery processes recovery cache coherency protocol consistent state hardware errors 
features lower levels described sections 
network level normal operation network level provides reliable communication service memory system 
error model network level errors assumed occur packet loss delivery packets wrong destination node 
surviving packet loss key part reliability design packets packets flagged errors data link level nontrivial physical level errors lead actual apparent packet loss 
local area wide area networks reliability design assumes congestion lead packet loss 
multiprocessor interconnects flash provides lossless 
operating system view firewall 
processor processor cell cell memory pages write permitted cell 
write permitted corresponding bit set firewall 
bus error corresponding bit clear firewall 
cache misses requesting data exclusive mode firewall bit vectors hive architecture flow control order enable endpoints avoid cost retransmission protocol 
packet loss assumed rare event occurring hardware error 
acceptable packet loss lead application visible error 
fault containment network level plays fault containment roles 
reconfigures routing tables interconnect surviving nodes remain connected physical level error 
guarantees physical level error cell cause loss internal packets sent nodes cell ensuring packets destined routed failed area discarded recovering physical level error 
guarantee required enable memory system implement memory fault model 
data link level normal operation data link level provides bit error detection correction network level 
error model data link level error occur corrupted packet delivered appeared correct 
reliability design assumes data link level errors 
expensive implement error correction protocol cope case low latency requirements memory system 
fault containment addition assuming data link layer detects corrects bit errors packets reliability design requires data link layer complete flag corrupt packets truncated error router link node 
feature allows network level treat packet delivery atomic 
physical level physical level consists hardware components machine active processors infrastructure power cooling 
error model errors physical level result logic faults power cooling failure portion machine link failure interconnect 
hardware components links assumed fail fast 
transient errors links masked bit error detection packet retransmission data link level 
minimal fail fast unit router magic chip memory module processor link error occurs 
flash internal self checks implemented recovery algorithms immediately convert halt component node node halt nodes operating system assume nodes fail fast units 
operating system software architecture fault containment reliability design requires hardware partitioned failure units support cellular structure operating system 
example physical level error loss fan removal board single point failure cells 
support memory fault model failure units contain sets nodes occupy non overlapping ranges physical address space 
sets nodes assumed convex interconnect 
node failure unit send packets node failure unit traversing links routers outside failure unit 
physical design assumed provide hardware reset functionality cell basis 
summary fault containment stack fault containment stack includes features operating system memory system network data link physical design levels system 
features hardware levels system implement memory fault model required hardware fault containment operating system 
hardware provides firewall enables operating system implement software fault containment 
goal features improve reliability naive applications allow sophisticated applications implement fault tolerance user level 
operating system software architecture having completed description error model fault containment architecture system describe software architecture hive 
brief section chapter covers lower levels stack 
converting smp operating system multicellular kernel hive conceptually proceeds stages 
modify create single system image distributed system separate kernels coexist single machine 
second add features required isolate cells error damage 
add resource sharing features required achieve performance competitive original smp operating system 
discuss parts system turn 
distributed system distributed system level hive similar previous single system image distributed systems sprite ocd locus pow solaris mc 
hive implemented techniques borrowed systems cells communicate primarily remote procedure calls 
hive architecture cells cooperate shared namespace distributed file system acting client server 
process management distributed users applications see single system image 
process forks new child may created automatically different cell 
operations related interaction processes extended communicate remote cells needed 
example cell sending signal process process group may send messages cells order complete operation 
major difference hive previous systems distributed system level support spanning tasks 
implement spanning tasks cell runs separate local process containing threads local cell 
cells keep shared process state address space map consistent component processes spanning task 
optimize implementation spanning tasks hive sets minimal essential dependency set process cells threads processes running 
enables hive shared memory cell boundaries performance migrate process state cells desirable thread need continue running failure threads process 
appear excessive limitation application perspective 
implementation spanning tasks 
cell process cell cell component cell component proc structure consistency protocols application view system view proc structure operating system software architecture application designed survive loss threads implemented multiple processes map shared memory 
cell isolation cell isolation challenge preventing corruption cell damaging cells 
channels fault cell damage cell sending corrupt rpc request reply providing corrupt data errors direct remote reads causing wild writes 
defense corrupt messages cell sanity checks information received cells sets timeouts waiting reply 
experience previous distributed systems shows approach provides excellent isolation defend byzantine faults 
defense corrupt remote reads reading cell responsibility defend crashing despite problems invalid pointers linked data structures contain infinite loops data values change middle operation 
implemented simple careful protocol includes checks various possible error conditions 
data safely read cell isolation provided sanity checks exactly case receiving message data 
defense wild writes cells write internal data structures cell isolation extremely difficult 
enforced firewall protect kernel code data remote writes 
cells frequently write user level pages pages shared processes running different cells 
creates possibility wild writes due software error cell corrupt user data read application cell 
sanity checks defend corrupt messages data read careful protocol standard mechanisms distributed systems 
novel cell isolation mechanism hive wild write defense prevent applications corrupted software errors kernel different cell 
wild write defense wild write defense components firewall management preemptive discard fast error detection fast null recovery 
firewall management system seeks minimize number cells write permission page 
nontrivial problem protection changes expensive 
revoking hive architecture write permission previously granted cell requires communication nodes cell ensure cached lines returned memory 
cost expensive guarantee cells actively page write permission 
policies file system virtual memory system reduce vulnerability pages possible causing excessive number permission changes 
cell guarantees user level pages processes local cell writable outside cell 
ensures processes resources cell vulnerable errors cells 
cells sized large boot time hive need share memory cell boundaries load balancing reasons small processes receive best possible fault containment system 
preemptive discard system discards pages writable failed cell software error detected 
called preemptive discard policy 
reduces chance application corruption preventing corrupted pages read applications written disk 
page clean respect disk backing store discarding transparent applications 
page dirty hive gives error applications subsequently try access discarded data 
naive applications simply exit sophisticated applications field error recover 
unfortunately preemptive discard policy prevent user visible data integrity violations caused wild writes 
corrupt data software error detected faulty cell corrupt page give write permission error detected case corrupted page discarded 
problem appears fundamental hardware shared memory fault containment boundaries impossible guarantee software errors detected immediately 
way prevent data integrity violations avoid write sharing user pages cell boundaries 
giving write shared pages give main performance advantages shared memory multiprocessor 
ways reduce probability data integrity violations 
designer reduce probability wild writes rewriting kernel type safe language microkernel reduce amount code direct access physical memory 
mechanisms beneficial complete example tlb entry dma request bypass type checks done compiler 
hive reduces probability data integrity violations general mechanism 
operating system software architecture fast error detection hive strives detect software errors quickly shorten time window corrupt data 
secondary benefit reducing delay experienced users applications stalled error 
error detection studied problem context distributed systems 
error model primary challenge algorithm reports error error 
cell declare failed cause rebooted erroneous cell mistakenly concluded cells corrupt destroy large fraction system 
hive uses part solution 
cells monitor normal operation number heuristic checks 
checks include timeouts sanity checks messages sanity checks type tag checks remote reads periodic active probes read internal state cells shared memory 
failed check provides hint triggers recovery immediately 
second consensus surviving cells required reboot failed cell 
hint alert broadcast cells temporarily suspend processes running user level run fault tolerant distributed agreement algorithm 
algorithm fault tolerant agreement reached correctly cells fail running 
surviving cells agree cell failed user processes remain suspended system restored consistent state potentially corrupt pages discarded 
approach ensures window vulnerability wild writes lasts check fails agreement process runs assuming software error correctly confirmed agreement algorithm 
window vulnerability reduced increasing frequency checks normal operation 
frequency checks performed tradeoff fault containment performance 
fast null recovery final component wild write defense fast null recovery 
null recovery occurs error check triggers error occurred 
system reaches agreement cells failed returns normal operation 
fast null recovery supports wild write defense making acceptable bias error checks give false alarms false negatives 
bias false alarms required checks provide hints error may occurred 
example active probe reading internal state cell synchronize processes modifying state observe transient situation mimics error condition 
hive architecture fast null recovery false alarms low performance impact developer system iteratively reduce chance data integrity violations adding checks targeted new software error observed 
error recovery assuming cell isolation successful error recovery hive essentially problem faced solved previous single system image distributed systems 
hive uses standard techniques ensure operations changing process groups sending signals modifying files exactly semantics 
processes executing kernel allowed continue executing error detection preemptive discard error recovery operating 
certain aspects recovery difficult example careful design needed cope external rpc requests arriving cell recovery running 
approach significant advantage recovery algorithms acquire kernel locks assume normal consistency data structures protected locks 
design error recovery algorithms values simplicity speed 
null recoveries frequent actual errors assumed rare user visible recovery pause times acceptable 
resource sharing policies turn features designed improve reliability features provide resource sharing 
compete performance efficiency existing smp operating systems hive share resources cell boundaries tightly necessary previous distributed systems 
policies mechanisms resource sharing raise challenging problems 
hive separates policy mechanisms intercell resource sharing 
mechanisms implemented cooperation various kernels policy implemented outside kernels user level spanning task called wax 
justification wax wax addresses problem faced previous distributed systems limited unattractive resource management strategies 
resource management distributed case kernel decisions incomplete view global state 
alternatively centralized case kernel running policy module performance bottleneck policy module difficulty responding rapid changes system 
operating system software architecture wax takes advantage shared memory support spanning tasks provide efficient intercell resource management 
wax complete date view system state limited running single cell 
policy modules running kernel level carefully maintain fault containment threads wax running different cells synchronize standard locks nonblocking data structures 
enables efficient resource management decisions 
moving intercell resource management kernels acceptable hive previous distributed systems resources cell belong system local user 
making correct tradeoff local remote requests requires global view system state available wax 
cell responsible maintaining internal correctness example preserving local free memory avoid deadlock optimizing performance resources allocated wax 
implementation wax despite special privileges wax special kind process 
spanning task threads running cells terminated cell fails 
recovery process starts new incarnation wax forks cells rebuilds picture system state scratch 
avoids considerable complexity trying recover consistency wax internal data structures damaged cell failure 
system may unbalanced run slowly policy modules wax restart acceptable cell failures assumed rare 
wax weaken fault containment boundaries cells 
cell protects sanity checking inputs receives wax 
operations required system correctness cell process process process wax state hints state hints cell 
intercell optimization wax 
hive architecture handled directly rpcs delegated wax 
wax damaged faulty cell hurt system performance correctness 
cells block waiting wax decisions remote cell allocate memory 
wax periodically modifies internal tables cell resource management routines 
cells provide state updates wax exporting data structures read sending signals action appears necessary 
resource sharing mechanisms appropriate global policy support wax remaining challenge resource sharing efficient 
resources need shared particularly efficiently cell boundaries memory devices processors 
memory sharing hive provides memory sharing mechanisms 
type called logical level sharing enables processes different cells share data 
called physical level sharing allows free memory pages flow cells demand high 
physical level sharing page frames cell cell cell cell process addr space process addr space logical level sharing data pages 
types memory sharing cell boundaries 
operating system software architecture logical level sharing implemented virtual memory system file system ensure processes open map file share cached pages file memory 
supports efficient file cache performance critical resource workloads allows processes forked parent share data pages efficiently cell boundaries 
physical level sharing implemented memory allocation modules different cells 
cell short memory wax may direct borrow set pages cell 
allocation module original owner moves pages reserved list ignores wax directs borrower return borrower fails 
mechanism supports load balancing memory preventing situation cell starts paging free memory system 
device sharing sharing devices implemented just distributed systems 
request access remote device forwarded cell physically owns device 
cell executes request returns result client cell 
hive avoids extra data copy design normally require memory cell owns device intercell memory sharing mechanisms allow devices issue dma accesses directly memory client cell 
processor sharing processor sharing hive different previous distributed systems 
systems shared memory option load balancing system migrate processes completely slow 
hive takes advantage spanning task mechanism 
new thread created cell shares process context original process thread original cell suspended state moved new thread 
new thread process created cell done pausing process execution moved cell faster traditional migration state transferred 
migrating process creates memory system costs due overheads remotely accessing working set process costs comparable created moving process processors smp kernel 
process running new location normal page migration replication mechanisms activate reduce memory system costs 
cell size tradeoffs resource sharing mechanisms just described cell owns resource retains ownership permanently 
choosing resource sharing way static cell size hive architecture important decision early design system 
possible transfer ownership nodes cells moving processors devices memory simultaneously effect allowing cells grow shrink system runs dynamic cell size 
static cell size attractive simplifies implementation 
hardware resources processors devices cell manages boot time code base smp operating system discovers manages resources need changed 
cell boundaries move dynamically hardware implementation memory fault model simplified 
exchange increased complexity dynamic cell size offers advantage potentially reduced inter cell communication rates 
kernel overheads reduced 
lightweight process migration spanning tasks 
cell active cell initial state promotion spanning task cell cell active thread idle thread lightweight migration cell cell idle thread active thread operating system software architecture system resize cells match applications run 
particular dynamic cell size able eliminate performance overheads caused running large applications spanning tasks 
possible significantly simplify system eliminating support spanning tasks 
deciding factor choosing static cell size hive assumption workload contain mix large small processes require fault containment small processes section 
assumption spanning tasks required implemented efficiently cell size dynamic implementation complexity moving nodes cells appear justified 
static cell size obviously workload dependent tradeoffs performance reliability choosing cell size boot time 
cell size increases resource allocation efficiency improves intercell communication costs drop kernel memory system synchronization costs increase 
system reliability initially increases cell size increases processes local section decreases process vulnerable larger fraction system 
turnaround point reliability determined average process size workload relative frequency hardware software errors 
hive leaves system administrator balance factors choose appropriate cell size boot time site workload requirements 
summary features hive architecture just described primary features 
internal distributed system similar distributed systems maintains single system image relies primarily rpcs communication 
second provides cell isolation distributed system wild write defense consists firewall preemptive discard policy fast error detection fast null recovery 
seeks achieve performance competitive smp operating systems physical logical level memory sharing lightweight process migration spanning tasks managed global policy modules wax 
addition primary features hive architecture secondary features important reliability scalability system 
implementations features designed clearly required 
graceful shutdown key advantage multicellular kernel commercial installations parts system shut transparently preventive maintenance 
graceful shutdown requires additional process migration mechanism leaves dependencies hive architecture home cell lightweight migration mechanism spanning tasks 
complete migration operation performance critical 
page migration replication improve locality cc numa machine virtual memory system able replicate read shared pages multiple cells migrate pages cells 
independent replication migration occur cell 
important architectural feature designed file system 
file system multicellular kernel provide globally shared namespace replication critical directories files striping software raid takeover dual ported disks backup cell primary cell fails 
tolerating loss cells system 
achieving goals preserving competitive performance substantial research challenge 
error recovery lower levels hive depends firewall memory fault model exported memory system described detail section 
completeness section surveys required error recovery functionality provided lower levels fault containment stack flash 
memory system level hardware fault containment requires recovering memory system consistent state error 
flash simplifies challenge memory system recovery avoiding need determine exact state system error 
consider event damages interconnect may damage arbitrary cache coherence operations incomplete time failure example deleting random network packets 
possibility errors recovery difficult achieve consensus state system 
flash recovers consistency memory system doing warm reset 
called algorithm 
error detected network recovery completed nodes flush caches network drained directory entries reset show lines clean memory uncached 
caches flushed line marked dirty reset phase lost fault 
error condition marked directory entry line ensuring operating system receive error time processor tries access line 
hardware errors occur recovery progress algorithm simply restarts 
error recovery lower levels memory system error recovery complicated desirable implement main compute processors protocol microcode 
system reserves non interrupt purpose 
issued memory system interrupt forces processor executing code uncached mode special range memory modifiable operating system 
network level error occurs prevents delivery packets power failure node halt broken link entire interconnect congested packets fill queues routers 
recovering condition creates key challenge dropping packets destined routed failed portion machine 
network level recovery algorithm starts diagnosing portions machine failed 
information sets flag routers bordering region packets attempting enter failed region discarded 
network packets discarded reach destinations 
network drained recovery algorithm modifies network routing tables reconnect surviving nodes avoiding failed region 
support communication network clogged flash reserves virtual lanes routers network level recovery 
network recovery complicated flash implements code runs main compute processor way memory system recovery 
information see details memory system network recovery gal normal operation network data link levels 
hive prototype chapter hive prototype architecture described previous chapter system design 
chapter describes current hive prototype implements design flash features support 
section surveys implementation status prototype 
remaining sections chapter describe aspects system necessary evaluate results experiments outside focus dissertation partition multiprocessor multiple cells rpc subsystem file system 
defer discussion fault containment memory sharing chapter chapter implementation described detail 
implementation status hive prototype implemented extensive modification sgi irix provide architectural features described chapter 
single system image limited 
system streams includes interprocess pipes network connections system shared memory direct device access supported cell boundaries 
wax implemented 
cell periodically reads data structures exported cells memory sharing decisions uses simple rotor decide cell fork 
spanning tasks implemented 
parallel applications workloads run mode fork independent subprocesses share memory 
intercell file system straightforward port nfs provide true shared name space file replication 
net effect missing features performance comparison hive irix conclusive 
performance problems may remain undetected 
prototype sufficient demonstrate solutions fundamental challenges fault containment shared memory environment memory sharing cell boundaries 
multiple cells multiple cells issues resolved kernel run just part machine owning entire system case current smp operating systems 
remap region mips processor architecture fixed addresses low memory processor jumps interrupt exception occurs 
support multicellular kernel hardware remap addresses cell local node local 
hardware software error node zero cause entire system fail 
flash provides remap region feature purpose 
logic magic chip processor interface modifies addresses received processor reach memory system proper performs inverse mapping addresses sent memory system processor 
remap region set power number bytes 
remap region set bytes lower bytes physical address space exchanged lower bytes local node memory 
kernel relocation base irix design accesses kernel code directly physical memory addresses 
simplicity hive retains design 
kernel build hive generates multiple versions kernel executable different starting address 

flash remap region 
node node address range address range memory data stored node memory data stored node physical address issued processor node location data physical memory hive prototype production quality system desirable store copy kernel code disk ways adapt code multiple starting addresses 
boot loader relocate kernel depending memory placed 
kernel relinked access code data virtual memory addresses 
third option place kernel code remap region waste memory remap region node local cell local memory 
virtual memory addresses appears best option especially necessary step running kernel reduced privilege state mips supervisor mode deny direct access physical memory reduce chance wild writes 
possible wired superpage tlb entry eliminate performance impact implementation complexity tlb misses kernel code 
booting initial kernel booted system full resource exploration partitions system number cells specified boot line 
boots single user mode partition compiled triggers recovery 
normal recovery algorithms run determine cells alive elect current cell master boot integrate remainder cells 
slave cells boot parallel approach efficient large machine 
rpc subsystem rpc latency critical hive previous distributed systems tight resource sharing cell boundaries expected applications 
flash provides hardware message primitive called short interprocessor sends sips designed support hive rpc 
hardware support sips facility delivers bytes processor receive queue inside magic chip node overflow node main memory 
minimum latency nearest neighbors sec uncached write sending processor interrupt delivered receiving processor 
quite fast comparable speed highly tuned hardware support cache misses take sec directed memory neighboring node 
receiver waiting message example reply half rpc issue cache advance special address stalls magic message arrives 
receive posting optimization eliminates interrupt dispatch overhead total latency word sent word received receiver cache sec 

performance measured experimental setup described chapter 
rpc subsystem rpcs implemented top normal cache coherent memory reads writes 
important justify custom hardware needed 
design points considered custom support shared memory interprocessor interrupts provided standard multiprocessor sufficient support rpc implementation 
sending processor places message producer consumer buffer sends ipi receiving processor 
fault containment problems limit usefulness design 
queue receiver granting global write permission shared queues allow faulty cell corrupt message system 
receiving cell poll sender queues determine cell sent ipi inherently non scalable approach 
single word messages ipi hardware extended transmit number argument bits receiving processor 
just bits transmitted specify sending processor cell eliminate polling problem 
ipi carries bits certain common messages ack directly encoded 
processor receives single word message frequently need read argument words sender 
expected latency communication message trips sender receiver initial message round trip receiver cache misses argument words 
assumes optimized design senders maintain receiver outgoing queues receiver need fetch queue metadata learn message stored 
performance cost lead savings hardware cost compared cache line messages similar complexity required send messages queue messages handle overflows designs 
cache line messages ipi mechanism extended transmit cache line worth data receiving processor 
design chosen sips mechanism flash 
messages efficient memory system optimized move data cache line sized units 
example interrupt delivered receiving processor force cache order retrieve data efficiently 
critical word restart provided processor hardware enables operating system dispatching message tail cache line flowing processor bus 
block move memory system provide capability efficiently copy number cache lines place 
useful system runs programs coded hive prototype message passing interface standards pvm mpi 
opportunities apply block moves optimize kernel level rpc performance key problem low latency transfer control high bandwidth transfer data 
cache line size flash bytes 
size sips message high fraction rpc requests replies require additional cache misses argument result data fits initial message 
expected latency message send just trip sender receiver 
rpc architecture hive rpc subsystem streamlined minimize overhead added efficient sips primitive 
timeouts loose checked infrequently purpose detect hardware faults server cell failure assumed rare 
message fragmentation reassembly required data cache line sent careful protocol access 
interrupt level rpcs base rpc system requests serviced immediately interrupt level receiving cell 
sending request client processor posts receive waits reply context switching different process 
reply delayed client processor eventually time block requesting process 
transparent client rpc occur server failure significant contention memory system server cell 
requiring client processor wait common case simplifies interrupt level rpc outbound path processor outstanding request time 
timeout occurs causing processor context switch process attempts send interrupt level rpc second process block previous rpc completes system declares fault enters recovery 
requirement sender able block creates single important constraint interrupt level rpc mechanism interrupt handlers send rpcs 
affects design higher levels system 
example handlers interrupt level rpcs send nested rpcs events stimulated timeouts queued server processes sending rpcs directly clock interrupt handler 
process level rpcs general mechanism layered top base interrupt level rpc mechanism 
process level rpcs provide queuing service server process pool handle longer latency requests including cause block kernel semaphores 
process level request structured initial interrupt level rpc launches operation rpc subsystem completion interrupt level rpc sent server back client return result 
context switch scheduler latencies process level rpcs significantly expensive interrupt level rpcs 
optimization process level handler paired corresponding interrupt level handler 
interrupt level handler initial best effort attempt scheduling server process execution satisfy request 
deadlock avoidance avoid deadlock interrupt level rpcs flash provides sips receive queues node requests replies 
reply queue raises interrupt request queue 
avoid deadlock process level rpcs processes server pool organized levels starting 
non server processes level zero 
cell receives request process level places queue level server process 
deepest rpc call chain system currently reaches level 
evaluation hive rpc subsystem achieves low latency goals 
latency null interrupt level rpc sent idle neighboring node sec 
process level rpc handled server process minimum sec latency sec comes scheduler context switch mechanism implementation kernel semaphores 
caveats excellent speed measured interrupt level rpcs 
current implementation implement full retransmission duplicate suppression protocol recovery network physical level errors receive queue overruns cause loss sips 
correctly handles types packet loss 
second measured time short measurement affected processor model simulations processor installed flash 
hive rpc subsystem sips mechanism closely resemble low latency message delivery systems active messages vec net bss 
key difference previous systems sips implemented directly memory controller take advantage optimizations unavailable systems 
example receive posting optimization processor waiting message issues receive cache advance cache stalls memory controller eventually controller returns incoming message returns timeout 
approach substantially faster polling interrupt approaches required memory controller part implementation 
hive prototype file system intercell file system hive called cell nfs 
variant standard nfs distributed file system 
file system provides functionality sufficient prototype designed provide functionality needed production quality multicellular kernel 
name space cell nfs cell file server portion file system name space 
cell fails portion name space inaccessible 
cell local root directory accesses parts name space served cells mount points directory 
names mount points hardwired kernel places 
remote cell root directory automatically mounted corresponding mount point accessed time 
second process forks child cell child process executes mount point corresponding parent cell order maintain consistent view file system name space 
initial hive design include turned necessary 
cell target load balancing cells attempt fork processes soon integrated system 
fork fail root directory parent cell mounted 
possible preemptively mount remote cells boot multiple cells may booting time 
fork attempt root directory parent cell 
universal file identifiers vnode pointer internal name file base irix code insufficient hive 
problem arises process forks child cell 
open files process new cell vnode pointer parent cell local cell name needed identify file file server cell reopen request 
external file name open file initially may longer valid 
cell nfs provides universal file identifier solve problem 
universal file identifier contains file server cell id vnode pointer cell 
sufficient identifier duration limited 
need valid process system file open 
file system ensures vnode file server cell deallocated long file 
universal file identifier stored local vnode represents remote file 
data access process reads writes file file server cell remote cell nfs behaves just nfs important exception 
returning copy requested summary page file server cell nfs returns address requested data stored memory 
uses memory sharing mechanisms provided virtual memory system remote read write access memory data 
standard nfs writeback mechanism write writeback design creates high number synchronous accesses nfs affect cell nfs 
data consistency cell nfs inherits metadata data consistency mechanisms nfs 
nfs metadata cached possibility invalidation seconds data cache coherence 
approach acceptable nfs files rarely write shared distributed environments 
multicellular kernel files frequently write shared processes running different cells better data consistency metadata consistency mechanisms needed 
returning data pages copies pages clients solves data consistency problem cell nfs cache coherence hardware maintains consistency 
metadata consistency problem remains unsolved hive 
application experiments potentially sensitive problem pmake console outputs parallel compilations written append mode shared file 
avoid problem parallel run mode parallel compilation processes output logs individual temporary files 
master process polls files copies output shared log file 
summary mechanisms just described implemented different levels completeness 
support multiple cells including remap region complete multicellular kernel approach long acceptable store multiple kernel executables disk 
rpc subsystem sips primitive support fairly fully implemented lacking completed retransmission protocol hardware support required efficient 
contrast file system provide features required scalable performance fault containment 
limitations features missing single system image mechanisms notably spanning tasks wax prevent dissertation making definitive performance multicellular kernels 
implementation complete experiments demonstrate fault containment possible inside shared memory system memory shared application kernel levels fault containment boundaries 
chapter describes experiments performed chapters describe implementation fault containment memory sharing detail 
experimental setup chapter experimental setup chapter describes setup experiments dissertation 
flash machine operational experiments simos machine simulation environment 
section chapter describes simos 
second gives information performance experiments 
third describes reliability experiments 
final section analyzes simulation affects experimental results 
simos simulation environment simos wir machine simulation environment simulates hardware uniprocessor multiprocessor computer systems detail boot run study commercial operating system 
specifically simos provides simulators processors caches memory systems number different devices including scsi disks ethernet interfaces console 
number unique features simos detailed workload kernel studies possible 
include multiple processor simulators checkpoints annotations 
multiple processor simulators addition configurable cache memory system parameters typically simulation environments simos supports range compatible processor simulators 
simulator speed detail trade 
highest speed simulator called embra uses binary binary translation techniques 
fast mode capable executing workloads times slower underlying host machine 
simos contains detailed processor simulators orders magnitude slower embra 
processor simulator called mipsy models static pipeline processor instruction cycle outstanding cache time simulator called mxs models dynamic pipeline processor order speculative execution multiple outstanding cache misses 

description section largely taken 
simos simulation environment checkpoints simos save entire state simulated machine time execution 
saved state includes contents registers main memory devices restored time 
checkpoints possible restart experiment point interest wasting time rebooting operating system positioning applications 
annotations better observe workload execution simos supports mechanism called annotations user specified routine invoked particular event occurs 
annotations debugger breakpoints trigger workload execution reaches specified program counter address 
annotations non intrusive 
affect workload execution timing access entire hardware state simulated machine 
simos simulates complete hardware system variety hardware related statistics kept accurately non 
statistics cover instruction execution cache misses memory stall interrupts exceptions 
simulator aware current execution mode processors current program counter 
provide information important aspects operating system current process id service currently executed 
track operating system execution simos provides set state machines processor process pushdown automaton processor keep track interrupts 
automata driven annotations set various places kernel 
example annotations set kernel idle loop separate idle time kernel execution time 
annotations context switch process creation process exit code keep track current running process 
access registers memory machine annotations non determine name id currently running process 
additional annotations set page fault routines interrupt handlers disk driver hardware exceptions 
attribute kernel execution time service performed 
annotations entry exit points routines acquire release spin locks determine synchronization time system individual spin lock 
simulator validation validating accuracy statistics issue dissertation simulation studies performance focus results 
simos partially validated reported 
correctness execution applications unmodified generate output data code runs real irix systems difficult imagine kernel code successfully completing complex workloads simos execute correctly 
experimental setup performance experiments performance experiments hardware configuration shown table 
simulation model experiments mipsy processor model configured caches mips flash 
memory access latencies computed memory system model developed flash hardware team called 
cache line clean local memory takes nsec processor clocks line clean remote memory takes nsec processor clocks line dirty remote processor cache sec processor clocks 
times increased queueing delays magic chips misses magic caches contention network arbitration processor busses 
includes protocol processor emulator executes times actual protocol microcode run machine 
workloads performance experiments workloads 
cases workload run warm file cache run measure execution time behavior 
workload pmake models general purpose system 
parallel files distribution compiled time 
fairly small table 
system model performance experiments 
component characteristics processors mhz mips cpi cache misses primary instruction cache kilobytes way associative byte lines primary data cache kilobytes way associative byte lines unified secondary cache megabyte way associative byte lines disks accurate model hp drive ktr mb memory flash nodes megabytes mesh interconnect mhz spider routers gal magic controllers mhz megabyte direct mapped data cache kilobyte way associative instruction cache reliability experiments compilation files compiled chosen source files distribution minimize length difference shortest longest source files 
avoids hiding operating system performance differences idle time occurs waiting compilation complete 
workloads ocean raytrace parallel scientific applications splash benchmark 
applications stress hive resource sharing mechanisms 
ocean uses grid second interval 
raytrace uses teapot data set rays pixel 
applications run threads mode threads created fork system call 
perspective hive run separate processes happen share memory 
operating system configurations performance baseline provided irix modified little possible port flash hardware modeled simos 
primary changes addition new disk ethernet keyboard device drivers modifications low level handlers manage flash interrupt architecture 
hive performance measured configurations cells processor system 
case hardware resources divided equally cells 
example cell system contains copy kernel text starting physical address manages processors memory megabytes copy kernel text starting physical address megabytes manages processors memory megabytes megabytes 
debugging purposes hive compiled optimization disabled 
performance comparison fair irix compiled way 
summary results table shows time completion performance workloads various operating system configurations 
cell hive configuration competitive irix pmake slower raytrace faster ocean 
interestingly pmake ocean faster cell configuration cell configuration pmake faster cells cells 
results explained chapter implementation details hive 
reliability experiments difficult predict reliability complex system extensively 
furthermore impossible demonstrate complete reliability fault error injection experimental setup tests 
tests provide indication hive fault containment mechanisms functioning correctly 
table summarizes system model reliability experiments 
possible run large number tests reliability experiments smaller hardware model accurate simulation mode smaller workload performance experiments 
simulation model experiments embra processor simulator gives timing accuracy achieve simulation speed 
effect binary translation techniques embra perspective hive running simulated system speed processor varies nondeterministically time relative speeds processors 
improve simulation speed memory accesses disk accesses complete cycle 
high level system model implies experiments stress software fault containment mechanisms hive hardware fault containment mechanisms flash 
dissertation focuses hive experiments assume flash successfully implements memory fault model 
workload workload reliability experiments parallel files distribution performance experiments compiled time 
primary run fault error injected completes terminated files successfully output look data corruption 
parallel executed 
success second parallel taken evidence surviving cells corrupted 
table 
time completion seconds performance experiments 
workload irix cell cells cells cells pmake raytrace ocean table 
system model reliability experiments 
component characteristics processors mips timing accurate disks single cycle latency mb memory zero latency impact simulation operating system configuration fault injection experiments cell hive configuration 
script driving parallel executions runs cell source files stored disk attached cell 
file system limitations cell essential compile processes 
experiments performed order check faults recovery algorithms reliability experiments avoid injecting faults errors cell 
summary results software hardware error injection experiments software fault injection experiments instances data corruption output parallel 
fault injection studies hive failure rate indicates fault containment mechanisms functioning properly 
multicellular kernels significantly better failure rate measured hive prototype immature half failures come just software faults easily fixed 
chapter analyzes results presenting implementation fault containment 
impact simulation simulation constrains experiments significantly limits size system studied 
discussing impact limited system size results describe ways differences execution environment simos flash machine affect performance reliability experiments 
limited system size processor systems studied smaller machines tens thousands processors hive designed 
reason performance problems may remain undetected 
difficult multicellular kernel achieve competitive performance small system sizes medium large system sizes 
memory system synchronization costs lower overheads smp operating systems lower 
competitive performance seen performance experiments promising result 
performance experiments show trends suggest multicellular kernels behave larger systems chapter 
reliability results affected small system sizes studied 
reliability affected number cells number processors production systems large processors may run just cells 
primary effect increasing experimental setup number processors cell increase rate interaction cells creates opportunities corruption cell spread latency detect software errors remains constant 
interaction cells represents opportunity execute heuristic failure check expected latency drop rate interaction increases 
conjecture proves correct demonstration fault containment possible shared memory system carries weight irrespective size system 
impact performance experiments differences simulated environment performance experiments flash machine significant impact performance results 
processor pipeline performance experiments static pipeline processor model dynamically scheduled pipeline processor flash 
accurate mxs processor model simos slow workloads large ones performance experiments 
effects measured operating system performance simpler processor model keeping rest hardware configuration unchanged examined detail 
workload uniprocessor version irix study dynamically scheduled processor hides kernel level cache time experienced static pipeline processor kernel level cache time 
memory system satisfies level cache misses uniform nsec substantially faster flash 
suggests changing static pipeline processor dynamically scheduled processor minor impact kernel level cache time flash 
primary effect change relative performance measured irix various hive configurations substantial reduction level cache time 
effect strengthens results performance experiments 
table shows percent kernel time spent level cache misses increases steadily number cells increases 
reduction time improve performance hive improves performance irix 
magic caches implementation constraints forced magic hardware design team switch aggressive instruction cache protocol microcode 
memory impact simulation system model performance experiments configured original magic design containing way set associative instruction cache 
changing direct mapped cache change relative costs local remote communication cache misses changing relative memory system costs irix various hive configurations 
hardware design team plans lay protocol microcode routines minimize conflict misses slowdown due cache effects spread evenly microcode 
effort succeeds effect changing cache design relative performance operating system configurations minor 
impact reliability experiments simulation significantly easier study reliability system 
simos deterministic enabling exact repetition fault error injection experiments effects analyzed detail 
simos affects results reliability experiments due limited address space 
simos implements bit virtual bit physical addresses 
significantly smaller bit virtual bit physical addresses flash 
peter chen argued bit virtual address space generation processor reduces probability wild writes cnc 
inaccuracy caused simos strengthens primary experimental results dissertation argue hive successfully provides fault containment despite possibility wild writes 
table 
percent kernel time spent level cache misses 
workload irix cell cells cells cells pmake raytrace ocean fault containment chapter fault containment chapter presents evaluates hive fault containment mechanisms 
mechanisms fall broad categories provide cell isolation provide failure recovery 
cell isolation hive includes software hardware features 
sections chapter describe software features preserve cell isolation despite possibility corrupt remote reads wild writes 
third section chapter gives precise definition memory fault model specifies hardware features required cell isolation despite possibility hardware errors 
failure recovery hive consists set software mechanisms restore operating system consistent state cells failed 
fourth section chapter provides overview mechanisms 
section chapter reports results fault injection experiments stress hive implementation fault containment 
safe remote reads cell reads internal data structures rpcs cases rpcs slow date view data required data needs published large number cells 
data read sanity checked just rpc received remote cell checked 
remote reads create additional fault containment problems 
mechanisms remote reads safe careful protocol publisher lock 
ensures reading cell checks misaligned pointers data structures loops 
allows reading cell synchronize cell owns may updating data 
careful protocol careful protocol defends types errors remote cell hardware errors reflected memory fault model bus errors software errors safe remote reads appear corrupt data 
bus errors significant cells normally panic shut detect hardware exceptions kernel execution 
essential self check frequently indication internal corruption important weaken self check little possible implementing careful 
reading cell follows steps 
call careful function captures current stack frame records remote cell kernel intends access 
bus error occurs due reading memory cell trap handler restores saved function context causing panic 
remote address check aligned properly expected data structure addresses memory range belonging expected cell 
copy data values local memory sanity checks order defend unexpected changes 
check remote data structure reading structure type identifier 
type identifier initialized memory allocator overwritten memory 
feature simple add allocator irix similarly straightforward kernels 
checking expected value tag provides line defense invalid remote pointers 
call careful done bus errors experienced processor correctly cause kernel panic 
implementation details careful implemented just setjmp 
sets careful register save areas process stored user area processor stored processor private data area 
register save areas allows process middle careful interrupted safely interrupt handler may need careful 
interrupt handlers processor register save area disable interrupts making careful 
process making careful allowed specify secondary target cell access simultaneously primary cell 
case forced adding capability obscure parent forks child process locked copy write pages child cell preemptively copy pages fork avoid breaking parent lock 
pages happen borrowed third cell child cell may need remotely access different cells simultaneously 
fault containment usage paradigm careful referencing careful come bus error occurred careful 
trap handler turns careful referencing automatically 
hint failure inform recovery subsystem error return error remote pointer remote structure type bad pointer careful return error bcopy remote pointer local buffer sizeof remote structure type careful return success macro abstracts necessary pointer type identifier checks define bad pointer sizeof int bad pointer void int align int size nodeid node number extract node bits address physical memory address return return size return align return return type tag stored convention field named 
functions sizeof compiler builtins 
macro converts type name unique integer compile time 
type tag field read checking pointer valid bus error occurs trap handler return saved careful point causing kernel panic 
publisher lock cell acquire lock remote data structure needs read 
cell allowed remote cells modify lock variable data structures cell longer trust accesses data structure free race conditions deadlocks 
publisher lock mechanism allows remote cell acquire consistent view data structure acquiring lock 
safe remote reads publisher lock integer variable incremented cell owns data structure modifying structure modification complete 
remote cell copies data structure uses publisher lock determine snapshot consistent 
typedef struct unsigned short publ lock void publ init publ lock void publ lock publ lock hardware store barrier multiprocessor sequentially consistent void publ unlock publ lock hardware store barrier precondition careful referencing enabled remote struct pointers checked int read publ structure volatile publ lock struct st remote struct struct st local copy int timeout timeout timeout unsigned short continue local copy remote struct return success return error subtlety algorithm placement hardware store barriers needed machines relaxed memory consistency models 
hive store barriers 
base irix implementation labeled flash run sequential consistency mode execute hive correctly 
implementing hive unexpected usage pattern publisher lock appeared 
hive publisher locks kernel data structures inherited irix new fault containment ones added support information dissemination cells 
reason cases cell reads internal data structures remote process creation operation invariably requested cell owns data structures lock data structures requesting operation 
evaluation mechanisms efficient read remote data directly 
example careful protocol clock monitoring algorithm clock handler cell checks cell clock value tick 
caches average latency initial call careful terminating careful call finishes sec cycles sec cycles average latency cache memory line containing clock value 
substantially faster sending rpc get data takes minimum sec requires interrupting processor remote cell 
larger question useful read remote data directly 
chapter examines question detail 
wild write defense second way cell corrupt writing memory 
hive defends wild writes part strategy 
manages flash firewall minimize number pages writable remote cells 
second cell failure detected cells preemptively discard pages writable failed cell 
hive implements mechanisms detect failures quickly occur 
sections describe part strategy turn 
firewall management describe implementation firewall flash detail presenting evaluating firewall management policy hive 
flash firewall firewall controls processors allowed modify region main memory 
flash provides separate firewall kilobytes memory specified bit vector bit write permission processor granularity number bits changed protocol microcode 
systems larger processors bit write permission multiple processors 
write request page corresponding bit set fails bus error 
wild write defense protocol microcode running node stores checks firewall bits memory node 
checks firewall request cache line ownership read misses count ownership requests 
uncached accesses devices cells receive bus errors dma writes devices checked writes processor node 
reduce performance impact firewall check dedicated logic determines incoming request node local cell dispatching protocol code 
cell change firewall page single uncached write 
rule firewall modification interesting cell write permission page may modify firewall page 
flash initially implemented intuitive rule cell owns page may modify firewall page 
section physical level memory sharing explains rule changed discusses implications current rule 
kilobyte firewall granularity matches operating system page size 
larger constrain operating system memory allocation unclear finer granularity useful 
bit vector page chosen options require storage 
single bit page granting global write access processes remote memory vulnerable errors machine 
byte page naming processor local cell range interconnect node id mask node id match request interconnect request local node sender local cell 
issue memory access check firewall issue memory access directory processing hardware dispatch logic protocol microcode 
firewall support magic 
fault containment write access inefficient allocate remote page parallel process multiple threads write share page 
performance cost firewall minimal 
firewall check increases average nearestneighbor remote write cache latency pmake ocean comparing runs workloads firewall enabled disabled 
increase little effect write cache misses small fraction workload run time 
fact run time pmake longer firewall disabled enabled due change interleaving process execution 
firewall management policy firewall management tradeoff fault containment performance 
best possible wild write defense write permission user data page granted write enabled mapping page active tlb processor cell 
relatively expensive change firewall write permission page 
requires sending rpc minimum sec cell manages page 
significant hardware cost retracting firewall write permission page 
lines cached exclusively processors cell losing permission retrieved line cached remains modifiable irrespective firewall state 
practice difficult determine lines cached exclusively cell manages page read entire page retracting write permission 
add sec lines cached exclusively 
total cost far expensive pay change tlb mappings occur user instructions workloads 
hive uses relaxed policy 
write access page granted processors cell process cell faults page writable portion address space 
granting access processors cell allows cell freely reschedule process processors sending rpcs 
write permission remains granted long process cell page mapped address space 
reduces number firewall write permission changes acceptable level 
evaluation effectiveness policy measured comparing pmake shares writable pages separate compile processes ocean shares data segment processes 
seconds execution sampled msec intervals pmake average remotely writable pages cell sample user pages cell ocean showed average remotely writable pages 
wild write defense low number writable pages pmake shows policy provide wild write protection system predominantly sequential applications 
highest recorded number writable pages workload cell acting file server directory compiler intermediate files stored tmp 
low extremely wild write bypass firewall corrupt memory 
case ocean current policy provides little protection global data segment write shared processes 
application running processors exit case cell fails efforts prevent pages discarded wasted 
firewall management policy avoids protection status changes create unnecessary performance overheads type application 
preemptive discard cell fails issues wild writes able corrupt data stored pages firewall write permission corrupting applications subsequently read data 
preemptive discard policy attempts prevent problem 
determining potentially corrupt pages difficult determine efficiently pages discard cell failure 
cells page need cooperate discarding cell manages page knows precise firewall status 
distributing firewall status information recovery cells page require significant communication 
hive avoids cost 
cell determines locally pages writable failed cell cells marks pages discarded 
additionally tlbs flushed remote mappings removed recovery 
ensures access remote page fault send rpc cell manages page checked 
recording discarded pages accesses discarded pages occur arbitrarily far making quite expensive record exactly pages file discarded 
hive solves problem relaxing application visible error semantics slightly 
current unix implementations file system attempt record dirty pages lost system crash 
simply fetches stale data disk reboot 
acceptable local processes survive crash process accessed dirty data observe unstable 
hive takes advantage semantics allowing process opens damaged file cell failure read data available disk 
processes opened file fault containment failure receive error accessing discarded pages 
implemented generation number maintained file system copied file descriptor address space map process opens file 
cell discards dirty page file instructs file system increment file generation number 
subsequent access file descriptor address space region mismatched generation number generates error 
evaluation preemptive discard policy costs 
number pages discarded damaged wild writes 
fault injection experiments reported show chance wild writes low 
safer discard valid page forcing user run application preserve corrupted page cause application generate incorrect output 
number pages discarded governed firewall management policy described earlier 
second cost increase execution time applications failure 
artifact implementation preemptive discard policy 
removing remote memory mappings recovery forces surviving applications rebuild mappings adds substantial number page faults 
cost acceptable errors require full recovery opposed null recovery assumed rare 
failure detection preemptive discard prevent corrupted data runs soon data corrupted 
part wild write defense detecting cell failures quickly possible heuristic checks 
heuristic checks prototype sophisticated 
particular timeouts chosen arbitrarily 
list allow evaluation reliability experiments dissertation 
timeout values chosen large avoid false alarms performance experiments somewhat larger ideal reliability 
cell considered potentially failed timeouts occurs cell fails respond rpc request 
timeout set msec rpcs serviced interrupt level second rpcs schedule server process 
cell fails respond ping request sent heartbeat algorithm part recovery 
timeout set msec 
shared memory location cell updates clock interrupt fails increment 
monitoring clock detects hardware errors halt processors entire nodes wild write defense operating system errors lead deadlocks inability respond interrupts 
timeout set msec 
additionally prototype uses mechanisms timeouts 
cell considered potentially failed conditions occurs attempt access memory cell causes bus error 
occur serious hardware error cell incorrectly denies firewall access page cell writing 
data read cell memory received message fails sanity checks 
detects software errors 
memory location alive field cell public area section changes away correct value indicating cell detected internal error halted 
cell alive field read cell clock interrupt 
detecting timeouts conditions provides hint cell may failed 
hint error may cell check algorithm executed 
instance case software fault caused clock interrupt handler cell execute frequently 
pending operations cell timed prematurely leading damaged cell wrongly suspect cell failed 
hint raised distributed agreement algorithm runs cells come consensus cells live failed 
prevent corrupt cell repeatedly broadcasting alerts damaging system performance cell broadcasts alert twice voted distributed agreement algorithm times panic 
evaluation set failure detection mechanisms minimal performance overhead checks performed case defend corruption due receiving bad data 
checks appear detect failures quickly prevent damage due wild writes shown fault error injection experiments reported 
major unanswered question failure detection number false alarms scale number cells number processors cell forcing looser timeouts frequent heuristic checks maintain acceptable performance overheads 
reliability system decrease size 
mechanisms implemented prototype rate rpc timeouts appears increase system size increased interaction rate cells creates greater possibility congestion greater chance detecting temporary congestion occurs 
small system sizes large timeouts fault containment experiments provide data needed judge problem rpc timeouts heuristic checks may added 
summary possible improvements wild write defense consists firewall policies manage write permission preemptive discard policy fast failure detection mechanisms 
fast failure detection mechanisms efficient null recovery case fast 
implementation described section 
architectural components wild write defense implemented differently better multicellular kernels 
firewall firewall implemented hardware flash provided software microkernel low level supervisor multiprocessors provide hardware 
design alternative discussed section 
implemented hardware software firewall extended additional vector records processors modified data page precise cached line page exclusive mode 
feature double storage requirements add significantly memory bandwidth consumed firewall currently read effectively cached magic 
adding write detection vector enable kernel significantly reduce number pages discarded failure may strong benefit justify hardware cost 
firewall management simple firewall management policy successful minimizing writable pages sequential workloads pmake prototype lacks processor sharing mechanisms 
implemented mechanisms tend increase number shared pages creating components spanning tasks multiple remote cells 
firewall management policy need coupled migration mechanism 
particular firewall permission granted remote cell processes cell page mapped inactive spanning task components migration mechanism leave components place long periods time support load balancing demands 
preemptive discard preemptive discard policy may appropriate applications 
example database process pair shared memory communication region application level sanity checks data read region 
system memory fault model permit sophisticated applications declare certain files mappings checked higher level correctness protocols preemptive discard necessary 
certain types cell failures cause wild writes 
particular cell failures due events power failures interconnect link failures assumed fail fast 
system differentiate types failures software errors preserve data possible including recovering data pages memory failed cells 
failure detection possible advanced failure detection mechanisms include running processes walk internal data structures cells looking inconsistencies periodically sending rpcs cells exercise system functionality file process management operations adding heartbeat outputs visible outside cell critical system processes virtual memory clock hand disk buffer flush daemon 
advanced mechanisms necessary particular multicellular kernel apparent tested long running stressful workloads large scale machine 
memory fault model previous sections described cell isolation implemented respect reading writing data shared memory 
channels corruption propagate 
flash tightly coupled machine hardware errors cell easily perturb operation cells 
hive requires hardware support lower levels fault containment stack recover hardware errors 
section describes memory fault model specifies support hive needs 
specification flash partitioned hardware failure units 
correct hardware errors reset 
home range memory lines cache line sized cache line aligned units main memory 
memory ranges different overlap 
memory lines function correctly coherent accessible 
coherent line reads processor correct return data specified memory consistency model machine 
accessible line cache misses processor correct complete fixed time fault containment definitions memory fault model properties hardware error memory lines home incoherent inaccessible 
hardware error memory lines different home inaccessible processors correct 
hardware error memory line different home incoherent processor faulty firewall write permission line error 
access incoherent line processor correct returns error 
access inaccessible line processor correct returns error fixed time 
reset operation available changes memory line incoherent coherent 
contents line undefined reset 
hardware error may cause uncached access machine fail complete issued satisfied correct 
note properties assumptions hive absolute guarantees hardware implementation 
hardware error may occur violates model case hive guarantee provide fault containment failure occur 
design discussion memory fault model compromise ideal multicellular kernel implemented cheaply efficiently cc numa multiprocessor 
aspects tradeoff interesting remote reads processor allowed read memory lines incoherent inaccessible fault model points 
processor fails line shared cache prevent writes line processors 
requires memory system implement fairly complex recovery mechanisms timeouts invalidation requests algorithm 
complexity justified importance memory sharing achieving competitive performance 
converse processor damaged reading remote lines 
interacts choice data link level error detection mechanisms 
highly pipelined memory fault model architecture flash word cache line arriving network delivered processor word received network interface 
sufficient put checksum network packet 
error detection correction provided word cache line transmission reduces fraction network bandwidth available transmitting data 
retracting firewall point stated way line remains vulnerable faults remote firewall permission retracted 
unavoidable sole copy line may remotely cached flight network firewall changes 
full application aspect model suggest cell reallocate page shared user data kernel data kernel data lost remote failure 
avoid limitation flash provides retract operation ensure line longer vulnerable remote cell previously write permission 
cell retract line reading word 
ensures clean copy written memory 
uncached accesses point concession implementation constraints 
turns quite difficult provide guarantees launch forget uncached writes uncached reads 
hive ensures correctness read write verification protocol issuing uncached operations boundaries example remotely changing firewall state line 
network partitions fault model implies hardware faults lead network partitions 
software techniques managing network partitions known pow separate links cross slice flash interconnect implementation complexity software techniques justified 
network partition cause hive fail 
limitations flash implementation flash implement full memory fault model 
major deviations potential loss incorrect packets potential corruption memory incorrect 
network packet loss flash non fully connected network 
communication pass 
suffers catastrophic failure loss power packets flight lost 
flash frequently sends sole copy line network example writing back dirty cache line fault containment means hardware failure destroy data processor failure occurs firewall write permission 
memory fault model include potential data loss due network packet loss errors application uses memory cell vulnerable hardware failures machine 
hive considered vulnerability making memory allocation decisions face significant restrictions attempting balance memory demand cell boundaries reducing performance system 
hardware errors violation memory fault model lead failures hive designed cope random user data lines suddenly incoherent due network packet loss 
standard recovery code exactly ecc errors current smp operating systems adds little complexity system 
network packet loss problem affect kernel data internal cell cells convex internal kernel data writable outside home cell 
incorrect implementation constraints flash protocol prevent firewall check dirty data 
problem arises sharing occur processor exclusive copy cache line transfers line processor requested shared copy 
processor exclusive copy sends line twice requesting processor home node update 
protocol header fixed single source processor id field order protocol microcode home node know processor add sharing list sending processor puts 
network packet loss violates memory fault model 
power failure packet sent memory fault model requesting processor id source field processor id processor sending writeback checked firewall 
problem checking 
race condition occur normal non sharing 
operating system deny writes processor happened line cached exclusively writeback denied line return clean state memory 
speculative execution impossible operating system guarantee line cached exclusively machine checks support intermediate firewall state 
state write requests denied permitted reversion full protection state retract operation issued line 
complications flash sinks memory checking firewall 
possible operating system force incorrect writeback directly corrupting cache tags general incorrect writeback represents memory system error outside error model 
evaluation precision abstraction memory fault model helpful design process 
precision enabled operating system hardware development teams discover resolve different assumptions early design 
example teams opposite assumptions legality remote reads firewall permission granted 
high abstraction level specification benefits 
sets goal allows hardware implementations varying degrees completeness 
network packet loss problem described example 
second covers hardware contingencies considered model developed 
example ongoing effort stanford implement cache memory architecture coma flash modifying protocol microcode 
coma systems evict memory lines node conflict misses associative memory cache 
developers coma microcode memory fault model determine nodes candidates receive evicted line 
features model implemented fairly completely 
algorithm memory system level combined network level recovery appropriately structured physical level covers cases 
fault containment aspect model turned impossible implement flash time limit cache misses accessible lines 
memory system hotspots easily occur cause significant queuing delays network congestion reach truly staggering levels pathological conditions 
timeout set cache misses memory system detect hardware errors loose 
cache misses internal cell stalled error complete timeout fired hardware recovery run 
result time detect line inaccessible multiple milliseconds suggesting large multiprocessors may environments real time constraints responsive memory systems developed 
timing assumption model hive delegates solution hotspot congestion problems memory system developers 
include features idling processors memory system overloaded required manage latency problems operating system level 
failure recovery cell isolation mechanisms described previous sections safe remote reads wild write defense memory fault model provide abstraction fail fast cells generate occasional false alarms 
failure recovery mechanisms responsible distinguishing false alarms actual failure conditions restoring system consistent state cells failed 
recovery subsystem contains kernel level processes table 
process provides abstraction layer process simplifies higher level task 
recovery layers lowest level process called ping real time process recovery 
hint raised cell isolation mechanisms hint alert received cell ping process activates begins exchanging heartbeats cells system 
provides continual awareness cells alive process 
process runs distributed algorithm ensure correct cells system agree cells alive correct 
cell takes initial vote output ping round 
votes intersected produce agreement 
distributed algorithm fault tolerant flood algorithm cell broadcasts current understanding intersection times cells system lyn 
process completes agreement cell independently compares new previous compute die set set cells failed 
process aborts rpcs waiting responses failed cells passes die set recovery process 
failure recovery recovery process responsible returning system consistent state excludes cells die set 
calls cleanup routines virtual memory system file system process subsystem memory manager 
requires communication cells implemented rpcs 
rpcs safe process run recovery process failure occurs 
reboot process executes hardware diagnostics cells die set resets loads new copies kernel image disk integrates rebooted cells back system 
reboot process activates cell master cell elected lowest numbered cell live set 
master cell fails cells rebooting recovery runs reboot process new master cell restarts rebooting cells 
reboot process detects cell rebooting ready join system raises hint triggers recovery 
recovery runs processes reach agreement new live set larger previous live set resources rebooted cell immediately visible rest system 
called reintegration recovery round 
support recovery reintegration initial system boot ping round recovery round sends pings cells system just previous live set 
recovery process internal structure recovery process supports important performance optimizations virtual memory system 
shows global barriers synchronize recovery processes different cells 
live cells reached global barrier cell knows virtual memory file system requests pending cells live set 
cell proceeds discard state related pages mapped cell boundaries part preemptive discard algorithm described section 
cells reached second global barrier cell knows cleaned table 
recovery subsystem design 
layer name responsibilities operating system features ping heartbeat algorithm track cells message send real time response flood algorithm reach distributed consensus message send recovery preemptive discard system cleanup rpc reboot diagnostics reboot failed cells rebooted cells file system fault containment internal state safe sending virtual memory requests part cleanup actions 
design moves synchronization virtual memory operations recovery rpc handlers service virtual memory requests enables requests serviced interrupt level paying cost scheduling server process 
hold outgoing vm file system operations global barrier clean internal vm file system state global barrier clean subsystems 
new die set trigger reboot process 
control flow recovery process 
outgoing vm file system operations fault error injection experiments performance primary performance metric recovery subsystem latency null recovery 
null recovery cell system takes msec 
artifact current recovery subsystem implementation runs recovery process completion including checking preemptive discard die set empty 
preemptive discard accounts msec current null recovery time 
preemptive discard expensive involves scanning firewall entries find pages writable failed cells 
costs sec megabyte storage worth parallelizing algorithm multiple processors cell reduce pause times cells large amounts memory 
improved implementation skips preemptive discard check null recovery dominant cost live set algorithm 
takes msec cells msec cells extrapolation algorithm msec cells maximum supported current flash firewall 
systems processors run cells time fast hive systems raise false alarms second significant performance impact 
number false alarms raised tuned changing tightness timeouts frequency active probes cell memory system administrator adjust performance overhead match requirements behavior workload running site 
fault error injection experiments types experiments evaluate implementation fault containment 
heap corruption models result programming errors leave incorrect values kernel data structures 
node failures model loss portion system due catastrophic hardware fault power failure 
instruction corruption directly models programming errors 
described section experiments parallel workload running system booted cells 
heap corruption heap corruption experiments stressful possible targeted data structures frequently kernel read remote cells shared memory table 
experiment word memory changed random time parallel begun 
experiment pointer listed data structure changed ways fault containment random address overwriting pointer valid random physical memory address forces case generate wild writes 
old value overwriting pointer small integer offset old value damages kernel malloc headers causes reads pointer return random data 
address pointer overwriting pointer address pointer creates pathological looping case stresses careful protocol remotely reading data structures 
cases listed table hive successfully confined effects fault cell injected 
hive initially succeed experiments performed failures occurred bug identified fixed experiment repeated successful 
cases listed failures occurred data output disk compiles incorrect 
node failures catastrophic hardware errors node failures create lesser software fault containment challenge heap instruction corruption hardware implements memory fault model correctly faulty cell simply halts memory inaccessible 
experiments node halted random time experiments node halted process cell creating child process cell 
remote process creation algorithm uses rpcs remote memory reads particularly table 
heap corruption experiments 
number tests type corruption corrupt pointer random address old value address pointer process address map process table entry page hash table run queue kernel malloc free list process user structure fault error injection experiments intensively stressful time node fail 
hive successfully contained effects node failure affected cell experiments 
instruction corruption provide interesting statistical sample available heap corruption node failure experiments instruction corruption experiments organized differently 
performing initial instruction corruption experiments fixing system bugs led failures hive reliability measured doing corruption experiments system modifications 
experimental design instructions corrupted selected follows 
simos configured measure number times instruction address cell kernel executed cycle window parallel workload starting just round compiles begun execution cells 
addresses sorted frequency common addresses discarded 
error commonly executed locations practice instructions tested 
discarding set eliminates routines expect errors occur idle loop page copy 
instructions corrupted chosen randomly remaining executed addresses 
simos deterministic experiments start checkpoint corrupted instruction guaranteed executed parallel run 
instruction corruption uses methodology developed cnc 
shows decision tree determine instruction corrupted 
instruction branch decision tree randomly decides change destination register sense branch inverted simulating bug inverted conditional 
instruction changed cycles run 
results results fall categories table 
experiments hive successfully limited effects fault failure cell 
fault visible effect time 
faults caused processes workload exit lead cell failure 
occur example cell incorrectly returns error request 
simulator limitations prevented completion experiments 
example simos implement mips bit mode supervisor mode malfunctioning cell writes value status register selects modes experiment terminated 
fault containment injected faults lead failures defined failure processes cells failure cells failure entire system 
failure rate cases smp operating system failed considered rows table 
causes failures failures come variety causes table 
causes listed table software faults implementation hive easily representative types faults lead failures multicellular kernels 
change nop corrupt modify source register modify destination register alu op branch op random source register op random destination random source 
decision tree instruction corruption experiments 
alu branch op instruction retry different address corrupt random executed address nop instruction retry different address overwrite fault error injection experiments due race condition live set process reboot process reboot process trigger reintegration recovery round failed cell finished rebooting 
reintegration recovery round concludes cell failed resets 
condition leads repetitive recovery executions 
client side file system occasionally neglects acquire lock recovery process hold file system activity 
file system request live cell arrives global barriers recovery process cause file server cell fail 
table 
instruction corruption experiments result type number occurrences percent cases smp kernel failed cell failure hive recovers successfully visible effect execution processes terminated cell crash simos limitations prevent completion experiment failure table 
causes failures 
category cause failure number occurrences software faults hive premature reintegration round file system neglects acquire recovery lock file system sanity checks insufficient conditions handled recovery subsystem injected fault prevents rpc replies cells panic injected fault prevents message sends faulty cell resets rest system injected fault causes repeated recovery cell bypasses self check case byzantine faults faulty cell issues requests causes value overflow server cell fault containment server side file system check validity request arguments sufficiently avoid failing certain types corruption 
types failures result injected faults create conditions handled recovery subsystem 
injected fault renders cell unresponsive rpc requests ping processes function cells commit suicide 
occurs cell tries interact faulty cell receives rpc timeout starts recovery learns faulty cell 
occurs twice row cell 
possible defend type fault adding self test checks success loopback rpc start process 
injected fault renders cell unable send messages leads total system failure 
occurs cell receives response outgoing pings concludes cells failed resets cells 
potential occur software hardware problem message subsystem argues adding heuristic mechanism 
recovery runs cell concludes cell left alive delay activating reboot process 
delay long cells system reset cell alive 
injected faults caused cell trigger recovery clock tick bypassing self check shuts cell triggers recovery times 
possible defend type fault extension algorithm 
addition reaching agreement cells vote cells shut despite 
final failure due byzantine fault 
injected faults caused cell repeatedly request mappings page cell 
legal operation eventually count cell overflowed negative caused internal vm operations cell fail 
possible defend overflow clear action cell take detects counter overflow 
overflow occurs necessarily cell currently requesting mapping erroneous 
general difficult defend non fail fast errors cause cells issue legal operations illegal patterns 
type error violates error model assumption sanity checks applied incoming messages detect corrupt requests 
fault error injection experiments evaluation repeated recovery fault technically byzantine fault faults prevent rpc replies 
faults easily handled extensions recovery system affect recovery communication substrate 
overflow error challenging faults arises software fault higher levels system 
byzantine behavior causes indistinguishable correct behavior higher level semantic knowledge 
type fault assumed rare hive design provide acceptable level reliability 
sample size experiment small predict probability faults practice encouraging fault occurred series fault injections 
observed failure rate clearly conservative 
half failures come just software faults hive 
mature system implements recovery system extensions discussed lower rate achievable 
effectiveness wild write defense failures series fault injection experiments resulted wild write 
experiments error injection experiments corrupted application output files 
raises question experiments demonstrate effectiveness wild write defense low probability wild writes 
probability wild writes software fault injection experiments firewall write violation provided indication software error cases 
cases faulty cell issued write eighth case cell corrupt address provided faulty cell 
omitting cases cell failed assuming wild writes experiments caused firewall violations gives wild write frequency 
additional eleven cases indication error write faulty cell memory address mb mb 
machine configured mb memory system wild write frequency 
clear wild writes nontrivial problem 
interesting compare results cnc similar fault injection methodology led memory corruption stimulated crashes digital unix running alpha workstation mb memory 
hive shows nearly rate firewall violations stimulated cell failures 
correspondence systems different code bases hardware architectures suggests results apply widely just particular implementation measured 
fault containment usefulness firewall wild writes observed additional eleven occurred larger system certainly led corruption memory firewall 
argues firewall provides useful service cell isolation 
usefulness preemptive discard unfortunately experiments provide little information benefits preemptive discard policy 
discussed section hive firewall management policy leaves pages unprotected running pmake workload little chance wild write successfully corrupt memory 
rate wild writes high create significant risk data corruption workloads heavy shared pages drive system significant intercell load balancing preemptive discard appears necessary 
latency detect errors reliability experiments provide approximate timing information system model section accurate 
estimate latency closely instruction corruption experiments repeated accurate system model 
model experiments repeated chosen reduce simulation time requirements 
system model system model performance experiments modified bus memory system mb sec nsec access latency memory request reaches head memory controller queue 
memory system model faster simulate flash model 
experiments repeated experiments repeated chosen randomly cell fails hive recovers successfully approximate timing shows system enters recovery algorithm second fault injection 
second constraint eliminates experiments assumed fault cause software error long injection 
possible experiments eliminated represent long delays detect errors data shows strong tail second latency number incorrectly eliminated cases presumed low 
table shows distribution latencies failure detection repeated experiments 
latency measured time error occurs assumed time corrupted instruction executed time cells begun executing process 
experiments show latency msec surprising 
error causes cell panic detected cell reads alive field cell public area section msec clock interrupt expected latency detect summary errors msec 
experiments show latency msec 
common case triggers immediate failure detection corruption rpc request reply data error 
remaining experiments half randomly distributed msec msec half randomly distributed msec msec 
bias low range may related expected msec latency clock monitoring algorithm detect deadlock section 
times fast avoid data corruption due wild writes small system sizes fault error injection experiments 
unclear larger systems require lower detection latencies 
expected latency detect errors may drop interaction rate cells increases section aggressive error detection mechanisms hive certainly added system section 
error detection latency appear significant problem wild write defense 
summary cell isolation features hive novel mechanisms enable distributed system fault containment techniques inside shared memory multiprocessor 
careful protocol publisher lock safe read internal data cells 
firewall preemptive discard policy prevent damage due wild writes distributed agreement algorithm fast null recovery allow system efficiently detect correctly recover cell failures 
memory fault model defines separate responsibilities hardware system software recovering hardware failures 
experiments show mechanisms implement cell isolation high probability despite wide range faults errors 
mechanisms primary costs 
add overhead communication cells reduces efficiency operating system assume hardware support increases complexity cost machine 
chapter discusses hive reduces performance overhead cell boundaries shared memory chapter considers question hardware support detail 
table 
latency error cell enters recovery experiments 
percentile min max latency msec memory sharing chapter memory sharing fault containment provided described previous chapter multicellular kernel implemented way single system image distributed system 
environment runs differs key respect distributed systems multiprocessor provides shared memory hardware cells 
hive supports shared memory cell boundaries applications cells 
application level allows processes multiple cells share memory sharing load balances memory pressure cells physical level sharing 
kernel level cells read data structures build data structures cross kernel boundaries 
section chapter describes virtual memory system file system implement application level memory sharing 
second gives details experiments memory sharing kernel level 
application level memory sharing repeats chapter shows application level memory sharing cell boundaries 
roles cells play sharing page 
client cell cell running process remotely accessing data 
cell left 
memory home cell owns physical storage page 
cell right 
data home cell manages data stored page 
cell right cell left 
data home provides name resolution manages firewall page shared manages coherency data structures page replicated ensures page written back disk dirty 
prototype data home page cell application level memory sharing owns backing store page feature filesystem prototype requirement design 
start description memory sharing introducing virtual memory page cache design irix basis implementation 
discuss types memory sharing turn 
irix page cache design irix page frame paged memory managed entry table page frame data structures 
pfdat records logical page id data stored corresponding frame 
logical page id components tag offset 
tag identifies object logical page belongs 
file file system pages node copy write tree anonymous pages 
offset indicates logical page object linked hash table allows lookup logical page id 
physical level sharing page frames process address logical level sharing data pages 
application level memory sharing cell boundaries 
space process address space client cell memory data home data home memory home memory sharing page fault mapped file page occurs virtual memory system checks pfdat hash table 
data page requested process virtual memory system invokes read operation vnode object provided file system represent file 
file system allocates page frame fills requested data inserts pfdat hash table 
page fault handler virtual memory system restarts finds page hash table 
page fault finds page memory called quick fault 
quick faults frequent speed critical system performance 
read write system calls follow nearly path page faults 
system call dispatcher calls vnode object file 
file system checks pfdat hash table requested page order decide necessary 
logical level sharing hive cell needs access data page cached cell allocates new pfdat record logical page id physical address page 
dynamically allocated called extended 
extended pfdat allocated inserted pfdat hash table kernel modules operate page aware part memory belonging cell 
hive virtual memory system implements export import functions set binding page cell extended pfdat table 
functions frequently called part page fault processing 
page fault initially processed just distributed file systems 
virtual memory system checks pfdat hash table client cell 
data page requested process virtual memory system invokes read operation vnode file 
cell nfs shadow vnode indicates file remote 
cell nfs retrieves universal file identifier vnode sends rpc data home requesting page 
complete file system sophisticated mechanism mechanism implement function locate appropriate data home send rpc 
file system data home issues disk read data home vnode page cached 
page located data home hive functions differently distributed systems 
systems copy page returned client 
hive data home returns address page client 
application level memory sharing file system data home calls export page records client cell data home pfdat 
information prevents page deallocated provides information necessary failure recovery algorithms 
export function modifies firewall state page write access requested 
second file system data home returns address page client cell 
client side file system calls import allocates extended pfdat page frame inserts client cell pfdat hash table 
faults page hit quickly client cell hash table making rpc data home unnecessary 
page remains data home pfdat hash table allows processes cells find share 
illustrates state virtual memory data structures export import completed 
client cell eventually frees page virtual memory system calls release putting frame local free list 
release function frees extended pfdat sends rpc data home 
remain data home returns frame local free list 
keeping frame data home free list client free lists increases memory allocation flexibility data home 
data page remains cached frame page frame reallocated providing fast access client cell faults data page 
table 
virtual memory system interface memory sharing 
logical level record client cell accessing data page 
export client cell pfdat writable allocate extended pfdat bind remote page 
import page address data home logical page id writable free extended pfdat send rpc data home free page 
release pfdat physical level record client cell control page frame loan frame client cell pfdat allocate extended pfdat bind remote frame 
borrow frame page address free extended pfdat send free rpc memory home 
return frame pfdat memory sharing performance logical level sharing overhead logical level sharing mechanism measured comparing minimal cost quick fault hits client cell page cache goes remote hits data home page cache 
local case averages sec remote case averages sec measured microbenchmark issues remote quick faults succession 
table shows detailed breakdown remote page fault latency 
time component averaged faults microbenchmark 
sec remote case due rpc costs 
component time spent moving data shared memory sec substantially smaller flash measurement simulation model lockup free caches 
sec listed table miscellaneous vm due implementation structure inherited irix 
irix assumes client cell hash table result disk access optimize code path 
reorganizing code provide substantial reduction remote overhead 
complex workloads cost quick fault vary microbenchmark results 
local fault handler acquire locks experience synchronization delays 
remote faults may take longer encounter synchronization conditions data home require queued rpc server process 
quick faults remote exported pfdat imported pfdat vnode pfdat table cell cell exp regular pfdat extended pfdat shadow vnode imp exp imp 
logical level sharing data pages 
memory pages data home client cell application level memory sharing pages may take substantially time client cell discovers page cached pfdat hash table 
pmake cell hive configuration effects combine slowdown due remote fault latency microbenchmark suggests 
quick faults local pages slow due synchronization quick faults remote pages speed due cache hits 
measured seconds execution processors quick faults remote pages quick faults local pages average quick fault remote page takes sec average combining local remote faults sec 
compares average quick fault sec single cell case running pmake processors 
application pause time quick faults cumulative processors execution pmake seconds cell system seconds cell system 
times increase times increase suggested microbenchmark 
optimization possible desirable fundamental design appears fast provide performance competitive smp kernels shown results performance experiments described 
physical level sharing logical level design just described major constraint user pages memory data home page cache 
design constrained pages stored data home table 
components remote quick fault latency 
total local quick fault latency total remote quick fault latency sec sec client cell file system miscellaneous vm import page interrupt dispatch return data home file system locate export page rpc sips interrupts rpc dispatch stubs argument result copy shared memory memory sharing memory hive poor load balancing able place pages better locality processes required performance cc numa machine 
physical level sharing solves problem 
hive reuses extended pfdat mechanism enable memory home cell loan page frames cell data home 
memory home moves page frame reserved list ignores data home frees fails 
data home allocates extended pfdat subsequently manages frame 
frame usually demand driven memory allocator 
memory allocator receives request may decide local free memory low relative cells time borrow memory cell 
chooses donor sends rpc asking set frames 
borrowed frames acceptable frame allocation requests 
example frames allocated internal kernel local firewall defend wild writes hardware errors memory home 
memory allocator supports constraints new arguments irix set cells acceptable request cell preferred 
pfdat borrowed pfdat vnode regular pfdat extended pfdat memory pages pfdat table cell cell 
physical level sharing page frames 
data home memory home application level memory sharing hive current policy freeing borrowed frames similar policy releasing imported pages 
sends free message memory home soon data cached frame longer 
poor choice results immediately flushing data 
better approach maintain separate free lists local borrowed memory return borrowed frames memory home directed wax 
remote firewall management initial experience physical level sharing demonstrated performance problem design 
flash initially implemented straightforward rule memory home modify firewall status page requiring data home send rpc request memory home modify firewall borrowed pages 
interrupt handlers hive send rpcs section 
rule implied client cells interrupt level rpcs request data home operations need change firewall 
added significant process level rpc overheads performance critical virtual memory file system operations run interrupt level 
flash modified implement rule allows data home manage firewall directly node may modify firewall state write permission page 
memory home initially write permission data home page 
rule create new wild write vulnerabilities 
consider possible cases data home sets firewall incorrectly case simpler rule 
memory home sets firewall incorrectly case simpler rule 
cell write permission attempts write firewall case simpler rule 
cell write permission write permission cell failure initial faulty cell eventually detected page discarded failed cell write permission 
additional wild writes cells problem 
firewall reset full recovery runs side effect removing remote mappings implement preemptive discard section 
multicellular kernels implement different preemptive discard policies section erroneous firewall issue 
avoid problems cause data home preemptive discard scan union set recorded vm data structures set recorded actual firewall determine cells page vulnerable 
erroneous cell memory sharing overwrite firewall page wild write occur third cell times data corruption survive preemptive discard 
cell write permission denies write permission cell cases consider 
erroneous cell deny write permission potentially causing data home preemptive discard scan incorrectly preserve page failure erroneous cell detected 
set data home internal vm data structures actual firewall state preemptive discard scan avoids problem 
second erroneous cell deny client cell write permission causing receive bus error 
recovery algorithms conclude erroneous cell failed preemptive discard run incorrect firewall state cleaned 
cell received bus error try receive bus error panic hint unconfirmed cause full recovery cleans firewall 
possible prevent panic innocent cell type corruption validating actual firewall state page causing bus error matches vm data structures data home 
erroneous cell deny data home write permission 
data home send rpc memory home requesting firewall reset occurs 
prototype implement complex mechanisms just proposed firewall corruption observed fault error injection experiments 
logical physical interactions general types application level memory sharing operate independently concurrently 
frame simultaneously borrowed exported data home excessive memory pressure caches pages borrowed frames 
interestingly frame simultaneously imported back memory home 
occurs frequently data home tries place page memory client cell faulted order improve cc numa locality 
support cc numa optimization efficiently virtual memory system reuses preexisting pfdat allocating extended pfdat page 
pfdat reuse possible logical level physical level state machines separate storage pfdat 
kernel level memory sharing memory sharing fault containment memory sharing allows corrupt cell damage user level processes running cells 
implications system page allocation migration policies sensitive number location borrowed pages allocated process 
pages allocated randomly longrunning process gradually accumulate dependencies large number cells 
user tunable policy needed trades optimizing page allocation migration minimizing number cells process depends 
generation number strategy preemptive discard file unit data loss cell fails 
page allocation migration policies sensitive number different cells memory homes dirty pages file 
requires similar user tunable policies 
tradeoffs page allocation fault containment performance complex 
subject studied depth flash hardware available large workloads examined 
summary application level memory sharing key organizing principle application level memory sharing distinction logical physical levels 
cell imports logical page gains right access data stored memory 
cell borrows physical page frame gains control frame 
extended cases allow kernel operate remote page local page 
naming location transparency provided file system 
operations memory sharing subsystem cell request return page page frame 
information available cell sufficient decide local memory requests higher lower priority remote processes pages 
information eventually provided wax direct virtual memory clock hand process running cell preferentially free pages memory home memory pressure 
kernel level memory sharing addition enabling application level sharing shared memory hardware offers promise significantly improving efficiency operating system 
section reports memory sharing experiments give insight advantages costs kernel level memory sharing 
experiments implementation cell public area remote process creation mechanism anonymous memory manager hive 
experiments moderately negative results 
cell public area works remote process creation anonymous memory manager show significant benefits shared memory 
section discuss ways kernel level shared memory important mechanisms implemented prototype 
cell public area cell public area data structure cell kernel memory fault containment subsystem publish information state beliefs cell 
key fields struct publ lock publisher lock unsigned int tick increments clock interrupt int alive set indicate cell ls current belief live cells int current generation number tick field supports clock monitoring algorithm part failure detection 
alive field set cell finished booting ready integrate system 
cell resets alive field notifying cells unambiguous way panic sending messages 
approach advantage cell repeatedly clog receive queues cells multiple panic messages 
ls fields allow consistency checks cell verify efficiently agreement succeeded live cells system 
agreement fails cell incorrect system prone significant corruption shut immediately known split brain syndrome sis 
evaluation straightforward integrate information dissemination shared memory publisher lock careful protocol design fault containment subsystem 
cases significantly simplified implementation alive field 
design challenge method cell locate public area cells 
known offset cell memory needed cell publish addresses data structures public area cells need read 
hive cells run kernel image loaded offset cell known offset kernel level memory sharing hive start data page kernel boot image 
multicellular kernels kernel boot images vary need different mechanism 
remote process creation process creation fairly complex task access parent process data structures primarily read 
suggests kernel level shared memory simplify remote process creation 
particular possible local process creation code reused remote process creation 
hive implements remote process creation remote reads parent data structures order test hypothesis 
experiment shows local code reused remote case operation 
implementation remote process creation proceeds steps 
describe detail illustrate complexities kernel level shared memory 
parent cell process requests fork executes child cell new process created 
parent prepares fork parent cell chooses child cell process creation allocates data structure represent child process parent process list children 
child initializes process table parent cell sends child cell pointer parent process table entry 
child cell uses careful protocol read associated credentials user area structures 
child cell uses information allocate initialize child process table entry associated structures 
child cell returns child process id parent cell 
aspects step slower equivalent portion local fork parent process table user areas copied directly child process 
child cell initialize process table entry user area copy data parent hardware failure parent cell operation cause bus error immediate jump careful error handler 
recovery runs process table cleanup routines behave unpredictably stale uninitialized information fields allocated process table entry 
child cell validate far possible values read parent 
value fails sanity check causes process creation attempt abort recovery subsystem notified parent cell may corrupt 
memory sharing third difference local process table initialization algorithm child cell copy kernel stack registers parent process child process parent process contains stack frames procedures parent cell child cell 
child process needs local kernel stack registers 
local user level fork system call cell extra copy parent process stack registers remote process creation 
parent identifies files locks address space child cell returns initializing process table entry parent cell iterates open files parent process storing universal file identifiers section array child cell read 
parent cell locks region structures parent process address space set address space modifications required forking process 
remainder modifications child step runs returns necessary information 
primary overhead added step leaves region structures locked steps 
processes parent cell share structures stall operations address spaces acquire region lock 
structures left locked child cell access directly shared memory building copy parent process address space child process 
child initializes files address space child mirrors parent previous step 
files universal file identifiers 
child cell uses careful protocol read address space data structures parent cell initialize child process address space 
step includes operations potentially send rpcs cells 
examples include files importing cached file swap pages allocating pages child cell 
deadlock easily occur stage 
failure occurs recovery begins recovery parent cell stall trying access locked regions parent process address space 
server process child cell block waiting rpc replies operation delayed recovery 
support operations requested cells step designed fail back client recovery begins 
child cell detects recovery begun aborts undoes changes step 
returning parent parent cell undoes changes previous step releases region locks gives processor recovery process run goes back start step 
kernel level memory sharing parent address space parent finishes parent side address space modifications releases region locks inserts child process appropriate process group console session 
parent requests process group session modifications logically belongs child parent cell frequently home cell objects 
step parent cell notifies child cell fork completed child cell adds new child process run queue 
evaluation little local process creation code survives unmodified remote process creation 
key problems cell isolation failure semantics 
cell prepared fail run resource limitations time requires careful attention mechanisms undoing partially completed fork 
deadlock problem child files address space step issue required significant code modification 
approach remote process creation desirable fast 
quite slow current prototype primarily attempt minimize code changes led files importing pages batching multiple requests file system virtual memory system 
averaging times creation child processes ocean benchmark local case takes msec remote case takes msec 
msec increase msec comes separate file msec comes separate page imports sends rpc parent cell 
performance problems fixed require substantial code modifications msec remaining msec slowdown rpc overhead process creation algorithm 
overhead comes need go back forth parent child cells order allocate lock modify data structures appropriate points algorithm 
caveat remote process creation implementation prototype tuned performance appears kernel level shared memory gives significant advantages remote process creation 
sufficient code modifications necessary simpler just fast traditional distributed system approach parent cell packages relevant state sends child cell single rpc 
anonymous memory manager third experiment kernel level shared memory anonymous memory manager 
represents aggressive shared memory hive manager builds doubly memory sharing linked tree links may cross cell boundaries 
experiment negative result workloads studied implementation complexity significant performance improvements 
anonymous memory set user data pages backing store swap partition file system 
anonymous memory manager responsible determining page satisfy page fault swap region 
irix anonymous manager tracks swap pages copy write tree similar mach approach 
anonymous page allocated process writes page address space shared copy write parent 
new page recorded current leaf copy write tree 
process forks leaf node tree split new nodes assigned parent child 
pages written parent process fork recorded new leaf node anonymous pages allocated fork visible child 
process faults copy write page searches tree find copy created nearest ancestor wrote page forking 
hive different processes different cells anonymous manager determine ancestor wrote page cell owns page 
implementation hive keeps tree structure inherited irix nearly intact allows pointers tree cross cell boundaries 
process forks leaf node created child process local process higher nodes tree may remote 
create wild write vulnerability information newly written pages recorded leaves tree interior nodes modified remain protected firewall 
child process faults shared page searches tree potentially careful protocol read kernel memory cells 
finds page recorded remote node tree sends rpc cell owns node requesting access page 
cell owns node data home anonymous page 
implementation details remote case complicated local case reduce performance 
recovery run cell remote lookup operations progress 
recovery may result deleting interior nodes cause pending lookups see stale data 
kernel level memory sharing 
anonymous memory manager data structures 
segments parent address space memory objects copy write tree handles identify pages 
handle labeled offset memory object 
handle enables finding page cached disk 
data structures process creation 
data structures remote process creation 
parent address space child address space parent cell child cell anon fringe list anon fringe list parent address space child address space data structures local process creation 
segments segments anon nodes anon nodes segments segments memory sharing avoid problem lookup routine acquires lock delays recovery lookup completed 
allow recovery routine find dangling failed cells nodes remote parents children linked list called anon fringe list 
managing list requires synchronization operations modify anonymous trees 
local case process exits removes mapping memory object corresponding anonymous tree collapsed 
frees kernel memory tree swap space pages records 
nodes remote children easily collapsed address node stored child data structures 
example node address component hash key look pages child file cache 
tree collapsing delayed child processes exited resulting greater memory swap space consumption 
kernel virtual memory addresses cell translated directly 
hive avoids problem physical addresses links anonymous trees partial solution 
multicellular kernels may run cell virtual address space improve fault containment new mechanisms needed translate remote virtual addresses 
evaluation fact anonymous memory manager appears reliably fault error injection experiments described previous chapter indicates distributed data structures built weakening fault containment 
stress anonymous memory manager directly additional kernel heap corruption experiments performed raytrace workload uses anonymous memory heavily 
nodes pointers anonymous trees corrupted pathological ways just remote lookups trees 
cell doing lookup successfully defended experiments 
appear substantial performance benefit distributed data structures 
child finds desired page usually send rpc bind page case shared memory save time tree spans multiple cells 
trees span multiple cells rare created process forks multiple times forks go remote child processes exec start new program 
conventional rpc implementation anonymous memory management simpler probably just fast 
kernel level memory sharing summary kernel level memory sharing prototype gives moderately negative results memory sharing experiments 
positive side memory sharing quite useful information dissemination cell public area 
similar results obtained memory load balancing algorithms prototype overcomes lack wax reading memory usage information remotely 
suggests forms information publishing enhance performance 
example cells publish translations internal virtual addresses making efficient remote cells walk virtual pointer chains long virtual translations remotely read data structures valid minimum time durations implemented techniques type stable memory management 
contrast aggressive experiments kernel level memory sharing appears successful 
remote process creation subsystem reuse local process creation code anonymous memory manager average save significant time 
kernel level shared memory may turn important areas explored prototype spanning tasks file system 
communication latency problem spanning tasks tight coupling component processes separate cells 
shared memory may key mechanism spanning tasks efficient 
may possible writes cell boundaries update spanning task data structures essential dependencies rule means loss data structures cell failure acceptable section 
similarly file system may able information publishing reduce number rpcs sends helping achieve execution efficiency competitive file system smp kernel 
system performance chapter system performance chapter concludes description hive implementation analyzing results performance experiments 
section characterizes performance prototype second considers implications trends observed prototype 
discussed earlier results show trends indicate possible behavior multicellular kernels larger systems allow definitive small system sizes studied limited feature set prototype 
performance characterization compares hive performance various configurations irix running simulated processor flash multiprocessor 
workloads axis shows performance irix labeled compared hive running cells 
axis gives wallclock time height bar gives performance 
bar sections show fraction time spent corresponding mode 
processors time spent mode execution workload times height corresponding section axis 
shows interesting trends 
performance hive cells competitive irix pmake ocean slightly degraded raytrace 
performance amount kernel increases number cells increases kernel overhead cache misses synchronization rpc costs drops significantly 
time spent executing applications unchanged pmake increases increasing number cells raytrace ocean 
idle time remains constant raytrace decreasing table 
wallclock time completion seconds 
workload irix cell cells cells cells pmake raytrace ocean performance characterization moderately pmake significantly ocean 
sections discuss trends kernel kernel overhead application time idle time detail 
increasing kernel number hive cells increases amount done operating system support workload increases 
time labeled kernel exec 
time completion workloads 
idle user kernel kernel exec seconds pmake raytrace ocean system performance computed total non idle time kernel minus time spent stalled cache misses spinning kernel locks spinning waiting rpc result 
big jump workloads transition cell cells rpc overheads software error checks required fault containment added system 
jump useful kernel continues increase total time spent kernel decreases significantly 
total time spent kernel decreases system grows cells overhead kernel execution drops 
decreasing kernel overheads kernel overhead time labeled kernel computed total time spent stalled kernel cache misses spinning kernel locks spinning waiting rpc replies 
numbers shown tables computed overhead time divided useful time shown table 
times added useful time components 
cache stall overheads drop dramatically number cells increases 
table 
total useful kernel time seconds 
workload irix cell cells cells cells pmake raytrace ocean table 
total non idle kernel time seconds 
workload irix cell cells cells cells pmake raytrace ocean table 
kernel time stalled cache misses percent useful kernel time 
workload irix cell cells cells cells pmake raytrace ocean performance characterization surprising partition cells reduces memory system costs simultaneously 
increases fraction kernel accesses go local data structures reducing expected latency 
spreads kernel data accesses nodes reducing queuing delays magic 
reduces number processors sharing data structure reduces number communication misses 
fewer processors cell synchronization overheads decrease number cells increases 
increase irix single cell hive configuration visible pmake ocean due increased synchronization added page fault handlers support systems 
overhead cost time spent waiting rpc replies 
general goes number cells increases decreasing fraction kernel operations completed locally 
overhead drops pmake large savings cache synchronization time number cells increases 
latency required server cells handle rpcs drops faster number rpcs increases 
rpc overhead ocean large application generates quick faults little kernel cells cell spend kernel time rpc waits 
table 
kernel time spinning locks percent useful kernel time 
workload irix cell cells cells cells pmake raytrace ocean table 
kernel time waiting rpcs percent useful kernel time 
workload irix cell cells cells cells pmake raytrace ocean system performance increasing user time surprisingly absolute time spent user mode parallel scientific workloads increases noticeably number cells increases 
application time increase raytrace ocean due effect 
multiple processes applications tightly synchronized 
process slow reaching barrier interrupted holding lock processes spin waiting 
initial jump application time cells occurs increased latency kernel operations processes cell opposed processes cell 
example kernel operation consumes time raytrace running cells quick fault averages sec cell sec cell 
subsequent increases application time number cells increase due increasing skew kernel execution cell 
cell data home memory segment shared processes services vm operations requested processes cells 
leads higher frequency kernel operations cell higher tlb cache rate caused increased kernel activity slow application process running cell 
effect appear pmake separate compile processes synchronize 
table 
total application time seconds 
workload irix cell cells cells cells pmake raytrace ocean table 
frequent kernel operations cell hive running raytrace 
cell average cells operation number avg cycles number avg cycles tlb release table dispatch queued rpc primarily quick faults evaluation decreasing idle time pmake shows moderate decrease idle time raytrace shows effect ocean shows significant decrease number cells increases 
pmake file creations deletions requires synchronous metadata update initial idle time due disk waits 
decrease idle time occurs system overlaps additional kernel overhead generated multiple cells disk wait time 
decrease idle time ocean strange behavior shows table side effect irix user level spinlock library 
execution trace ocean system shows period execution seconds seconds wallclock little application done 
poor behavior occurs application threads spin user level lock master process initializes computation 
spinlock library issues system call process thread fails acquire lock 
multiple threads system call repeatedly conflict internal lock process scheduler serialize system 
cell hive configurations avoid scheduler lock conflict reducing amount idle time 
evaluation performance trends suggest possible behavior multicellular kernels larger systems 
trend increasing kernel increasing numbers cells agrees intuition fundamental overheads caused replicating kernels 
small increases useful time cells suggests rate increase small low stress levels caused workloads 
performance costs mitigated second trend significant decrease kernel overhead occurs system partitioned increasing number cells 
exactly type benefit multicellular architecture designed provide 
table 
total idle time seconds 
workload irix cell cells cells cells pmake raytrace ocean system performance transition cells provides little reduction overhead 
little change memory system synchronization behavior switching processors cell processors cell level parallelization data structure replication current irix implementation 
explains pmake performs worst cell case system pay added costs crossing cell boundaries user kernel kernel exec time seconds percent execution time user kernel kernel exec time seconds percent execution time 
execution trace ocean 
running cell running cells evaluation replicating gaining benefits reduced overhead 
benefits appear number processors cell drops 
fundamental reason flash processors cell better 
uniprocessor kernel substantially efficient multiprocessor kernel 
example eliminate communication cache misses spin lock costs avoiding cc numa remote cache latencies local pages 
residual lock spin time cells shown table overhead acquiring releasing locks remains kernel recompiled uniprocessor mode 
interesting question explore larger machines benefits continue provide performance advantages system configured uniprocessor cells reduced load balancing efficiency increased kernel resource utilization uniprocessor cells create costs justify larger cells 
idle time behavior ocean interesting 
developers irix user level synchronization library added system call optimize performance 
turned interact lock scheduler driven certain workloads 
multicellular kernel provides systematic parallelization scheduler eliminates pathological case 
example design benefits multicellular kernel intended provide 
magnitude effect scientific applications surprise 
increase total kernel time raytrace cell cells minimal skew operation latency interrupt frequency leads substantial increases application run time 
implications design file system virtual memory system 
able distribute processing load evenly cells cases activity going file 
architectural evaluation chapter architectural evaluation having finished description evaluation hive prototype return larger question costs benefits multicellular architecture 
consider question levels 
multicellular kernel possible tradeoffs performance reliability implementation complexity designing hive 
section considers hardware support required multicellular kernel section considers improvements functionality implemented hive 
second large scale multiprocessor system designer choose invest multicellular kernel scale existing smp kernel 
section compares multicellular kernels smp kernels operating systems large scale multiprocessors 
fundamental questions multicellular architecture 
section summarizes limitations architecture section lists open questions answered dissertation 
hardware support major costs hive design nonstandard hardware support assumes multiprocessor runs 
property hive particular general property multicellular kernels 
summary flash hardware various hardware features added flash support hive described dissertation hardware fault containment memory system network data link level physical design flash support memory fault model section section section 
includes significant amounts microcode careful physical design small amount custom logic 
routers contain logic dedicated recovering hardware errors considered custom designed knowledge hive memory fault model gal 
hardware support software fault containment memory system provides firewall section section section 
firewall includes small amount custom logic plus data storage bits kilobyte page memory overhead 
communication memory system provides sips primitive support hive rpcs section 
implemented microcode 
miscellaneous flash provides remap region support running multiple cells section implemented custom logic 
provides physical level support issuing hardware reset cell time 
features add design cost small amounts manufacturing cost 
performance costs firewall check slows cache misses request exclusive copies cell boundaries 
assertions protocol microcode increase latency operations 
reserving virtual lanes interconnect network level recovery section prevents congestion reduction additional communication primitives 
costs significant 
firewall impact measured application performance section 
microcode assertions omitted performance critical operations section 
lanes reserved recovery congestion reduction flash coherence protocol relies order delivery network packets 
potential performance benefits lanes communication primitives unknown 
reducing hardware requirements reliability features added flash fall categories support hardware fault containment support software fault containment 
leaving types features turn generates reasonable design point hardware support flash 
hive flash called hardware fault containment design point hardware supported software fault containment multiprocessor provides firewall reliability communication primitive performance supporting multicellular kernel software reliability performance characteristics hive hardware fault containment 
eliminates hardware architectural evaluation complexity added flash support hive provide significant reliability benefits small medium system sizes hardware error rate acceptably low 
software fault containment multiprocessor provides custom hardware communication primitive performance 
requires running trusted microkernel monitor controls access physical memory provides firewall functionality support multicellular kernel 
approach reliability cost software error microkernel monitor cause system failure performance cost due increases tlb rate interrupt dispatch latency 
design points represent different ways support multicellular kernel trade hardware complexity reliability characteristics performance multiprocessor 
flexibility suggests hardware requirements fundamental barrier usefulness multicellular architecture 
additional operating system functionality hive relatively simple multicellular kernel 
ways significant additional functionality added design changing fundamental architecture 
include support heterogeneity administrative partitions integration distributed system clustering mechanisms 
heterogeneity hive assumes cells run kernel code 
multicellular kernels may wish allow greater heterogeneity reasons incremental software upgrades problem current multiprocessors entire machine restarted upgrade operating system software 
multicellular kernels support kernel software upgrades transparent applications mechanisms transparent hardware maintenance section 
upgrade progress different cells running different versions kernel code 
incremental hardware upgrades main advantages large multiprocessors smooth growth path customers starting smaller machines applications grow time 
economical customer able continue running older processors boards new components added system 
suggests different cells able run different generations hardware 
hardware may vary sufficiently different kernel code different cells desirable necessary 
key problem allowing heterogenous cells different versions kernel software may different data structure layouts making complicated cell read comparison smp kernels internal state 
problem solved limiting data structures read remotely invariant versions implementing self describing data structures object oriented design scheme revisions data structures subclasses earlier revisions 
combination mechanisms may required achieve complete solution 
administrative partitions hive follows unix tradition assuming resources machine form common pool usable equally process 
large multiprocessors expensive may shared multiple administrative domains may desirable partition resource pool administratively 
example department contributes machine cost may wish guarantee applications allocated hardware resources machine loaded 
suggests advanced accounting prioritization mechanisms may helpful large multiprocessors succeed commercial environments 
wax provides natural place implement functionality 
integration clustering hive assumes distributed single system image presents applications users ends boundaries multiprocessor 
natural integrate multiprocessors running multicellular kernels cluster machines single system image 
open question mechanisms required implement single system image shared inter cell inter machine cases 
comparison smp kernels clear multicellular kernel fundamental reliability advantages compared smp kernel 
cellular structure confines effects errors limited types reliability mechanisms added smp kernel recover specific types errors designed 
tradeoffs scalability implementation complexity multicellular smp kernels require discussion 
scalability multicellular kernels smp kernels face completely different scalability challenges 
multicellular kernel faces performance challenges number cells increases potential uneven distribution kernel cells increases hurt application performance described section resource sharing management mechanisms improved distribute evenly 
architectural evaluation rpc delays potential congestion server cells increase amount state cacheable client cells accessible shared memory steadily increased 
simple distinction cell local remote hardware resources sufficient achieve locality resource access higher level unit containing multiple cells needs identified resource allocation 
smp kernel faces different challenges number processors increases cache lines widely shared data structures memory system hotspots data structures partitioned components widely write shared reduce memory system costs 
kernel locks contended cause significant queuing delays data structures algorithms improved finer grained locking 
locality resource allocation process scheduling improved reduce memory system costs incurred applications 
key difference sets challenges number potential trouble spots 
smp kernel data structure kernel lock potential trouble spot higher number problems consider multicellular kernel operations cross cell boundaries potential scalability limitations 
multicellular kernel naturally exploits memory system locality cc numa multiprocessor modifying smp kernel exploit locality requires significant effort 
factors argue multicellular kernels easily scalable smp kernels 
complexity multicellular kernel undoubtedly complex smp kernel 
primary complexity differences lie areas 
extensions various kernel subsystems required implement single system image 
require creative implementation reduce overheads substantial new functionality preserve correct semantics despite potential failure 
second mechanisms policy resource sharing cells 
achieving sufficient reliability performance file system requires significant design 
relative simplicity smp kernels may lost extended improve reliability scalability large multiprocessors 
assuming software faults remain system software errors tolerated improving reliability requires adding code checks repairs internal data structures tandem nonstop ux kernel 
done thoroughly approach promises add comparable greater complexity limitations architecture integrating data structures distributed single system image disadvantage specific errors checked tolerated 
difficult judge impact improving performance scalability smp kernel 
improving scalability simple workload increase complexity substantially primary synchronization communication problems usually lie data structures algorithms improved affecting system 
goal efficient performance complex diverse workloads kernel data structures algorithms may substantially complex provide finer grained locking reduce communication cache misses 
changes reduce relative difference complexity smp kernel complex multicellular kernel 
multicellular architecture justifies complexity providing substantially better reliability implementable smp architecture making performance scalability easier achieve 
limitations architecture multicellular kernel architecture provides significant reliability scalability advantages faces fundamental limitations level reliability performance provide 
limitations reliability achievable architecture 
multicellular kernels provide reliability respect non fail fast hardware errors 
tightly coupled multiprocessor especially complicated memory system flash errors may probable distributed systems fault containment previously implemented 
multicellular kernels periodically pause user level processes distributed agreement protocol checks faults 
interruptions brief may acceptable systems tight scheduling deadlines video servers 
multicellular kernels provide complete protection wild writes 
system running feasible set software error detection mechanisms modern processor execute millions instructions error trigger internal assertion detected external observer 
possibility data corruption resulting errors function number pages unprotected firewall frequency data accessed applications 
architectural evaluation additionally fundamental limitations prevent multicellular kernel achieving performance competitive sufficiently parallelized smp kernel 
policy modules running wax loop allocation decisions inside cells respond slowly changes system state policy modules smp operating system 
policy delay reduces resource utilization efficiency 
resource utilization inefficiency policy delay imperfect resource sharing mechanisms leads unbalanced kernel workload cells 
increases run time tightly synchronized parallel applications effect discussed chapter 
open questions parts architecture strengths weaknesses evaluated hive prototype 
wax wax implemented necessary investigate rapidly respond changes system state running continuously wasting processor resources 
unknown closely level optimization architecture intercell decisions independently approximate resource management efficiency smp kernel 
resource sharing policies page migration intercell memory sharing effectively wide range workloads multicellular operating system viable replacement current smp operating system 
spanning tasks lightweight process migration implemented efficiency measured 
resource sharing policies systematically extended consider fault containment implications sharing decisions 
sophisticated statistical measures needed predict probability failures data integrity violations production operation 
networking efficient networking important requirement large multiprocessors 
new intercell sharing mechanisms necessary satisfy high bandwidth low latency demands applications web multimedia servers 
file system multicellular architecture requires fault tolerant high performance file system preserves single system semantics 
major design challenge prevent system achieving reliability scalability goals 
open questions questions remain open research reported dissertation step 
fundamental cell isolation resource sharing mechanisms shown providing fault containment roughly competitive performance necessary investigate higher level aspects architecture preserve advantages large scale systems 
related chapter related reliability scalability long major concerns researchers developers 
dependable operating systems primary source mechanisms improve reliability smp operating systems massively parallel computers distributed systems provides mechanisms improving scalability 
space constraints limit survey relevant problems large sharedmemory multiprocessors 
sections discuss systems techniques improving reliability scalability multiprocessors 
second survey tightly coupled distributed systems failure models multiprocessors 
improving smp kernels reliability techniques reducing rate impact software errors smp kernels 
techniques applied individual cells multicellular kernel improve reliability 
large area software engineering techniques 
object oriented designs choices cir spring improve modularity system adding strong internal interfaces operating system easier understand maintain fewer software faults time traditional monolithic kernel 
orthogonal approach partition system microkernel set services running independent address spaces making system easier test debug 
examples approach include hydra wlh mach chorus raa 
automated testing sah increase fraction faults fixed operating system development 
area techniques avoiding reboot required recover software errors standard smp kernels 
nonstop ux kernel tandem integrity ibm mvs xa system moa provide data structure specific repair routines invoked inconsistency detected 
approach implemented mvs xa fault tolerant mach rss abort operation progress error detected multiprocessor operating systems retry 
provides excellent analysis reasons aborted operation frequently succeeds retried 
significant improving reliability applications despite potential failure systems run 
common approaches include checkpointing lis lnp replication processes top microkernels lir acc 
scalability researchers investigated techniques required support generalpurpose workloads large scale shared memory multiprocessors machines available general purpose 
vdg investigate algorithms page replication migration processor scheduling reduce memory system costs 
chr characterizes performance bottlenecks smp kernel running cc numa system 
sdh describes new file system created silicon graphics support general purpose workloads access large amounts data 
multiprocessor operating systems multicellular architecture scalability benefits multicellular architecture multiprocessor operating systems investigated ongoing project university toronto calls design hierarchical clustering architecture 
initial hurricane operating system kri running hector multiprocessor vsl followed current tornado operating system running multiprocessor 
toronto researchers identify previous proposals improving scalability approaches similar multicellular structuring fer 
hurricane complete operating system implementation ideas previous proposals investigate reliability benefits architecture hive 
mechanisms hive parallel similar mechanisms hurricane tornado 
hurricane includes rpc subsystem inter kernel communication local representative page descriptors function hive extended home clusters files functions hive data home 
key differences 
hurricane tornado built ground multicellular kernels modified existing smp kernel implementations modular flexible 
example hurricane includes single system wide locking protocol interaction kernels regular 
second reliability issue boundaries related kernels flexible 
example different resources memory processors managed different cluster sizes simultaneously 
current tornado focuses efficient support applications data set size larger main memory providing predictable physical resource allocation applications compilers applications optimized predictable performance results 
techniques set scalability techniques developed hurricane tornado clearly complementary mechanisms developed improve reliability hive 
reliability large scale general purpose shared memory multiprocessor mmp developed carnegie mellon university 
previous general purpose multiprocessors burroughs ibm honeywell ran multics limited processors 
mmp scaled processors large perspective reliability issues low integration technology constructed 
developers necessary add novel reliability oriented features hydra operating system achieve adequate mtbf wlh 
hydra includes mechanisms tolerate hardware software faults 
hardware level watchdog mechanism detect fail processor faults resource exploration phase reboot time avoid bad memory pages 
tolerates certain non fail hardware faults including lost interrupts zeros ones corruption memory words 
software faults primary goal ensure system reboot quickly cleanly 
difficult virtual memory system integrated file system level object store integrity database recovered order reboot 
hydra improves reliability database techniques storing object identifier pointer provide redundant check deliberately setting counts high sensitive operations delaying deallocation objects counters reached zero 
techniques applied internal data structures modern smp kernel improve reliability respect software faults 
time mmp project active carnegie mellon university bbn built multiprocessor network switch arpanet kem 
general purpose multiprocessor notable focus fault containment implemented software reliability mechanism 
operating system partitions machine run separate instance operating system partition uses consensus mechanism integrate exclude partitions 
heuristic self checks run partition detect software hardware errors shut partition activated 
shows upper bound multiprocessor operating systems reliability achievable fault containment techniques operating system written scratch maximum fault containment modified existing smp kernel 
transaction processing multiprocessors built tandem set standard commercial high availability systems combining redundant fail fast hardware fault tolerant software 
tandem systems operating systems 
guardian system sis runs distributed system internal non shared memory multiprocessor 
separate kernels heartbeat protocol distributed consensus mechanism agree processors alive 
reliability respect software faults provided high number assertions shut kernel detects internal inconsistency 
tandem nonstop ux kernel runs uniprocessor unix kernel top redundant hardware 
redundant hardware masks hardware failures operating system detects software errors assertions recovers forward recovery routines 
system provides fault containment fault tolerance abstraction applications user process killed kernel state supports corrupted software error 
system reliability improved write protection implemented memory system tandem integrity coincidentally called firewall 
integrity firewall prevents malfunctioning device controllers modifying incorrect memory pages support operating system software error containment 
scalability addition university toronto research proposals scalable multicellular kernels described significant parallelizing unix performance small scale multiprocessors implementing unix semantics large scale non shared memory multicomputers 
mechanisms achieve efficient performance unix svr small scale parallel systems described spy pea cbb chs 
mechanisms sun microsystems versions unix svr described 
silicon graphics version described bab 
efforts focused achieving sufficient parallelization avoid synchronization bottlenecks processor systems 
largest scale systems constructed date provide standard smp semantics nonshared memory multicomputers 
mach smp kernel combined distributed system techniques locus create osf ad scales hundreds nodes 
operating system uses pure message passing communication token scheme transferring ownership kernel objects 
successfully manages hardware resources hundreds nodes designed support scientific related applications kernel requests provide scalable performance general purpose applications 
distributed systems multicellular architecture achieves reliability scalability applying distributed system techniques inside multiprocessor 
single system image locus pow pioneered single system image support distributed systems 
addition shared filesystem distributed process management demonstrates techniques coping network partitions 
locus initially commercialized aix tcf ibm wap portable ssi layer component osf ad 
versions include process migration distributed shared memory 
sprite ocd implements process migration high performance ssi distributed file system 
sprite includes mechanisms reducing recovery time distributed file system state bak applied reducing pause times required recover consistency distributed state multicellular kernel 
single system image distributed systems include mosix bgw chorus mix agh solaris mc 
solaris mc demonstrates techniques efficiently implementing single network identity separate kernels respect internal processes external clients 
resource sharing classic resource sharing problem distributed systems process migration implemented single system image systems 
condor lis provides checkpointing process migration top unix kernel changes describes process migration mechanism top mach transparent applications 
nut surveys systems provide migration 
high amount cost memory desktop workstations stimulated development systems memory systems local area network paging devices 
apollo domain early system implement functionality lld 
systems including cooperative caching gms implemented policies globally optimize file pages cached machines system 
techniques relevant multicellular kernels process migration mechanisms ability directly access remote pages shared memory fundamentally changes performance tradeoffs page placement decisions 
error model reliability prediction error diagnosis researchers working distributed system failure recovery encountered problems multicellular kernels solve order recover correctly cell failure 
key areas determining achieving consensus correct cells 
challenge determining classic system level diagnosis problem set test results cells testing compute set faulty cells 
theoretical problem surveyed dah les 
distributed implementations require separate fault free control processor execute diagnosis algorithm include dah bub 
algorithm hive simpler efficient known algorithms possible reduce null recovery latency chance incorrect diagnosis sophisticated approach 
diagnosis algorithm hive relies reliable broadcast achieve consensus new live set 
changing algorithm simple flood algorithm reliable broadcast improved terminate early cells failed 
mechanisms doing described 
hive diagnosis algorithm strong assumptions observability errors 
particular behave predictably respect software faults cause intermittent errors 
algorithms known tolerate wider range faults 
les surveys systemlevel diagnosis algorithms test results probabilistically correct cht considers general problem consensus voting weak failure detection possible 
error model reliability prediction field failure data studies errors affect systems field help design hive similar systems ways 
validate assumptions hive error model errors occur relative frequencies 
second support design sophisticated fault error injection studies possible predict reliability completed system accurately 
systematic efforts understand types errors affect operating systems started studies ibm mainframes dominant computing platform commercial 
ongoing study systems stanford produced bea vei moa 
studies confirm earlier ones key observations storage management addressing errors major immediate effects software errors accounting half entries system error logs recovery routines operating system successfully handle errors related half time exception timing errors rarely recoverable rate software errors closely linked intensity interactive workload system 
suc study ibm mainframes uses error logs study probability wild writes 
study finds significant operating system software faults cause wild writes wild writes occur field cause immediate addressing errors 
software faults potential cause data corruption 
study observes wild writes damage memory close intended target write type wild write causes data integrity problems hive 
tandem operating systems studied extensively intended environments high availability requirements 
lei lei examine guardian operating system examines nonstop ux operating system 
lei particularly interesting provide detailed information types locations programming mistakes operating system code 
lei notes timing errors significant single cause system failures field missing operations pointer variable initialization data update message send common type lowlevel programming mistake 
observes pointer manipulation errors missing checks illegal data values common low level mistakes nonstop ux 
relevant field failure data hive error model provided chc 
study attempts develop statistically valid error model field data large ibm operating system 
operating system software defects caused field failures year period study finds defects cause operating system incorrect address additional corrupt memory non deterministic way potentially cause wild write 
study breaks coarse categories joint distribution specific fault error types 
studies examine frequency types errors outside hive error model 
considers possibility non fail fast errors microprocessors concluding data integrity violation occurs month population processors built era technology 
suggests sophisticated data checking mechanisms may required large multiprocessors microprocessors incorporate internal redundancy integrity checks reduce error rate 
maf examines cases operating system begins babbling flooding network degrading performance system 
failure bypass recovery mechanisms error model reliability prediction hive 
cases caused installation new operating system software installation hardware network testing eliminated study cases babbling months observation network containing machines 
rate significant large multiprocessors partitioned minimally sized cells 
fault injection addition field failure data researchers fault injection studies examine detailed effects errors 
rio project reported cnc uses instruction corruption study probability data integrity violations due wild writes digital unix 
described section studies match results similar experiments hive quite closely 
kit reports results similar experiments sunos 
esprit project fault tolerant massively parallel systems supported error injection studies considered questions relevant improving reliability operating systems 
rms uses pin level error injection scm uses internal debugging features powerpc emulate errors architecturally hidden processor functional units 
studies demonstrate significant chance naive applications produce incorrect results errors injected system processors 
studies scientific workloads presumed spend time executing application code operating system code necessarily indicate application data integrity violations follow errors affect operating system 
reliability prediction papers discussed especially written advised iyer provide mathematical models assist predicting reliability failure data 
notable examples include lei lei tai tai 
chc considers design experiments reliability prediction fault injection studies statistically valid uses data system test unix network management system analyze validity various mathematical models 
ham provides requirements effectively predicting software reliability ham provides detailed methodology 
chapter goal research improve reliability scalability large multiprocessors general purpose compute servers 
contributions dissertation contributions goal 
demonstration fault containment possible inside shared memory multiprocessor 
fault injection experiments convincingly demonstrate multicellular kernel architecture provide fault containment multiprocessor 
best estimate available experiments dissertation hive failure rate random faults damage operating system 
far better failure rate smp operating system improved significantly testing debugging 
specification set hardware features flash generalizable multiprocessors sufficient support hardware software fault containment 
section summarizes feature set hive uses gives pointers precise specifications dissertation 
key features memory fault model supports hardware fault containment firewall supports software fault containment 
argue features added flash support hive necessary fault containment hive success fault containment simulation demonstrates sufficient 
demonstration cells take advantage shared memory hardware cell boundaries application kernel level preserving fault containment 
parallel workload running instruction corruption experiments stresses file system uses user level shared memory remote fork implementation uses kernel level shared memory 
experiments demonstrate types memory sharing weakening fault containment 
performance hive appears perform relatively research progressed point demonstrates multicellular architecture competitive smp operating systems 
particular small system sizes simulated limited functionality hive prototype suggest performance problems may remain undetected 
prototype lacks main features implemented performance results conclusive complete single system image wax spanning tasks advanced file system 
completing single system image cause performance problems networking subsystem 
missing features greater performance impacts 
wax function efficiently keep system balanced rapidly changing dynamic workloads 
spanning tasks require creative implementation shared memory extensively achieve reasonable performance 
significant missing features file system 
file system multicellular kernel provide globally shared namespace replication critical directories files striping software raid takeover dual ported disks backup cell primary cell fails 
tolerating loss cells system 
file systems nature just emerging research community adn require significant development widely 
implications multicellular kernels turn competitive performance system sizes commercially viable fault containment provide open larger markets multiprocessors available smp operating systems 
enable users benefit excellent resource sharing ease administration provided multiprocessors 
increasing sales volume machines multicellular kernels potential reduce cost give benefits users large multiprocessors 
fundamentally research demonstrates traditional assumptions inherent unreliability shared memory systems incorrect 
hive draws fault containment boundary inside shared memory boundary gains reliability sacrificing resource sharing 
regard hive part widespread research effort cnc gil wla reevaluate fundamental ways shared memory 
acc castellanos 
fault tolerant server mach 
nineteenth euromicro symposium microprogramming pp 
barcelona spain september 
available microprogramming vol 
september 
adn anderson dahlin neefe patterson roselli wang 
serverless network file systems 
acm transactions computer systems vol 
pp 
february 
agh armand herrmann rozier 
distributing unix brings back original virtues 
usenix workshop distributed multiprocessor systems pp 
fort lauderdale fl october 
berkeley ca usenix association 
ahmad 
semi distributed load balancing massively parallel multicomputer systems 
ieee transactions software engineering vol 
pp 
october 
bab barton 
scalable multi discipline multiple processor scheduling framework irix 
workshop job scheduling strategies parallel processing pp 
santa barbara ca april 
berlin germany springer verlag 
bak baker 
fast crash recovery distributed file systems 
doctoral dissertation technical report csd computer science division university california berkeley january 
bar bartlett 
nonstop kernel 
eighth acm symposium operating systems principles pp 
pacific grove ca december 
available operating systems review vol 
december 
bea 
statistical analysis failures slac computing center 
digest papers compcon spring pp 
san francisco february march 
new york ieee 
bgw barak wheeler 
mosix distributed operating system load balancing unix 
new york springer verlag 
bss black smith sears dean 
low latency messaging system distributed real time environments 
usenix annual technical conference pp 
san diego ca january 
berkeley ca usenix association 
bub 
distributed line diagnosis presence arbitrary faults 
third international symposium fault tolerant computing pp 
toulouse france june 
los alamitos ca ieee computer society press 
cas castillo siewiorek 
workload dependent software reliability prediction model 
twelfth international symposium fault tolerant computing pp 
santa monica ca june 
long beach ca ieee computer society 
cbb campbell barton browning curry davis edmonds holt slice smith 
parallelization unix system release winter usenix conference pp 
dallas tx january 
berkeley ca usenix association 
cbr chillarege rosenthal 
measurement failure rate widely distributed software 
fifth international symposium fault tolerant computing pp 
pasadena ca june 
los alamitos ca ieee computer society press 
chandra devine verghese gupta rosenblum 
scheduling page migration multiprocessor compute servers 
sixth international conference architectural support programming languages operating systems pp 
san jose ca october 
available sigplan notices vol 
november 
cheriton boyle 
paradigm highly scalable shared memory multicomputer architecture 
computer vol 
pp 
february 
chb chillarege 
identifying risk odc growth models 
fifth international symposium software reliability engineering pp 
monterey ca november 
los alamitos ca ieee computer society press 
chc chillarege 
generation error set emulates software faults field data 
sixth international symposium fault tolerant computing pp 
sendai japan june 
los alamitos ca ieee computer society press 
chr chapin herrod rosenblum gupta 
memory system performance unix cc numa multiprocessors 
acm sigmetrics joint international conference measurement modeling computer systems pp 
ottawa ontario canada may 
available performance evaluation review vol 
may 
chs campbell holt slice 
lock granularity tuning mechanisms svr mp 
symposium experiences distributed multiprocessor systems ii pp 
atlanta ga march 
berkeley ca usenix association 
cht chandra toueg 
unreliable failure detectors reliable distributed systems 
journal acm vol 
pp 
march 
cir campbell islam 
designing implementing choices object oriented system 
communications acm vol 
pp 
september 
cnc chen ng chandra aycock rajamani lowell 
rio file cache surviving operating system crashes 
seventh international conference architectural support programming languages operating systems pp 
cambridge ma october 
available operating systems review vol 
october 
jones 
operational availability large software telecommunications system 
third international symposium software reliability engineering pp 
research triangle park nc october 
los alamitos ca ieee computer society press 
dah 
efficient algorithm identifying fault set probabilistically system 
ieee transactions computers vol 
pp 
april 
dah 
system level diagnosis perspective third decade 
princeton workshop algorithm architecture technology issues models concurrent computation pp 
princeton nj september october 
available dickinson schwartz eds concurrent computations algorithms architecture technology 
new york plenum press 
dat data general 
numa delivering level commodity smp performance 
document viewpoint 
dahlin wang anderson patterson 
cooperative caching remote client memory improve file system performance 
usenix symposium operating systems design implementation pp 
monterey ca november 
berkeley ca usenix association 
ehrlich prasanna wu 
faults cause software failures implications software reliability engineering 
second international symposium software reliability engineering pp 
austin tx may 
los alamitos ca ieee computer society press 
kleiman barton faulkner smith stein weeks williams 
multiprocessing multithreading sunos kernel 
summer usenix conference pp 
san antonio tx june 
berkeley ca usenix association 
fer feitelson rudolph 
distributed hierarchical control parallel processing 
computer vol 
pp 
may 
feeley morgan karlin levy thekkath 
implementing global memory management workstation cluster 
fifteenth acm symposium operating systems principles pp 
copper mountain resort december 
available operating systems review vol 
december 
gal galles 
scalable pipelined interconnect distributed endpoint routing sgi spider chip 
hot interconnects symposium iv stanford ca august 
gil 
memory channel network pci 
ieee micro vol 
pp 
february 
gharachorloo lenoski laudon gibbons gupta hennessy 
memory consistency event ordering scalable shared memory multiprocessors 
seventeenth annual international symposium computer architecture pp 
seattle wa may 
los alamitos ca ieee computer society press 
gra gray 
census tandem system availability 
ieee transactions reliability vol 
pp 
october 
greenwald cheriton 
synergy non blocking synchronization operating system structure 
second usenix symposium operating systems design implementation pp 
seattle wa october 
berkeley ca usenix association 
ham hamlet 
testing true reliability 
ieee software vol 
pp 
july 
ham hamlet 
predicting dependability testing 
international symposium software testing analysis pp 
san diego ca january 
available sigsoft software engineering notes vol 
may 
huang 
techniques transient software error recovery 
hardware software architectures fault tolerance pp 
le mont saint michel france june 
berlin germany springer verlag 
horst lenoski 
risk data corruption microprocessor systems 
third international symposium fault tolerant computing pp 
toulouse france june 
los alamitos ca ieee computer society press 
heinrich baxter singh simoni gharachorloo horowitz gupta rosenblum hennessy 
performance impact flexibility stanford flash multiprocessor 
sixth international conference architectural support programming languages operating systems pp 
san jose ca october 
available sigplan notices vol 
november 
hp hp convex technology center 
convex scalable computing systems feature message passing shared memory programming flexibility 
press release november 
iyer 
experimental evaluation 
fifth international symposium fault tolerant computing special issue pp 
pasadena ca june 
los alamitos ca ieee computer society press 

integrity fault tolerant unix platform 
international symposium fault tolerant computing pp 
montreal quebec canada june 
los alamitos ca ieee computer society press 
joh johnson 
design analysis fault tolerant digital systems 
reading ma addison wesley 
khalidi shirriff 
solaris mc multicomputer os 
usenix annual technical conference pp 
san diego ca january 
berkeley ca usenix association 
kem mann roberts robinson wolf 
operational fault tolerant multiprocessor 
proceedings ieee vol 
pp 
october 
kit 
kao iyer tang 
fine fault injection monitoring environment tracing unix system behavior faults 
ieee transactions software engineering vol 
pp 
november 
koh heinrich simoni gharachorloo chapin baxter horowitz gupta rosenblum hennessy 
stanford flash multiprocessor 
annual international symposium computer architecture pp 
chicago april 
los alamitos ca ieee computer society press 
kri krieger 
hfs flexible file system shared memory multiprocessors 
doctoral dissertation department electrical computer engineering university toronto 
ktr kotz toh radhakrishnan 
detailed simulation hp disk drive 
technical report pcs tr dartmouth college 
lei lee iyer 
analysis software halts tandem guardian operating system 
third international symposium software reliability engineering pp 
research triangle park nc october 
los alamitos ca ieee computer society press 
lei lee iyer 
faults symptoms software fault tolerance tandem guardian operating system 
third international symposium fault tolerant computing pp 
toulouse france june 
los alamitos ca ieee computer society press 
les lee shin 
probabilistic diagnosis multiprocessor systems 
acm computing surveys vol 
pp 
march 
lir rozier 
fault tolerance enablers chorus microkernel 
hardware software architectures fault tolerance pp 
le mont saint michel france june 
berlin germany springer verlag 
lis litzkow solomon 
supporting checkpointing process migration outside unix kernel 
winter usenix conference pp 
san francisco january 
berkeley ca usenix association 
lld leach levine hamilton nelson 
architecture integrated local network 
ieee journal selected areas communications vol 
pp 
november 
lenoski laudon gharachorloo 
weber gupta hennessy horowitz lam 
stanford dash multiprocessor 
computer vol 
pp 
march 
lnp li naughton plank 
low latency concurrent checkpointing parallel programs 
ieee transactions parallel distributed systems vol 
pp 
august 
loc clapp 
sting cc numa computer system commercial marketplace 
third annual international symposium computer architecture pp 
philadelphia pa may 
available computer architecture news vol 
may 
lsp lamport shostak pease 
byzantine generals problem 
acm transactions programming languages systems vol 
pp 
july 
lyn lynch 
distributed algorithms 
san francisco morgan kaufmann 
maf maxion feather 
case study ethernet anomalies distributed computing environment 
ieee transactions reliability vol 
pp 
october 
mitchell gibbons hamilton kessler khalidi nelson powell 
overview spring system 
compcon spring pp 
san francisco february march 
los alamitos ca ieee computer society press 
moa mourad andrews 
reliability ibm mvs xa operating system 
ieee transactions software engineering vol 
se pp 
october 
milojicic 
task migration top mach microkernel 
usenix mach iii symposium pp 
santa fe nm april 
berkley ca usenix association 
nut 
brief survey systems providing process object migration facilities 
operating systems review vol 
pp 
october 
ocd ousterhout douglis nelson welch 
sprite network operating system 
computer vol 
pp 
february 
plank beck li 
transparent checkpointing unix 
usenix technical conference pp 
new orleans la january 
berkeley ca usenix association 
pea peacock 
file system multithreading system release mp 
summer usenix conference pp 
san antonio tx june 
berkeley ca usenix association 
parsons krieger stumm 
de clustering objects multiprocessor system software 
fourth international workshop object orientation operating systems pp 
lund sweden august 
los alamitos ca ieee computer society press 
pow popek walker eds 
locus distributed system architecture 
cambridge ma mit press 
raa rozier armand herrmann kaiser leonard 
chorus distributed operating systems 
computing systems vol 
pp 
fall 
rosenblum bugnion devine herrod 
simos machine simulator study complex computer systems 
acm press 
rosenblum herrod witchel gupta 
complete computer system simulation simos approach 
ieee parallel distributed technology systems applications vol 
pp 
winter 
rashid orr baron forin golub jones 
mach system software kernel 
compcon spring pp 
san francisco february march 
washington dc ieee computer society press 
rms rela madeira silva 
experimental evaluation fail silent behaviour programs consistency checks 
sixth international symposium fault tolerant computing pp 
sendai japan june 
los alamitos ca ieee computer society press 
rss russinovich segall siewiorek 
application transparent fault management fault tolerant mach 
third international symposium fault tolerant computing pp 
toulouse france june 
los alamitos ca ieee computer society press 
sah sankar hayes 
adl interface definition language specifying testing software 
acm workshop interface definition languages pp 
portland january 
available sigplan notices vol 
august 
scm silva carreira madeira costa moreira 
experimental assessment parallel systems 
sixth international symposium fault tolerant computing pp 
sendai japan june 
los alamitos ca ieee computer society press 
sdh sweeney hu anderson peck 
scalability xfs file system 
usenix annual technical conference pp 
san diego ca january 
berkeley ca usenix association 
sandberg goldberg kleiman walsh lyon 
design implementation sun network filesystem 
summer usenix conference pp 
portland june 
berkeley ca usenix association 
sil silicon graphics origin family technical enterprise servers 
press release october 
sis siewiorek 
reliable computer systems design evaluation second edition 
burlington ma digital press 
spy saxena peacock yang verma krishnan 
pitfalls multithreading svr streams processes 
winter usenix conference pp 
san diego ca january 
berkley ca usenix association 
suc sullivan chillarege 
software defects impact system availability study field failures operating systems 
international symposium fault tolerant computing pp 
montreal quebec canada june 
los alamitos ca ieee computer society press 
tai tang iyer 
analysis vax vms error logs multicomputer environments case study software dependability 
third international symposium software reliability engineering pp 
research triangle park nc october 
los alamitos ca ieee computer society press 
tai tang iyer 
analysis modeling correlated failures multicomputer systems 
ieee transactions computers vol 
pp 
may 
tai tang iyer 
dependability measurement modeling multicomputer system 
ieee transactions computers vol 
pp 
january 
baxter chapin rosenblum horowitz 
hardware fault containment scalable shared memory multiprocessors 
press 
tevanian 
architecture independent virtual memory management parallel distributed environments mach approach 
doctoral dissertation technical report cmu cs department computer science carnegie mellon university 
torrellas gupta hennessy 
characterizing caching synchronization performance multiprocessor operating system 
fifth international conference architectural support programming languages operating systems pp 
boston october 
available sigplan notices vol 
september 
thakur iyer young lee 
analysis failures tandem nonstop ux operating system 
sixth international symposium software reliability engineering pp 
toulouse france october 
los alamitos ca ieee computer society press 
krieger stumm 
experiences locking numa multiprocessor operating system kernel 
usenix symposium operating systems design implementation pp 
monterey ca november 
berkeley ca usenix association 
krieger stumm 
hierarchical clustering structure scalable multiprocessor operating system design 
journal supercomputing vol 
pp 

brown stumm krieger pereira sevcik 
multiprocessor 
unpublished white department electrical computer engineering university toronto june 
vdg verghese devine gupta rosenblum 
operating system support improving data locality cc numa compute servers 
seventh international conference architectural support programming languages operating systems pp 
cambridge ma october 
available operating systems review vol 
october 
von eicken basu vogels 
net user level network interface parallel distributed computing 
fifteenth acm symposium operating systems principles pp 
copper mountain resort december 
available operating systems review vol 
december 
vec von eicken culler goldstein schauser 
active messages mechanism integrated communication computation 
nineteenth annual international symposium computer architecture pp 
gold coast queensland australia may 
available computer architecture news vol 
may 
vei velardi iyer 
study software failures recovery mvs operating system 
ieee transactions computers vol 
july 
vsl stumm lewis white 
hector hierarchically structured shared memory multiprocessor 
computer vol 
pp 
january 
wap walker popek 
distributed unix transparency goals benefits tcf example 
winter conference january 
santa clara ca 
wir witchel rosenblum 
embra fast flexible machine simulation 
acm sigmetrics international conference measurement modeling computer systems pp 
philadelphia pa may 
available performance evaluation review vol 
may 
wla wahbe lucco anderson graham 
efficient software fault isolation 
fourteenth acm symposium operating systems principles pp 
nc december 
available operating systems review vol 
december 
wlh wulf levin harbison 
hydra mmp experimental computer system 
new york mcgraw hill 
woo singh gupta 
splash programs characterization methodological considerations 
second annual international symposium computer architecture pp 
santa margherita ligure italy june 
new york acm 
zhou 
processor pool scheduling large scale numa multiprocessors 
acm sigmetrics conference measurement modeling computer systems pp 
san diego ca may 
available performance evaluation review vol 
may 
roy black peak kemp lo verso barnett 
osf unix massively parallel multicomputers 
winter usenix conference pp 
san diego ca january 
berkley ca usenix association 
accounting active messages alpha annotations anon fringe list anonymous memory apollo babbling bbn binary compatibility block transfer booting bus error byzantine faults mmp cache memory architecture careful protocol cc numa cell cell isolation cell public area cell nfs challenge checkpointing choices chorus client cell clock monitoring clustering coma competitive performance condor conflict effect cooling failure cooperative caching copy write tree dash data corruption data general data home data mining databases deadlock decision support device drivers digital distributed agreement embra error containment defined model errors data link intermittent network level physical level software essential dependencies essential set extended pfdat failure failure unit hardware false alarms false sharing fast error detection fault containment goal stack fault containment defined fault tolerance fault defined fault tolerant mach file cache index index file system fine grained locking firewall cost protection changes general purpose multiprocessors general purpose workloads generation number gms graceful shutdown guardian hardware reset heartbeat hector heterogenous cells hewlett packard hierarchical clustering hurricane hydra ibm incremental hardware upgrades incremental software upgrades information dissemination integrity interconnect congestion link failure partitions interprocessor interrupts irix large sized systems load balancing local process locus logical page id logical level sharing mach magic instruction cache mainframes medium sized systems memory fault model memory home microcode see protocol microcode microkernel mips mipsy mosix mpi multimedia server multimedia servers mvs xa mxs naive applications network congestion networking nfs node halts nonstop ux null recovery ocean osf ad packet error correction loss retransmission packet retransmission page frame data structure see pfdat page migration page replication panic partial halt partitioned data structures pfdat physical level sharing pmake power failure preemptive discard preventive maintenance prioritization process management process migration process pairs processor sharing protocol microcode assertions errors protocol processor publisher lock pvm quick fault raytrace real time constraints receive posting optimization recovery data link level network level reintegration recovery round relaxed memory consistency reliability design defined remap region remote procedure call see rpc remote reads routing tables rpc interrupt level process level sanity checks scalability self checks sequent sharing short interprocessor sends see sips silicon graphics simos simulation single system image single system image distributed systems sips algorithm small scale multiprocessors smp cache stalls defined effects errors scalability smp kernels software fault containment solaris mc sophisticated applications spanning tasks spider splash spring sprite sun microsystems supercomputers supervisor mode swap pages symmetric multiprocessing see smp system failure system shared memory system streams system level diagnosis tandem timeouts tornado transaction processing truncated packets type tag type safe language uncached accesses failure net universal file identifier unix vaxcluster video servers virtual lanes virtual memory clock hand vms vnode wax web server web servers wild write windows nt 
