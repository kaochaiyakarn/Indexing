appear proceedings ieee workshop neural networks signal processing active learning weights rbf network kah kay sung niyogi artificial intelligence laboratory massachusetts institute technology cambridge ma usa describe principled strategy sample functions optimally function approximation tasks 
strategy works bayesian framework uses ideas optimal experiment design evaluate potential utility new data points 
consider application general framework active learning weight coefficients gaussian radial basis function rbf network 
derive sufficiency conditions learning problem analytical solutions data sampling procedure 
classical formulations learning examples data examples assumed randomly drawn learner 
case variety situations ranging network models rumelhart poggio girosi pac valiant frameworks classical pattern recognition 
sense learner passive recipient information target function 
contrast consider learner plays active role collecting examples 
judiciously selecting examples allowing possible random sampling active learning techniques conceivably faster learning rates better approximation results passive learning methods 
ideas optimal experiment design fedorov earlier sung niyogi formulated general procedure sampling unknown target function points domain 
consider application general framework encompass active learning coefficients radial basis function rbf network poggio girosi 
generally solution allows handle parameter estimation problems function classes linear parameters 
contributions 
application optimal experiment design framework estimating weights rbf type function classes analytical derivation optimal sampling strategy parameter estimation classes data 
represents significant improvement existing toy function classes step functions considered sung niyogi 
applied similar optimal experiment design framework estimating weights single perceptron unit 
best knowledge analysis done rbf type networks 

empirical comparisons data requirements active passive learning rbf type function classes 

development sufficiency conditions learning problem active data selection sequence analytically derived precomputed optimal experiment design framework 
provide brief statement problem solution active sampling strategy glimpse empirical simulations 
general formulation adopt bayesian formulation active example selection 
specifically pose problem follows dn ji ng set data points sampled unknown target function possibly presence noise 
class approximation functions prior probability pf regularization techniques approximate dn map sense means function want strategy determine input location sample data point xn order obtain best possible bayes optimal approximation unknown target function concept class approach active example selection problem stages 
define notion best possible maximum posteriori map approximation unknown target function 
propose optimality criterion evaluating solution approximates unknown target function 
active learning goal find solution best approximates unknown target function terms optimality criterion 

formalize mathematically task determining best input location sample 
express active learning optimality criterion cost function minimized 
task choosing data point minimizing cost function respect sample input location 
earlier cohn mackay tried similar optimal experiment design techniques collect data maximizes information unknown target function 
differs theirs respects 
different general optimality criterion evaluating solutions unknown target function measure function uncertainty incorporates bias variance components total output generalization error 
contrast mackay cohn consider variance component model parameter space 
second show active example selection strategy requires fewer training examples passive methods learn target function degree accuracy 
formulation establishing optimality criterion measuring quality approximation function respect unknown target 
recall active learning goal find solutions best approximate unknown target proposed optimality criterion 
optimality criterion derive scheme measuring new example utility terms example steers learner goal accompanying computational procedure selecting training example 
target function want estimate means approximation function target function known popular measure badly approximates integrated squared difference isd appropriate region interest ffi gamma dx function approximation tasks target unknown clearly express quality learning result terms compute expected integrated squared difference unknown target treating unknown target random variable approximation function concept class notice distribution simply posteriori likelihood dn data points seen far pf dn jg 
shall criterion evaluating quality approximation result unknown target function ffi ffi dg pf dn jg ffi dg gn approximation result data points dn learning goal minimize gn new reasonable active example selection strategy choose input location xn minimizes gn 
xn predict new results sampling data point follows suppose know target output value xn new data set dn dn xn yn new new estimate gn ef ffi new estimate derived new data set dn regularization 
reality know actual value derive conditional probability distribution jx dn jx df dn xn gjf df location sample xn compute expected value resulting measure jx dn yn ffi jd jx yn 
jx dn ef ffi yn dy clearly optimal location sample location minimizes cost function xn arg min xn jx dn important questions stage 
equations analytically solved yield specific sampling procedure 

sampling procedure allow learn target function fewer examples 
reduce sample complexity 
answer questions context estimating weight parameters radial basis function network 
particular able derive analytical solutions equations compare sample complexities active passive learning 
radial basis function networks function class consider class dimensional gaussian radial basis functions fixed centers ith basis function fixed covariance center coefficients learned denoted ak specifically arbitrary function class represented ga js exp gamma gamma gamma gamma priors class obtained putting normal distribution covariance sigma coefficients arbitrary sigma exp gamma sigma gamma learner access noisy data form dn ng unknown target function zero mean additive gaussian noise variance oe leads expression dn jg candidate form dn jg exp gamma gamma ga oe weight parameters map solution learning problem recovered follows cnb oe gk gamma sigma gamma sigma gamma sigma gamma theta matrix th element oe 
solution learner proposes basis data set irrespective data points selected 
provide active strategy selecting data optimally 
active sampling recall interested answering questions section 
rbf class considered possible show analytical expression exists equation cost function minimize yields optimal location sample jx dn jc cn form cn equation depends previous data sample locations fx ng weight priors sigma oe xn minimized xn get xn maximum utility location active learner sample unknown target function 

case active strategy outperforms passive 
investigate randomly generated target rbf functions fixed arbitrarily chosen centers 
priors weights provided sigma target rbf function collected data active strategy passive uniformly random strategy varying number data points 
shows mean error rate integrated squared difference equation actual target function map estimate averaged different target functions function number data points 
notice active strategy lower mean error rate passive number examples particularly true small number data 
left graph shows situation learner knowledge true priors right graph case learner incorrect prior assumptions true centers target rbf slightly different values learner assumes learner priors weights sigma different target class sigma 
despite incorrect prior assumptions active strategy outperforms passive case 
sufficiency conditions pre computing data sampling sequence noteworthy learning rbf weights new optimal sample location xn depend data values previously observed values sampled 
learner collect data points pre compute exact sequence points sample start receiving data target function 
behavior observed mackay active example selection strategy minimizes model parameter variance cost function 
cost functions class approximation functions linear model parameters exhibit behavior 
framework minimize output uncertainty cost function includes bias variance terms 
theorem provides sufficiency conditions learning problem active learning comparing active passive mean error rates learning unknown rbf target function fixed centers 
top learner uses priors model parameters process randomly generates unknown target functions 
bottom learner uses slightly different weight priors centers slightly displaced true centers 
leads data selection strategy depend previously observed data values 
theorem suppose class real valued functions parameterized basis data set dn ng map solution learning problem arg min 

conditions guarantee choice xn independent previously observed dn 
jd expressed gamma dn fx ng arbitrary function depend data dn words terms dn appear jd gamma dn fx ng 
linear parameters 

prior distribution model parameters support remarks extended previously developed bayesian framework active learning case learning weights rbf network 
derive specific sampling strategy show strategy allows learn fewer examples alternatively smaller error number examples random passive sampling 
application optimal experiment design paradigm function approximation bear promise design efficient learning algorithms 
cohn 
local approach optimal queries 
touretzky editor proc 
connectionist summer school san mateo ca 
morgan kaufmann publishers 
fedorov 
theory optimal experiments page 
academic press new york 
mackay 
bayesian methods adaptive models 
phd thesis california institute technology pasadena ca 
poggio girosi 
theory networks approximation learning 
technical report aim artificial intelligence laboratory massachusetts institute technology 
rumelhart mcclelland 
parallel distributed processing volume 
mit press cambridge massachusetts 

query construction entropy generalization neural network models 
physical review 
sung niyogi 
active learning function approximation 
advances neural information processings systems san mateo ca 
morgan kaufman 
valiant 
theory learnable 
proc 
stoc pages 
