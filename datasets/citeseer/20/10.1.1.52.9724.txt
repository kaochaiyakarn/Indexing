learning algorithm continually running fully recurrent neural networks ronald williams college computer science northeastern university boston massachusetts david zipser institute cognitive science university california san diego la jolla california appears neural computation pp 

exact form gradient learning algorithm completely recurrent networks running continually sampled time derived basis practical algorithms temporal supervised learning tasks 
algorithms advantage require precisely defined training interval operating network runs disadvantage require nonlocal communication network trained computationally expensive 
algorithms shown allow networks having recurrent connections learn complex tasks requiring retention information time periods having fixed indefinite length 
major problem connectionist theory develop learning algorithms tap full computational power neural networks 
progress feedforward networks attention turned developing algorithms networks recurrent connections important capabilities feedforward networks including attractor dynamics ability store information 
particular interest ability deal time varying input output natural temporal operation 
variety approaches learning networks recurrent connections proposed 
algorithms special case networks settle stable states regarded associative memory networks proposed hopfield lapedes farber almeida pineda rohwer forrest 
researchers focused learning algorithms general networks recurrent connections deal time varying input output nontrivial ways 
general framework problems laid rumelhart hinton williams unfolded recurrent network multilayer feedforward network grows layer time step 
call approach backpropagation time 
primary strengths generality corresponding weakness growing memory requirement arbitrarily long training sequence 
approaches training recurrent nets handle time varying input output suggested investigated jordan bachrach mozer elman cleeremans mcclelland robinson fallside stornetta hogg huberman gallant king pearlmutter 
approaches restricted architectures computationally limited approximations full backpropagation time computation 
approach propose enjoys generality backpropagation time approach suffering growing memory requirement arbitrarily long training sequences 
coincides approach suggested system identification literature mcbride narendra tuning parameters general dynamical systems 
bachrach mozer represents special cases algorithm robinson fallside alternative description full algorithm 
best knowledge investigators published account behavior algorithm unrestricted architectures 
learning algorithm variations basic algorithm network units external input lines 
denote tuple outputs units network time denote tuple external input signals network time concatenate form tuple denoting set indices output unit network set indices external input 
indices chosen correspond 
denote weight matrix network unique weight pair units input line unit 
adopting indexing convention just described incorporate weights single matrix 
allow unit bias weight simply include input lines input value 
follows discrete time formulation assume network consists entirely semilinear units straightforward extend approach continuous time forms differentiable unit computation 
kl denote net input kth unit time output time step unit squashing function 
system equations ranges constitute entire dynamics network values defined equation 
note external input time influence output unit time 
derive algorithm training network call temporal supervised learning task meaning certain units output values match specified target values specified times 
denote set indices exists specified target value output kth unit match time define time varying tuple gamma note formulation allows possibility target values specified different units different times 
set units considered visible time varying 
denote network error time moment assume network run starting time final time take objective minimization total error total trajectory 
gradient descent procedure adjusting negative total 
total error just sum errors individual time steps way compute gradient accumulating values time step trajectory 
weight change particular weight ij network written deltaw ij deltaw ij deltaw ij gammaff ij ff fixed positive learning rate 
gamma ij ij ij easily computed differentiating network dynamics equations yielding ij kl ij ffi ik ffi ik denotes kronecker delta 
assume initial state network functional dependence weights ij equations hold create dynamical system variables ij dynamics ij kl ij ffi ik initial conditions ij follows ij ij time step appropriate precise algorithm consists computing time step quantities ij equations discrepancies desired actual outputs compute weight changes deltaw ij ff ij correction applied weight ij net simply sum individual deltaw ij values time step trajectory 
case unit network uses logistic squashing function gamma equation 
real time recurrent learning algorithm derived assumption weights remained fixed trajectory 
order allow real time training behaviors indefinite duration useful relax assumption weight changes network running 
important advantage epoch boundaries need defined training network leading conceptual implementational simplification procedure 
algorithm simply increment weight ij amount deltaw ij equation time step accumulating values making weight changes time 
potential disadvantage real time procedure longer follows precise negative gradient total error trajectory 
exactly analogous commonly method training feedforward net making weight changes pattern presentation accumulating making net change complete cycle pattern presentation 
resulting algorithm longer guaranteed follow gradient total error practical differences slight versions nearly identical learning rate smaller 
severe potential consequence departure true gradient behavior realtime procedure training dynamics observed trajectory may depend variation weights caused learning algorithm viewed providing source negative feedback system 
avoid wants time scale weight changes slower time scale network operation meaning learning rate sufficiently small 
teacher forced real time recurrent learning interesting technique frequently dynamical supervised learning tasks jordan pineda replace actual output unit teacher signal subsequent computation behavior network value exists 
call technique teacher forcing 
dynamics teacher forced network training equations defined gamma 
equation 
derive learning algorithm situation differentiate dynamical equations respect ij time find ij gammat kl ij ffi ik ij teacher forced version alter learning algorithm dynamics ij values ij gammat kl ij ffi ik equation initial conditions 
note equation equation treat values ij zero computing ij 
teacher forced version algorithm essentially earlier simple alterations specified desired values place actual values compute activity network corresponding ij values set zero compute deltaw ij values 
computational features real time recurrent learning algorithms useful view indexed set quantities ij matrix rows corresponds weight network columns corresponds unit network 
looking update equations hard see general keep track values ij corresponding units receive teacher signal 
columns matrix 
weight ij trained happen example constrain network topology connection unit unit necessary compute value ij means matrix need row adaptable weight network having column unit 
minimal number ij values needed store update general network having units adjustable weights nr 
fully interconnected network units external input lines connection adaptable weight mn ij values 
simulation experiments tested algorithms tasks characterized requiring network learn configure stores important information computed input stream earlier times help determine output times 
words network required learn represent useful internal state accomplish tasks 
tasks described experiments run networks initially configured full interconnections units input line connected unit weights having small randomly chosen values 
units trained selected arbitrarily 
details simulations williams zipser 
pipelined xor task input lines carrying randomly selected bit time step 
unit network trained match teacher signal time consisting xor input values network time gamma tau computation delay tau chosen various experiments time steps 
units delay time steps network learns configure standard hidden unit multilayer network computing function 
longer delays units required network generally configures layers order match required delay 
teacher forcing task 
simple sequence recognition task units input lines 
input lines called lines serve special purpose serving distractors 
time step exactly input line carries carrying 
object selected unit network output immediately occurrence activity line activity line regardless intervening time span 
times unit output 
occurs corresponding considered time unit output new followed matching previous task performed feedforward network input comes tapped delay lines input stream 
solution consisting essentially flip flop gate readily version algorithm 
delayed sample task network remember cued input pattern compare subsequent input patterns outputting match don 
investigated simple version task network input lines 
line represents pattern set random cycle 
line cue set indicates corresponding bit pattern line remembered matching occurrence cue 
cue bit set randomly 
task elements common previous tasks involves internal computation xor appropriate bits requiring computation delay having requirement network retain indefinitely value cued pattern 
interesting features solutions version algorithm nature internal representation cued pattern 
single unit recruited act appropriate flip flop units performing required logic times dynamic distributed representation developed static pattern indicates stored bit 
learning turing machine elaborate tasks studied learning mimic finite state controller turing machine deciding tape marked arbitrary length string left right parentheses consists entirely sets balanced parentheses 
network observes actions finite state controller allowed observe states 
networks units learned task 
minimum size network learn task units 
learning oscillate simple network oscillation tasks studied training single unit produce training unit net units produces training unit net units produces approximately sinusoidal oscillation period order time steps spite nonlinearity units involved 
versions algorithm oscillation tasks teacher forcing version teacher forcing capable solving problems general 
reason appears order produce oscillation net initially manifests settling behavior initial small weight values weights adjusted bifurcation boundary gradient yield necessary information zero close zero 
free adjust weights initial conditions cases problem disappears 
appears heart success teacher forcing desired values net helping control initial conditions subsequent dynamics 
pineda observed similar need teacher forcing attempting add new stable points associative memory just moving existing ones 
discussion primary goal derive learning algorithm train completely recurrent continually updated networks learn temporal tasks 
emphasis uniform starting configurations contain priori information temporal nature task 
cases statistically derived training sets extensively optimized promote learning 
results simulation experiments described demonstrate algorithm sufficient generality power conditions 
algorithm described nonlocal sense learning weight access complete recurrent weight matrix error vector algorithm current form serve basis learning actual neurophysiological networks 
algorithm inherently quite parallel computation speed benefit greatly parallel hardware 
solutions algorithm obscure particularly complex tasks involving internal state 
observation familiar feedforward networks 
obscurity limited ability analyze solutions sufficient detail 
simpler cases discern going interesting kind distributed representation observed 
remembering pattern static local distributed group units networks incorporate data remembered functioning way static pattern represents 
gives rise dynamic internal representations sense distributed space time 
wish jonathan bachrach sharing insights issue training recurrent networks 
brought attention possibility line computation error gradient acknowledge important contribution development ideas 
research supported iri national science foundation author contract office naval research air force office scientific research system development foundation second author 
almeida 

learning rule asynchronous perceptrons feedback combinatorial environment 
proceedings ieee international conference neural networks 
bachrach 

learning represent state 
unpublished master thesis university massachusetts amherst 
elman 

finding structure time crl tech 
rep 
la jolla university california san diego center research language 
gallant king 

experiments sequential associative memories 
proceedings tenth annual conference cognitive science society 
hopfield 

neural networks physical systems emergent collective computational abilities 
proceedings national academy sciences 
jordan 

attractor dynamics parallelism connectionist sequential machine 
proceedings eighth annual conference cognitive science society 
lapedes farber 

self optimizing neural net content addressable memory pattern recognition physica 
mozer 

focused back propagation algorithm temporal pattern recognition tech 
rep 
university toronto departments psychology computer science 
mcbride jr narendra 

optimization time varying systems 
ieee transactions automatic control 
pearlmutter 

learning state space trajectories recurrent neural networks preliminary report tech 
rep aip 
pittsburgh carnegie mellon university department computer science 
pineda 

dynamics architecture neural computation journal complexity 
robinson fallside 

utility driven dynamic error propagation network tech 
rep cued infeng tr 
cambridge england cambridge university engineering department 
rohwer forrest 

training time dependence neural networks 
proceedings ieee international conference neural networks 
rumelhart hinton williams 

learning internal representations error propagation 
rumelhart mcclelland pdp research group parallel distributed processing explorations microstructure cognition 
vol 

foundations 
cambridge mit press bradford books 
servan schreiber cleeremans mcclelland 
encoding sequential structure simple recurrent networks tech 
rep cmu cs 
pittsburgh carnegie mellon university department computer science 
stornetta hogg huberman 

dynamical approach temporal pattern processing 
proceedings ieee conference neural information processing systems 
