appears cowan tesauro alspector 
eds 
advances neural information processing systems morgan kaufmann pub credit assignment time alternatives backpropagation bengio dept informatique recherche op universit de montr eal montreal qc paolo frasconi dip 
di sistemi informatica universit di firenze firenze italy learning recognize predict sequences long term context applications 
practical theoretical problems training recurrent neural networks perform tasks input output dependencies span long intervals 
starting mathematical analysis problem consider compare alternative algorithms architectures tasks span input output dependencies controlled 
results new algorithms show performance qualitatively superior obtained backpropagation 
recurrent neural networks considered learn map input sequences output sequences 
machines efficiently learn tasks useful applications involving sequence prediction recognition production 
practical difficulties reported training recurrent neural networks perform tasks temporal contingencies input output sequences span long intervals 
fact prove dynamical systems recurrent neural networks increasingly difficult train gradient descent duration dependencies captured increases 
mathematical analysis problem shows conditions arises systems 
case dynamics network allow reliably store bits information bounded input noise gradients respect error time step vanish exponentially fast propagates bell labs holmdel nj backward time 
second case gradients flow backward system locally unstable reliably store bits information presence input noise 
consideration problem understanding brought theoretical analysis explored compared alternative algorithms architectures 
comparative experiments performed artificial tasks span input output dependencies controlled 
cases duration parameter varied avoid short sequences algorithm easily learn 
tasks require learning latch store bits information arbitrary durations may vary example example 
tasks performed time delay neural networks recurrent networks memories gradually lost time constants fixed parameters network 
alternatives gradient descent explored approach probabilistic interpretation discrete state space similar hidden markov models hmms yielded interesting results 
difficult problem error propagation consider non autonomous discrete time system additive inputs recurrent neural network continuous activation function gamma corresponding autonomous dynamics gamma nonlinear map may tunable parameters network weights vectors representing respectively system state external input time order latch bit state information wants restrict values system activity subset domain 
way possible interpret ways inside outside sure remains region system dynamics chosen region basin attraction attractor attractor sub manifold subspace domain 
erase bit information inputs may push system activity basin attraction possibly 
bengio simard frasconi show conditions arise hyperbolic attractors latch bits information system 
system sensitive noise derivatives cost time respect system activations converge exponentially increases 
situation essential reason difficulty gradient descent train dynamical system capture long term dependencies input output sequences 
theorem show state region jm small perturbations grow exponentially yield loss information stored dynamics system theorem assume point exists open sphere centered jm 
exist km gamma kx gamma yk 
second theorem shows state region jm gradients propagated backwards time vanish exponentially fast theorem input system remains robustly latched jm attractor time 
see proofs bengio simard frasconi 
consequence results generally difficult train parametric dynamical system recurrent neural network learn long term dependencies gradient descent 
understanding brought analysis explored compared alternative algorithms architectures 
global search methods global search methods simulated annealing applied problem generally slow 
implemented simulated annealing algorithm martini optimizing functions continuous variables 
batch learning algorithm updating parameters examples training set seen 
performs cycle random moves coordinate parameter direction 
point accepted rejected metropolis criterion kirkpatrick gelatt vecchi 
simulated annealing algorithm robust respect local minima long plateaus 
global search method evaluated experiments multi grid random search 
algorithm tries random points current solution hyperrectangle decreasing size accepts reduce error 
resistant problems plateaus resistant problems local minima 
multi grid random search faster simulated annealing fail parity problem probably local minima 
time weighted pseudo newton time weighted pseudo newton algorithm uses second order derivatives cost respect instantiations weight different time steps try correcting vanishing gradient problem 
weight update weight computed follows deltaw gamma theta instantiation time parameter global learning rate cost pattern way temporal contribution deltaw weighted inverse curvature respect pseudo newton algorithm becker le cun prefer diagonal approximation hessian cheap compute guaranteed positive definite 
constant introduced prevent deltaw large small 
performance algorithm better regular pseudo newton algorithm better simple stochastic backpropagation algorithm algorithms perform worse worse length sequences increased 
discrete error propagation discrete error propagation algorithm replaces sigmoids network discrete threshold units attempts propagate discrete error information backwards time 
basic idea algorithm simple discrete element threshold unit latch write error propagation rule prescribes desired changes values inputs order obtain certain changes values outputs 
case threshold unit rule assumes desired change output unit discrete 
error information propagated backwards unit continuous value 
stochastic process convert continuous value appropriate discrete desired change 
case self loop clear advantage algorithm gradient back propagation sigmoid units error information vanish repeatedly propagated backwards time loop unit robustly store bit information 
details algorithm appear bengio simard frasconi 
algorithm performed better time weighted pseudo newton pseudo newton back propagation algorithms learning curve appeared irregular suggesting algorithm doing local random search 
em approach target propagation promising algorithms studied derived idea propagating targets gradients 
restrict sequence classification 
assume finite state learning system state time values 
different final states class targets 
system probabilistic interpretation assume markovian conditional independence model 
hmms system propagates forward discrete distribution states 
transitions may constrained state defined set successors state network network state delta gamma jju gamma gamma proposed architecture learning formulated maximum likelihood problem missing data 
missing variables expectation taken paths state space 
em expectation maximization gem generalized em algorithms dempster laird rubin help decoupling influence different hypothetical paths state space 
estimation step em requires propagating backward discrete distribution targets 
contrast hmms parameters adjusted unsupervised learning framework em supervised fashion 
new perspective successful training static models jordan jacobs 
transition probabilities conditional current input computed parametric function layer neural network softmax units 
propose modular architecture subnetwork state see 
subnetwork feedforward takes input continuous vector features output successor state interpreted gamma 
set tunable parameters 
markovian assumption distribution states time obtained linear combination outputs subnetworks gated previously computed distribution gamma gamma gamma subsequence inputs time 
training algorithm looks parameters system maximize likelihood falling correct state sequence tp tp tp ranges training sequences length th training sequence tp desired state time auxiliary function constructed introducing hidden variables state sequence complete likelihood function defined follows tp tp log th em gem iteration chosen maximize increase auxiliary function respect 
inputs quantized subnetworks perform simple look table probabilities em algorithm solved analytically 
networks non linearities hidden units softmax output constrain outputs sum gem algorithm simply increases example gradient ascent directly perform preferably stochastic gradient ascent likelihood 
extra term introduced optimization criterion cases target information propagate backwards diffused states 
experiments confirmed previous results indicating general difficulty training fully connected hmms em algorithm converging poor local maxima likelihood 
attempt understand better phenomenon looked quantities propagated forward quantities propagated backward representing credit blame training algorithm 
diffusion credit blame occurring forward maps matrix transition probabilities time step inputs map outputs ratio small volume image map respect corresponding volume domain small 
ratio absolute value determinant jacobian map 
optimization criterion incorporates maximization average magnitude determinant transition matrices algorithm performs better algorithms 
tricks important help convergence reduce problem diffusion credit 
idea possible structured model sparse connectivity matrix introducing prior knowledge state space 
example applications hmms speech recognition rely structured topologies 
reduce connectivity transition matrix sequence problem see section definition splitting nodes subsets specializing sequence classes 
possible introduce constraints parity problem 
trick drastically improved performance stochastic gradient ascent way helps training algorithm get local optima 
learning rate decreased likelihood improves increased likelihood remains flat system stuck plateau local optimum 
results section show performances obtained algorithm better obtained algorithms simple test problems considered 
experimental results results problems control span input output dependencies 
sequence problem classify input sequence sequence types elements experiments sequence carry information sequence class 
uniform noise added sequence 
methods see tables fully connected recurrent network units free parameters 
em algorithm state system sparse connectivity matrix initial state separate left right submodels states model types sequences 
parity problem consists producing parity input sequence produced final output number input odd 
target sequence 
methods minimal size network input hidden output free parameters 
em algorithm state system full connectivity matrix 
initial parameters chosen randomly trial 
noise added sequence uniformly distributed chosen independently training sequence 
considered criteria average classification error training stopping criterion met allowed number function evaluations performed task learned average number function evaluations needed reach stopping criterion 
tables stands pseudo newton 
column corresponds value maximum sequence length set trials 
sequence length particular training sequence picked randomly numbers reported averages trials 
recurrent networks parametric dynamical systems powerful ability represent context 
theoretical experimental evidence shows difficulty assigning credit time steps required order learn represent context 
studies fundamental problem proposes alternatives backpropagation algorithm perform learning tasks 
experiments show alternative approaches perform significantly better gradient descent 
behavior algorithms yields better understanding central issue learning context assigning credit transformations 
alternative algorithms showed improvement respect standard stochastic gradient descent clear winner comparison algorithm em algorithm probabilistic interpretation system dynamics 
experiments challenging tasks conducted confirm results 
furthermore extensions model possible example allowing inputs outputs supervision outputs states 
similarly performed recurrent networks trained gradient descent important analyze theoretically problems propagation credit encountered training markov models 
wish patrice simard collaborated analysis theoretical difficulties learning long term dependencies discrete error propagation algorithm 
becker le cun 
improving convergence back propagation learning second order methods proc 
connectionist models summer school eds 
touretzky hinton sejnowski morgan kaufman pp 

bengio simard frasconi 
learning long term dependencies gradient descent difficult ieee trans 
neural networks press 
martini 
minimizing multimodal functions continuous variables simulated annealing algorithm acm transactions mathematical software vol 
pp 

dempster laird rubin 
maximum likelihood incomplete data em algorithm royal stat 
soc vol 
pp 

jordan jacobs 
hierarchical mixtures experts em algorithm neural computation press 
kirkpatrick gelatt vecchi 
optimization simulated annealing science pp 
table final classification error sequence problem wrt sequence length back prop time weighted multigrid discrete err 
prop 
simulated anneal 
em table sequence presentations sequence problem wrt sequence length back prop time weighted multigrid discrete err 
prop 
simulated anneal 
em table final classification error parity problem wrt sequence length back prop time weighted multigrid discrete err 
prop 
simulated anneal 
em table sequence presentations parity problem wrt sequence length back prop time weighted multigrid discrete err 
prop 
simulated anneal 
em 
