practical feature subset selection machine learning mark hall lloyd smith las cs waikato ac nz department computer science university waikato hamilton new zealand 
machine learning algorithms automatically extract knowledge machine readable information 
unfortunately success usually dependant quality data operate 
data inadequate contains extraneous irrelevant information machine learning algorithms may produce accurate understandable results may fail discover 
feature subset selectors algorithms attempt identify remove irrelevant redundant information possible prior learning 
feature subset selection result enhanced performance reduced hypothesis search space cases reduced storage requirement 
describes new feature selection algorithm uses correlation heuristic determine goodness feature subsets evaluates effectiveness common machine learning algorithms 
experiments number standard machine learning data sets 
feature subset selection gave significant improvement algorithms 
keywords feature selection correlation machine learning 

machine learning computer algorithms learners attempt automatically knowledge example data 
knowledge predictions novel data provide insight nature target concept 
example data typically consists number input patterns examples concepts learned 
example described vector measurements features label denotes category class example belongs 
machine learning systems typically attempt discover regularities relationships features classes learning training phase 
second phase called classification uses model induced learning place new examples appropriate classes 
factors affect success machine learning task 
representation quality example data foremost 
irrelevant redundant information data noisy unreliable knowledge discovery training phase difficult 
feature subset selection process identifying removing irrelevant redundant information possible 
reduces dimensionality data allows learning algorithms operate faster effectively 
cases accuracy classification improved result compact easily interpreted representation target concept 
presents new approach feature selection machine learning uses correlation heuristic evaluate merit features 
effectiveness feature selector evaluated applying data pre processing step common machine learning algorithms 

feature subset selection feature subset selection problem known statistics pattern recognition 
techniques deal exclusively features continuous assumptions hold practical machine learning algorithms 
example common assumption monotonicity says increasing number features decrease performance 
assumptions monotonicity invalid machine learning approach feature subset selection machine learning borrowed search evaluation techniques statistics pattern recognition 
approach dubbed wrapper kohavi john estimates accuracy feature subsets statistical re sampling technique cross validation actual machine learning algorithm 
wrapper proved useful slow execute induction algorithm called repeatedly 
approach adopted feature subset selection called filter operates independently induction algorithm undesirable features filtered data induction commences 
filter methods typically training data selecting subset features 
look consistency data note combination values feature subset associated single class label 
method koller sahami eliminates features information content concerning features class subsumed number remaining features 
methods attempt rank features relevancy score kira rendell holmes nevill manning 
filters proven faster wrappers applied large data sets containing features 
searching feature subset space purpose feature selection decide initial possibly large number features include final subset ignore 
possible features initially possible subsets 
way find best subset try clearly prohibitive small number initial features 
various heuristic search strategies hill climbing best rich knight applied search feature subset space reasonable time 
forms hill climbing search best search feature selector described best search final experiments gave better results cases 
best search starts empty set features generates possible single feature expansions 
subset highest evaluation chosen expanded manner adding single features 
expanding subset results improvement search drops back best unexpanded subset continues 
time best search explore entire search space common limit number subsets expanded result improvement 
best subset returned search terminates 

cfs correlation feature selection majority feature selection programs cfs uses search algorithm function evaluate merit feature subsets 
heuristic cfs measures goodness feature subsets takes account usefulness individual features predicting class label level 
hypothesis heuristic stated feature subsets contain features highly correlated predictive class uncorrelated predictive 
equation formalises heuristic 
eqn 
kr ci ii number features subset ci mean feature correlation class ii average feature 
equation borrowed test theory measure reliability test consisting summed items reliability individual items 
example accurate indication person occupational success composite number tests measuring wide variety traits academic ability leadership efficiency individual test measures restricted scope traits 
equation fact pearson correlation variables standardised 
numerator thought giving indication predictive class group features denominator redundancy 
heuristic goodness measure filter irrelevant features poor predictors class 
redundant features ignored highly correlated features 
depicts components cfs feature selector 
training data fs machine learning algorithm search feature evaluation gs feature set heuristic goodness final feature set fig 

cfs feature selector 
feature correlations large classification tasks machine learning involve learning distinguish nominal class values may involve features ordinal continuous 
order common basis computing correlations necessary equation continuous features converted nominal binning 
number information measures association tried feature class correlations feature equation including uncertainty coefficient symmetrical uncertainty coefficient press gain ratio quinlan minimum description length principle kononenko 
best results achieved gain ratio feature class correlations symmetrical uncertainty coefficient feature 
discrete random variables equations give entropy observing equation gives amount information gained observing vice versa amount information gained observing 
gain biased favour attributes values attributes greater numbers values appear gain information fewer values informative 
gain ratio equation non symmetrical measure tries compensate bias 
variable predicted gain ratio gain dividing entropy symmetrical uncertainty coefficient gain dividing sum entropies gain ratio symmetrical uncertainty coefficient lie 
value indicates association value gain ratio indicates knowledge completely predicts value symmetrical uncertainty coefficient indicates knowledge variable completely predicts 
display bias favour attributes fewer values 
eqn 
log eqn 
log eqn 
gain eqn 
gain ratio gain eqn 
symmetrical uncertainty gain initial experiments showed cfs quite aggressive filter typically filtered half features data set leaving best features 
resulted improvement data sets clear higher accuracy achieved features 
reducing effect equation improves performance 
scaling factor generally gave results data sets learning algorithms experiments described 

experimental methodology order determine cfs common machine learning algorithms series experiments run machine learning algorithms feature selection standard data sets drawn uci repository 
naive bayes naive bayes algorithm employs simplified version bayes formula classify novel instance 
posterior probability possible class calculated feature values instance instance assigned class highest probability 
equation shows naive bayesian formula assumption feature values independent class 
eqn 
left side equation posterior probability class feature values observed instance classified 
denominator right side equation omitted constant easily computed requires posterior probabilities classes sum 
due assumption feature values independant class naive bayes classifier predictive performance adversely affected presence redundant features training data 
decision tree generator quinlan algorithm summarises training data form decision tree 
systems induce logical rules decision tree algorithms proved popular practice 
due part robustness execution speed fact explicit concept descriptions produced natural people interpret 
nodes decision tree correspond features branches associated values 
leaves tree correspond classes 
classify new instance simply examines features tested nodes tree follows branches corresponding observed values instance 
reaching leaf process terminates class leaf assigned instance 
build decision tree training data employs greedy approach uses information theoretic measure gain ratio cf equation guide 
choosing attribute root tree divides training instances subsets corresponding values attribute 
entropy class labels subsets entropy class labels full training set information gained splitting attribute 
chooses attribute gains information root tree 
algorithm applied recursively form sub trees terminating subset contains instances class 
overfit training data resulting large trees 
kohavi john cases feature selection result producing smaller trees 
ib similarity learner similarity learners represent knowledge form specific cases experiences 
rely efficient matching methods retrieve stored cases applied novel situations 
naive bayes algorithm similarity learners usually computationally simple variations considered models human learning cunningham 
ib aha implementation simplest similarity learner known nearest neighbour 
ib simply finds stored case closest usually euclidean distance metric instance classified 
new instance assigned retrieved instance class 
equation shows distance metric employed ib 
eqn 
equation gives distance instances refer jth feature value instance respectively 
numeric valued attributes symbolic valued attributes langley sage fewer training cases needed nearest neighbour reach specified accuracy irrelevant features removed 
experiment cfs vs cfs twelve data sets drawn uci repository machine learning databases 
data sets chosen prevalence nominal features predominance literature 
table summarises characteristics data sets 
data sets cr ly hc contain continuous features rest contain nominal features 
runs done feature selection machine learning algorithm data set 
run procedure applied 
data set randomly split training test set sizes table 

machine learning algorithm applied training set resulting model classify test set 

cfs applied training data reducing dimensionality 
test set reduced factor 
machine learning algorithm applied reduced data step 
accuracies runs machine learning algorithm averaged 
best search stopping criterion expanding non improving nodes row 
addition accuracy tree sizes recorded 
smaller trees generally preferred easier understand 

results tables show results cfs naive bayes ib respectively 
feature selection significantly improves performance naive bayes domains 
performance significantly degraded domains change domains 
cases cfs reduced number features factor greater 
horse colic domain hc significant improvements original features 
table 
domain characteristics 
missing column shows percentage data sets entries number features number instances missing values 
avg feat vals max min feat vals calculated nominal features data sets 
dom inst feat avg feat vals max min feat vals class vals train size test size mu vo cr ly pt bc dna au sb hc kr table 
accuracy naive bayes naive cfs naive feature selection 
column gives probability observed difference due sampling confidence minus probability 
values show significantly better level 
column shows number features selected versus number features originally 
domain naive bayes naive cfs features original mu vo cr ly pt bc dna au sb hc kr table 
accuracy ib ib cfs ib feature selection 
domain ib ib cfs features original mu vo cr ly pt bc dna au sb hc kr table 
accuracy cfs feature selection 
column gives probability observed differences accuracy due sampling 
second column gives probability observed differences tree size due sampling 
dom cfs size size cfs mu vo cr ly pt bc dna au sb hc kr results feature selection ib similar 
significant improvement recorded domains significant degradation 
unfortunately result horse colic domain due features causing ib crash 
cfs filter algorithm feature subsets chosen ib chosen naive bayes 
table shows results 
cfs successful naive bayes ib 
significant improvements significant degradations 
cfs effective significantly reducing size trees induced domains 

new approach feature selection machine learning 
algorithm cfs uses features performances guide search subset features 
experimental results encouraging show promise cfs practical feature selector common machine learning algorithms 
correlation evaluation heuristic employed cfs appears choose feature subsets useful learning algorithms improving accuracy making results easier understand 
preliminary experiments wrapper feature selector domains search method show cfs competitive 
cfs outperforms wrapper percentage points domains domains wrapper better larger margin 
cfs times faster wrapper 
soybean domain sb wrapper takes just days cpu time complete runs sparc server cfs takes minutes cpu time 
evaluation heuristic equation balances predictive ability group features level redundancy 
success certainly depend accurate feature class feature 
indication bias gain ratio symmetrical uncertainty coefficient may totally appropriate equation reducing effect gave improved results 
strongly biased favour features fewer values gain ratio increasingly number class labels increases 
furthermore measures biased upwards number training examples decreases 
factors may account cfs poor performance lymphography ly audiology au domains 
factor affecting performance presence feature interactions dependencies data sets 
extreme example parity concept single feature isolation appears better 
domingos pazzani shown exist significant pair wise feature dependencies class standard machine learning data sets 
attempt better understand cfs works effectively domains 
addressing issues raised measure bias feature interactions may help domains cfs performed 
experiments look closely cfs evaluation heuristic correlates actual performance machine learning algorithms randomly chosen subsets features 
aha kibler albert 

instance learning algorithms 
machine learning 
pp 

almuallim 

learning irrelevant features 
proceedings ninth national conference artificial intelligence 
san jose ca aaai press pp 

cunningham witten 

applications machine learning information retrieval 
working 
department university waikato new zealand 
domingos pazzani 

independence conditions optimality simple bayesian classifier 
proceedings thirteenth international conference machine learning 


theory psychological measurement 
mcgraw hill 
holmes nevill manning 

feature selection discovery simple classification rules 
proceedings international symposium intelligent data analysis ida 
kira 

practical approach feature selection 
proceedings ninth international conference machine learning 
aberdeen scotland 
morgan kaufmann 
pp 

kohavi john 

wrappers feature subset selection 
aij special issue relevance 
press 
koller sahami 

optimal feature selection 
proceedings thirteenth international conference machine learning icml 
san francisco ca 
morgan kaufmann 
pp 

kononenko 

biases estimating multi valued attributes 
proceedings fourteenth international joint conference artificial intelligence 
morgan kaufmann 
pp 

langley sage 

scaling domains irrelevant features greiner ed computational learning theory natural learning systems 
cambridge ma 
mit press 
press flannery teukolsky vetterling 

numerical recipes cambridge university press 
rich knight 

artificial intelligence 
mcgraw hill 
quinlan 

induction decision trees 
machine learning 
pp 

quinlan 

programs machine learning 
morgan kaufmann 
