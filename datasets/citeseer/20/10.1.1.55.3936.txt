small journal name fl kluwer academic publishers boston 
manufactured netherlands 
efficient agnostic learning michael kearns research att com robert schapire schapire research att com bell laboratories mountain avenue murray hill nj linda sellie sellie research att com department computer science university chicago chicago il editor lisa hellerstein 
initiate investigation generalizations probably approximately correct pac learning model attempt significantly weaken target function assumptions 
ultimate goal direction informally termed agnostic learning virtually assumptions target function 
name derives fact designers learning algorithms give belief nature represented target function simple succinct explanation 
give number positive negative results provide initial outline possibilities agnostic learning 
results include hardness results obvious generalization pac model agnostic setting efficient general agnostic learning method dynamic programming relationships loss functions agnostic learning algorithm learning problem involves hidden variables 
keywords machine learning agnostic learning pac learning computational learning theory 
major limitations probably approximately correct pac learning model valiant related models strong assumptions placed called target function learning algorithm attempting approximate examples 
restrictions permitted rigorous study computational complexity learning function representational complexity target function pac family models diverges setting typically encountered practice empirical machine learning research 
empirical approaches assumptions target function search limited space hypothesis functions attempt find best approximation target function cases target function complex best approximation may incur significant error 
initiate investigation generalizations pac model attempt significantly weaken target function assumptions possible 
ultimate goal informally termed agnostic learning virtually assumptions target function 
word agnostic root means literally known emphasize fact designers learning algorithms may prior knowledge target kearns schapire sellie function 
important note attempt remove assumption statistical independence examples seen learning algorithm worthwhile research direction pursued number authors aldous vazirani helmbold long 
describes preliminary study possibilities limitations efficient agnostic learning 
claim definitive model general model haussler allows easy consideration natural modifications 
surprisingly light evidence standard pac model efficient agnostic learning purest form assumptions target function distribution hard come results demonstrate 
consider variations overly ambitious criteria attempt find positive results target assumptions significantly weakened standard pac setting 
prior studies weakened target assumptions pac learning relevant 
due haussler describes powerful generalization standard pac model decision theory uniform convergence results 
haussler results central importance research described 
agnostic model describe quite similar haussler differing touchstone class see section 
haussler concern exclusively informationtheoretic statistical issues agnostic learning concerned exclusively efficient computation 
relevant large body research nonparametric density estimation field statistics see instance excellent survey 
relevant investigation probabilistic concepts kearns schapire yamanishi stochastic rules 
target function conditional probability distribution typically discrete range space 
significant portion research described extends 
results closely related pitt valiant heuristic learning pitt valiant valiant viewed variant agnostic pac model 
brief overview section motivate develop detail general learning framework 
section consider restriction general model case agnostic pac learning give strong evidence intractability simple learning problems model 
section discuss empirical minimization loss give general method agnostic learning piecewise functions dynamic programming 
section gives useful relationship agnostic setting common loss functions quadratic prediction loss gives applications relationship 
section investigate compromise agnostic learning strong target assumptions standard pac model providing efficient learning algorithm model learning problems involving efficient agnostic learning hidden variables 
section list problems remain open area 

definitions models section define notation generalized framework attempt weaken target function assumptions needed efficient learning 
approach strongly influenced decision theoretic learning model introduced computational learning theory community haussler 
giving definitions err side formality order lay groundwork research agnostic learning wish give model precise quite general 
various restrictions general model locally specified cumbersome notation 
set called domain refer points instances intuitively think instances inputs black box behavior wish learn model 
set called range set called observed range 
think space possible values output black box introduce may direct access output value quantity derived 
general assumptions relationship call pair theta observation 

assumption class assumption class class probability distributions observation space theta represent assumptions phenomenon trying learn model nature observations phenomenon 
note definition may functional relationship observation 
special cases generalized definition wish define 
special case functional relationship arbitrary domain distribution 
consider case class functions mapping suppose class obtained choosing distribution letting ad distribution generating observations drawn randomly say functional decomposition familiar distribution free function learning model 
second special case class functions mapping functional value directly observed 
class obtained choosing distribution letting ad distribution generating observations drawn randomly probability kearns schapire sellie probability gamma 
call class probabilistic concepts concepts say concept decomposition distribution free concept learning model 
case functional concept decomposition class refer target class distribution ad generates observations call target function target concept target distribution 

hypothesis class touchstone class introduce classes functions hypothesis class touchstone class usually case intuition learning algorithm attempt model behavior observes hypothesis function model seek eliminate restrictions possible ask standard hypothesis function measured nearness target may impossible undefined 
purpose touchstone class class provides standard measurement hypotheses ask performance hypothesis near performance best near best formalized shortly 
natural ask hypothesis chosen approach best performance class corresponding case see circumstances interesting important relax restriction 
leaving class fixed increasing power may overcome certain representational hurdles choice way term dnf disjunctive normal form formulas efficiently learnable standard pac model provided allow expressive cnf conjunctive normal form hypothesis representation kearns li pitt valiant pitt valiant 

loss function formalize possible meanings best function class 
domain range observed range loss function mapping theta positive real number observation theta function loss denoted 
loss function measures distance discrepancy observed value typical examples include prediction loss known discrete loss ae quadratic loss efficient agnostic learning gamma observations drawn distribution define expected loss function abbreviate clear context 
prepared define best possible performance class functions respect loss function hypothesis class define opt inf fe similarly touchstone class define opt inf fe note opt opt implicit dependence omit notational brevity 
need refer estimates quantities empirical data 
sequence observations estimate es jsj delta allows define estimated optimal performance defined opt inf opt inf es usually clear context write opt opt 

learning model ready give generalized definition learning 
definition 
domain range observed range theta loss function 
class distributions theta classes functions mapping say learnable assuming respect algorithm learn function ffl ffi bounded fixed polynomial ffl ffi distribution inputs ffl ffi learn draws ffl ffi observations halts outputs hypothesis probability gamma ffi satisfies opt ffl 
running time learn bounded fixed polynomial ffl ffi say efficiently learnable assuming respect 
case functional decomposition class replace phrase assuming phrase assuming function class case concept decomposition class replace phrase assuming concept class wish indicate touchstone class learnable assuming specific say efficiently learnable assuming natural complexity parameter associated domain distribution class function classes case understood xn hn tn standard examples number boolean variables number real kearns schapire sellie dimensions 
cases allow number observations running time algorithm definition polynomial dependence 
generating old new models define previously studied new models learning appropriate settings parameters class boolean functions functional decomposition prediction loss function obtain restricted pac model valiant hypothesis class target class 
retain condition allow obtain standard pac model kearns hypothesis class may powerful target class 
concept decomposition class concepts obtain concept learning model kearns schapire interesting choices loss functions 
choose prediction loss function ask optimal predictive model observations known bayes optimal decision may quite different actual probabilities rule minimum probability incorrectly predicting value random observation observation value 
alternatively may choose quadratic loss function known quadratic loss lead find hypothesis minimizing quadratic distance gamma kearns schapire white 
consider generalization standard pac model class boolean functions domain functional decomposition remove assumptions target concept existence concept consistent data 
choose prediction loss function wish find predictive concept regardless nature target concept 
refer particular choice parameters agnostic pac model 

agnostic pac learning section examine agnostic pac model 
main results demonstrate relationships agnostic pac model previously studied variations standard pac model provide strong argument need restrictions different models wish learning algorithms efficient 
related results indicating intractability learning weakened target concept assumptions valiant pitt valiant model heuristic learning 
efficient agnostic learning 
agnostic learning malicious errors result shows agnostic pac learning hard pac learning malicious errors kearns li valiant fact partial converse holds 
formally define model equivalent standard pac model addition new parameter called error rate fi observation probability fi generated malicious adversary target function target distribution 
goal malicious error model remains achieving arbitrarily predictive approximation underlying target function 
theorem class boolean functions efficiently learnable agnostic pac model assume vapnik chervonenkis dimension bounded polynomial complexity parameter efficiently learnable pac model algorithm tolerating malicious error rate fi theta ffl 
proof idea demonstrate equivalence problem learning agnostic pac model natural combinatorial optimization problem disagreement minimization problem problem known equivalent constant approximation factors problem learning malicious errors kearns li 
problem input arbitrary multiset pairs correct output instance minimizes ds jfi gj follows standard arguments blumer ehrenfeucht haussler warmuth vapnik chervonenkis dimension polynomially bounded complexity parameter algorithm efficiently solves disagreement minimization problem subroutine efficient algorithm learning agnostic pac model 
see section details 
direction equivalence suppose algorithm efficiently learning agnostic pac model wish algorithm order solve disagreement minimization problem fixed instance give argument assuming instance appears different labels pairs may thought consistent boolean function create distribution instances multiset giving equal weight instance instances appearing receive proportionally weight instances outside receive zero weight 
run agnostic learning algorithm choosing ffl drawing instances labeling target function note equivalent simply drawing labeled pairs randomly 
algorithm output hypothesis satisfies pr pr ffl pr kearns schapire sellie minimizes ds implies pr pr single disagreement incurs error respect pr ds ds ds optimization problem solved 
case contains conflicting labels instance consistent function simply remove pairs conflicting instances remaining multiset consistent function 
notice function disagrees exactly half gamma minimization ds reduces minimization 
simply perform reduction desired algorithm learning malicious error models follows equivalence agnostic learning disagreement minimization equivalence constant approximation factors disagreement minimization learning restricted pac model malicious errors fact proved kearns li theorem 
fact equivalence obtain weakened converse theorem learning malicious error rate fi theta ffl implies algorithm finding satisfying pr delta opt constant weaker multiplicative additive error bound 
number variations agnostic pac learning may directly covered theorem essentially interpret result negative evidence hopes efficient agnostic pac learning algorithms previous results indicate theta ffl malicious error rate achieved limited classes kearns li class symmetric functions boolean variables 
results agnostic pac learning may obtained theorem previous learning presence malicious errors 
instance class boolean functions efficiently learnable pac model efficient algorithm finding satisfying pr dh delta opt target function dh dimension hypothesis class follows theorems kearns li 

intractability agnostic pac learning conjunctions give reduction indicating difficulty learning simple boolean conjunctions agnostic pac model 
xn set tn hn class conjunctions literals boolean variables xn agnostic pac model wish find algorithm find conjunction tn near minimum rate disagreement unknown boolean target function show problem hard restricted theorem xn fn class polynomial size disjunctive normal form formulas tn class conjunctions efficient agnostic learning literals boolean variables xn efficiently learnable assuming function class rp np 
proof suppose contrary theorem statement exists efficient algorithm stated learning problem 
show algorithm probabilistically solve minimum set cover problem garey johnson polynomial time implying rp np 
similar proof context pac learning malicious errors kearns li theorem obtain similar weaker result derive 
instance minimum set cover problem set objects fo covered collection subsets objects fs sng 
goal find smallest subset covers objects exists 
loss generality assume objects contained set 
loss generality assume objects contained unique collection sets objects contained exactly sets remove objects valid set cover cover removed object 
reduction chooses target function term dnf formula tn variable set fx xng conjunction variables instances learning algorithm labeled object assignment ha values boolean variables assigned ij define ij ae 
construction object sets positions zero satisfy term negative examples 
set assignment hb jn jk ae 

note satisfies exactly term satisfies terms 
notice variable choose include monotone conjunction conjunction guaranteed cover negative examples object appears set including conjunction incurs single error positive examples 
goal force agnostic learning algorithm cover negative examples corresponding covering objects incurring positive error corresponding minimum cardinality cover 
kearns schapire sellie distribution defined set ffl run assumed agnostic learning algorithm examples drawn labeled clearly entire procedure takes time polynomial size set cover instance target dnf polynomial size 
high probability obtain conjunction having error bounded opt ffl respect fs appears hg 
show cover 
note conjunction variables delta delta delta xn error equal consistent opt implies opt ffl conjunction monotone inconsistent positive example giving error 
consistent negative instances error 
covers objects variable forces negative happens includes remains show minimum cover 
suppose exists smaller set cover construct monomial construction monotone consistent instance set cover consistent consistent jb elements opt pr jb 
hand pr jbj implies pr opt jbj gamma jb opt ffl choice ffl contradicting assumption error bounded opt ffl 
minimum set cover 
assume target distribution functionally decomposed distribution target function guaranteed small dnf formula hard problem find conjunction predictive power small additive factor best conjunction 
surprising theorem holds learning algorithm told target dnf efficient agnostic learning formula 
demonstrates important principle having perfect succinct description process generating observations may help finding succinct rule thumb explains observations 
difficulty may arise problem learning optimization 
similar results valiant pitt valiant 

agnostic learning weak learning describe connection agnostic pac learning weak pac learning standard pac criterion relaxed demand hypotheses error respect target bounded gamma polynomial complexity parameter kearns valiant schapire 
classes boolean functions domain parameterized say weakly approximates polynomial distribution xn tn function tn pr gamma 
theorem class boolean functions weakly approximates class efficiently learnable standard pac model efficiently learnable agnostic pac model 
proof idea weakly approximates target function opt significantly smaller agnostic learning algorithm effectively functions weak learning algorithm result follows boosting techniques schapire freund converting weak learning algorithm strong learning algorithm 
class boolean conjunctions weakly approximates class dnf formulas see instance schapire section immediately follows theorem learning conjunctions agnostic pac model hard learning dnf formulas standard pac model interpreted evidence difficulty problem assumption learning dnf hard standard pac model 
note theorem set result restrictions summary see agnostic pac learning intimately related number apparently difficult problems standard pac model 
leads preliminary look efficient agnostic learning models respect loss functions may want consider restrictions assumption class reverting standard pac model 
kearns schapire sellie 
tractable agnostic learning problems results section indicate prospects finding efficient agnostic pac learning algorithms may demonstrate section non trivial situations efficient agnostic learning fact tractable 
give learning method dynamic programming applicable general learning framework 

empirical loss minimization agnostic learning natural technique designing agnostic learning algorithm draw large random sample find hypothesis best fits observed data 
fact canonical approach successfully yields efficient agnostic learning algorithm wide variety settings assuming exists efficient algorithm finding best hypothesis respect observed sample 
section assumptions distributions expression agnostically learnable indicate hypothesis near best dropping indicate agnostically learnable class 
observed range touchstone hypothesis classes functions mapping loss function 
say efficiently empirically respect exists polynomial time algorithm finite sample theta computes hypothesis empirical loss optimal compared opt 
polynomial time means polynomial size sample 
instance class constant real valued functions efficiently empirically respect quadratic loss function average values observed minimizes empirical loss 
generally set real valued basis functions standard regression techniques efficiently minimize empirical quadratic loss set linear combinations basis functions duda hart kearns schapire 
empirical minimization sufficient agnostic learning 
question answered large part dudley haussler pollard vapnik 
show situations hypothesis class uniform convergence achieved reasonably small samples 
situations bound ffl ffi exists distribution theta random sample theta size ffl ffi chosen probability average empirical loss differs true expected loss ffl ffi pr fi fi fi es gamma fi fi fi ffl ffi efficient agnostic learning efficiently empirically uniform convergence achieved efficiently agnostically learnable done ffl ffi opt ffl 
may exist function achieves optimum loss choose function approximately optimal 
random sample size sufficiently large probability gamma ffi fi fi fi gamma fi fi fi ffl ftg 
note uniform convergence required entire touchstone class hypothesis class single element close optimal 
result applying assumed empirical minimization algorithm probability gamma ffi ffl ffl ffl opt ffl desired 
focus primarily empirical loss minimization worth noting alternative approach minimize empirical loss data plus measure complexity hypothesis see instance vapnik 

learning piecewise functions cases uniform convergence known occur problem agnostic learning largely reduced minimizing empirical loss finite sample 
apply fact problem agnostically learning families piecewise functions domain give general technique dynamic programming learning functions certain assumptions show instance technique applied agnostically learn step functions piecewise polynomials 
similar dynamic programming technique rissanen speed yu finding minimum description length histogram density function see yamanishi 
assume class functions say function piecewise function exist disjoint intervals called bins union functions pw denote set piecewise functions theorem hypothesis class empirically respect pw empirically pw time polynomial size sample 
kearns schapire sellie proof give general dynamic programming technique empirically minimizing pw 
xm ym sample assume loss generality delta delta delta xm interested computing piecewise function ij informally piecewise hypothesis precisely empirical loss ij exceed hypothesis pw 
clearly pms meet goal empirical minimization pw entire sample straightforward procedure compute ij consider placing observations bin observations belong bin piecewise function construction 
empirical minimization algorithm compute hypothesis ik empirical loss observations exceed hypothesis recursively compute gammak gamma gamma piece hypothesis remaining gamma observations 
combine gammak gamma jk obvious manner form piece hypothesis ij ij ij giving minimum loss summarize formally procedure computes ij follows 
compute ij ij 
ik gammak gammak compute ik hik ik ik recursively compute gammak gamma ij ae gammak gamma gammak ik 
ij ij arg min ij notation denote total loss sample 
described computation ij recursively fact store values table standard dynamic programming techniques 
procedure runs polynomial time follows fact ms piecewise functions ij computed stored table 
prove correctness procedure argue induction ij pw 
base case follows immediately assumption empirically function pw defined bins functions assume loss generality bins ordered sense efficient agnostic learning choose largest value points ik fall bin fx gammak hik ik ik assumption empirically gamma piecewise function defined bins gamma gamma functions gamma inductive hypothesis gammak gamma gammak gammak 
ij ij gammak gamma gammak hik ik gammak ik completing induction proof 
frequent case empirical minimization loss sufficient learning theorem may translate algorithm loss minimization agnostic learning algorithm functions piecewise application suppose observed range bounded gammam finite setting theorem implies efficient agnostic learnability respect quadratic loss function step functions steps piecewise functions piece constant function 
follows fact constant functions empirically fact uniform convergence achieved functions 
similar involved argument theorem invoked show generally piecewise degree polynomials agnostically learned polynomial time show 
proving theorem need review tools proving uniform convergence 
specifically interested pseudo dimension class functions combinatorial property largely characterizes uniform convergence dudley haussler pollard 
class functions finite subset theta say shatters gamma pos gamma fg pos positive 
shatters behavior points relative realized function pseudo dimension cardinality largest shattered finite subset theta maximum exists 
haussler corollary argues class lh fl hg uniformly bounded pseudo dimension sample size polynomial ffl ffi sufficient guarantee uniform convergence sense equation 
prove uniform convergence hypothesis space suffices upper bound pseudo dimension lh show lh uniformly bounded 
kearns schapire sellie concerned piecewise functions theorem useful purpose theorem class real valued functions pseudo dimension 
pseudo dimension pw gamma 
proof subset theta cardinality 
wish show shattered pw 
elements indexed pairs 
assume loss generality elements sorted ij ij jd ij 
ij distinct possibly shattered 
break ij blocks consisting consecutive points 
ij ij jd ith block 
pseudo dimension shattered means exist string oe included set gamma pos gamma fg oe oe oe delta delta delta oe concatenation strings oe claim oe member gamma pos gamma pos gamma pos gamma 
pos gamma pos gamma pw suppose contrary witnesses oe membership defined disjoint intervals union functions assume loss generality intervals sorted point smaller point inductively show invariant holds set delta delta delta contain elements fact contain elements follows definition oe witnesses oe 
suppose delta delta delta contains elements inductive assumption delta delta delta gamma contains points gamma interval contains elements gamma witness oe contradicts definition oe particular delta delta delta contain points clear contradiction delta delta delta claimed oe shattered proving theorem 
ready prove agnostic learnability piecewise polynomial functions efficient agnostic learning theorem gammam 
exists algorithm agnostically learning class piecewise degree polynomials respect quadratic loss function time polynomial ffl ffi 
sample complexity algorithm ffl ln em ffl ln ffi 
proof class real valued degree polynomials pw 
set polynomials range gammam similarly define goal show agnostically learnable 
function clamp function obtained clamping range gammam 
clamp ffi gammam gammam gammam class real valued functions define clamp fg 
noted collection linear combinations set basis functions empirically 
choosing basis functions see empirically applying theorem empirically show agnostically learnable suffice prove result unfortunately known techniques haussler pollard proving result require loss function bounded 
setting case functions hypothesis space uniformly bounded output piecewise polynomial minimum empirical loss output clamp 
note empirical loss greater observed range subset gammam 
empirically clamp 
argue polynomial size sample suffices achieve uniform convergence clamp respect loss function noted haussler corollary case clamp uniformly bounded polynomial pseudo dimension 
clearly function clamp bounded clamp uniformly bounded 
bound pseudo dimension clamp observations 
degree polynomial gamma humps clamp element clamp clamp pw 

function written linear combination basis functions yx yx follows definition quadratic loss fact degree polynomial 
kearns schapire sellie 
subset dimensional vector space functions 
pseudo dimension dudley haussler theorem 

theorem implies pseudo dimension pw 
pseudo dimension clamp 
complete proof overcome final technical difficulty show exists polynomial true expected loss ffl optimal empirical loss ffl true loss 
see section may difficult impossible prove may unbounded 
problem range gammam case empirical estimate true loss obtained hoeffding inequality 
empirically clamp effectively shown agnostically learnable clamp 
quite set prove goal show agnostically learnable 
proved fact function clamp fact piecewise polynomial range gammam 
specifically noted clamp clamp agnostically learnable clamp 
loss clamp worse function follows opt clamp opt 
implies agnostically learnable clamp 
stated sample complexity bound follows combination facts haussler corollary 
shown piecewise polynomials agnostically learnable number pieces fixed 
natural ask truly necessary fixed 
words efficient algorithm automatically picks right number pieces 
formally asking class pw pw agnostically learnable respect quadratic loss function 
allow learning algorithm time polynomial ffl ffi pieces necessary loss opt pw ffl 
unfortunately feasible construct situations information determine right number pieces large small 
specifically domain uniform distribution instances observed range assume degree polynomials zero words trying agnostically learn step functions 
consider concepts constant function 
words point labeled equal probability 
case optimal number pieces quadratic loss minimized single step entire domain 
second kind concept denoted deterministic function range equal size steps large 
value function efficient agnostic learning steps chosen random mentioned function deterministic 
case optimal number pieces intuitively clear learning algorithm distinguish cases observes points bin event occur points observed 
ability distinguish cases learning algorithm find hypothesis loss comes close optimal 
learning algorithm stops having seen examples distinguish data produced hypothesis far concepts learner reasonably high probability outputting concept far optimal chooses sample significantly smaller hand learner choose sample size larger risks drawing far examples large true target concept case 
omit details arguments rigorous instance randomized lower bound techniques blumer 

arbitrary shows arbitrarily large number observations needed agnostically learn piecewise polynomials finite number pieces 
mention results section generalized find piecewise functions continuous considering finite set endpoints hypothesis function interval adding choice endpoint variable dynamic program 

relations loss functions agnostic learning suppose assumption class functional decomposition class boolean functions 
common approach learning conditions find real valued hypothesis boolean function hope knowledge target boolean may easier find algorithms operate space functions characterized continuous parameterization may incremental changes pursue hillclimbing methods exist boolean classes 
general purpose learning algorithms known algorithm neural networks exactly approach 
algorithms searching real valued hypothesis invariably attempt minimize loss function incorporates actual real valued output quadratic loss explicitly address performance natural loss function boolean targets prediction loss precisely boolean target function finding minimizing gamma help predicting boolean target value 
obvious approach define theta theta boolean choices real valued works degree easy show general kearns schapire sellie theta pr theta proof inequality follows noting gamma observing theta gamma 
bound tight sense exist boolean real valued equality holds 
case small stated bound expected prediction loss nontrivial 
pursuit agnostic learning wish allow weakest assumptions case expect able find hypothesis small 
larger bound obtained theta better achieved random guessing 
find way predictions nontrivial probability mistake approaches expected quadratic loss corresponding random guessing achieved constant function 
function define boolean random variable probability probability gamma simply concept interpretation write denote pr probability taken random draw coin flip associated theorem boolean function real valued function 
distribution gamma proof gamma gamma gamma fh gamma gamma fh combining equations noting boolean gamma claimed 
stated upper bound quantity follows simply fact gamma real provided achieved nontrivial expected quadratic loss obtain nontrivial expected prediction loss 
precisely ff ff may considerably smaller boolean sense gamma small 
note efficient agnostic learning case small expected quadratic loss theta predictions theorem covers agnostic setting expected quadratic loss may large non trivial 
case expected quadratic loss quantity estimate choose predictor giving worstcase expected prediction loss min gamma 
note improved technique communicated warmuth 
technique replaces rule predicts probability gamma 
argument similar proof theorem shown rule predictive loss gamma delta 

application weak agnostic learning ac immediately apply theorem existing algorithms standard pac model obtain algorithms weak agnostic learning 
instance linial mansour nisan describe algorithm standard pac model target domain distribution restricted uniform hypothesis space algorithm class functions fourier expansion called parity basis high order coefficients coefficients basis functions size exceeds 
algorithm runs time polynomial ffl ffi 
shown algorithm finds real valued theta ffl provided boolean target function close hypothesis restricted hypothesis class optimal expected prediction loss close zero 
theta guaranteed near optimal agnostic setting unrestricted 
algorithm linial mansour nisan find nearly minimizes agnostic setting apply theorem show boolean target function min gamma bounds expected prediction loss 
instance means exists ac function weakly approximates target function uniform distribution agrees probability gamma polynomial results linial mansour nisan combined theorem imply existence quasi polynomial time algorithm finding hypothesis weakly approximates summarize ideas corollary corollary exists algorithm properties 
algorithm ffl ffi access randomly generated examples function 
fl exists ac circuit size depth property pr gamma fl 
probability gamma ffi algorithm finds hypothesis function pr gamma fl ffl probabilities computed respect uniform distribution 
algorithm runs time polynomial ffl log ffi gamma lg ffl delta kearns schapire sellie proof sketch proof uses properties fourier transform described detail linial mansour nisan 
function written form ng gamma useful fact parseval identity statement corollary defined ae gamma fl fl shown gamma gamma fl function defined jsj jsj sort mixture parseval identity gamma gamma gamma fl approximate function running algorithm linial mansour nisan choices ffi ffl set ffl 
access examples function algorithm linial mansour nisan approximates low order coefficients sets high order coefficients zero 
resulting hypothesis 
parseval identity definition gamma jsj gamma jsj sum bounded accuracy approximation coefficients second sum bounded main lemma linial mansour nisan lemma 
result gamma ffl 
gamma gamma gamma follows gamma gamma fl ffl 
theorem gamma fl ffl 
conclude section mentioning theorem generalized model target function discrete function assuming possible values output normalized vector intended model settings character recognition attempt find real valued hypothesis wish predict character represented input greatest possible accuracy 
efficient agnostic learning 
hidden variable problems far striving algorithms find hypothesis assumption target function arbitrarily complex 
insight frequently empirical theoretical machine learning communities function arbitrarily complex variable sets define new variables compute significant subfunctions target function representation target function may simplify dramatically 
approach simplifying target functions loosely referred feature discovery 
difficulty approach course right features may difficult discover target function fact scientific domains frontier research focuses just finding quantities relevant phenomenon may uncovered long periods experimentation theory 
section focus problem discovering features problem learning relevant variables known visible hidden 
motivated simultaneous realizations target functions may simple representations appropriate variable set variables may known time 
hidden variable model allows intermediate step strong assumptions standard pac model full 
model previously investigated kearns schapire 
disjoint sets variables 
say variables visible variables hidden 
setting learner observes random examples classified deterministic boolean function entire variable set learner allowed observe values visible variables 
assignment visible variables label assigned appears probabilistic 
specifically probability labeled just probability assignment chosen hidden variables causes evaluate 
learner appears examples labeled concept visible variables conditional probability visible variables assigned pr 
view hidden variable problems concept problems domain set assignments visible variables 
section goal find best possible predictor induced concept chosen class functions words interested finding rule called bayes optimal predictor minimizes expected prediction loss assume touchstone class large include bayes optimal predictor necessary assume independence distributions assignments hidden visible variables possible construct trivial target functions arbitrary 
kearns schapire sellie easy example suppose function chosen set conjunctions literals particular suppose conjunction rs variables hidden visible respectively 
hard see equals probability 
note bayes optimal constant function just conjunction shown kearns schapire approximate bayes optimal predictor applying valiant algorithm conjunctions approximate conjunction estimating approximation goal section obtain similar result general class term dnf 

algorithm term dnf hidden variable problems case conjunctions bayes optimal predictor zero function restriction conjunction 
restriction dnf formula formula obtained syntactically deleting hidden variables 
may general seen case term dnf formulas 
example suppose formula hidden visible 
suppose independently probability 
case bayes optimal predictor restriction formula generally term dnf formula delta delta delta terms respectively 
note behavior concept exactly determined values assumption hidden visible variables independent 
define probability 
furthermore seen monotone sense 

pr pr seen bayes optimal predictor need restriction fact hard come term formula distribution hidden variables half terms satisfied 
case bayes optimal predictor expressed dnf formula visible variables exponentially large 
original formula may quite simple bayes optimal predictor induced concept may quite complicated 
exist efficient algorithm finding bayes optimal predictor term dnf formula 
show represented probabilistic decision list increasing probabilities class concepts known exist efficient algorithm approximating bayes optimal predictor kearns schapire 
similar technique blum chalasani 
efficient agnostic learning probabilistic decision list pdl variable set sequence pairs conjunction literals 
require constant function slightly convenient requirement equivalent requirement 
defined index 
list said increasing probabilities see kearns schapire yamanishi background probabilistic decision lists 
kearns schapire show pdl increasing probabilities learned model probability describe algorithm finding approximation list expected difference jh gamma small 
suffices show pdl increasing probabilities kearns schapire algorithm find bayes optimal predictor furthermore find model function 
theorem term dnf formula 
equivalent pdl increasing probabilities 
proof show pdl increasing probabilities 
regard function variables possible assignment hz 
list consisting exactly set pairs assignments ordered fashion increasing probabilities 
claim see note increasing probabilities 
see observe 
definition means monotonicity implies 
claimed 
substitution 
list consisting pairs easily seen law dnf formula delta delta delta variables replace pair sequence pairs 
applying operation easily verified resulting list pdl increasing probabilities equals noted algorithm described kearns schapire applied prove corollary corollary term dnf formula variable set exists efficient algorithm finding bayes optimal predictor induced concept assignments kearns schapire sellie 
cnf may harder term dnf section give evidence suggesting learning may difficult target function cnf formula 
specifically show cnf exist cases bayes optimal predictor arbitrarily complicated requiring exponentially large representation 
delta delta delta dnf formula terms contain exactly visible variables 
note may exponentially large 
show create distribution hidden variables bayes optimal function term define assignment jth bit occur ff gamma number terms gamma ff ff assignments assignment visible variables 
term satisfied satisfied hidden variables assigned vector satisfied ff 
satisfied satisfying assignment hidden variables nonzero probability gamma ff case 
claimed bayes optimal predictor exactly exists doubly exponential number formulas specifically omega gamma formulas implies representation bayes optimal functions bayes optimal predictor exponentially long representation 
note functions construction easily approximated constant sized representation ff small close assignments remains open result section extended handle cnf formulas 

open problems fruits initial investigation properties agnostic learning models 
done area plausible right model obtaining powerful positive results choose middle ground balances assumptions target functions assumptions domain distributions remaining applicable problems arising practice 
simply studied extreme set assumptions order obtain idea accomplished efficiently 
main open research direction explore limits efficient learning algorithms agnostic models 
problems exist efficient learning algorithms 
instance section showed learn concepts induced partially visible term dnf formulas 
result extended handle cnf formulas 
problem may harder bayes efficient agnostic learning optimal predictor extremely complicated 
hand come case exist simple function approximates bayes optimal predictor 
trying find efficient algorithms specific learning problems explore theoretical power known algorithms 
ask proved capabilities various theshelf learning algorithms commonly practice neural networks decision tree algorithms 
understand theoretical properties models discussed 
instance fully agnostic pac model situation membership queries useful 
intuitively membership queries give power answers queries arbitrary target function arbitrary 
far unable derive rigorous proof intuition 
portions research conducted kearns international computer science institute schapire harvard university supported afosr nsf ccr sellie visiting bell laboratories 
grateful avrim blum pointing probabilistic function terms expressed pdl 
anonymous referees careful reading helpful comments 
notes 
best knowledge recollection term agnostic learning coined discussion sally goldman ron rivest authors 

certain assumptions required see haussler details 

ac class boolean functions computed constant depth boolean circuit composed unbounded fan gates 
aldous vazirani 

markovian extension valiant learning model 
st annual symposium foundations computer science pp 

blum chalasani 

learning switching concepts 
proceedings fifth annual acm workshop computational learning theory pp 

blumer ehrenfeucht haussler warmuth 

learnability vapnik chervonenkis dimension 
journal association computing machinery 
duda hart 

pattern classification scene analysis 
wiley 
dudley 

central limit theorems empirical measures 
annals probability 
kearns schapire sellie freund 

boosting weak learning algorithm majority 
proceedings third annual workshop computational learning theory pp 

freund 

improved boosting algorithm implications learning complexity 
proceedings fifth annual acm workshop computational learning theory pp 

garey johnson 

computers intractability guide theory np completeness 
san francisco freeman 
haussler 

decision theoretic generalizations pac model neural net learning applications 
information computation 
helmbold long 

tracking drifting concepts minimizing disagreements 
machine learning 
hoeffding 

probability inequalities sums bounded random variables 
journal american statistical association 


developments nonparametric density estimation 
journal american statistical association 
kearns li 

learning presence malicious errors 
siam journal computing 
kearns li pitt valiant 

learnability boolean formulae 
proceedings nineteenth annual acm symposium theory computing pp 

kearns valiant 

cryptographic limitations learning boolean formulae finite automata 
journal association computing machinery 
kearns schapire 

efficient distribution free learning probabilistic concepts 
st annual symposium foundations computer science pp 

appear journal computer system sciences 
linial mansour nisan 

constant depth circuits fourier transform learnability 
journal association computing machinery 
pitt valiant 

computational limitations learning examples 
journal association computing machinery 
pollard 

convergence stochastic processes 
springer verlag 
rissanen speed yu 

density estimation stochastic complexity 
ieee transactions information theory 
schapire 

strength weak learnability 
machine learning 
valiant 

theory learnable 
communications acm 
valiant 

learning disjunctions conjunctions 
proceedings th international joint conference artificial intelligence pp 

vapnik 

estimation dependences empirical data 
springer verlag 
white 

learning artificial neural networks statistical perspective 
neural computation 
yamanishi 

learning criterion stochastic rules 
machine learning 
yamanishi 

learning nonparametric densities terms finite hypotheses 
ieice transactions information systems 
