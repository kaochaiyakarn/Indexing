efficient agnostic learning linear combinations basis functions wee sun lee dept systems engineering aust 
national university canberra act australia 
lee anu edu au peter bartlett dept systems engineering aust 
national university canberra act australia 
peter bartlett anu edu au robert williamson department engineering australian national university canberra act australia 
bob williamson anu edu au consider efficient agnostic learning linear combinations basis functions sum absolute values weights linear combinations bounded 
quadratic loss function show class linear combinations set basis functions efficiently agnostically learnable class basis functions efficiently agnostically learnable 
show sample complexity learning linear combinations grows polynomially combinatorial property class basis functions called fat shattering function grows polynomially 
relate problem agnostic learning valued function classes showing class valued functions efficiently agnostically learnable function class discrete loss function class linear combinations functions class efficiently agnostically learnable quadratic loss function 
study efficient polynomial time learning linear combinations basis functions robust extension popular probably approximately correct pac learning model computational learning theory 
learning model commonly called agnostic learning robust respect noise mismatches model phenomenon modelled 
assumption phenomenon represented joint probability distribution theta domain bounded subset model adequately captures features practical learning problems measurements noisy little known target functions 
assumptions expect learning algorithm produce hypothesis small error 
demand high probability hypothesis produced close optimal class functions 
error hypothesis observation measured loss function demand expected loss hypothesis close smallest expected loss functions class 
generality agnostic learning model allows special cases learning real valued functions learning conditional expectation observations noisy learning best approximation target function class learning probabilistic concepts 
function classes study include widely practice 
linear combinations basis functions considered generalization layer neural networks 
usual sigmoidal hidden units arbitrary bounded function classes satisfying mild measurability constraints allowed basis function classes 
includes radial basis functions polynomial basis functions restrictions inputs 
bound number basis functions linear combination insist sum absolute values weights linear combination bounded 
extension results shown class linear combinations linear threshold functions bounded fanin efficiently agnostically learnable 
related works include koiran considered learning layer neural networks piecewise linear activation functions agnostic model maass agnostic learning fixed sized multilayer neural networks piecewise polynomial activation functions 
say function class ae gammab efficiently agnostically learnable exists hypothesis class ae gammab learning algorithm produces hypothesis probability gamma ffi expected loss ffl away expected loss best function algorithm runs time polynomial ffl ffi bound observation range appropriate complexity parameters 
commonly complexity parameters include dimension input space number parameters function class hypothesis class necessarily function class allows agnostic learning function class larger hypothesis class situation shown computational advantages certain cases learning pac framework 
section show quadratic loss function linear combinations basis functions bounded sum absolute values weights efficiently agnostically learnable basis function class efficiently agnostically learnable 
means show class linear combinations efficiently agnostically learnable need show basis function class efficiently agnostically learnable 
efficiently agnostically learnable classes basis functions include function classes enumerated time polynomial sample size complexity parameters fixed sized neural networks piecewise polynomial activation functions 
similarly representation independent hardness result learning basis function class imply hardness result class linear combinations functions basis function class 
section combinatorial property basis function class called fat shattering function bound covering number linear combinations provide sample complexity bounds better obtained results section :10.1.1.21.996
koiran provided sample complexity bounds case basis functions valued functions bounding fat shattering function convex closure basis function class 
lower bound results show class linear combinations basis functions sample complexity grows polynomially ffl complexity parameters function class basis functions grows polynomially ffl complexity parameters 
section give examples function classes efficiently agnostically learnable 
section shows class valued basis functions learned efficiently agnostically valued targets discrete loss function hypothesis class class linear combinations basis functions efficiently agnostically learned quadratic loss function 
relates efficient agnostic learning linear combinations valued basis functions agnostic learning valued functions 
unfortunately results agnostic learning negative 
section discuss hardness results open problems 
definitions learning model agnostic learning model agnostic learning model agnostic learning model described kearns schapire sellie :10.1.1.55.3936
set called domain points called instances 
observed range 
call pair theta observation 
assumption class class probability distributions theta represent assumptions phenomenon learned 
results hold class probability distributions theta special cases results automatically hold 
regression represents noisy measurement real valued quantity desired quantity learned conditional expectation case noise function learning class functions mapping ad formed distribution function observations drawn ad form drawn randomly learning probabilistic concepts 
class functions mapping ad formed distribution function observations drawn ad form drawn randomly probability probability gamma 
bounded subset touchstone class hypothesis class classes functions learning algorithm attempt model behaviour functions behaviour approximated functions require method judging hypothesis acceptable 
done requiring learning algorithm produces hypothesis performance close best touchstone class introduced computational reasons 
case unable find efficient algorithm learning touchstone class find efficient algorithm learning larger hypothesis class contains touchstone class 
resulting hypothesis learning algorithm may belong touchstone class performance close performance best function touchstone class 
best function function minimizes expected value loss function theta function loss 
main loss function quadratic loss function gamma discrete loss function 
observations drawn expected loss denote clear context 
class define opt inf 
quadratic loss function natural choice regression function minimum quadratic loss conditional expectation 
learning probabilistic concepts quadratic loss desirable properties shown :10.1.1.55.3936
results hold uniform cost logarithmic cost models computation see 
uniform cost model real numbers occupy unit space standard arithmetic operations addition multiplication take unit time 
logarithmic cost model real numbers represented finite precision operations charged time proportional number bits precision 
assume observation range gammat known hypothesis touchstone classes outputs range gammab 
learning problem indexed tb hb logarithmic cost model computation index problem rt bit length observation drawn rt bounded polynomial representation bit length bounded polynomial domain distribution class function class indexed natural complexity parameters dimension input space number parameters function class 
definition class functions tb efficiently agnostically learnable respect loss function exists function class hb function ffl ffi bounded fixed polynomial ffl ffi algorithm ffi ffl algorithm draws ffl ffi observations halts time bounded fixed polynomial ffl ffi outputs hypothesis hb probability gamma ffi opt ffl 
hypothesis evaluable time polynomial ffl ffi say properly efficiently agnostically learnable 
logarithmic cost model rt allow depend ffl ffi insist algorithm halt hypothesis evaluable time polynomial ffl ffi proper efficient learning touchstone class insist hypothesis number bits allowed grow polynomially ffl 
learning problem indexed complexity parameters insist running time algorithm evaluation time hypothesis polynomial parameters 
simplicity uniform cost model 
basis functions linear combinations definition class real valued functions admissible class basis functions permissible exists jg definition admissible class basis functions 
jw class linear combinations functions sum magnitudes weights bounded mild measurability condition satisfied function classes learning 
see haussler details 
equivalence efficient learning section show class bounded linear combinations functions admissible class basis functions efficiently agnostically learnable class basis functions efficiently agnostically learnable 
agnostic learning algorithm basis function class subroutine learn linear combinations basis functions assumed hypothesis class bound range output 
need approximation result 
result extension results jones barron 
related result koiran 
theorem hilbert space norm delta subset convex hull inf gamma suppose iteratively chosen satisfy gamma inf ff gamma gamma ff gamma ffl ff gamma ffl gammab fi fi gammafi gamma gammad ck fi fi hilbert space measurable functions inner product dp probability distribution admissible class basis functions class linear combinations functions derived convex hull fwg jwj gg 

kb theorem obtain approximation best function approximation result requires target function hilbert space 
agnostic learning target random variable 
quadratic loss minimizing loss respect observations minimizing loss respect conditional expectation function hilbert space target bounded 
theorem admissible class basis functions 
efficiently agnostically learnable respect quadratic loss function efficiently agnostically learnable respect quadratic loss function 
proof 
part trivial ffl rescaled subset function class convex hull fwg jwj gg 
kb theorem shows get ffl best expected loss number iterations equal ck fi ffl fi 
set ffl assume agnostic algorithm learning produces hypothesis ae gammab target function conditional expectation target input 
minimizing quadratic loss respect joint distribution equivalent minimizing quadratic loss respect conditional expectation 
satisfy theorem ith iteration find thetay gamma gamma gamma dp inf thetay gamma gamma gamma dp ffl gamma linear combination far 
furthermore thetay wg gamma gamma gamma dp thetay gamma gamma gamma gamma dp agnostic learning algorithm respect new target random variable magnitude bounded upper bound magnitude 
set confidence ffi accuracy ffl probability gamma ffi hypothesis produced thetay wh gamma gamma gamma dp inf thetay gamma gamma gamma dp ffl inf thetay wg gamma gamma gammay dp ffl done gammak iteration produce hypotheses choose 
way choosing hypotheses hypothesis testing 
hoeffding inequality sample size ln ln ffi ffl large empirical quadratic loss ffl expected quadratic loss functions probability gamma ffi 
choose hypothesis smaller empirical loss expected loss ffl away expected loss better hypothesis probability gamma ffi 
iteration efficient agnostic learning algorithm learning produce hypothesis satisfies requirements theorem probability gamma ffi probability failure iterations ffi produced learning algorithm easy check algorithm learning polynomial relevant parameters resulting algorithm learning polynomial desired parameters 
sample complexity proper efficient agnostic learning obtain sample complexity bounds learning problems certain combinatorial properties class basis functions pseudo dimension function known 
pseudo dimension obtain sample complexity bounds learning function classes particular multilayer neural networks haussler 
finiteness necessary condition agnostic learning 
example class non decreasing functions map shown learnable infinite pseudo dimension 
sequence points fl shattered ae gammab exists gammab fl gamma fl 
fl fat fl fl shatters maximum exists 
section give sample complexity bounds fat shattering function class basis functions show agnostically learnable polynomial sample complexity disregarding computational complexity fat shattering function grows polynomially ffl complexity parameters 
bounds show efficient algorithm proper agnostic learning linear combinations basis functions constructed proper efficient agnostic algorithm learning basis functions 
algorithm section significantly different algorithm previous section sample size determined drawn drawn addition new basis function linear combination 
sample complexity obtained approximately fat ffl ffl ignoring fi log factors 
comparison method section proper learning obtain sample size bound fat ffl ffl ffl ignoring fi log factors 
shown efficient agnostic learning function class absolute loss function possible fat shattering function function class grows polynomially ffl relevant complexity parameters 
essentially minor modifications 
theorem class valued functions defined suppose fl ffl fl ffi fat fl algorithm agnostically learn respect quadratic loss function accuracy ffl probability gamma ffi fewer log fl examples 
note gammab valued function class transformed valued function class adding function class dividing 
ffl fl similarly transformed lower bound holds gammab valued functions 
bounding covering numbers want bound sample complexity learning terms fat shattering function theorem bound number basis functions needed linear combination 
need able obtain upper bounds sample complexity linear combinations fixed arbitrary number basis functions 
jv gamma set metric pseudo metric ae ffl cover finite set ae ffl 
covering number ffl ae denotes size smallest ffl cover set jz denote 
set functions write jz fh jz hg 
loss function theta define lh fl hg 
define 
write meaning clear context 
theorem haussler suppose class functions mapping loss function theta lh permissible 
probability distribution theta ffl fi fi fi gamma fi fi fi ffl ffl lh jz gammaffl expectation drawn randomly class functions probability distribution pseudo metric defined jf gamma gj jf gamma bound ffl terms function 
provides bound ffl jx finite sequence points isometry jx jx jx empirical distribution 
generalization sauer lemma alon techniques haussler 
theorem fwg jwj gg probability distribution gg fixed 
ffl exp ffl kb ln ln fat ffl kb ffl ln proof omitted 
theorem need bound covering number lh quadratic loss function 
bound covering number loss function class qh terms covering number lemma 
lemma class functions gammab 
gammat theta ffl qf jz ffl jx theta learning algorithm give relationship learning problem optimization problem training sample 
method similar technique haussler 
ym function class opt inf gamma lemma ffl ffi function classes sample size ffl ffi probability distribution theta fi fi fi gamma fi fi fi ffl ffi 
suppose gamma opt ffl randomized algorithm produces sample size drawn pr ifi fi fi gamma opt fi fi fi ffl ffi 
pr ifi fi fie gamma opt fi fi fi ffl ffi probabilities taken random samples randomization algorithm 
proof sketch 
probability greater gamma ffi simultaneously fi fi fi gamma fi fi fi ffl fi fi fi opt gamma opt fi fi fi ffl 
desired result obtained triangle inequality 
show algorithm properly agnostically learning obtain randomized algorithm optimization required lemma 
theorem admissible function class 
properly efficiently agnostically learnable properly efficiently agnostically learnable 
furthermore sample complexity properly efficiently learning ffl kd ln ln ffl ln ln ffi fi fi ffl fi fat ffl 
proof 
aim proof set conditions lemma holds show done efficiently 
fwg jwj gg 
theorem note fixed sequence weights approximation rate ck fi fi achieved 
gg 
range gammab 
range observation gammat set independently sampled points theta form empirical distribution weighting member proportion number times point appears theorem find number gamma opt ffl 
empirical distribution exists fi fi fi gamma opt fi fi fi ffl 
opt opt fi fi fi gamma opt fi fi fi ffl 
theorem obtain approximation accuracy ffl fi fi ffl fi suffice 
theorem theorem lemma fi fi ffl fi finite sample size ffl exp kd ln ln ffl jj fat ffl 
theorem samples drawn distribution fi fi fi gamma fi fi fi ffl exp kd ln ln ffl jj gammaffl setting right hand side ffi satisfy lemma obtain sample complexity bound 
properly efficiently agnostically learnable theorem implies fat shattering function bounded polynomial ffl complexity parameters sample size bound learning polynomial ffl ffi complexity parameters 
need show proper efficient agnostic learning algorithm efficient randomized algorithm optimizing error sample idea learning algorithm sample learn empirical distribution stage iterative approximation error relative optimum ffl theorem probability greater gamma ffi 
proof theorem test hypotheses directly sample 
knowing fat shattering function enables bound size sample required sampled empirical distribution 
note sampling empirical distribution new observations need drawn original distribution 
theorem assures successful iteration ffl minimum error sample required lemma 
efficiently learnable function classes section give examples efficiently agnostically learnable function classes 
efficiently enumerable function classes shown linear combination linear threshold units bounded sum magnitudes weights efficiently agnostically learnable fan ins linear threshold units bounded 
complexity parameter case dimension input space basis function class class linear threshold units bounded fan 
similarly fixed dimension bounded fan linear combination axis parallel rectangles magnitudes weights efficiently agnostically learnable 
results generalized corollary particularly useful basis function classes 
corollary admissible basis function class 
xm arbitrary sequence points jx enumerated time polynomial complexity parameters properly efficiently agnostically learnable 
proof 
number functions jx polynomial complexity parameter fat shattering function bounded logarithmic function complexity parameter 
functions efficiently enumerated choosing function minimizes loss large polynomial sample size result efficient learning algorithm neural networks piecewise polynomial activation functions fixed 
fx 
oe delta gamma magnitudes threshold component weight vector oe oe oe 
possible enumerate jx number possible outputs finite 
shown koiran possible enumerate possible combinations inputs linear pieces activation function 
proper parametrization optimization problem solved solving family quadratic 
fixed dimension linear combination functions bounded sum magnitudes weights properly efficiently agnostically learnable 
maass shown fixed architecture neural network arbitrary number hidden layers piecewise polynomial activation functions efficiently agnostically learnable respect absolute loss function jy gamma result holds quadratic loss function 
class linear combinations networks bounded sum magnitudes weights efficiently agnostically learnable 
note maass larger hypothesis class learn class 
function class shown properly efficiently agnostically learnable 
relationship agnostic pac learning class valued functions 
target function chosen class valued functions kearns schapire sellie call proper efficient agnostic learning discrete loss assumptions agnostic pac learning :10.1.1.55.3936
section show agnostically pac learnable properly efficiently agnostically learnable 
shown jones iterative approximation result holds inner product basis function gamma target function current network minimized empirical quadratic error 
true proof koiran 
property transform problem minimizing inner product finite set examples problem agnostic pac learning 
theorem follows proof theorem minor changes 
theorem subset hilbert space convex hull inf gamma iteratively suppose chosen gamma gamma gamma chosen satisfy gamma gamma inf gamma gamma ffl ffl gamma gamma gammad cd theorem class admissible valued basis functions 
properly efficiently agnostically learnable quadratic loss agnostically pac learnable 
proof sketch 
set conditions necessary lemma 
target range bounded easily find bound theorem pick number basis functions linear combinations obtain approximation ffl 
fwg jwj gg 
theorem note fixed sequence weights desired approximation rate achieved 
gg 
fat shattering function bound pick sample large get accuracy confidence required lemma 
agnostically pac learnable fat shattering function vc dimension functions polynomial complexity parameters 
theorem shows approximating accuracy ffl iteration respect empirical distribution provide hypothesis desired error 
iteration find function nearly minimizes gamma gamma wg gamma gamma conditional expectation sample empirical distribution 
possible show equivalent agnostic pac learning function gamma gamma gamma gamma distribution jf gamma gammaf jf gamma gamma similar procedure carried gammak functions produced compared better chosen 
discussion open problems interesting open problem find limits complexity basis functions allow efficient agnostic learning 
agnostic pac learning basis functions available results include hardness learning monomials halfspaces assumption rp np 
implies networks functions classes efficient algorithm obtained approach sections 
rule efficient learning methods hypothesis classes requires representation independent hardness results 
shown class monomials efficiently agnostically learnable hypothesis class respect discrete loss function class dnf efficiently learnable pac learning model :10.1.1.55.3936
polynomial size dnf learned efficiently open problem computational learning theory posed valiant majority view polynomial sized dnf efficiently learnable 
techniques similar possible show class valued basis functions include monomials efficient agnostic learning algorithm class quadratic loss function efficiently find randomized hypothesis polynomial sized dnf :10.1.1.55.3936
say hypothesis randomized exists probabilistic polynomial time algorithm instance computes prediction 
assume hard find learning algorithm dnf agnostically learning basis function classes network basis functions hard 
theorem permissible class valued functions class monomials subset jf polynomial efficiently agnostically learnable respect quadratic loss function exists efficient algorithm produces randomized hypotheses learning term dnf 
proof sketch 
show exists weak learning algorithm produces randomized hypotheses term dnf 
result follows schapire boosting technique converting weak learning algorithm strong 
target term dnf exists monomial error negative example gets positive examples right terms cover positive examples 
equivalent monomial restricted 
quadratic error negative examples 
positive examples quadratic error zero monomial gives correct classification gives wrong classification 
algorithm producing randomized hypothesis goes follows 
get sufficiently large sample 
ex ample chernoff bounds get sufficient sample size 
significantly half examples labelled positive monomial classification 
significantly half examples labelled negative zero monomial classification 
probabilities negative positive examples approximately equal 
agnostic learning algorithm learn function quadratic loss 
section efficiently agnostically learnable efficiently agnostically learnable 
suppose probability positive example 
argument shows exists function expected quadratic error gamma gamma gamma delta gamma target dnf assume hypothesis away optimum 
theorem pr gamma boolean random variable probability zero probability gamma :10.1.1.55.3936
algorithm weak learning algorithm produces randomized hypotheses learning term dnf 
interesting find function classes properly efficiently agnostically learnable efficiently agnostically learnable hypothesis classes 
unaware example agnostic learning pac learning function classes learnable larger hypothesis classes target function classes term dnf properly pac learnable learnable cnf hypothesis class see 
pascal koiran numerous discussions resulted substantial improvements results 
partially supported australian research council 
aho hopcroft ullman 
design analysis computer algorithms 
addison wesley london 
alon ben david cesa bianchi haussler 
scale sensitive dimensions uniform convergence learnability 
proc 
th annu 
ieee sympos 

comput 
sci 
angluin valiant 
fast probabilistic algorithms hamiltonian circuits matching 
comput 
syst 
sci 
anthony biggs 
computational learning theory 
cambridge tracts theoretical computer science 
cambridge university press 
barron 
universal approximation bounds superposition sigmoidal function 
ieee trans 
information theory 
bartlett long williamson 
learnability real valued functions 
proc 
th annu 
acm workshop comput 
learning theory pages 
acm press new york ny 
blumer ehrenfeucht haussler warmuth 
learnability vapnik chervonenkis dimension 
acm 
koiran 
approximation learning real valued functions 
proc 
nd european conf 
comput 
learning theory 
haussler 
decision theoretic generalizations pac model neural net learning applications 
inform 
comput september 
hoeffding 
probability inequalities sums bounded random variables 
journal american statistical association march 
simon 
robust single neurons 
proc 
th annu 
workshop comput 
learning theory pages newyork ny 
acm press 
jerrum 
simple translation invariant concepts hard learn 
inform 
comput september 
jones 
simple lemma greedy approximation hilbert space convergence rates projection pursuit regression neural network training 
annals statistics 
kearns schapire 
efficient learning probabilistic concepts 
comput 
syst 
sci 
kearns schapire sellie :10.1.1.55.3936
efficient agnostic learning 
machine learning 
koiran 
efficient learning continuous neural networks 
proc 
th annu 
acm workshop comput 
learning theory pages 
acm press new york ny 
lee bartlett williamson 
efficient agnostic learning neural networks bounded fan 
technical report department systems engineering australian national university 
maass 
agnostic pac learning functions analog neural networks 
technical report institute theoretical computer science technische universitaet graz graz austria 
schapire 
strength weak learnability 
machine learning 
valiant 
theory learnable 
commun 
acm november 
