overcoming incomplete perception utile distinction memory andrew mccallum department computer science university rochester rochester ny mccallum cs rochester edu presents method reinforcement learning agent solve incomplete perception problem memory 
agent uses hidden markov model hmm represent internal state space creates memory capacity splitting states hmm 
key idea test determine state split agent splits state doing help agent predict utility 
agent create memory needed perform task hand required model perceivable world 
call technique udm utile distinction memory 
researchers explore problem learning situated behaviors robotic tasks face dual blessing curse perceptual aliasing whitehead ballard perceptual aliasing occurs mapping agent perception world different world situations 
blessing represent equivalent different world states action required 
curse confound different world states different actions required 
ideally keep generalization get rid bad 
task making desirable distinctions perceptually aliased states difficult agent suffers incomplete perception chrisman time redirection perceptual system produce immediate percepts distinguish significantly different world states agent incomplete perception 
world states significantly different agent best action 
incomplete perception function perceptual system world task 
example maze constructed identical looking barriers hallways different locations may appear matter carefully examines current surroundings 
situations agent require additional information disambiguate aliased states 
way introducing kind memory past percepts actions 
agent distinguish current location maze locations remembering got 
instance walking department blindfolded know north hallway south hallway remember turned right exiting elevator 
chrisman technique called perceptual distinctions approach solve incomplete perception problem chrisman agent builds hidden markov model rabiner acts predictive model environment 
states hmm internal states agent estimated discounted reward values action state action values stored hmm states 
agent uses watkins learning watkins agent retrieves adjusts values state action values hmm states proportionally hmm state occupation probabilities 
technique increases number hmm states statistical agent ability predict perception 
new state occupation probabilities hmm depend current percept previous state probabilities previous action hmm state represent memory 
instance possible set transition probabilities hmm extremely agent arrived particular state agent come particular previous state 
building chains states exclusive transition probabilities occupation state represent arbitrary amount memory past percepts actions 
plain table learning perception learning hmm perception actions states discounted reward action state influences choosing current state perception transition probability previous state action plain table learning current state determined entirely current percept 
hmm agent determine current state current percept previous action previous state 
plain learning internal states correspond rows table 
internal states agent hmm correspond states hmm 
case internal state holds collection discounted reward estimates action 
utile distinction memory udm method report shares hmm foundation chrisman technique 
uses hmm represent agent internal state space keeps state action values states hmm retrieves adjusts state action values state occupation probabilities 
chief difference lies way size hmm state space increased 
creating new states predicting perception udm creates new states show statistically significant increases agent ability predict reward 
difference causes fundamental shift agent representational approach 
utility distinctions build agent internal state space large needed perform task hand perception distinctions build reinforcement learning task performance defined reward agent receives 
state space large represent world agent perceives 
agent task simple world complex agent perceive manifestations world complexity agent internal state space simple 
difficulty learning proportional difficulty task complexity world 
utility distinctions memory agent know splitting state help predict utility 
perceptually aliased state affects utility wildly fluctuating reward values base state splitting test solely reward variance changes reward caused stochastic nature world splitting state help agent consistently get high reward 
agent able distinguish changes reward caused stochastic world changes predicted performing split 
new information splitting state give agent 
splitting state agent ability distinguish previously aliased states knowledge state came 
key idea utile distinction memory follows udm keeps discounted reward information associated incoming transitions state 
state satisfies markov property reward values similar statistically significant differences rewards identity state agent comes significant predicting reward state non markovian splitting state help agent consistently predict reward 
split may allow distinctions create new separate transitions states 
udm recursively build tree distinctions similar chapman kaelbling algorithm chapman kaelbling builds distinctions memory space perception space 
details algorithm slightly longer description udm mccallum hidden markov model comprised finite set states fs finite number observations percepts fo include finite set actions fa ak state vector observation probabilities write js probability seeing observation inclusion actions partially observable markov decision process 
state state action pair vector transition probabilities notation js signifies probability executing action state result state agent belief state world maintained vector state occupation probabilities written agent belief world state represented time calculate state occupation probabilities time agent uses forward part baum forward backward procedure rabiner delta js js constant needed agent observation time action executed agent time choosing actions altering state action values reward actions udm uses learning superimposed hidden markov models just chrisman write stateaction value action state agent obtains values representative agent current belief world state state action values kept hmm states letting hmm states vote proportion occupation probability agent chooses action largest value argmax 
stateaction values updated standard learning rule modified state occupation probabilities gamma fi fi flu max fi fi learning rate fl fl temporal discount factor expected utility current state agent beliefs states may 
effectively state occupation probability part learning rate 
far diverged rabiner chrisman describe part algorithm specific udm 
incoming transitions state udm keeps statistics discounted reward received leaving state question 
discounted reward called return 
state markovian respect return return values incoming transitions similar 
statistically significant differences incoming transitions splitting state appropriately dividing transitions split states help agent predict return 
possible outgoing actions state consideration splitting 
confidence intervals discounted reward possible actions 
state consideration splitting 
associated incoming transitions udm keeps confidence intervals discounted reward action executed state question 
perceptually aliased states world may utility require different actions udm keeps separate statistics different actions executed hmm state 
statistical significance tested confidence intervals calculate method described kaelbling interval keep running count sum values sum squares values upper lower bounds calculated sigma gamma ff sample mean gamma gamma sample standard deviation gamma ff student function gamma degrees freedom 
parameter ff determines confidence values fall inside interval 
return statistics kept transitions change agent state action values determine split state 
agent begins fully connected hidden markov model containing state percept 
observation probabilities js preset percept biased different state 
transition probabilities js equal 
agent executes series step trials 
trial updates values equations keeps record currently udm ability discover beneficial memory distinctions perceptual distinctions model initialized distinguish percepts 
split split confidence intervals discounted reward determining state split confidence intervals discounted reward 
consider incoming transitions state compare set confidence intervals corresponding action executed state 
intervals overlap agent determined treating state differently depending state came get statistically significant increase ability predict reward 
set intervals divided clusters overlapping intervals cluster assigned different copy state 
confidence interval table upper right shows intervals outgoing action 
actions percepts rewards history arrays 
length trial agent runs tests determining states split 
begins performing baum welch procedure updating model parameters 
procedure improves model ability percepts actions distinguish agent current state test splitting states separate baum welch agent create new memory capacity order predict perception 
process baum welch procedure agent calculates improved estimates state occupation probabilities time estimates transition occupation probabilities time jt probability time agent passed state state action 
see rabiner details 
agent calculates return values time return reward history return gamma return fl delta return fl temporal discount factor 
state aliased state action values resulting meaningless combination rewards different world states state action values state utilities defined maximum state action values trusted reward directly world calculate return 
agent information necessary assign return values specific transitions 
agent considers value return time step includes value statistics transition proportion transition occupation probability previous time step gamma transitions trans action gamma lr fi delta trans gamma trans count lr trans sum lr delta return trans lr delta return fi agent learning rate lr learning rate particular transition particular time trans gamma transition occupation probability jt gamma time gamma particular transition trans belonging source state action destination state trans dot quantities lines refer statistics necessary computing upper lower bounds trans count count trans sum sum trans sum squares 
subscripts indicate indexed different actions executed destination state transition 
count sum sum squares agent obtain upper lower bounds return value confidence intervals transition 
agent determines returns incoming transitions statistically significant measuring confidence intervals overlap 
confidence intervals outgoing action fail overlap state split 
agent divides set incoming transitions disjoint subsets transitions subset overlap subset large possible 
perform clustering optimally np complete problem minimum cover garey johnson practice greedy solutions adequate 
clustering state duplicated including incoming outgoing transitions times copy cluster 
incoming transitions elements duplicate state assigned cluster removed 
states may joined union incoming transitions overlap set incoming transitions subset 
analysis length trial results splits joins trans dot statistics reinitialized 
mazes successfully solved udm 
agent perception defined bit vector length bits specify barrier agent immediate north east south west 
numbers squares decimal equivalents bit vectors interpreted binary 
state perceptually equivalent agent learn go south center go north right left 
hidden markov model built udm states high probabilities observing representing center combining right left 
percept aliased split different actions required world states 
see diagrams hmm built solve maze 
experimental results demonstrated udm working grid world perception defined immediately adjacent barriers 
local perception grid world simulated worlds sutton whitehead thrun 
agent moves discrete grid executing actions north east south west 
agent attempts move grid occupied barrier agent remains receives reward gamma 
agent moves goal squares receives reward 
actions agent receives reward gamma 
reaching goal square agent randomly transported location maze 
difference grid worlds defining perception unique row column numbers agent position perception cause perceptual aliasing defines perception bit vector length bits specify barrier agent immediate north east south west 
perception rich perceptual aliasing possibilities bit realistic navigating robot build 
experiments added noise grid world 
probability agent perceives perception vector current world position specified 
probability agent chosen actions randomly changed 
maze solved udm appears 
noise udm consistently learned optimal policy maze trials steps 
agent learning rate fi temporal discount factor fl confidence upper lower bounds gamma ff random action probability exploration rate 
sequence hmm created appear 
notice percept represented states world states produce percept 
sides action required udm keeps world state aliased internal state 
noise udm learned optimal policy agent required trials steps 
utile distinction memory technique provides refutation utile distinction conjecture chrisman stated impossible introduce memory distinctions impact utility 
udm acts starting point design improved learners memory create selective task specific representations environment 
mechanisms creating memory hidden markov models 
lin mitchell successful results agents fixed size window past percepts actions agents recurrent neural networks context units lin mitchell methods split add nodes network learning memory capacity known fixed learning begins 
say tan deals incomplete perception robot remembers results perceptual actions determine internal state 
mechanism memory spans overt action algorithm works deterministic worlds 
point prevents tan agent solving tasks local perception grid world perceptual information limited immediately adjacent squares 
point memory problem simpler agent longer determine remember forget remembers results perceptual actions throws memory away overt action 
udm limitations problems 
computational requirements state splitting test significant terms storage calculation 
method uses baum welch procedure requires kn time storage 
long udm increase requirements increased constants insignificant 
hopefully udm minimal task specific state splitting allow small state spaces partially 
udm splits states order increase memory distinctions currently method splitting states increase perceptual distinctions predict reward 
reason udm begins state percept strategy obviously large perception spaces 
method making perceptual distinctions necessary plausible chapman kaelbling algorithm chapman kaelbling technique confidence intervals unused perception bits state conjunction udm 
udm build memory chains arbitrary length require statistically significant benefit detectable split individually sequence 
udm solved mazes multi step paths reward predicting percepts reward able solve problems conjunction percepts sequence predicted reward percepts independent reward 
assumption detectable relevance pieces information isolation chapman kaelbling maes brooks additionally key factor genetic algorithms 
primary goal working memory learning agents memory disambiguate ubiquitous perceptual aliasing occurs active perception mccallum fact memory active perception potential solve udm lack splitting perceptual distinctions quite nicely 
imagine agent chooses overt perceptual actions return bit bit agent entire immediate perception space bit perception extreme case 
think different perceptual actions returning different bit total perception vector available current world state 
agent generalize perception space executing actions necessary get bits currently relevant 
scheme better generalizing don cares mask perception vector agent policy effectively act decision tree specifies bits needed depending result bit gathered early process agent decide needs gather bits executes overt action 
scheme allow agent execute open loop sequences actions 
route require addressing assumption detectable relevance individual perceptual bits provide useful information conjunction violate assumption 
example student driver learning pass pull passing lane car front close mirror clear blind spot clear 
verifying conjunction requires remembering results eye movements 
true conjunction highly correlated reward bits individually 
acknowledgments benefited discussions colleagues including dana ballard mary jeff schneider jonas karlsson polly 
am grateful dana ballard jeff schneider virginia de sa making helpful comments earlier draft 
material supported nsf research 
iri nih phs research human science frontiers program 
chapman kaelbling david chapman leslie pack kaelbling 
learning delayed reinforcement complex domain 
proceedings ijcai 
chrisman chrisman rich caruana wayne 
intelligent agent design issues internal agent state incomplete perception 
working notes aaai fall symposium sensory aspects robotic intelligence 
chrisman chrisman 
reinforcement learning perceptual aliasing perceptual distinctions approach 
aaai 
garey johnson michael garey david johnson 
computers intractability guide theory np completeness 
freeman 
kaelbling leslie pack kaelbling 
learning embedded systems 
phd thesis stanford university 
lin mitchell long ji lin tom mitchell 
reinforcement learning hidden states 
proceedings second international conference simulation adaptive behavior animals animats 
maes brooks pattie maes rodney brooks 
learning coordinate behaviors 
proceedings aaai pages 
mccallum andrew mccallum 
results utile distinction memory reinforcement learning 
technical report university rochester computer science dept 
mccallum andrew mccallum 
learning incomplete selective perception 
technical report university rochester computer science dept april 
phd thesis proposal 
rabiner lawrence rabiner 
tutorial hidden markov models selected applications speech recognition 
proceedings ieee february 
sutton richard sutton 
integrating architectures learning planning reacting approximating dynamic programming 
proceedings seventh international conference machine learning austin texas 
morgan kaufmann 
tan ming tan 
cost sensitive reinforcement learning adaptive classification control 
aaai 
thrun sebastian thrun 
efficient exploration reinforcement learning 
technical report cmu cs cmu comp 
sci 
dept january 
watkins watkins 
learning delayed rewards 
phd thesis cambridge university psychology dept 
whitehead ballard steven whitehead dana ballard 
learning perceive act 
technical report university rochester computer science dept june 
whitehead steven whitehead 
reinforcement learning adaptive control perception action 
phd thesis department computer science university rochester 
trial trial trial trial sequence hidden markov models created udm learns task shown 
hmm states fully connected diagrams show transitions probability greater 
shown different actions cause transitions 
trial agent learned transition probabilities gathered statistics split states 
trial agent discovered discounted reward received leaving state significantly different arrives state arrives state 
udm duplicated state creating removed state incoming transition removed state incoming transition 
trial agent non overlapping confidence intervals transitions arriving state significantly different arriving states 
udm temporarily splits joined 
trial splits occur trial 
average reward leaving center state greater leaving side state center state agent locations closer goal 
udm splits state recognizing arriving different arriving 
state split confidence intervals leaving going east leaving going west don overlap transitions coming states 
agent arrive state state arrive state state 
changes hmm trial 
