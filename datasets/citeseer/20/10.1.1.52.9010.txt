appears ecml research note pruning decision trees misclassification costs jeffrey bradford clayton kunz ron kohavi cliff brunk carla brodley school electrical engineering data mining visualization purdue university silicon graphics shoreline blvd west lafayette mountain view ca ecn purdue edu ronnyk engr sgi com 
describe experimental study pruning methods decision tree classifiers goal minimizing loss error 
addition common methods error minimization cart cost complexity pruning error pruning study extension cost complexity pruning loss pruning variant laplace correction 
perform empirical comparison methods evaluate respect loss 
applying laplace correction estimate probability distributions leaves beneficial pruning methods 
error minimization somewhat surprisingly performing pruning led results par methods terms evaluation criteria 
main advantage pruning reduction decision tree size factor 
method dominated datasets domain different pruning mechanisms better different loss matrices 
pruning decision trees decision trees widely symbolic modeling technique classification tasks machine learning 
common approach constructing decision tree classifiers grow full tree prune back 
pruning desirable tree grown may overfit data inferring structure justified training set 
specifically conflicting instances training set error fully built tree zero true error larger 
combat overfitting problem tree pruned back goal identifying tree lowest error rate previously unobserved instances breaking ties favor smaller trees breiman friedman olshen stone quinlan 
pruning methods introduced literature including cost complexity pruning reduced error pruning pessimistic pruning error pruning penalty pruning mdl pruning 
historically pruning algorithms developed minimize expected error rate decision tree assuming classification errors unit cost 
objective different mentioned studies 
pruning minimize error aim study pruning algorithms goal minimizing loss 
practical applications loss matrix associated classification errors turney pruning performed respect loss matrix 
pruning loss minimization lead different pruning behavior pruning error minimization 
investigate behavior pruning algorithms 
addition common methods error minimization cost complexity pruning breiman error pruning quinlan study extension cost complexity pruning loss pruning variant laplace correction cestnik 
perform empirical comparison methods evaluate respect loss different matrices 
domain different pruning mechanisms better different loss matrices 
addition adjusting probability distributions leaves laplace correction beneficial methods 
pruning algorithms evaluation criteria pruning algorithms perform post order traversal tree replacing subtree single leaf node estimated error leaf replacing subtree lower subtree 
crux problem find honest estimate error breiman defined overly optimistic tree built minimize errors place 
resubstitution error error rate training set provide suitable estimate leaf node replacing subtree fewer errors training set subtree 
commonly pruning algorithms error minimization error pruning quinlan cart cost complexity pruning breiman 
attempted extend error pruning loss pruning 
cases extensions obvious error pruning confidence intervals extend easily 
naive idea computing confidence interval probability computing losses upper bound interval class yields distribution add 
experimental results variants normalizing probabilities perform 
decided laplace pruning method 
laplace pruning method introduce similar motivation error pruning 
laplace correction method biases probability uniform distribution 
specifically node instances class class problem probability assigned class cestnik 
laplace correction distribution leaves uniform extreme 
node compute expected loss loss matrix 
expected loss subtree sum expected loss leaves 
cost complexity pruning ccp algorithm cart penalizes estimated error subtree size 
specifically error estimate assigned subtree resubstitution error plus factor ff times subtree size 
efficient search algorithm compute distinct ff values change tree size parameter chosen minimize error holdout sample cross validation 
optimal value ff entire training set grow tree pruned optimal value 
experiments holdout method holding back training set estimate best ff parameter 
cost complexity pruning extends naturally loss matrices 
estimating error subtree estimate loss cost resubstitution loss penalizing size tree times ff factor error ccp 
comparison pruning algorithms goal designing experiments understand pruning methods decision tree classifier evaluated loss loss matrix 
basic decision tree growing algorithm implemented mlc kohavi sommerfield dougherty called mc mlc 
top decision tree induction algorithm similar 
algorithm grows decision tree standard methodology choosing best attribute gain ratio evaluation criterion stopping node fewer instances 
trees pruned pruning algorithms eb fr error pruning probabilities estimated frequency counts 
eb lc error pruning probabilities estimated laplace correction 
np lc pruning probabilities estimated laplace correction 
lp laplace pruning probabilities estimated laplace correction 
ccp lc cost complexity pruning loss probabilities estimated laplace correction 
leaves trees labeled class minimizes expected loss probability estimates leaf 
initial experiments laplace correction outperformed frequency counts variants 
excluding basic method error pruning pruning methods run laplace correction 
datasets chosen uci repository merz murphy adult salary classification census bureau data breast cancer diag chess crx credit german credit pima diabetes road dirt satellite images shuttle vehicle 
choosing datasets decided desiderata 
datasets class evaluation easier 
desideratum hard satisfy resorted converting multi class problems class problems choosing prevalent class goal class 

datasets unknowns 
avoid factor evaluation removed instances unknown values files 

standard error estimated loss small 
important loss matrices standard deviations estimates large 
decided require instances train data leaving remaining instances testing 
wanted test hypotheses 
laplace correction estimating probabilities leaves leads lower loss frequency counts 

considering loss matrix pruning leads lower loss pruning errors 
datasets trained data tested data repeating process times 
compared performance pruning algorithms different loss matrices respectively set loss misclassifying frequent classes 
done simulate real world scenarios frequent class important class 
experiments done losses reversed similar shown 
results displayed graphs showing average loss files bars scale left average relative loss symbols scale right 
relative losses computed ratio loss pruning method eb fr baseline method 
ratios averaged datasets create summary graphs 
cases losses small ratio better indicator performance 
average losses average relative losses loss matrices shown 
observations 
error pruning frequency counts performs worst 

laplace pruning lp performs best loss matrix comparable best loss matrix 

pruning np lc performs surprisingly loss matrices 

cost complexity pruning ccp lc slightly inferior pruning better error pruning eb loss matrix 

tree sizes radically different 
average tree sizes loss matrix ccp eb lp np 
cost complexity pruning far smallest confirming observation oates jensen error minimization 
average loss loss ratio absolute relative np lc lp ccp lc eb lc eb fr average loss loss ratio absolute relative np lc lp ccp lc eb lc eb fr fig 

average absolute relative losses different algorithms loss matrix left matrix right 
hypothesis laplace correction estimating probabilities leaves outperforms frequency counts confirmed 
confirmed np ccp pruning methods run frequency counts results shown 
interestingly pruning performed suggesting loss matrices tree size important pruning need done laplace correction 
result differs error minimization pruning consistently shown help 
pruning loss matrices performed better pruning error frequency counts methods 
result frequency counts observed previously reduced error cost pruning draper brodley utgoff 
laplace correction pruning loss matrices performed better error pruning eb lc ccp lc lp significant difference loss matrix 
pruning method applying laplace correction improved performance average 
cases laplace correction lead higher distribution mse mean squared error frequency counts 
distribution mse similar laplace correction algorithms 
main difference pruning algorithms tree size 
average tree sizes ccp eb lp np 
steps inducing decision tree growing pruning concentrated stage 
view step study studying different growing techniques done pazzani merz murphy ali hume brunk 
extended cost complexity pruning loss introduced new method loss matrices laplace pruning 
laplace pruning best pruning method loss matrix tied best pruning pruning laplace correction loss matrix 
study revealed laplace correction leaves extremely beneficial aids pruning methods 
datasets tested pruning help reducing loss lead smaller trees 
cost complexity pruning especially effective reducing tree size significantly increasing loss 
single pruning algorithm dominated datasets terms loss interestingly fixed domain different pruning algorithms better different loss matrices 
long version bradford kunz kohavi brunk brodley showed roc curves different algorithms including pruning method 
differences major 
fact little difference loss algorithms loss matrix tree pruning stage conclude usually suffice induce single probability tree different loss matrices 
bradford kunz kohavi brunk brodley 
pruning decision trees misclassification costs long 
robotics stanford edu ronnyk prune long ps gz 
breiman friedman olshen stone 
classification regression trees wadsworth international group 
cestnik 
estimating probabilities crucial task machine learning aiello ed proceedings ninth european conference artificial intelligence pp 

draper brodley utgoff 
goal directed classification linear machine decision trees ieee transactions pattern analysis machine intelligence 

estimation probabilities essay modern bayesian methods press 
kohavi sommerfield dougherty 
data mining mlc machine learning library tools artificial intelligence ieee computer society press pp 

www sgi com technology mlc 
merz murphy 
uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
oates jensen 
effects training set size decision tree complexity fisher ed machine learning proceedings fourteenth international conference morgan kaufmann pp 

pazzani merz murphy ali hume brunk 
reducing misclassification costs machine learning proceedings eleventh international conference morgan kaufmann 
quinlan 
programs machine learning morgan kaufmann san mateo california 
turney 
cost sensitive learning 
ai iit nrc ca bibliographies cost sensitive html 
