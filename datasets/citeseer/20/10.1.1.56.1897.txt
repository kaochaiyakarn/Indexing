sequential monte carlo methods dynamic systems jun liu rong chen general framework monte carlo methods dynamic systems provided wide applications indicated 
framework currently available techniques studied generalized accommodate complex features 
methods partial combinations ingredients importance sampling resampling rejection sampling markov chain iterations 
deliver guideline circumstance method suitable 
analysis differences connections consolidate methods generic algorithm combining desirable features 
addition propose general rao blackwellization improve performances 
examples econometrics engineering demonstrate importance rao blackwellization compare different monte carlo procedures 
keywords blind deconvolution bootstrap filter gibbs sampling hidden markov model kalman filter markov chain monte carlo particle filter sequential imputation state space model target tracking 
jun liu assistant professor statistics department statistics stanford university stanford ca 
rong chen associate professor statistics department statistics texas university college station tx 
liu research partly supported nsf dms dms fellowship stanford university 
chen research partly supported nsf dms 
grateful professor wing hung wong stimulating discussions helped formulate general sis framework professors gilks neil shephard letting read enlightening manuscript publication professors john rice mike west pointing related associate editor referees constructive suggestions 
dynamic modeling important statistical analysis tool attracted attention researchers different fields 
widely dynamic model linear state space model long active subject studying time series data control systems harvey west harrison 
despite computational complexities nonlinear non gaussian state space models important various applications 
partial list example 
models dynamic nature various occasions updating learning graphical models probabilistic expert systems spiegelhalter lauritzen kong liu wong simulating protein structures leach genetics cox kong combinatorial optimizations wong liang 
example expert system updating 

article study monte carlo computation methods real time analysis dynamic systems 
system abstractly defined follows definition sequence evolving probability distributions indexed discrete time called probabilistic dynamic system 
state variable evolve ways increasing dimension component multidimensional component ii discharging fewer component iii change article devoted situation situations ii iii handled similarly 
article refers target distribution dynamic system generic symbol probability distributions 
applications difference caused incorporation new information analysis 
interests systems usually prediction extended new component best prediction new information arrives updating smoothing revision previous state new information new estimation say light new information 
examples typical dynamical systems referred repeatedly article 
example bayesian missing data problem 
suppose iid model partially observed 
observed part missing part 

dynamic system case 
interest usually posterior distribution dx delta delta delta dx explicitly integrated case multivariate normal data missing components kong approach draw rao blackwellization approximate 
example state space model 
model consists parts observation equation formulated delta oe state equation represented markov process delta gamma 
observations referred unobserved states 
interest time posterior distribution oe 
target distribution time oe oe oe oe gamma initial distribution assumed known 
parameters oe engineering problems represents 
practice unobserved true signals signal processing liu chen actual words speech recognition rabiner target characteristics location velocity multitarget tracking problem gordon image characteristics computer vision isard blake gene indicator dna sequence analysis churchill underlying volatility economical time series pitt shephard 
applications dynamic state space model dna protein sequence analysis referred hidden markov models krogh liu neuwald lawrence 
special cases closed form analysis dynamical systems usually formidable 
surge interest designing monte carlo methods analysis models 
fact example monte carlo iterative methods 
implement monte carlo dynamic system need time random samples drawn directly drawn distribution say weighted properly importance sampling 
static methods popular mcmc schemes carlin carter kohn achieve treating separately repeating kind iterative processes 
words results random draws obtained time discarded system evolves system slowly varying distance small random samples obtained time re help construct random samples time improve efficiency 
imagine sample fx mg drawn system evolves desirable keep attach drawn appropriate distribution delta 
denote sample space foregoing idea equivalent drawing sample product space evolved sample needs reweighted resampled accommodate basic principle available sequential mc methods 
gordon 
hendry richard kitagawa kong 
liu chen maceachern clyde liu pitt shephard west elaborate ideas article describe general framework sequential monte carlo methods dynamic systems 
framework extend unify previously restrictive methods study various reweighting resampling techniques proposed discuss connections comparisons approaches 
main message want communicate article sequential importance sampling sis setting provides framework understanding existing methods improving rao blackwellization collapsing 
section describes general idea sequential importance sampling sis method key implementation issues choice sampling distribution resampling monte carlo inference 
section discusses local monte carlo methods needed sis encounters certain difficulties 
section proposes methods resampling provides generic algorithm combines sis resampling 
section brings rao blackwellization improving estimation 
section gives examples demonstrate rao blackwellization compare different procedures 
section concludes brief summary 
sequential updating dynamic system successful methods analyzing complicated probabilistic system nonlinear state space model gibbs sampler carlin carter kohn gelfand smith tanner wong 
gibbs sampler attractive interest real time prediction updating dynamic system 
situation gibbs sampler ineffective states resulting samples sticky rendering sampler difficult move maceachern 
case appears intelligently choosing dynamic system sequential updating efficient wong liang 
describe sequential updating strategies discuss key implementation issues 
sequential importance sampling sis useful way represent complicated high dimensional distribution multiple monte carlo samples drawn 
multiple imputation rubin successful example practice survey data 
article advocate similar methodology rubin analyzing dynamic systems 
definition random variable drawn distribution said properly weighted weighting function respect distribution integrable function fh fh set random draws weights said properly weighted respect lim integrable function practical sense think approximated discrete distribution supported probabilities proportional weights fx mg denote set random draws properly weighted set weights fw mg respect sample space trial distribution 
sis procedure consists recursive applications sis steps sis step draw jx attach form 
compute called incremental weight 
easy show properly weighted sample sis applied recursively accommodate changing dynamical system 
sis method useful non bayesian computation evaluating likelihood functions 
applications direction hendry richard 

briefly suppose interested evaluating likelihood function missing data problem example 
fixed value apply sis procedure impute sequentially gamma gamma kong 
show unbiased estimate 
section show rao blackwellization casella robert liu wong kong applied obtain better estimate 
choice sampling distribution sis choice sampling distribution directly related efficiency proposed sis method 
bayesian missing data problems example kong 
suggest incremental weight 
note exact value easily known computed normalizing constant sufficient estimation formula 
choice liu chen 
state space model example known oe similar trial distribution oe oe dx general dynamic system setting suggest chosen incremental weight note depend value feature important issues discussed 
reason drawing desirable arbitrary function clear rewriting incremental weight intuitively second ratio needed correct discrepancy different 
choices possible 
instance extended may jx example corresponds jx corresponding incremental weight example choice corresponds 
gordon 
kitagawa 
note trial distribution generates state equation 
compared distribution usually easier tends result greater monte carlo variation 
state space model case obvious choice desirable incorporates information 
advantage estimation discussed section 
applications may easy 
section provides methods coping difficulty 
resampling sis sisr suppose fx mg properly weighted fw mg respect call stream 
carrying weight system evolves legitimate beneficial liu chen insert resampling step described follows sis recursions procedure referred sis resampling sisr 
resampling step sample new set streams denoted weights ii assign equal weights streams immediately clear needs resampling certain stage detailed theoretical discussion liu chen mention heuristics issue 
firstly weights constant near constant case occurs draw directly resampling reduces number distinctive streams introduces extra monte carlo variation 
suggests perform resampling coefficient variation cv small 
argued kong effective sample size inversely proportional cv secondly kong 
show system evolves cv increases stochastically 
weights get skewed time carrying streams small weights apparently waste 
resampling provide chances important streams amplify sampler produce better result states system evolves improve inferences current state examples section illustrate heuristics 
resampling schedule resample deterministic dynamic sampling scheme simple random sampling weights residual sampling local monte carlo resampling section 
methods gordon 
kitagawa 
pitt shephard seen sis special choices resampling stage 
inference monte carlo samples dynamic systems interest obtain line inference state variables estimating time straightforward available sample fx properly weighted issues concerning statistical efficiency estimates worth mentioning 
casella provides general treatment issue 
ffl estimation done resampling step resampling introduces extra random variation current sample 
ffl rao blackwellization improve accuracy estimation 
example weight depend case optimal current state estimated drawn provided calculated easily 
mixture normal state space models example section examples achievable 
ffl delayed estimation estimate gammak time usually accurate concurrent estimation estimate gammak gammak time gamma estimation information 
precaution needs taken frequent resampling resampling reduces distinct past samples 
related methods state space model described example special markovian feature general dynamic models possess 
oe example satisfies previous gamma forgotten 
kalman filter posterior distribution obtained recursively principle 
main difficulty analytical formulas recursive updating exist certain exponential family models west harrison finite discrete state space model rabiner 
popularity simplicity state space model sequential monte carlo methods proposed deal nonlinear non gaussian cases 
particular hendry richard note potential sis models 
west suggests mixture distribution approximate time proceed adaptive importance sampling strategy produce mixture approximation time 
difficulties approach finding mixture approximations time consuming difficult implement dimensionality high 
gordon 
kitagawa propose importance resampling obtain discrete approximation jy set samples drawn jy 
call procedure bootstrap filter particle filter 
method successfully applied multiple target tracking gordon time series analysis kitagawa 
method essentially sis chosen resampling estimations performed resampling efficient 
pitt shephard proposed improved algorithms state space model 
discuss approaches detail section 
local monte carlo methods sis discussed section favorable choice recursive sampling distribution 
drawing may directly achievable incremental weight may easy compute 
premise collection methods developed overcome difficulty state space model 
see example 
pitt shephard 
propose extend methods general sis setting simultaneously estimating new weight drawing refer methods local monte carlo methods sis 
basic idea usual fx mg fw mg 
central idea section regard represented monte carlo sample weights stage treated random variable discrete priori distribution 
simplify notations introduce random variable takes values set mg indicate streams pitt shephard formulation call auxiliary variable 
joint distribution marginal distribution approximation true marginal distribution provided monte carlo sample size large distribution skewed 
marginal distribution dx exactly new weight time 
method draw sample sis step achieved estimate frequency fj jg sample 
form value paired sample 
methods generating samples described subsections remarks follows 
long estimates weights unbiased new sample properly weighted respect accurate estimation weights necessary 
replaced random draw 
interest estimation incremental weight set calculations 
local mc methods provide samples distribution achieve resampling effect automatically 
see details section 
methods described section necessary direct sampling optimal achievable 
rejection methods suppose draw trial distribution equal 
rejection methods sample joint distribution marginal covering constant sup rejection method ffl draw probability proportional ffl draw ffl accept probability rejection method steps identical method 
step ffl accept probability sample accepted methods follows 
method need redraw probability methods identical state space model case essential part 
generally method rao blackwellization method casella robert efficient 
importance resampling importance resampling method generate approximate samples 
ffl draw probability proportional ffl draw ffl assign sample weight obtained sample properly weighted respect 
point choices resampling achieve approximately implemented gordon 
kitagawa estimate directly weighted sample proceed newly sampled new weights proposed pitt shephard 
addition pitt shephard suggest adjustment multiplier improve efficiency 
briefly draw probability proportional adjust weight accordingly 
conceivable carefully choosing function achieve efficiency 
idea applied rejection sampling mcmc approach 
hastings independence chain approach alternatively hastings independence chain approach hastings suggested 
state space model 
prescribe generalization method dynamic systems 
detailed description general independence chain method appendix 
suppose draw trial distribution 
starting arbitrary iterate steps ffl draw probability proportional ffl draw reversible mcmc step delta invariant distribution see proof correctness appendix 
ffl set equal probability equal probability gamma min defined 
resulting equilibrium distribution exactly 
theoretical properties hastings chain studied liu shows method comparable rejection method terms statistical efficiency 
second rejection method described previous subsection take mcmc twist 
detail omitted 
advantages rejection methods iterations needed resulting sample exact disadvantage needs computed resulting scheme inefficient 
liu provides detailed comparisons methods 
interesting variation combine rejection importance samplings suggested liu chen wong 
difference large methods ideal 
alleviate problem state space model propose smoothing techniques pitt shephard suggest mode approximation find adjustment multiplier 
illustration state space model suppose interested estimating state space signal line example parameters oe 
simplicity suppress oe relevant formulas 
dynamic system state space model gamma sampling difficult usually draw state equation easily 
rejection methods identical situation 
sup jx 
procedure ffl draw probability proportional draw 
ffl accept probability samples drawn scheme follows distribution similarly jx importance resampling procedure draw probability proportional ii draw iii assign weight sample 
procedure gordon 
kitagawa exactly additional step resampling obtained sample assigned weight 
addition jx pitt shephard incorporate adjustment multiplier mode mean value resulting weight obtained sample independence chain approach rejection probability computed min rest carried routinely 
resampling generic algorithm early monte carlo methods state space model resampling played major role evolving system time gordon kitagawa 
section describe resampling methods discuss possible resampling schedules prescribe generic monte carlo algorithm dynamic systems 
resampling methods simple random sampling 
procedure samples replacement probability proportional weights liu chen approach modify skewed importance weights resulting sis 
general resampling step inserted sis steps 
weight depend sampling distribution resampling step inserted inside sis step 
specifically optimal sisr consists sis step resampling step sis step 
generates distinct samples performing resampling sis steps 
residual sampling 
scheme replace simple random sampling 
ffl retain mw copies renormalized weight gamma gamma delta delta delta gamma km ffl obtain iid draws probabilities proportional mw gamma ffl reset weights 
easily shown residual sampling dominates srs having smaller monte carlo variance favorable computation time disadvantages aspects 
comparison procedures section 
local monte carlo resampling 
local monte carlo methods described section provide samples distribution appears methods achieve resampling effect automatically 
precisely set draws obtained rejection method hastings method burning section 
set streams desirable resample 
weights associated new streams equal 
note necessarily equal procedure avoids weight estimation 
resampling schedule shown earlier arguments examples resampling stage necessary efficient 
desirable prescribe schedule resampling step take place 
schedules available deterministic versus dynamic 
deterministic schedule conducts resampling time depends difficulty particular problem may require experimentation 
dynamic schedule gives sequence thresholds monitors coefficient variation weights cv cv occurs invokes resampling 
typical sequence bt ff generic monte carlo algorithm recommend algorithm monte carlo computation dynamic systems 
main algorithm 
check weight distribution perform choices time dynamic weight estimated weight distribution skewed cv go step 
go step 
deterministic kt integer go step 
go step 
set 
invoke sis step section 
may need local mc procedure section accomplish recursive sampling weighting 
goto step 
set 
invoke sisr step section 
residual sampling possible 
avoid weight calculation local monte carlo resampling methods 
go step 
noticeable difference local monte carlo procedures pitt shephard decouple local mc outputs parts estimating new weights obtaining draws advantages doing decoupling obtaining explicit weighting tell different sis works ii provides means improve efficiency residual sampling delayed resampling 
local mc procedures merely achieve means leads considered possible 
rao blackwellization sisr procedures discrete representation sample weight degenerates rapidly number increases consequence estimating quantity interest inaccurate 
take example instance 
sisr proceeds number distinctive values decreases monotonically 
rapidly leads degenerate posterior distribution 
alleviate problem apply variant rao blackwellization casella robert kong liu 
suppose bayesian missing data setting example time observed information multiple draws properly weighted number distinctive values small fragment stream drawing jk complete data posterior distribution 
posterior distribution continuous km distinctive values rao blackwellization 
weight associated 
consequence fact steps mcmc transition respect target distribution invariant sample properly weighted respect see maceachern 

retain constant total number streams set resample streams km streams corresponding weight 
rao blackwellization described results sampling distribution closer target distribution uses information 
time want rao blackwellized estimate weighted histogram sampled compute likelihood function jy draw uniformly parameter space bounded need combine flat prior data apply sis draw multiple copies weight rao blackwellized estimate likelihood function complete data posterior distribution flat prior 

notice form conditional independence missing data problem parameter involved conditional missing data independent may early observation early imputations 
instance example disengaged 
similarly example gamma gamma disengaged 
advantage doing obvious saves memory may speed computation 
implemented rao blackwellization longer directly applicable 
remedy represent information contained disengaged components mixture distribution rao blackwellization proceed combination resampling 
numerical experiment method investigation 
examples econometric disequilibrium model initially proposed fair class models attracted attention econometrics researchers past decades 
provides theoretical foundation philosophical arguments generally called theory named economist keynes attacked dominant paradigm economics mainstream approach economics equilibrium methods 
see reviews discussions 
look special dynamic disequilibrium model hendry richards 
components relevant lagged variables prices environmental exogenous variables excluded sake simplicity 
illustrate improvement estimation rao blackwellization 
bivariate normal random variables ff ar gamma identity matrix 
observed datum model simplicity presentation initial states taken assumed known 
interest likelihood function posterior distribution ff ff 
ffi ff ff 
write ffi distribution involved sequential sampling involved weight updating detailed computations appendix 
fixed hendry richard sis evaluate likelihood equation kong 

putting flat prior treat likelihood computation bayesian computation sis method simulate weighted samples jointly 
rao blackwellization applied improve efficiency 
posterior distribution ff observations uniform prior 
line result rao blackwellization dots result standard sis 
simulated data observations model ff ff initial value assuming know ff ff ff sis method obtain likelihood function ff shown 
took seconds silicon graphics workstation microprocessor cv sis 
smooth curve result rao blackwellization 
blind deconvolution moving average system oe gammai seen digital communication 
input signal takes value known set discrete states oe 
observing blurred signals interest reconstruct estimate system coefficients oe liu chen 
took simulated example chen li system equation gamma gamma gamma input signals iid uniform 
signal noise ratio controlled db gives standard deviation noise 
oe case easily integrated normal prior sampling weighting calculations liu chen 
direct sis local mc procedure applies 
total signal sequences simulated sequential observations system 
interest testing simple sis different resampling schedules resampling methods simple random sampling versus residual sampling 
streams carried sis procedure 
estimated input signal map weight time 
table shows number misclassification signals simulations sequential signals 
resampling frequency means procedures applied implies resampling 
dynamic scheduling resampling procedure applied effective sample size defined cv 
example dynamic schedule leads times processing signals 
table shows resampling small rare big tends produce large number misclassifications 
resampling marked frequency monte carlo method right track resulting disastrous estimations 
reasonable range residual sampling method slightly better simple random sampling 
deterministic resampling schedule dynamic schedule error table numbers misclassified signals column total simulations sequential signals demonstrated 
columns column results different combinations sis strategies 
symbols row represent simple random sampling residual sampling respectively 
target tracking clutter tracking multiple targets clutter interest engineers computer scientists 
problem received attention solutions proposed method gordon 
closely related method described article 
mentioned earlier algorithm employ sampling distribution 
example show sampling distribution produce better tracking results 
tracking problem formulated state space model state variable location target straight line target velocity 
location observations 
evolve way independent 
assume probability location observation rate false signal clutter ff delta delta width detection region 
actual observation vector length true observation 
distribution bernoulli poisson delta 
false signals uniformly distributed detection region 
case sampling distribution easily shown mixture normal distributions means variances functions details appendix 
resampling conducted drawn concurrent estimation done rao blackwellization 
trick play example integrate state variable monte carlo impute indicator variable tells component true signal 
true signal identified trivial estimate true location target 
collapsing procedure produces better result 
shows plots tracking errors estimated location gamma true location simulated runs 
streams resampling done step 
top resulted optimal sampling distribution middle collapsing procedure 
bottom shows result optimal sampling distribution 
top runs absolute value tracking errors exceeding middle bottom 
similarly top runs exceeding limit middle bottom 
parameter combination slightly different smaller clutter density larger state equation variance 
configuration results similar differences different procedures smaller 
summary propose general framework line monte carlo computations time error time error time error tracking results different sequential monte carlo methods 
resampled step 
axis distance estimated true positions target 
top results prescribed middle results collapsing procedure bottom results 
dynamic systems 
clear available sequential monte carlo procedures unified framework 
general setting provides common ground understanding improving various similar methods developed specific models 
provides general guidance procedures practice different tricks developed specific problems combined achieve maximum efficiency 
particular discussed key issues implementing sequential monte carlo methods choices recursive sampling distribution advantages disadvantages resampling scheduling efficient monte carlo samples rao blackwellization 
obvious application sequential mc state space models problems formulated dynamic system solved techniques described article 
example sis procedure built mcmc scheme produce efficient transition proposal chain 
hastings rejection procedure described section combination 
type monte carlo methods called configuration biased monte carlo tested effective simulating biopolymers leach 
see 
kong 
wong liang examples 
hope results reported stimulate interest effort researchers type problems 
appendix invariant distributions hastings independence chain scheme discussed hastings section way importance sampling 
tierney generalizes discussion heading independence chains called independence sampling liu 
general scheme stated follows 
suppose known normalizing constant able draw independent samples 
markov chain fx constructed transition function minf gamma minf called importance ratio importance weight 
intuitively transition accomplished generating independent sample delta thinning comparison corresponding importance ratios 
shown invariant distribution constructed markov chain 
note scheme special example serious algorithms commonly dependent local moves independent global jumps 
eigen analysis chain provided liu 
suppose directly sample reversible mcmc procedure single step mcmc scheme satisfies condition transition function invariant distribution 

conduct metropolis step proposal chain target distribution rejection probability computed min ae oe min ae oe min ae oe procedure described section valid longer called independence chain approach 
computations involved example section computing weights compute gamma gamma oe gammaff gamma gamma phi gammaff gamma oe gammaff gamma gamma phi gammaff gamma missing data need ffi gamma gamma oe gamma ff gamma gamma phi gamma ff gamma gamma gamma ffi gamma gamma oe gamma ff gamma gamma phi gamma ff gamma ffi gamma gamma oe gamma ff gamma gamma phi gamma ff gamma suppose prior distribution ff ff complete observations posterior exp gamma gamma ff gamma gamma ff gamma loss generality take uniform posterior distribution simplified exp gamma ff gamma gamma ff gamma gamma gamma gamma gamma gamma gamma sample truncated normal distribution oe standard normal density strategy 
simply conduct simple normal random number generation rejection sample satisfying especially big exponential random variable rejection method 
suppose exponential distribution gamma envelop function need find minimum constant oe gamma phi gamma gives best solution expf gamma gamma phi acceptance rate exponential distribution achieve minimum rejection rate find best choice choice implement rejection method von neumann 
rejection rate scheme decreases increases rate small moderate large rejection rates 
computations target tracking section observed signals time 
gamma delta gamma delta delta delta gamma oe gamma delta delta gamma gamma 
oe gamma gamma delta gamma oe refers normal density variance furthermore oe qr exp gamma gamma gamma gamma gamma oe exp gamma gamma oe oe oe oe rq exp gamma gamma oe mixture normal distribution 

stochastic simulation bayesian approach multitarget tracking iee proceedings radar sonar navigation 
best gilks 
dynamic conditional independence models markov chain monte carlo methods amer 
statist 
assoc appear 
carlin polson stoffer 
monte carlo approach nonnormal nonlinear state space modeling journal american statistical association 
carter kohn 
gibbs sampling state space models biometrika 
casella 
statistical inference monte carlo algorithms test 
casella robert 
rao blackwellization sampling schemes biometrika 
chen li 
blind restoration linearly degraded discrete signals gibbs sampler ieee transactions signal processing 
churchill 
stochastic models heterogeneous dna sequences bulletin mathematical biology 
fair 
methods estimation markets disequilibrium econometrica 
gelfand smith 
sampling approaches calculating marginal densities journal american statistical association 
hastings 
monte carlo sampling methods markov chains applications biometrika 
gordon salmon smith 
novel approach nonlinear non gaussian bayesian state estimation iee proceedings radar signal processing 
gordon salmon ewing 
bayesian state estimation tracking guidance bootstrap filter aiaa journal guidance control dynamics 
harvey 
forecasting structure time series models kalman filter cambridge 
uk cambridge university press 
hendry richard likelihood evaluation dynamic latent variables models 
computational economics econometrics ch 
pau 
eds 
dordrecht kluwer 
monte carlo approximations general state space models 
research report eth zurich 
isard blake 
contour tracking stochastic propagation conditional density computer vision eccv buxton cipolla eds springer new york 
cox kong 
sequential imputation linkage analysis proceedings national academy science usa 
kitagawa 
monte carlo filter smoother non gaussian nonlinear state space models journal computational graphical statistics 
kong liu wong 
sequential imputations bayesian missing data problems amer 
statist 
assoc 
krogh brown mian haussler protein modeling hidden markov models journal molecular biology 
leach 
molecular modelling principles applications 
addison wesley longman singapore 
liu 
independent sampling comparisons rejection sampling importance sampling statistics computing 
liu chen 
blind deconvolution sequential imputations journal american statistical association 
liu chen wong 
rejection control sequential importance sampling technical report department statistics stanford university 
liu neuwald lawrence 
markov structures biological sequence alignments technical report stanford university 
liu wong kong 
covariance structure gibbs sampler applications comparisons estimators augmentation schemes biometrika 
maceachern clyde liu 
sequential importance sampling nonparametric bayes models generation canadian journal statistics press 
von neumann 
various techniques connection random digits national bureau standards applied mathematics series 
pitt shephard 
filtering simulation auxiliary particle filters preprint www ox ac uk users shephard 

econometric disequilibrium models discussions econometric review 
econometrics disequilibrium new york basil blackwell 
rabiner 
tutorial hidden markov models selected applications speech recognition proceedings ieee 
rubin 
multiple imputation surveys new york wiley 
spiegelhalter lauritzen 
sequential updating conditional probabilities directed graphical structures network 
tanner wong 
calculation posterior distributions data augmentation discussion journal american statistical association 
tierney 
markov chains exploring posterior distributions discussion annals statistics 

build energy minimization procedures compute low energy structures backbone biopolymers 
west mixture models monte carlo bayesian updating dynamic models computer science statistics 
west harrison 
bayesian forecasting dynamic models new york springer verlag 
wong liang 
dynamic importance weighting monte carlo optimization proceedings national academy science appear 
