spoken document retrieval evaluation investigation new metrics john garofolo ellen voorhees cedric vincent stanford national institute standards technology nist information technology laboratory building room gaithersburg md describes trec spoken document retrieval sdr track implemented evaluation retrieval broadcast news excerpts combination automatic speech recognition information retrieval technologies 
motivations sdr track background regarding development implementation discussed 
sdr evaluation collection topics described summaries analyses results track 
alternative metrics automatic speech recognition applicable retrieval applications explored 
plans sdr tracks described 

background spoken document retrieval sdr involves search retrieval excerpts recordings speech combination automatic speech recognition information retrieval techniques 
performing sdr speech recognition engine applied audio input stream generates time marked textual representation transcription speech 
transcription indexed may searched information retrieval engine 
community wide evaluation sdr technology implemented trec 
pilot evaluation implemented known item task particular relevant document retrieved set queries hour collection radio television news broadcasts 
retrieval conditions implemented examine effect recognition performance retrieval performance 
retrieval human generated transcripts purposes evaluation considered perfect recognition 

baseline retrieval ibm contributed recognizer generated transcripts word error rate 
provided common recognition error condition sites access recognition system 

speech retrieval recordings broadcasts requiring recognition retrieval technologies 
thirteen sites participated pilot evaluation implemented speech retrieval condition team site speech recognition system 
pilot evaluation proved evaluation sdr technology implemented existing technologies worked quite known item task small collection 
results nist chose highlight percent target stories retrieved rank systems 
percent retrieved rank metric university massachusetts retrieval system yielded best performance conditions 
umass system achieved retrieval rate retrieval condition baseline retrieval condition 
full sdr condition umass dragon systems produced best recognizer transcript word error rate achieved retrieval rate 
motivation sdr track designed address known inadequacies sdr track small corpus known item task provide realistically challenging retrieval task 
nominally hour broadcast news test set collected linguistic data consortium ldc selected traditional trec ad hoc style relevance task chosen topics relevance assessments generated human assessors 
recognizer produced transcript sets different word error rates provided nist ldc human generated transcripts 
time sites encouraged contribute best recognizer produced transcripts sites run retrieval 
improved test paradigm alternative transcription sets spectrum recognition error rates permitted examine relationship recognition errors retrieval accuracy 
new task permitted explore development alternative metrics automatic speech recognition technology address particular inadequacies technology regard information retrieval applications 

sdr evaluation plan complete evaluation plan trec spoken document retrieval track www nist gov speech sdr sdr htm evaluation modes sdr track included retrieval conditions provided component control experiments allowing sites access speech recognition technology participate required retrieval perfect human transcribed transcriptions broadcast news recordings 
baseline required retrieval sets speech recognition generated best transcripts produced nist cmu sphinx iii recognition system 
moderate word error rate substantially higher word error rate 
speech optional retrieval broadcast news recordings 
condition required speech recognition retrieval implemented different sites 
cross recognizer cr optional retrieval best speech recognizer generated transcripts contributed sites 
condition provided control recognition allowing evaluate retrieval variety recognition systems range error rates 
speech recognition retrieval experts encouraged team create pipelined hybrid sdr systems 
addition participation levels created allow participation retrieval sites access speech recognition system quasi sdr sites access speech recognition technology permitted run retrieval baseline recognizer transcripts transcripts 
retrieval conditions full sdr sites implemented recognition retrieval recorded news broadcasts retrieval baseline recognizer transcripts transcripts 
retrieval conditions minimally participants full sdr best word recognizers encouraged submit recognized transcripts nist recognition vs retrieval evaluation provide material cr condition 
test corpora ldc broadcast news corpus chosen sdr task contained news data radio television sources fully transcribed story subset hours broadcast news corpus collected june january chosen test corpus 
hours selected filtering 
contained stories words 
evaluation run summer results reported trec november darpa broadcast news workshop march 
baseline recognizer transcripts cmu permitted nist sphinx iii broadcast news recognition system create set transcripts baseline retrieval condition 
nist nodes cluster pc linux machines implement baseline recognition runs 
system yielded word error rate second word error rate 
sdr topics team nist trec assessors met april select topics test collection similar procedures trec ad hoc tasks 
assessors instructed find topics relevant news stories collection nist prise search engine 
limitations collection assessors able develop topics 
mean number relevant stories topic 
samples find reports fatal air crashes topic economic developments occurred hong kong incorporation chinese people republic topic 
evaluation results sites recognition retrieval teams participated sdr track 
sites performed full sdr task implementing recognition retrieval components 
sites required implement control conditions 
full sdr recognition retrieval att cmu group cmu cambridge university uk dera uk dera royal melbourne institute technology australia mds sheffield university uk shef tno tpd tu delft netherlands tno university ma dragon systems umass remaining sites performed quasi sdr portion task 
quasi sdr retrieval cmu group cmu nsa nsa university md umd speech recognition component performance primary purpose sdr track evaluate retrieval spoken documents 
sites best word recognition produce transcripts input retrieval systems encouraged submit sharing cross recognizer condition nist examine effect recognition error rate retrieval performance 
participating full sdr sites submitted recognition output nist scoring 
full sdr sites alternative recognition technique phone recognition lattices chose share recognition results 
best recognition results submitted cambridge university htk recognition system test set word error rate mean story word error rate shows histogram story word error rates submitted best systems 
histogram gives graphical profile recognizer performance stories collection complete table recognition scores submitted systems appendix sdr mean story word error rate histogram stories att dragon nist nist shef 
story word error rate histograms submitted recognized transcripts retrieval results test participants required submit relevance rank ordered list id top stories retrieved topic 
results scored assessments created nist assessors trec eval scoring software 
trec tasks primary retrieval metric sdr evaluation mean average precision map topics 
shows results retrieval conditions 
retrieval condition map att cmu cmu cu htk dera mds nsa shef tno umass umd mean average precision retrieval condition mean average precision required retrieval conditions graph shows retrieval conditions university massachusetts system achieved best mean average precision 
umass system achieved map retrieval human transcripts 
system applied moderate error baseline recognizer transcripts achieved map high error baseline recognizer transcripts map 
speech input condition team site dragon systems recognizer word error rate umass system achieved map system performed similarly condition map 
implemented second recognition retrieval system achieved map highest results input recognizer evaluation 
interesting note results exceeded results obtained map human transcripts 
attributes new approach implemented document expansion contemporaneous newswire texts 
applied new document expansion approach runs runs appendix gives complete tabulation mean average precision scores systems conditions 
general results evaluation quite seemingly linear gentle decline mean average precision recognition transcripts higher word error rate 
cross recognizer retrieval results explore apparent relationship 
cross recognizer retrieval results alternative recognition metrics cross testing retrieval systems recognizer transcripts sets permitted examine retrieval performance wider variety recognition data truly examine relationship recognition performance retrieval performance 
provided data evaluate recognition metrics suitability retrieval experiment new ones 
full sdr sites cambridge university dera rmit mds sheffield university implemented cross recognizer cr retrieval condition submitted recognized transcripts covered wide range word error rates 
see appendix tabulation results 
plot mean story word error rate mean average precision retrieval systems see linear trend mean average precision average recognition word error increases 
squared statistic tells retrieval systems recognition points average correlation 
high correlation indicates significant relationship word error rate retrieval accuracy 
retrieval results recognizer low far results dera recognizers significantly higher word error rates 
plot shows performance retrieval systems remains relatively parallel respect recognizer 
retrieval vs recognition mean story wer mean average precision dera mds shef mean cor coef ref cu htk att dragon shef dera dera 
cross recognizer results mean average precision vs mean story word error rate believed achieve higher correlation employed metric speech recognition emphasized information carrying words key retrieval 
metric predictive retrieval performance determine suitability recognizer retrieval task 
evaluated broad types metrics named entity metric evaluate error rate named entity words people locations organizations defined hub information extraction named entity evaluation disadvantage metric requires annotations transcripts 
great fortune gte bbn annotated sdr transcriptions named entity training data developed metric named entity word error rate ne wer score named entities recognizer transcripts 
implement ne wer named entity scoring software align annotated named entity words transcript words recognizer transcripts 
alignments embedded named entity tags scored nist speech recognition scoring software 
embedded tags permitted score named entity words 
introduce entity tagger error metric ignored named entity words inserted recognizer evaluated named entity words annotated transcripts 
general ir metrics ir approaches process filter weight words recognizer transcripts scored 
metrics useful predicting retrieval performance recognition performance tune recognizer retrieval task 
considered metrics word filtered word error rate swf wer apply word list words recognizer transcripts remove carrying words 
implement metric removed occurrences words word word list recognizer transcripts 
performed word error rate scoring filtered transcripts 
stemmed word filtered word error rate apply stemmer results swf wer filtering process remove word differences irrelevant retrieval algorithms 
implement metric applied implementation porter stemmer word filtered recognizer transcripts 
performed word error rate scoring filtered transcripts 
ir weighted stemmed word filtered word error rate wer 
apply ir indexing algorithm weight words prior word error rate scoring 
currently examining ir algorithms application implemented metric 
results alternative metrics applied sdr recognizer shown 
note named entity metrics appear clearly change relative ranking recognizer transcript sets 
metrics show recognition system poorer performer regard named entities evidenced word error rate 
hypothesis adjustment sphinx pruning thresholds artificially reduced likelihood longer words recognized words content carrying named entities 
scores metrics applied recognized transcripts appendix sdr recognition content error measures dragon att nist shef nist error rate word error rate wer mean story wer named entity wer named entity mean story wer word filtered wer word filtered mean story wer stemmed word filtered mean story wer stemmed word filtered wer alternative recognition metrics quantify efficacy metrics predictive tools performed squared analysis scores retrieval systems versus recognition metrics transcript sets 
results analysis shown table 
dera mds shef mean cor mean rank wer ne wer ne swf wer swf wer table correlation recognition metrics retrieval performance table shows average retrieval systems named entity test set word error rate newer named entity mean story word error rate provide best correlation retrieval performance mean system squared values 
visually depicted shows retrieval performance versus recognition performance terms named entity mean story word error rate ne 
retrieval vs recognition named entity mean story wer ne mean average precision dera mds shef mean cor coef ref cu htk dragon dera dera shef att 
cross recognizer results mean average precision vs named entity mean story word error rate 
successfully implement ad hoc retrieval task larger corpus broadcast news 
best performance retrieval speech recognizer map approached best performance retrieval perfect human generated transcripts 
near linear relationship recognition word error rate retrieval performance 
investigated alternative metrics recognition performance predictive retrieval performance 
named entity word error rate highly correlated retrieval performance word error rate 
hesitant declare retrieval speech recognition generated transcripts solved problem 
sdr collection stories quite small retrieval evaluation 
challenge determine retrieval performance scales larger realistic collections broadcast news remove artificially constrained components evaluation known story boundaries 

plan entire tdt corpus sdr evaluation 
tdt corpus collected linguistic data consortium darpa topic detection tracking tasks contains hours stories broadcast news evenly sampled month time period january june 
nist assessors create ad hoc style topics sdr track existing transcripts 
transcripts condition evaluation 
increased interest trec supporting evaluation condition story boundaries unknown naturally model real implementation sdr 
support condition systems permitted optional run baseline speech recognizer transcripts recognizer output sans story boundaries 
systems output time stamp top retrieved stories story id time stamps mapped story ids prior standard trec eval scoring 
duplicate stories removed systems generate time stamps penalized 
goal task systems find mid point single topical hotspot relevant stories 
notice views expressed authors construed represented endorsements systems official findings part nist government 
voorhees garofolo stanford sparck jones trec spoken document retrieval track overview results proc 
trec darpa speech recognition workshop february 
graff wu macintyre liberman broadcast news speech language model corpus proc 
darpa speech recognition workshop february 
garofolo voorhees stanford lund trec spoken document retrieval track overview results proc 
trec darpa broadcast news workshop march 
johnson moore sparck jones woodland spoken document retrieval trec proc 
trec november 
allan callan sanderson xu inquery trec proc 
trec november 
singhal choi hindle lewis pereira trec proc 
trec november 
przybocki fiscus garofolo pallett hub information extraction evaluation proc 
darpa broadcast news workshop march 
miller schwartz weischedel stone named entity extraction broadcast news proc 
darpa broadcast news workshop march 
hub ne scoring software recognition extraction evaluation pipeline saic ftp jaguar ncsl nist gov csr official scoring 
tar porter algorithm suffix stripping program july pp 

graff liberman tdt text speech corpus proc 
darpa broadcast news workshop march 
tno 
