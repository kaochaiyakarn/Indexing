fl kluwer academic publishers boston 
manufactured netherlands 
incremental multi step learning jing peng jp ucr edu college engineering university california riverside ca ronald williams ccs neu edu college computer science northeastern university boston ma received may editor 
presents novel incremental algorithm combines learning known dynamic programming reinforcement learning method td return estimation process typically actor critic learning known dynamic programming reinforcement learning method 
parameter distribute credit sequences actions leading faster learning helping alleviate nonmarkovian effect coarse state space quantization 
resulting algorithm learning combines best features learning actor critic learning paradigms 
behavior algorithm demonstrated computer simulations 
keywords reinforcement learning temporal difference learning 
incremental multi step learning learning method new direct model free algorithm extends step learning algorithm watkins combining td returns general sutton natural way delayed reinforcement learning :10.1.1.132.7760
allowing corrections incrementally predictions observations occurring past learning method propagates information rapidly important 
learning algorithm works significantly better step learning algorithm number tasks basis integration step learning td returns possible take advantage best features qlearning actor critic learning paradigms potential bridge 
serve basis developing various multiple time scale learning mechanisms essential applications reinforcement learning real world problems 

td returns direct dynamic programming reinforcement learning algorithms updating state values state action values state transitions experienced 
update turn particular choice estimator value updated spells differences various learning methods 
section describes important computationally useful class estimators td estimators sutton watkins :10.1.1.132.7760
world state time step assume learning system chooses action immediate result reward received learner world undergoes transition state objective learner choose actions maximizing discounted cumulative rewards time 
precisely fl specified discount factor 
total discounted return simply return received learner starting time flr fl delta delta delta fl delta delta delta objective find policy rule selecting actions expected value return maximized 
sufficient restrict attention policies select actions current state called stationary policies 
policy state define jx expected total discounted return received starting state policy 
optimal policy notation dynamic programming reinforcement learning methods involve trying estimate state values fixed policy 
important class methods estimating policy td estimators investigated sutton watkins 
watkins notation denote corrected step truncated return time flr fl delta delta delta fl gamma gamma fl estimate time equal corrected truncated returns unbiased estimators watkins watkins shows corrected truncated returns error reduction property expected value corrected truncated return closer sutton td return time gamma delta delta delta fl gamma fl fl gamma delta delta delta fl gamma flr td return just fl td return flr fl delta delta delta exact actual return 
watkins argues markov decision problem choice trade bias variance 
sutton empirical demonstration sutton favors intermediate values closer :10.1.1.132.7760
singh sutton describe algorithms achieve near optimal performance certain prediction tasks setting time step transition probability immediately preceding transitions 
details see sutton singh sutton watkins :10.1.1.132.7760

step learning step learning watkins simply learning simple incremental algorithm developed theory dynamic programming ross delayed reinforcement learning 
learning policies value function represented dimensional lookup table indexed state action pairs 
formally notation consistent previous section state action fl xy efr jx ag xy probability reaching state result action state follows max intuitively equation says state action value expected total discounted return resulting action state continuing optimal policy 
generally function defined respect arbitrary policy fl xy just optimal policy 
learning algorithm works maintaining estimate function denote adjusting values just called values actions taken reward received 
done sutton prediction difference td error sutton difference immediate reward received plus discounted value state value current state action pair fl gamma immediate reward state resulting action state max :10.1.1.132.7760
values adjusted gamma ff ff fl ff learning rate parameter 
terms notation described previous section equation may rewritten gamma ff ffr learning method uses td estimator expected returns 
note current estimate function implicitly defines greedy policy arg max 
greedy policy select actions largest estimated value 
important note step learning method specify actions agent take state updates estimates 
fact agent may take actions 
means learning allows arbitrary experimentation time preserving current best estimate states values 
furthermore function updated ostensibly optimal choice action state matter action followed state 
reason learning experimentation sensitive 
hand actor critic learning updates state value state actual action selected optimal choice action experimentation sensitive 
find optimal function eventually agent try action state times 
shown watkins watkins dayan equation repeatedly applied state action pairs order state action pair value updated infinitely converge converge probability long ff reduced suitable rate 
watkins described possible extensions step learning method different value estimators illustrated returns learning empirical demonstrations memorizing past experiences calculating returns learning period learning period specifies number experiences occurring past agent needs store 
section derives novel algorithm enables value estimation process done incrementally 

learning section derives learning algorithm combining td returns general learning incremental way 
note terms notation introduced step learning simply learning making special case 
simplicity follows drop superscript assume policy agent greedy policy 
fl gamma fl gamma arg max 
equation step equation difference td return equation estimated value written gamma fle fl delta delta delta fl gamma gamma learning rate small adjusted slowly second summation right hand side equation small 


forever current state choose action maximizes carry action world 
short term reward new state fl gamma fl gamma state action pair ffl flt ffl fft ffe 
learning algorithm 
learning algorithm summarized activity trace state action pair corresponding eligibility trace described barto sutton anderson 
main difficulty associated learning markov decision process rewards received non greedy action evaluate agent greedy policy policy followed 
words learning experimentation sensitive assuming fixed 
way difficulty zero step non greedy action taken 
argued rummery zeroing effect subsequent rewards prior non greedy action hindrance help converging optimal policies max may provide best estimate value state difficulty changes time step may affect turn affect 
effects may significant small ff proportional ff peng 
time step learning algorithm loops set stateaction pairs grow linearly time 
worst case set entire state action space 
number state action pairs actual updating required kept manageable level maintaining state action pairs activity trace fl significant quantity declines exponentially fl 
approach implement learning system parallel machine state action pair mapped separate processor 
corresponds directly kind neural network implementation envisioned actor critic approach barto sutton anderson 
interesting note learning actor critic learning td returns value estimators trace mechanism 
reasonable expect learning algorithm exhibit beneficial performance characteristics attributable td returns illustrated barto sutton anderson sutton :10.1.1.132.7760
time learning step learning construct value function state action space just state space making capable discriminating effects choosing different actions state 
learning experimentation sensitive step learning reasonable expect actor critic learning 
learning appears incorporate best features qlearning actor critic learning paradigms single mechanism 
furthermore viewed potential bridge 

discussion examined learning algorithm td returns computed maximum values state visited 
possibilities 
example algorithm may estimate td returns current exploration policy 
algorithm called sarsa described rummery niranjan 
algorithm update rule deltaw ff flq gamma fl gammak denotes weights connectionist networks associated action selected 
terms learning equations replaced fl gamma demonstrated rummery performance learning including sarsa shows sensitivity choice training parameters exhibits robust behaviors standard learning 
see 
experiments involving markovian non markovian tasks details omit carried validate efficacy learning algorithm 
results showed learning outperformed actor critic learning step learning experiments 
significant performance improvement learning system simple learning system including case learner experiences learner clearly due td return estimation process effect making alterations past predictions trial 
main benefit conferred td expect model multiple update methods priority dyna peng peng williams moore atkeson perform 
additional experiments techniques showed performed significantly worse learning 
believe reason coarse state space quantization effect making environment non markovian increasing td sensitive non markovian effect 
similar argument related algorithm 
noted fixed period learning process watkins sufficiently long learning periods experience replay process lin produce beneficial effects learning 
approaches operate batch mode replay backwards memorized sequence experiences learning agent 
computational standpoint incrementality learning attractive watkins batch mode learning lin experience replay process computation distributed time evenly circumstances ease demands memory speed 
similar arguments sutton :10.1.1.132.7760
furthermore incrementality speeds improvement performance 
experiment line learning applied experiences line learning results line learning cases worse obtained line learning 
additional pleasing characteristic learning method achieves greater computational efficiency having learn model world peng peng williams sutton suited parallel implementation 
look table representation main focus far shown difficulty learning implemented line connectionist networks done rummery 

learning algorithm interest incrementality relationship learning watkins actor critic learning barto sutton anderson 
algorithm step learning algorithm expected converge correct values arbitrary policy tries action state obvious strategies gradually reducing gradually turning boltzmann temperature learning proceeds probably allow convergence 
spite learning algorithm outperformed step learning algorithm problems experimented far 
clear continuous time systems systems time discrete fine grained algorithms propagate information back step time sense little value 
cases td methods luxury necessity 
general viewed time scale parameter situations argue better understanding regard important area research 
acknowledgments wish rich sutton valuable suggestions continuing encouragement 
reviewers insightful comments suggestions 
supported iri national science foundation 
barto sutton anderson 

neuronlike elements solve difficult learning control problems 
ieee transactions systems man cybernetics 
lin 

reinforcement learning robots neural networks 
ph 
dissertation carnegie mellon university pa moore atkeson 

prioritized sweeping reinforcement learning data time 
machine learning 


reinforcement learning control actions noisy non markovian domains 
unsw cse tr university new south wales australia 
peng 

efficient dynamic programming learning control 
ph 
dissertation northeastern university boston ma 
peng williams 

efficient learning planning dyna framework 
adaptive behavior 
ross 

stochastic dynamic programming 
new york academic press 
rummery niranjan 

line learning connectionist systems 
cued tr cambridge university uk 
singh sutton 

reinforcement learning replacing eligibility traces 
machine learning press 
sutton 

integrated architectures learning planning reacting approximating dynamic programming 
proceedings seventh international conference machine learning 
sutton 

learning predict methods temporal differences 
machine learning 
watkins dayan 

learning 
machine learning 
watkins 

learning delayed rewards 
ph 
dissertation king college uk 
