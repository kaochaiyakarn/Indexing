overfitting avoidance bias david wolpert santa fe institute old trail suite santa fe nm santafe edu sfi tr note pretty nfl papers ftp area 
supervised learning commonly believed occam razor works penalizing complex functions helps avoid overfitting functions data improves generalization 
commonly believed cross validation effective way choose algorithms fitting functions data 
schaffer presents experimental evidence claims 
current consists formal analysis contentions schaffer 
proves contentions valid experiments interpreted caution 
doing proves scenarios learning algorithm cross validation fails succeeds similarly learning algorithm far training set behavior concerned 
interestingly proof indicates scenarios test set fails accurately predict behavior test set test set works 
keywords overfitting avoidance cross validation decision tree pruning inductive bias extended bayesian analysis uniform priors 

overfitting avoidance bias previous papers schaffer disputes common claim trying avoid overfitting introducing complexity cost necessarily improves generalization 
presents evidence possible cross validation efron stone choose learning algorithms lead worse generalization despite common lore contrary 
generally argues desideratum occam razor fail works 
arguments schaffer part numerical experiments involving algorithms fit decision trees training sets buntine 
current presents formal mathematical analysis schaffer contentions 
analysis confirms major points shows experimental results interpreted caution 
intuitively points schaffer hold simple reason prove principles correct priors problem prove essentially concerning efficacy particular learning algorithm learning algorithm involves complexity penalization cross validation 
section outlines formalism analysis 
section investigates certain aspects schaffer parity experiment 
section considers ramifications replacing schaffer error function training set error function schaffer parity experiment random functions experiment 
section briefly discusses extensions analysis 

extended bayesian formalism formalism extended bayesian formalism introduced wolpert analyze statistical mechanics approach learning wolpert lapedes 
section describes salient aspects formalism 
label input space elements output space elements scenarios investigated schaffer finite elements respectively 
label training set pairs indicates component th pair similarly 
set values written similarly pairs altogether distinct 
training sets generated sampling target function usually noise added 
calculational simplicity restrict attention function noise scenario full fledged distribution 
entire process generating underlying function contained conditional probability distribution 
called likelihood function sampling assumption called prior called posterior 
trained learning algorithm outputs hypothesis function 
viewed learning algorithm guess loosely speaking 
goes pairs said reproduce training set 
relevant aspects learning algorithm completely specified distribution 
true regardless represented algorithm terms trees schaffer manner 
algorithms considered schaffer deterministic general supervised learning concerned learning algorithms access see wolpert 
words changes stays learning algorithm behavior unchanged 
general restriction concerning explicitly invoked 
conventional bayesian analysis application probability theory event space consisting doubles 
note analysis term hypothesis refers target function hypothesis function 
extended bayesian analysis ap plication probability theory event space consisting triples 
frameworks usually wishes inject element decision theory analysis 
current purposes done generalization error function er known loss function cost function 
function measures real world cost associated triple 
example case noise free symbolic classification scenarios cost average number differences er called sampling distribution kronecker delta function 
concerned evaluating distributions form er fixed kind distribution interest real world particular kind distribution schaffer investigated 
shown detail wolpert possible express frameworks pac valiant blumer blumer vapnik uniform convergence vapnik baum haussler statistical mechanics school tishby schwartz van der conventional bayesian analysis berger terms extended bayesian formalism 
example pac statistical mechanics school concern evaluating properties distribution certain 
reverse true extended bayesian analysis general frameworks 

schaffer parity experiment general determine particular learning algorithm preferred principles reasoning 
proven wolpert proof simple repeat 
fixed view vector dimension components indexed possible hypothesis functions 
note list real numbers 
view similarly 
theorem er invariant interchange written non euclidean inner product distributions 
proof write er 
rewritten er 
mentioned er 
hypothesis er symmetric formula just inner product vectors 
qed 
theorem means generalize probability distribution generalization error values determined aligned learning algorithm actual posterior 
prove principles certain form prove particular aligned prove concerning learning algorithm generalizes 
note er symmetric longer inner product 
error function true generalize determined aligned 
change aligned different meaning 
particular instantiation theorem fact prove principles learning algorithm tries mitigate overfitting introducing preference 
phrased differently claims principles proof automatic embodiment occam razor valid 
essence crux schaffer experiments 
implications theorem go 
shows prove principles desideratum results better generalization desideratum concerns complexity overfitting 
argument claiming proof wrong 
see wolpert 
hard dispute view ultimately interested know want know immediate implication considering misleading far real world generalization concerned 
worth noting conventional bayesian analysis concerned supervised learning formalisms pac uniform convergence formalism 
certainly minimum careful ascribing meaning results non distribution 
despite fact respects theorem suffices point schaffer doesn directly concern experiments 
go analysis wolpert fully understand schaffer results 
experiment schaffer presents follows vertices dimensional hypercube 

parity function evaluated mod 
fixed schaffer experiments means implicitly restrict things probabilities triples re concerned probabilities triples element training sets generated noisy 
independent identically distributed sampling noise consists replacing randomly certain probability precisely likelihood written 
schaffer experiments sampling sampling distribution 
reflects noise equals equals 
error function schaffer uses variation error function discussed 
modification allow noise occurs creation training set arise testing set er 
error function occurs likelihood 
schaffer concerns learning algorithms 
calls naive algorithm tries fit decision tree directly training set pruning 
indicated 
second calls sophisticated algorithm starts tree prunes effort avoid overfitting 
degree pruning set crossvalidation 
algorithm indicated 
pruned trees generically complex non pruned ones schaffer definition complexity 
note parity function full tree leaves maximum complexity 
algorithms schaffer experimentally evaluates average error averaging training sets sampled words evaluates algorithms 
large values investigates naive algorithm lower average error sophisticated algorithm 
interprets meaning target function introducing complexity cost try mitigate overfitting degrades performance 
sophisticated learning algorithm uses cross validation choose complexity level free choose small results indicate scenario cross validation perform 
substantial overfitting avoidance cross validation section 
results schaffer experiments shouldn surprising 
parity problem known sticking point algorithms 
example consider learning algorithm local guess output goes question input value depends elements lie close space 
example nearest neighbor algorithm fixed small local 
algorithm true average error test set inputs outside training set increases training set size parity target function low noise 
true back propagation small input spaces see wolpert 
shouldn surprised see parity rear frustrating head schaffer experiments 
schaffer result expected hold weren peculiar aspects parity target function 
see consider algorithm naive schaffer produces maximally complex trees 
algorithm ways guesses parity function regardless data parity 
call algorithm 
theorem independent sampling distribution learning algorithm lower algorithm guesses maximally complex hypothesis functions 
proof note er 
temporarily restrict consider element training sets write 
combine results get average er 
algorithm equals er 
error function independent turn equals er 
rewrite er turn equals er regardless learning algorithm 
result shows learning algorithm true min er 
algorithm er need prove argmin er conditions theorem 
break problem wise problems 
fix show expression argmin conditions theorem 
need consider possible values 
argument argmin equals argument equals long argument case exceed argument second case 
qed 
noted proof theorem depends parity function necessary learning algorithm guess specified 
note learning algorithm lower parity algorithm guesses anti parity function evaluated mod regardless data 
proof essentially identical proof theorem 
guesses maximally complex hypothesis functions 
just proven corollary parity function independent noise level sampling distribution minimized learning algorithm guesses maximally complex hypothesis functions 
noise function say noise function noise level 
scenario theorem goes long max 
similarly long min algorithm smaller value parity 
corollary holds function provided noise noise greater 
noise mixed optimal 
simple example set scenario optimal guessing guess function equals parity function equals anti parity function half half function need maximal complexity general 
schaffer doesn compare compares 
just usually guesses complex guessed doesn guess parity function 
underlying point schaffer wishes algorithms penalize complexity worse certain proven formally theorem 
theorem proves algorithm allows possibility guessing parity function worse assuming 
consider technique cross validation choose fixed set learning algorithms resultant learning algorithm 
technique considered mapping training sets hypothesis functions 
words learning algorithm 
note learning algorithms choosing fixed ahead time cross validation learning algorithm 
algorithm allows possibility guessing parity function assuming candidate learning algorithms don guess parity function 
accordingly theorem proves target function parity cross validation give worse results just candidate learning algorithms cross validation considers 
surprising 
schaffer emphasizes target functions parity preferred compared cross validation algorithm prefers low complexity hypothesis functions 
sense crucial question target functions real world 
answered section 

training set error note theorem implies hard beat learning algorithm algorithm guesses assured parity function large number values 
note expect guess equals parity function values values 
accordingly worry schaffer large training sets compared size schaffer experiments biased favor 
schaffer responds somewhat heuristic fashion 
formal treatment touches points schaffer response approach wolpert 
approach investigates uniform case filter biases accompanying contain peaks schaffer rediscovered trick see low 
modifies error function turning training set error function er 
error function give credit simply reproducing training set 
doesn get bias favor referred 
view way zero noise error function schaffer upper bound decreases increasing learning algorithm reproduces training set 
er viewed er rescaled remove effect put footing 
doing means re investigating learning algorithms re doing part input space things problematic noise optimal behavior known 
original pre modification error function schaffer parity experiments referred conventional error function 
stated assumed expression er really means training set error function er conventional error function 
don worry dividing zero implicitly assumed equal zero exactly fine infinitesimal equal zero exactly replacing er er effect theorem 
superior algorithms prefer lower complexity training set error 
modification error function affect things 
particular affects generalization behavior doesn fix affects generalization behavior admits truth embodied conventional bayesian analysis real world experiment don know float 
precisely error function schaffer likelihood function extension theorem wolpert 
theorem uniform regardless noise function sampling distribution independent learning algorithm 
proof er 
turn written er kronecker delta function 
assumption uniform proportional er 
sum equals er 
similarly expand distribution er uniform proportional er 
case constant proportionality depends depends likelihood implicitly assumed held fixed varies 
show inner sum appearing expression constant independent implies just equals inner sum constant independent claimed similarly 
prove inner sum independent expand 
likelihood schaffer considers independent values similarly value outer delta function independent values accordingly write union representing values values respectively 
doing transforms sum product sums 
sum independent second sum 
prove second sum independent evaluate inner sum getting 
hold fixed consider effect replacing particular inspection results new value delta function gotten leaving unchanged replacing words rewritten 
mapping set 
accordingly performing mapping summing doesn change value sum 
allows write sum 
just finished proving replace replace doing gives 
changing value replacing doesn affect value sum 
obviously changing value doesn affect value sum 
iterate process replacing different values see changing arbitrary function effect sum 
needed prove establish theorem 
qed 
immediate consequence theorem varies uniform 
expectation values cast average uniform quantities respectively 
picture emerges averaged algorithms equally measured 
pair learning algorithms algorithm better training set generalization second algorithm better 
proven formally 
emphasized just find cross say beaten algorithm result schaffer exper iments demonstrate 
just speak beats cross validation vice versa 
true algorithm recourse construction pathological scenarios needed 
don know accordingly average uniform average distinguish learning algorithms see way construct algorithm performs better algorithm random guessing non uniform algorithm explicitly implicitly exploits non uniformity 
relative efficacy different learning algorithms decision tree constructors cross validation choose set learning algorithms completely determined accurately algorithms encapsulate non uniformities 
theorem provides way coming important implications theorem rule uniform principles reasoning prove algorithm lower principles reasoning 
kept mind encounters papers appear provide just principles proof favor particular algorithm baum haussler 
theorem formally justifies schaffer main point 
theorem uniform viewed saying set generalizer preferring beats preference exactly offset set reverse true regardless complexity measure 
schaffer aware utility quantities addressed theorem uniform 
fact second set experiments fixing schaffer repeatedly chose target function random uniform set possible functions 
compared average behavior naive algorithm sophisticated averaged training sets generated words measured uniform 
discussion mean real world opposed experiments schaffer see appendix wolpert 
unfortunately theorem schaffer point partially negates significance second set experiments 
just parity experiment second set experiments schaffer conventional error function training set error function 
result naive algorithm better sophisticated 
theorem proves algorithms perform exactly equally training set error 
consequently result schaffer saw due fact naive algorithm performs better elements inside training set 
schaffer result doesn say merits pruning decision trees 
clear considering case noise way pruning possibly assistance results improved training set guessing assured giving worse training set guessing 
fact theorem comes surprise die hard advocate pruning 
fact sets scenario training set behavior filtered training set behavior distinguish various learning algorithms fixed things pruning schemes perform poorly 
theorem shows schaffer second experiment just scenario 
intuitively theorem holds fixed distributions theorem considers 
uniform fixed training set pattern just regardless accordingly scenarios considered theorem biases learning algorithm guessing certain training set patterns possibly help far set error concerned 
general property hold fixes distribution interest 
example certain learning algorithms perform better distribution interest see theorem 
interestingly theorem set limits distributions fixed theorem completely free float theorem learning algorithms independent noise function sampling distribution exists lower exists different lower ii exists lower exists different lower iii exists lower exists different lower iv exists lower exists different lower proof examine implicitly assume equal zero sum properly defined 
long meet implicit assumption sum independent equals er 
rewrite take uniform uniform turn write 
evaluate evaluating uniform prior 
theorem independent 
shown sum re interested independent 
accordingly lower reverse true 
proves 
prove ii indicate value occurring learning algorithm similarly 
hypothesis exists 
write convention summand equals zero equals zero 
means 
re done 
know different 
expansion know true 
proves ii 
prove iii write 
hypothesis exists 
ii means exists different equal zero equal zero undefined 
set get 
means write 
hypothesis exists 
means exists 
set plug expansion 
qed 
theorem implications different heuristic arguments schaffer presents section 
main advantage theorem arguments completely formal exploit theorem generate formal results making theorem 
note restrictions whatsoever concerning assumed theorems 
accordingly theorem means algorithm assured doing worse training set 
phrased differently means learning algorithms scenario algorithm beats second 
mean learning algorithm scenario algorithm beats algorithms simultaneously 
example true exists minimized 
see note theorem uses property parity function true algorithm particular minimize 
consider spread learning algorithm extends particular 
learning algorithm strict inequality algorithm lower spread learning algorithm 
find spread learning algorithm minimize learning algorithm better spread learning algorithm emphasize results theorem rely construction pathological cases worth recasting results 
define value uses learning algorithm 
value uses learning algorithm 
write theorem taken fixed theorem learning algorithms independent noise function sampling distribution averaged ii averaged training set iii averaged iv averaged training set 
proof result established proof theorem 
prove iii note possible dimensional real valued vector lying simplex 
indicate vector components 
formally acts average proportional quantity dp dp integral restricted dimensional simplex 
equals dp assume choice choice number elements similarly fixed doesn depend average equals dp 
write dp 
term inside square brackets independent average proportional 
establishes iii 
prove ii parallel proof 
examine 
sum rewritten take uniform 
turn write 
evaluate evaluating uniform 
uniform bayes theorem tells uniform accordingly theorem uniform conclude independent 
shown sum re interested independent 
establishes ii 
prove iv note average dp dp integral restricted dimensional simplex 
rewrite dp dp dummy value 
rewrite dp 
proof theorem break components fixes values values lying inside fixes values outside find terms sum depend note likelihood depends write 
expanding see independent allows write 
write term curly brackets dp dp 
re assuming equal zero exactly denominator sum non zero 
change variables integral rearranging indices 
pair discrete valued vectors real valued vector indexed value transform dependence rearranged ar invertible fashion equivalent mapping space vectors manner 
jacobian transformation transformation doesn change functional form constraint forcing lie simplex 
expressed new coordinate system integral dp new index corresponding old index integral value original integral arbitrary see integral independent accordingly rewrite sum func function func 
average proportional proportionality constant depends evaluate sum note independent fix value evaluate sum 
furthermore uniform independent constant independent likelihood mean 
allows write 
sum turn just equals evaluated uniform 
theorem expectation value independent 
establishes iv 
qed 
words measures utility learning algorithm generically known risks algorithms equivalent average 
particular algorithm cross validation better average doesn 
example assume re bayesian calculate bayes optimal guess assuming particular prior 
calculate minimizing data conditioned risk assumed 
compare guess uses non bayesian method 
iv means priors loosely speaking person lower data conditioned risk risk lower 
example learning algorithm know low vc dimension know happens training set low empirical misclassification rate iv assurances algorithm perform better novel test points 
learning algorithms algorithms average risk learning algorithm lower risk learning algorithms differ nonuniform different algorithms different data conditioned risk similarly kinds risk algorithms algorithm optimal algorithm beats algorithms algorithms optimal 
variations generically limit probability random value testing set lies goes usually implies training set error equivalent conventional error limit 
scenarios theorems imply algorithm generally superior measured uses conventional error function 
particularly interesting light fact pac statistical mechanics formalism concern conventional error function 
generally guided insight provided theorems return papers appear claim principles justification particular learning algorithm seemingly innocuous assumption theorems get results 
unfortunately process shed light formalisms papers tends quite laborious 
recall technique cross validation choose fixed set learning algorithms learning algorithm 
immediate implication said far applies just cross validation learning algorithms favor non complex accordingly cross validation justified principles reasoning find scenarios fails works loosely speaking scenarios weighted degree failure success 
need view cross validation learning algorithm see 
possible modify formalism having probabilities triples probabilities triples learning algorithm chooses choice determined 
opposed hypothesis function chooses choice determined 
technique cross validation particular just particular 
perform analysis space exploited 
gets essentially results gets replaced reflects fact close formal parallel analysis learning probabilities probabilities 
discussion implications formal parallel see section meta generalization wolpert 
results concerning cross validation peculiar number quite reasonable heuristic justifications cross validation 
example consider scenario learning algorithms target function 
suppose clear training sets size slightly smaller constructed sampling performs deal better training set guessing algorithms 
scenario expect cross validation optimize generalization averages training set chosen 
cross validation choose average best algorithms entire training set 
expect kind situation generic case cross validation generically 
theorems prove situation generic 
fixed cross validation works fails 
true judges failure examining behavior particular training set averages training sets 
course taken mean cross validation complexity costs won practice 
certainly don advocate avoid 
results indicate advocate principles reasoning 
words assumption nature learning problem confronted 
assumptions implicit cross validation empirically best ones known learning problems tend encounter real world 
results concern priori validity various learning algorithms cast formal doubt major empirical method testing learning algorithms 
take data divide training set test set algorithm want training set careful introduce information test set 
re done test set see learning algorithm worked 
primary empirical method testing learning algorithms 
theorems priori assurance behavior test set correlates behavior particular re situation ll see element little noise data automatically know guess questions lying re going interested element 
formally speaking provided assurances concerning elements simply examining behavior test sets far data behavior concerned formally unjustified 
certainly claim pointless test set 
point simply point doing formally justified scenarios considered 
final comment noted course error functions likelihoods ones considered 
ways quantity stuff measure efficacy particular learning algorithm get expectation value 
wish examine stuff stuff 
alternatives explored schaffer addressed systematic way machine learning community 
scope 
passing note quadratic error function likelihood independent values general results reported modified 
acknowledgments supported part nlm lm 
wray buntine helpful comments 
cullen schaffer going manuscript 
notes 
matter reasonable arguments particular set priors simple fact matter priors wrong 
somewhat conventional bayesians refer situation priors wrong opportunity learn 
requirement prove certain prior probabilities 
mean argue certain prior probabilities properly reflecting prior beliefs 
distinction explicit mathematics 
particular simple expedient treating distinct objects mathematics allow confuse prior beliefs concerning quantity prior probability quantity mistake commonly conventional bayesian analysis 
see section 

careful misinterpret 
particular take view just degree belief set believer wish 
formalism personal beliefs concerning probable function embodied need 
thought intuitively true probability 
contrast true input output function guessed function 
formally interpret terms personal degree belief amounts extra assumption 
assumption restriction probability distribution reflect fact intimately related 
extra assumption concerning needed 
assumption case subjective statistical preferences philosophy concerning means imposed mathematics letting mathematics speak 
imposing philosophy sin conventional bayesians sampling theory statisticians 
see appendix wolpert 

terms sum expression inside equals terms equals expression gives probability pair test set output value produced sampling noise 
proof theorem er algorithm equals er parity function 
hand rewrite er implies min er 
proof theorem breaking wise minimization problems see min er er moment re implicitly concerning values effect value er 
accordingly bounded er mentioned just value learning algorithm 
qed 

note argue example misleading different different probabilities flat average sense inappropriate 
construct weighted average argue favor particular algorithm 
don know appropriate weighting real world supervised learning don know 
correct misleading nature re lead ask average get algorithm superior average gives algorithm superior 
question answered affirmative iv 
alternatively compare algorithms averaging possible arrive iii 
stage try jump level argue shouldn perform flat average math responds way objection response new objection constructs new questions concerning probability distributions set questions answers state algorithms perform 
possible usual vc results suggest wise 
answer lies unfortunate fact vc results explicitly state probability conditioned refer probability exceeds 
turns vc results refer probabilities addressed theorems 
refer probability empirical misclassification rate 
refer different distributions 
question real world statistician distribution tells wants know 
vc results don explicitly concern training set appears results pac statistical mechanics formalisms results vc formalism overly sensitive uses conventional training set 

exist number interesting variations issue exists scenario particular learning algorithm optimal 
example wish restrict discussion certain kinds algorithms algorithms spread define optimal differently exists minimized variations scope 
baum haussler 

size neural net gives valid generalization 
neural computation 
berger 

statistical decision theory bayesian analysis 
springer verlag 
blumer alia 
occam razor 
information processing letters 
blumer alia 
learnability vapnik chervonenkis dimension 
journal acm 
buntine 

learning classification trees 
artificial intelligence statistics proceedings conference ai statistics florida 
hand ed 
efron 

computers theory statistics thinking siam review 
schaffer 
sparse data effect overfitting avoidance decision tree induction proceedings tenth national conference artificial intelligence aaai 
schaffer 
deconstructing digit recognition problem 
machine learning proceedings ninth international conference ml sleeman edwards eds morgan kaufmann 
schaffer 
overfitting avoidance bias 
machine learning 
schwartz alia 
exhaustive learning 
neural computation 
stone 

cross choice assessment statistical predictions roy 
statistic 
soc 

stone 

asymptotic equivalence choice model cross validation akaike criterion 
roy 
statistic 
soc 

stone 

asymptotics cross validation biometrika 
tishby levin solla 

consistent inference probabilities layered networks predictions generalization 
proceedings international joint conference neural networks washington pp 
ii ieee 
valiant 

theory learnable 
communications acm 
van der kawai 

generalization feedforward neural boolean networks 
international conference neural networks san diego ca 
vapnik 

estimation dependences empirical data 
springer verlag 
wolpert 

improving performance generalizers time series pre processing learning set 
los alamos laboratory report la ur 
wolpert 

connection sample testing generalization error 
complex systems 
wolpert lapedes 

investigation exhaustive learning 
santa fe institute report 
submitted 
wolpert 

relationship various supervised learning formalisms 
appear supervised learning 
wolpert ed 
addison wesley 
