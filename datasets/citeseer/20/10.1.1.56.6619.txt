language modeling variable length sequences theoretical formulation evaluation multigrams sabine deligne fr ed eric bimbot el paris enst dept signal cnrs ura rue paris cedex france european union 
mail deligne sig enst fr bimbot sig enst fr multigram model assumes language described output memoryless source emits variable length sequences words 
estimation model parameters formulated maximum likelihood estimation problem incomplete data 
show estimates model parameters computed iterative expectation maximization algorithm describe forward backward procedure implementation 
report results evaluation multigrams language modeling atis database 
objective performance measure test set perplexity 
results show multigrams outperform conventional grams task 

language viewed stream words put source 
source subject syntactic semantic constraints words independent dependencies variable length 
expect retrieve corpus text typical variable length sequences words 
multigram model aims modeling kinds dependencies 
presents theoretical background multigram model section details implementation means forward backward algorithm section 
report evaluation model task language modeling compare gram model section 

multigram model gram model assumes statistical dependencies words fixed length sentence 
model probabilities grams estimated interpolating relative frequencies grams way account variable length dependencies 
multigram model different assumption approach sentence considered concatenation independent variable length sequences words likelihood sentence computed sum individual likelihood corresponding possible segmentation 
delta delta delta delta delta delta denote string words denote possible segmentation sequences words delta delta delta 
multigram model computes joint likelihood corpus associated segmentation product probabilities successive sequences having maximum length denoting flg set possible segmentations sequences words likelihood gr flg decision oriented version model parses segmentation yielding approximation gr max flg instance abcd denoting sequence borders brackets gamma gr abcd max bcd abc ab cd ab bc cd classically abcd bja 
parameter estimation section derive maximum likelihood ml estimates multigram model parameters 
fs delta delta delta smg denote dictionary contains sequences formed combinations words language vocabulary 
multigram model fully defined set parameters theta consisting probability word sequence theta estimation set parameters theta training corpus obtained maximum likelihood ml estimation incomplete data observed data string words unknown data segmentation underlying string words 
iterative ml estimates theta computed em algorithm 
auxiliary function iteration flg theta log theta theta joint likelihood computed equation parameter estimates iteration theta conditional likelihood segmentation iteration theta theta theta term theta computed equation 
reestimation formula th parameter iteration derived directly maximizing auxiliary function theta constraint parameters sum 
yields flg theta theta flg theta theta number occurences sequence segmentation total number sequences equation shows estimate merely weighted average number occurences sequence segmentation 
iteration improves model sense increasing likelihood theta eventually converges critical point possibly local maximum 
decision oriented procedure readily derived reestimation formula segmentation iteration argmax theta flg setting theta ae reestimation formula reduces probability sequence simply reestimated relative frequency sequence best segmentation iteration procedure parses corpus ml criterion alternative way training done 
estimation multigram parameters equation refered em training equation viterbi training 

forward backward algorithm forward backward implementation avoid explicit search segmentations reducing complexity algorithm 
forward backward algorithm relies definition forward variables ff fl backward variable fi 
certain extent multigram model thought state ergodic hidden markov model state emitting sequence length transition probabilities equal 
forward backward training algorithm quite similar hmm training 
need third variable fl arises fact number sequences string words depends segmentation considered string number observations emitted classical hmm constant sequence states considered 
denote substring corpus words rank define variable ff likelihood partial corpus ff segmentation ends sequence words ff recursively calculated indicated box box recursion formula variable ff ff ff gamma gamma delta delta delta ff ff rest section notation ff ff gamma gamma delta delta delta represents likelihood associated segmentation sequence length define backward variable fi likelihood gamma words corpus fi recursive formula box box recursion formula variable fi fi delta delta delta fi fi fi third variable fl needed represents average number sequences segmentation segmentation ends sequence words segmentation sequence words average number sequences equal fl gamma fl fl gamma ff ff quantity ff ff shown likelihood segmentation sequence words deduce recursion formula fl box box recursion formula variable fl fl fl gamma ff ff fl fl parameter reestimation formula rewritten function variables ff fi fl iteration box 
box parameter reestimation formula sequence words ff fi ffi gammal delta delta deltaw fi fl ffi gammal delta delta deltaw ae gamma delta delta delta set parameters theta initialized relative frequencies occurences words length training corpus 
theta iteratively reestimated box training set likelihood increase significantly fixed number iterations 

evaluation section assess multigram model framework language modeling atis database values training methods described section em training viterbi training 
compare multigram model conventional gram model 
performances evaluated terms perplexity test training sets 
perplexity corpus size words obtained pp gamma log multigram model initialization procedure common em viterbi 
cases occurences symbols get initial estimates sequence probabilities 
avoid overlearning efficient discard occurences appearing strictly number times training iterations performed indicated equation em training described equation viterbi training 
sequence probabilities falling threshold set length assigned minimum probability initialization iteration probabilities renormalized add 
test phase likelihood multigram model computed equation equation yielding perplexity values respectively noted pp pp quantities computed em estimates viterbi estimates leads altogether different scores 
sequences length minimum probability likelihood string known words computed 
unknown words considered single unknown sequence length probability added dictionary normalization 
perplexity measure may biased depending way unknown words treated 
conventional perplexity compute adjusted perplexity proposed 
gram model probabilities estimated relative frequencies grams training corpus 
turing smoothing technique assign nonzero probability unseen grams known words quoted 
step followed renormalization conditional probabilities 
compute test likelihood conditional probability unknown word assigned fixed value instance appears left context history truncated probability corresponding gram obtained lower order 
instance cjb 
experiments carried filtered version atis database 
training corpus contains sentences words vocabulary words 
city names month names day names airline names hours numbers replaced specific word 
test corpus set sentences words occurences unknown words distinct 
report perplexity results gram multigram models 
multigrams initialization done globally optimal value results significantly different 
set fixed probability theta gamma half probability word occuring training corpus 
turing occurence number unseen ngrams depends approximately equal theta gamma value give perplexity grams perplexities pp pp multigrams trained em algorithm quantities multigrams trained viterbi algorithm 
table gives test set perplexities table training set perplexities table number units 
gram model pp multigram model em training pp pp multigram model viterbi training pp pp table perplexity values test corpus test set table optimal gram results obtained 
dictionary bigrams contains slightly units test set perplexity achieved 
equivalent performance obtained multigrams pp pp sequences having non zero probability 
higher order multigrams provide lower perplexities multigrams values pp pp reached sequences number bigrams 
results show multigrams model better language task lower number units conventional grams 
evaluation models training set table shows multigrams possess powerful generalization ability 
comparison perplexities pp pp test training sets indicates single best segmentation accounts corpus likelihood 
viterbi training em training large impact performances viterbi procedures leads sequences 
ratio adjusted perplexity reported tables perplexity pp grams multigrams invariably equal shows differences observed models owed way unknown words dealt 
give example segmentation obtained sentences test corpus multigrams trained em method 
segmentations show interesting correlations syntactic semantic groups 
depart city airlines depart city find cheapest way fare city city find flights leaving city city depart hour morning example multigram viterbi segmentation sentences test corpus 
perspectives experiments show multigram approach competitive alternative gram model terms language modeling 
task multigrams models outperform best gram model bigrams requiring units 
gram model pp multigram model em training pp pp multigram model viterbi training pp pp table perplexity values training corpus gram model multigram model em training multigram model viterbi training table number units dictionary models single framework instance estimating gram models sequences provided non supervised manner multigram approach 
interesting investigate application multigram approach issues instance natural language processing field search semantic equivalences word sequences view concept tagging acoustic phonetic level automatic definition speech synthesis recognition units 

jelinek 
self organized language modeling speech recognition readings speech recognition pp 

ed 
waibel lee 
morgan kaufmann publishers san mateo california 
kuhn niemann 
ergodic hidden markov models language modeling proc 
icassp vol 
pp 

bimbot pieraccini levin atal 
mod eles de equences horizon variable multigrams proc 
xx th tr france june 
dempster laird rubin 
maximum likelihood incomplete data em algorithm roy 
stat 
soc vol 
pp 


multi data collection spoken language corpus proc 
th darpa workshop speech natural language pp 


analysing simple language model general language models speech recognition computer speech language april vol 
pp 


population frequencies species estimation population parameters biometrika pp 

church gale 
comparison enhanced turing deleted estimation methods estimating probabilities english bigrams computer speech language january vol 
pp 

