connectionist natural language processing state art morten christiansen southern illinois university nick chater university warwick special issue connectionist models human language processing provides opportunity appraisal specific connectionist models status utility connectionist models language general 
provides background papers special issue 
development connectionist models language traced intellectual origins state current research 
key themes arise different areas connectionist psycholinguistics highlighted developments speech processing morphology sentence processing language production reading described 
argue connectionist psycholinguistics significant impact psychology language connectionist models important influence research 
connectionist modeling language processing highly controversial 
argued language processing phonology semantics understood connectionist terms argued aspects language fully captured connectionist methods 
controversy particularly heated connectionism just additional method studying language processing alternative traditional symbolic accounts 
degree connectionism complements existing approaches language matter debate see discussion papers part ii issue 
debate morten christiansen department psychology southern illinois university nick chater department psychology university warwick 
correspondence concerning article addressed morten christiansen department psychology southern illinois university il 
electronic mail may sent internet morten siu edu nick chater warwick ac uk 
connectionist natural language processing connectionist approaches language important test viability connectionist models cognition generally pinker prince 
special issue aims provide basis appraisal current state play 
part leading connectionists detail advances key areas language research 
marslen wilson statistical simulations exploring properties distributed connectionist model speech processing 
plunkett introduce new model english past tense plural morphology 
tanenhaus describe latest progress recurrent networks model sentence processing dynamic framework 
dell chang griffin report models language production including novel network model syntactic priming 
plaut presents new connectionist model sequential processing word reading 
part ii provides evaluation status prospects connectionist psycholinguistics range viewpoints 
seidenberg macdonald argue radical connectionist approach language acquisition processing smolensky argues integration connectionist symbolic approaches steedman assesses connectionist sentence processing point view symbolic cognitive science tradition 
aim set scene special issue providing brief historical theoretical background update current research specific topic areas outlined 
background sketch historical intellectual roots connectionism outline key debates concerning connectionist psycholinguistics 
consider central topics considered part speech processing morphology sentence processing language production reading 
topics illustrate range connectionist research language discussed depth papers part provide opportunity assess strengths weaknesses connectionist methods range setting stage general debate concerning validity connectionist methods part ii 
sum consider prospects connectionist research 
background perspective modern cognitive science tend see theories human information processing borrowing theories machine information processing 
symbolic processing general purpose digital computers successful method designing practical computers 
surprising cognitive science including study language processing aimed model mind symbol processor 
historically theories human thought inspired attempts build computers reverse 
mainstream computer science arises view cognition symbol processing 
tradition traced boole suggestion logic probability theory describe laws thought reasoning accordance laws conducted symbolic rules 
runs turing argument thought modeled symbolic operations tape turing machine von neumann design modern digital computer modern computer science artificial intelligence generative grammar symbolic cognitive science 
connectionist natural language processing connectionism known parallel distributed processing neural networks neuro computing different origin attempts design computers inspired brain 
mcculloch pitts provided early influential idealization neural function 
ashby minsky rosenblatt designed computational schemes related idealizations 
schemes interest systems learned experience designed 
self organizing learning machines plausible models learned cognitive abilities including aspects language processing chomsky challenged extent language learned 
period connectionist symbolic computation stood alternative paradigms modeling intelligence unclear prove successful 
gradually symbolic paradigm gained ground providing powerful models core domains language chomsky problem solving newell simon 
connectionism largely abandoned particularly view limited power current connectionist methods minsky papert 
limitations overcome rumelhart hinton williams re opening possibility connectionism constitutes alternative symbolic model thought 
neural inspiration connectionism mean practice 
coarse level brain consists large number simple processors neurons densely interconnected complex network 
neurons appear tackle information processing problems large numbers neurons operate simultaneously process information 
furthermore neurons appear communicate numerical values encoded firing rate symbolic messages neurons viewed mapping numerical inputs neurons numerical output transmitted neurons 
connectionist nets mimic properties consist large numbers simple processors known units nodes densely interconnected complex network operate simultaneously transmit numerical values output unit usually assumed function inputs 
connectionist nets realistic models brain see sejnowski level individual processing unit drastically knowingly falsifies features real neurons terms network structure typically bears relation brain architecture 
research direction seek increasing biological realism koch segev 
study aspects cognition language biological constraints available research concentrated modeling human behavior 
data taken cognitive psychology linguistics cognitive neuropsychology neuroscience 
connectionist nets compete head symbolic models language processing 
noted relative merits connectionist symbolic models language debated 
competition 
advocates symbolic models language processing assume symbolic processes implemented brain connectionists level implementation 
assume language processing described psychological level terms symbol processing implementational level terms term connectionism referring artificial neural networks model cognition coined pages journal feldman ballard 
connectionist natural language processing connectionism approximates 
right connectionist modeling start symbol processing models language processing implement connectionist nets 
advocates view fodor pylyshyn pinker prince typically assume implies symbolic modeling entirely autonomous connectionism symbolic theories set connectionism reverse 
chater argued view way influence symbolic connectionist theories symbolic accounts ruled precisely neurally implemented run real time 
connectionists field language processing radical agenda challenge reimplement symbolic approach 
discussing research key domains discussed special issue set recurring themes discussion value connectionist approach language learning 
connectionist nets typically learn experience fully prespecified designer 
contrast symbolic models language processing typically fully prespecified learn 
generalization 
aspects language simple learned rote 
ability generalize novel cases critical test connectionist models 
representation 
connectionist nets learn internal codes devised network appropriate task 
developing methods understanding codes important research problem 
internal codes may learned inputs outputs network generally code specified designer 
choice code crucial determining network performance 
codes relate standard symbolic representations language contentious 
rules vs exceptions 
aspects language exhibit quasi regularities regularities usually hold admit exceptions 
symbolic framework quasi regularities may captured symbolic rules associated explicit lists exceptions 
symbolic processing models incorporate distinction having separate mechanisms regular exceptional cases 
contrast connectionist nets may provide single mechanism learn general rules exceptions 
viability single route models major point controversy intrinsic connectionism 
separate mechanisms rules exceptions modeled connectionist terms pinker coltheart curtis atkins haller 
question networks really learn rules merely approximate rule behavior 
opinions differ important positive proposal may lead revision role rules linguistics rumelhart mcclelland smolensky cf 
smolensky issue fatal connectionist models language pinker prince 
general issues mind consider core domains focus discussion part special issue 
speech processing connectionist modeling speech processing initiated influential trace model mcclelland elman 
model interactive activation architecture consists sequence layers units 
units layer specific phonetic connectionist natural language processing features units second layer phonemes units third layer words 
layers inhibitory connections units stand incompatible states affairs 
example inhibitory connections word units candidate words compete 
similarly excitatory connections exist units stand mutually reinforcing states affairs 
addition standard interactive activation architecture shall encounter repeatedly trace includes feature deal temporal dimension speech copies entire network standing different points time utterance appropriate connections units copy 
models trace completely prespecified learn 
interactive character trace embodies controversial theoretical claim 
researchers assume speech processing involves successive computation increasingly levels representation assume feedback levels 
kind account known bottom realized connectionist networks shall see 
trace contrast allows information flow bottom top 
speech processing bottom interactive highly controversial debate reading literature perception fodor marr 
trace captures wide range empirical data apparent influence lexical context phoneme identification categorical aspects phoneme perception 
addition trace empirical predictions appear incompatible bottom model 
natural speech pronunciation phoneme altered surrounding phonemes known coarticulation 
speech processing system takes account phoneme recognition called compensation coarticulation 
appears provide way detecting lexical information feeds back top phoneme level 
elman mcclelland considered word boundaries word final influencing word initial christmas tapes 
lexical level feeds back phoneme level compensation occur relies lexically driven phoneme restoration identity experimental condition identity christmas obscured restored proceed normal 
trace prediction obvious bottom account speech perception prediction 
elman mcclelland conducted crucial experiment confirmed trace prediction 
argued bottom connectionist models despite appearances capture results 
norris trained simple recurrent network srn introduced elman see steedman issue description architecture input output consisting words word lexicon phoneme time 
input phonemes represented vectors phonetic features features intermediate values corresponding ambiguous phonemes 
output layer consisted unit phoneme 
net received input ambiguous word final phoneme ambiguous initial segments second word parallel observed percentages responses phoneme second word depended identity word elman mcclelland experiment 
explanation pattern results connectionist natural language processing top influence units representing words net words units case purely bottom 
norris small scale example suggestive question remains bottomup net trained natural speech show effect 
lindsey levy chater trained recurrent network close variant srn phonologically transcribed conversational english inputs outputs network represented terms phonetic features 
norris simulations lexical level representation processing strictly bottom 
phoneme restoration followed pattern elman mcclelland explained lexical influence 
bottom processes mimic lexical effects 

argue restoration occurs basis statistical regularities phonemic level lexical influence 
just happens words elman mcclelland experiment statistically regular phonemic level non words contrasted 
confirmed statistical analysis corpus natural speech net trained 
evidence ability bottom models accommodate apparently lexical effects speech processing provided hare marslen wilson 
trained srn version 
model map systematically altered featural representation speech canonical representation speech network showed evidence lexical abstraction tolerating systematic phonetic variation random change 
marslen wilson added new dimension debate presenting srn network sequentially phonetic input word mapped corresponding distributed representations phonological surface form semantics 
ability network model integration partial cues phonetic identity time course lexical access suggested distributed models may provide better explanation speech perception localist counterparts trace 
important challenge distributed models accommodate simultaneous activation multiple lexical candidates necessitated temporal ambiguity speech input captain captive 
lexical candidates distributed model results semantic blend vector 
statistical analyses vector spaces marslen wilson issue investigate properties semantic blends apply results explain empirical speech perception data 
interactive vs bottom debate illustrates connectionist models led unexpected theoretical predictions promoted empirical research seeking provide definitive evidence interactive samuel bottom approach pitt mcqueen 
morphology connectionist models created controversy rumelhart mcclelland model learning english past tense 
english past tense quasi regular mapping traditionally assumed require symbolic routes 
dual route account appears backed shaped pattern acquisition children appear initially correct fail due connectionist natural language processing regularization reestablish correctly 
traditionally explained assuming child initially uses memorization route rule route correct balance established 
rumelhart mcclelland argued pattern explained single processing route 
trained single layer network map roots past tense forms words perceptron learning algorithm rosenblatt 
representation encodes triples consecutive elements phoneme string details important 
trained verbs exposed verbs net approximated shaped learning curve 
training stage network performed perfectly verbs getting correct 
early second stage tended regularize irregular verbs getting correct 
training network approached perfect performance verbs 
model faced considerable criticism 
representation attacked pinker prince models switched styles representation 
second fundamental shaped learning appears artifact suddenly increasing total number verbs discontinuity developmental justification pinker prince 
plunkett shown shaped learning net trained fixed training set 
feedforward network hidden unit layer trained vocabulary artificial verb stems past tense forms patterned regularities english past tense 
constant training vocabulary obtained classical shaped learning observed various selective micro shaped developmental patterns children behavior 
example net able simulate number phonological form verb stem past tense form sleep keep kept captured rumelhart mcclelland model 
subsequently plunkett obtained similar results incremental realistic training regime 
initial training verbs vocabulary gradually increased verbs 
incremental training regime significantly improved net performance 
suggested intriguing theoretical claim critical mass verbs needed change rote learning memorization system building rule generalization behavior occur 
plunkett issue find similar critical mass effect model english noun verb morphology 
analyzed developmental trajectory feedforward network trained produce plural form nouns past tense form verbs 
model exhibited patterns shaped development nouns verbs noun inflections acquired earlier verb inflections demonstrated strong tendency regularize deverbal nouns verbs 
pinker argued connectionist models implicitly depend artifact idiosyncratic frequency statistics english 
focus default inflection words ed english regular verbs 
default inflection word assumed independent phonological shape occur word marked irregular 
pinker argue connectionist models generalize frequency surface similarity 
regular english verbs high type frequency relatively low token frequency allowing network construct broadly connectionist natural language processing defined default category 
contrast low type frequency high token frequency permits memorization irregular past tenses terms number narrow phonological subcategories alternation sing sang ring rang alternation grow grew blow 
pinker show default generalization rumelhart mcclelland model depends similar frequency distribution model training set 
furthermore contend connectionist model accommodate default generalization class words low type frequency low token frequency default inflection plural nouns german see marcus marcus pinker 
true lack cross linguistic validity pose serious problems connectionist models morphology 
connectionist addressed minority defaults 
hare elman trained multi layer feedforward network additional cleanup units see plaut issue explanation map phonological representations stems past tenses set verbs representative early old english 
training set consisted classes irregular verbs plus class regular verbs class containing number words 
words default generalization ed formed minority 
net learned appropriate default behavior faced low frequency default class 
appears generalization neural networks may strictly dependent similarity known items 
hare results show non default irregular classes sufficient degree internal structure default generalization may promoted lack similarity known items 
hahn press provide problems dual route approach 
compared connectionist implementations rule memorization routes single memorization route performance consistently superior rule route comprehensive sample german nouns 
rule frequency independent default generalization may pressing problem connectionist models 
marcus 
claim 
data concerning german noun inflection combination additional data arabic showed default generalization sensitive type frequency entirely rule 
pattern may fit better kind default generalization connectionist nets rigid defaults symbolic models 
issue humans employ single connectionist mechanism morphological processing far settled 
connectionist models fit wide range developmental linguistic data 
opponents connectionist models typically concede connectionist mechanism may explain complex patterns irregular cases 
controversial question single connectionist mechanism simultaneously deal regular irregular cases regular cases generated distinct route involving symbolic rules 
involve connectionist modeling cross linguistic phenomena detailed fits developmental data 
connectionist natural language processing sentence processing sentence processing provides considerable challenge connectionist research 
view difficulty problem early hand wired symbolic structures network architecture mcclelland kawamoto smolensky legendre small cottrell shastri 
connectionist symbolic systems interesting computational properties may illuminating regarding viability particular style symbolic model distributed computation chater 
connectionist research larger goal provide alternatives symbolic accounts syntactic processing 
classes models potentially provide alternatives learn process language experience implementing prespecified set symbolic rules 
ambitious class hanson stolcke learns parse tagged sentences 
nets trained sentences associated particular grammatical structure task assign appropriate grammatical structures novel sentences 
linguistic structure learned observation built training items 
models related statistical approaches language learning stochastic context free grammars charniak probabilities grammar rules prespecified context free grammar learned corpus parsed sentences 
second ambitious class models includes tanenhaus issue attempts harder task learning syntactic structure sequences words 
influential approach kind due elman trained srn predict input word sentences generated small contextfree grammar 
grammar involved subject noun verb agreement variations verb argument structure intransitive transitive optionally transitive subject object relative clauses allowing multiple embeddings complex long distance dependencies simulations suggested srn acquire grammatical regularities underlying grammar 
addition elman srn showed similarities human behavior center embedded structures 
christiansen extended complex grammars involving prenominal prepositional modifications noun phrases noun phrase conjunctions sentential complements addition grammatical features elman 
grammars incorporated cross dependencies weakly context sensitive structure dutch swiss german 
christiansen srns learn complex grammars exhibit qualitative processing difficulties humans similar constructions 
nets showed sophisticated generalization abilities overriding local word occurrence statistics complying structural constraints constituent level christiansen chater 
current models syntax typically toy fragments grammar small vocabularies 
aside raising question viability scaling difficult provide detailed fits empirical data 
attempts fitting existing data deriving new empirical predictions models 
example tanenhaus srn dynamic parsing model fits reading time data concerning interaction lexical structural con connectionist natural language processing straints resolution temporary syntactic ambiguities garden path effects sentence comprehension 
macdonald christiansen provide srn simulations reading time data concerning differential processing singly center embedded subject object relative clauses poor 
christiansen christiansen chater press describes srn trained recursive sentence structures fits grammaticality ratings data behavioral experiments 
derives novel predictions processing sentences involving multiple prenominal multiple prepositional phrase modifications nouns doubly center embedded object relative clauses subsequently empirically confirmed christiansen macdonald 
connectionist models syntactic processing early stage development 
tanenhaus issue advance extending dynamic parsing model account empirical findings concerning semantic effects sentence processing 
propose new approach distinction syntactic semantic incongruity 
research required decide promising initial results scaled deal complexities real language purely connectionist approach fundamental limitations connectionism succeed providing symbolic methods see papers part ii issue discussion 
language production connectionist research psychology language general relatively little language production 
important steps taken notably dell colleagues 
dell spreading activation model important early production model 
model sentence production model high level morphological syntactic semantic processing implemented 
implemented part model concerned moving choice word spoken finding phonological encoding word 
dell interactive activation network trace model described 
net layers corresponding morphemes lexical nodes syllables consonant clusters phonemes phonetic features 
approximation nodes connected bi directionally layers lateral connections layers 
processing trace model begins bottom speech input dell model begins top activation lexical node 
activation spreads network upwards feedback connections 
fixed time determined speaking rate nodes highest activations selected onset vowel coda slots 
model accounted variety common speech errors substitutions dog log deletions dog og additions dog 
errors occur incorrect node selected active correct node activated lexical node 
may occur due feedback connections activating nodes directly corresponding initial word node due general spread activation differences resting levels 
alternatively words activated product internal noise may interfere processing network 
model quantitative predictions concerning retrieval phonological forms production connectionist natural language processing confirmed experimentally dell 
model extended simulate aphasia dell schwartz martin gagnon martin dell schwartz see dell issue 
dell model considerable impact subsequent accounts speech production connectionist harley symbolic levelt 
model limitations obviously learn 
psychologically unattractive lexical information language specific innate 
inability learn practically difficult scale model connection hand coded 
problem addressed srn model dell learned map lexical items sequences phonological segments 
srn small additional modification current output copied back additional input network jordan current state hidden units 
dell account speech error data having build syllabic frames phonological rules network see dell issue discussion cf 
dell burger 
account syntactic priming implicit learning dell 
issue seen extension 
model trained generate words input message encoding proposition blocks semantic features child male event roles agent patient action descriptions giving walking 
dell simulated syntactic priming allowing learning occur testing 
contrast connectionist models learning disabled testing 
ongoing learning created sufficiently robust short term changes weight space ensure priming unrelated sentences 
current model focuses grammatical encoding couched broader theoretical framework provides step integrated connectionist account sentence comprehension production 
connectionist models language production modeled empirical data normal impaired performance contributed fundamental theoretical debates generated new experimental 
connectionist language production models important role shaping research speech production 
reading psychological processes engaged reading extremely complex varied ranging early visual processing printed word syntactic semantic pragmatic analysis integration general knowledge 
connectionist models concentrated simple aspects reading recognizing letters words printed text word naming mapping visually letter strings sequences sounds 
focus models processes 
earliest connectionist models mcclelland rumelhart interactive activation model visual word recognition see rumelhart mcclelland 
network layers units standing visual features letters letters particular positions word words uses principles trace described need temporal dimension entire word 
connectionist natural language processing word recognition occurs follows 
visual stimulus activates probabilistic fashion visual feature units layer 
features activated send activation excitatory inhibitory connections letter units turn send activation word units 
words compete inhibitory connections reinforce component letters excitatory feedback letter level word letter inhibition 
interactive process occurs bottom information visual input combined top information flow word units 
process involves cascade overlapping interacting processes letter word recognition occur sequentially overlap mutually constraining 
model accounted variety phenomena mainly concerning context effects letter perception 
example captures facilitation letter recognition context word comparison recognition single letters letters embedded random letter strings 
occurs partially active words provide top confirmation letter identity conspire enhance recognition 
similarly model explains degraded letters disambiguated letter context words recognized component letters visually ambiguous range effects 
connectionist models reading focussed word recognition word naming involves relating written word forms pronunciations 
model sejnowski rosenberg nettalk learns read aloud text 
nettalk layer feedforward net input units representing window consecutive letters text output units representing network suggested pronunciation middle letter 
network written text shifting moving input window text letter letter central letter pronounced moves onwards letter time 
english orthography course mapping letters phonemes 
nettalk relies ad hoc strategy deal clusters letters realized single phoneme th sh letter chosen mapped speech sound mapped speech sound 
nettalk learns exposure text associated correct pronunciation back propagation rumelhart hinton williams 
pronunciation largely comprehensible fed speech synthesizer 
nettalk intended demonstration power neural networks 
detailed psychological model word naming provided seidenberg mcclelland 
feedforward network single hidden layer represented entire written form word input entire phonological form output 
net trained monosyllabic english words dealing unrestricted text nettalk 
inputs outputs type representation proved controversial context past tense models discussed 
net performance captured wide range experimental data reasonable assumption net error mapped response time experimental paradigms 
past tense debate controversial claim concerning reading model uses single route handle quasi regular mapping 
contrasts connectionist natural language processing standard view reading assumes non semantic routes reading phonological route applies rules pronunciation lexical route simply list words pronunciations 
regular words read route read lexical route non words phonological route known lexical route 
seidenberg mcclelland claim shown dual route view necessarily correct single route pronounce irregular words non words 
provided fully explicit computational model previous dual route theorists merely sketched reading system level boxes arrows 
number criticisms leveled seidenberg mcclelland account 
besner mccann argued non word reading poor compared people see seidenberg mcclelland 
coltheart 
argued better performance non word reading achieved symbolic learning methods word set seidenberg mcclelland 
limitation seidenberg mcclelland model log frequency compression training 
plaut mcclelland seidenberg patterson shown feedforward network actual word frequencies learning process achieve human level performance word non word pronunciation 
past tense debate representation criticized leading alternative representational schemes 
example plaut mcclelland localist code exploits regularities english orthography phonology avoid completely position specific representation 
model learns read non words building lot knowledge representation having network learn knowledge 
plausibly assume cf 
plaut knowledge acquired prior reading acquisition children normally know pronounce words talk start learning read 
idea explored harm altmann seidenberg showed network phonology help learning mapping orthography phonology 
problems representational scheme works monosyllabic words 
hand obtains high nonword reading performance words length 
gives attempt provide single route model reading aims model phonological route variant nettalk orthographic phonological forms pre aligned designer 
having single output pattern network output patterns corresponding possible alignments phonology orthography 
possibilities considered nearest network actual output taken correct output adjust weights 
approach nettalk uses input window moves gradually text producing phoneme time 
simple phoneme specific code order phonemes implicit order network produces 
difficulty seidenberg mcclelland model apparent double dissociation phonological lexical reading acquired surface read exception words non words phonological pronounce non words irregular words 
standard certain inference connectionist natural language processing double dissociation modularity function suggests normal non word exception word reading distinct systems leading dual route model morton patterson 
acquired dyslexia simulated damaging seidenberg mcclelland network various ways removing connections units 
results damage neuropsychological interest patterson seidenberg mcclelland produce double dissociation 
analogue surface dyslexia preserved analogue phonological dyslexia observed 
furthermore chater explored range rule exception tasks feedforward networks trained back propagation concluded double rules exceptions occur single route models appears occur small scale networks 
large networks dissociation rules damaged exceptions preserved occur 
remains possible realistic single route model reading incorporating factors claimed important connectionist accounts reading word frequency phonological consistency effects cf 
plaut give rise relevant double dissociation 
chater results indicate modeling phonological dyslexia potentially major challenge single route connectionist model reading 
single dual route theorists argue non word exception word reading carried single system agree additional semantic route pronunciation retrieved semantic code 
pathway evidenced deep semantic errors reading aloud reading word aloud 
plaut 
argue route plays role normal reading 
particular suggest division labor emerges phonological semantic pathway reading acquisition roughly phonological pathway moves specialization regular consistent orthography phonology mappings expense exception words read semantic pathway 
putative effect pathway simulated plaut extra input phoneme units feedforward network trained map orthography phonology 
strength external input frequency dependent gradually increases learning 
result net comes rely extra input 
eliminated simulated lesion semantic pathway net loses ability read exception words retains reading regular words non words 
plaut provides accurate account surface dyslexia patterson 

conversely selective damage phonological pathway phonology produce pattern deficit resembling phonological dyslexia reasonable word reading impaired non word reading hypothesis tested directly plaut plaut account surface dyslexia challenged existence patients considerable semantic impairments demonstrate near normal reading exception words 
plaut presents simulations results suggesting variations surface dyslexia may stem pre morbid individual differences division labor phonological semantic pathways 
particular phonological pathway highly developed prior pattern semantic impairment exception word reading observed model 
harm seidenberg press appropriate double dissociation context developmental acquired dyslexia 
connectionist natural language processing connectionist models reading criticized capturing certain effects orthographic length naming latencies single word reading 
plaut issue takes challenge presenting srn model sequential processing reading 
output previous connectionist reading models nettalk models generated static phonological representations entire words new model words phoneme phoneme 
ability input unable pronounce part word 
model performs words nonwords provides reasonably fit empirical data orthographic length effects 
results encouraging suggest sequential reading model may provide step connectionist account temporal aspects reading 
seen connectionist accounts provided fit data normal impaired reading points controversy remain 
connectionist models contributed re evaluation core theoretical issues reading interactive purely bottom rules exceptions dealt separately single mechanism 
prospects connectionist natural language processing current connectionist models exemplified part special issue progress involve drastic simplifications respect real natural language 
connectionist models scaled provide realistic models human language processing 
part ii prospects provides different perspectives connectionist models may develop 
seidenberg macdonald issue argue connectionist models able replace current currently dominant symbolic models language structure language processing cognitive science language 
suggest connectionist models exemplify probabilistic rigid view language requires foundations linguistics cognitive science language generally radically 
smolensky issue contrast argues current connectionist models handle full complexity linguistic structure language processing 
suggests progress requires match insights generative grammar approach linguistics computational properties connectionist systems constraint satisfaction 
exemplifies approach grammar formalisms inspired connectionist systems harmonic grammar optimality theory 
steedman issue argues claims connectionist systems take territory symbolic views language syntax semantics premature 
suggests connectionist symbolic approaches language language processing viewed complementary currently dealing different aspects language processing 
steedman believes connectionist systems may provide underlying architecture high level symbolic processing occurs 
outcome important debates note connectionist psycholinguistics important influence cognitive science language 
connectionist models raised level theoretical debate areas challenging theorists viewpoints provide computationally explicit accounts 
provided basis informed discussions processing architecture single vs dual route mechanisms interactive vs bottom processing 
second connectionist natural language processing learning methods connectionist models interest computational models language learning bates elman 
chomsky argued innate universal aspects language vast amount language specific information child acquires learned 
connectionist models may account learning occurs 
furthermore connectionist models provide test bed learnability linguistic properties previously assumed innate 
dependence connectionist models statistical properties input contributed interest statistical factors language learning processing macwhinney taraban mcdonald chater finch press 
connectionism considerable influence psychology language 
final extent influence depends degree practical connectionist models developed extended deal complex aspects language processing psychologically realistic way 
realistic connectionist models language processing provided possibility radical rethinking just nature language processing structure language may required 
ultimate description language resides structure complex networks approximated rules grammar 
connectionist learning methods scale connectionism succeed reimplementing standard symbolic models 
connectionist psycholinguistics important implications theory language processing language structure traditional psychological linguistic assumptions 
hope papers special issue contribute determining bring 
ashby 

design brain 
new york wiley 
bates elman 

connectionism study change 
johnson ed brain development cognition pp 

cambridge ma basic blackwell 
besner mccann 

connection connectionism data words necessary 
psychological review 
boole 

laws thought 
london macmillan 


modelling reading spelling past tense learning artificial neural networks 
brain language 
chater 

connectionist modelling implications neuropsychology language cognitive processes 


regular morphology lexicon 
language cognitive processes 
charniak 

statistical language learning 
cambridge ma mit press 
connectionist natural language processing chater 

autonomy implementation cognitive architecture reply fodor pylyshyn 
cognition 
chomsky 

aspects theory syntax 
cambridge ma mit press 
chomsky 

knowledge language 
new york praeger 
christiansen 

infinite languages finite minds connectionism learning linguistic structure 
unpublished doctoral dissertation university edinburgh 
christiansen 

intrinsic constraints processing recursive sentence structure 
manuscript preparation 
christiansen chater 

generalization connectionist language learning 
mind language 
christiansen chater 
press 
connectionist model recursion human linguistic performance 
cognitive science 
christiansen macdonald 

processing recursive sentence structure testing predictions connectionist model 
manuscript preparation marcus 

regular irregular inflection acquisition german noun plurals 
cognition 
coltheart curtis atkins haller 

models reading aloud dual route parallel distributed processing approaches 
psychological review 
dell 

spreading activation theory retrieval language production 
psychological review 
dell 

retrieval phonological forms production tests predictions connectionist model 
journal memory language 
dell burger 

language production serial order functional analysis model 
psychological review 
dell chang griffin 
issue 
connectionist models language production lexical access grammatical encoding 
cognitive science 
dell 

structure content language production theory frame constraints phonological speech errors 
cognitive science 
dell schwartz martin gagnon 

lexical access aphasic speakers 
psychological review 
elman 

finding structure time 
cognitive science 
elman 

distributed representation simple recurrent networks grammatical structure 
machine learning 
connectionist natural language processing elman 

learning development neural networks importance starting small 
cognition 
elman mcclelland 

cognitive penetration mechanisms perception compensation coarticulation lexically restored phonemes 
journal memory language 


context free parsing connectionist networks tech 
rep 
tr rochester ny university rochester department computer science 
feldman ballard 

connectionist models properties 
cognitive science 
fodor 

modularity mind 
cambridge ma mit press 
fodor pylyshyn 

connectionism cognitive architecture critical analysis 
cognition 
hare marslen wilson 

connectionist model phonological representation speech perception 
cognitive science 
marslen wilson 

integrating form meaning distributed model speech perception 
language cognitive processes 
marslen wilson 
issue 
ambiguity competition blending speech perception 
cognitive science 
hahn 
press 
german inflection single dual route 
cognitive psychology 
hanson 

connectionist network learns natural language grammar exposure natural language sentences 
proceedings eighth annual meeting cognitive science society pp 

hillsdale nj lawrence erlbaum associates 
hare elman 

learning morphological change 
cognition 
hare elman 

default generalization connectionist networks 
language cognitive processes 
harley 

phonological activation semantic competitors lexical access speech production 
language cognitive processes 
harm altmann seidenberg 

connectionist networks examine role prior constraints human learning 
proceedings sixteenth annual conference cognitive science society pp 

hillsdale nj lawrence erlbaum associates 
harm seidenberg 
press 
phonology reading acquisition dyslexia insights connectionist models 
psychological review 
connectionist natural language processing 

vital connectionist parser 
proceedings tenth annual conference cognitive science society 
hillsdale nj lawrence erlbaum associates 
jordan 

serial order parallel distributed approach tech 
rep 
san diego university california san diego institute cognitive science 
koch segev 
eds 
methods neuronal modeling synapses networks 
cambridge ma mit press 
levelt 

speaking intention articulation 
cambridge ma mit press 
macdonald christiansen 

individual differences working memory reply just carpenter waters caplan 
manuscript submitted publication 
macwhinney taraban mcdonald 

language learning cues rules 
journal memory language 
marcus pinker 

german inflection exception proves rule 
cognitive psychology 
marr 

vision 
san francisco ca freeman 
martin dell schwartz 

origins deep testing consequence decay impairment interactive spreading activation model lexical retrieval 
brain language 
mcclelland elman 

interactive processes speech perception trace model 
mcclelland rumelhart 
eds parallel distributed processing volume pp 

cambridge ma mit press 
mcclelland kawamoto 

mechanisms sentence processing 
mcclelland rumelhart eds parallel distributed processing volume pp 

cambridge ma mit press 
mcclelland rumelhart 

interactive activation model context effects letter perception part 
account basic findings 
psychological review 
mcculloch pitts 

logical calculus ideas nervous activity 
bulletin mathematical biophysics 
minsky 

neural nets brain model problem 
unpublished doctoral dissertation princeton university nj 
minsky papert 

perceptrons 
cambridge ma mit press 
connectionist natural language processing morton patterson 

new attempt interpretation attempt new interpretation 
coltheart patterson marshall eds deep dyslexia pp 

london routledge 
smolensky legendre 

distributed representation parallel distributed processing recursive structures 
proceedings fifteenth annual meeting cognitive science society pp 

hillsdale nj lawrence erlbaum associates 
newell simon 

human problem solving 
englewood cliffs nj prentice hall 
norris 

bottom connectionist models interaction 
altmann eds cognitive models speech processing second meeting pp 

hillsdale nj lawrence erlbaum associates 
patterson seidenberg mcclelland 

connections disconnections acquired dyslexia computational model reading processes 
morris ed parallel distributed processing implications psychology neuroscience pp 

oxford oxford university press 
pinker 

rules language 
science 
pinker prince 

language connectionism analysis parallel distributed processing model language acquisition 
cognition 
pitt mcqueen 

compensation coarticulation mediated lexicon 
journal memory language 
plaut 

structure function lexical system insights distributed models word reading lexical decision 
language cognitive processes 
plaut 
issue 
connectionist approach word reading acquired dyslexia extension sequential processing 
cognitive science 
plaut mcclelland 

generalization componential attractors word non word reading attractor network 
proceedings fifteenth annual conference cognitive science society pp 

hillsdale nj lawrence erlbaum associates 
plaut mcclelland seidenberg patterson 

understanding normal impaired word reading computational principles quasi regular domains 
psychological review 
plunkett 
issue 
connectionist model english past tense plural morphology 
cognitive science 
plunkett 

shaped learning frequency effects multilayered perceptron implications child language acquisition 
cognition 
connectionist natural language processing plunkett 

rote learning system building 
cognition 
pinker 

similarity rule generalizations inflectional morphology 
language cognitive processes 
chater finch 
press 
potential contribution distributional information early syntactic category acquisition 
cognitive science 
rosenblatt 

principles neurodynamics 
new york spartan books 
rumelhart hinton williams 

learning internal representations error propagation 
mcclelland rumelhart 
eds parallel distributed processing volume pp 

cambridge ma mit press 
rumelhart mcclelland 

interactive activation model context effects letter perception part 
contextual enhancement effects tests enhancements model 
psychological review 
rumelhart mcclelland 

learning past tenses english verbs 
mcclelland rumelhart 
eds parallel distributed processing volume pp 

cambridge ma mit press 
samuel 

lexical activation produces potent phonemic percepts 
cognitive psychology 
seidenberg macdonald 
issue 
probabilistic constraints approach language acquisition processing 
cognitive science 
seidenberg mcclelland 

distributed developmental model word recognition naming 
psychological review 
seidenberg mcclelland 

words lexicon reply besner 

psychological review 
sejnowski 

open questions computation cerebral cortex 
mcclelland rumelhart 
eds parallel distributed processing volume pp 

cambridge ma mit press 
sejnowski rosenberg 

parallel networks learn pronounce english text 
complex systems 
lindsey levy chater 

phonologically motivated input representation modelling auditory word perception continuous speech 
proceedings fourteenth annual conference cognitive science society pp 

hillsdale nj lawrence erlbaum associates 
small cottrell shastri 

connectionist parsing 
proceedings national conference artificial intelligence 
pittsburgh pa connectionist natural language processing smolensky 

proper treatment connectionism 
behavioral brain sciences 
smolensky 
issue 
grammar connectionist approaches language 
cognitive science 
steedman 
issue 
connectionist sentence processing perspective 
cognitive science 
stolcke 

syntactic category formation vector space grammars 
proceedings thirteenth annual conference cognitive science society pp 

hillsdale nj lawrence erlbaum associates 
tanenhaus 

parsing dynamical system attractor account interaction lexical structural constraints sentence processing 
language cognitive processes 
tanenhaus 
issue 
dynamical models sentence processing 
cognitive science 
turing 

computable numbers application entscheidungsproblem 
proceedings london mathematical society series 
