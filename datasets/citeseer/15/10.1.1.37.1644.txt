evaluating high level parallel programming support irregular applications icc andrew chien julian ganguly vijay karamcheti zhang department computer science university illinois urbana champaign springfield avenue urbana il authors contacted mail concert cs uiuc edu phone fax object oriented techniques aids managing complexity enhancing reuse improving readability irregular parallel applications 
performance major reason employing parallelism programmability high performance delivered 
suite challenging irregular applications mature illinois concert system high level concurrent object oriented programming model backed aggressive implementation evaluate programming efforts required achieve high performance 
applications achieve performance comparable best reported low level programming means large scale parallel systems 
general high level concurrent object oriented programming model supported aggressive implementation techniques eliminate programmer management concerns procedure computation granularity namespace management low level concurrency management 
study indicates concerns fully automated applications 
decoupling concerns managing remaining fundamental concerns data locality load balance easier 
cases data locality load balance complex algorithm pointer data structures automatically managed compiler runtime general programmer intervention required 
cases detailed control required specifically explicit task priority data consistency task placement 
system integrates expression information cleanly programming interface 
small changes sequential code required express concurrency performance optimizations source code lines changed cases 
supporting sequential parallel performance single code base 
keywords object oriented programming high level parallel models overview object oriented techniques aids managing complexity enhancing reuse improving readability parallel applications 
numerous variants explored great volume technical papers published 
concurrent object systems appear particularly promising irregular applications modularity encapsulation help manage complex data parallelism structures 
systematically evaluate high level concurrent object system programmability achieving high performance irregular applications 
significant bodies research exist implementing irregular parallel applications efficiently concurrent object oriented programming know studies addresses 
wealth research programming irregular parallel applications approaches provide single programming interface supporting dimensions irregularity data control concurrency 
research efforts largely focused building special libraries provide runtime support dimensions irregularity 
example inspector executor approach uses library support iterated irregular communication finite element simulations 
library provides set tools build irregular applications uniform high level interface 
libraries support particular classes algorithms sparse matrices adaptive meshes 
concurrent object oriented programming systems nearly chosen ignore support fine grained parallelism 
greatly complicates task programming irregular applications forcing programmers map irregular concurrency grains large achieve efficiency 
example systems pc cc languages combined hpc mentat basis legion project heavyweight threads 
systems explored fine grained concurrency abcl mpc illinois concert system study systematic evaluation programming effort significant range irregular applications 
application study done context illinois concert system high performance compiler runtime parallel computers vehicle extensive research compiler optimization runtime techniques past years 
system contains known optimizations concert system contains wide range aggressive optimizations demonstrate high performance absolute terms wide range applications 
effect concert system automatically addresses concerns programmers explicitly manage lower level programming models explicit threads message passing 
rich suite irregular parallel applications mature concert system evaluate programming effort irregular parallel computations graphics polygon overlay hierarchical radiosity body codes barnes hut fast multipole method adaptive mesh codes structured adaptive mesh refinement symbolic computation grobner basis phylogeny 
application delivered performance close best possible 
detailed descriptions parallelization optimization steps provide insight concerns programming system model implementation succeeded automating remained 
addition descriptions provide insight key aspects parallelizing application 
experience indicates high level concurrent object oriented programming model sophisticated eliminate concerns easing programming irregular parallel applications 
despite fact underlying machines distributed memories programmers freed explicit namespace management computation granularity procedure data low level memory race management 
second cases programmers needed express application specific data locality load balance orthogonal framework functionality data placement eased effort allowing radical changes data layout modest code changes 
decomposition computation data layout similar conceptually hpf decompositions applications irregular 
quantitative metrics moderately indicative programming effort provide tangible point comparison 
applications modest code changes lines required express careful data locality load balance needed achieve top performance 
course diversity applications studied required detailed control data locality load balance 
applications employing heuristic methods task ordering control required 
applications explicit thread placement complementary data placement owner computes convenient perspective 
radiosity grobner phylogeny applications benefited annotations relaxed data consistency 
remainder organized follows section details key elements programming interface implementation irregular application suite 
section presents methodology section describe algorithmic data structure application summarize efforts required achieve high parallel performance 
quantitative performance data illustration performance impact type program transformation shown section 
conclude discussion summary sections 
background evaluating programming approach goal identify concerns addressed programmer achieve performance concerns managed 
experimental evaluation done context actual programming system attendant strengths shortcomings 
experiments involve community programmers particular application programs 
describe turn basic programming interface interfaces performance optimization compiler runtime technology illinois concert system 
conclude discussing application programs study workload 
basic programming interface programming interface basic object oriented programming system augmented simple extensions concurrency 
programmers take sequential programs move parallel environment little change 
concurrency added appropriate 
basic model providing single namespace scalars objects single inheritance 
details programming model illinois concert icc see 
simple extensions concurrency include annotating standard blocks compound statements loops conc keyword 
conc block defines partial order statements allowing concurrency preserving local data dependences 
conc applied existing blocks 
conc indicates non binding concurrency allowing programmers specify concurrency depend underlying system create threads required 
simple semantics object concurrency provides synchronization method invocations object execute exclusive access 
state object sequentially consistent operations multiple objects may 
raises level consistency single memory locations user defined objects 
performance optimization interfaces data locality load balance managed data placement colocation data consistency thread placement interfaces 
data locality essential high performance processor load balance critical achieving high application performance 
data placement colocation managed collections distributed arrays objects user specify standard layouts block cyclic full custom distributions 
individual objects colocation control expressed object allocation time 
combining mechanisms placement individual object system controlled 
additional interfaces support control data consistency thread placement task ordering 
data placement interfaces useful applications additional interfaces required applications 
data consistency interface allows concert system relaxed consistency models higher performance 
interface necessary applications expressed local annotation blocks 
shown code fragment local annotation specifies set objects need localized provides information access semantics 
example annotation declares access objects obj obj block read allowing system cache efficient protocols 
function obj obj 
local obj read obj read obj obj 
func obj obj 
thread placement interfaces provide support controlling data locality load balance alternative perspective threads objects 
thread placement specified annotating thread creation target keyword takes integer argument 
code fragment thread corresponding thread function executed random processor 
thread function arg arg 
target rand 
task ordering useful applications search prioritize promising tasks higher efficiency 
task ordering specified annotating thread creation site priority keyword takes task priority argument 
additionally interface permits programmer specify custom scheduler thread execution 
concert runtime system implements library schedulers sender initiated scheduling task stealing priority task stealing 
concert runtime defines interface user extend library attaching custom schedulers 
code fragment thread corresponding thread function executes priority arg managed global priority scheduler custom scheduler 
thread function arg arg 
priority arg scheduler global priority 
illinois concert system application study done context illinois concert system high performance compiler runtime parallel computers vehicle extensive research compiler optimization runtime techniques past years 
system contains known optimizations concert system contains wide range demonstrate high performance absolute terms wide range applications 
effect concert system automatically addresses concerns programmers manage lower level programming models procedure granularity virtual function call overhead aggressive interprocedural optimization cloning chooses efficient granularities essentially eliminates virtual function call overhead 
compound object structuring object data inlining interprocedural analysis allows concert compiler reorganize data structures inline allocation reducing storage management cost eliminating associated pointer dereferences 
resulting code efficiency surpass hand tuned manual inline optimization 
thread granularity speculative stack heap execution reduces cost allowing threads execute procedure calls stack lazily creating heap threads needed 
greater processor efficiency thread granularity demonstrated 
distributed name management concert provides global namespace distributed memory shared memory systems 
fast name translation data movement microseconds enable efficient manipulation remote names data 
initial data placement data consistency sharing fast flexible software object caching system eliminate importance initial data placement applications 
compiler analyses provide coarse sharing information cases allows automatic management data consistency prefetching 
naturally cases explicit programmer management required initial data placement data consistency 
applications concurrent object oriented programming techniques proposed manage complex parallel applications collected suite challenging irregular programs see table irregular data structure parallel control structure concurrency 
short applications complex program sequential systems sophisticated pointer data structures efficient algorithms 
applications description computer graphics computes overlay polygon maps 
barnes hut computational cosmology computes interaction system particles barnes hut hierarchical body method :10.1.1.104.6469
fmm computational cosmology computes interaction system particles fast multipole method :10.1.1.104.6469
samr computational fluid dynamics solves hyperbolic partial differential equations structured adaptive mesh refinement 
grobner computational algebra computes grobner basis 
phylogeny evolutionary history determines evolutionary history set species 
radiosity computer graphics computes graph illumination hierarchical radiosity method :10.1.1.104.6469
table irregular applications suite complexity programming applications implies widespread scalable parallel machines 
extraordinary programming low level interfaces previously required achieve performance :10.1.1.30.5842
knowledge study study evaluates high level general purpose programming interface challenging suite parallel applications 
concert achieved performance parity best known low level coded results available applications 
application summaries constitute entirety programmer optimizations required obtain high performance 
experimental methodology achieve efficient parallel execution irregular application concerns addressed programmer 
approach adopt study evaluate high level programming approaches identifying concerns context individual applications examining extent various facets programming system help manage concerns 
provide common context discussion describe general concerns 
transforming sequential algorithm efficient parallel expression requires programmer address high level concerns concurrency synchronization specification data locality load balance 
concurrency synchronization specification parallelism structure application expressed concurrency synchronization specifications 
concurrency specifications relax total order imposed sequential algorithm producing decomposition program control structures tasks 
synchronization specifications ensure correct program execution specify task dependencies consistency conditions shared objects 
programmer may restructure program describe specifications support provided underlying programming system determines amount restructuring 
high level features concert system considerably simplify task describing concurrency synchronization specifications 
non binding concurrency specification incurs loss efficiency due aggressive implementation object level concurrency control permit programmer describe specifications natural style allowing parallel program retain core structure sequential program 
data locality load balance second high level concern programmer address ensuring parallel expression executes high performance 
data locality essential high performance processor load balance critical achieving high application performance 
goal data locality ensure parallel tasks incur reduced communication access data objects 
achieving goal requires specifications data distribution mapping objects processors data alignment colocation objects respect tasks objects 
dynamic nature irregular applications data locality may require dynamic techniques caching 
specifications precise data consistency semantics enable efficient relaxed consistency protocols 
goal load balance ensure parallel application effective processors machine 
achieving goal requires specifications mapping tasks processors machine 
task mapping controlled specifying data distribution executing tasks local objects accesses specifying task distribution 
addition applications rely prioritized execution tasks efficiency require additional specifications thread ordering 
programmer effort required ensure data locality load balance depends application structure capabilities underlying system 
concert shared global namespace provides efficient uniform naming allows locality load balance expressed independent program functionality concurrency 
addition range compiler runtime techniques implicitly address locality load balancing concerns 
described section concert provides interfaces specifying data alignment distribution relaxed consistency semantics ensuring data locality data distribution task placement task ordering ensuring load balance 
application case studies illustrate benefits high level programming features support irregular applications case studies 
application describe algorithmic structure 
describe parallelization application focusing programmer effort associated concurrency synchronization specification ensuring data locality load balance 
summarize description application discussing extent support concert system helps manage programmer concerns 
barnes hut barnes hut code computes interactions system particles barnes hut hierarchical body method 
algorithmic structure barnes hut method exploits physical insight interactive force particles decreases rate proportional square distance reduce computational complexity 
primary data structure oct tree data structure hierarchically divides simulation space cubes called spatial cells 
leaf spatial cells turn point contained particles 
spatial cells represent aggregate center mass force information contained particles 
interactions particles sufficiently far apart summarized particle cell interactions reducing computational complexity log 
complexity reduction comes cost complex data structures 
particle spatial distributions highly non uniform producing highly unbalanced oct trees 
addition floating point computations corresponding interactions traversing oct tree efficiently crucial performance barnes hut 
computation proceeds iterations 
iteration oct tree built updated position particles 
tree compute force exerted particle subsequently velocities positions particles updated 
force computation phase dominates computation representing sequential computation time 
force computation phase particle traverses global oct tree recursively 
traversal partial traversal 
cell distance center mass current position particle calculated 
distance larger cutoff traversal subtree terminates interactions particle particles contained cell computed particle cell interaction 
distance cutoff traversal continues children cells current cell 
data access pattern tree traversal highly data dependent irregular 
program parallelization parallel algorithm derived barnes hut application splash benchmark suite :10.1.1.104.6469
concurrency synchronization specification levels parallelism dominant force computation phase 
oct tree traversals particles concurrent traversal traversal subtrees concurrent 
levels parallelism naturally expressed concert conc annotations corresponding code sequential version 
global namespace default object sequential consistency provided icc ensures additional code required parallel tasks access potentially remote objects traversals global oct tree 
code fragment illustrates barnes hut force computation phase 
conc keyword annotates concurrency conc particles compute root cutoff compute cell cutoff distance cell cutoff conc ndim cell children null compute cell children cutoff 
cell pos 
cell mass cost history load balancing cost conc annotations needed levels parallelism separate tree traversals traversals subtrees level 
conc annotations additional code required manage name translation ensure consistency accessing updating global oct tree 
addition conc annotations non binding liberally expose concurrency allowing compiler exploit concurrency parallelism ignore concurrency sequential efficiency 
lines lines code conc annotations 
data locality load balance barnes hut exhibits data dependent irregular data access patterns computation granularity 
dominant computation phase consists concurrent partial traversals oct tree data structures portion tree traversed depth traversal dependent particle spatial location 
computation fine grained single particle particle particle cell interaction incurs floating point operations accessing tree elements performance requires data locality optimize remote accesses balancing amount processors 
ensure data locality load balance spatial partitioning tree data structure history load balancing algorithm adopted 
spatial partitioning constructs ordering particles spatial coordinates 
linear ordering divided blocks assigned processors 
peano hilbert ordering inherently groups particles close spatially assignment maximizes access locality data reuse accessing remote tree nodes 
history load balancing counts number interactions particle incrementing cost variable code example counts weights assign number particles processors iterative step 
number interactions previous iteration estimate amount particles move slowly simulations 
data placement interface concert system achieve assignment particles processors 
default owner computes policy concert separate code distribute tree handles loadbalancing initial placement 
computation se decoupled data placement enabling easy experimentation data placement load balancing policies 
data placement load balancing code consists line changes lines 
summary changes concurrency synchronization specification barnes hut code straightforward corresponding naturally program structure 
non binding concurrency global namespace object consistency support icc allow expression specifications minimal changes sequential version 
ensuring data locality load balancing requires understanding application 
programmer needs provide initial data placement particles processors achieve data locality load balance 
data placement decoupled algorithmic specification 
lines require changes parallel versions lines 
fmm fmm code computes interactions system particles fast multipole fmm hierarchical body method :10.1.1.104.6469
algorithmic structure fmm method exploits physical insight interactions particles decreases rate proportional square distance reduce computational complexity 
primary data structure quad tree data structure hierarchically divides simulation space squares called boxes 
leaf boxes turn point contained particles 
particle particle interactions box computed directly 
spatial boxes hierarchically summarize particle information multipole expansion 
expansion compute interactions particles lie box particle box box box interactions 
fmm method excellent error control reduce computational complexity 
barnes hut code complexity reduction comes cost complex data structures 
particle spatial distributions highly non uniform producing highly imbalanced quad trees non uniform compute granularity irregular data access patterns 
computation proceeds iterations 
iteration quad tree built updated position particles 
quad tree traversed compute box interaction lists boxes 
force computation phase box traverses interaction list computes interactions box lists terms multipole expansions 
expansions hierarchically distributed particles update velocities positions particles 
force computation phase dominates computation representing sequential computation time 
program parallelization parallel algorithm derived fmm application splash benchmark suite :10.1.1.104.6469
concurrency synchronization specification levels parallelism dominant force computation phase interactions corresponding separate boxes computations different interaction lists single box interactions boxes interaction list 
levels parallelism naturally expressed concert conc annotations corresponding code sequential version illustrated code 
global namespace default object sequential consistency provided icc ensures additional code required parallel tasks access potentially remote boxes interaction lists 
lines lines code corresponding concurrency specifications 
conc boxes type conc conc conc conc xlist conc vlist data locality load balance fmm exhibits data dependent irregular data access patterns computation granularity 
interaction lists box dependent spatial coordinates producing irregular data accesses 
addition lengths interaction lists highly irregular boxes making data locality load balancing essential performance 
similar barnes hut ensure data locality load balance spatial partitioning particles boxes history load balancing algorithm adopted 
spatial partitioning constructs peano hilbert ordering particles spatial coordinates 
linear ordering divided blocks assigned processors 
fmm method operates boxes particles box assigned node boxes assigned processor owns majority sub boxes ties broken arbitrarily 
history load balancing counts number interactions particle counts weights assign particles boxes processors iterative step 
number interactions previous iteration estimate amount particles move slowly simulations 
data placement interface concert system achieve assignment particles processors 
default owner computes policy concert separate code distribute tree handles load balancing initial placement 
computation decoupled data placement enabling easy experimentation different policies 
data placement load balancing code consists line changes lines 
summary changes concurrency synchronization specification fmm code straightforward correspond naturally program structure 
non binding concurrency allows concurrency exposed compiler performance penalties global namespace object consistency support icc allow expression specifications minimal changes sequential version 
ensuring data locality load balancing requires understanding application 
programmer needs provide initial data placement particles processors achieve data locality load balance 
data placement decoupled algorithmic specification 
lines require changes parallel versions lines 
samr fields simulations involve solving hyperbolic partial differential equations equations fluid dynamics 
practical task numerically solving equations involves discretizing space time 
structured adaptive mesh refinement samr methods dynamically created meshes discretize simulations conserving computational resources 
investigation parallel method sequential code computational cosmology group national center supercomputing applications 
algorithmic structure basic idea samr adapt hierarchy grids eq 
meshes time order model physical space efficiently 
context hierarchy means logical arrangement grids series levels 
grids higher level model precision grids lower levels 
grids created deleted hierarchy simulation evolving 
lower level higher precision grids created necessary model rapidly changing areas physical space 
dynamic regions physical space modeled coarsely saving unnecessary computation 
implementation uses primary data structures grid hierarchy data structure grid data structures 
grid hierarchy data structure represents state current simulation 
comprised series levels lists grids 
grid structures responsible actual modeling physical space contain arrays floating point values represent state information discretized area 
grid contains number boundary zones grid grid communication 
simulation proceeds advancing virtual clock number timesteps comprised sequence phases 
phases compute phases grid grid communication 
phases operate grids sets grids particular level hierarchy 
examples include compute intensive numerical solve phases grids level communication heavy grid grid copy phase pairs grids level 
global dependences levels prevent phases executing level boundaries 
algorithm terminates appropriate number iterations completed virtual clock exceeds predetermined limit 
program parallelization parallelism samr implementation resides phase execution grids particular level hierarchy 
numerical solve phase example independent grids level 
full simulation contains independent phases total 
simulation executed concurrently parallelizing phases 
samr poses number significant challenges parallelization 
method adapts hierarchy course simulations execution data structures parallel created dynamically 
means way knowing compile initialization time extent parallel phases 
need language support expression dynamic parallelism dynamic distribution data structures runtime load balance 
granularity control significant challenge 
simple parallelization exploit parallelism grids level distributing grids machine 
produces insufficient parallelism general poor load balance 
approach overcomes challenges system support careful implementation choices 
concurrency synchronization specification parallelism simulation expressed set parallel method calls grids sets grids level 
levels adapted iteration type parallelism dynamic 
assume grids level distributed processing elements 
conc keyword easily express parallelism follows 
conc grids level 
grid independent grids conc allows calls execute independently synchronizing loop 
concert runtime support automatically distributes calls concurrent execution loop happens transparently 
granularity control load balance issues load balance grain size control addressed tiling 
divide grid set independent uniform sized tiles 
simulation point view changed manipulating hierarchy grids 
logical transformation grids array independent tile objects gives load balancing granularity control 
grid container tiles distributed icc distributed array data type collections 
conc simply transformation concurrent loop doubly nested 
conc grids level conc tiles grid 
tile independent tiles construction allows calls independent method calls distributed objects 
controlling tile size control granularity independent pieces adjusting size individual tiles 
adjustments grain size control affect size grids simulations point view 
transformation promotes load balance size units generally proportional tile size uniform 
summary changes complexity involved parallelizing samr simulation method modest terms programmer effort 
task code tiling data structure transformation 
modifications required far reaching code overly complex tile structures closely resembled grid data structures 
mainly interfaces grid functions modified entire bodies functions 
step instrumenting code run concurrently 
largely consisted adding conc keyword number loops implementing grids collections tiles 
due icc similarity took little programmer effort parallel loops identified 
annotate concurrent execution lines modified 
note case samr additional load balance distribution effort needed part programmer 
collection data type ensures cyclic distribution concurrent method calls synchronization happen transparently concurrent loops 
computes overlay polygon maps 
starting point fast sequential algorithm described 
algorithmic structure polygon maps referred left right map represented vector link list respectively 
parallelism polygons left map polygon left map intersected turn polygon right map intersection procedure efficient keeping track polygon overlap areas 
procedure terminates polygon fully covered 
addition polygon pruned right map list entirely covered left map polygons 
optimizations produce irregular concurrency tricky sharing dynamic data structures 
poses main implementation challenges 
primary data structure linked list implementation right map concurrently traversed modified 
second order sensitivity polygon sizes comparisons produces challenging load balance problems 
program parallelization parallel algorithm exploits data parallelism polygons left map pipeline parallelism right map 
concurrency synchronization specification parallelize programmer provide concurrency annotations key inner loops polygon comparisons area information 
annotation shown code example conc annotations required 
global namespace object level sequential consistency icc permits natural expression code implicitly maintains consistency list 
code required changes lines 
right true conc conc link null pl poly null add new polygon area link link delete list traverse data locality load balance implement efficiently icc programmer control data layout 
challenge maintain reasonable load balance efficiency computation extremely finegrained solve problem distributing list data structure block distribution 
size block ensures high efficiency node 
reasonable load balance achieved relying degree randomness inputs despite irregularity computation load balance occur 
alternative approach block cyclic distribution number polygons quite large 
expressing block distribution straightforward completely orthogonal expression program 
data distribution right map simple annotation 
program required changes lines data locality load balance 
summary changes changes required parallelization quite small 
conc annotations provided requisite concurrency 
computation specification completely decoupled data placement permitting easy experimentation placement policies 
lines require changes sequential version 
global address space clear improvement distributed namespace distributed model require explicit name management list elements 
programming easier shared memory system programmer need explicit locking 
grobner grobner application symbolic algebra domain computes grobner basis set polynomials 
sequential algorithm due buchberger starting point 
algorithmic structure algorithm starts initial basis set polynomials equal input set 
possible pair polynomials evaluated ranked heuristic metric 
evaluation polynomial pair consists arbitrary precision arithmetic operations isolate irreducible polynomial terms potentially lead addition new polynomial basis 
turn generates additional polynomial pairs evaluation correspond new polynomial existing basis polynomials 
algorithm completes polynomial pairs left evaluation 
algorithm requires data structures represent arbitrary precision arithmetic polynomials structures dynamically grow store basis 
addition requires iterative control structures data dependent termination criteria 
algorithm challenging express sequential platforms 
algorithm implementation uses classes polynomials basis 
polynomial objects store coefficients exponents terms polynomials manipulated 
basis object implements multiset abstraction stores pointers different polynomials array expands required 
polynomial pairs stored priority queue required due heuristic evaluation metric computation consists dequeuing pair priority queue evaluating queue empty 
program parallelization parallel algorithm derived previous parallelization grobner basis problem distributed memory machines chakrabarti 
concurrency synchronization specification parallelism application arises parallel evaluation polynomial pairs 
naturally expressed concert conc loop pair generation code sequential program 
pair creates dynamic task accesses basis polynomial objects 
task produces polynomial irreducible terms invokes method basis object augment 
support global namespace concert ensures additional code required tasks access potentially remote objects polynomial basis tasks code identical sequential methods 
synchronization specification required application basis object kept consistent concurrent access basis accessed augmented critical section 
specification trivially ensured concert due object level concurrency control semantics 
natural expression concurrency synchronization specifications require changes lines program icc lines 
data locality load balance application exhibits data dependent irregularity long pair evaluation takes 
additionally varies order pair evaluations affects basis grows 
consequently careful management data locality load balance required efficiency 
data locality ensures pair tasks efficient access basis polynomials 
load balancing reasons pairs need evaluated arbitrary processors requiring remote access polynomial basis objects 
demand driven caching polynomial basis objects 
concert implementation optimizes caching performance utilizing additional information relaxed data consistency semantics 
additional information conveys fact polynomials accessed readonly fashion computation proceed inconsistent date view basis 
permits overlap pair evaluations basis augmentations expressed shown code segment 
entire program programmer needs specify access information kinds views requires changes lines program 
inconsistent copy global basis basis local read async 
load balance required irregular computation granularity 
balanced demand ordering pair evaluations heuristic metric crucial avoid redundant 
solution accomplishes goals attaching pair evaluation task priority stealing scheduler provided part concert system shown code fragment 
code illustrates aspects programmer input required load balancing application programmer needs identify task granularity load balancing case function supply integer priority task priority keyword select scheduler enqueuing task case global priority scheduler 
similar custom load balancing annotations required lines program 
pnew added basis generate iterating existing polynomials conc 
pnew test pnew priority calculate priority pnew pnew priority priority scheduler global priority 
summary changes concurrency synchronization specification grobner application require little programmer effort corresponding naturally structure sequential algorithm 
furthermore high level features concert system non binding concurrency global namespace object level concurrency allow specifications expressed minimal changes original sequential source 
hand ensuring locality load balance requires effort primarily due dynamic irregular nature application 
programmer effort required things providing relaxed data consistency specifications ensuring efficient data locality polynomials basis specifying thread ordering scheduling load balance attaching task priority scheduler 
case concert system provides support requires minimal changes source code allowing programmer focus high level structural issues 
code required modification lines describing concurrency synchronization specifications lines ensuring data locality lines load balance lines 
phylogeny phylogeny application computes evolutionary history set species representing history tree path root species showing evolutionary path species 
sequential algorithm described jones starting point 
algorithmic structure phylogenetic trees constructed considering characters exhibited species 
set species species represented vector character values max max maximum number characters considered character compatible phylogenetic tree value character arises path tree 
perfect phylogenetic tree set species set characters phylogenetic tree compatible characters 
algorithm general phylogeny problem method known character compatibility uses solution perfect phylogeny problem determining perfect phylogenetic tree exists see details subroutine 
essence character compatibility method search largest compatible subset characters rationale subset large corresponding perfect phylogeny estimate evolutionary history species 
algorithm searches power set characters possible subset asking subset compatible 
algorithm relies extensive pruning property subsets compatible set compatible 
algorithm maintains failure set subsets 
computation corresponds bottom search character lattice depth right left small subsets progressing larger subsets 
subset checked failure set subset subset failed requires perfect phylogeny procedure perfect phylogeny procedure called 
result procedure subset added failure set additional subsets generated exploration 
implementation algorithm uses kind object solution node organized trie implement failure set facilitate efficient subset checking 
solution node maintains character subset child parent pointers trie 
sophisticated nature data structures pruning search procedure algorithm complicated express sequential platforms 
program parallelization parallel algorithm follows strategy evaluates multiple character subsets parallel 
adequate parallelism exists level chosen utilize bundled solution parallel phylogeny problem sequential leaf subroutine 
concurrency synchronization specification parallelism application naturally expressed concert annotating generation loop conc keyword 
non binding concurrency semantics concert permit parallelism expressed essentially concern computation granularity task 
task traverses global failure set 
support global namespace concert ensures additional code required tasks access potentially remote solution nodes tasks code identical sequential methods 
synchronization specification required application failure set kept consistent concurrent access failure set accessed augmented critical section 
specification trivially ensured concert fact weaker form specification implemented due object level concurrency control semantics associated solution node objects 
natural expression concurrency synchronization require changes lines program lines 
data locality load balance application exhibits data dependent irregularity long subset evaluation takes 
additionally varies order subset evaluations affects failure set grows 
consequently careful management data locality load balance required efficiency 
data locality ensures subset tasks efficient access solution nodes comprising failure set 
load balancing reasons subsets need evaluated arbitrary processors may require remote access solution node objects 
demand driven caching solution node objects 
concert implementation optimizes caching performance utilizing additional information relaxed data consistency semantics 
additional information conveys fact subset tasks proceed inconsistent date view failure set 
permits overlap subset evaluations failure set augmentation illustrated code segment 
entire program programmer needs specify access information kind view requires changes lines program 
elt cset cset member elt 
recursively traverse children local left read async right read async 
left elt cset 
right elt cset load balance required irregular computation granularity 
balanced demand ordering subset evaluations correspond bottom traversal character lattice crucial avoiding redundant calls perfect phylogeny routine 
solution accomplishes goals attaching subset task priority stealing scheduler 
grobner application programmer effort required identify task granularity load balancing subset task supply integer priority bit encoding subset select scheduler case local priority scheduler 
custom load balancing annotations required places program 
summary changes concurrency synchronization specification phylogeny application require little programmer effort corresponding naturally structure sequential algorithm 
furthermore high level features concert system non binding concurrency global namespace object level concurrency allow specifications expressed minimal changes original sequential source 
hand ensuring locality load balance requires effort primarily due dynamic irregular nature application 
programmer effort required things providing relaxed data consistency specifications ensuring efficient data locality solution nodes specifying thread ordering scheduling load balance attaching subset task priority scheduler 
case concert system provides support requires minimal changes source code allowing programmer focus high level structural issues 
code required modification lines describing concurrency synchronization specifications lines ensuring data locality lines load balance lines 
radiosity radiosity method computes global illumination scene containing reflecting surfaces expressing radiosity elemental surface patch linear function patches weighted distance occlusion patches 
starting point sequential algorithm due hanrahan modeled hierarchical body methods 
algorithmic structure traditional radiosity approaches subdivide polygonal surfaces describe scene small elemental patches roughly uniform radiosity compute radiosity patch aggregation pair wise interactions patch patch 
contrast hierarchical algorithm starts initial patches comprising scene computes light transport pairs patches hierarchically subdividing patch needed ensure accuracy 
demand division patches responsible reducing complexity sequential algorithm 
complexity reduction comes cost new algorithm involves sophisticated data structures quad tree maintain subdivision structure complex control structures recursion child patches data dependent iteration 
algorithm challenging express sequential platforms 
algorithm implementation uses objects patches interactions 
patches represent discrete elements reflecting surface interactions store information visibility patch pairs 
objects organized principal data structures quad tree patches list interactions binary space partitioning bsp tree input patches 
patch maintains interaction lists potentially visible neighbors initially patch 
computation proceeds iterations patch compute radiosity linear combination patches interaction list subdividing patches hierarchically required ensure accuracy error computation 
radiosity computation requires visibility determines contribution patch radiosity patches depends shape distance relative orientation patches 
interaction previously calculated 
calculation depends entirely patch geometries visibility calculation requires ray tracing 
visibility calculation dominates computation time efficient bsp tree 
subdivided patches organized quad tree acquire interaction lists contribute weighted term parent radiosity 
iterations terminate change total radiosity weighted sum input patches falls threshold 
program parallelization parallel algorithm derived radiosity application splash benchmark suite :10.1.1.104.6469
concurrency synchronization specification levels parallelism iteration input patches radiosity calculation child patches subdivided patch radiosity calculation neighbor patches stored interaction list visibility error radiosity calculation levels naturally expressed concert conc annotations corresponding functions sequential program 
support global namespace icc ensures additional code required parallel tasks access potentially remote objects tree traversal tasks code identical sequential methods 
synchronization specifications required ensure correct execution 
specifies dependence tasks radiosity calculation child patch nested radiosity calculation parent patch 
second specifies task accessing patch object exclusive access 
specifications trivially ensured concert code body sequential parallel versions due object level concurrency control semantics 
fact specifications required places program lines 
data locality load balance application exhibits data dependent irregularity levels parallelism deep patch refined interactions need computed patch long particular interaction calculation takes 
additionally iterations uniform computation occuring iteration 
consequently careful management data locality load balance required efficiency 
data locality issue primarily radiosity visibility tasks 
radiosity computation patch accesses state neighboring patch neighbor state state stored interaction object 
visibility task accesses state pair patches traverses bsp tree data dependent fashion 
ensure data locality combination data alignment demand driven object caching 
locality interaction objects bsp tree ensured data alignment 
interaction objects aligned source patch ensuring locality computations executing local source patch 
fact bsp tree built computation explicitly replicate processor 
patch locality requires demand driven caching 
concert implementation optimizes caching performance utilizing additional information relaxed data consistency semantics 
simplified visibility calculation code shows programmer uses local annotation specify application specific access semantics view compiler runtime system 
case input polygonal patches specified read allowing concert system efficiently cache data exploit data reuse 
entire program programmer needs specify access information kinds views 
entire program ensuring data locality increases source code length lines compared original lines bulk ensures replication bsp structure rest describes alignment data consistency specifications 
cur local read read compute visibility vis func pos pos 
cur vis vis load balance primarily required balance radiosity visibility tasks machine achieved combination data distribution dynamic load balancing 
allocating dynamically created patches random nodes time creation default policy concert executing radiosity tasks local ensures radiosity task calculation load balanced 
similar scheme introduces significant imbalance visibility tasks 
load balance utilize dynamic load balancing algorithm requires programmer identify task balanced specify target destination 
code fragment programmer chooses function granularity load balancing uses target keyword place task random destination node creation 
target annotations required places program 
conc cur cur cur cur cur vis cur dest cur target rand summary changes concurrency synchronization specification radiosity application require low programmer effort corresponding naturally structure sequential algorithm 
furthermore high level features concert system non binding concurrency global namespace object level concurrency allow expression specifications minimal changes original source sequential program 
hand ensuring locality load balance requires effort part programmer primarily due irregular nature application 
programmer input required things replication bsp tree structural change program data consistency semantics efficient patch caching load balancing visibility tasks 
case concert system provides support requires minimal changes source code allowing programmer focus high level structural issues 
code required modification lines describing concurrency synchronization specifications lines ensuring data locality lines load balance lines 
summary applications globally shared pointer data structures irregular concurrency structures 
global namespace conc annotations icc enables computations expressed naturally 
application suite falls main classes 
applications barnes hut fmm samr irregular concurrency semi static random load balancing schemes 
efficiently implement applications icc programmer needs provide concurrency annotations sequential version algorithms load balancing data placement codes expressed collection maps 
result fewer lines typically require changes lines barnes hut lines fmm lines samr 
hand applications radiosity grobner phylogeny additionally require fully dynamic load balancing application specific priority scheduling consistency model efficiency 
applications programmer provide dynamic load balancing policies additionally priority information application specific data access semantics 
requires intimate knowledge applications 
concert provides high level annotations allow incorporated computation small code changes grobner phylogeny radiosity 
parallel performance high level language features simplify program expression commonly thought imply performance degradation 
aggressive compiler runtime technology concert delivered performance application suite 
discuss performance cray implementation illinois concert system 
programs achieve sequential performance factor corresponding programs 
achieved aggressive interprocedural analysis concert compiler optimizations method object inlining eliminate overheads object orientation 
access region eliminates overhead global object space speculative stack heap execution reduces overheads implicit non binding concurrency specification 
results show aggressive implementation technology help high level programming approaches realize potential eliminating programmer management concerns samr code performance results sgi origin system 
procedure thread granularity object structuring required performance 
shows speedup applications respect non overhead portion single node execution time 
speedups compare favorably best speedups reported codes 
speedup best sequential algorithm 
speedup levels insufficient parallel algorithm comparable reported numbers 
barnes speedup fmm speedup nodes competitive shared memory versions hand optimized versions reported literature :10.1.1.104.6469:10.1.1.30.5842
radiosity speedup processors compares previously reported speedup processors dash machine despite dash hardware support cache coherent shared memory order magnitude faster communication terms processor clocks better facilitate scalable performance 
speedups grobner phylogeny compare favorably implementations despite fully replicating data structures cases 
competitive speedups possible various transformations described section enhance data locality load balance 
transformations realizable modest code changes due orthogonal framework functionality data placement 
illustrate incremental performance gains due transformation considering application radiosity example 
shows speedup versions radiosity program base version corresponds base programming model random data distribution threads execute local target object locality version corresponds program transformations data locality load balance version corresponds program transformations load balance 
described section locality load balance versions realizable changes source code 
clearly shows importance data locality load balance transformations improve speedup nodes respectively nodes respectively 
barnes fmm samr radiosity grobner phylogeny number processors speedup application suite cray 
ideal load balance locality base number processors incremental performance benefits data locality load balance program transformations radiosity application 
discussion related despite wealth parallel programming research know application studies systematically evaluate programmability delivered parallel language system 
knowledge study looks collection irregular application programs general purpose parallel programming system evaluate support high level expression performance optimization 
find outcome encouraging 
concert system difficulty managing irregular concurrency modifying program data distribution eliminated 
commercial parallel programming environments incorporate similar technologies prospects high performance high level parallel object oriented programming encouraging 
programming techniques irregular applications important body related 
techniques pursue library approach consider irregularity single dimension 
inspector executor approach supports iterative irregular communication finite element simulations designed spmd execution provides little support irregular control flow 
libraries support particular classes algorithms sparse matrices adaptive meshes basically solver libraries general programming interface irregular applications 
closest related yelick libraries provide set tools build irregular applications 
libraries provide uniform high level interface project include optimizing compiler support 
best known parallel programming approaches message passing embodied mpi data parallel embodied 
approaches widely build irregular applications 
believe provides appropriate support 
compared message passing icc dramatically simplifies programmability irregular codes eliminating need explicit name management reducing programming effort allowing data locality load balance optimized independently 
message passing versions barnes hut fmm samr radiosity considered require programming chiefly due difficulties implementing sophisticated distributed data structures 
data parallel approaches provide global namespace require structured data parallelism poor match irregular applications 
alignment adds significant task program formulation generally forces programmer build structures quite different sequential program versions 
increases programming effort 
flexible models data parallels nested parallelism promising 
body related series application studies cache coherent shared memory systems employ model shared address space threads 
leveraged studies borrowing algorithmic structure data locality load balance optimization fmm radiosity real goal studies architectural evaluation programmability 
contrast concert system allows applications programmed higher level achieve excellent scalability 
example aggressive compiler optimization concert frees programmers worries procedure granularity 
addition concert lightweight multithreading eases concerns parallelization making expression sufficient parallelism easier 
concert employs shared object namespace easier implement flexible object consistency implemented software implemented hardware 
concert system aggressive implementation techniques enable high performance implementations irregular applications distributed shared memory systems unclear hardware systems provide best performance 
shared memory machines simplify bookkeeping implementing global namespace introduce new problems inflexible coherence sharing granularity protocols opaque memory behavior difficult reason optimize 
interesting direction performance comparisons platforms 
summary application studies conclude development irregular parallel applications remains challenging concert system enables programs written high level approach 
high level expression enhances program optimization compiler dealing fundamental challenges data locality load balance irregular applications 
virtually cases complexity data locality load balance scheme significant ways comparable application algorithmic complexity 
clean separation data locality issues naming clear benefit improving program flexibility dramatically 
addition program optimizations concert system enable programmers relegate number concerns programming system procedure computation granularity namespace management low level concurrency management 
optimizations may expensive incorporate production compilers today optimistic algorithmic improvements approximations optimizations parallel programming systems 
appropriate high level programming support optimized parallel versions irregular applications identical basic program structure sequential counterparts 
concurrency added small program perturbation data locality load management achieved minor program changes 
fact cases original source lines needed modified deliver high parallel performance 
software developers inevitably need support single code base sequential parallel users 
results encouraging concurrent object oriented approaches improved ways increase programmability advantages 
compiler technology aggressively propagates locality information pointer data structures potential reduce program optimization effort irregular applications 
second flexible concurrency structures expressible icc require global concurrency analysis general open research problem enable synchronization optimizations 
increases explicit annotations required achieve high performance 
promising direction research 
third cases explicit annotations locality load balance refined programmer information task granularity object consistency priority scheduling 
finding automatic techniques 
looked significant irregular programs learned systematic application studies parallel programming systems 
hope study provide insights application programmers programming system designers stimulating research real problems facing programmers 
acknowledgments authors acknowledge john plevyak hao hua chu illinois concert system 
research described supported part darpa order air force rome laboratory contract nsf mip onr nasa nag supercomputing resources jet propulsion laboratory 
support intel tandem computers hewlett packard motorola gratefully acknowledged 
andrew chien supported part nsf young investigator award ccr 
vijay karamcheti supported part ibm computer sciences fellowship 
barnes hut 
hierarchical log force calculation algorithm 
technical report institute advanced study princeton new jersey 
peter beckman dennis gannon elizabeth johnson 
portable parallel programming hpc 
available online www extreme indiana edu hpc docs icpp ps 
blelloch 
compiling collection oriented languages massively parallel computers 
journal parallel distributed computing 
greg bryan 
numerical simulation ray clusters 
phd thesis university illinois urbana champaign august 
buchberger 
multidimensional systems theory chapter grobner basis algorithmic method polynomial ideal theory pages 
reidel publishing 
chakrabarti yelick 
implementing irregular application distributed memory multiprocessor 
proceedings fourth acm sigplan symposium principles practices parallel programming pages may 
mani chandy carl kesselman 
compositional compositional parallel programming 
proceedings fifth workshop compilers languages parallel computing new haven connecticut 
yaleu dcs rr springer verlag lecture notes computer science 
chatterjee 
compiling nested data parallel programs shared memory multiprocessors 
acm transactions programming languages systems 
chien feng karamcheti plevyak 
techniques efficient execution fine grained concurrent programs 
proceedings fifth workshop compilers languages parallel computing pages new haven connecticut 
yaleu dcs rr springer verlag lecture notes computer science 
chien karamcheti plevyak zhang 
case study irregular parallel programming 
dimacs workshop specification parallel algorithms may 
available springer verlag lncs 
chu george liu ng 
waterloo sparse matrix package user guide 
technical report cs department computer science university waterloo waterloo ontario canada 
julian 
automatic inline allocation objects 
proceedings acm sigplan conference programming language design implementation june 
high performance fortran forum 
high performance fortran language specification version 
technical report rice university january 
ganguly greg bryan michael norman andrew chien 
exploring structured adaptive mesh refinement samr methods illinois concert system 
proceedings eighth siam conference parallel processing scientific computing minneapolis minnesota march 
leslie greengard :10.1.1.104.6469
fast algorithms classical physics 
mit press cambridge ma 
grimshaw 
easy object oriented parallel processing mentat 
ieee computer may 
hanrahan 
rapid hierarchical radiosity algorithm 
computer graphics proc siggraph july 
yuan shin hwang raja das joel saltz bernard brooks milan 
parallelizing molecular dynamics programs distributed memory machines 
ieee computational science engineering pages summer 
saltz manual chaos runtime library 
technical report cs tk department computer science university maryland 
jeff jones 
parallelizing phylogeny problem 
master thesis computer science division university california berkeley december 
shu 
kernel language parallel programming 
proceedings international conference parallel processing 
vijay karamcheti andrew chien 
view caching efficient software shared memory dynamic computations 
proceedings international parallel processing symposium 
vijay karamcheti john plevyak andrew chien 
runtime mechanisms efficient dynamic multithreading 
journal parallel distributed computing 
available www cs uiuc edu papers ps 
lee gannon 
object oriented parallel programming 
proceedings acm ieee conference supercomputing 
ieee computer society press 
stephen barbara ryder 
non concurrency analysis 
proceedings fourth symposium principles practice parallel programming pages may 
message passing interface forum 
mpi message passing interface standard 
technical report university tennessee knoxville april 
available www mcs anl gov mpi mpi report ps 
parsons quinlan 
array classes architecture independent finite difference computations 
proceedings second annual object oriented numerics conference oregon april 
john plevyak 
optimization object oriented concurrent programs 
phd thesis university illinois urbanachampaign urbana illinois 
john plevyak andrew chien 
incremental inference concrete types 
technical report uiucdcs department computer science university illinois urbana illinois june 
john plevyak andrew chien 
precise concrete type inference object oriented programs 
proceedings oopsla object oriented programming systems languages architectures pages 
john plevyak andrew chien 
type directed cloning object oriented programs 
proceedings workshop languages compilers parallel computing pages 
john plevyak vijay karamcheti andrew chien 
analysis dynamic structures efficient parallel execution 
proceedings sixth workshop languages compilers parallel machines pages august 
john plevyak zhang andrew chien 
obtaining sequential efficiency concurrent object oriented programs 
proceedings acm symposium principles programming languages pages january 
daniel scales monica lam 
design evaluation shared object system distributed memory machines 
symposium operating systems design implementation 
pal singh 
parallel hierarchical body methods implications multiprocessors 
phd thesis stanford university department computer science stanford ca february 
pal singh anoop gupta marc levoy 
parallel visualization algorithms performance architectural implications 
ieee computer july 
pal singh wolf dietrich weber anoop gupta 
splash stanford parallel applications shared memory 
technical report csl tr computer systems laboratory stanford university stanford university ca april 
available ftp mojave stanford edu pub splash report splash ps 
warren salmon :10.1.1.30.5842
parallel hashed oct tree body algorithm 
proceedings supercomputing conference pages 
chih po wen soumen chakrabarti etienne arvind krishnamurthy katherine yelick 
run time support portable distributed data structures 
third workshop languages compilers run time systems scalable computers pages boston may 
kluwer academic publishers 
gregory wilson paul lu editors 
parallel programming 
mit press 
gregory wilson paul lu editors 
parallel programming chapter icc 
mit press 
steven cameron woo evan pal singh anoop gupta :10.1.1.104.6469
splash programs characterization methodological considerations 
proceedings international symposium computer architecture pages 
akinori yonezawa editor 
abcl object oriented concurrent system 
mit press 
isbn 
zhang andrew chien 
dynamic pointer alignment tiling communication optimizations parallel pointer computations 
proceedings acm sigplan symposium principles practice parallel programming las vegas nevada june 
zhang vijay karamcheti tony ng andrew chien 
optimizing coop languages study protein dynamics program 
ipps 

