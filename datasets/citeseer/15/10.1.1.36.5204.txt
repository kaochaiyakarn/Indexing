applying new optimization algorithms model predictive control stephen wright mathematics computer science division argonne national laboratory argonne il connections optimization control theory explored researchers optimization algorithms applied success optimal control 
rapid pace developments model predictive control rise host new problems optimization applied 
concurrently developments optimization especially interior point methods produced new set algorithms may especially helpful context 
reexamine relatively simple problem control linear processes subject quadratic objectives general linear constraints 
show new algorithms quadratic programming applied efficiently problem 
approach extends general problems straightforward ways 
keywords optimization model predictive control interior point methods apply developed techniques optimization literature core problem model predictive control control linear process quadratic objectives subject general linear constraints 
describe algorithms formulation min gamma qx ru xn ax bu gamma fixed gu jx gamma positive semidefinite matrices ir ir ir problem known optimal control literature revived context model predictive control mpc 
mpc applications receding horizon control rawlings muske constrained linear quadratic regulation rawlings controls obtained solving problems repeatedly 
describe methods outline extended easily general forms may contain outputs cx penalties control jumps gamma 
approaches consider detail involve infeasible interior point method active set method 
methods able exploit special structure problem obtain solution reasonable amount time 
techniques discussed represent just potential contributions optimization mpc 
developments mpc created demand fast reliable solution problems nonlinearities noise constraints states controls may 
algorithmic developments areas interior point methods stochastic optimization produced powerful tools tested mpc problems 
means expect optimization algorithms panacea problems arise mpc 
cases specialized algorithms motivated particular control problem hand appropriate 
expect mpc problems benefit optimization viewpoint interactions optimizers engineers best way realize benefits 
section sketch way algorithmic research optimization relates applications illustrating point problem optimal control 
interior point algorithm show applied efficiently problem 
move active set approach impact computer science outline application 
applications paradigms field optimization founded separate academic discipline 
emergence due number factors 
demand side desire approach huge logistical problems posed economy systematic way realization techniques applied logistical problems faced industry commerce 
supply side dantzig development simplex method appearance digital computers factors played important role 
connections optimization mathematical disciplines calculus variations game theory recognized earliest researchers field 
today research optimization continues give impetus areas mathematics nonsmooth analysis linear algebra combinatorics 
applications operations research industrial engineering economics experimental sciences statistics problem fitting observed data models optimization problem wild physical sciences example meteorological data assimilation national research council modeling garner wright 
researchers optimization set standard paradigms mathematical formulation supposed represent large class applications 
examples include linear programming convex quadratic programming unconstrained nonlinear optimization nonlinear programming 
paradigms proposed early days optimization focus research effort area 
optimization paradigms interface optimization research optimization applications 
focus efforts theoreticians software developers welldefined tasks freeing effort acquainted details individual application 
linear programming possibly successful paradigm vast range linear programs solved single piece software little need case case interactions software developers users 
complex paradigms nonlinear programming easy apply 
general software problems unable take advantage special features instance resulting inefficiency 
algorithms customized remove inefficiencies 
optimal control model predictive control illustrate point 
problems areas fit standard optimization paradigms unclear optimization algorithms applied efficiently 
cases special purpose algorithms devised example differential dynamic programming jacobson mayne cases standard optimization algorithms newton method conjugate gradient method gradient projection algorithms adapted successfully optimal control setting polak bertsekas dunn bertsekas 
close section example classical discrete time optimal control problem objectives problem arises frequently mpc literature rawlings meadows muske 
problem provides nice illustration potential impact optimization control 
shows naive application optimization algorithms control problems lead gross inefficiencies remedied little customization adaptation 
problem min gamma ln gamma fixed ir ir viewed unconstrained optimization problem unknowns un gamma states xn eliminated state equations 
highly inefficient solve general implementation newton method unconstrained optimization 
code usually requires user evaluate function gradient hessian request set variable values 
hessian respect un gamma dense code require operations simply compute newton step 
dunn bertsekas shows step obtained specialized calculation takes advantage structure requires operations 
newton algorithm tailored special form hope solving problem efficiently 
problem viewed nonlinear programming problem variables un gamma xn state equations viewed equality constraints 
special structure transparent formulation jacobian constraints hessian objective function sparse block banded matrices 
nonlinear programming code implements variant sequential quadratic programming may perform quite efficiently provided uses exact second derivatives exploits sparsity 
disadvantage nonlinear programming algorithms tend weaker global convergence properties unconstrained optimization algorithms 
optimization model predictive control special case linear convex quadratic problem convex programming problem global convergence attained easily formulation 
constraints controls added choice eliminating states formulations yield nonlinear programming problem 
formulation states eliminated simpler constraints complicated objective function 
constraints states introduced elimination problematic little choice view problem nonlinear programming problem unknowns un gamma xn 
consider linear quadratic form problem sections 
interior point methods linear quadratic problems section consider linear quadratic problem 
frequently noted problem optimization terms convex quadratic program 
successful methods addressing class problems active set method described fletcher interior point method described wright 
special structure means take care applying approach problem 
naive application quadratic programming code active set method instance gill murray saunders wright give poor results typically requiring operations number rows matrices section show interior point algorithm applied section examine adaptations needed active set approach efficient 
state outset interior point approach describe adapted various modifications generalizations significant loss efficiency 
instance matrices vary output vector cx incorporated objective constraints incorporate constraints objective terms involve states controls adjacent stages example penalty control move gamma component generalization useful problem obtained discretization continuous problem discretization schemes ordinary differential equations differential algebraic equations lead relationships states controls number adjacent time points 
rest section organized follows 
define mixed monotone linear complementarity problem powerful paradigm generalizes optimality conditions linear quadratic programs convenient platform describing interior point methods 
outline point algorithm discuss properties 
customize algorithm convex quadratic programming linear quadratic problem 
mixed linear complementarity point framework defined terms square positive semidefinite matrix ir thetan vector ir problem find vectors square submatrices dimensions respectively vector partitioned accordingly 
infeasible interior point algorithm starts point interior nonnegative orthant possibly infeasible respect constraints 
iterates retain positivity properties infeasibilities complementarity gap defined gradually reduced zero 
step algorithm modified newton step system nonlinear equations defined feasibility conditions complementarity conditions write system def gamma notation diag xn diag algorithm form algorithm iip oe solve gammai deltaz deltax deltas gammar gammar gammax oe impact computer science obtain deltaz deltax deltas gamma set ff deltaz deltax deltas ff retains 
note differs pure newton step term oe right hand side 
term plays stabilizing role ensuring algorithm converges steadily solution remaining inside positive orthant defined 
parameters choose implementing algorithm iip scalars oe ff convergence analysis leaves choice oe relatively confined range oe oe fixed parameter typically gamma ff required satisfy conditions 
reduction factor infeasibility norms kr kr smaller reduction factor ratios kr kr decrease monotonically ii pairwise products approach zero roughly rate ratios remain bounded away zero 
note represents average values terms 
iii require sufficient decrease sense decrease obtained small fraction decrease predicted linear model 
see dennis schnabel discussion sufficient decrease conditions 
iv chosen value ff smaller largest possible value iii satisfied 
details see wright wright 
oe ff satisfy conditions global convergence solution attained solution exists 
slightly enhanced version algorithm oe allowed zero iterations exhibits superlinear convergence additional assumptions 
property theoreticians working interior point methods polynomial complexity attained starting point sufficiently large components relative initial residuals solutions 
practical implementations algorithm iip ff chosen simple heuristic 
set ff max supremum set fff ff deltaz deltax deltas set ff min ff max forget theoretical conditions iv simply choose ff step way boundary nonnegative orthant 
tension theory practice existed long time development primal dual methods 
reconciled differences 
exist relaxed versions conditions iv satisfied practical choice ff 
parameters oe ff chosen algorithm iip practically efficient theoretically rigorous 
major operation performed step algorithm iip solution linear system 
matrix system obviously lot structure due presence zero blocks diagonal components additionally matrix sparse cases practical interest including motivating problem sparse matrix factorizations called 
general fairly complex pieces software problems form require banded factorization code comparatively simple 
step solving eliminate deltas component 
diagonal elements positive rearrange block row obtain deltas gamma gammax oe gamma deltax gammas gamma oe gamma deltax substituting rows obtain gamma deltaz deltax gammar gammar gamma oe gamma cases partitions zero diagonal simple structure reduction system usually possible 
phenomenon happens instance derived linear quadratic program show 
factorization coefficient matrix comprises iteration optimization model predictive control may led ask really necessary compute fresh factorization time 
set heuristics factorization essentially re alternate steps proposed mehrotra 
mehrotra algorithm proved successful practice basis vast majority interior point codes linear programming 
linear quadratic programming show linear convex quadratic programming problems expressed form solved algorithm iip 
consider linear program standard form min subject ax vectors ir ir thetan dual max subject ir dual variables alternatively lagrange multipliers constraints ax ir dual slacks 
karush kkt conditions follows ax convex programming problem kkt conditions necessary sufficient 
find primal dual solution linear program finding vector satisfies conditions 
verify form making identifications systems gammaa gammab kkt conditions obtain solutions linear program dual simultaneously applying algorithm iip 
consider general convex quadratic program min qz hz gz symmetric positive semidefinite matrix 
kkt conditions system qz gammac gammah gammah gamma gammag identifications confirm system gammah theta gammag reduced form linear system solved iteration algorithm iip gammah gammag gamma deltaz deltai delta gammar gammar gammar gamma oe gamma defined number inequality constraints 
customary multiply block rows gamma coefficient matrix symmetric indefinite 
obtain gamma gamma deltaz deltai delta gammar gamma oe gamma gamma diagonal positive diagonal elements eliminate delta obtain compact form gamma deltaz deltai gammar gamma gamma oe gamma factorizations symmetric indefinite matrices studied extensively linear algebra literature see golub van loan major algorithms 
standard software available dense case anderson bai bischof demmel dongarra du greenbaum hammarling mckenney sorensen 
algorithms sparse case received considerable attention grimes lewis implemented context interior point methods mehrotra 
solving linear systems free reorder rows columns coefficient impact computer science matrix way choose factorization 
see problem benefits dramatically reordering coefficient matrix case narrow banded matrix easily factored existing linear algebra software lapack anderson 
soft constraints penalty terms cost introducing slack variables dummy variables formulation surprisingly small quadratic program solved technique outlined total number variables problem may increase appreciably 
reason new variables substituted linear system may affect size linear system solve 
comment relevant adding norm constraints soft constraints problems rawlings 
suppose instance wish include soft constraint choose including term gamma objective function 
subscript denotes positive part vector obtained replacing negative components zeros 
restore problem form standard convex quadratic program introduce dummy vector add term objective introduce additional constraints gamma computing form expanded problem applying algorithm iip reducing step equations far simple substitutions find linear system ultimately larger 
details messy omit discussion 
case soft constraints penalty terms consequence observation amount iteration really sensitive choose norm penalty norm penalty option adds fewer dummy variables formulation problem 
solving lqr efficiently formulation linear quadratic problem obviously special case see making identifications data 

gammai gammai 
gammai 
un gamma xn 


suggested earlier matrices problem block banded nonzeros clustered line connecting upper left lower right corners matrix 
formulate linear system nonzeros widely dispersed 
interleaving states controls adjoints lagrange multipliers constraints gu jx return coefficient matrix banded form 
order unknowns system deltau delta deltap deltax deltau delta gamma deltap deltax rearrange rows columns matrix accordingly obtain gammad gammai gammai gammad gammai gammai 
optimization model predictive control denote sections diagonal matrix gamma 
system reduced diagonal entries eliminate delta gamma 
computational savings obtained recovering significant 
assume dimensions ir ir ir banded matrix total dimension half bandwidth gamma 
time required factor matrix compared dense matrix size 
absence constraints cost exactly order cost solving linear quadratic version dynamic programming techniques 
stagewise ordering equations variables key obtaining banded structure 
mentioned maintain banded structure outputs cx constraints introduced model provided continue order variables stage 
techniques outlined quite similar described wright 
difference lies infeasible interior point methods contrast techniques wright combined feasible interior point methods embedding original problem expanded problem feasible initial point easy compute 
new approach cleaner practical efficient remaining theoretically rigorous 
active set methods linear quadratic problems structure problem exploited active set method place interior point method described 
find linear algebra step performed terms banded matrices general dense matrices 
details different somewhat complicated interior point case 
start sketching single iterate active set approach 
complete description see fletcher 
active set methods general convex program generate sequence feasible iterates 
iterate certain subset constraints gz active hold equalities 
step choose subset active set known working set 
typically working set identical active set contains just constraint 
compute step current point minimizes objective function maintaining activity constraints working set ensuring original equality constraints hz remain satisfied 
denote subset rows working set step deltaz obtained solving system min deltaz deltaz deltaz deltaz subject deltaz deltaz equivalently min deltaz deltaz deltaz deltaz deltaz deltaz qz 
kkt conditions deltaz solution system vectors deltai delta deltaz deltai delta satisfies system deltaz deltai delta gamma obtain deltaz system line search direction stopping new constraint encountered minimum objective function direction reached 
note similarity 
coefficient matrices differ diagonal term lower left rows deleted problem matrices banded stagewise reordering rows columns produces banded system 
matrix quite regular missing columns similar savings achieved factoring 
banded matrix best factored gaussian elimination row partial pivoting implemented lapack routine affiliated solution routine anderson 
author knowledge software exploit fact matrix symmetric addition banded 
updating factorizations noted matrix banded problem software tailored matrices obtain significant savings dense linear algebra usually employed standard quadratic programming software 
story quite simple 
systems solve iteration closely related differing column added deleted sense solve system scratch try modify matrix factorization computed earlier iteration accommodate minor changes occurred 
general case dense matrices updating factorizations studied extensively implemented software kinds optimization problems 
simplex algorithms linear impact computer science programming instance depend heavily efficient updating basis matrix iteration 
question face perform efficient updating factorization exploiting answer question affirmative sketching technique re solving row added deleted start addition column system 
changing notation convenience denote original coefficient matrix new row column assume loss generality ordered 
updated matrix assume lu factorization original matrix known permutation matrix lower triangular upper triangular pm lu banded factors nonzeros locations near diagonals 
easily modify accommodate new row column adding extra row column factor obtain gamma gamma pa ff ff gammaa gamma gamma factorization updated cost triangular substitution cost process expensive refactoring scratch small 
new row column participate pivoting new row column dense general factors sparse factorization scratch 
stability issues may arise diagonal element ff may small 
strategy dealing problems compute fresh factorization ff small measure iterations passed 
turn case row column deleted assume factorization known obtain new coefficient matrix deleting row column matrix pm 
note indices deleted row column identical original matrix symmetry row pivoting may cause differ permuted matrix pm obviously modify product equals simply deleting ith row jth column unfortunately deletions cause entries appear diagonal modified version restore removing ith column ith row similarly removing jth row jth column 
modified matrix expressed terms modified factors follows vw matrix ith row column removed jth row column removed 



expression shows product triangular matrices differs matrix accounted sherman morrison woodbury formula golub van loan page 
expression gamma vw gamma gamma gamma gamma gamma solution system written gamma gamma gamma gamma gamma computing formula main operations perform pairs triangular substitutions pair compute gamma compute gamma hot starts mpc problem usually encountered isolation 
contrary usually need solve sequence problems data starting point vary slightly problem 
highly desirable algorithms able take advantage fact 
information instance choose starting values variables guess active constraint set matrix 
process information called hot starting 
optimization model predictive control sequences similar linear quadratic problems arise control nonlinear systems 
apply sequential quadratic programming algorithm constrained version obtain search direction iteration solving problem 
solve slightly general problem data varies stage index linear terms may appear objectives constraints 
iterates converge solution nonlinear problem data matrices similar iteration 
starting guess best subproblem obtained approximating nonlinear problem current iterate 
excellent guess active set initial working set particularly iterations 
active set methods typically require just steps identify correct active set initial guess 
model predictive control gives rise sequence similar linear problems 
usual procedure solve problem current state system initial value apply control time point reached 
process repeated rawlings rawlings muske 
absence disturbances problems similar successive steps differing initial value excellent starting point obtained solution previous set setting xn new gamma gamma new un gamma gamma gamma gamma gamma new gamma gamma gamma solution components previous step new new initial state new new gamma chosen estimates final stages 
case active set approach excellent starting guess active constraint matrix situations particularly disturbances starting point chosen obvious techniques may feasible close solution 
represents problem infeasible interior point approach desirable algorithm iip initial complementarity comparable size initial infeasibilities 
active set method assumes feasible starting point 
remedy phase approach solve phase problem find feasible point phase ii problem find optimum 
second option introduce penalty terms objective infeasibilities obtain solution single phase provided heavy penalty imposed 
active set methods typically gain hot starting interior point methods reasons fully understood 
linear programming problems best interior point codes gain factor compute time hot started comparison cold prior information start 
relative savings simplex active set methods significantly higher 
difficult predict situation change consider problem class 
numerical testing way find 
acknowledgments supported mathematical information computational sciences division subprogram office computational technology research department energy contract eng 
anderson bai bischof demmel dongarra du greenbaum hammarling mckenney sorensen 

lapack user guide siam philadelphia 
grimes lewis 

accurate symmetric indefinite linear equation solvers 
preparation 
bertsekas 

projected newton methods optimization problems simple constraints siam journal control optimization 
dennis schnabel 

numerical methods unconstrained optimization prenticehall englewood cliffs nj 
dunn bertsekas 

efficient dynamic programming implementations newton method unconstrained optimal control problems journal optimization theory applications 
fletcher 

practical methods optimization second edn john wiley sons new york 
mehrotra 

solving symmetric indefinite systems interior point method linear programming mathematical programming 
garner wright 

critical fields josephson coupled physical review 
gill murray saunders wright 

inertia controlling methods general quadratic programming siam review 
golub van loan 

matrix computations nd edn johns hopkins university press baltimore 
jacobson mayne 

differential dynamic programming american elsevier new york 
wallace 

stochastic programming john wiley sons 
mehrotra 

implementation interior point method siam journal optimization 
impact computer science national research council 
dimensional model assimilation data national academy press 
polak 

computational methods optimization academic press new york 
rawlings meadows muske 

nonlinear model predictive control tutorial survey proceedings tokyo japan 
rawlings muske 

stability constrained receding horizon control ieee transactions automatic control 
rawlings 

constrained linear quadratic regulation technical report department chemical engineering university wisconsin madison 
rawlings 

infeasibilities model predictive control chemical process control cpc cache 
wild 

nonlinear regression john wiley sons new york 
vandenberghe boyd 

semidefinite programming technical report electrical engineering department stanford university stanford ca 
appear siam review 
wright 

interior point methods optimal control discrete time systems journal optimization theory applications 
wright 

path interior point algorithm linear quadratic optimization problems preprint mcs mathematics computer science division argonne national laboratory argonne ill appear annals operations research 
wright 

primal dual interior point methods 
book preparation 
see url www mcs anl gov home wright html 
