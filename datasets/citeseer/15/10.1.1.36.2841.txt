variational bayesian framework graphical models attias gatsby ucl ac uk gatsby unit university college london queen square london wc ar presents novel practical framework bayesian model averaging model selection probabilistic graphical models 
approach approximates full posterior distributions model parameters structures latent variables analytical manner 
posteriors fall free form optimization procedure naturally incorporates conjugate priors 
large sample approximations posteriors generally nongaussian hessian needs computed 
predictive quantities obtained analytically 
resulting algorithm generalizes standard expectation maximization algorithm convergence guaranteed 
demonstrate approach applied large class models domains including mixture models source separation 
standard method learn graphical model data maximum likelihood ml 
training dataset ml estimates single optimal value model parameters xed graph structure 
ml known tendency data 
tting severe complex models involving high dimensional real world data images speech text 
problem ml prefers complex models parameters data better 
ml optimize model structure 
bayesian framework provides principle solution problems 
focusing single model bayesian considers nite nite class models 
model posterior probability dataset computed 
predictions test data averaging predictions individual models weighted posteriors 
bayesian framework avoids tting integrating parameters 
addition complex models automatically penalized assigned lower posterior probability optimal structures identi ed 
unfortunately computations bayesian framework intractable term model refer collectively parameters structure 
simple cases factor analysis see 
existing approximation methods fall classes markov chain monte carlo methods large sample methods laplace approximation 
mcmc methods attempt achieve exact results typically require vast computational resources impractical complex models high data dimensions 
large sample methods tractable typically drastic approximation modeling posteriors parameters normal parameters positive de nite covariance matrices 
addition require computation hessian may quite intensive 
variational bayes vb practical framework bayesian computations graphical models 
vb draws variational ideas intractable latent variables models bayesian inference turn draw 
framework facilitates analytical calculations posterior distributions hidden variables parameters structures 
posteriors fall free form optimization procedure naturally incorporates conjugate priors emerge standard forms normal 
computed iterative algorithm closely related expectation maximization em convergence guaranteed 
hessian needs computed 
addition averaging models compute predictive quantities performed analytically 
model selection done posterior structure particular bic mdl criteria emerge limiting case 
general framework restrict attention directed acyclic graphs dags bayesian networks 
fy yn denote visible data nodes runs data instances fx xn denote hidden nodes 
denote parameters simply additional hidden nodes distributions 
model xed structure fully de ned joint distribution 
dag joint factorizes nodes pa pa set parents parametrize edges directed addition usually assume independent instances yn xn 
shall consider set structures controls number hidden nodes functional forms dependencies pa including range values assumed node number components mixture model 
associated set structures structure prior 
marginal likelihood posterior parameters 
xed structure interested quantities 
rst parameter posterior distribution 
second marginal likelihood known evidence assigned structure data 
usually omitted implied 
quantities obtained joint 
models hidden nodes required computations performed analytically 
presence hidden nodes quantities computationally intractable 
shall approximate variational approach follows 
consider joint posterior hidden nodes parameters 
intractable consider variational posterior restricted factorized form data parameters hidden nodes independent 
restriction key approximate tractable 
notice require complete factorization parameters hidden nodes may correlated 
compute optimizing cost function fm de ned fm log log inequality holds arbitrary follows jensen inequality see equality true posterior 
note understood include conditioning 
fm bounded marginal likelihood obtain optimal posteriors maximizing shown equivalent minimizing kl distance true posterior 
optimizing fm produces best approximation true posterior space distributions satisfying tightest lower bound true marginal likelihood 
penalizing complex models 
see vb objective function fm penalizes complexity useful rewrite fm hlog kl jj average rst term taken 
rst term corresponds averaged likelihood 
second term kl distance prior posterior parameters 
number parameters increases kl distance follows consequently reduces fm penalized likelihood interpretation transparent large sample limit parameter posterior sharply peaked probable value shown kl penalty reduces log linear number parameters structure fm corresponds precisely bayesian information criterion bic minimum description length criterion mdl see 
popular model selection criteria follow limiting case vb framework 
free form optimization em algorithm 
assuming speci parametric form posteriors fall free form optimization vb objective function 
results iterative algorithm directly analogous ordinary em 
step compute posterior hidden nodes solving fm get hlog xj average taken 
step optimal parameters compute posterior distribution parameters solving fm get hlog xj ix average taken 
concept conjugate priors useful 
denoting exponential term choose prior family distributions belongs family 
said conjugate 
procedure allows select prior fairly large family distributions includes non informative ones limiting cases compromise generality facilitating mathematical simplicity elegance 
particular learning vb framework simply amounts updating hyperparameters transforming prior parameters posterior parameters 
point conjugate priors widespread statistics far applied models nodes visible 
structure posterior 
compute exploit jensen inequality de ne general objective function fm log log 
computing fm structure posterior obtained free form optimization fm prior assumptions likelihood di erent structures encoded prior ect selection optimal model structures performed 
predictive quantities 
ultimate goal bayesian inference estimate predictive quantities density regression function 
generally quantities computed averaging models weighting model posterior 
vb framework exact model averaging approximated replacing true posterior variational 
density estimation example density assigned new data point 
situations source separation estimate hidden node values new data may required 
relevant quantity conditional value hidden nodes extracted 
vb approximates 
variational bayes mixture models mixture models investigated analyzed extensively years 
known problems regularizing likelihood divergences determining required number mixture components open 
theory bayesian approach provides solution satisfactory practical algorithm emerged application involved sampling techniques approximation methods problem 
solution provided vb 
consider models form yn yn yn denotes nth observed data vector denotes hidden component generated 
components labeled structure parameter denoting number components 
approach applied arbitrary models simplicity consider normal component distributions yn mean precision inverse covariance matrix 
mixing proportions hindsight conjugate priors parameters mixing proportions jointly dirichlet means conditioned precisions normal precisions wishart 
nd parameter posterior xed factorizes 
posteriors obtained iterative algorithm termed vb mog 
step 
compute responsibilities instance yn yn yn noting 
expression resembles responsibilities ordinary ml di erences stem integrating parameters 
special quantities log hlog log hlog ji log log log dx digamma function averages 
taken 
parameters described 
step 
compute parameter posterior stages 
compute quantities yn yn stage identical step ordinary em produces new parameters 
vb quantities help characterize new parameter posteriors 
posteriors functionally identical priors di erent parameter values 
mixing proportions jointly dirichlet means normal precisions wishart 
posterior parameters updated second stage simple rules nal values posterior parameters form output vb mog 
speci assumptions parameter posteriors emerge suitable non trivial generally non normal functional forms 
computational overhead vb mog compared em minimal 
covariance parameter posterior reduces em regularized priors 
vb mog divergence problems 
stability guaranteed existence objective function 
approximate marginal likelihood fm required optimize number components obtained closed form omitted 
predictive density 
posteriors integrate parameters show density assigned model new data vector mixture student distributions component mean covariance proportion 
reduces mog 
nonlinear regression 
may divide data vector input output parts model estimate regression function error spheres 
may extracted conditional turns mixture student distributions means linear covariances mixing proportions nonlinear terms posterior parameters 
buffalo post office digits misclassification rate histogram vb mog applied handwritten digit recognition 
vb mog applied boston housing dataset uci machine learning repository inputs predict single output house price 
random divisions dataset training test points resulting average mse 
discriminative method competitive breiman bagging technique regression trees mse 
comparison em achieved mse 
classi cation 
separate parameter posterior computed class training dataset test data vector classi ed conditional fy form identical dependent parameters multiplied relative size vb mog applied bu alo post oce dataset contains examples digit 
digit gray level pixel array see examples fig 
left 
random digit batches training separate batch testing 
average misclassi cation rate obtained components em achieved 
misclassi cation histograms vb solid em dashed shown fig 
right 
vb intractable models blind separation example discussion far assumed free form optimization vb objective function feasible 
unfortunately interesting models particular models ordinary ml intractable case 
models modify vb procedure follows specify parametric functional form posterior hidden nodes optimize parameters spirit 
parameter posterior fall free form optimization 
illustrate approach context blind source separation bss problem see 
problem described yn un xn unobserved dim source vector instance unknown mixing matrix noise un normally distributed unknown precision task construct source estimate xn observed dim data sources independent non normally distributed 
assume high kurtosis distribution cosh appropriate modeling speech sources 
important heretofore unresolved problem bss determining number sources data 
avoid tting mixing matrix 
problems typical ml algorithms remedied vb 
non normal nature sources renders source posterior intractable bayesian treatment 
normal variational posterior xn instance dependent mean precision 
mixing matrix posterior emerges normal 
simplicity optimized integrated 
resulting vb bss algorithm runs follows log pr snr db source reconstruction error application vb blind source separation algorithm see text 
step 
optimize variational mean iterating convergence xed point equation yn tanh source covariance conditioned data 
variational precision matrix turns independent step 
update mean precision posterior rules omitted 
algorithm applied dim data generated linearly mixing speech music signals obtained commercial cds 
gaussian noise added di erent snr levels 
uniform structure prior 
resulting posterior number sources fig 
left peaked correct value 
sources reconstructed test data 
log reconstruction error plotted vs snr fig 
right solid 
ml error includes model averaging shown dashed larger re ecting tting 
vb framework applicable large class graphical models 
fact may integrated junction tree algorithm produce general inference engines minimal overhead compared ml ones 
dirichlet normal wishart posteriors special models treated emerge general feature 
current research orts include applications multinomial models learning structure complex dynamic probabilistic networks 
matt beal peter dayan david mackay carl rasmussen especially zoubin ghahramani important discussions 
attias 

independent factor analysis 
neural computation 
bishop 

variational principal component analysis 
proc 
th icann 
chickering heckerman 

ecient approximations marginal likelihood bayesian networks hidden variables 
machine learning 
hinton van camp 

keeping neural networks simple minimizing description length weights 
proc 
th colt 
jaakkola jordan 

bayesian logistic regression variational approach 
statistics arti cial intelligence smyth madigan eds 
neal hinton 

view em algorithm justi es incremental sparse variants 
learning graphical models jordan ed 
kluwer academic press norwell ma 
richardson green 

bayesian analysis mixtures unknown number components 
journal royal statistical society 
saul jaakkola jordan 

mean eld theory sigmoid belief networks 
journal arti cial intelligence research 
waterhouse mackay robinson 

bayesian methods mixture experts 
nips touretzky eds 
mit press 
