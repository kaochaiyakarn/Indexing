stream computations organized reconfigurable execution score tutorial caspi michael chu randy huang joseph yeh andr dehon john wawrzynek berkeley brass group andre acm org primary impediment wide spread exploitation reconfigurable computing lack unifying computational model allows application portability longevity sacrificing substantial fraction raw capabilities 
introduce score stream computation organized reconfigurable execution compute model virtualizes reconfigurable computing resources compute storage communication dividing computation fixed size pages time multiplexing virtual pages available physical hardware 
consequently score applications scale automatically exploit wide range hardware sizes 
hypothesize score model ease development deployment reconfigurable applications expand range applications benefit reconfigurable execution 
believe engineered score implementation efficient wasting little capabilities raw hardware 
introduce key components score system 
large body evidence exists documenting raw advantages reconfigurable hardware fpgas conventional microprocessor systems selected applications 
reconfigurable computing remains limited popular primarily application specific domains replacement asics significantly reduced extended article appears fpl copyright springer verlag 
expanded version www cs berkeley edu projects brass documents score tutorial html august version rapid prototyping fast time market 
limited popularity due lack raw hardware capability gate devices readily available seen advances high clock rates rapid reconfiguration highbandwidth memory access :10.1.1.21.5165
believe limited applicability reconfigurable technology derives largely lack unifying compute model away fixed resource limits devices restrict software expressibility longevity device generations 
existing targets non portable 
software reconfigurable hardware typically tied particular device set devices limited source compatibility binary compatibility vendor specific family devices 
program bigger devices alternatively smaller cheaper lower power device typically requires substantial human effort 
best requires potentially expensive pass mapping tools 
worst requires significant rewrite fully exploit new device features sizes 
contrast program written microprocessor systems automatically run benefit additional resources isa compatible device recompilation 
existing targets expose fixed resource limitations 
exposure fixed resource limitations existing programming models tends impair expressiveness broad applicability 
programming models application choice algorithm spatial structure restricted size available hardware 
furthermore computation structure size fixed compile time allowance dynamic resource allocation 
algorithms data dependent structures potentially unbounded resource usage easily mapped reconfigurable hardware 
virtualize resources score compute model introduced addresses issue fixed resource limits virtualizing computational communication memory resources reconfigurable hardware 
fpga configurations partitioned fixed size communicating pages analogy virtual memory loaded hardware demand 
streaming communication pages simultaneously hardware may transparently buffered memory 
scheme allows partitioned program run arbitrarily physical pages automatically exploit available physical pages recompilation 
proper hardware design scheme permits binary compatibility scalability architectural family page compatible devices 
convenient efficient model software benefit additional physical resources pages programming model expose page level parallelism permit spatial scaling 
score programming model natural abstraction communication occurs spatial hardware blocks 
data flow communication graph captures blocks computation operators communication streams 
captured exploit wealth known techniques efficiently mapping computational graphs arbitrary sized hardware 
furthermore run time composition graphs supported enabling data driven program structure dynamic resource allocation integration separately compiled developed library components 
section discusses systems compute models influenced formulation score 
section presents key components score model 
section discusses hardware requirements score implementation reasonable today technology 
section gives brief programming constructs score 
section show execution sample section describes basic architecture current implementation score run time system 
section shows results jpeg encoder score example early experience implementing score system 
data dependent computational structures constructed specialization recompilation requires complete pass mapping tools 
related technique time multiplexing large spatial design small reconfigurable system demonstrated 
hand partitioning particular design motion wavelet video coder graph fpga sized pages manually reconfiguring device pages able run design third devices physical pages originally required performance overhead 
key approach efficiency amortize cost reconfiguration having page process sizable stream data buffered memory reconfiguring 
score aims automate partitioning efficient dynamic reconfiguration performed manually 
ease success automation depends appropriate models program description dynamic reconfiguration 
regard score builds prior art developing isa data flow distributed streaming computation models 
remainder section discuss relation score prior models 
isa models attempts define compute model reconfigurable computing devices focussed augmenting traditional processor isa reconfigurable instructions 
prisc ra allowed definition single cycle programmable function unit pfu operations tlb management replacement scheme virtualize space pfu instructions exploiting local dynamic reuse pfu instructions 
size fixed architecture constrained sequential isa execute sequentially 
model directly allow architecture scale exploit additional parallel hardware 
disc garp expand prisc model allow variable size multiple cycle array configurations 
architectures pack multiple configurations instructions available array case garp support implementation dependent number cached array configurations 
prisc array configuration smaller available physical logic reconfigurable instructions composed sequentially isa 
consequently architectures prevent scaling array size automatically exploiting additional parallel hardware 
expands isa extension model allowing operations memory memory isa 
sequential isa computing model potentially facilitates multiple parallel 
long rfuop operates independent memory banks rfu operations interlock may proceed parallel 
technique exposes memory buffers pipelined operations 
forces user compiler pick blocking factors schedule blocked operations parallel processor instruction dispatch window 
fact approach prevents direct pipeline assembly chained operations 
furthermore isa forces compiler schedule invocation rfu operations limiting opportunity schedule components reconfigurable computation data driven manner 
dynamic reconfiguration ling describe multi processor scalable fpga architecture partitions time multiplexes large applications fpga sized pages score 
primary limitation performance page communication buffered small fixed set device registers token router 
small communication buffer page operate short time available inputs output space page time multiplexed triggering reconfiguration 
running design larger available hardware execution time may dominated reconfiguration time 
proposes similar demand paged reconfigurable system arbitrary sized swappable logic units communicate periphery registers subject inefficiency 
score avoids inefficiency allowing large unbounded communication buffers enabling longer page execution reconfigurations 
cmu piperench defines reconfigurable fabric paged horizontal stripes communicate vertically pipeline :10.1.1.21.5165
execution model fully virtualizes stripes enables hardware scaling number physical stripes 
stripes communicate input output registers piperench pipelined reconfiguration scheme hides excessive reconfiguration overhead seen 
sequential reconfiguration scheme suited simple feed forward pipelines 
scheme support computation graphs feedback loops may waste available parallelism squeezing wide graphs linear sequence stripes 
particular virtualizing computation parallelism available single architected stripe non communicating stripes simultaneously fit hardware parallel harness model arranged mesh communicate nearest neighbors periphery registers 
data parallel sea accelerators model communicate incur virtualization overhead discussed 
load sequence incurring added latency area cost buffering stripe score restrictions execution order allowing parallel reconfiguration physical pages 
data flow original dennis formulation data flow described processor isa represented data flow graphs directly instruction operator 
execution model included single result register instruction allowing instruction execute time successors execute 
restriction instruction ordering reasonable microprocessor large instruction store fast instruction issue available reasonable reconfigurable device reconfiguring instruction issue costly 
hybrid data flow berkeley tam define operators straight line blocks instructions relaxing frequency inter instruction synchronization entry exit points blocks 
models inherit problem fixed communication buffers dennis data flow face inefficiency time multiplexed reconfigurable implementation 
streaming formulations data flow remove limitation fixed input output buffers allowing arbitrarily tokens queue arc data flow graph 
generalization allows time multiplexed implementation fire operator times succession reconfiguring amortizing cost reconfiguration large data set 
lee synchronous data flow sdf incorporates streaming restricted case static flow rates 
model computation lacks data dependent control flow guarantees conforming graphs statically scheduled run bounded stream buffers 
buck integer controlled data flow idf incorporates data dependent control flow adding sdf set canonical dynamic rate operators switch select 
score permits dynamic rate model allowing data dependent control flow inside operator 
score programs essentially equivalent idf expressiveness score operator equivalent idf graph containing dynamic operators 
score shares gross similarity heterogeneous systems streaming data flow tie arbitrary processors conventional special purpose reconfigurable including mit berkeley 
programming model systems restricted score typically pre defined set streaming operations 
furthermore score provides stronger model allowing pages processors swapped needed hiding implementation limitations buffer sizes 
streaming apis virginia tech defines streaming api networks reconfigurable devices 
standardizing form applications may written api virtualize size fabric compute resources allow definition portable scalable designs 
serves hardware interface layer manually reconfigure devices network 
maya gokhale defines streaming programming model 
score streams exploits fact reconfigurable hardware efficiently organized collection spatial pipelines streams provide natural abstraction hardware linkage separate design components 
streams serves convenient way compactly describe spatial designs 
virtualization performed burden handling placement fixed buffer size restrictions placed entirely programmer 
regards score attempts provide higher level programming model providing semantics decoupled hardware artifacts buffer sizes physical hardware size automatically filling lower level details compile run time 
csp ways score computational model similar hoare communicating sequential processes csp 
score operator viewed single process 
operators communicate designated stream connections somewhat csp named ports 
csp ports score streams buffered offer unbounded stream abstraction 
significantly score operators csp processes allowed non deterministic 
composition score operators yields deterministic observable results 
fairness csp non determinism facilitate modeling unpredictable dynamic effects real systems score modeled top csp 
score allows dynamic construction computational graphs original csp formulation course added 
score computational models compute model defines computational semantics developer expects physical machine provide 
compute model captures essence computation proceeds defining meaning computation 
compute model concrete embodiment programming models 
programming model provides high level view application composition execution adding number practical conveniences programmer 
ultimately models grounded execution model defines way computation described physical hardware meaning associated description 
execution model programming model computational model consistent views computation 
differs level detail expose see 
execution model abstracts number key resources alus pages allow scaling different hardware platforms 
programming model abstracts architectural characteristics execution model isa details limited resource sizes exposed architectural level 
compute model abstracts away concrete syntax primitives provided particular programming language system 
compute model score computation graph computation nodes operators memory blocks linked streams 
streams provide node node communication simply single source single sink fifo queues unbounded length 
graph nodes operators forms finite state machine fsm nodes interact rest graph stream links turing complete tm nodes support resource allocation addition stream operations 
score fsms property state identifies set inputs read input streams 
full set inputs fsm consumes inputs appropriate set input fifos may conditionally emit outputs close input output streams 
standard fsm score fsms transition new state inputs state 
score fsm distinguished done state may enter signal completion remove running computation 
score tm node similar score fsm node adds ability allocate memory create new graph nodes fsm tm operators edges streams score compute graph 
memory allocated finite sized blocks called segments 
segment may owned single operator time 
score tm may allocate new segments pass fsm tm node creates 
termination tm fsm node enters done state compute model programming model execution model model capturing essential semantics computation particular set programming constructs providing convenient way express computations compute model programming model abstracted certain details arise execution model architectural page size number registers 
low level executable description computation semantics hardware expected provide interpreting description execution model abstracted certain hardware size details number resources 
sequential execution unix mips isa single global memory unix abi isa score tdf mips isa linux linux abi score lut cps score mb segments csp occam transputer isa id tl sparc am solaris spmd sparc isa cm runtime solaris vector vectorized isa sunos abi sdf ptolemy graphs tms levels abstraction computational model computational compute programming execution element model model model hardware compute score fsm operator page physical compute page cp physical network communication stream stream stream cmb main memory data storage segment segment segment cmb main memory score computational elements various levels abstraction returns ownership received segments back operator created 
operator attempts access memory segment presently access blocked operator stalls operator ownership memory segment 
operational semantics score compute model fully deterministic 
follows determinism individual operators timing independent communication discipline fact operators side effect state 
particular operators communicate streams token flow semantics guarantee order execution memory segments single unique owner time suffer multiple accessor read write ordering hazards 
observable results score computation completely independent timing operator delay stream 
appendix defines compute model precisely 
programming model programming model gives programmer framework describing computation manner independent device limits guidelines efficient execution hardware implementation 
execution model compiler take care translating higher level description provided programmer details needed execution 
key abstractions score programming model operators streams memory segments 
basic components operators operator represents particular algorithmic transformation input data produce output data 
operators computational building blocks computation multiplier fir filter fft 
operators may behavioral primitives hierarchical graph compositions operators 
shows example video processing operator composed pipeline transformations including motion estimation operator image transformation operator data quantization operator coding operator 
size operator hardware implementation dependent way limited programming model 
operators may need partitioned fit architectural compute page 
partitioning integral part automated compilation process 
streams inter operator communication uses streaming data flow discipline 
programmer needs connect oper links producer consumer operator stream link 
link serves define data logically routed acts queue data tokens 
operators signal producing data need consume data 
signalling translates data presence signals stream links synchronize communication operators 
memory segments memory segment contiguous block memory serves basic unit memory management 
memory segments may size architecturally defined maximum 
memory segment may score computation giving specific operating mode sequential read random access read write fifo appropriate stream interface linking data flow graph operator see 
dynamic features top basic components score supports number important dynamic features 
dynamic rate operators dynamic graph composition instantiation dynamic handling uncommon events dynamic rate operators operator may consume produce tokens rates 
expressive power allows score describe efficient operators tasks data compression decompression searching filtering 
section shows possible set linguistic constructs supporting dynamic rate consumption production 
exploit dynamic rates scheduling decisions run time dynamic rates actual data availability known 
dynamic composition instantiation score allows run time instantiation operators data flow graphs 
computational graph may created extended modified execution 
extending graph means creating new graph nodes edges may defined data dependent manner 
operating node may terminate execution existing stream links may shut attached operators 
mechanism benefits describing computation strictly static graph compile time 
gives programmer opportunity postpone avoid allocating resources parts computation immediately resource requirements bound run time 
enables creation data dependent computational structures motion estimation transformation stance exploit dynamically unrolled parallelism 
creates framework aggressive implementations may dynamically specialize operators instantiation parameters 
operator may parameters bound instantiation time operator composed data flow graph 
mechanism allows operators initialized unchanging slowly changing scalar data specialized parameter values 
examples section show set linguistic constructs support composition instantiation 
exception handling exception handling falls naturally data flow discipline score 
unusual condition occurs operator may raise exception 
point operator stops producing output data 
dependent downstream operators may stall waiting operator resume produce output data flow disciplines guarantees wait properly operator handle exception produce result 
exception handled raising operator resumes operation producing data allowing downstream operators resume turn 
execution model key idea computer architecture defines computational description machine run semantics running isa popular architectural definition processors 
building conforming device free implement detailed computer organization reads executes computational description pentium different implementations run computational description 
technique execution model score defines run time computational description architecture family semantics executing description 
quantization video processing operator coding score execution model defines computation terms key components compute page cp fixed size block reconfigurable logic basic unit virtualization scheduling 
memory segment contiguous block memory basic unit data page management 
stream link logical connection output page cp segment processor input page 
stream implementations physically bounded execution model provides logically unbounded stream abstraction 
computational description execution model independent size reconfigurable array admitting architectural implementations large number compute pages memories 
model provides semantics unlimited number independently operating physical compute pages memory segments 
compute pages segments operate stream data tagged input presence produce output data streams similar manner 
data presence tags provides operational semantics independent timing particular score compatible computing platform 
fixed compute page sizes compute pages basic unit virtualization scheduling reconfiguration relocation 
analogy virtual memory page compute page minimum unit hardware mapped physical hardware managed atomic entity 
compute page represents fixed size piece reconfigurable hardware luts 
compute pages differ operators compute model pages architecturally imposed resource limitations size maximum number streams 
decomposition computation compute pages takes stand feasible desirable manage primitive computational building block const les add les example page decomposition original operator seen programming model mapped logic elements les target architecture partitioning le pages match execution model page size final graph execution model lut independent entity just generally desirable manage bit memory independent block 
grouping larger block resources management overhead amortized larger number computational blocks 
grouping allows hard problems placement routing performed offline page 
note necessary page size fixed architecture family family member run run time binary description 
page re packing placement routing need performed online 
fixed page discipline requires compilers partition pack computational operators fixed size pages 
shows example decomposition operator graph pages 
compute pages may contain internal state saved restored page swapped physical compute page 
swapping may necessary time multiplexed implementation key supporting semantics unbounded number compute pages 
memory segments configurable memory blocks memory segment contiguous block memory managed single atomic memory block purposes swapping relocation 
memory segment may modes sequential read random access read write fifo 
configured segment segment data flow computation graph compute pages segments particular mode segment logical stream ports connect graph pages streams random access address input data input data output control input 
shows example graph connecting pages segments 
memory segment run time system map configurable memory block cmb 
cmb physical memory block inside reconfigurable array see example active stream links interconnect connect memory segment active computation 
addition holding user specified segments hold segments containing cp configurations segments containing cp state segments associated stream buffers 
single cmb may hold number types segments long aggregate memory requirement exceed cmb capacity see sample memory layout cmb 
current vision single segment may active cmb point time score definition prevents implementation designed handle multiple active segments cmb 
physically finite logically unbounded streams streams form data flow links pages 
page cp segment indicates producing valid data output band data bit 
valid data value associated presence bit termed token 
token transported destination input consuming operator 
stream delivers data items generated producer order consumer storing consumer indicates consumed head input queue see 
data presence tag token serves similar role stall signal conventional virtual memory cache architecture lets processing unit know data available continue processing processing unit wait data arrive 
stream empty downstream operator stall waiting input data 
discipline hides detailed timing operations programming model guaranteeing correct behavior allowing variations implementations computing architecture 
run time level streams provide abstraction unbounded capacity links producers consumers 
practice streams finite implementation dependent buffer capacity 
implement semantics unbounded fifo stream links implementation backpressure see essary 
see appendix discussion unbounded buffers nec stall production data items run time system allocate additional buffer space fifo segments needed see example stream buffer expansion 
physically virtual stream may realized ways producer consumer virtual stream loaded physical hardware stream link implemented spatial connection inter page routing network pages 
see 
ends stream resident stream data sourced stream buffer segment active cmb component 
see 
allows efficient pipelined chaining connected operators space permits deep intermediate data buffering computation sequentialized 
hardware virtualization compute pages segments streams fundamental units allocation virtualization management hardware resources 
run time operating system manager schedules virtual pages streams available physical resources including page assignment migration inter page routing 
physical resources page computation graph may simultaneously loaded reconfigurable hardware enabling maximum speed computation 
shows case video processing operator 
hardware resources limited computation graph time multiplexed hardware 
streams virtual pages simultaneously loaded transparently buffered chip memory 
shows case video processing operator 
component operator loaded hardware sequence input memory buffer producing output 
model implications advice programmers goal compute model high level focus developer style computation efficient hardware execution model 
better utilize scalable reconfigurable hardware score developers describe computations spatial pipelines multiple independent computational paths 
hardware implementation choose implement link statically configured path fpgas time switched path dynamically routed path 
ctrl data addr base bound mode read memory cmb data config 
segment state user segment user segment fifo unused segments data mapped cmb source page backpressure data data data transmitted network stages stream signals full asserting backpressure input queue active segment sink page 
op sends data op cp data 
data sent cp cp cp 
runtime creates new links segment segment cp cp 
segment serves large buffer cp cp 
op full cp cp data buffer segment cp 
runtime allocates new segment buffer stream cp 
op sends data stream buffer segment cp cp 
runtime may decide replace op cp cp full backpressure 
backpressure stops op sending data cp cp cp 
runtime direct connection cp cp 
op take data takes segment cp 
op may continue data stream buffer expansion finite stream buffer provide unbounded stream buffer semantics cp buffer motion estimation transform quantize code fully spatial implementation video processing operator score hardware buffer motion buffer buffer transform buffer buffer code estimation swap swap swap buffer cmb cp buffer quantize capacity limited temporal implementation video processing operator implementation attempt concurrently execute specified parallel paths possible 
avoid minimize feedback cycles 
cyclic dependencies introduce delays pipelined away increase total run time lead small hardware implementations 
expose large data streams score operators 
large data sets help amortize overhead loading computation reconfigurable hardware especially small time multiplexed hardware implementations 
generality described score hardware model terms single processor homogeneous computational pages memories model admits number extensions 
score accomodate heterogeneous specialized computational pages seen 
specialized pages efficiently scheduling problem interesting operators may run multiple kinds specialized pages 
prohibits score multiple conventional processors executing sequential operators run time scheduler 
conventional techniques multiprocessing distributed scheduling relevant case 
hardware requirements score assumes combination sequential processor reconfigurable device 
reconfigurable array divided number equivalent independent compute pages 
multiple distributed memory blocks required store intermediate data page state page configurations 
interconnect pages critical achieving high performance supporting run time page placement 
support high bandwidth low latency communication compute pages memory allowing memory pages concurrently 
interconnect buffer pipeline data provide back pressure signals stall upstream computation network buffer capacity exceeded 
routing resources sufficiently rich facilitate rapid online routing 
compute pages may reconfigurable fabric supports rapid reconfiguration provision save restore array state quickly 
brass subarray design feasible concrete implementation compute page 
provides microsecond reconfiguration high speed pipelined computation 
configurable memory block cmb selfcontained unit stream memory port address generator see 
may accessed independently concurrently scalable system 
memory fabric may external ram onchip memory banks brass embedded dram additional logic tie data flow synchronization interconnect network 
memory controllers need support simple paged segment model including address relocation memory block segment bounds 
streaming data support obviates need external addressing reconfiguration stream buffering 
sequential processor plays important part score system 
runs page scheduler needed virtualize computation array executes score operators run efficiently reconfigurable implementation 
consequently processor able control communicate array efficiently 
single chip score system see integrating processor reconfigurable fabric memory blocks provide tight efficient coupling components 
single chip score implementation offers benefits performance design efficiency score model permits wide range implementations including conventional commercial components 
degenerate case page sacrifices strengths score model 
cmb cp cp cmb network cmb cp cp cmb hypothetical single chip score system language instantiations computational model number languages obey score semantics defined describe score computations 
define subsets conventional hdls verilog vhdl stylized input output primitives describe score operators operator composition 
similarly define subsets conventional programming languages java perform tasks 
focus necessary semantics defined intermediate level language rtl describe score operators composition initial development 
view intermediate language tdf device independent assembly language target way architecture specific executable operators 
score language requirements indicated semantics score compute model score operators synchronous single clock entities state 
operators communicate designated streams 
operation gated data presence streams 
operator viewed finite state machine associated data path 
multithreaded language java appropriate thread package score operator independent thread communicates rest program single writer streams 
specifically score global shared memory abstraction operators 
operator may chunk address space memory segment operation return completed operators may fir param signed param signed param signed param signed param bound instantiation time input unsigned output unsigned state fire notation picks nth previous value input stream 
notation patterned goto loop state tdf specification tap fir static rate operator piece memory 
tdf tdf basically rtl description special syntax handling input output data streams operator 
common data path operators described syntax 
example shows fir computation implemented tdf 
operators may parameters values bound operator instantiation time parameters identified keyword param 
fir example coefficient weights parameters specified operator created values persist long operator 
fir defines single input stream produces single output stream 
behavior state gated arrival value producing input 
allow dynamic rate operators basic form behavioral tdf operator finite state machine 
state specifies inputs fire 
inputs arrive operator consumes inputs fsm may choose change states data consumed inputs 
simple merge operator shown demonstrating state machine allow data dependent consumption input values 
output value production conditioned shown 
allow user specify arbitrary deterministic operators 
course fsm gives user semantic power describe heavily sequential complex control oriented version simplified illustration properly handle stream condition 
signed merge param unsigned parameters define data width input signed input signed signed signed states show dynamic data consumption state start merge goto merge goto note assignment function name signifies output operator return output stream state merge goto merge goto state merge goto merge goto tdf specification merge operator dynamic input rate operator uniq behaves unix command name filters input stream removing adjacent duplicate entries passing output stream 
signed uniq param unsigned input signed signed state start uniq goto loop state loop uniq goto loop tdf specification uniq operator dynamic output rate operator merge uniq param unsigned input signed input signed input signed output signed signed merge merge uniq tdf compositional operator operators 
programmer avoid sequentialization complex control possible operator states spatial computing resources efficiently 
larger operators composed smaller operators straight forward manner shown 
integration composition suitable stream implementation interface code score operators instantiated conventional multithreaded programming language 
shows example program uses merge uniq operators defined 
note score operator instantiation composition performed code 
created score operators behave independently running threads operating parallel main execution thread 
general score operator run input streams closed output streams freed 
primitive behavioral leaf operators defined tdf suitable form compiled page level implementation large programs composed entirely programming language shown 
thinks tdf portable assembly language critical computational building blocks language binding allows high level language compose building blocks way assembly language kernels composed high level languages order efficiently program early dsps supercomputers 
instantiation parameters tdf operators allow definition generic operators highly customized needs application 
execution example example demonstrates execution design 
shows array compute page reconfiguration execution scheduled behavioral code fundamental control signals 
ground explanation particular hardware configuration constraints assumptions reconfigurable array parameters tdf design user application design consists behavioral operators 
full implementation operator requires compute page 
reconfigurable array contains compute page cp configurable memory blocks 
include score include merge include uniq int main char data char data char data declare streams signed score stream create bit wide input streams new signed score stream new signed score stream new signed score stream instantiate operators note instantiation passes parameters streams score operators merge merge uniq alternately new merge uniq write data streams demonstration purposes real streams longer probably come main int stream write data stream write data stream write data stream close close input stream close streams stream close output results demonstration purposes int cnt stream eos cnt cout result cnt stream read endl stream free return merge merge uniq instantiation usage example cmb partitioned segments 
segments buffer computation data 
example capacity tokens 
segments store state fifo buffers state machines internal registers configuration compute page 
cmb state maintained controller details shown example 
cp input output fifo buffers 
example clear size buffers set zero 
scheduling array reconfiguration performed timeslice 
refer timeline shown execution event sequence 
table shows physical view array point timeline 
diagrams easier read single letter identifiers assigned follows operators design merge inputs merge inputs uniq 
contents segments identified stream variable name program listing tokens buffered 
horizontal empty full bar indicates qualitatively number tokens segment point time assuming full segment capacity tokens 
score run time environment section describe pragmatics associated current run time architecture tools tdf language processing code generation 
details part basic score definition may help understand score better providing particular concrete grounding 
particular section fills details design shown execution example demonstrated table 
building applications score run time system user applications implemented linux processes shown 
compilation linking process user application shown 
tdf compiler processes tdf sources corresponding files files describe parameters operator instance 
example operator uniq param unsigned input signed file uniq contains uniq instructing produce code instance operator operates bit data path 
table step step execution example time physical array view description ii iii continued page cp cmb configuration active seg mode mode fifo fifo 
logic fsms fifo 
conf st fifo cp cmb configuration mode run active seg mode fifo fifo logic fsms fifo 
conf st fifo cp cmb configuration mode stall active seg mode fifo fifo logic fsms fifo 
conf st fifo empty cp cmb configuration active seg mode mode fifo fifo logic fsms fifo 
conf st fifo empty cmb active seg mode 
conf st cmb active seg mode conf st cmb active seg mode conf st cmb active seg mode conf st cmb active seg mode conf st cmb active seg mode 
conf st cmb active seg mode 
conf st cmb active seg mode 
conf st initially assume contents streams andi loaded main processor segments cmb cmb cmb 
addition configuration initial state pages operators loaded segments cmb cmb cmb respectively 
reconfiguration 
page merge scheduled run timeslice 
cp configured contents cmb 
streams setup cp shown diagram 
array status 
cp running behavioral code operator merge 
cmb controller set appropriate active segment operation mode cmb 
diagram cmb cmb act sequential source cmb sequential sink relative connected streams 
time approximately half tokens consumed sources cmb cmb sunk cmb 
timeslice 
array status 
tokens sources cmb cmb consumed cp sunk cmb 
source node stream producing tokens empty segment cmb sink node stall due unavailability input tokens cp stalled operator requires tokens input fire 
diagram streams identified empty 
scheduler uses information empty streams optimize schedule timeslice 
reconfiguration 
cp reconfiguration consists logically sequential steps parallelized array implementation permits 

save current configuration state cp cmb allocated 
load configuration state page cp cmb 
cp configured streams created compute nodes shown diagram 
cp ready run 
continued previous page time physical array view command description iv vi continued page cp cmb configuration active seg mode run mode fifo fifo logic fsms fifo 
conf st fifo cp cmb configuration mode stall active seg mode fifo fifo logic fsms fifo conf st fifo empty full cp cmb configuration mode active seg mode fifo fifo logic fsms fifo conf st fifo cp cmb configuration active seg mode run mode fifo fifo logic fsms fifo conf st fifo cmb active seg mode 
conf st cmb active seg mode 
conf st empty cmb active seg mode 
conf st cmb active seg mode 
conf st cmb active seg mode 
conf st cmb active seg mode conf st cmb active seg mode conf st cmb active seg mode 
conf st array status 
cp running behavioral code operator merge 
approximately half tokens cmb cmb consumed cp sunk cmb 
second timeslice 
array status 
tokens sources cmb cmb consumed cp sunk cmb 
sink node stream consuming tokens full cmb source node stall stream write 
diagram streams identified full 
scheduler uses information full streams optimize schedule timeslice 
reconfiguration 
main steps reconfiguration similar time iii 
cp loaded configuration state page uniq 
array status 
cp running behavioral code operator uniq 
approximately half tokens cmb consumed cp sunk cmb 
timeslice reconfiguration page execution run run run ii iii iv vi vii timeline execution example continued previous page time physical array view command description vii cp cmb configuration mode stall active seg mode fifo fifo logic fsms fifo conf st fifo empty cp cmb configuration active seg mode mode fifo fifo logic fsms fifo conf st fifo cmb active seg mode conf st cmb active seg mode conf st cmb active seg mode 
conf st cmb active seg mode 
conf st time third timeslice 
array status 
tokens cmb consumed cp sunk cmb 
reconfiguration 
current configuration state cp saved cmb 
note application demonstrated saving configuration state cp necessary 
scheduled runs cp timeslice state longer needed 
saving configuration state shown completeness 
pages scheduled run non consecutive state saved time preempted restored scheduled 
analogous context switching traditional operating systems 
produce master files merge cc uniq cc parameterized instance files merge cc uniq cc 
step compile sources including driver code main generated sources 
build process terminates driver code linked master files produce user application executable instance objects linked run time system libraries produce dynamically linked shared object libraries merge uniq containing instance code 
purpose process clear section describe score runtime environment detail 
run time environment run time system consists scheduler simulator processes execute linux shown 
real system os kernel contain scheduler reconfigurable hardware array replace simulator 
components connected pair streams permit bidirectional communication transmit scheduler commands resource state array 
scheduler consists instantiation scheduling engines 
instantiation engine 
independent process scheduler knowledge user applications compute graphs 
run time system shared object files built user application provide way communicate structure compute graphs user application scheduler 
invocation user application places series requests scheduler instantiate compute graph nodes 
accomplished code master files produced linked user executable 
code contains sequence operations connect scheduler ipc channel request instantiate operator 
example code routine requests instantiation merge operator andi 

request scheduler receives pointer shared object file contains behavioral code attributes parameterized instance operator 
run time system dynamically links shared object file merge scheduler instantiates operator places waiting list scheduled 
note shared object necessary order get user application code loaded address space scheduler course built knowledge user code asked run 
array simulator executes behavioral code resident compute node 
scheduling engine 
scheduling engine invoked timeslice responsible resource allocation utilization placement routing array 
acts resource manager capable enforcing variety policies fair sharing compute resources multiple user applications favoring particular application meet real time constraints 
array simulator 
simulator provides cycle accurate simulation executing compute node behavioral code corresponding dynamically linked shared object files merge 
noted earlier communicates scheduler pair streams 
implemented shared memory streams provide direct communication user application array simulator 
example streams ando 
example jpeg described previous section implemented complete score run time system simulator top linux develop applications guide understanding critical design issues systems 
early exercise demonstration vehicle implemented complete jpeg joint photographic experts group image compression algorithm tdf performed basic scaling experiments vary number computational pages system 
application jpeg compressor mathematically decomposes input data high low frequency components 
image segmented pixel blocks decomposition performed individual block dct discrete cosine transform unitary transform takes pixel block input returns block coefficients close zero 
coefficients scalar quantized scanned dimensional stream zigzag scan 
quantized coefficients subsequently compacted zero length encoding runs lengths huffman encoded 
see 
tdf implementation uses lut pages order realize fully spatial jpeg compressor capable processing image sample cycle 
dct address data address data linux user processes score runtime user application scheduler simulator ipc array hardware instantiate cp merge start cp instantiate merge pc merge start cmb cmb uniq cp 
schedule merge uniq link runtime merge shared mem array status streams score run time structure interaction user application write seg segment segment quant cmb cmb read huff read read read large numbers computational blocks indicate number lut pages required 
jpeg data flow including page segment decomposition bitstream assembly main merge uniq main ld merge tdf uniq tdf merge cc uniq cc merge uniq master instance merge cc uniq cc merge uniq ld shared merge uniq note operator described separate tdf source file 
tools tdf compiler standard compiler standard linker capability building stand executables shared dynamically linked libraries 
build process user application fig 

simulator parameters value assumed reconfiguration time cycles schedule time slice cycles compute page cp size luts configurable memory block cmb size mbits external memory bandwidth gb table system parameters experiment smaller hardware score scheduler automatically manages run time reconfiguration necessary share physical cps virtual cps 
system assumptions experiments assume single chip system described section external memory needed application 
table summarizes parameters assume system experience embedded dram memory 
experiments page decomposition performed manually 
scheduler list operates fashion conventional operating system scheduler scheduler takes care decisions place cps manages reconfiguration data transfer including data movement component necessitated finite chip memory capacity 
assume scheduling time overlapped computation takes cycles 
currently model limitations pages 
simulator accounts time required reconfigure pages store state transfer data memories chip 
results study scalability performance efficiency score ran jpeg implementation series simulated architecture compatible score systems varying numbers physical compute pages 
plots total run time makespan system versus number physical pages system 
particular experiment scale memory results shown reflect fixed memory unlimited memory 
comparison show native mmx implementation intel ijpeg library 
curves demonstrate score automatically run jpeg application hardware graceful performance degradation 
score automatically realize area time performance tradeoff 
curves show application automati total time makespan millions cycles jpeg encode performance pentium ii mmx score score unlimited physical compute pages cally virtualized half compute pages implementation incurring substantial performance penalty 
kind result common load compute operators vary widely lightly loaded operators time share compute page increasing runtime 
experiment exhibits anomalies scheduler 
cp makespan curves strictly monotonic due heuristics list page selection approach 
scheduler optimized minimize memory usage buffering streams 
fact possible scale number physical cps small hardware virtualize streams pages 
experiment assumes fixed memory availability twice number cps application 
factor effect unoptimized stream buffering performed experiment unlimited memory 
results exhibit speedup twofold limited memory case suggesting room improvement scheduling memory management 
experiment represents single set score system parameters 
ongoing exploring system parameters gain insight regions operation score scheduling robust determine parameters provide efficient jpeg cp vs makespan balanced system design 
parameters include compute page size page bandwidth memory block size reconfiguration times 
summary reconfigurable computation defined simply computation performed collection fpga fpga hardware shown remarkable promise point applications achieved wide spread acceptance usage 
large commitment particular fpga system develop application 
readily predict industry produces newer larger faster hardware steady pace 
unfortunately unifying computational model transcends particular fpga implementation application developed stuck redoing significant port application newer hardware 
particularly onerous established alternative technology microprocessor offers users steady performance improvements little time investment adapt new hardware 
overcoming liability requires computational model abstracts computational resources allowing application performance scale automatically adapting new hardware available 
computa tional model expose exploit strengths reconfigurable hardware help users understand optimize applications reconfigurable execution 
computational model allow problems deal efficiently dynamic unbounded resource requirements dynamic program characteristics 
model support efficient composition solutions building blocks 
introduced particular computational model attempts address needs 
score uses paging model virtualize hardware resources including computation storage communication 
allows dynamic instantiation dynamically sized computational operators supports dynamic rate applications 
page partitioner compiler run time scheduler takes care automatically mapping unbounded dynamically unfolding computational graph fixed resources particular hardware platform 
outlined hardware requirements model kind programming languages needed describe integrate score computations 
implemented complete score run time system simulator 
initial experiments suggest achieve desired scalability sample applications 
initial success attempting broaden range applications automate score tool flow systematically explore design space score compatible architectures 
research part berkeley reconfigurable architectures software systems brass effort supported defence advanced research projects agency contract number dabt california micro program 
arthur jan rabaey 
ultra low power domain specific multimedia processors 
proceedings ieee vlsi signal processing workshop october 
altera orchard parkway san jose ca 
apex device family march 
www altera com html products apex html 
bhattacharyya praveen murthy edward lee 
software synthesis dataflow graphs chapter synchronous dataflow 
kluwer academic publishers 
vincent michael bove jr john 
reconfigurable data flow system video processing 
ieee transactions circuits systems video technology april 
wad www media mit edu people wad html 
gordon 
virtual hardware operating system xilinx xc 
proceedings th international workshop field programmable logic applications fpl pages 
gordon 
logic unit paradigm virtual hardware 
proceedings th ieee symposium fpgas custom computing machines fccm pages april 
joseph buck 
scheduling dynamic dataflow graphs bounded memory token flow model 
phd thesis university california berkeley 
erl technical report 
stephen brian robert parker 
distributed architecture adaptive computing 
proceedings ieee symposium field programmable custom computing machines fccm pages april 
david culler seth goldstein klaus schauser thorsten von eicken 
tam compiler controlled threaded machine 
journal parallel distributed computing june 
jack dennis 
data flow supercomputers 
computer november 
jack dennis david 
preliminary architecture basic data flow processor 
proceedings nd annual symposium computer architecture january 
daniel gajski ramachandran 
high level synthesis 
ieee design test computers 
maya gokhale janice stone jeff arnold 
stream oriented fpga computing streams high level 
proceedings ieee symposium field programmable custom computing machines fccm 
ieee april 
seth goldstein herman schmit matthew moe mihai budiu srihari reed taylor ronald laufer :10.1.1.21.5165
piperench coprocessor streaming multimedia acceleration 
proceedings th international symposium computer architecture isca pages may 
scott hauck thomas fry matthew jeffery kao 
chimaera reconfigurable functional unit 
proceedings ieee symposium fpgas custom computing machines pages april 
john hauser john wawrzynek 
garp mips processor reconfigurable coprocessor 
proceedings ieee symposium field programmable gate arrays custom computing machines pages 
ieee april 
hoare 
communicating sequential processes 
international series computer science 
prentice hall 

dataflow von neumann hybrid architecture 
proceedings th international symposium computer architecture pages may 
jeffery jacob paul chow 
memory interfacing instruction specification reconfigurable processors 
proceedings international symposium field programmable gate arrays fpga pages february 
mark jones luke jonathan scott chris matthew yao peter athanas 
implementing api distributed adaptive computing systems 
proceedings ieee symposium field programmable custom computing machines fccm pages april 
edward lee 
advanced topics dataflow computing chapter static scheduling data flow programs dsp 
prentice hall 
ling 
data driven computer virtual hardware 
proceedings ieee workshop fpgas custom computing machines fccm pages april 
bruce 
signal processing xilinx fpgas 
www xilinx com apps sd pdf june 
mark frederic chong timothy sherwood 
active pages model computation intelligent memory 
proceedings th international symposium computer architecture isca june 
ahn andr dehon john wawrzynek 
embedded dram reconfigurable array 
proceedings symposium vlsi circuits june 
jan rabaey 
reconfigurable computing solution low power programmable dsp 
proceedings ieee international conference acoustics speech signal processing icassp april 
rahul michael smith 
high performance microarchitecture hardware programmable functional units 
proceedings th annual international symposium microarchitecture pages 
ieee computer society november 
paul sasaki 
fast fpga active interconnect 
proceedings international symposium field programmable gate arrays fpga page february 
edward tau ian derrick chen jeremy brown andr dehon 
generation dpga implementation 
proceedings third canadian workshop field programmable devices pages may 
william tsu kip atul joshi randy huang norman walker tony tung varghese george john wawrzynek andr dehon 
high speed hierarchical synchronous reconfigurable array 
proceedings international symposium field programmable gate arrays pages february 
john chris jones brian schoner 
video communications rapidly reconfigurable hardware 
ieee transactions circuits systems video technology december 
john brian schoner kang chia charles zapata 
configurable computer solutions automatic target recognition 
proceedings ieee workshop fpgas custom computing machines pages 
ieee april 
gregory wallace 
jpeg picture compression standard 
communications acm april 
john 
architecture media processing implementation 
thesis proposal mit media laboratory january 
wad www media mit edu people wad tp 
john michael bove jr system parallel media processing 
proceedings workshop parallel processing multimedia april 
michael brad hutchings 
disc dynamic instruction set computer 
proceedings spie reconfigurable computing conference field programmable gate arrays fpgas fast board development reconfigurable computing pages october 
xilinx logic drive san jose ca 
virtex series fpgas 
www xilinx 
com products virtex htm 
zhong margaret martonosi sharad malik 
accelerating boolean satisfiability configurable hardware 
proceedings ieee symposium field programmable custom computing machines fccm pages april 
data switch switch control true side false side select select control data switch select example motivating unbounded stream buffers unbounded stream buffers levels score computational model provide abstraction unbounded stream buffers 
picking finite buffer size streams introduce artifact model difficult reason 
particular programs prone deadlock time number tokens application needed queue stream pair operators data dependent 
canonical instance deadlock hazard exemplified pair nodes switch select 
switch takes inputs boolean control stream data stream 
sends output output streams value control input 
select takes inputs control stream input streams 
read tokens cycle 
reads control token 
value control token reads input streams passes single output stream 
nodes hooked directly outputs node connected inputs select node shown 
provide separate control streams switch select nodes 
stream prefix control stream contains vice versa select control stream stream true side switch select nodes hold tokens 
streams limited fixed size buffer subgraph deadlock 
loss generality consider case switch node received followed false node initially receives false control signal followed 
true side stream fill tokens 
switch node able perform operations write data true side stream 
select node process token false side order continue tokens false side consume 
node forward process token false side 
switch node progress downstream operator switch consumes token true side 
operators deadlocked cyclic dependence 
note unbounded deadlock occur processing able proceed 
general control streams completely independent possible say particular property 
control streams coming outside system certainly control knowledge relationship 
generated inside system general question computation produces particular token value finite number operations equivalent halting problem 
order provide reasonable semantics programmer accept unbounded buffer size abstraction include support execution model expand finite buffers necessary meet abstraction limit amount memory available system 
score compute model section described compute model informally 
section defines precisely 
score score computation graph notes 
ei vf vt vi vo vi vf vi vt stm vi vi sin vi vo sout typically initialized node vs vt start computation 
may start nodes ended unbounded fifo may created closed freed 
eos ree eos boolean ree boolean port port queue operations vertex operation invoked 
op requirement action write outs eos add close outs eos add ins free false empty read ins rm free false eos false eos true eos ins free false eos free ins free false free true notes freed closed removed score graph 
eos free true queue queue unbounded queue 
data ordered list qn created finite alphabet empty value data add data qn qn rm empty value data qn empty error eos value qn fsm stream operations 
vf sc sd sc sd pin pout sc sn sd finite sd sc sc sd sd sc bn bi ii ai fci fdi ii pin ai ai ai ai mi ai fi wi ci csd current data state sd wi gi csd write pout pout vi csd ci gi csd close pout pout fi gi csd free pin vi csd sd gi csd sd boolean fci sd sc fdi sd sd id ad sd fdd csd identity id ad fdd operation 
read state si inputs ii read inputs state stay state perform actions transitions 

action perform guarded writes closes frees ai guards true 

transition update state fci fdi 
notes operations read available read mechanism available arbitrary 
done state done enters ues 
values state csd change reflect input val formed close output streams free input streams making final transition properly specified specify eof data transitions eof input transition state read input reach state read said input 
properly formed performing close action output machine transition state reach state performs write close said output 
separating sc sd artificial introduced clarity 
want think state sc sc multi state sequence read action transition 
strictly speaking memory segment particular mode operation formal standpoint necessary define separate entities 
execution csd seen modified 
entering done state available operators see 
notably segments contents memory segment mi resulting 
close corresponds close edge source port free write similar correspondence 
enters done state may removed score compute graph memory segments owns reverted creating vertex vf mi vc vc allocated vf 
port port simply designation input output computational node 
stream edge vertex come 
sin sin input stream outside score computational graph 
vi data arriving placed tokens attached 
policy conversion outside computational model simply represents edge compute model algorithmic handling needed boundary represented 
sout sout output stream outside score computational graph 
vo data arriving stream attached exposed outside 
provisions signal consumption associated 
policy conversion outside computational model simply represents edge compute model 
single owner finite sized random access memory segment 
sz finite integer sz operations vertex operation invoked 
op requirement action alloc return new sz free write sz read sz notes describes model kind lock exclusive ownership 
relatively straight forward define model allowed multiple readers 
stm stm turing complete vertex 
stream operations 
ability create score graph nodes edges add score computational graph 
ability lock region memory block waiting access region memory defined superset add memory allocation allow unbounded memory vertex additional actions new items 
vt sc sd sc sd pin pout sc sn sd finite mm sc sc sd sd sd bn bi imi isi ai fci fdi imi isi pin ai ai ai ai ki ai fi wi ci ami fmi wmi nei csd current data state sd wi gi csd write pout pout vi csd ci gi csd close pout pout fi gi csd free pin ami gi csd alloc vi csd fmi gi csd free mk wmi gi csd write vi csd vi csd mk nei gi csd alloc gi csd vertex prototype alloc vertex vi csd vi csd vi csd esi msi stm definition esi esi esi esi ki esi msi msi msi msi oi msi vi csd sd vi csd sd gi csd sd boolean fci sd sc fdi sd sd id ad sd fdd csd identity id ad fdd operation 
read state si inputs isi memories imi owned vertex read inputs state stay state perform actions transitions 

action mi written writes freed memory frees ai owned vertex perform guarded writes frees closes allocates news ai guards true stay substate perform actions transitions 

transition update state fci fdi 
actions operations previously defined 
close corresponds close edge source port free write similar correspondence 
op requirement action alloc returns empty ne source sink unbound ne alloc vertex es mi ms mi ms nv new es ms defined mi ms mi nv args match set es sources sinks ti ports nv set initial state nv ti nv notes operations read available read mechanism available arbitrary stm 
similarly operation read available read mechanism 
done state stm done enters formed close output streams free inputs making final transition properly specified specify eof data transitions eof input transition state read input reach state read said input 
properly formed performing close action output machine transition state ues 
values state csd change reflect input val reach state performs write close said output 
separating sc sd artificial introduced clarity 
want think state sc sc multi state sequence read action transition 
stm enters done state removed score compute graph memory segments owns reverted creating vertex vt mi vc vc allocated vf 

