proceedings th annual international symposium computer architecture june microprocessor industry currently struggling higher development costs longer design times arise exceedingly complex processors pushing limits parallelism 
designs especially ill suited important commercial applications line transaction processing oltp suffer large memory stall times exhibit little instruction level parallelism 
commercial applications constitute far important market high performance servers trends emphasize need consider alternative processor designs specifically target workloads 
abundance explicit thread level parallelism commercial workloads advances semiconductor integration density identify chip multiprocessing cmp potentially promising approach designing processors targeted commercial servers 
describes piranha system research prototype developed compaq aggressively exploits chip multiprocessing integrating simple alpha processor cores level cache hierarchy single chip 
piranha integrates chip functionality allow scalable multiprocessor configurations built modular fashion 
simple processor cores combined industry standard asic design methodology allow complete prototype short time frame team size investment order magnitude smaller commercial microprocessor 
detailed simulation results show piranha processor core substantially slower aggressive generation processor integration cores single chip allows piranha outperform generation processors times chip basis important workloads oltp 
performance advantage approach factor full custom asic logic 
addition exploiting chip multiprocessing piranha prototype incorporates unique design choices including shared second level cache inclusion highly optimized cache coherence protocol novel architecture 
high microprocessor designs increasingly complex past decade designers continuously acm copyright notice permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage copies bear notice full citation page 
copyrights components owned acm honored 
abstracting credit permitted 
copy republish post servers redistribute lists requires prior specific permission fee 
piranha scalable architecture single chip multiprocessing luiz andr kourosh gharachorloo robert mcnamara andreas qadeer barton sano scott smith robert ben verghese western research laboratory systems research center nonstop hardware development compaq computer compaq computer compaq computer palo alto ca palo alto ca austin tx pushing limits instruction level parallelism speculative order execution 
trend led significant performance gains target applications spec benchmark continuing path viable due substantial increases development team sizes design times 
furthermore complex designs yielding diminishing returns performance applications spec 
commercial workloads databases web applications surpassed technical workloads largest fastest growing market segment high performance servers 
number studies radically different behavior commercial workloads line transaction processing oltp relative technical workloads 
commercial workloads lead inefficient executions dominated large memory stall component 
behavior arises large instruction data footprints high communication rates characteristic workloads 
second multiple instruction issue order execution provide small gains workloads oltp due data dependent nature computation lack instruction level parallelism 
third commercial workloads high performance floating point multimedia functionality implemented modern microprocessors 
uncommon high microprocessor stalling time executing commercial workloads leading severe utilization parallel functional units high bandwidth memory system 
trends question wisdom pushing complex processor designs wider issue speculative execution especially server market target 
fortunately increasing chip densities transistor counts provide architects alternatives better tackling design complexities general needs commercial workloads particular 
example generation alpha aggressively exploits semiconductor technology trends including scaled ghz core shrink current alpha processor core um technology levels caches memory controller coherence hardware network router single die 
tight coupling modules enables efficient lower latency memory hierarchy substantially improve performance commercial workloads 
furthermore reuse existing high performance processor core designs alpha effectively addresses design complexity issues provides better time market sacrificing server performance 
higher transistor counts exploit inherent explicit thread level process level parallelism available commercial workloads better utilize onchip resources 
parallelism typically arises relatively independent transactions queries initiated different clients traditionally hide latency workloads 
previous studies shown techniques simultaneous multithreading smt provide substantial performance boost database workloads 
fact alpha successor alpha planning combine aggressive chip level integration see previous paragraph instruction wide order processor smt support simultaneous threads 
alternative approach referred chip multiprocessing cmp involves integrating multiple possibly simpler processor cores single chip 
approach adopted generation ibm power design integrates superscalar cores shared second level cache 
smt approach superior single thread performance important workloads explicit thread level parallelism best suited wide issue processors complex design 
comparison cmp advocates simpler processor cores potential loss single thread performance compensates throughput integrating multiple cores 
furthermore cmp naturally lends hierarchically partitioned design replicated modules allowing chip designers short wires opposed costly slow long wires adversely affect cycle time 
presents detailed description evaluation piranha research prototype developed compaq jointly corporate research nonstop hardware development explore chip multiprocessing architectures targeted parallel commercial workloads 
piranha architecture highly integrated processing node simple alpha processor cores separate instruction data caches core shared second level cache memory controllers coherence protocol engines network router single die 
multiple processing nodes build multiprocessor modular scalable fashion 
primary goal piranha project build system achieves superior performance commercial workloads especially oltp small team modest investment short design time 
requirements heavily influence design decisions methodology implementing prototype 
opted extremely simple processor cores single issue order stage pipelined design 
second opted semi custom design industry standard asic methodologies tools making heavy synthesis standard cells 
achieve acceptable performance rely state art um asic process limited custom designed memory cells time area critical memory structures 
modules larger area target clock speed half achieved custom logic process technology 
detailed performance evaluation piranha design full system simulations including operating system activity oracle commercial database engine running compaq tru unix 
results show piranha processor core substantially slower aggressive generation processor integration cores single chip allows piranha outperform generation processors times chip basis important commercial workloads 
true potential piranha architecture fairly judged considering full custom design 
approach clearly requires larger design team investment maintains relatively low complexity short design time characteristics 
results show custom design enhance piranha performance advantage workloads oltp times better chip basis relative generation processors 
results clearly indicate focused designs piranha directly target commercial server applications substantially outperform 
project name motivated analogy seemingly small fish concert larger creature 
interconnect links output queue router input queue packet switch system control home engine remote engine direct rambus array cpu il dl intra chip switch general purpose microprocessor designs higher complexity 
addition exploring chip multiprocessing piranha architecture incorporates number novel ideas 
design shared second level cache uses sophisticated protocol enforce inclusion level instruction data caches order maximize utilization chip caches 
second cache coherence protocol nodes incorporates number unique features result fewer protocol messages lower protocol engine occupancies compared previous protocol designs 
piranha unique architecture node full fledged member interconnect global shared memory coherence protocol 
rest structured follows 
section presents overview piranha architecture 
describe experimental methodology including workloads architectures study 
section presents detailed performance results piranha prototype considers custom implementations better illustrate full potential approach 
design methodology implementation status described section 
discuss related conclude 
piranha architecture overview shows block diagram single piranha processing chip 
alpha cpu core cpu directly connected dedicated instruction il data cache dl modules 
level caches interface modules intra chip switch ics 
side ics logically shared second level cache interleaved separate modules controller chip tag data storage 
attached module memory controller mc directly interfaces bank direct rambus dram chips 
memory bank provides bandwidth gb sec leading aggregate bandwidth gb sec 
connected ics protocol engines home engine remote engine re support shared memory multiple piranha chips 
interconnect subsystem links multiple piranha chips consists router rt input queue iq output queue oq packet switch ps 
total interconnect bandwidth piranha processing chip gb sec 
system control sc module takes care miscellaneous maintenance related functions system configuration initialization interrupt distribution exception handling performance monitoring 
noted various modules communicate exclusively connections shown represent mc 
cpu il dl mc 

block diagram single chip piranha processing node 
interconnect links output queue router input queue packet switch system control home engine remote engine direct rambus array cpu il dl intra chip switch actual signal connections 
modular approach leads strict hierarchical decomposition piranha chip allows development module relative isolation defined transactional interfaces clock domains 
piranha processing chip complete multiprocessor system chip capability 
actual performed piranha chip shown relatively small area compared processing chip 
chip stripped version piranha processing chip cpu mc module 
router chip simplified support links alleviating need routing table 
programmer point view cpu chip indistinguishable processing chip 
similarly memory chip fully participates global cache coherence scheme 
presence processor core chip provides benefits enables optimizations scheduling device drivers processor lower latency access virtualize interface various devices having alpha core interpret accesses virtual control registers 
pci interface available asic library modules chip identical design processing chip 
simplify design reuse level data cache module dl interface pci module 
dl module provides pci address translation access space registers interrupt generation 
piranha chip may customized support standards fiber channel system shows example configuration piranha system processing chips 
piranha design allows scaling nodes arbitrary ratio processing nodes adjusted particular workload 
furthermore piranha router supports arbitrary network topologies allows dynamic reconfigurability 
underlying design decisions piranha treat uniform manner full fledged member interconnect 
part decision observation available bandwidth best invested single switching fabric forms global resource dynamically utilized memory traffic 
key design decisions piranha remain binary compatible alpha software base including applications system software compilers operating system 
user applications run modification expecting minimal porting effort os tru unix 
remaining sections provide detail various modules piranha architecture 
mc 
pci dl bus 
block diagram single chip piranha node 
chip chip chip chip chip alpha cpu core level caches processor core uses single issue order design capable executing alpha instruction set 
consists mhz pipelined datapath hardware support floating point operations 
pipeline stages instruction fetch register read alu write back 
stage alu supports pipelined floating point multiply instructions 
instructions execute single cycle 
processor core includes performance enhancing features including branch target buffer pre compute logic branch conditions fully bypassed datapath 
processor core interfaces separate instruction data caches designed single cycle latency 
kb way set associative blocking caches virtual indices physical tags 
cache modules include tag compare logic instruction data tlbs entries way associative store buffer data cache 
maintain bit state field cache line corresponding states typical protocol 
simplicity instruction data caches virtually design 
alpha implementations instruction cache kept coherent hardware 
treating instruction data caches way simplifies inclusion policy level 
intra chip switch chip chip chip 
example configuration piranha system processing cpus chips 
conceptually intra chip switch ics crossbar interconnects modules piranha chip 
managing data transfers clients efficiently poses number implementation challenges arbitration flow control layout 
ics primary facility decomposing piranha design relatively independent isolated modules 
particular transactional nature ics allows add remove pipeline stages design various modules compromising piranha timing 
ics uses uni directional push interface 
initiator transaction sources data 
destination transaction ready ics schedules data transfer datapath availability 
issued initiator commence data transfer rate bit word cycle flow control 
concurrently destination receives request signal identifies initiator type transfer 
transfers atomic implied ordering properties exploited supporting intra chip coherence 
port ics consists independent bit datapaths plus bit parity ecc bits sending receiving data 
ics supports back back transfers dead cycles transfers 
order reduce latency modules allowed issue target destination request ahead actual transfer request 
hint ics pre allocate datapaths speculatively assert requester signal 
ics implemented set internal datapaths run center piranha chip 
internal ics capacity gb sec times available memory bandwidth achieving optimal schedule critical achieve performance 
ics supports logical lanes low high priority avoid intra chip cache coherence protocol deadlocks 
adding extra datapaths multiple lanes supported ready lines distinct ids module 
initiator specify appropriate lane transaction corresponding id destination 
second level cache piranha second level cache mb unified instruction data cache physically partitioned banks logically shared cpus 
banks interleaved lower address bits cache line physical address byte line 
bank way set associative uses round robin loaded replacement policy invalid block available 
bank control logic interface private memory controller ics interface communicate chip modules 
controllers responsible maintaining intra chip coherence cooperate protocol engines enforce inter chip coherence 
piranha aggregate capacity mb maintaining data inclusion mb potentially waste full capacity duplicate data 
piranha opts maintaining inclusion property 
non inclusive chip cache hierarchies previously studied context single cpu chip technique context cmp leads interesting issues related coherence allocation replacement policies 
simplify intra chip coherence avoid snooping caches keep duplicate copy tags state controllers 
controller maintains tag state information lines map address interleaving 
total overhead duplicate tag state controllers total chip memory 
order lower latency best utilize capacity misses filled directly memory allocating line 
effectively behaves large victim cache filled data replaced 
clean lines replaced may cause write back 
avoid unnecessary multiple copies line duplicate state extended include notion ownership 
owner line valid copy exclusive state typically requester multiple sharers 
information decision write back data piggybacks information reply request caused replacement 
case multiple sharers writeback happens owner replaces data 
approach provides near optimal replacement policy affecting hit time 
ruled alternative solutions require checking states state victim require multiple tag lookup cycles critical path hit 
intra chip coherence protocol 
controllers responsible enforcing coherence chip 
controller complete exact information chip cached copies subset lines map 
access duplicate tag state tag state checked parallel 
intra chip coherence similarities full map centralized directory protocol 
information sharing data chips kept directory stored dram accessed memory controller see section 
full interpretation manipulation directory bits done protocol engines 
controllers partially interpret directory information determine line cached remote node cached exclusively 
partial information kept duplicate states allows controller home avoid communicating protocol engines majority local requests 
cases partial information avoids having fetch directory memory copy line cached chip 
memory request sent appropriate bank address interleaving 
depending state possibly service request directly forward request local owner forward request protocol engines obtain data memory memory controller home local 
responsible chip invalidations triggered local remote requests 
ordering characteristics intra chip switch allow eliminate need acknowledgments chip invalidations 
invalidating forwarding requests remote nodes handled protocol engines 
requests forwarded home engine carry copy directory updated home engine written back memory 
forwarding cases keeps request pending entry block conflicting requests duration original transaction 
small number entries supported controller order allow concurrent outstanding transactions 
memory controller piranha high bandwidth low latency memory system direct rambus 
keeping modular design philosophy memory controller associated channel bank total memory controllers 
rambus channel support chips 
mbit memory chip generation piranha processing chip support total gb physical memory gb gb mb gb chips 
channel maximum data rate gb sec providing maximum local memory bandwidth gb sec processing chip 
latency random access memory channel ns critical word additional ns rest cache line 
chip modules memory controller direct access intra chip switch 
access memory controlled routed corresponding controller 
issue read write requests memory granularity cache line data associated directory 
design memory controller consists parts rambus access controller rac memory controller engine 
rac provided rambus incorporates high speed interface circuitry 
memory controller engine functionality includes mc interface scheduling memory accesses 
complexity comes deciding pages keep open various devices 
fully populated piranha chip byte pages open 
hit open page reduces access latency ns ns 
simulations show keeping pages open microsecond yield hit rate workloads oltp 
protocol engines shown piranha processing node separate protocol engines support shared memory multiple nodes 
home engine responsible exporting memory home local node remote input stage hardwired execution stage firmware controlled output stage hardwired packet switch input buffers test execution unit input controller fsm instruction output buffers microcode ram packet switch output controller fsm intra chip switch conditional branching intra chip switch 
block diagram protocol engine 
engine imports memory home remote 
sections describe protocol engine design directory storage inter node coherence protocol detail 
protocol engine structure protocol engines piranha implemented controllers home remote engines virtually identical microcode execute 
approach uses design philosophy protocol engines mp project 
shows high level block diagram protocol engine consisting independent decoupled stages input controller execution unit output controller 
input controller receives messages local node external interconnect output controller sends messages internal external destinations 
bottom right section depicts consists microcode memory current instruction register 
microcode memory supports instructions current protocol uses microcode instructions engine 
microcode instruction consists bit opcode bit arguments bit address points instruction executed 
design uses instruction types send receive local node local node test set move 
receive test instructions behave multi way conditional branches different successor instructions achieved ing bit condition code significant bits bit instruction address field 
allow mhz operation interleaved execution model fetch instruction addressed odd addressed thread executing instruction odd addressed addressed thread 
actual protocol code specified slightly higher level symbolic arguments style code blocks sophisticated microcode assembler appropriate translation mapping microcode memory 
typical cache coherence transactions require instructions engine handles transaction 
example typical read transaction remote home involves total instruction remote engine requesting node send request home receive reply test state variable replies waiting processor node 
new transaction protocol engine allocates entry transaction state register file represents state thread addresses program counter timer state variables 
thread waiting response local remote node entry set waiting state incoming response matched entry transaction address 
design supports total entries protocol engine allow concurrent protocol transactions 
believe design provides nice balance flexibility late binding protocol performance 
design flexible general purpose processor flash specialized powerful instructions lead lower protocol engine latency occupancy 
directory storage piranha design supports directory data virtually memory space overhead computing ecc coarser granularity utilizing unused bits storing directory information 
ecc computed bit boundaries typical bit leaving bits directory storage byte line 
compared having dedicated external storage datapath directories approach leads lower cost requiring fewer components pins provides simpler system scaling 
addition leverage low latency high bandwidth path provided integration memory controllers chip 
different directory representations depending number sharers limited pointer coarse vector 
bits directory state bits available encoding sharers 
directory maintain information sharers home node 
furthermore directory information maintained granularity node individual processors 
node system switch coarse vector representation past remote sharing nodes 
inter node coherence protocol piranha uses invalidation directory protocol support request types read read exclusive exclusive requesting processor shared copy exclusive data 
support features clean exclusive optimization exclusive copy returned read sharers reply forwarding remote owner eager exclusive replies ownership invalidations complete 
invalidation acknowledgments gathered requesting node 
protocol depend point point order allowing external interconnect techniques adaptive routing 
unique property protocol avoids negative acknowledgment nak messages corresponding retries 
reasons naks scalable coherence protocols requests naked avoid deadlock outgoing network lanes back ii requests naked due protocol races request fails find data node forwarded 
avoid naks virtual lanes 
explaining need lane scope 
low priority lane requests sent home node writeback replacement requests high priority lane forwarded requests replies 
deadlock solution relies sufficient buffering network explained 
avoid second naks guaranteeing forwarded requests serviced target nodes 
example owner node writes back data home maintains valid copy data home 
corresponds alpha write hint instruction wh indicates processor write entire cache line avoiding fetch line current contents useful copy routines 
acknowledges writeback allowing satisfy forwarded requests 
cases forwarded request may arrive owner node early owner node received data 
case delay forwarded request data available 
lack naks retries leads efficient protocol provides important desirable characteristics 
owner node guaranteed service forwarded request protocol complete directory state changes immediately 
property eliminates need extra confirmation messages sent back home ownership change dash eliminates associated protocol engine occupancy 
protocol handles hop write transactions involving remote owner efficiently 
second inherently eliminate livelock starvation problems arise due presence naks 
contrast sgi origin uses number complicated mechanisms keeping retry counts reverting strict request reply protocol protocols naks ignore important problem dash flash 
number unique techniques limit amount buffering needed network avoiding deadlocks 
network uses hot potato routing increasing age priority message non optimally routed 
enables message theoretically reach empty buffer network making buffering requirements grow linearly opposed quadratically additional nodes 
second buffer space shared lanes need separate buffer space lane 
third bound number messages injected network result single request 
key place necessary invalidation messages 
developed new technique called cruise missile invalidates cmi allows invalidate large number nodes injecting handful invalidation messages network 
invalidation message visits predetermined set nodes eventually generates single acknowledgment message reaches final node set 
studies show cmi lead superior invalidation latencies avoiding serializations arise injecting invalidation messages home node gathering corresponding acknowledgments requesting node 
properties allow provide limited amount buffering node need grow add nodes 
system interconnect piranha system interconnect consists distinct components output queue oq router rt input queue iq 
oq accepts packets packet switch protocol engines system controller 
rt transmits receives packets nodes deals transit traffic passes rt impacting modules 
iq receives packets addressed local node forwards target module packet switch 
interconnect system initialize piranha chips 
method relies rt initialize channels automatically 
default reset rt forwards initialization 
protocol needs support single forwarded request request outstanding owner node 
entry allocated outstanding request save information delayed forwarded request 

example entries protocol engine cmi limit invalidation messages total buffering total message headers protocol engines invalidations needed node requiring space data 
note buffer size function number nodes system 
packets system controller sc interprets control packets access control registers piranha node 
sc capabilities related initialization include accessing chip memories updating routing table starting stopping individual alpha cores testing chip memory 
piranha initialized traditional alpha boot process primary caches loaded small external bit serial connection 
router rt rt similar connect design developed mp project 
connect rt uses topology independent adaptive virtual cut router core common buffer pool shared multiple priorities virtual channels 
piranha nodes separated long distances band clock distribution synchronization mechanisms connect 
furthermore piranha links nearly times faster connect links internal structure router advanced 
piranha processing node channels connect nodes point point fashion 
node channels allowing connected nodes redundancy 
system interconnect supports distinct packet types 
short packet format bits long data transactions 
long packet bit header format byte bit data section 
packets transferred interconnect clock cycles 
interconnect channel consists sets wires set direction 
wires high quality transmission lines driven special low voltage swing cmos drivers terminated chip remote matching receivers 
signaling rate times system clock frequency sec wire 
channels piranha processing node total interconnect bandwidth gb sec 
channels piggyback handshake mechanism deals transmission error recovery 
piranha uses encoding scheme minimize electrical problems related high speed data transmission 
guaranteeing wires state state net current flow channel zero 
allows voltage differential receivers generated termination doubling number signal wires 
signaling scheme encodes bits bit dc balanced word 
piranha sends data bits extra bits crc flow control error recovery 
design set codes represent bits elements complementary 
allows th bit generated randomly encoded inverting bits 
resulting code inversion insensitive dc balances links statistically wire 
piranha fiber optic ribbons interconnect nodes transformer coupling minimize emi problems cables connecting piranha boxes 
input iq output oq queues oq provides modest amount buffering set fifos de couple operation router local node 
fall path optimized single cycle delay router ready new traffic 
interconnect load increases router gives priority transit traffic accepts new packets free buffer space incoming packets 
policy results better performance 
oq supports priority levels ensures lower priority packets block higher priority traffic 
property maintained system interconnect 
iq receives packets rt forwards target modules packet switch 
important quickly remove terminal packets rt high speed operation buffering rt expensive 
reason iq buffer space oq 
oq iq supports priority levels 
improve system performance iq allows low priority traffic bypass high priority traffic blocked proceed destination 
iq complex oq interpret packets determine destination module 
process controlled disposition vector indexed packet type field bits encode major packet types 
normal operation packets directed protocol engines packets interrupts delivered system controller 
reliability features piranha supports number elementary reliability availability ras features redundancy memory components crc protection datapaths redundant datapaths protocol error recovery error logging links band system reconfiguration support 
furthermore piranha attempts provide platform investigating advanced ras features large scale servers 
developing complete solutions ras large scale systems scope project design provides hardware hooks enable research area 
ras features implemented changing semantics memory accesses flexibility available programmable protocol engines 
examples ras features interest persistent memory regions memory mirroring dual redundant execution 
persistent memory regions survive power failures system crashes transient errors greatly accelerate database applications currently rely committing state disk transaction boundaries 
adding battery main memory banks designing memory controller power cycle safely persistent memory requires mechanisms force volatile cached state safe memory mechanisms control access persistent regions 
implemented making protocol engines intervene accesses persistent areas perform capability checks persistent memory barriers 
similarly piranha protocol engines programmed intervene memory accesses provide automatic data mirroring perform checks results dual redundant computation 
evaluation methodology section describes workloads simulation platform various architectures study 
workloads oltp workload modeled tpc benchmark 
benchmark models banking database system keeps track customers account balances balances branch teller 
transaction updates randomly chosen account balance includes updating balance branch customer belongs teller transaction submitted 
adds entry history table keeps record submitted transactions 
dss workload modeled query tpc benchmark 
tpc benchmark represents activities business sells large 
associated protocol transaction maintains state keeps track expected replies 
protocol engines monitor failures mechanisms time outs error messages 
necessary state encapsulated control message directed recovery diagnostic software 
number products worldwide scale 
consists inter related tables keep information parts customer orders 
query scans largest table database assess increase revenue resulted discounts eliminated 
behavior query representative tpc queries queries exhibit parallelism 
oracle commercial database management system database engine 
addition server processes execute actual database transactions oracle spawns daemon processes perform variety duties execution database engine 
daemons database writer log writer participate directly execution transactions 
database writer daemon periodically flushes modified database blocks cached memory disk 
log writer daemon responsible writing transaction logs disk allows server commit transaction 
oltp dss workloads set scaled similar way previous study validated scaling 
tpc database branches shared memory segment sga size approximately mb size metadata area mb 
runs consist transactions warm period 
oracle dedicated mode workload client process dedicated server process serving transactions 
hide latencies including latency log writes oltp runs usually configured multiple server processes processor 
processes processor study 
dss oracle parallel query optimization option allows database engine decompose query multiple sub tasks assign oracle server process 
dss experiments memory mb database queries parallelized generate server processes processor 
simulation environment simulations simos alpha environment alpha port simos previous study commercial applications validated alpha multiprocessor hardware 
simos alpha full system simulation environment simulates hardware components alpha multiprocessors processors mmu caches disks console detail run alpha system software 
specifically simos alpha models micro architecture alpha processor runs essentially unmodified versions tru unix 
ability simulate user system code simos alpha essential rich level system interactions exhibited commercial workloads 
example oltp runs study kernel component approximately total execution time user kernel 
addition setting workload simos alpha particularly simple uses disk partitions databases application binaries scripts hardware platforms tune workload 
simos alpha supports multiple levels simulation detail enabling user choose appropriate trade simulation detail slowdown 
fastest simulator uses onthe fly binary translation technique similar embra position workload steady state 
medium speed simulation time processor module simos alpha models single issue pipelined processor 
slowest speed processor module models multiple issue order processor 
medium speed order model evaluating piranha processor cores slow speed order model evaluate aggressive generation processors 
parameter piranha simulated architectures generation microprocessor ooo table presents processor memory system parameters different processor configurations study 
microprocessor model aggressive design similar alpha integrates ghz order core levels caches memory controller coherence hardware network router single die comparable area piranha processing chip 
asic process limits frequency processor cores piranha mhz 
addition lower density asic sram cells integration simple processor cores limits amount second level chip cache piranha 
lower target clock frequency piranha allows higher associativity cache 
full custom piranha parameters illustrate potential piranha architecture design done larger team investment 
simple single issue order pipeline reasonable assume full custom approach lead faster clock frequency issue order design 
table shows memory latencies different configurations 
due lack inclusion piranha cache latency parameters corresponding servicing request hit request forwarded serviced chip fwd 
shown table piranha prototype higher hit latency full custom processor due slower asic sram cells 
performance evaluation piranha full custom piranha processor speed mhz ghz ghz type order order order issue width instruction window size cache line size bytes bytes bytes cache size kb kb kb cache associativity way way way cache size mb mb mb cache associativity way way way hit fwd latency ns ns ns na ns ns local memory latency ns ns ns remote memory latency ns ns ns remote dirty latency ns ns ns table 
parameters different processor designs 
section compares performance piranha aggressive order processor ooo table single chip multi chip configurations 
addition results potential full custom piranha design table fairly judges merits architecture 
oltp dss database workloads described previous section evaluation 
shows results single chip configurations oltp dss 
study configurations hypothetical single cpu piranha chip generation order processor ooo hypothetical single issue order processor identical ooo ino actual cpu piranha chip 
ino configurations better isolate various factors contribute performance normalized execution time ino ooo mhz ghz ghz mhz issue issue issue issue oltp hit cpu differences ooo 
shows execution time normalized ooo 
execution time divided cpu busy time hit stall time stall time 
configuration hit stall time includes hits forwarded requests served see fwd latency table 
focusing oltp results observe ooo outperforms expected times 
ino result shows faster frequency ghz vs mhz lower hit latency ns ino ooo vs ns account improvement times 
wider issue order features provide remaining times gain 
integrate simple cpus piranha outperforms ooo times 
shown reason piranha exceptional performance oltp achieves speedup nearly times chip cpus relative single cpu 
speedup arises abundance thread level parallelism oltp extremely tight coupling chip cpus shared second level cache leading small communication latencies effectiveness chip caches piranha 
effect clearly observed shows behavior cache chip cpus added 
shows breakdown total number misses served hit forwarded chip fwd served memory 
fraction hits drops go cpus fraction misses go memory remains constant past single cpu 
fact adding cpus corresponding piranha cache hierarchy increases amount chip memory doubles chip memory compared partially offsets effects increased pressure 
trend number cpus increases misses served going memory 
fwd accesses slower hits ns vs ns faster memory access ns 
piranha non inclusion policy effective utilizing total amount chip cache memory contain working set parallel application 
addition chip memory effects simultaneous execution multiple threads enables piranha tolerate long latency misses allowing threads cpus proceed independently 
result piranha chip sustain relatively high cpu utilization level despite having number misses compared ooo simulation data shown 
chip chip bandwidths problem cpus oltp primarily latency bound 
oltp workloads shown exhibit constructive ino ooo mhz ghz ghz mhz issue issue issue issue dss 
estimated performance single chip piranha cpus chip versus ghz order processor 
speedup normalized breakdown misses interference instruction data streams works benefit piranha 
piranha performance edge ooo transaction processing robust specific workload changes design parameters 
workload modeled tpc benchmark results showed outperforms ooo factor times 
studied sensitivity piranha performance pessimistic design parameters mhz cpus kb way latencies ns hit ns fwd 
execution time increases parameters piranha holds times performance advantage ooo oltp 
referring back see piranha outperforms ooo dss narrower margin oltp times 
main reason narrower margin comes workload smaller memory stall component execution time better utilization issue slots wide issue order processor 
dss composed tight loops exploit spatial locality data cache smaller instruction footprint oltp 
execution time dss spent cpu ooo faster clock speed nearly doubles performance compared vs ino doubling due wider issue execution ino vs ooo 
smaller memory stall component dss benefits piranha achieves speedup cpus single cpu 
interesting alternative consider piranha trade cpus larger cache 
fraction stall time relatively small improvement execution time infinite number cores mhz issue fwd hit 
piranha speedup breakdown oltp 
normalized execution time speedup piranha ooo number chips 
speedup oltp multi chip systems mhz cpu piranha chips versus ghz order chips 
single chip cpu piranha approximately faster single chip ooo 
ooo ghz issue mhz issue oltp ghz issue modest 
piranha cpus small relatively little sram added cpu removed 
result trade advantageous piranha 
relatively wide design space considers increasingly complex cpus chip multiprocessing system 
thorough analysis trade scope 
addition single chip comparisons important evaluate piranha system performs multi chip numa configurations 
shows speedup trends oltp going single chip chip system piranha ooo dss scalability shown near linear systems 
experiments piranha chip uses cpus chip 
shows piranha system scales better ooo vs range system sizes studied 
somewhat surprising operating system scalability limitations adversely affect piranha higher total count albeit slower cpus versus ooo 
observe effectiveness chip communication piranha offsets os overheads normally associated larger cpu counts 
general expect piranha system scalability par ooo systems 
far considered performance piranha constraints asic design methodology implement prototype 
fairly judge potential piranha approach evaluate performance full 
current version operating system simulation environment limits cpus 
study multi chip scaling consider piranha chips chip cpus 
ooo ghz issue mhz issue dss ghz issue hit cpu 
performance potential full custom piranha chip oltp dss 
custom implementation see table parameters 
compares performance full custom piranha ooo single chip configurations 
shows faster full custom implementation boost piranha performance times ooo oltp times dss 
dss sees particularly substantial gains performance dominated cpu busy time benefits boost clock speed vs 
gains oltp faster clock cycle relative improvement memory latencies smaller respect original parameters 
piranha architecture better match underlying thread level parallelism available database workloads typical generation order superscalar processor design relies ability extract parallelism 
design methodology implementation status design methodology starts architectural specification form models major piranha modules cache protocol engine 
models implement behavior cycle accurate fashion boundary signals actual implementation 
models form starting point verilog coding 
currently completed pass verilog processor core doing initial synthesis timing 
remaining modules different phases verilog development 
models execute faster verilog counterparts allowing efficient functional architectural verification 
environment allows verilog models interchanged mixed development verification purposes 
coherence protocols verified formal methods 
piranha implemented semi custom micron asic design flow 
design flow uses industry standard hardware description languages synthesis tools 
advantage improved portability evolving asic process technologies shorter time market compared design methodologies 
achieve target mhz frequency depend small number custom circuit blocks time critical sram cache memory specialized synthesis layout tools specifically target datapaths arithmetic units 
asic process technology includes high density sram cell sizes order gate delays ps worst case unloaded input nand 
entire piranha implementation complete infer clock frequency preliminary logic synthesis processor core critical path estimates various modules 
calculated area major modules estimates compilable memory arrays logic synthesis simple gate counts 
area estimates developed general floor plan piranha processing node illustrated 
roughly piranha processing node area dedicated alpha cores caches remaining area allocated memory controllers interconnect router protocol engines 
discussion related addition chip multiprocessing cmp piranha project incorporates interesting ideas general area scalable shared memory designs 
related referenced earlier sections 
discuss previous pertinent database workloads cmp section 
router packet switch home engine cntr mc remote engine state tag cntr state tag mc rac data cpu il dl cpu il dl data rac mc state tag cntr rac data cpu il dl intra chip switch cntr state tag mc cpu il dl state tag rac data cpu clock large number studies database applications oltp dss due increasing importance workloads 
best knowledge provides detailed evaluation database workloads context chip multiprocessing 
ranganathan study user level traces database workloads context wide issue order processors show gains dss substantial gains oltp limited consistent results section 
number studies address issues related effectiveness different memory system architectures oltp workloads 
show need large chip caches mb 
lo show large chip cache mb adversely affected cache interference caused fine grain multithreading 
study shows smaller associative caches mb way integrated chip outperform larger direct mapped chip caches 
results show small associative second level chip caches mb way case effective shared multiple processors threads 
show aggressive chip level integration memory system coherence network modules single chip alpha provide large gains oltp workloads 
piranha advocates focused design targets commercial applications currently constitute largest segment high performance servers possible expense types workloads 
contemporary processor designs specifically focused commercial markets 
papers stanford advocated evaluated chip multiprocessing cmp context workloads spec hydra project exploring cmp focus thread level speculation 
current implementation integrates mhz processors kb instruction data caches shared kb second level cache small chip 
number differences hydra piranha 
example piranha cores cache maintain inclusion high speed switch bus connect chip cores provides scalability past single chip integrating required chip function 
latest ibm rs iii mhz processor currently holds tpc benchmark record models oltp processors outperforming systems processors see www tpc org 
mc cntr cntr il dl cpu il dl rac mc state tag cntr cntr mc rac data cpu il dl cpu il dl data state data state data tag tag rac mc rac 
floor plan piranha processing node cpu cores 
ality support multiprocessing 
furthermore piranha focuses commercial workloads abundance explicit thread level parallelism 
support speculation proposed hydra necessary achieving high performance workloads 
cmp design progress ibm power 
power chip ghz issue order superscalar processor cores chip shared cache 
chips connected multi chip module form processor system logically shared cache 
information ibm elaborate expansion capabilities past chips 
piranha takes extreme approach incorporating simpler processor cores single chip provides chip functionality scalable design 
sun microsystems announced new cmp design called implementation architecture targeted multimedia java applications 
contains mhz vliw processors capable issuing instructions cycle 
cores kb instruction cache share kb way data cache 
choice sharing cache clearly scale cores 
furthermore small size lack chip cache design non optimal commercial workloads oltp 
simultaneous multithreading smt forms multithreading alternative cmp exploiting parallelism commercial workloads 
fact lo shown smt provide substantial gain oltp workloads reasonably large gain dss workloads coupled wide issue order processors 
smt processor adds extra functionality resources larger register file order core support multiple simultaneous threads 
smt increases implementation verification complexity comes designs 
furthermore intelligent software resource management necessary smt avoid negative performance effects due simultaneous sharing critical resources physical register file caches tlbs 
advantage smt cmp provides superior performance workloads exhibit thread level parallelism 
piranha design targets workloads abundance parallelism opted forgo single thread performance favor design simplicity 
evaluation piranha primarily focused commercial database workloads 
expect piranha suited large class web server applications explicit thread level parallelism 
previous studies shown web server applications altavista search engine exhibit behavior similar decision support dss workloads 
concluding remarks chip multiprocessing inevitable microprocessor designs 
advances semiconductor technology enabling designs transistors near 
generation processors alpha appropriately exploiting trend integrating complete cache hierarchy memory controllers coherence hardware network routers single chip 
transistors available increasing chip cache sizes building complex cores lead diminishing performance gains possibly longer design cycles case option 
techniques simultaneous multithreading remedy diminishing gains address increasing design complexity 
time extra transistors integrate multiple processors chip quite promising especially abundance explicit thread level parallelism important commercial workloads 
couple generation processor designs subscribe philosophy integrating superscalar cores single die 
key questions designers processors chip multiprocessing appropriate trade number cores power core best partition memory hierarchy multiple cores 
described piranha architecture takes extreme position chip multiprocessing cmp integrating simple processor cores complete cache hierarchy memory controllers coherence hardware network router single chip built generation um cmos process 
due small design team modest investment research prototype opted asic design simple single issue order processor cores 
handicap results show piranha outperform aggressive generation processors factor times chip basis important commercial workloads oltp 
design require larger design team potential extend performance advantage times 
results clearly indicate focused designs piranha directly target commercial server applications substantially outperform general purpose microprocessor designs higher complexity 
hand piranha wrong design choice goal achieve best specint specfp numbers lack sufficient thread level parallelism workloads 
hope experience building piranha prototype provides proof point cmp designs simple processor cores 
hope design options exploring lack inclusion shared second level cache interaction intra node inter node coherence protocols efficient inter node protocol unique architecture provide insight cmp processors scalable designs general 
acknowledgments people contributed piranha effort preparation manuscript 
gary campbell sponsorship project alan eustace bob continuing support marco bob early support idea 
bill harold miller jim key steering piranha real design effort 
people significant technical contributions piranha joan wrote initial verilog alpha core dan scales helped inter chip coherence protocol early member architecture team robert bosch developed simos alpha order processor model jeff helped verilog development 
technical discussions pete bannon john joel emer rick kessler important focusing effort refining strategy 
grateful keith farkas jeff mogul dan scales deborah wallach careful review manuscript 
anonymous reviewers comments 
agarwal simoni hennessy horowitz 
evaluation directory schemes cache coherence 
th annual international symposium computer architecture pages may 
bannon 
alpha scalable single chip smp 
microprocessor forum www digital com htm october 
gharachorloo verghese 
impact chip level integration performance oltp workloads 
th international symposium high performance computer ar chitecture pages january 
gharachorloo bugnion 
memory system characterization commercial workloads 
th annual international symposium computer architecture pages june 

th generation bit powerpc compatible commercial processor design 
www rs ibm com resource technology pulsar pdf 
september 
crowder technical digest page 

characterization alpha axp performance tp spec workloads 
st annual international symposium computer architecture pages april 
donaldson 
alphaserver performance characterization 
digital technical journal pages 

power focuses memory bandwidth ibm confronts ia says isa important 
microprocessor report vol 
october 
digital equipment 
digital semiconductor alpha microprocessor hardware manual 
march 
eggers emer levy lo tullsen 
simultaneous multithreading platform generation processors 
ieee micro pages october 
johnson liu 
evaluation multithreaded uniprocessors commercial application environments 
rd annual international symposium computer architecture pages may 
emer 
simultaneous multithreading multiplying alpha performance 
presentation microprocessor forum october 
gupta 
weber mowry 
reducing memory traffic requirements scalable directory cache coherence schemes 
international conference parallel processing july 
hammond olukotun 
single chip multiprocessor 
ieee computer pages september 
hammond olukotun 
data speculation support chip multiprocessor 
th acm international symposium architectural support programming systems san jose california october 
hammond siu prabhu chen olukotun 
stanford hydra cmp 
hot chips august 
hennessy 
systems research 
ieee computer vol 
pages august 
ibm microelectronics 
asic sa 
international business machines 
jouppi 
tradeoffs level chip caching 
st annual international symposium computer architecture pages april 
keeton patterson raphael baker 
performance characterization quad pentium pro smp oltp workloads 
th annual international symposium computer architecture pages june 
krishnan torrellas 
hardware software support speculative execution sequential binaries chip multiprocessor 
acm international conference supercomputing ics pages june 
armstrong vitale 
system optimization oltp workloads 
ieee micro vol 
may june 
stanford flash multiprocessor 
st annual international symposium computer architecture april 
laudon lenoski 
sgi origin ccnuma highly scalable server 
th annual international symposium computer architecture pages june 
lenoski laudon gharachorloo gupta hennessy 
directory cache coherence protocol dash multiprocessor 
th annual international symposium computer architecture pages may 
lo eggers gharachorloo levy parekh 
analysis database workload performance simultaneous multithreaded processors 
th annual international symposium computer architecture june 
maynard donnelly 
contrasting characteristics cache performance technical multi user commercial workloads 
th international conference architectural support programming systems pages october 
hammond olukotun 
evaluation design alternatives multiprocessor microprocessor 
rd annual international symposium computer architecture may 
browne 
connect networks workstations supercomputing performance 
nd annual international symposium computer architecture pages may 
browne kelly 
mp scalable shared memory multiprocessor 
international conference parallel processing icpp pages july 
browne kelly 
exploiting parallelism cache coherency protocol engines 
europar international conference parallel processing august 
olukotun hammond wilson 
chang 
case single chip multiprocessor 
th international symposium architectural support programming systems october 
perl sites 
studies windows nt performance dynamic execution traces 
nd symposium system design implementation pages october 
ranganathan gharachorloo adve 
performance database workloads shared memory systems outof order processors 
th international conference architectural support programming systems pages october 
rosenblum bugnion herrod witchel gupta 
impact architectural trends operating system performance 
th symposium system principles december 
rosenblum bugnion herrod devine 
simos machine simulator study complex computer systems 
acm transactions modeling computer simulation vol 
pages january 
pong 
missing memory wall case processor memory integration 
rd annual international symposium computer architecture 
may 
sites 
alpha axp architecture manual second edition 
digital press 
standard performance council 
spec cpu benchmark suite 
www org 
mowry 
potential thread level data speculation facilitate automatic parallelization 
th international symposium high performance computer architecture pages february 

performance oltp application symmetry multiprocessor system 
th annual international symposium computer architecture pages may 
transaction processing performance council 
tpc benchmark standard specification revision 
june 
transaction processing performance council 
tpc benchmark decision support standard specification revision 
november 
transaction processing performance council 
tpc benchmark standard specification revision october 

pey zhang torrellas 
memory performance dss commercial workloads shared memory multiprocessors 
rd annual international symposium high performance computer architecture pages february 
tremblay 
vliw convergent 
microprocessor forum october 
witchel rosenblum 
embra fast flexible machine simulation 
acm sigmetrics conference measurement modeling computer systems pages may 

