appear tesauro touretzky leen eds 
advances neural information processing systems mit press cambridge ma 
neural network ensembles cross validation active learning anders krogh copenhagen denmark jesper vedelsby electronics institute building technical university denmark lyngby denmark learning continuous valued functions neural network ensembles committees give improved accuracy reliable estimation generalization error active learning 
ambiguity defined variation output ensemble members averaged unlabeled data quantifies disagreement networks 
discussed ambiguity combination cross validation give reliable estimate ensemble generalization error type ensemble cross validation improve performance 
shown estimate optimal weights ensemble members unlabeled data 
generalization query committee shown ambiguity select new training data labeled active learning scheme 
known combination different predictors improve predictions 
neural networks community ensembles neural networks investigated authors see instance :10.1.1.133.8090
networks ensemble trained individually predictions combined 
combination usually done majority classification simple averaging regression weighted combination networks 
author correspondence addressed 
email krogh dk workshop nips conference december entire session devoted ensembles neural networks putting chaired michael perrone 
interesting papers showed area getting lot attention 
combination output networks predictors useful disagree inputs 
clearly information gained identical networks just see 
quantifying disagreement ensemble turns possible state insight rigorously ensemble approximation realvalued functions regression 
simple beautiful expression relates disagreement called ensemble ambiguity generalization error basis derive delay 
bias variance tradeoff assume task learn function sample examples examples assumed drawn randomly distribution 
easy generalize output variables 
ensemble consists networks output network ff input called ff 
weighted ensemble average denoted bar ff ff ff final output ensemble 
think weight ff belief network ff constrain weights positive sum 
constraint sum crucial results 
ambiguity input single member ensemble defined ff ff gamma ensemble ambiguity input ff ff ff ff ff ff gamma simply variance weighted ensemble weighed mean measures disagreement networks input quadratic error network ff ensemble ffl ff gamma ff gamma respectively 
adding subtracting yields ff ff ffl ff gamma little algebra weights sum 
calling weighted average individual errors ffl ff ff ffl ff ffl gamma formulas averaged input distribution 
averages input distribution denoted capital letter ff ffl ff ff ff generalization error ambiguity respectively network ff generalization error ensemble 
find ensemble generalization error gamma term right weighted average generalization errors individual networks ff ff ff second weighted average ambiguities ff ff ff refer ensemble ambiguity 
beauty equation separates generalization error term depends generalization errors individual networks term contain correlations networks 
furthermore correlation term estimated entirely unlabeled data knowledge required real function approximated 
term unlabeled example borrowed classification problems context means input value target function unknown 
equation expresses tradeoff bias variance ensemble different way common bias variance relation averages possible training sets ensemble averages 
ensemble strongly biased ambiguity small networks implement similar functions agree inputs outside training set 
generalization error essentially equal weighted average generalization errors individual networks 
hand large variance ambiguity high case generalization error smaller average generalization error 
see 
equation immediately see generalization error ensemble smaller weighted average ensemble errors particular uniform weights ff ff noted authors see 
cross validation ensemble obvious increasing ambiguity increasing individual generalization errors improve generalization 
want networks disagree 
increase ambiguity ensemble 
way different types approximators mixture neural networks different topologies mixture completely different types approximators 


ensemble networks trained approximate square wave target function 
final ensemble output solid smooth curve outputs individual networks dotted curves shown 
square root ambiguity shown dash dot line 
training random examples network cross validation set size trained examples 
obvious way train networks different training sets 
furthermore able estimate term desirable kind cross validation 
suggests strategy 
chose number network ensemble hold examples testing test sets minimal overlap training sets different possible 
instance possible choose test sets overlap 
enables estimate generalization error ff individual members ensemble time sure ambiguity increases 
holding examples generalization errors individual members ensemble ff increase conjecture choice size ensemble test set size ambiguity increase get decrease generalization error 
conjecture tested experimentally simple square wave function variable shown 
identical feed forward networks hidden layer units trained independently back propagation random examples 
network cross validation set examples held testing described 
true generalization estimated set random inputs 
weights uniform ff non uniform weights addressed 
average results independent runs shown values solid line shows generalization error uniform weights function size cross validation sets 
dotted line error estimated equation 
dashed line optimal weights estimated generalization errors individual networks estimated crossvalidation sets described text 
bottom solid line generalization error obtain individual generalization errors known exactly best possible weights 
size cv set generalization error top solid line 
note generalization error cross validation set size size lower supports conjecture weaker form 
done experiments depending experimental setup curve take form error larger zero vice versa 
experiments shown ensembles converging networks 
ensembles kept error significantly higher half runs networks ensemble converged seldom happened cross validation set 
unclear circumstances expect drop generalization error cross validation fashion 
dotted line error estimated equation cross validation sets networks estimate ff notices agreement 
optimal weights weights ff estimated described 
suggest unlabeled data estimate way minimize generalization error 
analytical solution weights said minimum point generalization error 
calculating derivative subject constraints weights setting equal zero shows ff gamma ff ff calculation shown space limitations easy 
ff gamma ff networks 
notice ff depends weights ensemble average outputs 
shows optimal weights chosen network contributes exactly ff generalization error 
note member ensemble poor generalization correlated rest ensemble optimal set weight zero 
weights learned unlabeled examples gradient descent minimization estimate generalization error 
efficient approach finding optimal weights turn quadratic optimization problem 
problem non trivial constraints weights ff ff ff 
define correlation matrix fffi ff fi weights sum equation rewritten ff ff ff fffi ff fffi fi gamma ff ff ffff having estimates ff fffi optimal weights linear programming optimization techniques 
just ambiguity correlation matrix estimated unlabeled data accuracy needed provided input distribution known 
results experiment weight optimization shown 
dashed curve shows generalization error weights optimized described estimates ff cross validation examples lowest solid curve idealized case assumed errors ff known exactly shows lowest possible error 
performance improvement quite convincing cross validation estimates 
important notice estimate generalization error individual networks equation 
certain individual networks overfit training errors estimates ff see 
possible kind regularization cross validation sets small 
active learning neural network applications time consuming expensive acquire training data complicated measurement required find value target function certain input 
desirable examples maximal information function 
methods learner points examples called active learning 
propose query active learning scheme applies ensembles networks continuous valued output 
essentially generalization query committee developed classification problems 
basic assumption patterns input space yielding largest error points benefit including training set 
generalization error non negative see weighted average individual network errors larger equal ensemble ambiguity ffl generalization error training set size training set size plots full line shows average generalization active learning dashed line passive learning function number training examples 
dots left plot show results individual experiments contributing mean active learning 
dots right plot show passive learning 
tells ambiguity lower bound weighted average squared error 
input pattern yields large ambiguity large average error 
hand low ambiguity necessarily imply low error 
individual networks trained low training error set examples error ambiguity low training points 
ensures pattern yielding large ambiguity close neighborhood training example 
ambiguity extent follow fluctuations error 
ambiguity calculated unlabeled examples input space scanned areas detail 
ideas illustrated correlation error ambiguity quite strong perfect 
results experiment active learning scheme shown 
ensemble networks trained approximate square wave function shown experiments function restricted interval gamma 
curves show final generalization error ensemble passive dashed line active learning test solid line 
training set size independent tests starting initial training set single example 
examples generated added time 
passive test examples generated random active example selected input gave largest ambiguity random ones 
shows distribution individual results active passive learning tests 
obtain significantly better generalization active learning scatter results 
easier ensemble learn actively generated set 
central idea show lot gained unlabeled data training ensembles 
dealt neural networks theory holds type method individual members ensemble 
shown apart getting individual members ensemble generalize important generalization individuals disagrees possible discussed method identical networks disagree 
done training individuals different training sets holding examples individual training 
added advantage examples testing obtain estimates generalization error 
discussed find optimal weights individuals ensemble 
simple test problem weights improved performance ensemble significantly 
method active learning described method query committee developed classification problems 
idea ensemble disagrees strongly input find label input include training set ensemble 
shown active learning improves learning curve lot simple test problem 
peter salamon numerous discussions implementation linear programming optimization weights 
lars kai hansen discussions great insights david wolpert valuable comments 
hansen salamon 
neural network ensembles 
ieee transactions pattern analysis machine intelligence oct 
wolpert 
stacked generalization 
neural networks 
michael perrone leon cooper 
networks disagree ensemble method neural networks 
editor neural networks speech image processing 
chapman hall 
geman bienenstock doursat 
neural networks bias variance dilemma 
neural computation jan 
ronny meir 
bias variance combination estimators case linear squares 
preprint technion israel 
seung opper sompolinsky 
query committee 
proceedings fifth workshop computational learning theory pages san mateo ca 
morgan kaufmann 
freund seung shamir tishby 
information prediction query committee 
advances neural information processing systems volume san mateo california 
morgan kaufmann 

