weighted majority algorithm nick littlestone manfred warmuth ucsc crl revised october center computer engineering information sciences university california santa cruz santa cruz ca usa research primarily conducted author university calif santa cruz support onr harvard university supported onr darpa afosr 
current address nec research institute independence way princeton nj 
mail address research nj nec com 
supported onr 
part research done author sabbatical aiken computation laboratory harvard partial support onr 
address department computer science university california santa cruz 
mail address manfred cs ucsc edu 
study construction prediction algorithms situation learner faces sequence trials prediction goal learner mistakes 
interested case learner reason believe pool known algorithms perform learner know 
simple effective method weighted voting introduced constructing compound algorithm circumstance 
call method weighted majority algorithm 
show algorithm robust presence errors data 
discuss various versions weighted majority algorithm prove mistake bounds closely related mistake bounds best algorithms pool 
example sequence trials algorithm pool mistakes weighted majority algorithm log jaj mistakes sequence fixed constant 

study line prediction algorithms learn protocol 
learning proceeds sequence trials 
trial algorithm receives instance fixed domain produce binary prediction 
trial algorithm receives binary label viewed correct prediction instance 
evaluate algorithms mistakes ang bf lit lit 
mistake occurs prediction label disagree 
briefly discuss case predictions labels chosen interval 
investigate situation pool prediction algorithms varying numbers mistakes 
aim design master algorithm uses predictions pool prediction 
ideally master algorithm mistakes best algorithm pool priori knowledge algorithms pool mistakes sequence trials 
protocol proceeds follows trial instance fed algorithms pool 
algorithm prediction predictions grouped form instance fed master algorithm 
master algorithm prediction receives label passes pool 
probabilistic assumptions choice instances labels bounds prove predictive performance various algorithms hold worst case sequence instances labels 
goal thought constructing learning algorithms learn take advantage patterns input order mistakes 
scheme combining algorithms general sense types patterns handled algorithms pool patterns handled resulting combination algorithms 
types results relative absolute 
relative results give number mistakes master algorithm function number mistakes pool members results depend details patterns handled algorithms 
apply relative results want start pool algorithms contains members capable handling input face mistakes 
pool big combination algorithm formed applying master algorithm pool mistakes 
addition algorithms pool efficient pool sufficiently small resulting algorithm computationally efficient 
absolute results particular assumptions input look pool assumptions 
take particular interest patterns information seen learner labels usually depend functionally instances 
function maps instances labels called target function 
say trial consistent particular target function label trial value instance trial 
class potential notable differences earlier versions lw lw include general derivations bounds various versions weighted majority algorithm section insertion previously omitted proofs sections 
addition modified algorithm wmi called wmi section included new results infinite pools section 
amplified discussion randomized algorithm wmr section 

target functions target class interested algorithms mistakes sequence trials consistent target function chosen class 
put additional structure target classes ask algorithms fewer mistakes functions class example functions consider simpler 
discuss construction algorithms various target classes applying master algorithms pools tailored target classes 
want able handle cases data seen learner noisy reason quite consistent function target class function 
number anomalies sequence trials respect target function defined number trials consistent function 
number anomalies sequence trials respect target class number anomalies sequence respect member class 
interested algorithms able learn target classes presence anomalies algorithms able mistakes sequence trials anomalies respect target class 
look rate growth number mistakes number anomalies 
straightforward way construct canonical algorithm learning particular finite target class computable functions 
pool algorithms formed functions target class 
function target class represented algorithm pool computes trial prediction instance pool member just algorithm pays attention labels 
refer pools type interchangeably pools algorithms pools functions 
halving algorithm ang bf name lit interpreted master algorithm learns target class pool 
instance halving algorithm predicts majority consistent functions pool 
function consistent values agree labels instances seen previous trials 
note functions consistent eliminated decision process 
time master algorithm mistake majority consistent functions eliminated 
sequence trials consistent function pool halving algorithm log jf mistakes 
type scheme previously studied barzdin freivalds bf bf worked variation applies infinite pools functions 
algorithm function presence anomalies 
number anomalies sequence trials respect pool functions non zero halving algorithm eventually eliminate functions pool functions left base predictions 
new master algorithm develop called weighted majority algorithm wm robust describe deterministic version weighted majority algorithm applies finite pools arbitrary prediction algorithms making binary predictions 
give versions algorithm qualifications varied 
weighted majority algorithm wm initially positive weight associated algorithm function pool 
weights initially specified 
algorithm wm forms prediction comparing total weight algorithms related ideas working pools learning algorithms developed framework inductive inference limit fsv pit ps 

pool predict total weight algorithms predicting 
wm predicts larger total arbitrarily case tie 
wm mistake weights algorithms pool disagreed label multiplied fixed fi fi 
wm applied pool functions fi initial weights equal identical halving algorithm 
fi wm gradually decreases influence functions large number mistakes gives functions mistakes high relative weights 
single inconsistency eliminate function 
suppose wm applied pool functions sequence trials anomalies respect case wm constant times log jf mistakes constant depends fixed parameter fi 
case vapnik chervonenkis dimension vc omega gamma jf lower bounds imply wm optimal multiplicative constant 
general case wm applied pool algorithms show upper bounds number mistakes sequence trials 
log jaj algorithm mistakes 

log jaj subpool algorithms mistakes 

log jaj total number mistakes subpool algorithms note subpool size omega gamma jaj bounds cases respectively 
give example case applied 
suppose instance domain boolean functions form called threshold functions iff distinct integers range suppose wish design prediction algorithm small mistake bound target class threshold functions vary range 
algorithm winnow lit fixed threshold algorithm lit bound kn log mistakes 
obtain bound priori knowledge know upper bound value mistake bound winnow improved kr log grows logarithmically remain small 
improvement mistake bound obtained choosing parameters winnow depend denote winnow tuned receives sequence consistent threshold function number mistakes greatly exceed overcome knowing applying wm pool fa ne results follows number mistakes manner sequence consistent threshold function bounded constant times log log kr log 
applications weighted majority algorithm consider fall categories 
category illustrated previous example combine small number algorithms produce combination computationally efficient mistake bounds 
examples show weighted majority algorithm powerful tool constructing new efficient learning algorithms 
mistake bounds prove hold versions wm modifies weights mistake version modifies weights trial multiplicative changes described 

cases types prediction algorithms available choice parameters learning algorithm learner unsure choice best 
resulting algorithm efficient pool consists small number efficient algorithms 
second category involves applying weighted majority algorithm pools functions 
gives mistake bounds grow close optimal rate number anomalies grows establishes best achieved respect deterministic algorithms 
function classes interest large wm computationally practical 
note smaller pool size required efficiency small mistake bounds pool algorithms need simulated number mistakes grows logarithmically size pool 
section prove mistake bounds weighted majority algorithm wm show sense select predictions right subpool algorithms 
section discuss modification wm decreases weights certain lower threshold 
variant wm call wml stronger selective capabilities 
suppose sequence trials algorithm pool mistakes say initial segment sequence second algorithm mistakes second segment sequence forth 
assume original sequence partitioned segments 
wml priori knowledge segments different segments algorithms perform segment 
show number mistakes wml bounded constant times log jaj constant depends parameters algorithm 
example suppose algorithms pool functions segment sequence consistent function pool 
intuitively means sequence labeled target function pool segment target function changes 
time target changes new function cost log jaj mistakes mistake bound wml 
describe details modification section 
section investigate second variant wm deals countably infinite pools algorithms indexed positive integers 
barzdin freivalds bf bf considering pools recursive functions show algorithm log log log log log mistakes sequence trials consistent th function 
adaptation method applicable pools algorithms case algorithm pool consistent sequence trials algorithm mistakes 
describe variant wmi weighted majority algorithm property countably infinite pool sequence trials index number mistakes bounded constant times log number mistakes th algorithm pool sequence trials 
give number upper bounds obtainable different initial weights algorithms 
variant wmi incorporates method barzdin freivalds lets deal explicitly computational imprecision 
application techniques deal unknown parameter 
classes diff discussed property function class certain depth non negative integer 
basic algorithm requires input estimate depth target 
mistake bounds derived assumption estimate 
large actual depth obtain bound larger actual depth 
version wmi deal case estimate depth unavailable 
algorithm wmi applied infinite pool algorithm pool basic algorithm pool member different estimate depth 
algorithm wmi priori upper bound depth finite pool suffice 
constitutes example weighted majority techniques lead efficient algorithm learning parameterized function class provided efficient algorithm know parameter 
section generalize wm wmg able handle pools members produce predictions chosen interval 
wmg uses weighted average predictions pool members form prediction predicts average larger average average predictions wmg labels binary 
section prove bounds continuous variant wmc wm allows predictions algorithms pool master labels 
wmc simply predicts weighted average predictions pool members 
purpose section unified treatment proofs upper bounds wmg wmc randomized version wmr introduced section 
concluding section give table compares varieties weighted majority algorithm discuss 
simple specialized proofs upper bounds wmg wmr provided appendix 
section discuss properties randomized version wmr weighted majority algorithm 
main result section expected mistake bound wmr 
proof relies lemmas prove bounds wmg wmc previous section 
deterministic algorithms wmg wmc randomized algorithm wmr uses weighted average predictions pool members making prediction predicts probability equal average 
alternate interpretation randomized algorithm involves making prediction choosing member pool random probability proportional current weight predicting probability equal prediction chosen pool member 
randomized version algorithm property weights updated rate wmr expected mistakes long run arbitrarily close rate best prediction algorithm underlying pool mistakes 
appropriate measure loss holds deterministic algorithm wmc predictions allowed 
represents improvement factor limiting mistake bound give deterministic algorithm wmg predictions binary 
consider sections special case basic algorithm wm applied pools functions 
section assume pool contains function consistent trials 
bound number mistakes wm ith function pool consistent 
changing initial weights decrease expense increasing 
certain classes functions characterize sets possible mistake bounds weighted majority algorithm show classes functions algorithm better 
section consider case function pool consistent trials prove lower bound rate mistake bounds grow 
proving mistake bounds weighted majority algorithm number anomalous trials grows 
compare lower bound upper bounds obtain weighted majority algorithm 
certain conditions weighted majority algorithm provably small constant factor optimal 
similar comparison randomized algorithm wmr 
concluding section section gives overview various algorithms introduced mentions number directions research 
markowsky wegman applied algorithm similar wmc countably infinite pool wmi completely different setting 
countably infinite indexed pool conditional probability distributions goal iteratively construct master conditional probability distribution assigns probability examples seen far close highest probability assigned examples conditional probability distribution pool 
learning classes boolean functions ho 
presents bayesian approach considers average case upper bounds loss prediction algorithms gives upper bounds loss terms dimension vc 
bayes optimal classification algorithm consider special case weighted majority algorithm wm randomized version wmr gibbs algorithm ho 
furthermore algorithm wmr similar learning procedure studied lts 
analysis different 
notations assumptions design various master algorithms predictions pool algorithms predictions 
algorithm pool initially positive non negative weight updated trial 
default values initial weights 
total initial weight algorithms pool denoted init total final weight examples processed fin logarithms ln denote natural logarithms log denote logarithms base 
choice base significant big little notation formulas consisting ratios logarithms omit designation base ratios intend base chosen numerator denominator 
shall implicitly assume sequences instances labels finite 
note fixed mistake bound holds finite sequences hold infinite sequences 
proving mistake bounds weighted majority algorithm section prove bounds number mistakes basic weighted majority algorithm wm sections predictions algorithms pool master algorithm binary 
recall description wm 
trial denote total weight algorithms pool predict respectively 
generalizations wmg wmc deterministic master algorithm wm section 
master algorithms allow predictions algorithms pool continuous binary 
predictions wmg binary predictions wmc allowed continuous 
theorem gives bounds wmg theorem slightly better bound wmc theorem 

proving mistake bounds weighted majority algorithm parameter fi factor weights multiplied case mistake range fi 
suppose run wm pool prediction algorithms indexed integers jaj 
notation introduced section 
proofs surprisingly simple 
show trial mistake occurs sum weights times sum weights trial 
trials mistake occurs total weight may decrease 
init fin hold number mistakes wm implies log init fin log proof uses fi theorem sequence instances binary labels 
number mistakes wm sequence applied pool log init fin log fi proof 
discussion need show trials wm mistake ratio total weight trial total weight trial fi trial total weight suppose loss generality trial learner prediction case total weight trial gammafi gamma fi 
note fin fi denotes initial weight ith algorithm pool denotes number mistakes algorithm sequence fi convention 
discussed fi initial weights equal wm halving algorithm 
case positive fin bound theorem vacuous 
corollaries assume fi initial weights 
assumptions notation theorem 
corollary assume pool prediction algorithms number mistakes th algorithm pool sequence instances binary labels 
wm applied pool equal initial weights log log fi log fi mistakes sequence jaj 
proof 
follows theorem fact init fin fi note initial weights assumed equal log bound need replaced log init initial weight ith algorithm 
corollary assume pool prediction algorithms subpool size algorithms subpool mistakes sequence instances binary labels 
wm applied pool equal initial weights log log fi log fi mistakes sequence proof 
follows theorem fact init fin kfi 
shifting target corollary assume pool prediction algorithms subpool size algorithms subpool mistakes total sequence instances binary labels 
wm applied pool equal initial weights log log fi log fi mistakes sequence proof 
loss generality algorithms subpool 
fin fi sum kfi easy derive bound corollary previous theorem 
similar bounds corollaries proven case fi number mistakes subpool 
case upper bound number mistakes wm log shifting target crux shown far fi wm selects right information pool algorithms fi allows home right information fi assures change gradual update leads away goal reversed 
cost changing weights gradually algorithm home fast 
modify wm recovery capabilities stronger 
suppose particular unknown subpool pool algorithms predictive performance characterized corollaries 
number trials different subpool performance performance original subpool degrades 
third subpool takes forth 
want modify wm keeps track predictions right subpool additional mistakes 
follows consider size 
generalizations spirit corollaries easily obtained 
scenario algorithm pool mistakes initial number trials 
algorithm pool performs number trials forth 
modified weighted majority algorithm wml priori knowledge algorithm prediction currently accurate trials 
weighted majority algorithm wml algorithm fixed parameters fi fl ranges fi fl respectively 
algorithm pool receives positive initial weight 
algorithm wml identical wm original algorithm updates current weight multiplying fi modified algorithm update weight update larger fl jaj times total weight algorithms trial 
note case fl wml identical wm lemma sequence instances binary labels minimum number mistakes sequence pool algorithms 
initial weight algorithm times total initial weight wml applied pool log log fi log mistakes fi gamma fi fl 
furthermore final weight algorithm times total final weight 

selection infinite pool proof 
assume loss generality prediction wml trial mistake 
weights pool members predicting decreased 
total weight trial algorithms predicting respectively 
total weight algorithms pool predicted weights changed trial small 
assumption fl 
total weight trial fi gamma fi gamma fi ratio total total trial bounded fi gamma fi fl 
assumed fl bound 
bound wml follows argument previous section fact final sum weights init fi easy see total weight increased individual weights decreased small final weight algorithm specified relation total final weight 
apply lemma subsequences sequence trials get 
theorem sequence instances binary labels delta delta delta partitioning subsequences 
number trials th subsequence 
pool algorithms minimum number mistakes algorithm pool subsequence initial weights algorithms times total initial weight wml applied pool min fl log log fi log mistakes fi gamma fi fl 
proof 
simply apply previous lemma subsequence selection infinite pool section assume countably infinite pool algorithms indexed computable indexing assume exists simulation algorithm pool simulate algorithm pool index 
assume algorithm mistakes sequence instances labels develop version weighted majority algorithm inf log mistakes constant depends parameters algorithm 
assume moment original algorithm wm keep track infinitely weights suppose example initial weight associated ith pool member 
total initial weight init 
final sum weights fi fi applying theorem get wm inf log log fi log fi mistakes applied countably infinite pool sequence approach flawed obviously run wm infinitely weights 
construct algorithm wmi uses increasing initial segment 
selection infinite pool active weights algorithm analysis techniques introduced barzdin freivalds bf consider fi case 
techniques addition dealing infinite pool size allow finite precision approximations weights calculating predictions algorithm 
consider issue analysis algorithm taken example take approximate computations account 
weighted majority algorithm wmi algorithm run countably infinite pool algorithms indexed positive integers 
fixed parameter fi range fi 
computable functions defined positive integers needs computable arbitrary precision determine initial weights second function satisfying lim 
flexibility choice assume convenience chosen exactly computable 
algorithm similar wm working weights algorithms wmi works weights active subpool trial 
prediction algorithm solely predictions active subpool weights corresponding algorithms active subpool subject change 
active subpool consists pool members indices determined algorithm 
value initialized zero 
initially trial mistake increased necessary inequality gammafi satisfied fi number mistakes 
increased weights newly active algorithms initialized 
initial weight th algorithm pool set 
prediction wmi computes sum weights active pool members predicting current instance sum weights active pool members predicting 
suppose total mistakes previous trials 
gammafi wmi predicts gammafi wmi predicts 
inequalities holds wmi allowed predict 
allows wmi finite precision approximations label received trial wmi mistake weight active pool member disagreed label multiplied fi 
final action trial mistake increase necessary described earlier 
sums tails series easy compute natural take 
done sum initial values full infinite sequence weights 
note storing current weights implementation wmi store number mistakes pool member active information calculate necessary precision 
theorem sequence instances binary labels ith algorithm mistakes 
computable functions satisfying inequality description wmi initial sequence trials current value th weight wmi name wmi distinguish algorithm similar algorithm called wmi earlier versions lw lw 
version improves best mistake bound obtained algorithm match essentially optimal bound barzdin freivalds bf bf case fi exists consistent pool member 
earlier version wmi lw lw take finite precision account 

selection infinite pool fi holds wmi applied countably infinite pool sequence 
trial size active pool minimum gammafi number mistakes wmi prior trials 

initial sequence trials mistakes wmi gamma mc 
fi total number mistakes wmi inf log log fi log log 
fi total number mistakes wmi bounded log 
proof 
follows immediately construction wmi proof 
prove induction clearly holds 
suppose claim holds consider trial st mistake 
sum weights trial active pool members predicting sum weights active members predicting sum initial weights inactive members 
gammafi 
suppose wmi predicted 
analogous argument applies case wmi predicted 
gammafi st mistake causes total weight updated gamma fi gamma gamma fi fi gamma fi gamma fi fi gamma mc gamma fi gamma fi mc gamma desired 
proof 
consider case fi 
th algorithm pool mistakes final sum weights fi 
wmi mistakes final sum weights mc 
solving resulting inequality gives desired upper bound final sum weights 
fi wmi mistakes case yields desired bound 
vary choices initial weights encounter trade size mistake bound obtain size active pool may grow 
weight sequence earlier algorithm wmi lw lw uses slightly smaller active pool expense slightly larger mistake bound 
see lw lw corresponding versions corollaries 
selection infinite pool examine trade corollaries follow immediately 
brevity omit statement results fi 
results regarding fi case section 
corollary sequence instances binary labels ith algorithm mistakes 
holds wmi applied countably infinite pool sequence 
mistakes wmi size active pool gamma fi fi gamma 
fi total number mistakes wmi inf log log fi log fi section derived bound running wm infinitely weights 
bound infeasible algorithm identical corollary absence numerator 
note corollary pool size grows exponentially course improvement infinite pool size 
section study best done fi regard computational complexity give weight sequence gives somewhat smaller mistake bound bound corollary 
pool size 
rest current section take point view interested practical application algorithm questions computational efficiency size active pool important 
want pool size grow exponentially number mistakes particularly cases number mistakes grows linearly number trials expected example learner faces noisy data see section 
choosing different weight sequence assure pool size grows linearly increases rate growth mistake bound index corollary dependence index logarithmic linear 
corollary sequence instances binary labels ith algorithm mistakes 
gamma holds wmi applied countably infinite pool sequence 
mistakes wmi size active pool log gamma fi log log 
total number mistakes wmi inf log fi log fi 
observe weights chosen corollaries useful particular applications 
example omega gamma choosing weights sequence decreasing exponentially done second corollary increases mistake bound 
selection infinite pool constant factor bound choice weights corollary 
cost increasing mistake bound constant factor significantly smaller pool size obtained exponentially decreasing weight sequence 
general infinite sequence algorithms mistake bounds reasonable choose initial weights 
finite 
summands log log fi numerator bound total number mistakes part theorem roughly equal 
corollary gives weight sequence useful grows exponentially similar version applied 
corollary sequence instances binary labels ith algorithm mistakes 
gamma gamma holds wmi applied countably infinite pool sequence 
mistakes wmi size active pool log log gamma fi log log fi 
total number mistakes wmi inf gamma log fi log fi 
proof 
inequality parts corollary straightforward application parts respectively previous theorem 
show inequality bound infinite sum geometric series ratio summand gamma gamma gamma gamma give example making results corollary 
example generalization threshold function example 
assume learn class target functions algorithm takes single parameter associated target function parameters thought likes measures complexity target function 
assume algorithm property parameter number mistakes bounded delta target function parameters parameter assumed bound number mistakes fact number mistakes algorithm unbounded 
assume choices bound form minimized choose parameter 
claiming algorithm necessarily fewest mistakes target function best bound obtain information obtained bounds accurately reflect behavior algorithm choice 
goal construct algorithm number mistakes close knowing target function parameters chosen target class 

generalized analysis assumptions mistake bound range 
suffices purposes apply wmi infinite pool algorithms 
optimal choice may included pool algorithm included bound twice bound optimal choice 
mistake bound th algorithm pool gamma gamma gamma bound grows exponentially weights corollary serve 
corollary get mistake bound gamma log fi log fi positive integer gamma assumption expression bounded gamma log fi log fi delta exists gamma obtain bound log fi log fi 
active pool size algorithm grows logarithmically number mistakes 
generalized analysis section introduce new master algorithms wmg wmc 
original algorithm wm special case generalized version wmg 
generalized analysis new variants section deriving bounds randomized version wmr wm purpose section give unified analysis versions wmg wmc wmr 
separate direct simple proofs upper bounds wmg wmr provided appendix 
assumptions wmg wmc new algorithms allow predictions algorithms pool chosen binary wm 
predictions wmg binary predictions wmc allowed chosen interval 
labels associated instances assumed binary wmg wmc 
update step weighted majority algorithm variants step weight multiplied factor 
algorithm wm step occurs trials mistake 
easy see mistake bound obtained updates performed trial 

generalized analysis update criteria wmg wmc wmc updates trial criterion wmr introduced section 
wmg update step executed trial trials mistake occurs 
introduce notation enable refer values occur trial update step occurs 
term update trial refer jth trial update step occurs 
assume total trials 
denotes total number trials total number mistakes depending update criterion 
assume master algorithm applied pool algorithms letting denote prediction ith algorithm pool update trial denote prediction master algorithm update trial ae denote label update trial denote weights update trial 
consequently denote weights final trial 
assume initial weights positive 
fl init fin prediction wmc wmg case wmc prediction equals fl wmg fl greater fl prediction allowed fl 
predictions continuous notion mistake replaced quantity measures far prediction correct label 
absolute loss 
algorithm predicts trial label ae say loss trial jx gamma aej definition applies algorithms pool master algorithm 
denote total loss master algorithm trials total loss trials algorithm pool update occurs trials master algorithm mistake total loss master algorithm gamma ae algorithms pool may incur loss trials updates updates occur trials get total loss algorithms pool summing update trials 
inequality jx gamma ae note case predictions pool members labels binary losses pool members numbers mistakes similarly predictions master algorithm labels restricted binary loss master measure mistakes 
form update possible 
specify class possible updates bounds apply members class 
updates class coincide update wm special case pool members produce boolean predictions 
update step wmg wmc update step wmg wmc randomized version wmr section weight multiplied precisely bounds wm wmg hold long update step occurs trial mistake occurred possibly trials mistake occurred simplicity restrict discussion extreme cases mentioned 

generalized analysis factor depends fi ae factor satisfies fi jx gammaae gamma gamma fi jx gamma ae lemma implies factor exists particular upper lower bound chosen update factor 
recall defined equal 
lemma fi fi fi gamma 
proof easy check inequality case fi 
fi inequality follows convexity fi function fi 
convexity implies fi gamma fi rfi way writing inequality 
lemma basic lemma derive bounds loss master algorithms wmg wmc expected loss wmr section 
lemma assume assume fi ae assume gamma gamma fi jx gamma ae fi jfl gamma ae tg fin 
ln fin init ln gamma gamma fi jfl gamma ae proof deal case fi trial jfl gamma ae 
case fl gamma ae occur jx gamma ae 
forces update factors fin desired 
case occur inequality gamma gamma fi jx gamma ae gamma gamma fi jx gamma ae triangle inequality bounded gamma gamma fi gamma ae gamma gamma fi jfl gamma ae gamma gamma fi jfl gamma ae gamma gamma fi jfl gamma ae theorems obtained requiring satisfy lower bound specified inequality bounds theorems grow shrinks bounds may uninteresting small 
bounds inequality obtain corollaries 

generalized analysis logarithms gives desired result 
lemma obtain bound wmg wm theorem 
recall wmg ae allowed chosen 
case discrete algorithms wm wmg identical 
theorem sequence instances binary labels 
suppose algorithm wmg updating trial mistakes run fi sequence total number mistakes wmg applied pool prediction algorithms 
log init fin log fi proof fi jfl gamma ae tg fin bound vacuous 
wmg mistake update trial 
total number mistakes wmg 
version wmg updates mistake paying attention trials mistake 
note fl ae fl gamma ae log gamma gamma fi jfl gamma ae log gamma gamma fi jfl gamma ae log gamma gamma fi jfl gamma ae log gamma gamma fi log fi easy see conditions application lemma apply log fin init log fi simple proof bound rely lemma appendix 
method redefine essentially proof theorem 
definition total loss ith algorithm definition update step inequality fi fin fi inequalities equalities update occurs trial update factors small allowed 
inequalities corollaries spirit corollaries easily derived 
state corresponding corollary 
corollary assume initial weights equal fi 
corollary assume pool prediction algorithms total loss th algorithm pool sequence instances binary labels 
wmg applied pool equal initial weights log log fi log fi mistakes sequence jaj 

generalized analysis general larger lower bound fin better upper bound provided theorem 
update smallest allowable factor inequality fin fi largest factor update get fin gamma gamma fi jx gamma ae may larger previous lower bound fin fi close 
upper bounds derive wmc wmr form theorem wmg 
comments lower bound fin apply bounds 
lemma deriving bounds wmc wmr section 
lemma conditions lemma satisfied jfl gamma ae ln init fin gamma fi proof lemma follows observation ln gamma gamma fi jfl gamma ae gamma gamma fi jfl gamma ae lemma 
recall wmc predictions labels allowed prediction wmc trial fl lemma gives upper bound total loss wmc section see bound identical bound expected total loss obtain wmr theorem 
recall wmc wmr update trial 
wmc assume instances labels deterministic 
theorem sequence instances labels labels 
total loss wmc sequence applied pool prediction algorithms 
ln init fin gammafi proof theorem follows immediately lemma 
interesting compare bound wmc continuous prediction bound obtained theorem wmg discrete prediction 
bound similar form ln init fin ln fi numerators bounds identical 
denominators approach zero fi approaches 
denominator bound wmc larger fi 
ratio denominators approaches fi approaches making bound wmc better bound wmg nearly factor fi close 
corollary assume pool prediction algorithms total loss th algorithm pool sequence instances labels 
apply wmc pool equal initial weights total loss ln ln fi gammafi sequence jaj 

randomized predictions randomized predictions section give randomized version wm called wmr 
assumptions wmr predictions pool members 
prediction wmr binary probabilistic 
labels associated instances binary 
notation introduced previous section 
recall trial wmc predicts fl weighted average prediction pool members 
wmg predicts fl fl fl 
prediction wmr new randomized algorithm wmr simply predicts probability fl note predictions members pool binary fl total weight algorithms predicting trial defined similarly 
update criterion wmr wmc wmr updates trial 
update step wmr update step wmr update step wmg wmc described previous section inequality 
bounds performance wmr depend adequate independence randomization performed wmr choice instances labels 
obtain bounds expected number mistakes wmr assumption refer weak independence condition ae ae fl 
conditional expectations respect random variables described example bil shi 
notation stands surely probability 
variables appearing condition defined section 
note definition allows ae random variables 
deterministically chosen weights fl deterministic construction algorithm guarantees weak independence condition holds 
case weak independence condition hold suffices fl follows construction algorithm 
addition bounds expected number mistakes give bound probability total number mistakes exceeds expected number margin 
bound assumption refer strong independence condition ae ae gamma fl note addition conditioning past predictions conditioning entire past sequence instances labels 
satisfaction strong independence condition implies satisfaction weak 
independence conditions hold variety assumptions way randomization wmr performed way instances labels chosen 
assume instances labels generated deterministically randomly paying attention predictions algorithm 
weak independence 
randomized predictions condition depend arbitrarily predictions algorithm previous trials 
case generation instances labels independent randomization algorithm weak independence condition hold random choices algorithm independent 
example way implement randomization wmr choose real number uniformly range predict fl purpose obtaining expected mistake bounds matter random numbers trial chosen independently fact single choice initially number trial 
case randomization algorithm confined single choice 
new value chosen independently trial independently choice instances labels strong independence condition holds 
way perform randomization 
prediction choosing member pool random probability proportional current weight predicting pool member predicts predicts 
pool member prediction strictly prediction determined biased coin flip probability predicting equal value prediction chosen pool member 
probability predicting fl random choices version wmr independent choices instances labels strong independence condition holds 
special case fi predictions pool members binary maass observed independent research maa suffices new random choices just trials mistake 
mistake trial uses pool member prediction trial 
maass views algorithm wmr special case lazy update criterion different perspective obtained elegant derivation ln bound expected number mistakes wmr algorithm pool algorithms equal initial weights consistent examples 
appendix show random choices trial mistake independent strong independence condition holds wmr lazy update criterion restrictions fi predictions pool members binary 
section show weaker result weak independence condition holds restrict instances predictions pool members deterministic 
assume equal initial weights 
theorem gives ln bound expected number mistakes pool member consistent examples 
instances predictions pool members deterministic show weak independence condition holds suffices show fl sufficient show trial member predict equally remaining consistent pool members 
choice pool member independent trial 
induction trial number prove claim 
claim clearly holds trial 
note sequence instances algorithms pool deterministic determination pool members consistent trial probabilistic event 
probability choosing particular inconsistent pool member response trial 
probability choosing particular consistent pool member sum probability chosen member previous trial 
randomized predictions plus probability mistake previous trial times probability choosing particular member trial 
consistent members previous trial current trial induction hypothesis probability making mistake previous trial gamman probability choosing particular consistent member trial gamman proving claim 
theorem gives bound expected number mistakes wmr weak independence condition holds 
theorem sequence instances binary labels 
number mistakes wmr sequence applied pool prediction algorithms 
weak independence condition ln init fin gammafi proof weak independence condition gamma ae ae ae jfl gamma ae gamma ae jfl gamma ae 
gamma ae jfl gamma ae 
desired bound follows immediately lemma 
predictions algorithms pool labels chosen deterministically weights deterministic bound theorem ln init fin gammafi bound identical bound total loss proven wmc theorem 
comparison section bound wmg applies 
bound wmr bound wmc better bound wmg nearly factor fi close 
case wmc improvement obtained allowing predictions interval predictions binary wmg improvement comes randomization 
vovk shown different rule computing randomized prediction obtain somewhat better expected mistake bound ln init fin ln fi exactly factor better deterministic bound 
bound obtained vovk bound algorithm approach fi approaches 
obtain corollary analogous corollary wmg 
corollary assume pool prediction algorithms total loss th algorithm pool sequence instances binary labels 
wmr applied pool equal initial weights expected number mistakes log log fi gammafi sequence jaj 
proof follows immediately theorem fact fin fi case strong independence condition holds chernoff bounds obtain bounds probability actual number mistakes larger expected number 

randomized predictions theorem regarding chernoff bounds applied lit 
probability space oe algebras contained sequence random variables probability space 
sequence delta delta delta measurable js finite jg gamma 
theorem lit probability space delta delta delta oe algebras contained sequence random variables probability space measurable random variables gamma 
ff measurable random variable 
sequence statement holds point 
ff gammaff obtain bound 
theorem sequence instances binary labels pool prediction algorithms 
suppose wmr applied run sequence 
random variable giving number mistakes 
suppose strong independence condition holds 
contains instances functions ae ae mj ae ae 
bj ae ae gammaa proof suppose trials 
gamma ae ae ae gamma mj ae ae 
gamma 
denote oe algebra generated ae ae denote oe algebra generated note measurable fl function instances labels measurable 
note jg ae ae ways writing thing 
strong independence assumption gamma gamma jg gamma jfl gamma ae gamma measurable equals gamma jg jg gamma 
jg gamma gamma gamma jg gamma gamma 
pools functions ff ab hypotheses theorem satisfied yielding gamma gammaff way writing desired inequality 
give application theorem section 
instances labels deterministically chosen 
theorem random instances labels case exists constant gamma ffi small ffi 
ae ae gammaa 
gammaa ffi 
pools functions section consider case weighted majority algorithm applied pool functions 
functions instance domain 
think pool functions pool prediction algorithms interpreting function pool prediction algorithm predicts trial instance results section apply case exists function pool consistent sequence trials 
section consider deterministic algorithms case predictions binary master algorithm wm section independent sections 
study achieved weighted majority algorithm desires obtain better mistake bounds functions pool algorithms expense worse bounds 
show certain strong restrictions target class wm optimal deterministic algorithm strong sense deterministic line prediction algorithm exists way choose initial weights wm function target class mistake bound wm small mistake bound 
note stronger just saying worst case mistake bound wm target class larger worst case bound target class 
section turn finite infinite target classes giving results regarding sequences mistake bounds possible arbitrary target classes 
theorem pool functions range non negative integers gammam 
algorithm wm applied pool initial weights gammam fi sequence trials consistent function pool algorithm mistakes 
proof theorem obtain upper bound number mistakes wm log gammam gamma log gammam number mistakes integers gives desired bound 

pools functions show special case weighted majority algorithm optimal 
say set valued functions domain shattered ranges ranges notion shattering considered ass 
dual notion shattering define vapnik chervonenkis dimension concept class vc 
pool functions shattered domain lower bound matching upper bound number mistakes deterministic algorithm 
note applicability optimality result follows limited 
simple counting argument shows domain shatter function class requires size function class greater log jx give lemmas need 
lemma suppose positive real numbers quotient integer gamma 
integer exists proof done 
largest integer gamma hypothesis gamma choice complete proof lemma show case see note exist integers gamma lr assuming lr desired 
lemma suppose log log integers necessarily positive suppose integer max exists set jg proof permutation delta delta delta log gammak integer gamma 
similarly integer 
lemma exists theorem collection valued functions shattered domain domain finite infinite deterministic line learning algorithm denote maximum number mistakes sequence consistent assume finite 
gammam proof give rough outline proof 
adversary constructs sequence trials learning algorithm 
adversary assigns function target class weight equal gammam weights remain fixed trial generated adversary sets zero weights functions consistent trials generated 
adversary picks instances labels learner mistake trial matter learning algorithm sum weights consistent functions decreases approximately factor trial 
eventually consistent functions eliminated trial consistent function weight substantial fraction sum weights remaining consistent functions 
gammam sum weights remaining consistent functions roughly gammat number 
pools functions trials generated 
weight consistent function gammam argument show value gammat gammam bounded small constant call 
learner mistakes consistent trials gammam 
fill details give bound theorem 
give details proof 
adversary maintains weights corresponding functions variables initially gammam trials generated repeating procedure iteration final generates trial 
denote number trial generated current iteration 
iteration incremented iteration 
rth iteration adversary sets blog kr kr 
note necessarily positive 
max kr gamma adversary stops generating trials 
total gamma trials generated 
apply lemma lemma corresponding current values 
iterations zero case apply lemma subsequence consisting non zero 
max kr gamma lemma tells exists ng kr gamma adversary chooses instance current trial possible target class shattered domain 
chooses label ae equal prediction algorithm 
argue function target class consistent pairs instances labels chosen adversary 
learner mistake trial 
preparation iteration adversary sets zero ae adversary continues generate trials repeating strategy terminates described 
moment argue terminate 
note rth iteration kr strategy terminate iteration set determined iteration kr gamma kr gamma adversary updates iteration decrease kr gamma furthermore new value kr gamma strategy terminate non zero set zero iteration 
argument previous paragraph tells rth iteration gamma gamma time kr gamma gamma 
gamma gamma 
gamma gamma 
holds termination occurs iteration total trials generated termination occurs st iteration time max gamma gamma show choose target function choose iteration max initial value 
substituting definitions inequality previous paragraph obtain gammam blog gammam gamma get 
pools functions gamma log gammam non zero termination function consistent labels target function 
learner mistakes log gammam log gammam gammam desired 
suppose set integers want construct learning algorithm mistakes target function turns domain shatters target class corollary tells algorithm accomplish way choose initial weights wm 
corollary collection functions range shattered domain suppose line learning algorithm positive integers mistakes sequence trials consistent suppose algorithm wm applied pool initial weights gammam fi 
exists range sequence trials consistent wm mistakes 
proof theorem gammam 
obtain desired mistake bound theorem 
obtain interesting results infinite pools algorithm wmi theorem infinite sequence non negative integers 
assume values computable function statements gammam finite value computed arbitrary precision 
exists domains computably indexed collections total recursive functions exists line prediction algorithm positive integers sequences trials consistent algorithm mistakes 
domains computably indexed collections total recursive functions exists line prediction algorithm integers sequences trials consistent algorithm mistakes 
gammam finite 

pools functions interesting state contrapositive implication gammam diverges exists domain computably indexed collection total recursive functions line prediction algorithms exists integer sequence trials consistent algorithm greater mistakes 
holds positive fact exist infinitely proof statement weaker 
prove theorem suffices demonstrate implies implies 
see implies construct line prediction algorithm wmi apply wmi pool initial weights gammam construct function needed wmi ability compute sum initial weights arbitrary precision 
coupled ability compute initial partial sums weights lets compute upper bounds gammam approach goes infinity 
set parameter fi wmi zero 
consistent sequence trials theorem gives bound log 
take log 
implies choose domain consist finite strings 
choose collection functions functions defined string length ith bit 
choose appropriately obtain mistake bounds promised hypothesis 
notice collection shattered domain 
algorithm mistake bounds gamma theorem 
partial sums gammam increasing bounded series converges finite sum desired 
algorithm parts version wmi indicated preceeding proof take constant part log 
value computable upper bound gammam making upper bound sufficiently precise take log gammam 
additive constant replaced constant greater 
choose constant regardless target class proof indicates gamma gamma log gammam gamma 
theorem lets rederive results barzdin freivalds bf bf corollary 
corollary bf bf algorithm function denote maximum number mistakes sequence trials consistent domain computably indexed collection total recursive functions exists line prediction algorithm finite log log log log log 
exists computably indexed collection total recursive functions line prediction algorithm exist infinitely log log log proof part follows immediately theorem fact ln ln ln converges 
part follows theorem fact ln diverges 
see 
anomalies statement contrapositive final implication theorem appears immediately theorem 
obtain result slightly weaker form bf 
lower bound log gamma 
theorem exists domain computably indexed class total recursive functions line prediction algorithm exists sequence trials consistent function blog nc mistakes 
proof choose domain target class proof implication theorem 
mistake bound algorithm sequences consistent gammam 
max ng gammam 
log gamma 
integer implies blog nc desired 
anomalies continue consider pools functions requirement shattering imposed part previous section 
section consider case function pool consistent trials 
respect function define number anomalies sequence trials number trials inconsistent function 
pool functions say sequence anomalies minimum number anomalies sequence respect function case necessarily say particular trials anomalous example may functions minimizing number anomalies may consistent different trials 
previous section assume functions pool valued 
section consider randomized algorithm wmr wm assume instances labels chosen deterministically 
note types situations anomalies arise sequence trials fact consistent target function function pool instance appears sequence different labels different appearances 
may occur errors instances labels insufficient information reported instance uniquely determine appropriate label 
upper bounds apply types situations 
lower bounds second type situation 
give lower bound shows rate mistake bound deterministic version wm grows number anomalies appropriate choice fi arbitrarily close best possible deterministic prediction algorithms 
show rate expected mistake bound wmr grows arbitrarily close best possible rate growth randomized algorithms 
class functions collection sequences trials anomalies 
define opt minimum deterministic algorithms maximum sequences number mistakes value opt best bound class obtained deterministically presence anomalies 
define opt rand minimum algorithms including randomized algorithms 
anomalies maximum sequences expected number mistakes randomization assumed independent choice sequence 
theorem target classes jf opt opt 
proof opt 
jf 
saying opt equivalent saying deterministic learning algorithm exists function sequence trials having anomalies respect mistakes sequence trials 
algorithm show adversary choose function sequence instances mistakes 
notion mistake tree lit lit 
mistake tree pool functions domain binary tree nodes non empty subset internal nodes labeled point root internal node labeled left child node subset consisting functions 
left child subset non empty 
right child set functions 
subset non empty right child 
complete mistake tree mistake tree complete binary tree height 
define height tree length edges longest path root leaf 
shown lit lit pool functions exists complete opt mistake tree 
adversary strategy divided stages 
stage adversary keeps track current mistake tree 
initially complete mistake tree adversary proceeds immediately second stage 
instance chosen adversary label root tree 
algorithm predicts adversary tells algorithm prediction wrong 
response adversary eliminates functions possible target functions 
subtrees root adversary current mistake tree complete gamma mistake tree remaining candidate functions 
adversary sets current mistake tree subtree 
chooses instance label root new current tree 
adversary continues manner forcing algorithm wrong instance 
mistakes adversary current tree complete gamma mistake tree remaining candidate target functions 
long root current tree children corresponding non empty subclasses adversary choose point label root force algorithm mistake 
adversary continues stage gamma mistakes 
stage instance pair functions consistent gamma trials 
instance label root final mistake tree adversary functions chosen left right children root 
adversary generates trials instance trial adversary chooses label unequal prediction algorithm 
suppose loss generality majority second stage trials generated labels consistent trials inconsistent sequence trials anomalies respect algorithm mistakes desired 

anomalies case anomalies mistake bound wm log jf log fi log fi see corollary 
fi approaches coefficient approaches optimal value 
time coefficient log jf approaches infinity 
compromise needed 
fi bound log jf log concept classes opt log jf strictly additional gap lower bound give bound wm establish lower bound randomized algorithms essentially argument 
lower bound applies case choice instances labels dependence predictions algorithm applies model dependence allowed 
theorem target classes jf opt rand opt proof describe adapt proof theorem 
require adversary compute sequence instances labels ahead time want sequence depend random choices algorithm 
assume adversary full knowledge algorithm 
adversary proceeds exactly choosing labels determines probability algorithm predict current trial preceding sequence instances labels chosen adversary 
probability conditioned previous responses algorithm 
adversary chooses label probability exceeds trial expected number mistakes giving lower bound half size lower bound obtained deterministic algorithms 
suppose run wmr fi pool size equal initial weights anomalies 
denote number mistakes wmr 
corollary gives upper bound expected number mistakes ln ln fi gamma fi section shown best possible value coefficient upper bound algorithm 
value upper bound wmr arbitrarily close choosing fi sufficiently close 
interesting look upper bound theorem situation 
instances labels assumed chosen deterministically theorem 
ln gamma fi ln fi gamma fi gammaa ln ln fi gammafi making fi close close factor ln fi gammafi multiplying arbitrarily close 
natural assumption number anomalies grows proportion number trials 
case sufficiently trials fi sufficiently close high confidence ratio number mistakes number anomalies 
contrast bound deterministic algorithm wm factor multiplying greater 

master predictions predictions labels bound comments algorithm pool members master algorithm wm binary binary binary ln init fin ln fi wml binary binary binary thm shifting target wmi binary binary binary thm infinite pools wmg binary binary ln init fin ln fi wmc ln init fin gammafi wmr binary binary ln init fin gammafi randomized predictions table summary weighted majority versions investigated various master prediction algorithms predictions pool algorithms predictions 
initially member pool positive weight 
weight pool member represents belief master algorithm predictions member 
weights updated depending predictions corresponding algorithms 
algorithms prediction master weighted average predictions pool members 
basic master algorithm predictions pool members master labels examples required binary 
algorithm master simply predicts way weighted majority pool members 
various variants basic algorithm introduced allow continuous predictions labels 
probabilistic variant 
table gives summary 
developed upper bounds predictive performance master algorithms terms performance algorithms pool 
applied wm various settings corollaries 
similar corollaries hold variants 
simplest corollary gives bound performance master function best algorithm specialist pool corollary 
version wml wm works cases different pool members specialists various sections trial sequence 
probabilistic versions versions allow continuous predictions capability tracking specialist easily developed 
holds version wmi wm allows pool countably infinite 
furthermore easy combine capabilities tracking specialist handling infinite pool size single algorithm 
tried keep exposition simple master algorithm focuses setting weighted majority techniques useful 
developed unified proof method bounds simple direct proofs unrelated 

acknowledgments note master algorithms spends little computation time required simulating algorithms pool done parallel 
number operations required trial compute prediction master algorithm update weights necessary linear size pool algorithms wmi linear size active subpool plus time required compute discussed techniques find best setting parameters algorithm establish loss bounds learning classes functions 
methods justify philosophy application find large pool candidate prediction algorithms application combine predictions master algorithms 
practical size pool depend available computational resources 
additional loss master best pool member essentially logarithmic size pool minimal computational overhead aside running pool member parallel 
design schemes removing algorithms pool weight degenerates reasonable number examples processed schemes replacing removed algorithms new trial candidates 
trial sequence non stationary previously bad algorithm may specialist point algorithms removed temporarily 
design master algorithms case algorithms pool available initial trials algorithms pool wake fall asleep various times 
master algorithms designing simple networks algorithms 
node input predictions combined output prediction master algorithms 
simplest network tree algorithms pool leaves wm algorithms internal nodes 
case mistake weights subpool children root predicted wrong updated recursively nodes weights updated weights subpool children predicted wrong updated 
hierarchical application advantage single master combines predictions pool 
master algorithms parameter fi fi measures drastic update 
smaller fi drastic update 
kept fi constant trials 
particularly statistical noise examples advantageous slowly increase fi time 
acknowledgments david haussler david helmbold wolfgang maass valuable discussions 
ang dana angluin 
queries concept learning 
machine learning 
ass patrick 
dimension 
ann 
inst 
fourier grenoble 
blumer andrzej ehrenfeucht david haussler manfred warmuth 
learnability vapnik chervonenkis dimension 
jacm 
bf barzdin freivalds 
prediction general recursive functions 
sov 
math 
dokl 
bf barzdin freivalds 
nyi prediction limit synthesis effectively enumerable classes functions 
barzdin editor theory algorithms programs volume pages 
state university 
russian 
bil patrick billingsley 
probability measure 
wiley new york 
alfredo george mark wegman 
learning probabilistic prediction functions 
proceedings workshop computational learning theory pages published morgan kaufmann san mateo ca 
fsv freivalds smith 
trade parameters affecting inductive inference 
information computation 
david haussler michael kearns robert schapire 
bounds sample complexity bayesian learning information theory vc dimension 
proceedings fourth annual workshop computational learning theory pages published morgan kaufmann san mateo ca 
ho david haussler manfred opper 
calculation learning curve bayes optimal classification algorithm learning perceptron noise 
proceedings fourth annual workshop computational learning theory pages published morgan kaufmann san mateo ca 
david helmbold robert sloan manfred warmuth 
learning nested differences intersection closed concept classes 
machine learning 
special issue second annual workshop computation learning theory santa cruz california 
lit nick littlestone 
learning quickly irrelevant attributes abound new linear threshold algorithm 
machine learning 
lit nick littlestone 
line batch learning 
proceedings second annual workshop computational learning theory pages published morgan kaufmann san mateo ca 
lit nick littlestone 
mistake bounds logarithmic linear threshold learning algorithms 
phd thesis technical report ucsc crl university calif santa cruz 
lts esther levin naftali tishby sarah solla 
statistical approach learning generalization layered neural networks 
proceedings second annual workshop computational learning theory pages published morgan kaufmann san mateo ca 
lw nick littlestone manfred warmuth 
weighted majority algorithm 
proceedings th annual symposium foundations computer science pages 
ieee 
appendix lw nick littlestone manfred warmuth 
weighted majority algorithm 
technical report ucsc crl university calif computer research laboratory santa cruz ca 
maa wolfgang 
line learning oblivious environment power randomization 
proceedings fourth annual workshop computational learning theory pages published morgan kaufmann san mateo ca 
pit pitt 
probabilistic inductive inference 
jacm 
ps pitt smith 
probability plurality aggregations learning machines 
information computation 
shi albert 
probability 
springer new york 
vc vapnik ya 
chervonenkis 
uniform convergence relative frequencies events probabilities 
th 
prob 
appl 
vovk 
private communication 
vovk 
aggregating strategies 
proceedings third workshop computational learning theory pages published morgan kaufmann san mateo ca 
appendix give alternate proofs theorems give upper bounds loss wmg wmr respectively 
secondly show algorithm introduced maass described section satisfies strong independence condition 
alternate proofs simple similar proof upper bound number mistakes wm theorem 
recall proof sum current weights th algorithm predicts defined similarly 
th algorithm contributes weight wmg wmr allow prediction continuous weight split leading general definition gamma note total weight trial coincides previous definition wm alternate proof theorem upper bound total loss wmg recall wmg may predict mistake occurs trial weight may decrease 
proof theorem show wmg mistake total weight update fi 
assume trial wmg prediction correct prediction ae 
total update gamma gamma fi gamma fix appendix case assume wmg prediction correct prediction ae 
gamma gamma fi gamma fi gamma case larger multiplied fi proof theorem easy show total update fi 
alternate proof theorem upper bound expected total loss wmr assume trials 
th trial sums weights described label ae complement label ae init fin th trial probability wmr mistake total weight decreases gamma fi observe gamma gamma fi dx dx gamma fi expected number mistakes trials sum probabilities making mistake trial equals gamma fi dx gamma fi dx ln gamma fi desired 
show appropriately constructed version algorithm introduced maass described section satisfies strong independence condition 
lemma regarding conditional expectations cf 
bil exercise lemma random variable jj 
valued random variable finite oe algebra oe algebra generated je jjg jjg jja proof atom jjg jja je ja unique value ja set gamma 
equals jjg abp unique value jjg set unique value jjg obtain cp abp 
gives desired result point gamma 
result trivial rest proving equality holds surely desired 
appendix theorem assume pool members binary predictions 
suppose algorithm wmr implemented follows weights updated trial described section fi 
pool members predictions binary amounts leaving weight unchanged setting zero depending corresponding pool member prediction matches label 
prediction making prediction randomly chosen pool member 
new pool member chosen trial 
initially trial immediately mistake new choice 
chosen pool member 
suppose random choice follows trial new pool member chosen algorithm independently chooses real number uniformly 
chooses pool member index satisfies gamma 
event probability choose arbitrarily event assume real numbers available discuss approximations necessary actual implementation algorithm 
algorithm run choices independent choices instances labels strong independence condition satisfied 
proof suppose trials 
denote index pool member chosen trial suffices show kj ae ae gamma prove induction 
clearly holds trial 
trials kj ae ae gamma gamma ae gamma ae ae gamma gamma ae gamma ae ae gamma look terms separately gamma ae gamma ae ae gamma gamma gamma ae gamma ae ae gamma gamma ae gamma equals term gamma ae gamma gamma ae gamma ae ae gamma gamma ae gamma ae ae gamma gamma ae gamma ae ae gamma lemma 
new random choice mistake equals gamma gamma ae gamma ae ae gamma gamma ae gamma ae ae gamma appendix induction hypothesis gamma gamma ae gamma ae ae gamma gamma gamma gamma ae gamma equals gamma gamma ae gamma ae ae gamma gamma ae gamma gamma gamma gamma get gamma ae gamma ae ae gamma equals gamma ae gamma equals gamma ae gamma putting terms started yields desired result 
