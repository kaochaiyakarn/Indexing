estimating support high dimensional distribution bernhard scholkopf john platt john shawe taylor alex smola robert williamson microsoft research street cambridge cb nh uk microsoft research microsoft way redmond wa usa royal holloway university london egham uk department engineering australian national university canberra australia bsc microsoft com microsoft com john dcs rhbnc ac uk alex smola anu edu au bob williamson anu edu au november technical report msr tr microsoft research microsoft microsoft way redmond wa suppose dataset drawn underlying probability distribution want estimate simple subset input space probability test point drawn lies outside bounded priori specified 
propose method approach problem trying estimate function positive negative complement 
functional form kernel expansion terms potentially small subset training data regularized controlling length weight vector associated feature space 
expansion coefficients solving quadratic programming problem carrying sequential optimization pairs input patterns 
provide preliminary theoretical analysis statistical performance algorithm 
algorithm natural extension support vector algorithm case unlabelled data 
keywords 
support vector machines kernel methods density estimation unsupervised learning novelty detection years new set kernel techniques supervised learning developed vapnik scholkopf 
specifically support vector sv algorithms pattern recognition regression estimation solution inverse problems received considerable attention 
attempts transfer idea kernels compute inner products feature spaces domain unsupervised learning 
problems domain precisely specified 
generally characterized estimating functions data tell interesting underlying distributions 
instance kernel pca characterized computing functions training data produce unit variance outputs having minimum norm feature space scholkopf 
kernel unsupervised learning technique regularized principal manifolds smola computes functions give mapping manifold minimizing regularized quantization error 
clustering algorithms examples unsupervised learning techniques kernelized scholkopf 
extreme point view unsupervised learning estimating densities 
clearly knowledge density allow solve problem solved basis data 
addresses easier problem proposes algorithm computes binary function supposed capture regions input space probability density lives support function data live region function nonzero scholkopf 
doing line vapnik principle solve problem general need solve 
applicable cases density data distribution defined singular components 
article organized follows 
review previous sec 
propose sv algorithms considered problem 
sec 
gives details implementation optimization procedure followed theoretical results characterizing approach 
sec 
apply algorithm artificial real world data 
conclude discussion 
previous part motivation ben david lindenbaum 
turns considerable amount prior statistical literature section briefly summarise 
attempt detailed comparison proof techniques specific results achieved confine placing previous context 
order summarize methods convenient introduce definition multi dimensional quantile function introduced mason 
random variables set distribution class measurable subsets real valued function defined quantile function respect ff inff ff cg ff empirical distribution empirical quantile function ff inff ff cg ff denote ff ff necessarily unique attains infimum achievable 
common choice lebesgue measure case ff minimum volume contains fraction ff probability mass assume lebesgue measure 
estimators form ff called minimum volume estimators 
estimating support density 
observe borel measurable sets support density corresponding assuming exists 
note defined exist 
smaller classes minimum volume containing support problem estimating appears studied considered piecewise constant estimators 
number works studying natural nonparametric estimator chevalier devroye wise see 
nonparametric estimator simply ffl ffl ball radius ffl centered ffl appropriately chosen decreasing sequence 
devroye wise showed asymptotic consistency respect symmetric difference hausdorff distance 
studied asymptotic consistency plug estimator plug fx kernel density estimator 
order avoid problems plug analyzed plug fi fx fi fi appropriately chosen sequence 
clearly distribution ff related fi connection readily exploited type estimator 
relating estimation asymptotic minimax study estimators functionals 
examples vol center 
see tsybakov chapter 
estimating high probability regions ff 
turning case ff reported sager hartigan considered class closed convex sets 
considered density contour clusters see definition 
nolan considered higher dimensions class ellipsoids 
tsybakov studied estimator piecewise polynomial approximation ff shown attains asymptotically minimax rate certain classes densities studied estimation ff ff 
derived asymptotic rates convergence terms various measures richness considers vc classes classes log ffl covering number bracket ing order ffl gammar 
summarizes number previous works minimum volume estimators mentioned 
studied excess mass approach muller construct estimator generalized ff clusters related ff 
define excess mass level ff ff ff cg ff delta gamma ff delta denotes lebesgue measure 
set gamma ff ff ff gamma ff called generalized ff cluster replace definitions obtain empirical counterparts ff gamma ff 
words estimator gamma ff arg max gamma ff cg max necessarily unique 
whilst gamma ff clearly different ff related attempts find small regions excess mass similar finding small regions amount probability mass 
gamma ff closely related determination density contour clusters level ff ff fx ffg simultaneously independently ben david lindenbaum studied problem estimating ff 
vc classes stated results stronger form meaningful finite sample sizes 
point curious connection minimum volume sets distribution differential entropy case dimensional 
suppose dimensional random variable density support define differential entropy gamma log dx ffl define typical set ffl respect ffl gamma log gamma fflg 
sequences notation means lim log cover thomas show ffl ffi vol ffl vol gamma ffi point result indicates volume smallest set contains probability approximately dimensional volume corresponding side length provides interpretation differential entropy 
applications 
number applications suggested techniques 
include problems medical diagnosis tarassenko marketing ben david lindenbaum condition monitoring machines devroye wise estimating manufacturing yields econometrics generalized nonlinear principal curves tsybakov tsybakov regression spectral analysis tests multimodality clustering muller 
shown estimators ff construct density estimators 
point doing allows encode range prior assumptions true density impossible traditional density estimation framework 
shown asymptotic consistency rates convergence densities belonging vc classes known rate growth metric entropy bracketing 
relationship 
describes algorithm finds regions close ff 
class defined implicitly kernel smoothness boundary controlled choice try find minimum volume region 
hand algorithm tractable computational complexity variables 
theory uses similar tools gives results expect finite sample size setting 
algorithms introduce terminology notation conventions 
consider training data number observations set 
simplicity think compact subset phi feature map map dot product space dot product image phi computed evaluating simple kernel boser vapnik scholkopf phi delta phi gaussian kernel indices understood range compact notation :10.1.1.103.1189
bold face greek letters denote dimensional vectors components labelled normal face typeset 
remainder section shall develop algorithm returns function takes value small region capturing data points gamma 
strategy map data feature space corresponding kernel separate origin maximum margin 
new point value determined evaluating side hyperplane falls feature space 
freedom utilize different types kernel functions simple geometric picture corresponds variety nonlinear estimators input space 
separate data set origin solve quadratic program min ae kwk gamma ae subject delta phi ae gamma parameter meaning clear 
nonzero slack variables penalized objective function expect ae solve problem decision function sgn delta phi gamma ae positive examples contained training set sv type regularization term kwk small 
actual trade goals controlled multipliers ff fi introduce lagrangian ae ff fi kwk gamma ae gamma ff delta phi gamma ae gamma fi set derivatives respect primal variables ae equal zero yielding ff phi ff gamma fi ff patterns fx ff called support vectors 
sv expansion transforms decision function kernel expansion sgn ff gamma ae substituting obtain dual problem min ff ij ff ff subject ff ff show optimum inequality constraints equalities ff fi nonzero ff 
recover ae exploiting ff corresponding pattern satisfies ae delta phi ff note approaches upper boundaries lagrange multipliers tend infinity second inequality constraint void 
problem resembles corresponding hard margin algorithm penalization errors infinite seen primal objective function 
feasible problem placed restriction ae ae large negative number order satisfy 
required ae start ended constraint ff corresponding equality constraint multipliers ff diverged 
conclude section note balls describe data feature space close spirit algorithms scholkopf 
hard boundaries tax duin soft margins 
try put data small ball solving min subject phi gamma ck leads dual min ff ij ff ff gamma ff subject ff ff solution ff phi corresponding decision function form sgn gamma ij ff ff ff gamma similar computed ff argument sgn zero 
kernels depend gamma constant 
case equality constraint implies linear term dual target function constant problem turns equivalent 
shown holds true decision function algorithms coincide case 
optimization section formulated quadratic programs computing regions capture certain fraction data 
constrained optimization problems solved shelf qp package compute solution 
possess features set apart generic notably simplicity constraints 
section describe algorithm takes advantage features empirically scales better large data set sizes standard qp solver time complexity order cf 
platt 
algorithm modified version smo sequential minimal optimization sv training algorithm originally proposed classification platt subsequently adapted regression estimation smola scholkopf :10.1.1.127.1519
strategy smo break constrained minimization smallest optimization steps possible 
due constraint sum dual variables impossible modify individual variables separately possibly violating constraint 
resort optimizing pairs variables 
elementary optimization step 
instance consider optimizing ff ff variables fixed 
shorthand ij reduces min ff ff ff ff ij ff ff ij ff ff ij subject ff ff ff delta delta gamma ff discard independent ff ff eliminate ff obtain min ff delta gamma ff delta gamma ff ff ff delta gamma ff ff derivative gamma delta gamma ff delta gamma ff ff gamma setting zero solving ff get ff delta gamma gamma gamma ff ff recovered ff delta gamma ff new point ff ff outside constrained optimum projecting ff region allowed constraints re computing ff offset ae recomputed step 
additional insight obtained rewriting equation terms outputs kernel expansion examples optimization step 
ff ff denote values lagrange parameter step 
corresponding outputs cf 
read ff ff eliminate update equation ff explicitly depend ff ff ff gamma gamma shows update essentially fraction second derivative objective function direction constraint satisfaction 
clearly elementary optimization step applied pair variables just ff ff briefly describe optimization 
initialization algorithm 
start setting random fraction ff 
integer examples set value ensure ff 
set initial ae ff 
optimization algorithm 
select variable elementary optimization step ways 
shorthand sv nb indices variables bound sv nb fi ff correspond points sit exactly hyperplane strong influence precise position 
scan entire data set find variable violating kkt condition bertsekas point gamma ae delta ff ae gamma delta gamma ff 
say ff pick ff arg max sv nb jo gamma ii scan performed sv nb practice scan type followed multiple scans type ii kkt sv nb optimization goes back single scan type 
type scan finds kkt optimization terminates 
unusual circumstances choice heuristic positive progress 
hierarchy choice heuristics applied ensure positive progress 
heuristics case pattern recognition cf 
platt experiments reported 
experiments smo applied distribution support estimation converge 
ensure convergence rare pathological conditions algorithm modified slightly cf 
keerthi 
session stating trick importance practical implementations 
practice nonzero accuracy tolerance quantities considered equal differ 
particular comparisons type determining point lies margin 
want final decision function evaluate points lie margin need subtract constant ae 
scan accelerated checking patterns correct side hyperplane large margin method joachims 
theory section analyse algorithm theoretically starting uniqueness hyperplane proposition 
describe connection binary classification proposition show parameter characterizes fractions svs outliers proposition 
give robustness result soft margin proposition briefly state error bounds theorem 
section italic letters denote feature space images corresponding patterns input space phi definition data set called separable exists delta 
proposition data set separable exists unique supporting hyperplane properties separates data origin distance origin maximal hyperplanes 
ae min kwk subject delta ae proof due separability convex hull data contain origin 
existence uniqueness hyperplane follows supporting hyperplane theorem bertsekas 
separability implies exists ae delta ae rescaling seen arbitrarily large ae 
cauchy schwartz inequality distance hyperplane fz delta aeg origin ae kwk 
optimal hyperplane obtained minimizing kwk subject constraints solution 
result elucidates relationship single class classification binary classification 
proposition suppose ae supporting hyperplane data 
optimal separating hyperplane passing origin vapnik labelled data set gammax gamma gammax gamma ii suppose optimal separating hyperplane passing origin labelled data set sigma suppose aligned delta positive ae kwk margin optimal hyperplane 
ae supporting hyperplane unlabelled data set fy proof ad 
observe gammaw ae supporting hyperplane data set reflected origin parallel ae 
provides optimal separation sets distance ae separating hyperplane 
ad ii 
assumption delta ae cf 
vapnik delta ae 
note relationship holds true consider nonseparable problems 
case margin errors binary classification points wrong side separating hyperplane fall inside margin translate outliers single class classification points fall wrong side hyperplane 
proposition holds cum training sets margin errors outliers respectively removed 
utility proposition lies fact allows recycle certain results proven binary classification scholkopf single class scenario 
explaining significance parameter case 
proposition assume solution satisfies ae 
statements hold upper bound fraction outliers 
ii lower bound fraction svs 
iii suppose data generated independently distribution contain discrete components 
suppose kernel analytic non constant 
probability asymptotically equals fraction svs fraction outliers 
parts ii follow directly proposition fact outliers dealt exactly way margin errors optimization problem binary classification case scholkopf 
basic idea imposes constraints fraction patterns ff upper bounding fraction outliers fraction patterns ff svs 
alternatively result proven directly primal objective function sketched presently note changing ae term change proportionally number points nonzero outliers plus changing ae positive direction number points just get nonzero ae sit hyperplane svs 
optimum ii 
part iii proven uniform convergence argument showing covering numbers kernel expansions regularized norm feature space behaved fraction points lie exactly hyperplane asymptotically negligible cf 
scholkopf 
proposition resistance local movements outliers parallel change hyperplane 
proof suppose outlier kkt conditions bertsekas ff 
transforming ffi delta kwk leads slack nonzero ff 
ff ff feasible primal solution ae 
ffi delta ff ffi delta ff ae computed 
kkt conditions satisfied ff 
bertsekas ff optimal solution 
note hyperplane change parametrization ae 
move subject generalization 
goal bound probability novel point drawn underlying distribution lies outside estimated region certain margin 
start introducing common tool measuring capacity class functions map definition pseudo metric space subset ffl 
set ffl cover exists ffl 
ffl covering number ffl minimal cardinality ffl cover finite cover defined 
idea finite approximate respect pseudometric distance finite sample pseudo metric space functions dx max jf gamma ffl sup ffl 
logarithms base 
theorem consider distribution suppose generated probability gamma ffi sample find fl pfx gamma flg log ffi dlog fl basis proof shawe taylor lemma 
consider possibility small number points fails exceed fl 
corresponds having non zero slack variable algorithm take fl ae kwk class linear functions feature space application theorem 
known bounds log covering numbers class 
introduce notation size shortfall 
definition real valued function space fix define fl maxf fl gamma similarly training sequence define fl fl theorem fix consider fixed unknown probability distribution input space class real valued functions range 
distance function differs metric semidefinite probability gamma ffi randomly drawn training sequences size fl fx gamma fl xg log ffi log fl gammaa fl fl log fl fl log gammaa fl jm proof similar proofs classification case shawe taylor cristianini theorem 
theorem bounds probability new point falling region value gamma fl complement estimate support distribution 
algorithm described hyperplane shifted fl kwk origin define region 
note restriction placed class functions functions probability density functions 
choice fl gives trade size region bound holds increasing fl increases size region size probability holds increasing fl decreases size log covering numbers 
result shows bound probability points falling outside region estimated support quantity involving ratio log covering numbers bounded fat shattering dimension scale proportional fl number training examples plus factor involving norm slack variables 
result stronger related results ben david lindenbaum bound involves square root ratio pollard dimension fat shattering dimension fl tends number training examples 
bounds entirely satisfactory inclusion sanity check closed case theory algorithm 
whilst apparent technical gaps readily filled example determining covering numbers class functions induced particular kernel methods williamson 
considerable gaps 
gaps invalidate algorithm simply indicate incomplete theory hope complete stage 
key difficulty relating margin achieved algorithm parameter fl 
support vector machine case natural linkage imposed problem 
furthermore whilst immediately apparent results stated give guidance chose kernel parameter connection fl margin achieved forced 
connection necessary motivated noting width frac 
svs ols margin ae kwk pictures single class svm applied toy problems domain gamma note cases fraction examples estimated region cf 
table 
large value causes additional data points upper left corner influence decision function 
smaller values third picture points ignored anymore 
alternatively force algorithm take outliers ols account changing kernel width fourth picture data effectively analyzed different length scale leads algorithm consider outliers meaningful points 
plausible obtain large margin separation origin accept large fl associated risk false positives unknown class 
measuring fl relative margin lead bounds depend margin justify algorithm tries maximize margin 
equivalently argue try maximize margin order freedom large fl leading smaller values error bounds including unknown class 
evidently argument implicitly prior assumptions unknown class particular sense centered origin try separate data 
algorithm modified accomodate case presently shall go detail matter 
experiments apply method artificial real world data 
displays toy examples shows parameter settings influence solution 
shows plot outputs delta phi training test sets postal service database handwritten digits 
database contains digit images size theta constitute test set 
fed test train threshold test train threshold experiments postal service ocr dataset 
recognizer digit output histogram exemplars training test set test exemplars digits 
axis gives output values argument sgn function 
top get svs outliers consistent proposition true positive test examples zero false positives class 
bottom get svs outliers respectively 
case true positive rate improved false positive rate increases 
threshold ae marked graphs 
note plots show parzen windows density estimate output histograms 
reality examples sit exactly threshold value svs 
peak smoothed estimator fractions outliers training set appear slightly larger 
algorithm gaussian kernel width delta common value svm classifiers data set cf 
scholkopf 
training instances digit 
testing done digit digits 
shown leads zero false positives learning machine seen non training correctly identifies non recognizing digits test set 
higher recognition rates achieved smaller values get correct recognition digits test set fairly moderate false positive rate 
whilst leading encouraging results experiment really address actual task algorithm designed 
focussed problem novelty detection 
utilized usps set time trained algorithm test set identify outliers folklore community usps test set fig 
contains number patterns hard impossible classify due segmentation errors vapnik 
experiment augmented input patterns extra dimensions corresponding class labels digits 
rationale disregarded labels hope identify patterns outliers 
vice versa labels algorithm chance identify unusual patterns usual patterns unusual labels 
fig 
shows worst outliers usps test set respectively 
note algorithm extracts patterns hard assign respective classes 
experiment kernel width value 
experiment tested scaling behaviour proposed smo solver training learning machine fig 

depend value utilized 
small values typically outlier detection tasks algorithm scales larger data sets dependency training times sample size quadratic 
subset examples randomly drawn usps test set class labels 
fraction ols fraction svs training time table experimental results various values outlier control constant note bounds fractions outliers support vectors respectively cf 
proposition 
asymptotic regime slack bounds control fractions 
note training times cpu time seconds pentium ii running mhz increase approaches 
related fact lagrange multipliers upper bound case cf 
sec 

system outlier detection experiments shown bold face 
outliers identified proposed algorithm ranked negative output svm argument 
outputs convenience units gamma written underneath image italics alleged class labels bold face 
note examples difficult atypical 
discussion view attempt provide new algorithm line vapnik principle solve problem general interested 
situations interested detecting novelty necessary estimate full density model data 
density estimation difficult doing respects 
mathematically speaking density exist underlying probability measure possesses absolutely continuous distribution function 
general problem estimating measure large class sets say sets borel sense solvable discussion see vapnik 
need restrict making statement measure sets 
small class sets simplest estimator accomplishes task empirical measure simply looks training points fall region interest 
algorithm opposite 
starts number training points supposed fall region estimates region desired property 
regions solution unique applying regularizer case enforces region small feature space associated kernel 
keep mind measure sense depends kernel way different method feature space 
similar problem appears density estimation done input space 
denote density training times vs data set sizes axes depict logs base cpu time seconds pentium ii running mhz 
table seen larger values generally lead longer training times note plots different axis ranges 
differ scaling sample size 
large values training times roughly proportional sample size raised power right plot 
values left plot typically outlier detection experiments fig 
scaling exponent exponents directly read slope graphs plotted log scale equal axis spacing 
note scalings better cubic expect solving optimization problem patterns cf 
sec 

experiments delta trained subsets usps test set 
perform nonlinear coordinate transformation input domain density values change loosely speaking remains constant delta dx dx transformed 
directly estimating probability measure regions faced problem regions automatically change accordingly 
attractive property measure chose placed context regularization theory leading interpretation solution maximally smooth sense depends specific kernel 
specifically assume green function operator mapping dot product space smola girosi take look dual objective function minimize ff ff ff ff delta ffi ff ff delta pk ff ff delta pk ff delta ff ff 
regularization operators common kernels shown correspond derivative operators poggio girosi minimizing dual objective function corresponds maximizing smoothness function thresholding operation function estimate 
turn related prior fk function space 
interestingly minimization dual objective function corresponds maximization margin feature space equivalent interpretation terms prior distribution unknown class novel class novelty detection problem trying separate data origin amounts assuming novel examples lie origin 
main inspiration approach stems earliest vapnik collaborators 
proposed algorithm characterizing set unlabelled data points separating origin hyperplane vapnik lerner vapnik chervonenkis 
quickly moved class classification problems terms algorithms terms theoretical development statistical learning theory originated days 
algorithmic point view identify shortcomings original approach may caused research direction decades 
firstly original algorithm vapnik chervonenkis limited linear decision rules input space secondly way dealing outliers 
conjunction restrictions severe generic dataset need separable origin hyperplane input space 
modifications incorporated dispose shortcomings 
kernel trick allows larger class functions nonlinearly mapping high dimensional feature space increases chances separation origin possible 
particular gaussian kernel separation exists data set see note dot products mapped patterns positive implying patterns lie inside orthant 
unit length 
separable origin 
second modification directly allows possibility outliers 
incorporated softness decision rule trick scholkopf obtained direct handle fraction outliers 
believe approach proposing concrete algorithm behaved computational complexity convex quadratic programming problem far mainly studied theoretical point view abundant practical applications 
turn algorithm easy black box method questions selection kernel parameters width gaussian kernel tackled 
expectation theoretical results briefly outlined provide solid foundation formidable task 

supported arc dfg ja 
parts done bs gmd berlin 
ben david bishop oliver platt schnorr tipping helpful discussions 
ben david lindenbaum 
learning distributions density levels paradigm learning teacher 
journal computer system sciences 
bertsekas 
nonlinear programming 
athena scientific belmont ma 
boser guyon vapnik 
training algorithm optimal margin classifiers 
haussler editor proceedings th annual acm workshop computational learning theory pages pittsburgh pa 
acm press 
chevalier 
estimation du support du contour du support une de probabilit annales de institut henri poincar section calcul des probabilit es statistique 
nouvelle erie 
cover thomas 
elements information theory 
wiley new york 

pattern analysis non convex case 


plug approach support estimation 
annals statistics 
devroye wise 
detection abnormal behaviour nonparametric estimation support 
siam journal applied mathematics 
mason 
generalized quantile processes 
annals statistics 

estimation functional density support 
mathematical methods statistics 

sur un probleme estimation 
publications de institut de statistique de universit de paris 
girosi 
equivalence sparse approximation support vector machines 
neural computation 
hartigan 
estimation convex density contour dimensions 
journal american statistical association 
joachims 
making large scale support vector machine learning practical 
scholkopf burges smola editors advances kernel methods support vector learning pages 
mit press cambridge ma 
keerthi bhattacharyya murthy 
improvements platt smo algorithm svm classifier design 
technical report cd dept mechanical production engineering natl 
univ singapore singapore 
tsybakov 
minimax theory image reconstruction 
springer new york 
muller 
excess mass approach statistics 
zur statistik universit heidelberg 
nolan 
excess mass ellipsoid 
journal multivariate analysis 
platt 
fast training svms sequential minimal optimization 
scholkopf burges smola editors advances kernel methods support vector learning pages 
mit press cambridge ma 
poggio girosi 
networks approximation learning 
proceedings ieee 

density estimation qualitative assumptions higher dimensions 
journal multivariate analysis 

measuring mass concentrations estimating density contour clusters excess mass approach 
annals statistics 

minimum volume sets generalized quantile processes 
stochastic processes applications 
sager 
iterative method estimating multivariate mode 
journal american statistical association 
scholkopf burges vapnik 
extracting support data task 
fayyad uthurusamy editors proceedings international conference knowledge discovery data mining 
aaai press menlo park ca 
scholkopf burges smola 
advances kernel methods support vector learning 
mit press cambridge ma 
scholkopf smola 
muller 
kernel principal component analysis 
scholkopf burges smola editors advances kernel methods support vector learning 
mit press cambridge ma 

scholkopf smola williamson bartlett 
new support vector algorithms 
appear neural computation 
neurocolt tr 
scholkopf williamson smola shawe taylor 
single class support vector machines 
buhmann maass ritter tishby editors unsupervised learning dagstuhl seminar report pages 
shawe taylor bartlett williamson anthony 
structural risk minimization data dependent hierarchies 
ieee trans 
inf 
theory 
shawe taylor cristianini 
margin distribution bounds generalization 
paul fischer hans ulrich simon editors computational learning theory th european conference eurocolt 
springer 
smola scholkopf 
tutorial support vector regression 
neurocolt tr svm gmd de 
smola scholkopf 
muller 
connection regularization operators support vector kernels 
neural networks 
smola williamson mika scholkopf 
regularized principal manifolds 
computational learning theory th european conference volume lecture notes artificial intelligence pages 
springer 

improving electronic designs 
ieee spectrum 
tarassenko hayton brady 
novelty detection identification masses mammograms 
proceedings fourth iee international conference artificial neural networks pages cambridge 
tax duin 
data domain description support vectors 
verleysen editor proceedings esann pages brussels 
facto 
tsybakov 
nonparametric estimation density level sets 
annals statistics 
vapnik 
nature statistical learning theory 
springer verlag new york 
vapnik 
statistical learning theory 
wiley new york 
vapnik chervonenkis 
theory pattern recognition russian 
nauka moscow 
german translation theorie der akademie verlag berlin 
vapnik lerner 
pattern recognition generalized portraits 

williamson smola scholkopf 
entropy numbers operators support vector kernels 
scholkopf burges smola editors advances kernel methods support vector learning pages 
mit press cambridge ma 

