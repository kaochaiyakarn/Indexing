deterministic stochastic wavelet decomposition recovery signal noisy data 
huang cressie institute statistical science department statistics academia sinica ohio state university taipei taiwan columbus oh series papers nonparametric regression donoho johnstone developed wavelet shrinkage methods recovering unknown piecewise smooth deterministic signals noisy data 
wavelet shrinkage bayesian approach involves specifying prior distribution wavelet coefficients usually assumed distribution zero mean 
priori reason prior means zero imagine certain types signals choice model 
take empirical bayes approach propose estimator prior mean plugged bayesian shrinkage formulas 
way general previous assume underlying signal composed piecewise smooth deterministic part plus zero mean stochastic part signal may contain reasonably large number nonzero wavelet coefficients 
goal predict signal noisy data 
develop new estimator noise variance method considers behavior near origin 
simulation studies show method decompshrink outperforms known visushrink sureshrink methods recovering wide variety signals 
insensitive choice lowest scale cut parameter typically case wavelet shrinkage methods 
key words em algorithm empirical bayes image denoising multiscale graphical model nonparametric regression normal probability plot 

wavelets mathematical constructs great potential statistical methodology 
applied extensively diverse applications including data compression signal processing image analysis object detection turbulence numerical analysis neural networks economics astronomy statistics 
informative overview wavelet multiresolution analyses see jawerth sweldens woodward 
mathematical properties wavelets daubechies chui meyer 
wavelets functions varying scales locations obtained translating single basic function known mother wavelet 
certain functions ir family gamma gamma delta zz constitutes orthonormal basis ir 
associated mother wavelet scaling function oe yield multiresolution analysis ir 
choosing initial scale ir expressed zz oe zz oe oe gamma 
spatial wavelets ir easy generalization onedimensional wavelets mallat 
purpose illustration consider case wavelet analysis dimensional images important application 
dimensional scaling function defined separable form phi oe oe ir basic wavelet functions psi oe psi oe psi zz write phi phi gamma gamma psi psi gamma gamma function gamma ir delta expanded phi psi direct connection onedimensional wavelets spatial wavelets shall fl american statistical association american society quality control technometrics vol 
huang cressie methodological development ir 
subsequent section give application wavelet methodology dimensional spatial prediction image 
series papers donoho johnstone donoho johnstone kerkyacharian picard developed wavelet shrinkage method reconstructing signals noisy data nonparametric regression 
suppose signal sampled equally spaced points gamma delta observed additive gaussian white noise yn discrete wavelet transform gamma gamma delta wn orthogonal matrix gamma gamma delta vector scaling function coefficients gamma gamma delta vector wavelet coefficients th scale gamma 
methods dealing problem sample size power cohen daubechies vial ogden section 
wavelet shrinkage method proceeds follows 
data transformed discrete wavelet transform wn resulting decomposition wavelet domain fi ffl fi ffl gau gamma oe delta wn orthogonal matrix 
suppress noise empirical wavelet coefficients shrunk zero shrinkage rule 
usually wavelet shrinkage carried thresholding wavelet coefficients wavelet coefficients absolute values prespecified threshold replaced zero 
processed empirical wavelet coefficients transformed back original domain inverse wavelet transform practice discrete wavelet transform inverse transform computed quickly operations pyramid algorithm mallat 
suitably chosen shrinkage method donoho johnstone donoho 
show resulting estimate delta nearly minimax large class function spaces wide range loss functions 
importantly delta automatically adaptive smoothness underlying function delta need adjust bandwidth kernel smoothing method 
crucial step procedure choice thresholding shrinkage method 
approaches proposed including minimax donoho johnstone cross validation nason hypothesis testing abramovich benjamini ogden parzen bayesian methods vidakovic muller clyde vidakovic chipman kolaczyk mcculloch lu huang tung abramovich silverman vidakovic crouse nowak baraniuk vidakovic :10.1.1.124.5288
discussion choices thresholding rules nason hall patil 
donoho johnstone proposed hard thresholding soft thresholding rules 
wavelet coefficient threshold hard thresholding value th ae jw jw soft thresholding value ts gamma jw gamma thresholding rules choice threshold parameter essential 
donoho johnstone proposed visushrink method applies soft thresholding rule universal threshold oe log donoho johnstone proposed sureshrink method selecting threshold parameter level level fashion soft thresholding rule 
resolution level threshold chosen minimize stein unbiased risk estimate sure provided wavelet representation level sparse sparsity condition gamma oe threshold oe log chosen 
practice suggested donoho johnstone scaling function coefficients shrunk 
usually noise parameter oe unknown case donoho 
proposed robust estimator median absolute deviations mad wavelet coefficients highest resolution oe gamma median phi gamma gamma gamma gj psi bayesian wavelet shrinkage rule obtained specifying prior fi oe defined 
vidakovic assumes ffi independent identically distributed degrees freedom oe independent ffi exponential distribution 
wavelet shrinkage rule posterior mean bayesian hypotheses testing procedure requires numerical integration 
chipman 
assume independent prior ffi signal sparse wavelet distribution heavy tail consider mixture zero mean normal components ffi small variance large variance 
treating oe hyperparameter shrinkage rule posterior mean closed form representation 
clyde technometrics vol 

deterministic stochastic wavelet decomposition 
abramovich 
consider mixture normal component point mass zero wavelet coefficients ffi clyde 
assume prior distribution oe inverse gamma ffi independent conditional oe stochastic search variable selection algorithm george mcculloch search nonzero wavelet coefficients signal markov chain monte carlo technique obtain posterior mean mean predictive distribution 
closed form approximations posterior mean posterior variance provided 
abramovich 
consider sum weighted absolute errors loss function resulting thresholding rule coefficients absolute values certain threshold level replaced zero bayes shrinkage rule bayes squared error loss 
thresholding rule posterior median closed form representation assumption oe known 
lu 
apply nonparametric mixed effects model scaling function coefficients assumed fixed wavelet coefficients assumed random zero mean 
empirical bayes estimator shown gauss markov type optimality 
discrete wavelet transform excellent wide variety stochastic processes yield completely uncorrelated wavelet coefficients 
practice wavelet coefficients observed process somewhat correlated especially coefficients closer scale nearby scales locations 
describe structure crouse 
consider state gaussian mixture models wavelet coefficients 
state variables linked tree structure markovian manner 
empirical bayes approach taken prior parameters estimated em algorithm 
vidakovic muller consider conjugate normal inverse gamma prior gamma fi oe delta assume holds fi oe gau gamma oe sigma delta oe inverse gamma distribution 
resulting shrinkage rule posterior mean written fi gamma sigma gamma delta gamma notice different choices sigma lead different shrinkage procedures 
bayesian approaches described assume prior fi zero mean 
article put strong assumption prior mean estimate plug bayesian shrinkage formulas 
way general previous assume underlying signal composed piecewise smooth deterministic part plus zero mean stochastic part 
model section develops empirical bayesian wavelet shrinkage method call decompshrink 
various prior covariance models considered section 
simulations onedimensional test signals dimensional images described section comparisons known wavelet shrinkage methods 
application environmental monitoring data taiwan air quality monitoring network 
discussion section 
empirical bayesian wavelet shrinkage bayesian hierarchical model assume bayesian model fi oe gau fi oe signal fi gamma fi fi fi gamma delta assumed prior distribution fi gau gamma sigma delta gamma gamma delta deterministic mean structure sigma describes variability correlations signal 
prior different priors literature assume necessarily gamma gamma delta 
regard prior parameter specified represents large scale variation fi 
may write fi gau sigma stochastic component representing small scale variation 
presence deterministic mean structure stochastic structure allows recover wider variety signals including piecewise smooth signals considered nonparametric regression model non smooth signals appearing time series spatial statistics 
time space nearby data tend alike far apart usually case stochastic description dependence parsimonious deterministic description box jenkins cressie 
stochastic description allows optimal temporal spatial prediction unknown values time series spatial process respectively 
circumstances noise process white signal fi colored 
identification deterministic stochastic ill posed problem 
possible separate asymptotically assumptions johnstone silverman finite data impossible distinguish 
signal sum extraction show achieved high precision 
optimal predictor fi written fi sigma gamma sigma oe delta gamma gamma estimates oe section section section empirical bayes shrinkage rule technometrics vol 

huang cressie call decompshrink fi sigma gamma sigma oe delta gamma gamma empirical bayes predictor fi obtained sigma gamma sigma oe delta gamma gamma estimation noise parameter oe estimation noise parameter oe crucial success wavelet shrinkage method 
oe overestimated noise removed important features signal 
underlying signal sparse wavelet representation usually case piecewise smooth deterministic signal mad estimator oe performs quite 
underlying signal contains stochastic components confound noise component wavelet coefficients 
mad estimator tends overestimate true value oe 
propose method original process estimating oe seen reliable signal fi deterministic sigma stochastic 
assume delta stationary covariance structure autocovariance lag ir 
cov ae oe follows fl var gamma ae gamma oe gamma delta notice fl oe 
estimate oe linear extrapolation zero ordinate small lags provided extrapolation nondegenerate 
consequently estimator oe oe oe fl gamma fl gamma fl fl fl fl fl fl fl fl gamma mad fy gamma delta robust estimator lag onedimensional signals may choose 

normal probability plot wavelet coefficients scale noisy blocks signal rsnr 
idea extended higher dimensions 
example may choose twodimensional image 
estimation mean parameter turn estimation deterministic mean structure wavelet domain 
scaling function coefficients correspond large scale features signal estimate authors writing bayesian wavelet shrinkage 
shall assume stochastic counterpart 
declare coefficients purely deterministic 
wavelet coefficients th level deterministic trend considered coming components potential outliers normal probability plot significant trend components usually stand zero mean stochastic components evenly distributed wavelet scale 
estimate slope fitted line normal probability plot jg gamma corresponding normal quantile gamma residual 
estimate gamma jw jw threshold parameter determined theta max jw jq jg shows normal probability plot wavelet coefficients scale noisy block signal dashed lines correspond threshold values 
explain choice note phi gamma psi normally distributed threshold value large residuals phi gamma psi small 
points estimated deterministic trend components importantly values small 
hand signal large typically large corresponding hardly shrunk 
write estimate gamma gamma delta gamma delta gamma gamma delta delta gamma gamma delta gamma 
note threshold chosen connection false discovery rate approach proposed abramovich benjamini 
approach different theirs technometrics vol 

deterministic stochastic wavelet decomposition control certain false discovery rate prespecified significance level followed classification wavelet coefficient pure signal pure noise 
conservative threshold wavelet coefficient passes threshold shrunk value residual normal probability plot 
optimal choice lowest scale cut parameter usually difficult problem 
abramovich benjamini demonstrate choice may strongly affect estimate signal number shrinkage procedures 
choose lowest possible scale allowed filter length discrete wavelet transform 
fact simulation results section show method insensitive choice deterministic part stochastic part tend compensate 
signal caught deterministic part caught stochastic part vice versa 

prior covariance sigma section consider modeling prior covariance sigma equivalently corresponding stochastic component gamma gamma delta gamma delta classes statistical models considered section zero mean scale independent models zero mean multiscale graphical models 
vector hyperparameters estimated maximum likelihood marginal distribution data plug values oe obtained respectively 
arg sup oe arg inf phi log fi fi sigma oe fi fi gamma gamma sigma oe delta gamma gamma psi pseudo maximum likelihood estimator gong 
scale independent models consider scale independent models correspond block diagonal matrices sigma 
specifically assume stochastic scaling function coefficients line earlier assumption scaling function coefficients attributed deterministic trend component section 
assume stochastic wavelet coefficients gamma statistically independent scale independence zero means allows model level separately 
temporal process natural assume gaussian autoregressive moving average arma model gamma 
higher dimensional process specify gaussian markov random field model gamma 
discrete wavelet transform excellent wide variety stochastic processes may assume wavelet coefficients independent scale var oe gamma 
pseudo maximum likelihood estimator gamma oe oe gamma delta oe max gamma gamma delta gamma gamma delta gamma oe gamma 
simple independence model decompshrink rule gamma gamma written fi oe oe oe gamma oe oe 
results new level dependent wavelet shrinkage rule call decompshrink method 
procedure requires log computation due ordering fw gamma wavelet scale gamma computing contrast decompshrink ii method develop scale dependent prior model 
scale dependent multiscale graphical models discrete wavelet transform wide variety stochastic processes yield completely uncorrelated wavelet coefficients 
practice wavelet coefficients observed process somewhat correlated scale nearby scales locations 
natural way describe structure multiscale models form flexible class models corresponding general class covariances sigma 
gamma gamma delta convolved noise optimal predictor gamma gamma delta computed efficiently change scale kalman filter algorithm huang cressie 
follows shall concentrate tree structure models important subclass multiscale graphical models computation relatively simple 
general multiscale graphical models reader referred huang cressie 
consider multiscale model gamma binary tree structure assume ff gamma gamma gamma denotes largest integer larger gau gamma oe delta gamma gau gamma oe delta gamma gamma fj mutually independent 
vector parameters estimated denoted gamma ff ff gamma oe oe oe gamma delta technometrics vol 

huang cressie notice ff ff delta delta delta ff gamma obtain scale independent models special case 
chou willsky developed fast optimal prediction kalman filter algorithm models 
algorithm consists steps filtering step followed smoothing step 
filtering step algorithm goes upward leaves tree successively computes optimal predictor state variables fj data observed node descendent nodes 
roots optimal predictor data obtained 
algorithm goes back downward leaves recursively computes optimal predictor state variables fj data 
steps complicated analogous filtering step smoothing step standard kalman filter time series see harvey 
algorithm extended general graphical markov models huang cressie 
introduce notation 
gamma gamma denote de data observed descendent nodes de node de theta gammaj var gamma computed recursively oe ff gamma oe gamma filtering step start leaves tree proceed upward paths roots tree 
node jw de var jw de obtained recursively 
conditional distribution results multivariate gaussian processes leaf gamma gamma gamma gamma jw gamma gamma oe delta gamma var gamma jw gamma gamma oe delta gamma oe gamma gamma recursions gamma fi fi de delta var gamma fi fi de delta theta oe gamma gamma fi fi de delta var gamma fi fi de delta var gamma fi fi de delta var gamma fi fi de delta gamma gamma var gamma fi fi de delta delta var gamma fi fi de delta oe gamma fi fi de delta var gamma fi fi de delta theta var gamma fi fi de delta gamma gamma fi fi de delta var gamma fi fi de delta gamma gamma fi fi de delta var gamma fi fi de delta var gamma fi fi de delta gamma var gamma fi fi de delta gamma gamma gamma gamma fi fi de delta ff gamma jw de gamma fi fi de delta ff gamma jw de var gamma fi fi de delta ff gamma jw de gamma oe var gamma fi fi de delta ff gamma jw de gamma oe roots nodes coarsest scale obtain gamma fi fi delta var gamma gamma delta gamma 
smoothing algorithm moves downward roots leaves tree allowing gamma fi fi delta gamma var gamma gamma delta computed recursively gamma gamma gamma fi fi de delta ff gamma var gamma fi fi de delta var gamma gamma fi fi de delta theta phi gamma gamma gamma gamma fi fi de delta psi gamma var gamma fi fi de delta ff gamma var gamma fi fi de delta var gamma gamma fi fi de delta theta phi gamma gamma gamma var gamma gamma fi fi de delta psi prediction covariance nodes gamma gamma gamma computed recursively gamma gamma cov gamma gamma gamma gamma gamma delta ff gamma var gamma fi fi de delta var gamma gamma fi fi de delta theta gamma gamma complete derivation algorithm chou 
bayesian approach huang cressie 
plugging pseudo maximum likelihood estimator section decompshrink rule call decompshrink ii method multiscale setting written fi gamma gamma fj obtained change scale kalman filter 
technometrics vol 

deterministic stochastic wavelet decomposition pseudo maximum likelihood estimation estimate pseudo maximum likelihood data plug values oe obtained respectively 
noted usual state space model time series direct evaluation likelihood function multiscale graphical models difficult 
follows derive pseudo maximum likelihood estimator em expectation maximization algorithm dempster laird rubin 
log likelihood written log gamma log oe jo gamma oe gamma gamma gamma log oe oe gamma gamma gamma ff gamma delta gamma oe gamma gamma gamma gamma constant 
incomplete likelihood data complicated mixture difficult maximize directly 
apply em algorithm treat missing data 
em algorithm iterative procedure starting initial estimator iteration consists steps expectation step step followed maximization step step 
th iteration consists evaluating gamma log gamma denotes conditional expectation parameter gamma obtained gamma th iteration 
find maximizes step 
procedure repeated convergence 
shown likelihood increases iteration algorithm guaranteed converge exponential family see dempster wu 
case take conditional expectations obtain step log jw gamma log oe gamma oe gamma gamma gamma delta gamma gamma ae log oe oe gamma gamma ff gamma ff gamma gamma gamma ff gamma gamma ff gamma gamma oe 
test signals blocks bumps heavisine doppler 
gamma oe gamma gamma gamma gamma gamma delta gamma fj 
follows oe gamma gamma gamma ff gamma gamma gamma gamma gamma gamma gamma gamma oe gamma ff gamma gamma ff gamma gamma ff gamma gamma gamma ff gamma gamma 
simulation application section apply decompshrink method dimensional signal processing dimensional image denoising problems 
simulation comparison dimension test functions blocks bumps heavisine doppler created donoho johnstone sample standard deviation sd 
stochastic signal generated random walk sd stochastic signal generated ar stationary process autoregressive parameter equal sd signals deterministic stochastic components 
signals obtained adding test functions referred ar stationary process autoregressive parameter equal rescaled sd sd 
consider different sample sizes different root signal noise ratios rsnr 
goal predict original signal noisy data gamma yn delta apply decompshrink decompshrink ii methods shrinkage functions respectively 
compare methods commonly wavelet shrinkage methods visushrink computation sureshrink log computation defined section 
remarked mad estimator oe tends overestimate noise parameter signal contains stochastic components 
consider adjusted sureshrink method uses estimator oe 
cases choose nearly symmetric wavelet smooth finite support vanishing moments family technometrics vol 

huang cressie 
noisy blocks signal rsnr visushrink reconstruction sureshrink reconstruction adjusted sureshrink reconstruction decompshrink reconstruction decompshrink ii reconstruction 

noisy random walk signal rsnr visushrink reconstruction sureshrink reconstruction adjusted sureshrink reconstruction decompshrink reconstruction decompshrink ii reconstruction 

boxplots mse performance blocks signal various wavelet shrinkage techniques replications rsnr 

boxplots mse performance ar signal various wavelet shrinkage techniques replications rsnr 
wavelets called daubechies 
visushrink sureshrink adjusted sureshrink choose gamma match donoho johnstone donoho 

programs implemented wavelets bruce gao 
illustration reconstructions methods blocks random walk signals shown respectively 
performance various wavelet methods estimating signal compared mean squared error mse criterion mse gamma delta gamma gamma delta random variation stochastic signal noise replications obtained 
replicate gives mse gamma delta display boxplots mse values blocks ar signals rsnr 
easy see distributions mse values methods smaller means smaller variances 
summarize simulation results table table 
table 
average mse performance simulated examples various wavelet shrinkage techniques replications rsnr 
values parentheses estimated standard deviations 
rsnr visu sure adjusted sure decomp decomp ii blocks bumps heavisine doppler random walk ar blocks ar bumps ar heavisine ar doppler ar table shows average mse values replications obtained wavelet shrinkage methods rsnr values parentheses estimated standard deviations 
seen decompshrink decompshrink ii methods smaller average mse values methods signals considered 
comparing sureshrink adjusted sureshrink method see perform similarly deterministic test signals random walk adjusted sureshrink method performs better sureshrink method signal contains ar component 
mse value adjusted sureshrink method ar signal larger oe 
visushrink method largest average mse values methods 
notice signal known generated ar process advance parameters estimated maximum likelihood construct parametric bayes predictor delta 
case average mse value rsnr reduces decompshrink 
practice form true signal rarely known realistic situations approach wavelets powerful 
table shows proportion mse values smaller sureshrink method various shrinkage techniques replications technometrics vol 

deterministic stochastic wavelet decomposition rsnr 
decompshrink decompshrink ii methods beat sureshrink method time heavisine signal beat sureshrink method time signals 
table 
proportion mse values smaller sureshrink method various shrinkage techniques replications rsnr 
rsnr visu sure decomp decomp ii blocks bumps heavisine doppler random walk ar blocks ar bumps ar heavisine ar doppler ar table shows estimation noise parameter oe rsnr mad method method 
expect mad estimator tends overestimate noise parameter oe signal contains stochastic components 
estimate oe method unbiased deterministic stochastic signals 
surprising adjusted sureshrink method outperforms sureshrink method signals contain stochastic components 
table 
estimation noise parameter oe mad methods replications rsnr 
rsnr mad mean bias mse mean bias mse blocks gamma bumps heavisine gamma gamma doppler gamma random walk gamma ar blocks ar gamma bumps ar gamma heavisine ar doppler ar gamma table shows signals sample sizes root signal noise ratios consider decompshrink decompshrink ii methods perform uniformly better methods terms average mse values advantage significant signals contain stochastic components 
comparing decompshrink decompshrink ii interesting see methods perform 
improvement decompshrink ii method decompshrink method small 
decompshrink method preferable terms performance ease computation 
table shows effect choice wavelet shrinkage methods rsnr 
seen decompshrink decompshrink ii methods insensitive choice particular smaller case visushrink sureshrink gives justification preferring decompshrink 
application environmental monitoring data section apply decompshrink method environmental monitoring data pm refers matter aerodynamic diameter equal nominal 
major sources pm include dust road traffic motor vehicles production facilities power plants fires construction activities emitted gases transformed air 
due small size pm penetrate lungs environmental laws regulations set effect pm human health 
technometrics vol 

huang cressie table 
average mse performance simulated examples various wavelet shrinkage techniques replications rsnr 
rsnr rsnr rsnr signals methods blocks visu sure sure decomp decomp ii bumps visu sure sure decomp decomp ii heavisine visu sure sure decomp decomp ii doppler visu sure sure decomp decomp ii random walk visu sure sure decomp decomp ii ar visu sure sure decomp decomp ii blocks ar visu sure sure decomp decomp ii bumps ar visu sure sure decomp decomp ii heavisine ar visu sure sure decomp decomp ii doppler ar visu sure sure decomp decomp ii technometrics vol 

deterministic stochastic wavelet decomposition table 
average mse performance simulated examples various wavelet shrinkage techniques various lowest scale cut parameters replications rsnr 
rsnr blocks visu sure decomp bumps visu sure decomp heavisine visu sure decomp doppler visu sure decomp 
daily pm observations dashed line decompshrink reconstruction solid line august december difference daily pm observations decompshrink reconstruction august december 
pm data observed hourly monitoring station taiwan air quality monitoring network 
chose period observations august may 
average pm values taken hours resulting daily observations observations analyzed approach 
pm measurements known contain measurement error noise 
goal produce precise pm values 
decompshrink method filter noise 
displays portion observations dashed line analyzed decompshrink reconstruction solid line 
seen decompshrink method peaks fills valleys differentially adapting largescale small scale noise components pm variable 
difference observed fitted curves shown 
surprising resulting pm estimate smooth true daily pm values expected high daily variations 
image denoising section consider image denoising problem dimension 
peppers image shown test image 
consists theta picture elements pixels 
choose dimensional wavelet constructed dimensional wavelet vanishing moments tensor product method described section 
empirical wavelet coefficients th scale setting written gamma gamma delta gamma delta gamma delta delta component corresponds horizontal vertical diagonal spatial orientations respectively mallat 
apply decompshrink sureshrink methods components scale 
decompshrink method choose smallest scale possible allowed discrete wavelet transform 
sureshrink method try different values find works best 
log signal noise ratio quantitative measure image quality expressed log variance signal variance noise db unit 
show peppers images degraded adding gaussian white noise db db respectively 
images reconstructed sureshrink method shown respectively reconstructions decompshrink method shown respectively 
results show image reconstructed original image noise added decompshrink method exactly original image hope expect 
reconstructed images increases substantially noisy images 
example peppers images shown see increase db db see increase db db 
notice decompshrink method performs better sureshrink method recovering noisy images terms 

discussion 
original peppers images theta pixels db db db respectively 
sureshrink reconstructions db db db respectively 
decompshrink reconstructions db db db respectively 
technometrics vol 

huang cressie developed empirical bayesian wavelet shrinkage method assuming observed process decomposed large scale deterministic trend plus zero mean gaussian stochastic process plus noise 
proposed shrinkage rule advantages current wavelet shrinkage methods 
adaptive smoothness signal regardless signal sparse wavelet representation decompshrink method catches signal components larger wavelet coefficients normal probability plots catches signal components smaller wavelet coefficients prior covariance 
second estimate oe method turns robust mad estimate especially stochastic signals 
third decompshrink estimate computationally efficient requiring log operations insensitive choice lowest scale cut parameter simulation studies show method simple independence covariance multiscale tree structure covariance superior mse performance visushrink sureshrink methods deterministic stochastic signals 
surprising visushrink sureshrink methods perform recovering stochastic signals shrinkage rules simple hard thresholding functions rely heavily sparse wavelet representation case typically stochastic signals 
acknowledgments research supported office naval research numbers 
referees editors helpful comments jing hwang help obtaining environmental monitoring data 
abramovich benjamini 
thresholding wavelet coefficients multiple hypotheses testing procedure wavelets statistics eds 
antoniadis oppenheim lecture notes statistics new york springer verlag pp 

abramovich benjamini 
adaptive thresholding wavelet coefficients computational statistics data analysis 
abramovich silverman 
wavelet thresholding bayesian approach journal royal statistical society ser 

woodward 
wavelet transforms time frequency approach chemometrics intelligent laboratory systems 
box jenkins 
time series analysis forecasting control san francisco holden day 

convergence em algorithm journal royal statistical society ser 

bruce gao 
wavelets user manual seattle 
chipman kolaczyk mcculloch 
adaptive bayesian wavelet shrinkage journal american statistical association 
chou willsky 
multiscale systems kalman filters riccati equations ieee transactions automatic control 
chui 
wavelets new york academic press 
clyde vidakovic 
multiple shrinkage subset selection wavelets biometrika 
clyde vidakovic 
bayesian strategies wavelet analysis statistical computing statistical graphics newsletter 
cohen daubechies vial 
wavelets interval fast wavelet transforms applied computational harmonic analysis 
cressie 
statistics spatial data rev ed new york wiley 
crouse nowak baraniuk 
wavelet statistical signal processing hidden markov models ieee transactions signal processing 
daubechies 
lectures wavelets philadelphia siam 
dempster laird rubin 
maximum likelihood incomplete data em algorithm journal royal statistical society ser 

donoho johnstone 
ideal spatial adaptation wavelet shrinkage biometrika 
donoho johnstone 
adapting unknown smoothness wavelet shrinkage journal american statistical association 
donoho johnstone 
minimax estimation wavelet shrinkage annals statistics 
donoho johnstone kerkyacharian picard 
wavelet shrinkage 
discussion journal royal statistical society ser 

george mcculloch 
variable selection gibbs sampling journal american statistical association 
technometrics vol 

deterministic stochastic wavelet decomposition george mcculloch 
approaches bayesian variable selection statistica sinica 
gong 
pseudo maximum likelihood estimation theory applications annals statistics 
hall patil 
effect threshold rules performance wavelet curve estimators statistica sinica 
harvey 
forecasting structural time series models kalman filter cambridge cambridge university press 
huang 
cressie 
multiscale spatial modeling american statistical association proceedings section statistics environment alexandria va american statistical association pp 

jawerth sweldens 
overview wavelet multiresolution analysis siam review 
johnstone silverman 
wavelet threshold estimators data correlated noise journal royal statistical society ser 

lu huang tung 
wavelet shrinkage nonparametric mixed effects models technical report institute statistics national tung university 
mallat 
theory multiresolution signal decomposition wavelet representation ieee transactions pattern analysis machine intelligence 
meyer 
wavelets operators cambridge cambridge university press 
meyer 
wavelets algorithms applications philadelphia siam 
nason 
choice threshold parameter wavelet function estimation wavelets statistics eds 
antoniadis oppenheim lecture notes statistics new york springer verlag pp 

nason 
wavelet shrinkage crossvalidation journal royal statistical society ser 

ogden 
essential wavelets statistical applications data analysis boston birkhauser 
ogden parzen 
change point approach data analytic wavelet thresholding statistics computing 
ogden parzen 
data dependent wavelet thresholding nonparametric regression change point applications computational statistics data analysis 
vidakovic 
bayesian decision theoretic approach wavelet thresholding statistica sinica 
vidakovic 
nonlinear wavelet shrinkage bayes rules bayes factors journal american statistical association 
vidakovic muller 
wavelet shrinkage affine bayes rules applications discussion isds duke university dept statistics 
wu 
convergence em algorithm annals statistics 
technometrics vol 

