panda portable platform support parallel programming languages tim ruhl koen langendoen henri bal vrije universiteit amsterdam department mathematics computer science frans kaashoek mit laboratory computer science cambridge ma june current parallel programming languages require advanced run time support implement communication data consistency 
runtime systems usually layered top specific operating system 
reports early experiences panda portable virtual machine provides general flexible support implementing run time systems parallel programming languages 
panda interfaces panda interface providing threads rpc totally ordered group communication system interface encapsulates machine dependencies providing machine independent thread communication abstractions 
describe interfaces experience initial unix implementation development new portable scalable run time system orca parallel programming language top panda 
modern parallel programming languages require advanced run time support communication data consistency 
order fully exploit machine particular features run time systems parallel languages tend built directly top host operating system 
experience parallel research supported part netherlands organisation scientific research 
unix trademark unix systems laboratories programming language orca amoeba distributed operating system strategy results language implementation difficult port operating systems 
orca shared objects 
shared objects may replicated speed read accesses implementation distributed memory machines requires advanced run time support 
investigate scalability portability shared data objects aim port orca variety parallel architectures 
operating systems amoeba offer virtual machine abstraction shared objects implemented generally tied particular machine architecture 
current operating systems generally provide functionality needed wanted parallel processing virtual memory management 
modern operating systems clouds support object model 
model simple flexible lightweight layering object model top troublesome inefficient 
higher level approaches supporting parallel programming include distributed shared memory dsm systems munin 
dsm systems rely manipulation virtual memory management unit suffer portability problems 
highly portable message passing systems exist provide limited functionality 
pvm instance provide low level message passing support threads totally ordered group communication 
relying page dsm operating systems low level message passing developed portable virtual machine called panda 
panda designed portability requirements parallel languages mind currently implement new orca run time system 
panda restrict users orca object model 
provides general abstractions ffl threads ffl remote procedure call rpc ffl totally ordered group communication 
experience similar amoeba abstractions shown efficient implementations shared objects built top 
threads provide simple lightweight unit activity 
rpc general mechanism high level point point communication nodes implementation remote object invocation 
totally ordered group communication successfully employed previous orca run time systems keeping replicated objects consistent implementation distributed checkpointing algorithm 
assures members group receive group messages order parallel applications easier implement 
hardware broadcast mechanisms usually guarantee rpc group comm 
communication sockets signals messages threads messages system interface system layer host interface unix fragmentation threads pthreads panda interface panda layer orca run time system application panda architecture unix 
strong semantics 
panda abstractions language independent believe panda implement run time systems languages orca 
panda architecture illustrated consists layers reflect design goals portability support parallel programming languages 
support parallel programming languages achieved providing high level abstractions panda interface 
software implements panda interface called panda layer 
portability achieved implementing panda layer top system interface encapsulates machine dependencies 
panda layer fully machine independent 
implementation system interface system layer constructed basic operating system support exploit features underlying operating system kernel threads scatter gather interfaces hardware broadcasting multicasting 
panda takes layering approach portability 
layering effective way machine dependencies bears danger poor performance 
layering may result loss information essential achieving performance 
identified panda performance critical parts threads message manipulation nature underlying network 
performance critical parts implemented system layer access low level operating specific features 
main elements system layer threads message manipulation primitives communication primitives 
implementing threads messages system layer benefit operating system specific features achieve better performance 
communication takes place virtual processors called platforms identified platform identifiers 
communication primitives provide unreliable point point communication multicasting platforms 
porting panda new architecture requires porting system layer 
minimal support required operating system implementing system layer consists facility unreliable message passing facility handling signals generated incoming messages expired timers 
host operating system offers system interface implemented scratch top operating system 
current operating systems offer threads usable communication facilities 
implementing system layer systems easier 
constructed initial implementation panda collection sparc workstations running unix connected mbit ethernet 
intend port panda near parallel machine alewife cm 
section describes panda system interface detail 
machine independent implementation panda interface outlined section 
section describe experience initial implementation system interface unix 
section discusses implementation new portable orca run time system top panda 
section panda compared related systems 
section 
panda system interface section describe relevant parts current panda system interfaces 
reasons efficiency threads messages implemented system layer part system interface primitives associated visible panda interface 
distinguish panda layer system layer functions panda layer function name prefixed pan system layer function sys 
functions part interfaces prefixed 
panda interface panda interface provides rpc totally ordered group communication thread abstractions panda applications built 
threads thread interface see table pthreads threads interfaces 
threads implemented system layer see thread primitives pan prefix 
experience amoeba threads learned thread package parallel programming languages support priority scheduling handle incoming messages immediately arrive 
automatically experience panda orca interfaces may evolve 
void thread create thread thread void func void arg void arg long int priority void thread exit void result void thread yield void int thread thread thread void thread thread thread int priority table thread interface implies preemption running threads new message arrives 
priorities supported operations thread thread return set priorities 
specify scheduling policy threads priority provide function thread yield tries run runnable thread priority 
synchronization threads mutexes condition variables 
provide strong semantics construct monitors 
rpc rpc interface see table notion service provides number operations 
service implemented servers 
server register services panda name server pan export service giving arguments number operations supports array pointers operations 
operation called client get handle server pan import service 
handle identify server handle rpc request pan rpc 
request message comes thread started 
thread calls registered function specified service operation 
function parameters operation index number input message reply message 
totally ordered group communication group abstraction panda see table supports totally ordered closed groups 
total ordering assures group member receives group messages order 
group closed members send messages group 
efficient implementation possible 
group identified character string registered panda name server 
platform wants join group calls pan group join initializes group structure 
group exist created 
int pan export service char name int nr operations void func int operation message request message reply int pan import service char name int pan rpc int handle int operation message request message reply table rpc interface int pan group join group group char name message mesg join void receive message mesg void pan group leave group group message message void pan group send group group message message table totally ordered group communication interface group messages handled asynchronously upcall specified receive routine handles incoming messages ensure total ordering 
system interface system interface hides machine dependencies providing abstractions threads messages communication primitives 
explained threads implemented system layer system panda interface identical respect threads 
communication communication facilities divided parts send primitives see table addressing 
startup time platform gets unique platform identifier pid integer number ranging number platforms 
pid point point address 
pids grouped platform set pset serves logical multicast address 
send primitives provided system layer sys unicast unreliable point point communication sys multicast unreliable oneto communication 
panda layer initializes registers message receive handler system layer 
platforms run system layer receive daemon 
time unicast multicast message arrives daemon upcall message receive handler panda layer 
void sys unicast int pid message message void sys multicast sys pset pset message message table system communication primitives void message push message message int length int align void message pop message message int length int align void message look message message int length int align void message copy message message message copy int sys message data len message message void sys message mark message message int sys message fragment message message message fragment int offset void sys message assemble message message message fragment table message interface messages interface level messages look stacks 
construct message senders push data fields specified size alignment message stack fields popped reverse order receivers see table 
message look similar message pop pop data field message 
communication primitives hide machine dependencies handle messages length larger underlying system supports 
system interface provides primitives fragment messages handled communication primitives 
fragmentation common header header placed front fragment 
sys message mark panda layer specify data part start common header 
data field pushed mark belongs common header 
sys message fragment initializes new fragment message containing common header part data original message 
function takes parameter offset indicating start fragment data original message returns offset fragment data 
getting fragment message fields common header part filled information identifies fragment 
receiving side sys message assemble reassemble original message 
primitives resemble kernel primitives fragmenting messages 
common header part message fragment fragment fragment part part common header common header common header part part part 
fragmentation common header fragment message need contain copies common header data fields original message pointers may 
support sharing common header fragments fragment may exist time creating fragment predecessor fragment released 
clear fragment identification information common header refers 
pointers original message fragment message avoids unnecessary copying 
portability efficiency panda system interface designed allow efficient implementations 
abstractions system layer may high level providing abstractions low level primitives gives opportunity exploit advanced features offered modern operating systems 
features efficient user level thread packages kernel threads scatter gather message transmission access hardware broadcasting multicasting decided message passing system interface unreliable 
reliable message passing prohibited efficient rpc implementation architectures provide unreliable message passing sending message reliably requires network packets 
implementation panda interface panda interface implemented primitives provided system interface 
code entirely machine independent 
group communication structure protocol group communication implementation describes efficient totally ordered atomic group communication protocol 
concerned fault tolerance implemented protocol non resilient way loosing atomicity delivery presence processor crashes 
possible synchronous checkpointing top totally ordered group communication atomicity 
totally ordered group communication achieved having special member group sequencer assigns sequence number group message 
gives possibilities group send 
method send point point message sequencer sequencer broadcasts message filling sequence number called pb method 
second method sender broadcast 
sequencer receives broadcast message assigns sequence number broadcasts short message containing sequence number bb method 
method saves network bandwidth data transmitted generates interrupts 
choice methods dynamically message size information system layer 
way message arrives sequence number checked sequence number received 
sequence number indicates message message delivered application level receiver asks sequencer missing messages 
incoming group messages handled single daemon thread upcalls receive handler specified pan group join 
prevent loosing ordering group messages unpredictable thread scheduling daemon thread group 
underlying architecture may stronger semantics need system layer define compilation flags reliable multicast specify multicast messages lost interconnect ordered multicast specifies multicast messages arrive total order 
code group implementation adapted parameters 
rpc structure protocol rpc protocol birrell nelson 
rpc requires messages normal execution request reply 
architectures network transputers reliable message passing provided necessary 
system layer define compilation flag reliable unicast implies messages reliable 
compiled flag set sent 
experience panda unix implemented panda unix sunos 
subsections describe implementation system layer performance panda 
parts implementation tuned 
implementation system interface contrast modern operating systems parallel computers unix provides threads multicasting 
selected unix initial target operating system provides complete programming environment widely available 
avoid writing large amount software expect provided target platforms public domain software threads unreliable multicast implementation 
implemented threads interface pthreads posix conformant user space threads implementation 
extended kernels sparc workstations ip multicast kernel extension multicasting 
point point message passing implemented top udp 
pthreads provides functionality need including priority scheduling runs entirely user space 
user space threads efficient pure kernel implementations thread context switches involve trapping kernel 
suffer poor integration virtual memory management blocking :10.1.1.13.9310
virtual memory performance predictable page fault block threads missing page brought disk 
blocking network serious problem 
thread waits incoming messages block threads contained process 
platform receive daemon thread uses unix asynchronous nonblocking options prevent blocking entire process reading network 
finds pending messages waits signal 
pthreads supports signals thread basis receive daemon blocked entire process 
incoming messages generate signals cause receive daemon rescheduled immediately highest priority 
udp support panda stack message manipulation fragmentation routines system layer code devoted implementation routines 
code machine independent need changed panda ported 
may beneficial adapt code platform specific features scatter gather facilities 
system interface designed allow modifications implementation 
changes need interface 
test case performance thread switching unicast message passing latency ms multicast message passing latency ms null rpc latency ms rpc throughput kbyte group communication latency ms table performance figures performance compare overhead protocols table gives overview performance communication primitives system panda layer 
performance figures obtained collection diskless slc running mhz connected mbit ethernet 
included overhead thread context switching 
message passing latencies measured platforms running different machines sending messages sending 
null rpc latency measured empty request reply messages empty server routine throughput rpc messages request message size bytes empty reply message 
latency empty group message ms protocol uses negative latency independent number platforms 
rpc group communication performance initial panda implementation factor amoeba rpc group communication built microkernel 
comparable hardware amoeba null rpc ms null group message collection mhz mc takes ms implementing orca rts panda orca type secure parallel distributed programming language 
orca programs consist processes communicate solely shared objects instances data types 
speed read accesses shared objects objects may replicated 
replication strategy combination compile time run time techniques 
orca run time system rts responsible keeping replicas consistent state 
orca re implemented obtain programming system portable efficient scalable 
new orca compiler generating fast portable ansi code implemented reimplementing runtime system top panda 
new rts heavy panda facilities threads orca rts uses panda threads implementation orca processes 
threads created implicitly result incoming rpc requests group messages 
rpc rpc rts performing operations remote objects transmitting objects migrated replicated 
group communication shared replicated object simultaneously updated orca processes replica holders object apply updates order 
achieve rts uses group communication 
rtss belong single group simply send update messages group 
communication group totally ordered rtss receive process update messages order keeping replicas consistent 
contrast previous orca implementations amoeba machine dependencies hidden rts panda making rts portable 
seen interface descriptions primitives panda interface language independent implementation language run time systems 
related panda differs existing parallel programming platforms designed requirements run time systems parallel programming languages mind 
previous orca implementations demonstrated run time systems benefit high level support form rpc totally ordered group communication 
section compare panda systems implementing parallel programming languages systems language come library form 
consider portable message passing systems pvm distributed shared memory systems munin midway arcade isis horus 
panda pvm provide portable communication primitives 
pvm provide message passing primitives provides high level communication form rpc totally ordered group communication 
experience rpc group communication simplify implementation complex run time systems 
pvm supports lightweight threads 
high context switching overhead processes suitable hiding communication latencies 
pvm provide visualization tools support heterogeneity 
extending orca panda performance debugging tools progress 
pvm panda support heterogeneity 
dsm systems munin midway support parallel programming providing shared memory abstraction hides message passing programmer 
panda provide abstraction gives sufficient support layer shared memory model top 
munin programmers annotate shared variables expected access pattern 
shared variables kept consistent release consistency protocol 
munin implementation protocol relies memory management unit mmu detect writes pages containing shared data rendering implementation machine dependent 
midway system shared variables associated synchronization objects kept consistent memory consistency protocol called entry consistency 
midway rely mmu manipulation enforce entry consistency need mmu implement stronger memory consistency models release consistency processor consistency 
munin midway support parallel programming providing shared memory abstraction weak consistency models 
consider support low level application programming programmers annotate variables low level locking 
munin midway needs manipulate mmu orca implementations guarantee sequential consistency stronger previously mentioned forms consistency mmu manipulation 
layering orca run time system top panda results portable implementation sequential consistency 
panda arcade supports implementation parallel programming languages high level abstractions 
arcade abstractions language independent data units communication mechanisms 
data unit abstraction typed region memory named moved shared multiple nodes distributed environment 
language specific objects mapped arcade data units 
orca isis currently reimplemented reasons portability scalability 
new isis system horus core interface provides reliable causal multicasting 
services implemented top interface 
interface somewhat panda interface ordering semantics weaker panda group communication totally ordered group communication rpc implemented top 
layer implemented top portable operating system abstraction multicast transport service similar panda system layer 
described motivation design implementation panda implementation platform parallel programming languages combines portability flexibility efficiency 
panda provides users flexible abstractions effectively employed implementation orca run time systems threads rpc totally ordered group communication 
abstractions implement orca object model 
panda achieves portability defining machine independent system interface addition panda interface 
panda interface implemented top system interface machine independent 
implementation system interface requires basic operating system support context switching threads unreliable message passing 
current system interface implementation machine independent easily reused 
careful interface design modular implementation allow incorporation efficient native thread packages communication facilities scatter gather message passing hardware broadcasting multicasting 
porting system layer parallel architectures straightforward 
early experience sparc unix implementation panda shown feasibility layering approach support parallel programming languages 
gerard kok anil testing panda rpc implementation 
greatly appreciate helpful comments jacobs van doorn earlier drafts 
agarwal souza johnson kranz 
lim maa yeung 
mit alewife machine large scale distributed memory multiprocessor 
technical report mit lcs tm mit 
anderson bershad lazowska levy :10.1.1.13.9310
scheduler activations effective kernel support user level management parallelism 
proc 
th symposium operating systems principles pages 
acm 
bal kaashoek 
object distribution orca compiletime run time techniques 
conference object oriented programming systems languages applications washington september october 
published 
bal kaashoek tanenbaum 
orca language parallel programming distributed systems 
ieee transactions software engineering march 
bershad 
midway distributed shared memory system 
computer conference 
birman joseph 
exploiting virtual synchrony distributed systems 
proc 
th acm symposium operating systems principles pages 
birrell nelson 
implementing remote procedure calls 
acm transactions computer systems february 
butler lusk 
monitors messages clusters parallel programming system 
journal parallel computing 
submitted 
carter bennett zwaenepoel 
implementation performance munin 
proc 
th symposium operating systems principles 
acm 
cohn banerji casey kulkarni 
basing micro kernel abstractions high level language models 
proc 
autumn technical conference pages utrecht holland november 
cooper draves 
threads 
technical report cmu cs department computer science carnegie mellon university pittsburgh 
dasgupta chen menon pearson ramachandran ahamad leblanc jr khalidi 
design implementation clouds distributed operating system 
computing systems journal 
deering cheriton 
multicast routing datagram internetworks extended lans 
acm transactions computer systems january 
hoare 
monitors operating system structuring concept 
communications acm october 
ieee 
threads extensions portable operating systems draft edition february 
kaashoek 
group communication distributed computer systems 
phd thesis vrije universiteit amsterdam 
kaashoek bal tanenbaum 
transparent fault tolerance parallel orca programs 
symposium experiences distributed multiprocessor systems iii pages march 
mueller 
implementing posix threads unix description progress 
proc 
nd software engineering research forum pages november 
mullender van rossum tanenbaum van renesse van staveren 
amoeba distributed operating system 
ieee computer 
peterson hutchinson malley rao 
kernel platform accessing internet resources 
ieee computer pages may 
postel 
user datagram protocol 
internet request comments rfc september 
sunderam 
pvm framework parallel distributed computing 
concurrency practice experience december 
tanenbaum kaashoek bal 
parallel programming shared objects broadcasting 
ieee computer august 
tanenbaum van renesse van staveren sharp mullender jansen van rossum 
experiences amoeba distributed operating system 
communications acm december 
thinking machines 
connection machine cm technical summary 
van renesse birman cooper stephenson 
reliable multicast microkernels 
proccedings usenix workshop micro kernels kernel architectures pages april 

