proceedings fifteenth conference uncertainty arti cial intelligence inferring parameters structure latent variable models variational bayes attias gatsby ucl ac uk gatsby unit university college london queen square london wc ar current methods learning graphical models latent variables xed structure estimate optimal values model parameters 
approach usually produces tting suboptimal generalization performance carrying bayesian program computing full posterior distributions parameters remains dif cult problem 
learning structure models latent variables bayesian approach crucial harder problem 
variational bayes framework provides solution problems 
approach approximates full posterior distributions model parameters structures latent variables analytical manner resorting sampling methods 
laplace approximation posteriors generally non gaussian hessian needs computed 
resulting algorithm generalizes standard expectation maximization algorithm convergence guaranteed 
demonstrate algorithm applied large class models domains including unsupervised clustering blind source separation 
focuses learning graphical models data 
standard method learn model maximum likelihood ml 
method estimates optimal values model parameters xed graph structure dataset 
main problems ml learning 
produces model ts data subsequently suboptimal generalization performance 
second learn structure graph complicated graphs assign higher likelihood data 
third computationally tractable small class models 
bayesian framework mackay cooper herskovits heckerman provides principle solution rst problems 
framework considers ensemble models characterized probability distribution possible parameter values structures 
learning single model dataset computes distribution ensemble models data 
model uncertainty taken account leading enhanced generalization performance 
addition complex models ectively penalized assigned lower posterior probability optimal structures identi ed 
models contain hidden variables posterior computes joint distribution models hidden variables data 
unfortunately computations bayesian framework seldom performed exactly due need integrate models 
approximations see cheeseman stutz chickering heckerman friedman major schemes markov chain monte carlo methods laplace approximation 
attempts achieve exact results typically requires vast computational resources 
lower complexity number parameters dataset sample size approximation limit particular assumes posterior distributions normal see discussion mackay 
naturally situation worse hidden variables exist 
variational bayes framework computations graphical models 
framework facilitates analytical calculations distributions hidden variables parameters structures 
draws variational ideas intractable hidden variables models saul jaakkola jordan ghahramani jordan bayesian inference waterhouse mackay robinson jaakkola jordan mackay turn draw neal hinton 
posteriors obtained iterative em algorithm convergence guaranteed 
focusing parameter posterior resulting approximation ecient laplace hessian needs computed produces non trivial posteriors sample size 
addition bic mdl model selection criteria obtained vb large sample limit 
vb framework developed section applied mixture models section blind source separation problem section 
learning structure complex models discussed section 
notation 
shall dirichlet normal wishart distributions parametrization note inverse covariance precision matrix normal wishart distribution nw variational bayes framework graphical models de nitions restrict attention directed acyclic graphs bayesian networks 
denote set model structures 
variables structure divided sets visible data variables hidden latent variables variable vector dimension coordinates may assume discrete continuous values 
structure speci es visible set models hidden set dependencies directed edges variables parametrized form probabilistic dependencies pa pa set parents denotes parameter set conditional distribution 
di erent structures may di erent numbers hidden variables hidden variable may di erent dimensionality assume di erent set values di erent structures 
reason consider vector variables customary scalars shall occasionally real valued distributions pa allow correlations coordinates conditionally independent course achieved slightly complicated graph 
denoting complete set parameters relevant joint distribution pa prior distribution parameters structure prior set structures 
nal note terminology term model refer pair speci structure speci parameter values 
interested ensemble likelihood 
likelihood yn assigned dataset ng model averaged ensemble models described 
quantity known marginal likelihood evidence 
note calculation requires averaging con gurations hidden units model 
ensemble likelihood generally computationally intractable requires integrating joint parameters typically performed analytically summation possible values hidden variables 
discrete variables number terms sums exponential number nodes real valued variables sums may turn analytically intractable integrals summation possible structures number grows exponentially maximum numbers allowed nodes edges 
address issues variational framework 
ensemble likelihood occam factor variational bayes framework formulated follows 
starting ensemble neal hinton representation neal hinton place lower bound log log sum ranges possible values hidden variables implies integration continuous variables 
inequality holds arbitrary conditional distribution optimal obtained setting functional derivative right hand side respect zero resulting equation solved true posterior obtained bayes rule 
easy show case inequality equality 
computation true posterior intractable approximations 
approach restricts space allowed distributions parameters conditionally independent hidden variables structure form posterior generally di er true termed variational posterior 
optimized produce best approximation true posterior 
get lower bound ensemble likelihood splits terms log kl jj average 
rst term computed respect model posterior second term kullback leibler kl distance hlog dependence variational posteriors data henceforth omitted 
shall see rst term corresponds likelihood term second term occam factor penalizes complex models 
score function may interpreted penalized likelihood penalty kl distance posterior prior distributions ensemble models 
large sample limit gain insight consider large sample limits 
case model posterior strongly peaked mean maximum likelihood ml model covariance typically decreases compute rst term limit ml model needs included relative correction 
compute second term approximate multivariable gaussian distribution mean covariance 
obtain log log log number parameters ml model dependence omitted 
rst focus maximizing 
shown neal hinton generalized representation ml problem seek single parameter value ordinary em algorithm obtained maximizing alternately step rth iteration set gives step solve obtain variational em algorithm saul introduced cases computation exact posterior intractable 
form allows performing calculation set parameters step parameters optimized minimize kl distance true posterior 
second penalty reduces limit term linear number ml model parameters plus simple regularizer log 
point bayesian information criterion bic schwartz minimum description length criterion mdl rissanen emerge special case large sample expression corresponding prior exact variational posterior 
optimal posteriors relation em nd optimal variational posterior parameters structure set obtain log hlog log log average 
rst term computed hidden variable posterior normalization constant 
spite apparent complexity resulting posterior typically quite simple 
averaging hidden variables variational posterior obtained closed form key property variational approach see 
second directed graphs node parameters parameter prior factorizes nodes parameter posterior factorizes 
see recall joint distribution nodes assume 
log hlog pa log log log proving particular graph structure posteriors parameters di erent nodes mutually independent 
third functional form parameter posterior determined distributions de ne model priors 
general standard forms distributions leads standard form posterior 
demonstrate cases interest 
discrete discrete assume node parents pa discrete connection described probability table pa parameters satisfy normalization condition case appropriate prior parameters dirichlet distribution hyperparameters 
perform average need variational posterior hidden variables factorizes pa 
straightforward show averaging gives posterior product dirichlet distributions modi ed hyperparameters pa 
ii discrete normal assume node continuous normally distributed conditioned parents pa discrete pa parameters having normal wishart prior nw independently shown posterior modi ed hyperparameters nw determined rst moments hidden variable posterior pa pa cases posterior form prior 
notice covariance 
fact show sample size increases uence prior form posterior diminishes 
results revisited speci models considered 
nd variational posterior hidden variables structure may try similarly set arriving log hlog jm log average 
jm computed parameter posterior di erent normalization constant 
procedure successful models illustrated 
interesting models resulting posterior quite dicult computing normalization constant intractable performing average 
cases choose parametric form separate set parameters termed variational parameters optimized maximize crucial consideration choice posterior allows performing required calculations analytically providing approximation relevant sense 
shall demonstrate done 
posterior structures similarly shown log hlog log jm log illustrated parameter posteriors emerge vb turn parametric functional form parameters confused model parameters suf cient statistics ss computed data iterative step em algorithm 
step hidden variable posterior computed old ss step new ss computed updating parameter posterior 
large sample limit algorithm reduces ordinary em dempster 
predictive quantities labeling probability hypothesis true data determined averaging models posteriors 
example density estimation applications predictive density new data vector approach directly replace true posterior variational 
unfortunately variational posterior designed compute analytically averages logarithm actual distribution additional approximations may necessary 
variational approach allows attractive alternative route 
considering predictive density consider logarithm log log log fyg augmented dataset 
repeat exact steps compute lower bound log compute log 
calculation requires little additional ort required posterior close old initialization 
predictive distribution fact large sample limit obtain hlog jm additional computation 
applications unsupervised classi cation blind source separation value hidden variable new data vector required 
value map estimate arg max 
approach directly replace true variational posterior giving arg max hp jm jm average 
jm performed hidden variables marginalizing alternatively may compute posterior augmented dataset focus factor marginalize hidden variables obtain arg max 
labeling problem may arise computing map estimate hidden variables new data 
consider graph structures contain hidden variables assume invariant permutation node labeled may labeled producing incorrect estimate summing structures 
problem may arise permutation discrete values hidden variable component label mixture models 
honest way avoid labeling problems incorporating appropriate prior information relevant hidden variables model break permutation invariance 
practical solution approximate sum structures small number probable ones post processing correct label switches 
course problem completely avoided single probable structure place sum 
variational bayes mixture models de nitions mixture models constitute useful tool exible density estimation 
models investigated analyzed extensively see titterington ecient methods exist tting model data 
issue determining required number mixture components open problem 
viewed framework unsupervised classi cation issue determining number unobserved classes 
bayesian approach provides solution principle satisfactory practical algorithm emerged application involved sampling techniques richardson green rasmussen approximation methods cheeseman stutz problem 
show elegant solution provided vb approach 
consider models form compare number component determines structure model 
denotes observed data vector hidden component label component probabilities sum component distributions 
approach applied arbitrary models simplicity shall rst consider classical mixture model data real valued component distributions normal mean inverse covariance matrix 
non informative priors point revisited parameters dim hypercube edge length structure prior maximal number components 
learning algorithm follow steps outlined dataset derive variational posterior distributions parameters component label structure 
doing nd parameter posterior factorizes predicted 
mean inverse covariance jointly normal wishart note factorize 
component probabilities jointly dirichlet 
results consistent general properties 
results transparent restricted parameter posterior factorize mean covariance calculations fully carried joint distribution arriving means normal inverse covariances wishart component probabilities remain dirichlet 
parameters appearing de ned shortly 
label posterior factorizes data instances yn obvious reasons instance yn yn data dimensionality normalization constant exp exp psi digamma function 
vb approach led em algorithm structure step learn label posterior step learn parameter posteriors 
fact obtain learning algorithm parameters determine sucient statistics ss distributions 
initialize appropriate values 
compute 
compute new ss sjm hyi sjm sjm 
sjm implies averaging label posterior hf sjm yn steps repeated convergence case commented 
remarks deserve point 
prior assumptions parameters posteriors emerge applying vb mixture model uninformative priors non trivial functional forms intuitively appealing 
importantly general normal resulting laplace approximation 
fact complications appear generalize priors functional form corresponding posteriors appearance appropriate hyperparameters 
quantities learned rules means posterior distributions parameters precisely mean component probabilities large sample limit 
covariances posteriors 
large sample limit posteriors collapse means limit recover ordinary em algorithm 
strikingly number data vectors assigned component shown rules replaced ectively declaring component nonexistent 
property important protects algorithm wellknown problem ordinary em algorithm mixture models component may centered single data vector sending covariance zero model likelihood nity resulting wrong model assigned higher likelihood correct 
vb algorithm automatically eliminates component 
posterior distributions parameters label conditioned structure obtained posterior distribution model structure exp hlog jm 
jm refers averaging 
resulting closed form expression omitted 
omitted expression obtained predictive density point mixture normal distributions 
results applied vb mixture model algorithm toy problems fig 
presents results 
rst top data points generated dim mixture model normal components covariances represented ellipses top left panel 
algorithm applied maximum number components resulting log posterior number components shown top right panel indicating posterior sharply peaked correct value 
second problem bottom vb algorithm applied data points generated dim noisy spiral 
correct number resulting posterior logarithm plotted bottom right panel peaked 
means resulting posterior covariances cluster data log component number posterior spiral data log component number posterior application vb mixture model density estimation 
top data generated component model left resulting log posterior number components 
bottom data generated noisy spiral means covariance posterior corresponding component model left log posterior number components 
case represented axes bottom left 
larger numbers components observed produce overlaps 
vb mixture currently applied task handwritten character recognition 
blind source separation de nitions blind source separation bss problem independent component analysis ica jutten herault bell sejnowski cardoso attias schreiner multivariable time series data 
assumed data generally correlated arise individual source signals mutually statistically independent 
sources unobservable mixed unknown linear transformation corrupted unobservable noise 
task recover sources data 
successful solution problem applications areas involving processing multisensor signals speech recognition enhancement analysis classi cation biomedical data target localization tracking radar sonar devices wireless communication 
denote signal emitted source time denote signal received sensor time 
instantaneous mixing version problem assume linearly related yn un matrix termed mixing matrix gaussian noise signals inverse variance assume approximations independent source densities 
shall model cosh shown accurate purpose separating speech sources bell sejnowski attias schreiner 
graph consider cosh ij terms hidden variables fx parameters fa ij structure determined number sources 
existing ica algorithms address simpli ed case noise vanishes mixing matrix square invertible number sensors equals number sources 
furthermore assumed known advance 
maximum likelihood computes distribution assigned data model chooses maximize 
sources recovered general case non square mixing nonzero noise harder compute dx dim integration non trivial due non gaussian nature sources 
lewicki sejnowski integrated sources laplace approximation 
attias solved problem modeling source density dim mixture gaussians allows integral calculated analytically 
sources reconstructed map estimate arg maxx 
approach results em algorithm learns mixing noise covariance matrices source distributions noisy data 
computational complexity algorithm increases exponentially number sources large case treated attias structured variational approximation ghahramani jordan 
realistic cases observed data generated unknown number sources exploit vb approach compute posterior distribution dataset sensor signals 
point realistic situations include additional complications multipath propagation conditions see attias schreiner treatment zero noise convolutive blind separation problem non stationarity issues scope 
learning algorithm prior distribution mixing matrix choose elements ij independent zero mean normal variables precision single hyperparameter dm exp ij ij prior uninformative limit 
simplicity keep noise precisions hyperparameters vb treat full probabilistic manner little added ort 
structure prior employed maximal number sources 
discussion section nd mixing matrix posterior normal mean covariance ij yx xx ij ij kl xx 
jl ik xx xx ij kl expectation ij ij kl kl 
viewing ij dm vector formed concatenating columns large column note block diagonal form consisting blocks dimension correlation matrices yx xjm yn xx xjm averages computed source posterior 
point large sample limit covariance vanishes mean yx xx form appearing ordinary em algorithms factor analysis rubin thayer independent factor analysis attias 
source posterior obtained directly optimizing see due nongaussian nature sources 
variational tricks 
noting source posterior factorizes instances xn yn choose normal distribution instance xn xn mean general inverse covariance termed variational parameters may depend data yn adapted help posterior best approximate optimal 
second order adapt compute expected value log posterior poses diculty due form 
overcome exploit jensen inequality compute bound quantity hlog xjm hlog cosh xjm tr log cosh general accuracy lower bound depends variational parameters especially note zero noise case vanishes bound exact 
experimentally distributions arising cases treated mean error bound smaller 
posterior set derive xed point equation yn tanh xx diag 
equation solved iteratively initial value exact limit low noise large sample size 
variational precision matrix turns independent xx optimizing hyperparameter gives dm tr xx optimization rule hyperparameters similarly derived omitted 
omit structure posterior 
mixture model case vb led em algorithm step learns source posterior step learns parameter posterior 
algorithm learns source number posterior snr db source reconstruction error left application vb source separation algorithm dim data generated linear mixing speech music signals 
left resulting posterior number sources 
right log error reconstructed sources original ones di erent noise levels 
ss distributions follows 
initialize appropriate values 
compute ss 
compute new ss 
steps repeated convergence 
variational bayes algorithm conventional method factor analysis straightforwardly derived 
case sources normal source posterior optimal vb framework 
resulting equation solved single iteration 
known gaussian nature sources prevents factor analysis performing source separation 
results applied vb source separation algorithm dim data generated mixing speech music signals obtained commercial cds 
signal sec long sampling frequency khz 
signals mixed random mixing matrix di erent levels gaussian noise added 
posterior number sources algorithm plotted fig 
left peaked correct value 
sources reconstructed data map estimate 
log error reconstructed original sources plotted di erent signal noise snr levels fig 
right seen decrease increasing snr expected 
additional experiments di erent numbers sources sensors gave similar results 
hierarchical mixtures probable structures integration model parameters structures tractable models discussed complicated models full bayesian treatment practically impossible 
consider hierarchical mixture model constructed follows 
mixture component probability distribution blind separation model sources mixing matrix graphical model described joint distribution model potentially useful pattern recognition speech image data 
reason data typically long tailed distributions modeled eciently exponential normal component distributions 
denoting maximal number sources component maximal number components possible structures model 
simple way obtain polynomial time algorithm include probable structure possibly neighboring structures 
formally procedure amounts making factorized variational approximation ks covering small range numbers including zero satisfying 
form allows single number components restricts component range possible source numbers probabilities quantities variational parameters depend dataset optimization amounts performing local search structure space probable structures may xed 
course alternative variational structure posteriors possible 
developed approximation scheme bayesian inference graphical models hidden variables demonstrated density estimation blind source separation tasks 
comparison accuracy vb laplace approximation monte carlo standard important undertaking 
exciting apply vb framework complex bayesian networks attias including dynamic models demonstrate performance real world tasks speech recognition scene analysis 
attias schreiner 

blind source separation deconvolution dynamic component analysis algorithm 
neural computation 
attias 

independent factor analysis 
neural computation 
attias 

hierarchical ifa belief networks 
statistics arti cial intelligence heckerman whittaker eds 
morgan kaufmann 
bell sejnowski 

approach blind separation blind deconvolution 
neural computation 
cardoso 
laheld 

equivariant adaptive source separation 
ieee transactions signal processing 
cheeseman stutz 

bayesian classi cation autoclass theory results 
advances knowledge discovery data mining fayyad eds 
aaai press menlo park 
chickering heckerman 

ecient approximations marginal likelihood bayesian networks hidden variables 
machine learning 
cooper 

bayesian method induction probabilistic networks data 
machine learning 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society 
friedman 

bayesian structural em algorithm 
proceedings fourteenth conference uncertainty arti cial intelligence 
ghahramani jordan 

factorial hidden markov models 
machine learning 
heckerman geiger chickering 

learning bayesian networks combination knowledge statistical data 
machine learning 
jaakkola jordan 

bayesian logistic regression variational approach 
statistics arti cial intelligence smyth madigan eds 
jutten herault 

blind separation sources part adaptive algorithm neuromimetic architecture 
signal processing 
lewicki sejnowski 

learning nonlinear overcomplete representations ecient coding 
advances neural information processing systems jordan eds 
mit press cambridge ma 
mackay 

bayesian interpolation 
neural computation 
mackay 

practical bayesian framework backpropagation networks 
neural computation 
mackay 

choice basis laplace approximation 
machine learning 


ensemble learning hidden markov models 
technical report cavendish laboratory cambridge university 
neal hinton 

view em algorithm justi es incremental sparse variants 
learning graphical models jordan eds 
kluwer academic press norwell ma 
rasmussen 

countably nite bayesian gaussian mixture density model 
technical report tech 
denmark 
richardson green 

bayesian analysis mixtures unknown number components 
journal royal statistical society discussion 
rissanen 

stochastic complexity discussion journal royal statistical society 
rubin thayer 

em algorithms ml factor analysis 
psychometrika 
saul jaakkola jordan 

mean eld theory sigmoid belief networks 
journal arti cial intelligence research 
schwartz 

estimating dimension model 
annals statistics 
titterington smith 
makov 

statistical analysis finite mixture distributions 
wiley chichester 
waterhouse mackay robinson 

bayesian methods mixture experts 
advances neural information processing systems touretzky eds 
mit press 
