trec question answering track report ellen voorhees national institute standards technology gaithersburg md ellen voorhees nist gov trec question answering track large scale evaluation domain independent question answering systems 
summarizes results track giving brief overview different approaches taken solve problem 
accurate systems correct response questions 
relatively simple bag words approaches adequate finding answers responses long paragraph bytes sophisticated processing necessary direct responses bytes 
trec question answering track initial effort bring benefits large scale evaluation bear question answering qa task 
goal qa task retrieve small snippets text contain actual answer question document lists traditionally returned text retrieval systems 
assumption users usually prefer answer find answer document 
summarizes retrieval results track companion trec question answering track evaluation gives details evaluation implemented 
necessity track report give overview different approaches track 
readers urged consult participants papers proceedings details regarding particular approach 
task successful evaluation requires task easy difficult current technology 
task simple systems learned 
similarly task difficult systems poorly learned 
accordingly chose constrained version general question answering problem focus track 
document collection task trec ad hoc collection set documents trec disks minus congressional record documents 
documents consist newspaper articles contain information wide variety subjects 
participants fact short answer questions 
question guaranteed document collection explicitly answered question 
participants returned ranked list document id answer string pairs question answer string believed contain answer question 
answer strings limited bytes extracted corresponding document automatically generated information contained document 
human assessors read string binary decision string contain answer question context provided document 
document context account allowed system correctly derived response document incorrect full credit response 
set judgments strings score computed submission mean reciprocal rank defined follows 
individual question received score equal reciprocal rank correct response returned responses contained correct answer 
score submission mean individual questions reciprocal ranks 
reciprocal rank advantages scoring metric 
closely related average precision measure extensively document retrieval 
bounded inclusive averages 
run penalized ffl big mac 
ffl won nobel prize medicine 
ffl american space 
ffl voice piggy 
ffl 
ffl costume designer decided michael jackson wear glove 
ffl year joe compile game hitting 
ffl language commonly bombay 
ffl grand slam titles bjorn borg win 
ffl th president united states 
example questions question answering track 
labs research project iowa cl research new mexico state maryland college park cymfony ntt data massachusetts ge pennsylvania national taiwan ottawa ibm research royal melbourne inst 
technology sheffield limsi cnrs seoul national xerox research centre europe mitre southern methodist participants question answering track 
retrieving correct answer question unduly 
measure drawbacks 
score individual question take values 
question answering systems credit retrieving multiple different correct answers 
track required response question system receive credit realizing know answer 
retrieval results different organizations participated question answering track 
participants listed 
total runs submitted runs byte limit runs byte limit 
table gives mean reciprocal rank number questions answer run 
submissions contained errors omitted table 
scores computed questions comprised official test set 
table split byte byte runs sorted decreasing mean reciprocal rank run type 
number questions answer shows accurate systems able find answer questions 
furthermore answer usually ranked shown fact mean reciprocal rank close systems 
run highest mean reciprocal rank score byte run direct comparison byte submissions participant shows byte task difficult 
organization submitted runs lengths byte limit run higher mean reciprocal rank 
surprising result system greater chance including correct response table mean reciprocal rank mrr number questions correct response question answering track submissions 
run name participant mrr textract cymfony southern methodist research ibm sc xerox research centre europe maryland mtr mitre ibm qs ntt data research qs ntt data crl new mexico state inq massachusetts ge pennsylvania inq massachusetts sheffield sheffield iowa iowa runs byte limit length response 
southern methodist research ge pennsylvania research qa multitext project mds royal melbourne inst 
tech lc xerox research centre europe ql ntt data mtr mitre ibm ibm inq massachusetts ql ntt data limsi cnrs inq massachusetts ge pennsylvania clr cl research crl new mexico state university iowa seoul national sheffield sheffield ntu national taiwan iowa runs byte limit length response 
longer string guaranteed result 
longer strings include correct response correct response 
response strings contained multiple entities semantic type answer specifically indicate entities answer marked incorrect 
example question capital 
byte response miles northwest demonstrators judged correct byte response called military intervention 
miles northwest demonstrators reported injured apparently clashes police 
violent clashes judged incorrect unclear response capital 
submissions research labs demonstrate existing passage retrieval techniques successful byte runs suitable byte runs 
question answering system traditional vector retrieval system select documents scored sentence documents number question words surrounding context 
passage runs highest scoring sentences returned response 
runs high scoring sentences processed linguistic module 
passage method competitive byte limit nearly successful restricted just bytes 
ntt data note similar effects runs 
results suggest relatively simple bag words approaches successfully text retrieval sufficient extracting specific fact answers 
retrieval strategies participants variant general strategy question answering problem 
system attempted classify question type answer suggested question word 
example question begins prime minister japan 
implies person organization sought question period 
implies time designation needed 
system retrieved small portion document collection standard text retrieval technology question query 
system performed shallow parse returned documents detect entities type answer 
entity required type sufficiently close question words system returned entity response 
appropriate answer type system fell back best matching passage techniques 
approach works provided query types recognized system broad coverage system classify questions sufficiently accurately 
systems answer questions began accurately 
questions sought person name private citizen fly space 
nobel expelled conference east 
difficult 
difficult questions answers entity specific type head start 
david ask fbi word processor 
course pattern matching expected answer types fool proof matches 
response question american space 
jerry brown taken document says wilson senator defeating jerry brown called american space 
broadly speaking organizations variant general strategy labs research cl research cymfony ge university pennsylvania limsi cnrs mitre new mexico state university ntt data southern methodist university university maryland university ottawa ncr university sheffield xerox research centre europe 
approach ibm research similar spirit approach located entities indexing time bag words scoring metric incorporated entities providing efficient retrieval question answering time 
university iowa classified questions type filtering system learn features answers 
seoul national university performed initial document retrieval run selected phrases top ranking documents extracting immediate neighborhood highest weighted question word 
multitext project national taiwan university royal melbourne institute technology csiro university massachusetts traditional passage retrieval techniques 
question answering track large scale evaluation domain independent question answering systems 
questions track deliberately constrained fact short answer questions task amenable evaluation 
systems generally classified question type answer performed shallow parse documents find objects entailed type 
accurate systems able answer questions correctly 
existing techniques adequate finding answers relatively long responses permissible sophisticated processing need focus answer 
question answering track trec trec track 
change track test set questions questions fewer questions constructed target document 
james allan jamie callan fang fang feng malin 
inquery trec 
voorhees harman 
eric breck john burger lisa ferro david house marc light inderjeet mani 
sys called 
voorhees harman 
cormack clarke palmer 
fast automatic passage ranking multitext experiments trec 
voorhees harman 
david eichmann srinivasan 
filters webs answers university iowa trec results 
voorhees harman 
olivier ferret brigitte grau gabriel christian jacquemin nicolas masson 
question answering program language cognition group limsi cnrs 
voorhees harman 
michael fuller marcin kaszkiel sam kimberly corinna ng ross wilkinson wu justin zobel 
rmit csiro ad hoc web interactive speech experiments trec 
voorhees harman 
david hull 
xerox trec question answering track report 
voorhees harman 
humphreys robert gaizauskas mark hepple mark sanderson 
university sheffield trec system 
voorhees harman 
chuan jie lin hsin hsi chen 
description preliminary results trec qa task 
voorhees harman 
kenneth 
question answering semantic relation triples 
voorhees harman 
joel martin chris 
ask tomorrow nrc university ottawa question answering system 
voorhees harman 
dan moldovan sanda harabagiu marius pasca rada mihalcea richard rus 
lasso tool surfing answer net 
voorhees harman 
thomas morton 
coreference question answering 
voorhees harman 
douglas oard wang dekang lin ian 
trec experiments maryland clir qa routing 
voorhees harman 
bill ogden jim cowie eugene hugo molina sergei nirenburg nigel 
crl trec systems cross lingual ir voorhees harman 
john prager dragomir radev eric brown valerie 
predictive annotation question answering trec 
voorhees harman 
dong ho shin yu kim sun kim jae hong eom shin tak zhang 
trec experiments 
voorhees harman 
amit singhal steve abney michiel michael collins donald hindle fernando pereira 
trec 
voorhees harman 
rohini srihari wei li 
information extraction supported question answering 
voorhees harman 
toru 
ntt data overview system approach trec ad hoc question answering 
voorhees harman 
voorhees harman editors 
proceedings eighth text retrieval conference trec 
electronic version available trec nist gov pubs html 
